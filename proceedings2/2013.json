[
    {
        "title": "A Comparative Study Of Indian And Western Music Forms.",
        "author": [
            "Parul Agarwal",
            "Harish Karnick",
            "Bhiksha Raj"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416882",
        "url": "https://doi.org/10.5281/zenodo.1416882",
        "ee": "https://zenodo.org/records/1416882/files/AgarwalKR13.pdf",
        "abstract": "Music in India has very ancient roots. Indian classical music is considered to be one of the oldest musical traditions in the world but compared to Western music very little work has been done in the areas of genre recognition, classification, automatic tagging, comparative studies etc. In this work, we investigate the structural differences between Indian and Western music forms and compare the two forms of music in terms of harmony, rhythm, microtones, timbre and other spectral features. To capture the temporal and static structure of the spectrogram, we form a set of global and local frame-wise features for 5genres of each music form. We then apply Adaboost classification and GMM based Hidden Markov Models for four types of feature sets and observe that Indian Music performs better as compared to Western Music. We have achieved a best accuracy of 98.0% and 77.5% for Indian and Western musical genres respectively. Our comparative analysis indicates that features that work well with one form of music may not necessarily perform well with the other form. The results obtained on Indian Music Genres are better than the previous state-of-the-art.",
        "zenodo_id": 1416882,
        "dblp_key": "conf/ismir/AgarwalKR13",
        "keywords": [
            "Music",
            "India",
            "ancient",
            "roots",
            "Indian",
            "classical",
            "music",
            "world",
            "comparative",
            "studies"
        ],
        "content": "A COMPARATIVE STUDY OF INDIAN AND WESTERN MUSIC FORMS\nParul Agarwal1, Harish Karnick2\nIndian Institute of Technology, Kanpur, India\nparulagarwal89@gmail.com1\nhk@cse.iitk.ac.in2Bhiksha Raj\nCarnegie Mellon University, USA\nbhiksha@cs.cmu.edu\nABSTRACT\nMusic in India has very ancient roots. Indian classical\nmusic is considered to be one of the oldest musical tra-\nditions in the world but compared to Western music very\nlittle work has been done in the areas of genre recognition,\nclassiﬁcation, automatic tagging, comparative studies etc.\nIn this work, we investigate the structural differences be-\ntween Indian and Western music forms and compare the\ntwo forms of music in terms of harmony, rhythm, micro-\ntones, timbre and other spectral features. To capture the\ntemporal and static structure of the spectrogram, we form\na set of global and local frame-wise features for 5- genres\nof each music form. We then apply Adaboost classiﬁcation\nand GMM based Hidden Markov Models for four types of\nfeature sets and observe that Indian Music performs bet-\nter as compared to Western Music. We have achieved a\nbest accuracy of 98.0% and 77.5% for Indian and Western\nmusical genres respectively. Our comparative analysis in-\ndicates that features that work well with one form of music\nmay not necessarily perform well with the other form. The\nresults obtained on Indian Music Genres are better than the\nprevious state-of-the-art.\n1. INTRODUCTION\nDue to technology advances the size of digital music col-\nlections have increased making it difﬁcult to navigate such\ncollections. Music content is very often described by its\ngenre [1], though the deﬁnition of genre may differ based\non culture, musicians and other factors. Considerable anal-\nysis has been done on western music for genre classiﬁca-\ntion [2] [3] and content analysis [4].\nAlthough, Indian music, especially classical music, is\nconsidered to be one of the oldest musical traditions in\nthe world, not much computational analytical work has\nbeen done in this area. Indian music can be divided into\ntwo broad categories classical andpopular . Classical mu-\nsic has two main variants Hindustani classical prevalent\nlargely in north and central India and Carnatic classical\nprevalent largely in the south of India. Each variety has\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.multiple genres. For example Hindustani music has Dhru-\npad, Khayal, Tarana as classical genres and Thumri, Dadra,\nTappa, Bhajan as semi-classical genres - amongst others.\nPopular music has multiple folk genres based on region,\nﬁlm music and adhunik or modern music which is inﬂu-\nenced by multiple Indian and western genres.\nAt the base of classical and semi-classical Indian mu-\nsic is a raga(s) which is described as a mood or senti-\nment expressed by a microtonal scale form [28]. A raga\ncan be sung in many Indian genres like Dhrupad, Khayal,\nThumri etc with its corresponding microtonal scale based\non a natural harmonic series. Popular Indian music, on\nthe other hand, is not necessarily based on ragas and can\nhave tonal elements that can differ from the traditionally\naccepted classical norms. Almost all Indian music is based\non melody - single notes played in a given order. Western\nmusic has strong harmonic content i.e. a group of notes\ncalled chords played simultaneously. Unlike Indian tonal\nsystem, the Western tonal system is divided into twelve\nequal intervals. Each Indian genre has its own well-deﬁned\nstructure which makes it different from other forms of mu-\nsic. To capture the inherent structure of Indian music and\nuse it for genre classiﬁcation is a challenging problem.\nThrough our work we try to explore this area and focus\non the following research questions:\n\u000fWhat are the structural differences between Indian\nand Western musical forms?\n\u000fHow well can the audio features used for genre clas-\nsiﬁcation in Western music, capture the characteris-\ntics of Indian genres?\n\u000fIs there a feature set that can be used for cross-cultural\nmusic genre classiﬁcation? In this case, for Indian\nand Western music.\n2. RELEV ANT WORK\n2.1 Study on Genre classiﬁcation by humans\nResearch has been done on the human ability to classify\nmusic genres. Perrot et al. [16](1999) reported that hu-\nmans with little musical training were able to classify gen-\nres with an accuracy of about 72% on a ten-genre dataset of\na music company, based on only 300 milliseconds of audio.\nSoltau (1997) conducted a Turing-test in which he trained\n37 people to distinguish rock from pop music and foundthat his automatic music classiﬁer was similar to their per-\nformance on the same dataset. Lippens et al. [15](2004)\nconducted an experiment with six genres and observed that\npeople were able to classify with an accuracy of around\n90%. Considerable work has been done in the area of\nautomatic music genre classiﬁcation and with increasing\nimprovement in classiﬁcation methods there is a distinct\npossibility that they may outperform humans in the near\nfuture.\n2.2 Genre classiﬁcation of Western Music\nMany supervised and unsupervised methods have been used\nto automatically classify recordings into genres. Commonly\nused supervised classiﬁcation methods are k-nearest Neigh-\nbours (KNN) [5] [17], Support Vector Machines (SVMs)\n[19] [17], Gaussian Mixture Models (GMMs) [5] [20] [17],\nLinear Discriminant Analysis (LDA) [17] [18], non-negative\nMatrix Factorization (NMF) [2], Artiﬁcial Neural Networks\n(ANNs) and Hidden Markov Models (HMMs). In the un-\nsupervised domain, there is work using the k-means ap-\nproach and hierarchical clustering.\nTo the best of our knowledge, there is no work in the\nliterature that analyses which features are best for music\ngenre classiﬁcation. There is work using Fourier analy-\nsis, cepstral analysis and wavelet analysis [17] [24]. Li\net al. [17] and J. Bergstra et al. [3] worked with a set of\nparameters like spectral centroid, spectral rolloff, spectral\nﬂux, zero crossings and low energy. Y .Panagakis et al. [25]\nused sparse representation-based classiﬁers on audio tem-\nporal modulations of songs for genre classiﬁcation. Sala-\nmon et al. [26] used a combination of low level features\nwith high level melodic features derived from pitch con-\ntours and showed that the combination outperforms the re-\nsults from using only low level features.\nTzanetakis et al. [5] proposed to classify musical genres\nusing K -Nearest Neighbours and GMMs based on timbral\ntexture and rhythmic features. E. Benetos et al. [2] used\nnon-negative tensor factorization (NTF) with GTZAN data\nset [5]. J. Bergstra et al. [3] used GMM based Adaboost\nclassiﬁcation with aggregate features dividing frames into\nsegments. Hidden Markov Models were used by T. Lan-\nglois et al. [10] for genre classiﬁcation based on timbre\ninformation.\n2.3 Genre classiﬁcation of Indian Music\nCompared to western music very little work has been done\nin Indian musical genre classiﬁcation. S.Jothilakshmi et\nal. [12] used kNN and GMM on spectral and perceptual\nfeatures and achieved an accuracy of 91.23% with 5 In-\ndian genres- Hindustani, Carnatic, Ghazal, Folk and Indian\nwestern. P. Chordia et al. [13] used pitch proﬁles and Pitch-\nClass Distributions for raga identiﬁcation. A. Vidwans et\nal. [14] extracted melodic contours to classify Indian clas-\nsical vocals. They also performed a listening test in which\naround 85% samples were correctly identiﬁed by people\nwithout any training.2.4 Contributions of the paper\nThere is no study, to the best of our knowledge, that com-\npares classiﬁcation of Indian musical genres with West-\nern musical genres. Our work makes the following con-\ntributions: ﬁrst, this paper introduces a new research prob-\nlem on cross-cultural music genre classiﬁcation by doing a\ncomparative study on Indian and Western musical forms in\nterms of their structure and characteristics. Second, while\nthere are many standard datasets on genre classiﬁcation\navailable for Western music like GTZAN dataset [5], 2005\nMIREX dataset, Magnatune [21], USPOP [22] etc. There\nis no standard dataset for Indian genre classiﬁcation. For\nour study, we built a dataset of 5 Indian genres- Dhru-\npad, Thumri, Carnatic, Punjabi and Ghazals. There are\n100 samples in each genre where each sample is an audio\nclip of 30 seconds length. This dataset can be used for\nfuture work on Indian music genre classiﬁcation. Third,\nwe have incorporated short-time and long-time features to\ncapture the static and temporal structure of the spectro-\ngram and have analysed both cultural forms. Fourth, we\npropose a novel approach of using timbre and chromagram\nfeatures with Gaussian Mixture Models(GMM) based Hid-\nden Markov Models to identify the patterns in Indian music\nwhich gave an overall accuracy of 98.0%, which is better\nthan the previous state-of-the-art [12] which also worked\nwith 5-genres.\n2.5 Outline of Paper\nIn section 3 we discuss the dataset used for the experiments\nand the structure and characteristics that make an Indian\ngenre distinct from other genres. Sections 4 and 5 give a\ndetailed explanation of the features chosen and classiﬁers\nused. Section 6 discusses the experimental results. Section\n7 outlines possible future work.\n3. DATASET\nIn this section, we look at the structural differences be-\ntween Indian and Western music forms and discuss the\ncharacteristic features of the Indian genres used in the ex-\nperiment.\n3.1 Indian Music\nFor the experiments we have considered ﬁve popular gen-\nres - Hindustani (Dhrupad + Thumri), Carnatic, Folk (Pun-\njabi) and Ghazal. We constructed our own dataset because\nno standard dataset is available. Each genre contains 100\naudio recordings each 30 seconds long extracted randomly\nfrom recordings available on-line. All samples are vocals.\n3.1.1 Indian classical Music\nIndian classical music is based on melody without the pres-\nence of any major harmonic structure. It has seven basic\nnotes with ﬁve interspersed half- notes, resulting in a 12-\nnote scale. In Indian music any frequency can be set as\nthe base frequency, known as swara (note) ’Sa’ with other\nnotes taking frequency values following their ﬁxed ratioswith respect to Sa. Indian classical music has three im-\nportant characteristics- raga (melodic aspect of music) and\ntaal(cycle of ﬁxed number of beats repeated over and over\nas shown in Figure 1) and a drone (a sustained note). Indian\npopular music have the the melodic (though not raga) and\ntaal/beat components but do not usually have the drone .\nFigure 1 . Repetition of beats (taal) in Indian Classical\nDhrupad : Dhrupad is a very old form of Hindustani\nmusic that is primarily devotional in content. It has two\nmajor parts- Alap (sung without words) and Dhrupad (ﬁxed\ncomposition part) accompanied by Pakhawaj, a two headed\nbarrel drum. A performance begins slowly with alap where\nusually the syllables of the following mantra is recited:\n”Om Anant tam Taran Tarini Twam Hari Om Narayan\nAnant Hari Om Narayan” . In alap an artist develops each\nnote and uncovers the personality of the chosen raga. The\ntempo gradually increases and an alap ends after exploring\nthree octaves. Then begins the dhrupad in which the artist\nis joined by the Pakhawaj. It is usually set to the following\ntalas/taal : tala chautal (12 beats), tivra (7 beats), sulfak (10\nbeats), matt (9 beats), or farodast (14 beats). In the dataset\nsamples were extracted from dhrupad performances where\nabout 25% of them were by female artists.\nThumri : Thumri is a semi-classical form of Hindustani\nmusic which is romantic and devotional in nature. The text\nrevolves around a girl’s love for Lord Krishna. The lyrics\nare written in dialects of Hindi called Awadhi and Brij\nBhasha. Thumri has greater ﬂexibility and more liberties\nare taken with the raga as compared to Dhrupad or Khayal.\nThe compositions are usually of 8 beats ( kaherava ), 14\nbeats ( dipchandi ) or of 16 beats ( addha tala ). Samples\nwere extracted from 10 Thumri performances where 3 were\nby female artists.\nCarnatic : Carnatic music is the classical music of South-\nern India. Like Hindustani music, it is based on ragas and\ntaals . Performances in Carnatic music are divided into\na number of sections. It starts with a varanam , literally\nmeaning ”a description”, which unfolds the important fea-\ntures of the raga being performed. Then comes the ﬁxed\ncomposition in the raga, known as kritis . The main com-\nposition is alapana , which explores the raga and uses the\nsounds like aa, ri, na, ta etc to elaborate the notes. This\nstarts slowly and ﬁnally builds a complicated exposition ofthe raga without any rhythmic accompaniment. There are\nniruval andkalpana swara(notes) that provide opportuni-\nties to improvise. In the dataset, about 55% of the samples\nare sung by female singers.\nHindustani and Carnatic music differs mainly in the fol-\nlowing characteristics-\n\u000fHindustani music like Dhrupad gradually builds up\ntempo from very slow to very fast while in Carnatic\nmusic there is a constant and fairly fast tempo through-\nout.\n\u000fIn Hindustani music notes are held longer and are\nmostly approached and released by ornaments or small\nornamental phrases whereas in Carnatic music notes\nare not held for long and usually released by a char-\nacteristic oscillation using indeterminate pitch.\n\u000fHindustani music has more improvisation around the\nraga unlike Carnatic which has a some what more\nrigid structure and is composition based.\n3.1.2 Folk- Punjabi\nPunjabi music has a smaller range which seldom extends\nbeyond an octave. It is accompanied by a lively rhythm\n(chaal ) and is usually based on kaherava taal , a cycle of\neight beats. Greater emphasis of the music is on the lyrical\nnature of the songs. The characteristic which makes it dif-\nferent from other music forms is the use of Dhol - a barrel\nshaped drum that gives a bass sound. Samples are taken\nfrom Folk N Duet 2010 Album contributed by 16 singers\nwhich includes 3 female singers.\n3.1.3 Ghazal\nGhazal is deﬁned as a poetic genre and consists of a se-\nries of rhymed verses, each symmetrically divided into two\nhalf-verses. The structure of a Ghazal is deﬁned by its me-\ntre and rhyme which are maintained throughout the poem.\nSamples were extracted from ghazal performances where\n19% of total pieces were sung by female singers.\n3.2 Western Music\nFor western music we considered the GTZAN dataset [5].\nIt consists of 10 distinct genres- Blues, Classical, Country,\nDisco, Hip hop, Jazz, Metal, Pop, Reggae and Rock. Each\ngenre is represented by 100 samples of 30 seconds length\neach. Since we have 5 genres for the Indian music, we\nconsidered only top 5 genres (Classical, Hiphop, Jazz, Pop\nand Reggae) from the GTZAN dataset based on the results\nof [5].\n4. FEATURE EXTRACTION\nFor feature extraction, we used the MIRToolbox [27], an\nopen source toolbox for musical feature extraction. For\nthe experimental set-up we used three types of feature sets\nas described below:\n4.1 Feature set 1- Frequency of chromagram notes\nA chromagram is a redistribution of the spectrum energy\nalong the different pitch levels (or chromas). We have con-sidered 12 pitch classes- C, C#, D, D#, E, F, F#, G, G#, A,\nA# and B. We used a framesize of 100 milli-seconds with\n50% overlap. The choice of a large framesize is to cover\nfor the semitone energies in lower frequencies. For each\nframe, we assigned the note that has the maximum energy\nin the frame’s 12-dimensional chromagram vector; we call\nthis the ”dominant note”. The use of the dominant note in\neach frame captures the melodic information as the melody\nnote is the one that typically dominates the feature. Con-\ncatenating the dominant note in each frame gave us a note\nsequence representing the sample. We then formed a 12-\ndimensional vector of the normalized frequencies (counts)\nof the notes in the note sequence. This feature set is moti-\nvated by the repetition of notes in Indian music and the use\nof chromagrams in [6] and [7].\n4.2 Feature set 2- Global features\nWe extract a set of global features based on the main di-\nmensions of music which includes melody, rhythm, timbre\nand spatial features as described in [8] [9] and [10]. We\nused the following features to form a vector of 86 global\nfeatures-\n\u000fEnergy features - mean and variance of the energy\nenvelope, Root Mean Square (RMS) energy, low-\nenergy rate(percentage of frames having less than\naverage energy).\n\u000fRhythm features - mean and variance of notes on-\nset time (successive bursts of energy indicating es-\ntimated positions of the notes), event density (aver-\nage frequency of events, i.e., the number of note on-\nsets per second), tempo, pulse clarity [11](strength\nof beats).\n\u000fPitch features - mean and variance of pitch.\n\u000fTonality features - 12-chromagram pitch class ener-\ngies, 24- key strength major and minor (cross cor-\nrelation of the chromagram), 6- dimensional tonal\ncentroid vector from the chromagram (corresponds\nto projection of the chords along circles of ﬁfths, of\nminor thirds, and of major thirds).\n\u000fTimbre features - mean and variance of attack time\nof notes onset, rolloff frequency, brightness (percent-\nage of energy above 1500Hz), 13 Mel-Frequency\nCepstral Coefﬁcients (MFCCs), roughness(sensory\ndissonance), irregularity (degree of variation of the\nsuccessive peaks of the spectrum)\n\u000fSpectral-shape features - zero-crossing rate, spread,\ncentroid, skewness, kurtosis, mean and variance of\nInverse Fourier Transform of logarithm of spectrum,\nﬂatness, mean and variance of spectrum, mean and\nvariance of spectral ﬂux, mean and variance of spec-\ntrum peaks.\nThese features are taken to capture the static structure\nof the spectrogram.\n4.3 Feature set 3- Frame-wise features\nWe analyse each song using a framesize of 100 milli sec-\nonds with 50% overlap and extract the following features:\n12-chromagram features, 13 MFCC, 13 delta- MFCC, 13delta-delta-MFCC, 11 spectral features (zero-crossing rate,\nrolloff, brightness, centroid, spread, skewness, kurtosis,\nﬂatness, entropy, roughness and irregularity). For the chro-\nmagram features, we ﬁrst perform a logarithm transform\nand then a discrete cosine transform (DCT) which essen-\ntially decorrelates the features. We use these modiﬁed chro-\nmagram features with the above mentioned features to get\na vector of 62 features for each frame. A sample is now\nrepresented by a 2-D matrix of size 62\u0002frames , where\nframes are the number of frames in the sample. These fea-\ntures carry the temporal information of the spectrogram.\n5. EXPERIMENTAL RESULTS\nWe performed four experiments based on the above three\nfeature sets. For each run, we randomly divided the dataset\ninto 80% train and 20% test.\n5.1 Experiment 1: Adaboost on Feature set 1\nWe train One-Vs-One Adaboost classiﬁers. A sample is\nassigned the class which gets the maximum votes. In case\nof a tie, the class with the lower index is the predicted la-\nbel. Table 1 shows the confusion matrix for 10 runs of the\nexperiment for Indian Music and Western Music. Over-\nall the accuracies obtained for Indian Music and Western\nMusic are 58.70% and 46.90% respectively.\nTable 1 .Confusion Matrix (in %) for Exp 1\n(a) Indian Music\nCa Dh Gh Pu Th\nCa 66.50 9.00 5.50 6.50 12.50\nDh 10.50 64.00 6.00 9.00 10.50\nGh 16.50 6.50 64.50 7.00 5.50\nPu 18.50 12.50 14.00 47.50 7.50\nTh 19.50 6.50 13.00 10.00 51.00\nLegend: Ca:Carnatic, Dh:Dhrupad, Gh:Ghazal, Pu:Punjabi, Th:Thumri\n(b)Western Music\nCl Hi Ja Po Re\nCl 60.00 8.00 13.50 12.50 6.00\nHi 3.50 58.50 12.00 14.00 12.00\nJa 8.50 21.00 48.00 13.00 9.50\nPo 9.50 18.50 22.00 32.00 18.00\nRe 14.00 17.50 16.50 16.00 36.00\nLegend: Cl:Classical, Hi:Hip-hop, Ja:Jazz, Po:Pop, Re:Reggae\n5.2 Experiment 2: Adaboost on Feature set 2\nThe Adaboost classiﬁcation technique is used in a manner\nsimilar to Experiment 1. The confusion matrices for Indian\nmusic and Western music are shown in Table 2. The overall\naccuracies obtained are: Indian music- 87.30%, Western\nmusic- 74.50%.\n5.3 Experiment 3: Adaboost on Feature set 1+2\nAdaboost classiﬁcation technique is used similar to Exper-\niment 1. Here we take a combination of feature set 1 and\nfeature set 2 for the training and testing. Table 3 shows the\nconfusion matrices. The overall accuracies achieved are:\nIndian music- 87.80% , Western music- 77.50%.Table 2 .Confusion Matrix (in %) for Exp 2\n(a) Indian Music\nCa Dh Gh Pu Th\nCa 89.50 1.50 5.00 0.50 3.50\nDh 5.00 90.00 2.50 0 2.50\nGh 5.50 6.50 81.00 1.50 5.50\nPu 3.00 0 2.00 95.00 0\nTh 10.50 5.00 3.50 0 81.00\nLegend: Ca:Carnatic, Dh:Dhrupad, Gh:Ghazal, Pu:Punjabi, Th:Thumri\n(b)Western Music\nCl Hi Ja Po Re\nCl 87.50 0 10.50 0 2.00\nHi 0.50 69.50 0.50 14.00 15.50\nJa 9.00 2.00 78.50 0 10.50\nPo 1.50 10.50 3.00 74.50 10.50\nRe 4.50 18.50 4.00 10.50 62.50\nLegend: Cl:Classical, Hi:Hip-hop, Ja:Jazz, Po:Pop, Re:Reggae\nTable 3 .Confusion Matrix (in %) for Exp 3\n(a) Indian Music\nCa Dh Gh Pu Th\nCa 90.00 1.50 4.00 0.50 4.00\nDh 4.50 91.00 2.00 0 2.50\nGh 6.50 5.50 81.50 2.00 4.50\nPu 3.00 0 2.00 95.00 0\nTh 7.50 3.50 7.50 0 81.50\nLegend: Ca:Carnatic, Dh:Dhrupad, Gh:Ghazal, Pu:Punjabi, Th:Thumri\n(b)Western Music\nCl Hi Ja Po Re\nCl 90.50 0 9.00 0.50 0\nHi 0.50 79.00 1.00 12.50 7.00\nJa 10.50 2.00 78.00 0.50 9.00\nPo 0.50 15.00 1.50 72.50 10.50\nRe 3.50 11.50 9.50 8.00 67.50\nLegend: Cl:Classical, Hi:Hip-hop, Ja:Jazz, Po:Pop, Re:Reggae\n5.4 Experiment 4: HMM with GMM based on\nFeature set 3\nWe train Hidden Markov Models on Feature set 3 with 62\nhidden states and full covariance matrix. The emissions of\nall states were randomly initialized and the states need not\nrepresent exactly the underlying feature. The algorithms\nuses maximum likelihood parameter estimation using Ex-\npectation Maximization. The state emissions were 62 di-\nmensional chroma and timbre data modelled by Gaussian\nmixtures with 8 Gaussians. We used the Hidden Markov\nModel Toolbox of Matlab by [23] for the same. The confu-\nsion matrix for Indian music and Western music are shown\nin Table 4. Overall accuracies of 98.00% and 67.00% were\nobtained for Indian and Western music respectively.\n6. DISCUSSION\nOur experiments performed better on Indian music than on\nWestern music for given classiﬁcation techniques and sets\nof features (Figure 2). The differences in performance of\nthe two cultural forms of music can perhaps be traced to the\nmore well-deﬁned structural form and strong melodic con-\ntent of Indian Music. In Indian music, melody and rhythmTable 4 .Confusion Matrix (in %) for Exp 4\n(a) Indian Music\nCa Dh Gh Pu Th\nCa 95.00 0 3.00 2.00 0\nDh 0 99.00 1.00 0 0\nGh 1.00 0 98.50 0.50 0\nPu 0 0.500 0 99.50 0\nTh 1.00 0 1.00 0 98.00\nLegend: Ca:Carnatic, Dh:Dhrupad, Gh:Ghazal, Pu:Punjabi, Th:Thumri\n(b)Western Music\nCl Hi Ja Po Re\nCl 85.00 0 10.00 0 5.00\nHi 0 75.00 5.00 15.00 5.00\nJa 25.00 15.00 55.00 5.00 0\nPo 0 20.00 5.00 65.00 10.00\nRe 0 15.00 0 30.00 55.00\nLegend: Cl:Classical, Hi:Hip-hop, Ja:Jazz, Po:Pop, Re:Reggae\noffer a variety of cues that do not seem to be present in\nWestern music.\nFigure 2 . Comparison of performance of Indian and West-\nern Genres on four experiments\nThe better performance of Indian music over Western\nmusic on similar experiments answers the second question\nof our research problem which suggests that the state-of-\nthe-art features (spectral, timbre, harmony etc) used for\nWestern music genre classiﬁcation can be applied to Indian\nmusic where they show higher accuracies.\nIn Indian music, the classical forms (Carnatic and Dhru-\npad) performed better than Ghazal. This is may be be-\ncause the Indian classical music is more structured as it has\n3 components of raga (melody), taal(rhythm) and drone\n(sustained note) and has a strong melodic structure. In last\nthree experiments Punjabi music has lowest confusion be-\ncause its two characteristics features: lively rhythm and\ndistinct timbre due to bass sound produced by Dhol ; are\naccounted for in features used. In Western music, Classi-\ncal and Jazz performed better than other genres, with Reg-\ngae performing the worst. These results are consistent with\nG.Tzanetakis et al. [5].\nPerformance on Feature set 1 (which is just based on\nthe frequency of chromagram notes) for Indian music andWestern music is about 59% and 47% respectively. This\nis expected as the Indian music is based on melody, i.e. a\nsequence of notes played in a given order. Whereas, the\nWestern music is more harmonic in nature, i.e. group of\nnotes played simultaneously, which is not captured by a\nsingle dominant note in Feature set 1.\nIn Feature set 2 we have used the 12-chromagram pitch\nclass energies which tries to capture the dominance order\nof notes. The dominance order of notes is also represented\nby the frequency of chromagram notes in Feature set 1.\nThus, there is no signiﬁcant difference in accuracies of Ex-\nperiment 2 (using Feature set 2) and Experiment 3 (using\nFeature set 1+2).\nThe highest accuracy of 98.0% for Indian music genres\nin Experiment 4 is better than S.Jothilakshmi et al. [12] and\nthe state-of-the-art to the best of our knowledge. The ma-\njor difference in accuracies between local frame-wise fea-\ntures and global features in case of Indian music may be\nbecause the local frame-wise features are better in captur-\ning the characteristics of Indian music like raga notes and\ntaal(repetition of beats) which require small size windows\nto be analysed. An accuracy of 77.5% for Western music\ngenres in Experiment 3 is better than G.Tzanetakis et al. [5]\non the same dataset of ﬁve genres (Classical, Hiphop, Jazz,\nPop and Reggae).\n7. FUTURE WORK\nThis work can be extended in various ways: forming a\n‘golden-set’ of features that are genre-speciﬁc like rhyme\nin Ghazal, beats in Folk Punjabi etc.; recognition of pat-\nterns like taalin Indian music and chords in Western mu-\nsic; expansion of classes in terms of genres and sub-genres\nso that we can work with more classes in both datasets\n(GTZAN has 10-genres); studying music forms of other\ncultures for example Chinese and Japanese and comparing\nthem with Indian and Western genres.\n8. REFERENCES\n[1] J. J. Aucouturier, F. Pachet. ”Representing musical genre: A\nstate of the art” Journal of New Music Research , pp. 83-93,\n2003.\n[2] E. Benetos, C. Kotropoulos ”A tensor-based approach for au-\ntomatic music genre classiﬁcation” Proc. EUSIPCO,2008\n[3] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, B. Kegl ”Ag-\ngregate features and Adaboost for music classiﬁcation” Ma-\nchine Learning vol. 65, no. 2-3, pp. 473-484, 2006\n[4] S. Sukittanon, L. E. Atlas,J. W Pitton ”Modulation-scale\nanalysis for content identiﬁcation” IEEE Trans. Signal Pro-\ncessing vol. 52, no. 10, pp 3023-3035, Oct. 2004\n[5] George Tzanetakis and Perry Cook “Music Genre Classiﬁ-\ncation of Audio Signals” IEEE Transactions on Speech and\nAudio Processing vol. 10, no. 5, pp. 293-302, 2002.\n[6] D. P. W. Ellis and G. E. Poliner. “Identifying cover songs with\nchroma features and dynamic programming beat tracking”\nICASSP Hawaii, USA 2007.\n[7] S. Kim and S. Narayanan “Dynamic chroma feature vec-\ntors with applications to cover song identiﬁcation” In MMSP\nCairns, Australia, 2008.[8] N. Scaringella, G. Zoia, and D. Mlynek, “Automatic Genre\nClassiﬁcation of Music Content: A Survey“ IEEE Signal\nProcessing Magazine 2006, 23(2), pp. 133141.\n[9] Kaichun K. Chang, Jyh-Shing Roger Jang, and Costas S. Il-\niopoulos. ”Music Genre Classiﬁcation via Compressive Sam-\npling“ 11th International Society for Music Information Re-\ntrieval Conference (ISMIR 2010) 2010, pp. 387-392.\n[10] Thibault Langlois, and G. Marques ”A Music Classiﬁca-\ntion Method Based on Timbral Features” 10th International\nSociety for Music Information Retrieval Conference (ISMIR\n2009) , 2009, pp. 81-86.\n[11] Olivier Lartillot, Tuomas Eerola, Petri Toiviainen, Jose\nFornari ”Multi-feature modelling of pulse clarity: Design,\nvalidation, and optimization” International Conference on\nMusic Information Retrieval Philadelphia, 2008\n[12] S.Jothilakshmi,N.Kathiresan ”Automatic Music Genre Clas-\nsiﬁcation for Indian Music” ICSCA 2012\n[13] P. Chordia and A. Rae ”Raag recognition using pitch-class\nand pitch-class dyad distributions” Proc. of ISMIR, 2007\npp.431436.\n[14] A.Vidwans, K.K. Ganguli and Preeti Rao ”Classiﬁcation of\nIndian Classical V ocal Styles from Melodic Contours” Proc.\nof the 2nd CompMusic Workshop , Istanbu, Turkey, July 12-\n13, 2012\n[15] J. Martens Lippens, S. and T. De Mulder ”A comparison of\nhuman and automatic musical genre classiﬁcation” In IEEE\nInternational Conference on Acoustics, Speech, and Signal\nProcessing , volume 4, pages 233236, 2004\n[16] D. Perrot and R. R. Gjerdigen. ”Scanning the dial: an ex-\nploration of factors in the identiﬁcation of musical style” In\nProceedings of the 1999 Society for Music Perception and\nCognition , 1999.\n[17] M. Ogihara Li, T. and Q. Li. ”A comparative study on\ncontent-based music genre classiﬁcation” In Proceedings of\nthe 26th annual international ACM SIGIR conference on\nResearch and development in information retrieval , pages\n282289, 2003.\n[18] West, K. and S. Cox, ”Features and classiﬁers for the auto-\nmatic classiﬁcation of musical audio signals” Proc. 5th In-\nternational Conference on Music Information Retrieval (IS-\nMIR) , 2004\n[19] Michael I Mandel and Daniel P.W.Ellis. ”Song-level features\nand support vector machines for music classiﬁcation” In In-\nternational Society for Music Information Retrieval , 2005.\n[20] A. Flexer Pampalk, E. and G. Widmer ”Improvements of\naudio-based music similarity and genre classiﬁcation” In\nCrawford and Sandler , 2005.\n[21] www.magnatune.com (Access date: 5 May 2013)\n[22] www.ee.columbia.edu/ dpwe/research/musicsim/ (Access\ndate: 5 May 2013)\n[23] http://www.cs.ubc.ca/ murphyk/Software/HMM/hmm.html\n(Access date: 5 May 2013)\n[24] C. Marques, I. R. Guilherme, R. Y . M. Nakamura and J. P.\nPapa ”New trends in musical genre classiﬁcation using opti-\nmum path forest” ISMIR , 2011.\n[25] Yannis Panagakis, Constantine Kotropoulos, Gonzalo R.\nArce ”Music genre classiﬁcation via sparse representations\nof Auditory Temporal Modulations” EUSIPCO ,2009.\n[26] Justin Salamon, Bruno Rochay and Emilia Gomez ”Music\ngenre classiﬁcation using melody features extracted from\npolyphonic music signals” ICASSP ,2012.\n[27] Olivier Lartillot, Petri Toiviainen ”A Matlab Toolbox for Mu-\nsical Feature Extraction From Audio” International Confer-\nence on Digital Audio Effects , Bordeaux, 2007\n[28] ”Raga” http://www.britannica.com/EBchecked/topic/489518\n(Access date: 5 May 2013)"
    },
    {
        "title": "Verifying Music Tag Annotation Via Association Analysis.",
        "author": [
            "Tom Arjannikov",
            "Chris Sanden",
            "John Z. Zhang"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417123",
        "url": "https://doi.org/10.5281/zenodo.1417123",
        "ee": "https://zenodo.org/records/1417123/files/ArjannikovSZ13.pdf",
        "abstract": "Music tags provide descriptive and rich information about a music piece, including its genre, artist, emotion, instrument, etc. While many work on automating it, at present, tag annotation is largely a manual process. It often involves judgements and opinions from people of different",
        "zenodo_id": 1417123,
        "dblp_key": "conf/ismir/ArjannikovSZ13",
        "keywords": [
            "automating",
            "music tags",
            "descriptive",
            "information",
            "genre",
            "artist",
            "emotion",
            "instrument",
            "manual process",
            "judgements"
        ],
        "content": "VERIFYING TAG ANNOTATIONS THROUGH ASSOCIATION ANALYSIS\nTom Arjannikov\nUniversity of Lethbridge\ntom.arjannikov@uleth.caChris Sanden\nUniversity of Lethbridge\nsanden@cs.uleth.caJohn Z. Zhang\nUniversity of Lethbridge\nzhang@cs.uleth.ca\nABSTRACT\nMusic tags provide descriptive and rich information about\na music piece, including its genre, artist, emotion, instru-\nment, etc. While many work on automating it, at present,\ntag annotation is largely a manual process. It often in-\nvolves judgements and opinions from people of different\nbackground and level of musical expertise. Therefore, the\nresulting tags are usually subjective, ambiguous, and error-\nprone. To deal with this situation, we seek automatic meth-\nods to verify and monitor this process. Furthermore, be-\ncause multiple tags can annotate each music piece, our task\nlends itself to multi-label methods which capture the inher-\nent associations among annotations in a given music repos-\nitory. In this paper, we propose a novel approach to verify\nthe quality of music tag annotations via association anal-\nysis. We demonstrate the effectiveness of our approach\nthrough a series of simulations using four publicly avail-\nable music datasets. To our knowledge, our work is among\nthe initial efforts in verifying music tag annotations.\n1. INTRODUCTION\nDue to the advances in technology, such as data storage and\ncompression, media processing, information retrieval, and\nthe Internet, digital music collections have been growing\nenormously in volume. Millions of songs previously in\nphysical formats are now readily available through on-line\naccesses, presenting many new challenges in related ﬁelds.\nAmong them is Music Information Retrieval (MIR), an\ninterdisciplinary area that attracts practitioners from com-\nputer science, cognitive science, information retrieval, mu-\nsicology, psychology, etc. One of its current tasks is the\ndesign and implementation of algorithmic approaches for\nmanaging large collections of digital music; these include\nbut are not limited to music classiﬁcation, automatic tag\nannotation, recommendation and playlist generation. [6]\nCurrently, it is a common strategy to organize and ac-\ncess digital music collections via textual meta-data , usu-\nallytags, such as genre ,style,mood ,artist , etc. The pro-\ncess of annotating a music piece with appropriate meta-\ndata is referred to as music tag annotation . It relies heavily\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.on music experts as well as amateurs [19]. Due to the am-\nbiguity and subjectivity introduced in the annotation pro-\ncess, music tags can be inconsistent, incomplete and some-\ntimes even error-prone, making it difﬁcult to maintain them\nin large music collections [12]. Moreover, the manual an-\nnotation process can be complex, involving substantial ﬁ-\nnancial and labor costs [5]. These are among the many\nissues, which automatic approaches aim to tackle.\n1.1 Previous work\nAutomatic music tag annotation is an important problem\nin MIR with numerous applications. For instance, an accu-\nrate prediction of tags is often the ﬁrst step toward making\nmusic recommendations. As of recently, music tag annota-\ntion has received considerable attention and many related\ntechniques have been proposed.\nTurnbull et al. [18] employ a generative probabilistic\nmodel and propose one of the ﬁrst automatic tag anno-\ntation systems. In addition, they create a music dataset\ncalled The Computer Audition Lab 500 (CAL500 ), which\nhas since become a de facto benchmark for evaluating the\nperformance of tag annotation systems.\nHoffman et al. [8] present another probabilistic model,\ncalled the Codeword Bernoulli Average , which attempts to\npredict the probability that a tag can be associated with a\nmusic piece. In addition, Bertin-Mahieux et al. [3] propose\nAutotagger , a model that uses ensemble learning schemes\nto associate tags with music pieces. Ness et al. [10] de-\nscribe how stacked generalization of the probabilistic out-\nputs of a Support Vector Machine (SVM ) can be used to\nimprove the performance of automatic tag annotation.\nShen et al. [16] propose a framework called MMTag-\ngerthat combines advanced feature extraction techniques\nand high-level semantic concept modeling for music tag\nannotation. The proposed framework uses a multilayer ar-\nchitecture that gathers multiple Gaussian mixture models\nand SVMs.\nSanden and Zhang [13] treat music tag annotation as a\nmulti-label classiﬁcation problem and discuss various is-\nsues related to tag annotation through extensive experi-\nments using a set of a multi-label classiﬁers and a set of\nevaluation measures.\nNeubarth et al. [11], use association analysis to ﬁnd\ndifferent, musicologically motivated, associations between\nvarious folk music genres and their geographical distribu-\ntion.Set of \nTagsMusic \nRepository DStage 1 Stage 2\nApriori\nAlgorithmRules set \n(TARs)\nInvoke the apriori algorithm and \ngenerate Tag Annotation Rules setStage 3 Stage 4\nExperts Some peopleAuto-\nmatic\nOR\nMusic SetPTAR\nGTAR\nZTAR\nSTAR Review\nAdjustM\nMTR\nExpertsScoring\nAlgorithmFigure 1 : The four stages of our proposed approach to veriﬁcation of tag annotations through association analysis.\n1.2 Tag Annotation\nWe follow Ness et al. [10] in their formulation of the mu-\nsic tag annotation process as follows. Given a set of tags\nT=ft1;t2;:::;t Agand a set of music pieces M=fm1;\nm2; :::; m Rg, then each music piece mj2Mis an an-\nnotation vector A= (a1,a2,\u0001\u0001\u0001,aA), whereai>0if tag\ntihas been associated with the piece, and ai= 0, other-\nwise. These ai’s, referred to as semantic weights , describe\nthe strength of the semantic correspondence between a tag\nand its music piece. When mapped to a binary assign-\nment off0,1g, the semantic weights can be interpreted as\nclass labels, i.e., whether a tag is assigned to the music\npiece or not. Naturally, each music piece can have multi-\nple tags [10, 13].\nTags for a given music piece reveal the inherent musical\nnature that it attempts to convey and express. As a coherent\nexpression, these tags represent features that distinguish\nthis music piece from others. This expression intuitively\nshows a strong association of these tags to the music piece\nor to a set of similar music pieces in terms of their musical\nnature. We work toward this intuition and aim to capture\nassociations between tags and utilize them to verify the an-\nnotation process.\n1.3 Association Analysis\nAssociation analysis attempts to discover the inherent re-\nlationships among data objects in an application domain.\nSuch relationships are represented as association rules. An\nexample of such application domain is the basket analy-\nsis in supermarkets, where one tries to discover the rela-\ntionships among commodities in baskets. For example, the\nassociation rulefmilkg,feggsg!fbreadgimplies that,\nifmilk andeggs are bought together by a customer, then\nbread is likely to be bought as well, i.e., they have some\ninherent statistical relationships.\nLetI=fi1,i2,\u0001\u0001\u0001,ingbe a set ofnitems andT=\nft1,t2,\u0001\u0001\u0001,tmgbe a transactional database, where each\ntransactionti2Tis a nonempty subset of I. An asso-\nciation rule is of the form A!B, whereAandBare\ncalled itemsets andA\u0012IandB\u0012I. It is required that\nATB=\u001e.Ais called the antecedent of the rule while B\nis its consequent . We say that the rule A!Bholds forT\nwith supports, wheresis the percentage of transactions in\nTthat contain both AandB, and with conﬁdencec, where\ncis the percentage of transactions containing Athat also\ncontainB, i.e,support (ASB)=support (A).It should be noted that our work presented in this paper\nfocuses on the application of association analysis to tag an-\nnoation in music. We employ one of the many algorithms\nthat ﬁnd the association rules in a transactional database,\ntheApriori algorithm [1, 2], which generates association\nrules that satisfy user-speciﬁed minimum support andmin-\nimum conﬁdence . For a simple but illustrative example of\nassociation rule generation, see [7].\n2. VERIFYING TAG ANNOTATIONS\nIn our experience, we observe that tag annotation by music\nexperts, though at a high cost, is relatively more accurate,\ndue to their music expertise, as compared to the one per-\nformed by amateurs. Based on this, we propose to take\nadvantage of the judgments and opinions of music experts\nfor a tag annotation task. It is our belief that tags of high\nquality reveal inherent associations among music pieces in\na repository and can be utilized in a tag annotation pro-\ncess. Our proposed approach to verifying tag annotations\nis conducted in four stages, as shown in Figure 1.\nFirst, given an annotation task, a group of experts with\nextensive music background and experience are solicited.\nThey examine the repository and select a set Dcomprised\nof those songs that best represent the repository as a whole.\nThen, they manually annotate this set using a predeter-\nmined and ﬁxed set of tags. Consequently, the tag anno-\ntations contain expert knowledge and show authoritative\njudgments and opinions of those experts regarding the rep-\nresentative music pieces.\nIn the second stage, we treat the selected music pieces\nas a transactional database, where tags are the transactional\nitems and music pieces are the transactions. We then in-\nvoke the Apriori [1] algorithm to ﬁnd the association rules\nin it under the current minimum support and minimum\nconﬁdence, both of which are speciﬁed in advance. We de-\nnote the resulting association rule set R=fr1;r2;\u0001\u0001\u0001;rkg,\nwhere eachriis called a tag annotation rule .\nDuring the third stage, we introduce new music pieces\nwhose annotations are to be veriﬁed. They could be sam-\nples from the same music repository, which would be the\ncase where the task is to verify the annotations of a given\nmusic repository. They could also be an addition to the ex-\nisting repository, which would happen when the repository\ngrows and the newly added music pieces need to contain\nannotations that are consistent with the existing repository.\nWe collectively call these new music pieces M=fm1,m2,\u0001\u0001\u0001,mng; they are annotated manually or automati-\ncally as discussed in Section 1.1. We represent each mias\na set of tags.\nIn the last stage, we verify and adjust the annotations\nofM. First, we run the Scoring Algorithm and generate\na set of evaluation measures, both of which are discussed\nin the following two subsections. Then, we examine the\nevaluation and adjust Maccordingly. Once Mreaches a\ndesired level of agreement with R, and hence the reposi-\ntory, the whole process of verifying tag annotations comes\nto completion.\n2.1 Scoring Algorithm\nThe annotation score S(mi)of a music piece mirepre-\nsents the number of rules in Rthatmisatisﬁes. It is cal-\nculated as follows. For each tag annotation rule A!B,\nif the songmicontains both, the antecedent ( A2mi) and\nthe consequent ( B2mi), then we increment the song’s\nscoreS(mi)by 1. However, there could be situations where\nA!BandA!Ccoexist inR, representing the multi-\nlabel nature of tag annotation. If a music piece misses the\nﬁrst rule but satisﬁes the second one, its score still incre-\nments by 1 instead of 0. To achieve this, we iterate through\nRtwice. At ﬁrst, we build a list of rules ( hitlist), which\nare satisﬁed by the song mi, and increment S(mi)accord-\ningly. Then we look for the rules which are not satisﬁed by\nthe song’s annotations, such as A!B, whereA2mibut\nB =2mi. If their antecedents are not found in the hitlist,\nthen we decrement the song’s score S(mi)by 1 for each\nproblematic rule.\nScoring Algorithm: used together with R during Stage 4.\n1. for each mi2Mf\n2. S(mi) = 0 ;\n3. for each rule ri= (A!B)2R\n4. if A2mithen\n5. if B2mithenf\n6. record the rule in hitlist\n7. S(mi)++\n8. g\n9. for each rule ri= (A!B)2R\n10. if A2mithen\n11. if B =2mithen\n11. if A6=any antecedent in the hitlist then\n14. S(mi)--\n15.g\n2.2 Evaluation Measures\nIf a music piece has a positive score, we say that it has\nasound tag annotation (STA). Otherwise, we say that it\nhas a problematic tag annotation (PTA). Furthermore, we\nattempt to distinguish music pieces that have a certain de-\ngree of ambiguity and subjectivity. A music piece has a\ngray tag annotation (GTA) if its annotation score is be-\ntween [l,h], wherelandhare user-speciﬁed range values.\nFor instance, they can be -1 and +1 respectively. In our\nsimulations below we use [\u00002;0]for this range. A mu-\nsic expert, depending on her/his musical expectation and\nexperience, may set a different range.We calculate four measures. The ﬁrst is the Problematic\nTag Annotation Rate (PTAR ) for the annotation process. It\nis the ratio between the number of music pieces with PTA\nand the total number of music pieces. The second is the\nSound Tag Annotation Rate (STAR ), which is the ratio be-\ntween number of STA music pieces and the total number of\nmusic pieces. These two rates represent the quality of the\ntag annotation process. It is obvious that the higher PTAR\nis, the worse the tag annotation quality; while for STAR\nit is the opposite. In addition, we calculate the Gray Tag\nAnnotation Rate (GTAR ), which is the ratio between the\nnumber of GTA music pieces and the total number of mu-\nsic pieces in the dataset. It represents the uncertainty in the\nannotation process. The fourth measure that we calculate\nis the Zero Tag Annotation Rate (ZTAR ), which represents\nthe percentage of music pieces that do not contain any of\nthe tag annotation rules. These measures divide the whole\nmusic set into partitions and add up to 1.\n3. SIMULATIONS\nWe are facing three major challenges when examining the\neffectiveness of our proposed approach to tag annotation\nveriﬁcation.\nIt would be ideal to examine our approach using the\nprocess as depicted by Figure 1. However, such a pro-\ncess involves a great amount of ﬁnancial and labor costs.\nGiven our current situation and circumstances, it would be\nextremely hard, if not impossible, for us to deploy such a\nprocess, although involving a group of experts with exten-\nsive music background and experience could prove invalu-\nable, as they would inevitably provide important feedback\nabout our approach.\nTo make the situation worse, it is often the case that dif-\nferent music repositories and datasets vary in the sets of\ntags used to annotate their music pieces. Furthermore, as-\nsociation analysis captures each dataset’s own associations\nthat do not necessarily translate to others. Therefore, it is\npossible for two datasets to be completely incompatible in\nsuch a way that one could not be used to verify another.\nMoreover, there is a lack of good quality datasets that\ncould be used for benchmarking, although the MIR com-\nmunity is making efforts to come up with such datasets,\nwhich is evidenced by the Million Song Dataset [4, 9] and\nthe Million Song Dataset Benchmarks [14].\nTaking these issues into consideration, we design and\nimplement a series of simulations to demonstrate the ef-\nfectiveness of our proposed approach. Through these sim-\nulations, we aim to achieve three goals: ( G1) demonstrate\nthat our approach is stable, in that it will not behave arbi-\ntrarily when given different music datasets or, put in an-\nother way, given similar music datasets, it should behave\nsimilarly; (G2) assess the four music datasets using our\nevaluation measures and conﬁrm that they maintain differ-\nent relationships among their tags due to the differences\nin their annotation processes; and ( G3) conﬁrm that, when\nthe quality of annotations in a music dataset improves, our\nproposed measures reﬂect this improvement.We run our simulations with different minimum sup-\nport and conﬁdence value pairs; for each minimum support\nvalue ranging from 5 to 95 we use a different minimum\nconﬁdence ranging from 5 to 95.\n3.1 Music Datasets\nFor our simulations, we use four datasets described below\nand presented in Table 1. They have been previously anno-\ntated with tags by music experts and amateurs.\nTheCAL500 [18] dataset, denoted as SCAL500 , is a col-\nlection of 502 Western songs recorded by different artists.\nEach song is manually annotated by at least three human\nannotators using a vocabulary of 174 tags, which is dis-\ntributed across six categories: Mood, Genre, Instrument,\nSong, Usage, and V ocal. All tags are manually generated\nunder controlled experimental conditions. In our work, we\nuse the “hard” annotations found in CAL500, which give a\nbinary value for all tags in every song indicating whether a\ntag applies to a song.\nThe CAL10K [17] dataset, SCAL10K , is comprised of\nmeta-data collected from the Pandora website. It consists\nof 10,886 songs annotated by expert musicologists who\nmaintain a high level of agreement. This dataset contains\n1053 unique tags. Furthermore, it contains a set of 55 tags\nin common with SCAL500 . We use this information to ﬁnd\nthe same subset of tags in the remaining datasets.\nTheMagnatagatune [15],SMAGNA, contains annotations\nof 21,642 music clips with a subset of 188 different tags.\nThe annotations are collected through an on-line game,\nreferred to as “TagATune”, developed to collect tags for\nmusic and sound clips. Each clip, 29 seconds in length,\nis an excerpt of music provided by (magnatune.com) and\n(freesound.org). All of the tags in the collection have been\nveriﬁed, i.e. a tag is associated with a clip only if it is gen-\nerated independently by more than two players. Moreover,\nonly those tags that are associated with more than 50 clips\nare included in the collection. It is important to note that\nMagnatagatune has not been used as widely as CAL500\ndue to its size and skewed tag distribution.\nTheMillion Song Dataset (MSD ) [4], is the largest MIR\ndataset publically available to date. Its purpose is to en-\ncourage research of scalable solutions and to provide a ref-\nerence dataset for benchmarking. Unlike all other datasets,\nMSD is a conglomeration of complimentary datasets. For\nour simulations, we use the portion provided by LastFM\n(www.lastfm.com) excluding the known duplicates. For\nthis reason, we denote it as SLASTFM.\nSince theSCAL500 dataset is too small to produce good\nsample size, we perform our simulation 10 times and ob-\ntain an arithmetic mean for each of our evaluation mea-\nsures, similar to the 10-fold cross validation. Each time we\nchoose the training set at random, and the remainder of the\ndataset becomes the testing set. This is done to preserve\nthe 30/70 split used in our simulations, as described be-\nlow. The other three datasets are large enough and do not\nrequire this kind of repeated random sub-sampling valida-\ntion.Dataset Number of Number of Label Songs with\nname songs labels cardinality at least 2 tags\nSCAL500 502 174 26.04 502\nSCAL10K 10886 1053 11.88 10886\nSMAGNA 25863 188 3.46 18097\nSMAGNA -CLN 25863 188 4.74 18097\nSMAGNA -ADJ 25863 166 5.15 13991\nSLASTFM 449503 522366 15.94 449503\nSLASTFM-CLN 449503 1046 8.76 280922\nSLASTFM-ADJ 449503 287 7.13 315254\nTable 1 : Music datasets and their statistics. Thelabel cardi-\nnality of a dataset is the arithmetic mean of the number of labels\nper music piece in the dataset. Datasets denoted by CLN undergo\nsimple data cleaning, such as the removal of songs with label car-\ndinality less than 1. The datasets denoted by ADJ undergo the\nadjustment process depicted by Stage 4 in Figure 1 and discussed\nin Simulation 3.\n3.2 Simulation 1\nThis simulation demonstrates G1, that our approach is sta-\nble. Here, we randomly split a dataset in half and see if the\nresulting halves, H1andH2, behave similarly in terms of\nour evaluation measures as outlined in Section 2.2. In this\nsimulation, for each half, we go through all of the sages\ndepicted by Figure 1 except the review and adjust steps in\nstage 4. First, we split each half into two subsets at ran-\ndom: we call one ( 30%) the training set , it corresponds to\nD, and the other ( 70%) corresponds to M and we call it the\ntesting set . Then we compare the results from the scoring\nAlgorithm between H1andH2.\n3.3 Simulation 2\nSimilarly to Simulation 1, we randomly divide each music\ndataset into a training set ( 30%), from which we derive\nassociation rules, and a testing set ( 70%), which we score\nagainst the association rules. They respectively correspond\nto D and M in Figure 1. Then we compare our evaluation\nmeasures between different datasets and hope to conﬁrm\nG2that they maintain different relationships among their\ntags.\nIn this simulation we also use one dataset to verify an-\nother. Since CAL10K provides 55 tags that appear in all\nfour datasets, we reduce all datasets to just those tags. Then\nwe use one dataset as the training set D and another as the\ntesting set M and perform all steps outlined in Figure 1\nexcept the adjustment cycle.\n3.4 Simulation 3\nTo fully demonstrate stage 4, we adjust the two datasets,\nSMAGNA andSLASTFM, by amalgamating some equivalent\ntags into one, thus reducing the diversity of tags in the\ndatasets. For example, we convert several tags like fin-\nstrumentsingermale g,fmalesingergandfmalevoi -\ncesginto one tagfmale vocalsg. After this adjustment,\nwe run the scoring Algorithm and hope to observe that the\nnew PTAR is lower while the new STAR is higher. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n5101520253035404550556065707580859095Evaluation Measure Value\nConfidence Increments  PTAR1  GTAR1  ZTAR1  STAR1  PTAR2  GTAR2  ZTAR2  STAR2Figure 2 : Comparison between H1(denoted by subscript\n1) andH2(denoted by subsript 2) for SCAL10KH1using\nour four evaluation measures.\n 0 0.2 0.4 0.6 0.8 1\n9590858075706560555045403530252015105STAR Value\nConfidence Increments  CAL500\n  CAL10K\n  LastFM-CLN\n  Magna-CLN\nFigure 3 : Comparison of STAR between four datasets.\n4. RESULTS AND DISCUSSIONS\nSimulation 1 demonstrates that our association-based tag\nannotation veriﬁction is stable. Figure 2 illustrates the dif-\nferent measure values that we obtain when we apply our\napproach to SCAL10K . Here, the minimum support is set\nto5%while the conﬁdent increases from 5%to95%. We\nobserve that STAR values are very similar at all conﬁdence\nlevels. The same applies to all other measures that we dis-\ncuss in Section 2.2. We ﬁnd similar results in the other\nthree datasets across various minimum support thresholds\nand thus conclude our work toward G1, i.e., our approach\nis stable. Due to space limit, we do not show all of the\nﬁgures in this paper.\nSimulation 2 applies our approach to all four music data-\nsets. We obtain the values of our proposed four measures\nand report the STAR values for all four datasets in Figure 3.\nThe ﬁgure shows that SCAL500 clearly achieves the highest\nSTAR values, when compared to the other three datasets.\nThe same argument is applied to other measures. For in-\nstance, we have observed lower PTAR values on SCAL500\nbut higher ones on SLASTFM-CLN . Due to the space consid-\neration, we do not report all of the ﬁgures here.\nOur simulations clearly show that dataset SCAL500 has a\nbetter tag annotation quality than the other three datasets.\nSome similar observations have also been made in previous\n 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5\n9590858075706560555045403530252015105STAR Value\nConfidence Increments  CAL10K\n  LastFM-CLN\n  Magna-CLNFigure 4 : UsingSCAL500 to verify the other datasets.\n 0 0.1 0.2 0.3 0.4 0.5 0.6\n858075706560555045403530252015105STAR Value\nConfidence Increments  LastFM-CLN\n  LastFM-ADJ\n  Magna-CLN\n  Magna-ADJ\nFigure 5 : Comparison of STAR in two datasets before and\nafter the adjustment step.\nworks, as mentioned in Section 1.1, and further suggested\nby the annotation processes as discussed in Section 3.1.\nTherefore, we used SCAL500 as the representative dataset\nDin Figure 1 to evaluate the other datasets, each consid-\nered asMin the same ﬁgure. As clearly shown in Figure 4,\nexcept for a few lower conﬁdence ranges, SCAL10K outper-\nforms the other two datasets in terms of STAR. The same\napplies to the other measures, which are not shown due to\nspace limitation.\nTowards our goal ( G3), we adjusted the two datasets\nthat were evaluated as the worst in Simulation 2, namely\ntheSLASTFM-CLN and theSMAGNA -CLN. We then applied our\napproach to them again, as outlined in Stage 4 in Figure 1,\nhoping to see some improvements in terms of our evalu-\nation measures. As presented in Figure 5, we clearly see\nthat improvement. After the adjustment step, both datasets\nshow better performance than before in terms of STAR.\nThe same can be said about the other measures as well,\nwhich are not shown for the sake of space.\n5. CONCLUSION\nIn this work, we have presented a novel approach to the\nveriﬁcation of music tag annotation. We believe that there\nexist inherent associations among music tags that can be\nfurther utilized to verify and monitor a tag annotation pro-\ncess. The simulations presented here show the effective-\nness of our approach.There are several directions along which we can extend\nour work. It would be very interesting to explore whether\nwe can use content-based information, such as MFCC or\nZCR [5], in our analysis and veriﬁcation process. We con-\njecture that this additional information will help improve\nour approach. Furthermore, we also plan to examine the\nindividual rules that were generated for each music piece.\nIn addition, we could calculate the tag annotation rate\nfor a speciﬁc category, such as style, mood and instrument.\nFurthermore, we could consider the representative music\npieces of a single tag. For example, we could examine\nthe tag annotation rules of the music genre pop. These\nrules may provide more insight into the nature of this genre\nand why a music piece is associated with it as opposed to\nothers.\n6. REFERENCES\n[1] Rakesh Agrawal, Tomasz Imieli ´nski, and Arun Swami.\nMining association rules between sets of items in large\ndatabases. In Proceedings of the ACM SIGMOD In-\nternational Conference on Management of Data , vol-\nume 22, pages 207–216. ACM, 1993.\n[2] Rakeshl Agrawal and Ramakrishnan Srikant. Fast\nalgorithms for mining association rules in large\ndatabases. In Proceedings of the 20th International\nConference on Very Large Data Bases , volume 1215,\npages 487–499. Morgan Kaufmann Publishers Inc.,\n1994.\n[3] Thierry Bertin-Mahieux, Douglas Eck, Francois Mail-\nlet, and Paul Lamere. Autotagger: A model for predict-\ning social tags from acoustic features on large music\ndatabases. Journal of New Music Research , 37(2):115–\n135, 2008.\n[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInProceedings of the 12th International Society for\nMusic Information Retrieval Conference , pages 591–\n596. University of Miami, 2011.\n[5] Antti Eronen. Signal Processing Methods for Audio\nClassiﬁcation and Music Content Analysis . PhD thesis,\nTampere University of Technology, Tampere, Finland,\nJune 2009.\n[6] Joe Futrelle and Stephen J. Downie. Interdisciplinary\ncommunities and research issues in music information\nretrieval. In Proceedings of the 3rd International Soci-\nety for Music Information Retrieval Conference , pages\n121–131, 2002.\n[7] Jiawei Han and Micheline Kamber. Data Mining: Con-\ncepts and Techniques . Morgan Kaufmann Publishers\nInc., the second edition, 2006.\n[8] Matthew D Hoffman, David M Blei, and Perry R Cook.\nEasy as CBA: A simple probabilistic model for tagging\nmusic. In Proceedings of the 10th International Societyfor Music Information Retrieval Conference , volume 9,\npages 369–374, 2009.\n[9] Brian McFee, Thierry Bertin-Mahieux, Daniel PW El-\nlis, and Gert RG Lanckriet. The million song dataset\nchallenge. In Proceedings of the 21st International\nConference Companion on World Wide Web , pages\n909–916, 2012.\n[10] Steven R Ness, Anthony Theocharis, George Tzane-\ntakis, and Luis Gustavo Martins. Improving automatic\nmusic tag annotation using stacked generalization of\nprobabilistic svm outputs. In Proceedings of the 17th\nACM International Conference on Multimedia , pages\n705–708. ACM, 2009.\n[11] Kerstin Neubarth, Izaro Goienetxea, Colin Johnson,\nand Darrell Conklin. Association mining of folk mu-\nsic genres and toponyms. In ISMIR , pages 7–12, 2012.\n[12] Franois Pachet. Content management for electronic\nmusic distribution. Communications of the ACM ,\n46(4):71–75, 2003.\n[13] Chris Sanden and John Z. Zhang. An empirical study\nof multi-label classiﬁers for music tag annotation. In\nProceedings of the 12th International Society for Mu-\nsic Information Retrieval Conference , pages 717–722,\n2011.\n[14] Alexander Schindler, Rudolf Mayer, and Andreas\nRauber. Facilitating comprehensive benchmarking ex-\nperiments on the million song dataset. In Proceedings\nof the 13th International Society for Music Information\nRetrieval Conference , pages 469–474, 2012.\n[15] Klaus Seyerlehner, Gerhard Widmer, Markus Schedl,\nand Peter Knees. Automatic music tag classiﬁcation\nbased on block-level features. In Proceedings of the\nSound and Music Computing Conference , pages 126–\n133, 2010.\n[16] Jialie Shen, Wang Meng, Shuichang Yan, HweeHwa\nPang, and Xiansheng Hua. Effective music tagging\nthrough advanced statistical modeling. In Proceedings\nof the 33rd international ACM SIGIR conference on\nResearch and development in information retrieval ,\npages 635–642. ACM, 2010.\n[17] Youngmoo E. Kim Tingle, Derek and Douglas Turn-\nbull. Exploring automatic music annotation with\nacoustically-objective tags. In Proceedings of the in-\nternational conference on Multimedia Information Re-\ntrieval , pages 55–62. ACM, 2010.\n[18] Douglas Turnbull, Luke Barrington, David Torres, and\nGert Lanckriet. Semantic annotation and retrieval of\nmusic and sound effects. IEEE Transactions on Au-\ndio, Speech and Language Processing , 16(2):467–476,\nFebruary 2008.\n[19] Kris West. Novel Techniques for Audio Music Classiﬁ-\ncation and Search . PhD thesis, University of East An-\nglia, UK, September 2008."
    },
    {
        "title": "Semi-Supervised Polyphonic Source Identification using PLCA Based Graph Clustering.",
        "author": [
            "Vipul Arora 0001",
            "Laxmidhar Behera"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418293",
        "url": "https://doi.org/10.5281/zenodo.1418293",
        "ee": "https://zenodo.org/records/1418293/files/AroraB13.pdf",
        "abstract": "For identifying instruments or singers in the polyphonic audio, supervised probabilistic latent component analysis (PLCA) is a popular tool. But in many cases individual source audio is not available for training. To address this problem, this paper proposes a novel scheme using semisupervised PLCA with probabilistic graph clustering, which does not require individual sources for training. The PLCA is based on source-filter approach which models the spectral envelope as a weighted sum of elementary band-pass filters. The novel graph based approach, embedded in the PLCA framework, takes into account various perceptual cues for characterizing a source. These cues include temporal cues like the evolution of F0 contours as well as the acoustic cues like mel-frequency cepstral coefficients. The proposed scheme shows better results in identifying vocal sources than a state of the art unsupervised scheme. In addition, the proposed framework can be used to incorporate perceptual cues so as to enhance the performance of supervised schemes too.",
        "zenodo_id": 1418293,
        "dblp_key": "conf/ismir/AroraB13",
        "keywords": [
            "supervised probabilistic latent component analysis (PLCA)",
            "polyphonic audio",
            "individual source audio",
            "semisupervised PLCA",
            "probabilistic graph clustering",
            "source-filter approach",
            "spectral envelope",
            "elementary band-pass filters",
            "perceptual cues",
            "vocal sources"
        ],
        "content": "SEMI-SUPERVISED POLYPHONICSOURCE IDENTIFICATIONUSING\nPLCABASED GRAPH CLUSTERING\nVipul Arora, Laxmidhar Behera\nDepartmentofElectricalEngineering,Indian Instituteof Technology,Kanpur\nvipular@iitk.ac.in, lbehera@iitk.ac.in\nABSTRACT\nFor identifying instruments or singers in the polyphonic\naudio, supervised probabilistic latent component analysi s\n(PLCA) is a popular tool. But in many cases individual\nsource audio is not available for training. To address this\nproblem, this paper proposes a novel scheme using semi-\nsupervisedPLCAwithprobabilisticgraphclustering,whic h\ndoesnotrequireindividualsourcesfortraining. ThePLCA\nis based on source-ﬁlter approach which models the spec-\ntral envelope as a weighted sum of elementary band-pass\nﬁlters. The novel graph based approach, embedded in the\nPLCA framework, takes into account various perceptual\ncues for characterizing a source. These cues include tem-\nporal cues like the evolution of F0 contoursas well as the\nacousticcueslikemel-frequencycepstralcoefﬁcients. Th e\nproposed scheme shows better results in identifying vocal\nsourcesthana state ofthe artunsupervisedscheme. Inad-\ndition,theproposedframeworkcanbe usedtoincorporate\nperceptualcuessoastoenhancetheperformanceofsuper-\nvisedschemestoo.\n1. INTRODUCTION\nWe humans can selectively focus our attention on listen-\ning to a particularsoundsource evenin the midst of many\ninterfering sounds. This ability makes it possible for us\nto listen to the polyphonic music where we can intently\nhear a particular instrument (or singer) as if it were play-\ningalone. Inordertounderstandthenatureofhumancog-\nnition it is important to ﬁnd out what features human ears\nidentify in order to be familiar with and cognize a sound.\nIn music, several perceptual attributes have been found to\ncorrespondtoparticularmathematicalpropertiesoftheau -\ndio signal. For example, pitch approximatelycorresponds\nto the fundamental frequency (F0), loudness roughly cor-\nresponds to the amplitude/intensity etc. However, even if\nthe pitch and loudness may vary during the song, we tend\nto recognize and cluster apart a particular musical source\nfromthemixtureofsounds,evenifwehavenotlistenedto\nthatsourcebefore.[9]discussesvariousacousticattribu tes\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2013 International Society for MusicInformation Retrieva l.whichhelpinperceptualgroupingofvariousacousticstim-\nuli. Thisgroupingisconceptualizedastakingplaceat two\nlevels. At the ﬁrst level, the acoustic stimuli are grouped\ninto group objects or the perceptual units, and at the sec-\nondlevel,thesegroupobjectsarelinkedtodifferentsourc e\nstreams. Our present work mostly focusses on the second\nlevel of grouping, which relies upon the features which\ncharacterize a source. The ﬁrst level of grouping in one\ntime frame is characterised by the given F0 value and its\nharmonics.\nThere have been several works on ﬁnding the multiple\npitches or fundamentalfrequencies(F0’s) in a polyphonic\naudio, so much so that it is a popular task in the MIREX\nchallenge1. This paper focuses on identifyingthe instru-\nments associated with producing those F0’s. One popular\nparadigmtoquantifythequalityofsoundisthespectralen-\nvelope. There are several works in the literature that learn\nspectral envelopesfor instrumentidentiﬁcationin a super -\nvisedfashion.[7]usessinusoidalmodelingtorepresentth e\naudio signal and groups the peaks over a few consecutive\nframes using heuristic auditory cues and spectral cluster-\ning. These clusters are then assigned to a source with the\nhelp of timbre models trained beforehand. [6] models the\nharmonic partials and temporal evolution using Gaussian\nmixture model (GMM). Instrument identiﬁcation is per-\nformedusingpre-trainedsupportvectormachineclassiﬁer\noverstatistical featuresderivedfromthe spectral and tem -\nporal parameters. A popular tool to analyze polyphonic\nsoundisthenon-negativematrixfactorizarion(NMF).[12]\nusesasource-ﬁltermodelbasedNMFforsoundseparation\nand subsequently, the instrument identiﬁcation is carried\noutusingpre-trainedGMM’soverMel-Frequencycepstral\ncoefﬁcients.\nThese supervised methods are dependent on the avail-\nability of isolated sounds from each contributing source.\nBut manytimes, the isolated sourceaudiois not available,\nfor example for commercial CD’s. Hence, we have to re-\nsort to unsupervised techniques for source identiﬁcation.\nThispossibilityisassertedbythehumanabilitytocognize\nandclusteranysingerfromthepolyphonicsongevenifthe\nsongorthesingerisheardforthe ﬁrst time.\nSeveral works aiming in this direction cluster the var-\nious sources in an unsupervised way by matching their\ntimbral properties. [8] applies NMF to extract dictionary\ncomponents, which are then grouped into sources by un-\nsupervised clustering over the Mel-frequency cepstral co-\n1http://www.music-ir.org/mirex/wiki/2012efﬁcients extracted from the dictionary components. [11]\nimproves upon this method by using shifted NMF for the\nunsupervised clustering of the dictionary components. [5]\nuses agglomerative approach to cluster the matrices, ob-\ntainedby theNMF decompositionofthe inputsignal,into\ndifferentsources. Theseworkshavetheirfoundationsroot ed\nin the hypothesis that the sounds having similar spectral\nenvelopsoriginatefromthesamesource.\nThe present work is an exploration of this hypothesis\nfor identifying the sources present in a polyphonic audio\nin a semi-supervised way. A polyphonic audio, with sev-\neral musical sources playing together, is given. Since this\nworkfocusesonidentifyingtheunderlyinginstrument,we\nassume the active fundamental frequencies to be given a-\npriori, as they can be extracted using a multi-F0 extrac-\ntor [3]. Our goal is to identify the instrument associated\nwith each F0. The system is semi-supervised in the sense\nthat a human user annotates a few segments of pitch con-\ntours with the corresponding source labels and feeds this\ninformation for initializing the system. The proposed al-\ngorithm then labels the entire audio with the correspond-\ning instruments. A source-ﬁlter based Probabilistic La-\ntent Component Analysis (PLCA) framework is used to\ndecompose the given polyphonic audio spectrum into an\nappropriate representation in terms of various elementary\nspectraembeddedinaBayesiannetworkofvariouslateral\nvariables. The clustering is performed using probabilisti c\ngraph clustering framework. All the samples in the given\nF0 contours act as nodes of the graph and the edges con-\nnecting them are modelled by a similarity relation. The\nsimilarity between nodes is derived from various acoustic\ncues like pitch continuity, spectral characteristics and s i-\nmultaneityconstraints.\nThemajoradvantageofourapproachis that it doesnot\nneedindividualsourceaudiosfortrainingbutrequiresonl y\na few annotations per instrument. Hence this approach\ncan be used even for unseen instruments and commercial\nrecordings. Conceptually,thenoveltyofthisworkisthati t\napplies graph clustering to the NMF framework. Graph\nbased method helps to model the metrics (or distances)\nbetween the sound objects in a non-Euclidean way and\nhencegivesalotofﬂexibilitytomodeltheauditoryspace.\nThe probabilistic graph based clustering can incorporate\nvarious perceptual cues which may help in grouping the\nsources,therebyenhancingthe performance. Inthis work,\nwe proposenovelwaystomodelthese constraints.\nThepolyphonicsignalrepresentationusingPLCAframe-\nworkisexplainedinSection2. Thegraphbasedmodeling\nof instrument similarity is proposed in Section 3 followed\nby the probabilistic graph clustering method in Section 4.\nSection 5 givesthe complete overviewof the proposedal-\ngorithmandtheexperimentalevaluationofthesame.\n2. SIGNALREPRESENTATION\nThe single channelpolyphonicaudiois consideredto be a\nlinear additive mixture of various source waveforms. The\naudio waveform is transformed to frequency domain by\ntakingits2048-pointshort-timeFouriertransformwithha n-ning window of length 56ms and hop size of 10ms. The\nmagnitude spectrum is boosted by 6dB per octave. PLCA\n[10] assumes the magnitude spectrogram V(f, t)of the\npolyphonicsignaltobeahistogramof energyquanta piled\nup in the time-frequencybins, indexed by tandfrespec-\ntively,andgeneratedbyanunderlyingprobabilitydistrib u-\ntion function(pdf) Pt(f). It furtherassumesthat the mag-\nnitude spectrum of polyphonic audio is the linear sum of\nthe magnitude spectra of individual source signals. Using\nthe source-ﬁlter model [12], Pt(f)can be factorized into\nthefollowinglatentvariablerepresentation:\nPt(f) =/summationdisplay\np,s,z,aPt(f|p, a)Pt(p)Pt(s|p)Pt(z|p, s)P(a|s, z)\n(1)\nHere, p, s, z, aare latent variables underlying the genera-\ntion of energy quanta at each time-frequency bin, which\ncan take values from 1toNp, Ns, Nz, Na, respectively.\npindexes the pitch, sindexes the source instrument, z\nindexes the dictionary component and aindexes the fre-\nquencybandassociatedwitheacht-fbin. Pt(f|p, a)isthe\nﬁxed spectrumconsisting of the Gaussian harmonicpeaks\nplaced at the integer multiples of the given F0 associated\nwiththepitchindex pattime tandﬁlteredthroughthe ath\ntriangular mel-frequency band pass ﬁlter. The other pa-\nrameterscanbelearntusingtheExpectationMaximization\n(EM)algorithmwhichminimizestheKullback-Leiblerdi-\nvergence between the observed spectrum V(f, t)and the\nunderlyingdistributionmodel Pt(f),asexplainedbelow.\nE-step:\nPt(p, s, z, a|f) =Pt(f|p, a)Pt(p)Pt(s|p)Pt(z|p, s)P(a|s, z)\nPt(f)\n(2)\nM-step:\nPt(p) =/summationtext\nf,s,z,aV(f, t)Pt(p, s, z, a|f)/summationtext\np/summationtext\nf,s,z,aV(f, t)Pt(p, s, z, a|f)(3)\nPt(s|p) =/summationtext\nf,z,aV(f, t)Pt(p, s, z, a|f)/summationtext\ns/summationtext\nf,z,aV(f, t)Pt(p, s, z, a|f)(4)\nPt(z|p, s) =/summationtext\nf,aV(f, t)Pt(p, s, z, a|f)/summationtext\nz/summationtext\nf,aV(f, t)Pt(p, s, z, a|f)(5)\nP(a|s, z) =/summationtext\nf,t,pV(f, t)Pt(p, s, z, a|f)/summationtext\na/summationtext\nf,t,pV(f, t)Pt(p, s, z, a|f).(6)\nAlso, we assume the total numberofinstruments Nsto\nbe known apriori. Our goal is to cluster all the (t, p)units\nbased on the underlying sources s. This problem can be\nseen asthat ofestimating Pt(s|p).\n3. INSTRUMENT SIMILARITY MODELING\nAftersuitablymodelingthespectra,thenexttaskistoclus -\nter them based upon their originating sources. In order to\ncluster the sources, we need to know what features quan-\ntify the distinction among the spectra of different sources\nandthesimilaritybetweenthoseofthe samesource.All the objects to be clustered, i.e. the (t, p)units, are\nmodeled as the nodes of a graph. The edges of the graph\naremodeledbythelink-weightsmatrix A,suchthat Aki;lj\nrepresentsthe closenessbetweenthe pair of nodes (tk, pi)\nand(tl, pj). Here, tkimplies that the time index is kand\npiimplies that the pitch index is i. The link-weights are\nconstructedto lie within the range [0,1]. A largervalueof\nthelink-weightdenotesmoreclosenessbetweenthepairof\nnodes, i.e. it is morelikely that both the nodescorrespond\nto thesamecluster.\nTo model the closeness between the nodes, we con-\nsider three factors which quantify the similarity between\nthe spectra from same sources, namely, the spectral enve-\nlope, the temporalcontinuityofthe pitch contoursand the\nsimultaneity constraint. Each one of these is explained as\nfollows.\n3.1 SpectralEnvelope\nThisstepusesMel-frequencycepstralcoefﬁcients(MFCC’s )\nin order to measure spectral-timbral closeness of sounds.\nAlthough the source-ﬁlter PLCA is also based on spectral\nenvelope,butMFCCfeaturesprovidedifferentadvantages,\nlike de-correlatingthe ﬁlter bankweights. MFCC features\nhave been very successful in the areas of speech research\nlike speaker characterization [4]. The use of MFCC fea-\nturesforsourcecharacterizationisbasedonthehypothesi s\nthat the spectra from the same source have similar spec-\ntral envelopes. The MFCC’s quantify the position of the\nnode(t, p)in the space of spectral envelopes. They are\ncomputedfromthePLCA representationasfollows.\nTheathband-ﬁlteredspectrumassociatedwithpitchin-\ndexpat time tisestimatedusingthefollowingWeinerﬁl-\nter,\nVp,a(f, t) =Pt(f, p, a)\nPt(f)V(f, t) (7)\nwhere, Pt(f)isgivenbyEquation(1)and\nPt(f, p, a) =/summationdisplay\ns,zPt(f|p, a)Pt(p)Pt(s|p)Pt(z|p, s)P(a|s, z)\nThe energy values in the Nasub-band spectra associated\nwith a (t, p)unit are used to compute the 13 dimensional\nMFCC vectors mt,p. The closeness between two nodes,\nbased on these features, is modeled using the cosine simi-\nlaritymeasurewhichisdeﬁnedas\nDcos(m1,m2) =m1·m2\n|m1||m2|(8)\nwhere,·represents the vector dot product and |m|stands\nfor the Euclidean norm of vector m. The link-weightma-\ntrixisestimatedas\nAS\nki;lj=Dcos(mtk,pi,mtl,pj) + 1\n2(9)\nwith the superscript Sdenoting that it has been derived\nfrom the spectral features. The aforementioned equation\nensuresthat although Dcos∈[−1,1],ASis constrainedto\nlie inthe interval [0,1].(k,i’) (k−1,j’)\n(k,i)(k−1,j)\nk tf\nk−1\nFigure 1. TemporalContinuity: dashed-dottedline shows\ntwo pitchcontours.\n3.2 TemporalContinuity\nThe pitch contour produced by a musical source changes\nslowlyduetophysicalaswellasmusicologicalconstraints .\nWe can use this information as a cue [13] for clustering\nup the (t, p)units by forming the link-weight matrix AT,\nwhere the superscript Tdenotes the temporal connected-\nnessmeasure. TheclosenessbetweenF0sismodeledas\ndlj\nki= exp{−(F0ki−F0lj)2/σ2}.(10)\nForAT\nki;ljtobelarge,weneedtosatisfythefollowingcon-\nstraints:\n1.|tk−tl|= 1\n2. Ifdlj\nkiis large, dlj′\nkishould be small, for j′/ne}ationslash=j, in\nordertoavoidtheambiguitywhentheF0satboththe\nnodes are close to each other. For the same reason,\ndki′\nkishould also be small, for i′/ne}ationslash=i(see Figure 1\nwithl=k−1).\n3. If(t, p)is well connectedto (t′, p′)which is further\nwellconnectedto (t′′, p′′),then(t, p)shouldalsobe\nwell connected to (t′′, p′′). We call this as agglom-\neration.\nConstraint 2 ensures that the temporal connectedness is\nstrong only when the F0 trajectory under consideration is\nfarapartinfrequencyfromtheotherF0trajectories.\nIn accordancewith the abovefactorswe devise the fol-\nlowingnovelschemetoconstruct AT.\nfork= 2toNtdo\nAT\nki;(k−1)j=d(k−1)j\nki/bracketleftBigg\nd(k−1)j\nki/summationtext\njd(k−1)j\nki+/summationtext\nj/negationslash=idkj\nki/bracketrightBigg\n(11)\n{Agglomeration:}\nifAT\nki;(k−1)j> ǫthen\nAT\nki;k′i′←AT\n(k−1)j;k′i′, k′={1, ...,(k−2)},∀i′\nend if\n{Making ATsymmetric:}\nAT\nk′i′;ki←AT\nki;k′i′,∀k′,∀i′\nend for3.3 SimultaneityConstraint\nWe assume that an instrument can play only a single note\natatime. Thisassumptionisundoubtedlyvalidforthevo-\ncal sources. For instruments, this assumption is not valid\nfor harmony-basedmusic systems where same instrument\nplays many notes simultaneously to form chords, but is\nvalidformelody-basedmusicsystemswhereaninstrument\nplays only one note at a time. This constraint implies that\nNs=Np.\nTheﬁnallink-weightmatrix Aisformedbycombining\nASandATas,\nA= (1−α)AS+αAT(12)\nwhere, α∈[0,1]. The simultaneity constraint is imposed\nas,\nAki;kj=/braceleftbigg1,ifi=j\n0,otherwise(13)\nAnotherimplicationofthesimultaneityconstraintingrap h\nclusteringismentionedin thenextsection,viz.,Section4 .\n4. GRAPHCLUSTERING\nForclustering,severaltechniquesareusedlikeagglomera -\ntive clustering, probabilistic clustering, spectral clus tering\netc. In this work, we use the probabilistic framework for\ngraph clustering as proposed by [1]. The clustering prob-\nlemisseenasanoptimizationproblemwhichaimsatmax-\nimizingthelog-likelihoodfunction,\nL(Ω, A) =/summationdisplay\ns/summationdisplay\nk,l,i,jlnP(ωs\nki, ωs\nlj|Aki;lj)(14)\nwhere, ωs\nki∈Ωis the cluster membershipfunctionwhich\nmeasures the degree of afﬁnity of the unit (tk, pi)to the\nsth source cluster. We can see that ωs\nkicorresponds to\nPtk(s|pi).P(ωs\nki, ωs\nlj|Aki;lj)is modeled as a Bernoulli\ndistribution,\nP(ωs\nki, ωs\nlj|Aki;lj) = (Aki;lj)ωs\nkiωs\nlj(1−Aki;lj)1−ωs\nkiωs\nlj\n(15)\nThisequationensuresthatifthelinkweight Aki;ljislarge,\nthen there is a large probabilityof ωs\nkiandωs\nljto be close\nto unity, meaning that the two nodesget clustered into the\nsame source s. On putting this in Equation(14) and max-\nimisingwith respectto ωs\nki,we get,\n∂L(Ω, A)\n∂ωs\nki=/summationdisplay\nljωs\nljlnAki;lj\n1−Aki;lj(16)\nThe updated cluster membership function is estimated\nusingsoft-assignansatz as\nˆωs\nki=exp[∂L/∂ωs\nki]/summationtext\nsexp[∂L/∂ωs\nki](17)\nThisequationalsotakescareofthefactthat ωs\nkirepresents\nPtk(s|pi)and hence ought to be normalized with respect\ntos.1 2 3pi00.51ωs\nki(a) Before\n1 2 3pi00.51\ns=1\ns=2\ns=3(b) After\nFigure 2.ωs\nkivspifor different values of sand a ﬁxed\ntk. (a) All piare strongly linked to the cluster s= 3; (b)\nEquation(18)imposesthesimultaneityconstraint.\nUnderthisformulation,itisquitepossiblethatboth ωs\nki\naswellas ωs\nkj,forsome j/ne}ationslash=i,havelargevalues. However,\nthe simultaneity constraint prevents this from happening.\nThisisachievedbyconsideringthepoint-clusterrelation in\na two-waysense, i.e. howsimultaneous (t, p)unitslink to\noneclusteraswellashowmanyclustersrelatetoonesuch\nunit. Thesimultaneityconstraintis imposedina non-rigid\nway by simply updating the cluster membership function\nas,\nωs\nki←ωs\nki/radicalBig/summationtext\njωs\nkj, (18)\nfollowedbynormalizationwithrespectto s,soastomain-\ntain its probabilistic interpretation as Pt(s|p). To under-\nstand this equation, let us consider the binary clustering\ncase with Ns= 2. Fora certain k, ifωs1\nkj> ωs2\nkj,∀j, it im-\npliesthat boththe simultaneousunits, (tk, pj), j={1,2},\nare strongly associated with the source s1. This situation\nimplies that/summationtext\njωs1\nkj>1and/summationtext\njωs2\nkj<1. The simul-\ntaneity constraint howeverimposesthat onlyone pshould\nbe associatedwith one s. Hence,we attenuate ωs1\nkj,∀jand\namplify ωs2\nkj,∀jby simply dividing with/radicalBig/summationtext\njωs\nkj. This\neffect is also achieved for larger values of Ns, as graphi-\ncallydepictedin theFigure2.\n5. EXPERIMENTS\n5.1 ImplementationofCompleteAlgorithm\nGiven a polyphonic song along with all the active pitch\nvalues indexed by pat time frame index t, our task is to\ncluster the (t, p)unitsinto the underlyingsources. Instead\nof completely unsupervised clustering, we make the task\nsemi-supervisedbyrandomlychoosingafew(3here)time\nindicesandlabelingthe (t, p)unitsatthosetimeswiththeir\nrespective sources. This is the only little annotation work\nwhich hasto be manuallyperformedin case of absenceof\nanyothertrainingdata.\nThe source dictionaries P(a|s, z)are randomly initial-\nized and pre-trained using this little annotated data from0 2 4 6 8\nt (s)50100150200250300F0 (Hz)Source 1\nSource 2\n(a) Ground Truth0 2 4 6 8\nt (s)50100150200250300F0 (Hz)\n(b) Proposed Algorithm0 2 4 6 8\nt (s)50100150200250300F0 (Hz)\n(c) Algorithm [8]\nFigure3. ExampleofSourceClusteringforamixtureofKen-Gensinge rpair. (a)Groundtruth(b)theproposedalgorithm\nand(c)thealgorithmof[8]\nthe same song. These source dictionaries are then used as\nseedsforthe completealgorithmasexplainedbelow.\n1. PLCAdecompositionoftheentiresongisperformed,\nusing the EM algorithm Equations (2)-(6), iterated\n10times.\n2. The spectral and temporal link-weight matrices are\ncomputedasexplainedin Section3.\n3. The probabilistic source labels ωs\nkiare initialized as\nPtk(s|pi)and are updated using graph clustering as\nexplainedinSection4. Then, Pt(s|p)isupdatedas\nPtk(s|pi)←Ptk(s|pi) +ωs\nki\n2\nbecause generally ωs\nkiis found to have extreme val-\nues(0or1)duetoitsexponentialupdaterule.\n4. Thesesteps1to3arerepeatedforaﬁxednumberof\ntimes(3here).\nFinally, the source index is estimated using maximum\naposterioriestimatoralongwiththesimultaneityconstra int\nas\nˆst= argmax\nst{/summationdisplay\npPt(st(p)|p)}.(19)\nHere,stisthevectorformedbypermutationsofthevector\n[1, ..., N s], so that the ﬁnal instrument labels are ˆst,p=\nˆst(p).\nNotably, the Pt(s|p)for the given (t, p)units are ﬁxed\nto the given values throughoutthe algorithm. The various\nparameters were chosen heuristically. Number of dictio-\nnary componentsfor each source is set as Nz= 2and the\nnumberofmel-frequencybandﬁlters Na= 20. Insubsec-\ntion3.2,we set σ= 20andǫ= 0.6.\n5.2 Evaluation\nThe clustering of vocal sources is more challenging than\nthat of mostof the musicalinstruments. Thereasonis thatfor a certain F0, the spectrum of the latter remains mostly\nthe same but that of the former varies signiﬁcantly due to\nthevarietyofvoicedsounds(orvowels)whichcanbepro-\nducedbya humansinger.\nThe proposedalgorithmwas evaluatedwith the help of\nvocal songs from the MIR-1k database [2], which is pub-\nlicly available. The evaluation dataset consisted of audio\nrecordingsof 4 different singers - 2 males (Abj, Gen) and\n2 females (Ken, Hey). Total 60 single-channelaudio mix-\ntures of polyphonyorder Ns= 2were formedby linearly\nadding the monophonicaudio waveforms, with a total du-\nrationofabout350seconds.\nThe proposed approach was compared with that pro-\nposed by Spiertz and Gnann [8], whose implementation\nis available as open source. Their method is oriented to\nproduce separated signals instead of instrument clusters.\nToestimatetheirperformanceoninstrumentidentiﬁcation ,\nwe estimated P(s|p, t)bycomparingthespectrausingthe\ncosinesimilaritymeasure,deﬁnedin Equation(8).\nˆPSG(s|p, t) =Dcos(Yp,t,Ss,t) (20)\nHere,Yp,tandSs,tare the magnitude spectral vectors of\nthepthoutputchannelandthe sthmonophonicsourceused\nto constructtheinput,respectively,at timeframe t.\nFor evaluation metrics, all the (t, p)units are clustered\nintoNsnumberof groups. Eachobtainedcluster is linked\nto a ground truth source in a way which maximizes the\nclassiﬁcationaccuracy. Theclassiﬁcationaccuracyismea -\nsured as the fraction of (t, p)units correctly labeled with\nthegroundtruthsource.\nThe evaluation scores are presented in Table 1. We\ncan see that the proposed semi-supervised algorithm per-\nforms better than the unsupervised algorithm. Although\nthis comparison is not well balanced, but still it gives an\nideaoftheperformance. Also,wecanseethatforouralgo-\nrithm,themale-femalevoicedifferentiabilityislargert han\ntheothertwocases, viz.,male-maleandfemale-female.\nThis algorithm works quite well with short audio ex-\ncerpts, as we tested it for upto 10s long excerpts, i.e. the\nsize of Awas upto 2000×2000. But for long songs,ClassiﬁcationAccuracyin%\nSingerspair Proposedalgorithm Algorithm[8]\nAbj-Hey 79.5(16.5) 62.7(15.0)\nKen-Hey 72.9(13.7) 68.3(13.5)\nGen-Hey 86.2(13.0) 65.4(11.2)\nAbj-Ken 79.5(16.8) 74.3(9.9)\nAbj-Gen 68.1(13.5) 66.0(10.0)\nKen-Gen 87.2(9.9) 71.4(14.8)\nTable 1. Evaluation Results with standard deviations in\nbrackets\nthe size of the link-weight matrix becomes very large and\nhencethecomputationsbecomeextensive. Thus,it iswise\nto breakthelongsongintoshortersegments( <10sin du-\nration). In such a case, one may pre-train the dictionaries\nfor each audio segment using the few labeled (t, p)units,\neven if they fall in different audio segments, and perform\ntheproposedalgorithmoveronesegmentat atime.\n6. CONCLUSION\nThispaperproposedanovelalgorithmforinstrumentiden-\ntiﬁcationinpolyphonicmusic. Themajoradvantageofthe\nalgorithm is that it is semi-supervised as it does not re-\nquireanytrainingdataintheformofindividualsourceau-\ndios. It can initialize from annotations only at a few time\ninstants. The method uses source-ﬁlter based PLCA for\ndecomposingthe magnitudespectra of the polyphonicau-\ndio. This PLCA based representation is used to form a\ngraph which is clustered into the underlying sources with\nthe help of various perceptual cues, including the spectral\nand temporal cues. Novel ways of modeling these cues\nwere proposed. The temporal constraints take care of not\nonly the continuity of the pitch contour individually but\nalso its interaction with other pitch contours. To model\nthe simultaneity constraints, a new equation is introduced\nwhichtakes careof thepoint-clusterrelationin a two-way\nsense, so that the simultaneously occuring points are as-\nsignedtodifferentclusters.\nThe experimental results show that the proposed algo-\nrithm performs better than a state of the art unsupervised\nsource separationalgorithmadaptedfor instrumentidenti -\nﬁcation. The proposed framework can also be used with\nsupervised schemes so as to enhance the performance by\nincorporatingvariousacousticcues.\n7. REFERENCES\n[1] A. Robles-Kelly, and E. R. Hancock: “A Probabilistic\nSpectral Framework for Grouping and Segmentation,”\nInPatternRecognition ,Vol.37,No.7,pp.1387–1405,\n2004.\n[2] C.-L. Hsu, and J.-S. R. Jang: “On the Improvement\nof Singing Voice Separation for Monaural Recordings\nUsingtheMIR-1KDataset,” IEEETransactionsonAu-dio, Speech and Language Processing , Vol. 18, No. 2,\npp.310–319,2010.\n[3] C.Yeh,A.Roebel,andX.Rodet: “Multiplefundamen-\ntal frequency estimation and polyphony inference of\npolyphonic music signals,” IEEE Transactions on Au-\ndio, Speech and Language Processing , Vol. 18, No. 6,\npp.1116-1126,2010.\n[4] D. A. Reynolds: “An overview of automatic speaker\nrecognition technology,” Proceedings of International\nConference on Acoustics, Speech and Signal Process-\ning(ICASSP) ,Vol.4,pp.4072–4075,2002.\n[5] J. M. Becker, M. Spiertz, and V. Gnann: “A\nprobability-based combination method for unsuper-\nvised clustering with application to blind source sep-\naration,”Latent Variable Analysis and Signal Separa-\ntion(LVA/ICA) ,pp.99-106,2012.\n[6] J. Wu, E. Vincent, S. A. Raczynski, T. Nishimoto, N.\nOno, and S. Sagayama: “Polyphonic pitch estimation\nandinstrumentidentiﬁcationbyjoint modelingof sus-\ntained and attack sounds,” IEEE Journal of Selected\nTopics in Signal Processing , Vol. 5, No. 6, pp. 1124–\n1132,2011.\n[7] L. G. Martins, J. J. Burred, G. Tzanetakis, and M.\nLagrange: “PolyphonicInstrument RecognitionUsing\nSpectral Clustering,” Proceedings of International So-\nciety for Music Information Retrieval Conference (IS-\nMIR),2007.\n[8] M.Spiertz,andV.Gnann: “Source-ﬁlterbasedcluster-\ningformonauralblindsourceseparation,” Proceedings\nof International Conference on Digital Audio Effects ,\n2009.\n[9] M. Weintraub: “A theory and computational model of\nmonaural auditory sound separation,” Ph. D. disserta-\ntion,StanfordUniversity,1985.\n[10] P. Smaragdis, B. Raj, and M. Shashanka: “A proba-\nbilistic latent variable model for acoustic modeling,”\nAdvances in models for acoustic processing, NIPS ,\n2006.\n[11] R. Jaiswal, D. FitzGerald, D. Barry, E. Coyle, and\nS. Rickard: “Clustering NMF basis functions using\nshifted NMF for monaural sound source separation,”\nProceedingsofInternationalConferenceonAcoustics,\nSpeechandSignalProcessing(ICASSP) ,pp.245–248,\n2011.\n[12] T. Heittola, A. Klapuri, and T. Virtanen: “Musical in-\nstrumentrecognitioninpolyphonicaudiousingsource-\nﬁlter model for sound separation,” Proceedings of In-\nternational Society for Music Information Retrieval\nConference(ISMIR) ,2009.\n[13] V. Arora, and L. Behera: “On-line melody extraction\nfrom polyphonic audio using harmonic cluster track-\ning,”IEEE Transactions on Audio, Speech and Lan-\nguageProcessing, Vol.21,No.3,pp.520–530,2013."
    },
    {
        "title": "An Analysis of Chorus Features in Popular Song.",
        "author": [
            "Jan Van Balen",
            "John Ashley Burgoyne",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415624",
        "url": "https://doi.org/10.5281/zenodo.1415624",
        "ee": "https://zenodo.org/records/1415624/files/BalenBWV13.pdf",
        "abstract": "This paper presents a computational study of the perceptual and musicological audio features that correlate with the structural function of sections in pop songs, specifically the chorus. Choruses have been described as more prominent, more catchy and more memorable than other sections in a song, yet chorus detection applications have always been primarily based on identifying the most-repeated section in a song. Inspired by cognitive research rather than applied signal processing, this computational analysis compiles a list of robust and interpretable features and models their influence on the ‘chorusness’ of a collection of song sections from the Billboard dataset. This is done through the unsupervised learning of a probabilistic graphical model. We show that timbre and timbre variety are more strongly related to chorus qualities than harmony and absolute pitch height. A regression and a classification experiment are performed to quantify these relations.",
        "zenodo_id": 1415624,
        "dblp_key": "conf/ismir/BalenBWV13",
        "keywords": [
            "pop songs",
            "chorus detection",
            "structural function",
            "computational study",
            "perceptual and musicological audio features",
            "song sections",
            "Billboard dataset",
            "timbre and timbre variety",
            "chorus qualities",
            "regression and classification experiment"
        ],
        "content": "AN ANALYSIS OF CHORUS FEATURES IN POPULAR SONG\nJan Van Balen1John Ashley Burgoyne2Frans Wiering1Remco C. Veltkamp1\n1Utrecht University, Department of Information and Computing Sciences\n2Universiteit van Amsterdam, Institute for Logic, Language and Computation\nfj.m.h.vanbalen,f.wiering,r.c.veltkamp g@uu.nl ,j.a.burgoyne@uva.nl\nABSTRACT\nThis paper presents a computational study of the percep-\ntual and musicological audio features that correlate with\nthe structural function of sections in pop songs, speciﬁcally\nthe chorus. Choruses have been described as more promi-\nnent, more catchy and more memorable than other sections\nin a song, yet chorus detection applications have always\nbeen primarily based on identifying the most-repeated sec-\ntion in a song. Inspired by cognitive research rather than\napplied signal processing, this computational analysis com-\npiles a list of robust and interpretable features and mod-\nels their inﬂuence on the ‘chorusness’ of a collection of\nsong sections from the Billboard dataset. This is done\nthrough the unsupervised learning of a probabilistic graph-\nical model. We show that timbre and timbre variety are\nmore strongly related to chorus qualities than harmony and\nabsolute pitch height. A regression and a classiﬁcation ex-\nperiment are performed to quantify these relations.\n1. INTRODUCTION\n1.1 Chorus Analysis\nThe term chorus originates as a designation for those parts\nof a musical piece that feature a choir or other form of\ngroup performance. When solo performance became the\nnorm in popular music, the term chorus was retained to in-\ndicate a repeated structural unit of musical form. In terms\nof musical content, the chorus has been referred to as the\n‘most prominent’, ‘most catchy’ or ‘most memorable’ part\nof a song and ‘the site of the more musically distinctive\nand emotionally affecting material’ [4, 10].\nWhile agreement on which section in a song constitutes\nthe chorus generally exists among listeners, attributes such\nas ‘prominent’ and ‘catchy’ are far from understood in mu-\nsic cognition and cognitive musicology [6]. On the other\nhand, as a frequent subject of study in the domain of Mu-\nsic Information Retrieval, the notion of chorus has been\nshown to correlate with a number of computable descrip-\ntors. Yet when studied more closely, the chorus detec-\ntion systems that locate choruses most successfully turn\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.out to rely on rather contextual cues such as the amount of\nrepetition and relative energy of the signal, with more so-\nphisticated systems also taking section length and position\nwithin the song into account [4, 5]. The central research\nquestion of this paper is therefore: in which computable\nproperties of popular music are choruses, when compared\nto other song sections, musically distinct?\nThe main motivations for a deeper study of the partic-\nularities of choruses are two-fold: ﬁrst, the chorus being\na central element of form in popular music, insight may\nbe gained in the popular song as a medium, and conscious\nas well as unconscious choices in songwriting may be un-\nveiled. Second, as choruses can be related to a catchy\nand/or memorable quality, to the notion of hooks, and per-\nhaps to a general cognitive salience underlying these as-\npects, the nature of choruses may indicate some of the mu-\nsical properties that constitute this salience, prominence or\nmemorability.\nThis investigation relates to chorus detection as known\nin Music Information Retrieval, but it does not have the\nsame goal. While chorus detection systems are built to lo-\ncate the choruses given unsegmented raw audio for a song,\nthis investigation aims to use similar and novel computa-\ntional methods to improve our understanding of choruses.\n1.2 Related Work\nExisting work on chorus detection strongly relates to au-\ndio thumbnailing, music summarization and structural seg-\nmentation. Audio thumbnailing and music summarization\nrefer to the unsupervised extraction of the most representa-\ntive short excerpt from a piece of musical audio, and often\nrely on full structure analysis as a ﬁrst step. An overview\nof relevant techniques is given by Paulus et al. [13].\nDeﬁnitions of the chorus in the MIR literature charac-\nterize the chorus as repeated, prominent and catchy. Since\nthe last two notions are never formalized, thumbnailing\nand chorus detection are essentially reduced to ﬁnding the\nmost often-repeated segment or section. A few chorus de-\ntection systems make use of additional cues from the song\naudio, including RefraiD by Goto and work by Eronen [4,\n5]. RefraiD makes use of a scoring function that favors\nsegments C occuring at the end of a longer repeated chunk\nABC and segments CC that consistently feature an internal\nrepetition. Eronen’s system favors segments that occur1\n4\nof the way through the song and reoccur near3\n4, as well as\nsegments with higher energy. In most other cases, heuris-\ntics are only used to limit the set of candidates from whichthe most frequent segment is picked, e.g. restricting to the\nﬁrst half of the song or discarding all segments shorter than\n4 bars.\nPaulus and Klapuri use a Markov model to label seg-\nments given a set of tags capturing which segments corre-\nspond to the same structural section (e.g. ABCBCD) [12,\n14]. This approach performs well on UPFBeatles , a dataset\nof annotated Beatles recordings, and fairly well on a larger\ncollection of songs (TUTstructure07).1Ann-gram method\nwithn= 3and a Variable-order Markov Model come out\nas the best techniques. The same methods have also been\nenhanced by using limited acoustic information: section\nloudness and section loudness deviation [14]. This boosts\nthe best performance (in terms of per-section accuracy) by\nup to 4 percent for TUTstructure07. Whether the model\ncould be improved with more acoustic information remains\nan open question that this paper aims to address.\nThe contribution of this paper is to introduce the notion\nof chorusness, and a statistical model of this measure for\nMIR applications, for popular music understanding and for\npopular music perception and cognition, using a novel and\nrigorous take on corpus-scale audio music analysis.\n2. METHODOLOGY\nThe research question formulated above is addressed by\nmeans of a statistical analysis of a selection of music de-\nscriptors, computed over a dataset of pop songs. The Bill-\nboard dataset, described in section 2.1, will be used as the\nground truth. The expert structure annotations available for\nthese data allow for parsing audio descriptors, detailed in\npart 2.2 of this paper, into per-section statistics. The anal-\nysis of the resulting variables will then be formalized by\nlearning a probabilistic graphical model from the data, as\nexplained in section 2.3.\n2.1 The Dataset\nTheBillboard dataset is a collection of time-aligned tran-\nscriptions of the harmony and structure of over 1000 songs\nselected randomly from the Billboard ‘Hot 100’ chart in\nthe United States between 1958 and 1991 [2]. The anno-\ntations include information about harmony, meter, phrase,\nand larger musical structure. The Billboard dataset is one\nof the largest and most diverse popular music datasets for\nwhich expert structure annotations exist and one of few to\nbe consistently sampled from actual pop charts. It can be\nexpected to reﬂect both important commonalities and sig-\nniﬁcant trends in popular music of the period of focus.\nIt includes a wide variety of genres, and suits the goal\nof drawing broadly-applicable musicological conclusions,\nmaking it the best available dataset for analysis of popular\nmusic choruses. For the present study, the complete v1.2\nrelease is used (649 songs), and of the annotations, only\nthe structural annotations are retained.\nThe structural annotations in the dataset follow the for-\nmat and instructions established in the SALAMI project\n1Dataset descriptions and links at http://www.cs.tut.fi/\nsgn/arg/paulus/structure.html[18]. The transcriptions contain start and end times for ev-\nery section and section labels for almost all sections. The\nsection labels the annotators were allowed to assign were\nrestricted to a list of 22, some of which were not used. The\nmost frequently recurring section labels are: verse (34% of\ntotal annotated time), chorus (24%), intro, solo, outro and\nbridge. The total number of sections, including the unla-\nbeled ones, is 7762.\n2.2 Perceptual Audio Features\nThe proposed corpus analysis–centered study requires some-\nwhat different kinds of descriptors than traditionally used\nin machine-learning applications in Music Information Re-\ntrieval. Four important constraints are applied. First, all\ndescriptors are demonstrated correlates of a relevant per-\nceptual or cognitive attribute of music. Second, we fa-\nvor musically interpretable descriptors. A classic exam-\nple of an audio feature with demonstrated perceptual cor-\nrelates but low interpretability are MFCC’s [1]. Third,\nonly transparent statistics over these features are consid-\nered. Higher order statistical moments and learned code-\nwords can be very informative from an engineering per-\nspective, but amount to highly uninterpretable descriptions.\nFinally, we also limit the set to a small number of hand-\npicked descriptors since the amount of data required for\nthe proposed analysis grows exponentially with the num-\nber of variables. All features are one-dimensional.\nLoudness The loudness descriptor is the standard psy-\nchological analogy of energy. It is obtained through com-\nparison of stimuli spectra and a standardized set of equal\nloudness curves. We will make use of the implementation\nby Pampalk [11]. The model applies outer-ear ﬁltering and\na spreading function before computing speciﬁc loudness\nvalues (Nkin sones) per band kand summing these values\nover all bands to obtain the total loudness T:\nS= max\nk(Nk) + 0:15\u0001X\nk6=maxNk (1)\nwhere the factor 0.15 serves as a weighting that empha-\nsizes the strongest band’s contribution. For every section,\nthe loudness mean is computed and stored, as well as the\ninter-quartile range ( Loudness IQR ), as a measure of the\nsection dynamics.\nSharpness The sharpness descriptor is the psychoa-\ncoustic analog of the spectral centroid. It characterizes the\nbalance between higher- and lower-band loudness. We will\nmake use of the speciﬁc loudnesses Nkas computed by\nPampalk [11] and summing as formulated by Peeters [15]:\nA= 0:11\u0002X\nkg(k)\u0001k\u0001Nk;where (2)\ng(k) =\u001a1 k<15\n0:066\u0002exp(0:171\u0001k)k>15(3)\nFor every section, we use the mean sharpness.\nRoughness Like the loudness descriptor, roughness\nis a mathematically deﬁned psychoacoustic measure. Itcharacterizes a timbral property of complex tones, relat-\ning to the proximity of its consituent partials. We will\nmake use of the MIRToolbox implementation by Lartillot\net al. [9], which is based on a model by Plomp and Lev-\nelt [16]. Since the roughness feature is very nonlinearly\ndistributed, the roughness feature is summarized for every\nsection by taking its median .\nMFCC MFCCs are established multidimensional cor-\nrelates of several aspects of timbre, designed to be max-\nimally independent. Typically around 13 MFCC coefﬁ-\ncients are used. In this model, the descriptor of interest is\nthe variety in timbre. This will be modeled by computing\nthe trace of the square root of the MFCC covariance ma-\ntrix, a measure of the timbre total variance . The MFCCs\nare computed following [11], and the ﬁrst component (di-\nrectly proportional to energy) is discarded.\nChroma Variance Chroma features are widely used\nto capture harmony and harmonic changes. In the most\ntypical implementation, the chroma descriptor or pitch class\nproﬁle consists of a 12-dimensional vector, each dimension\nquantifying the energy of one of the 12 equal-tempered\npitch classes. These energies can be obtained in several\nways. The Chordino chroma features distributed along\nwith the Billboard dataset are used here.2\nIn this study, the variety in the section’s harmony will\nbe measured by modeling the normalized chroma features\npas a 12-dimensional random variable and estimating a\nDirichlet distribution to the total of all of the section’s chro-\nma observations. Note that estimating just the total vari-\nance, as done for MFCC, would neglect the normalisa-\ntion constraint on chroma vectors and the dependencies it\nentails between pitch classes. The Dirichlet distribution\nD12(\u000b), can be written:\nf(p)\u0018D 12(\u000b) =\u0000(P12\nk=1\u000bk)Q12\nk=1\u0000(\u000bk)12Y\nk=1p\u000bk\u00001\nk;(4)\nwhere \u0000is the Gamma function, and can be seen as a distri-\nbution over distributions. We use the sum of the parameters\n\u000b, commonly refered to as the Dirichlet precision s:\ns=12X\nk=1\u000bk (5)\nThe Dirichlet precision quantiﬁes the difference between\nobserving the same combination of pitches thoughout the\nwhole section (high precision) and observing many differ-\nent distributions (low precision) [3]. There is no closed-\nform formula for sor\u000b, but several iterative methods ex-\nist that can be applied to obtain a maximum-likelihood es-\ntimation (e.g. Newton iteration). Fast ﬁtting can be done\nusing the fastﬁt Matlab toolbox by Minka.3\nPitch Salience The notion of pitch salience exists in\nseveral contexts. Here, it will refer to the strength of (a\n2http://www.isophonics.net/nnls-chroma\n3http://research.microsoft.com/en-us/um/\npeople/minka/software/fastfit/discrete set of) pitches, i.e. a measure of the combined\nstrength of a frequency and its harmonics, as in [17]. The\nmean of the strongest (per frame) pitch strength will be\ncomputed for every section.\nPitch Centroid The pitch height of polyphonic au-\ndio can be deﬁned and computationally approximated in\nseveral ways. A predominant fundamental frequency in\nthe classic sense is not always reliably found, especially\nfor the case of polyphonic pop music. We therefore use\nthe more robust pitch centroid . We deﬁne this as the aver-\nage pitch height with all pitches weighted by their salience.\nNote that the pitch salience proﬁle used here spans multi-\nple octaves and exploits spectral whitening, spectral peak\ndetection and harmonic weighting in order to capture only\ntonal energy and emphasize the harmonic components.\nOur feature set includes the section mean of the pitch\ncentroid as well as the inter-quartile range of this measure,\nrepresented by the Pitch Centroid IQR .\nSection Length The length of the section in seconds.\nSection Position The position of the section inside\nthe song is included as a number between 0 and 1.\nThe descriptors above have proven useful in a variety of\nother contexts and fall within the constraints listed above.\nThe ﬁrst three originate in psychoacoustics, the next three\ndescriptors stem from MIR research. The pitch centroid is\na novel descriptor designed with robustness in mind.\n2.3 Analysis Method\n2.3.1 PGM\nThe resulting descriptors make up a dataset of 7762 obser-\nvations (sections) and 12 variables (descriptors) for each\nobservation: the above perceptual features and one section\nlabel. These data will be used to model what features cor-\nrelate with a section being a chorus or not. However, mod-\neling all dependencies between a set of variables quickly\nleads to complex representations that are hard to manage.\nProbabilistic graphical models (PGM) are graph-based\nrepresentations of such complex higher-dimensional distri-\nbutions that focus on modeling the direct probabilistic in-\nteractions between variables [8]. Unlike a correlation ma-\ntrix, they encode which variables are conditionally depen-\ndent, i.e. correlate given the state of all other variables, re-\ngardless of any indirect effects from other inﬂuencing vari-\nables. Examples of the use of a PGM in music analysis can\nbe found in [3].\nEssential to a PGM is its graph structure. It is typically\nconstructed using prior expert knowledge but, with enough\ndata, can also be learned. Learning the PGM structure gen-\nerally requires a great amount of conditional independence\ntests. The PC-algorithm optimizes this procedure and, in\naddition, provides information about the direction of the\ndependencies [7]. When not all directions are found, a par-\ntially directed graph is returned.\nGiven the limitations of currently available software pack-\nages, an important practical requirement for learning thegraph structure is that the variables follow similar distribu-\ntions, e.g. all discrete, or all continuous Gaussian. In the\nanalysis in the next section, all data are modeled as contin-\nuous. This means that also the Section Type variable will\nhave to be modeled as continuous. We do this by introduc-\ning the notion of Chorusness .\n2.3.2 Chorusness\nThe Chorusness variable is derived from the Chorus Prob-\nabilitypC, a function over the domain of possible section\nlabels. The chorus probability pC(T)of a section label T\nis deﬁned as the probability of a section with label Tbe-\ning labeled ‘chorus’ by an independent annotator. In terms\nof the annotations x1andx2of two independent listeners,\npC(T)can be written:\npC(T) =p(x1=Cjx2=T) +p(x2=Cjx1=T)\n2;\n(6)\nwhereCrefers to the label ‘chorus’.\nThe Billboard dataset has been annotated by only one\nexpert per song, therefore it contains no information about\nany of thepC(T). However, in the SALAMI dataset, an-\nnotated under the same guidelines and conditions, two in-\ndependent annotators were consulted per song [18]. The\nannotators’ behaviour can therefore be modeled by means\nof a confusion matrix M(T1;T2)2[0;1]22\u000222:\nM(T1;T2) =f(x1=T1\\x2=T2) (7)\nwith frequencies fin seconds (of observed overlapping la-\nbelsT1andT2). Since the two annotators are interchange-\nable (and have in fact been randomized), Mmay be aver-\naged out to obtain a symmetric confusion matrix M?:\nM?=M+MT\n2(8)\nFrom here we can obtain the empirical Chorus Probability:\npC(T) =M?(T;C)P\nkM?(T;k)2[0;1]: (9)\nChorus Probability values for every section type were\nobtained from the Codaich-Pop subset of the SALAMI data-\nset (99 songs). Finally, the Chorus Probability is scaled\nmonotonically to obtain the Chorusness measure C(T), a\nstandard log odds ratio ofpC:\nC(T) = log\u0012pC(T)\n1\u0000pC(T)\u0013\n2(\u00001;1): (10)\nIt ranges from\u00008:41(for the label ‘spoken’) to 0:83(for\nthe label ‘chorus’).\n2.3.3 Implementation\nBefore the model learning, a set of Box-Cox tests is per-\nformed to check for rank-preserving polynomial transfor-\nmations that would make any of the variables more normal.\nThe Chroma Precision sis found to improve with a power\nparameter\u0015=\u00001, and therefore scaled as:\nS=s\u0015\u00001\n\u0015= 1\u00001\ns(11)The Section Length, Loudness IQR and Pitch Centroid IQR\nare found to improve with a logtransform. Weeding out\ndivergent entries in the dataset leaves us with a subset of\n6462 sections and 12 variables.\nThe R-package pcalg implements the PC-algorithm. Be-\nginning with a fully connected graph, it estimates the graph\nskeleton by visiting all pairs of adjecent nodes and test-\ning for conditional independence given all possible subsets\nof the remaining graph.4The procedure is applied to the\n6462\u000212dataset, with ‘conservative’ estimation of direc-\ntionalities, i.e. no directionality is forced onto the edges\nwhere no V-structures were found indicating a speciﬁc di-\nrection.\n3. ANALYSIS RESULTS\nThe resulting graphical model is shown in Figure 1. It is\nobtained with p < 3:5\u000210\u00005, the signiﬁcance level re-\nquired to bring the overall probability of observing one or\nmore edges due to chance, under 5 percent. In terms of\nthe signiﬁcance level \u000bCIof the conditional independence\ntests and\u000bPGMof the model:\n\u000bCI= 1\u0000(1\u0000\u000bPGM)1=n\u0019\u000bPGM\nn(12)\nwith\u000bPGM\u001c1(here 0:05) andnthe number of tests per-\nformed (\u00181500 ). Note thatp\u001910\u00005is a conservative pa-\nrameter setting for an individual test. As a result, we may\nchoose to view the model as a depiction of dependencies\nrather than independencies, since the latter may always be\npresent at a lower signiﬁcance than required by the \u000b.\n3.1 Discussion\nAt least three kinds of feature relations are expected. First,\nthere are the correlations between features that are closely\nrelated on the signal level (black edges): Loudness and\nPitch Salience, for example, measure roughly the same as-\npects of a spectrum (and can be expected to be proportional\nto roughness), and so do Sharpness and Pitch Centroid.\nRoughness is a highly non-linear feature that is known to\nbe proportional to energy. The model reﬂects this.\nThe second kind of correlations are the relations be-\ntween variance-based features and the Section Length vari-\nable. Musically, it is expected that longer sections allow\nmore room for an artist to explore a variety of timbres\nand pitches. This effect is observed for Chroma Variance\nand Pitch Centroid IQR, though not for MFCC Variance\nand Loudness IQR. Interestingly, correlations with Section\nLength point towards it rather than away (dotted edges):\nthe length of a section length is a result of its variety in\npitch and timbre content, rather than a cause. The impor-\ntance of this distinction can be debated.\nThird, some sections might just display more overall va-\nriety, regardless of the section length. This would cause\ndifferent variances to relate, resulting in a set of arrows be-\ntween the four variance features. Four such relations are\nobserved (lighter, orange edges).\n4http://cran.r-project.org/web/packages/pcalg/Chroma Var.\nPitch Centr. IQRPITCH CENTR.\nLengthMFCC VAR.LOUDNESS IQR\nCHORUSNESSROUGHNESSSHARPNESSPITCH SAL.LOUDNESSPositionFigure 1 . Graphical model of the 11 analyzed perceptual features and chorusness variable C.\u000bPGM= 0:05.\nWe now note that Sharpness, Pitch Salience and Rough-\nness predict Chorusness, as well as the MFCC Variance\n(bold edges). All of these can be categorized as primarily\ntimbre-related descriptors. Section Length, Section Posi-\ntion and Chroma Variance are d-separated from Chorus-\nness, i.e. no direct inﬂuence between them has been found.\nThe status of Pitch Centroid, Loudness, and Loudness IQR\nis uncertain. Depending on the true directionality of the\nChorusness, MFCC Variance and Roughness relations, they\nmay be part of the Chorusness Markov blanket , the set\nof Chorusness’ parents, children, and parents of children,\nwhichd-separates Chorusness from all other variables [8].\nAlso interesting are the more unexpected dependencies.\nFor example, two variables depend directly on the Section\nPosition, while Chorusness does not. This may be due to\nthe limitations of the normal distribution by which all vari-\nables are modeled; it is fair to say that it might not re-\nﬂect the potentially complex relation of Chorusness vari-\nable and Section Position. However, the Position variable\ndoes predict Sharpness and Pitch Centroid to some extent\n(dotted edges). A simple regression also shows both vari-\nables correlate positively . This suggests some kind of over-\ntime intensiﬁcation along the frequency dimension exists\nin the songs of the Billboard corpus.\nFinally, the dashed red edges in the diagram indicate\ndependencies that are most unintuitive. Tentative explana-\ntions may be found, but since they have no effect on Cho-\nrusness, we will omit such speculations here.\n3.2 Regression\nWe are more interested to see in more detail how the set of\nChorusness-related features predict our variable of inter-\nest. Table 1 lists the coefﬁcients of a linear regression on95% CI\nCoeff. LL UU\nSharpness 0.11 0.10 0.13\nMFCC variance 0.12 0.09 0.15\nRoughness 0.12 0.08 0.16\nPitch Salience (\u000210) 0.04 0.03 0.05\nLoudness 0.03 -0.01 0.06\nLoudness IQR -0.33 -0.48 -0.18\nPitch Centroid 0.10 0.07 0.12\nTable 1 . Results of a multivariate linear regression on the\nChorusness’ Markov blanket ( p < 10\u000015for all coeff.).\nCI=conﬁdence interval, LL=lower limit, UU=upper limit.\nthe Chorusness variable and its Markov blanket, i.e. those\nvariables for which a direct dependency with chorusness is\napparent from the model. Since there is no certainty about\nthe exact composition of the Markov blanket, all candi-\ndates are included. Note that, having deﬁned Chorusness\nas a log odds ratio, this linear regression is de facto a com-\nmon form of logistic regression on the section’s original\nChorus Probability pC2[0;1].\nOne can see that all features but the Loudness IQR have\npositive coefﬁcients. We conclude that, in this model, sec-\ntions with high Chorusness are louder, sharper and rougher\nthan other sections. Chorus-like sections also feature a\nslightly higher and more salient pitch, a smaller dynamic\nrange and greater variety in MFCC timbre.\n4. VALIDATION\nFinally, a validating experiment is performed. It consists\nof the evaluation of a 2-way classiﬁer that aims to labelsections as either ‘chorus’ or ‘non-chorus’. A k-nearest\nneighbour classiﬁer ( k= 1) is trained on half of the avail-\nable sections, and tested on the other half (randomly par-\ntitioned). This procedure is repeated 10 times to obtain an\naverage precision, recall and F-measure. The results are\npositive: using just the Markov blanket features of table 1,\nthe classiﬁer performs better than random: F= 0:52,\n95% CI [0:51;0:52]vs. a maximum random baseline of\nF= 0:36. The classiﬁer also performs better than one\nthat uses all features ( F= 0:48), or only Loudness and\nLoudness IQR ( F= 0:48), the features used in [14].\n5. CONCLUSIONS\nThis paper presents a computational study of the musi-\ncally interpretable and robust audio descriptors that cor-\nrelate with the ‘chorusness’ of sections in pop songs. A se-\nlection of existing and novel perceptual and computational\nfeatures is presented. The set has been analyzed using\na probablistic graphical model and a measure of chorus-\nness that is derived from annotations and an inter-annotator\nconfusion matrix. The resulting model was complemented\nwith a regression on the most important variables. The re-\nsults show that choruses and chorus-like sections are louder,\nsharper and more rough, and feature a higher and more\nsalient pitch, a smaller dynamic range and greater variety\nof MFCC-measurable timbre than other sections.\nThe results obtained in a validating classiﬁcation exper-\niment show that our model does not reach the level of ac-\ncuracy obtained by the state of the art techniques that in-\ncorporate repetition information. However, it demonstrates\nfor the ﬁrst time that there is a class of complementary mu-\nsical information that, independently of repetition, can be\nused to locate choruses. This suggests that our model can\nbe applied to complement existing structure analysis ap-\nplications, while repetition information and section order\ncan in turn enhance our model of chorusness for further\napplication in popular music cognition research and audio\ncorpus analysis.\n6. ACKNOWLEDGEMENTS\nThis research is supported by the NWO CATCH project\nCOGITCH (640.005.004), and the FES project COMMIT/.\n7. REFERENCES\n[1] J. J. Aucoutourier and E. Bigand: “Mel Cepstrum &\nAnn Ova: The Difﬁcult Dialog between MIR and Mu-\nsic Cognition,” Proc. of the Int. Society for Music In-\nformation Retrieval Conf. , pp. 397–402, 2012.\n[2] J. A. Burgoyne, J. Wild and I. Fujinaga: “An Expert\nGround Truth Set for Audio Chord Recognition and\nMusic Analysis,” Proc. of the Int. Society for Music\nInformation Retrieval Conf. , pp. 633–638, 2011.\n[3] J. A. Burgoyne: Stochastic Processes and Database-\ndriven Musicology , PhD Thesis, McGill University,\nMontr ´eal, Qu ´ebec, 2011.[4] A. Eronen and F. Tampere: “Chorus Detection with\nCombined Use of MFCC and Chroma Features and Im-\nage Processing Filters”, Proc. of the Int. Conf. Digital\nAudio Effects , 2007.\n[5] M. Goto: “A Chorus-section Detecting Method for\nMusical Audio Signals,” Proc. IEEE Int. Conf. Acous-\ntics, Speech and Signal Processing , pp. 437–440, 2003.\n[6] H. Honing: “Lure(d) into listening: The Potential of\nCognition-based Music Information Retrieval,” Empir-\nical Musicology Review , V ol. 5, No. 4, 2010.\n[7] M. Kalisch et al.: “Causal Inference Using Graphical\nModels with the R Package pcalg,” Journal of Statisti-\ncal Software , V ol. 47, No. 11, pp. 1–26, 2012.\n[8] D. Koller and N. Friedman: Probabilistic Graphical\nModels: Principles and Techniques , MIT Press, 2009.\n[9] O. Lartillot and P. Toiviainen: “MIR in Matlab (II): A\nToolbox for Musical Feature Extraction from Audio,”\nProc. of the Int. Society of Music Information Retrieval\nConf. , pp. 127–130, 2007.\n[10] R. Middleton: “Form,” in: Continuum Encyclopedia of\nPopular Music of the World , eds. J. Shepherd, D. Horn,\nD. Laing, Continuum Int. Publishing Group, 2003.\n[11] E. Pampalk: “A Matlab Toolbox to Compute Similarity\nfrom Audio,” Proc. of the Int. Society for Music Infor-\nmation Retrieval Conf. , 2004.\n[12] J. Paulus and A. Klapuri: “Labelling the Structural\nParts of a Music Piece with Markov Models,” in:\nComputer Music Modeling and Retrieval: Genesis\nof Meaning in Sound and Music , Berlin: Springer,\npp. 166-176, 2009.\n[13] J. Paulus et al.: “Audio-based Music Structure Analy-\nsis,” Proc. of the 11th Int. Society for Music Informa-\ntion Retrieval Conf. , pp. 625–636, 2010.\n[14] J. Paulus: “Improving Markov Model-Based Music\nPiece Structure Labelling with Acoustic Information,”\nProc. of the 11th Int. Society for Music Information Re-\ntrieval Conf. , pp. 303–308, 2010.\n[15] G. Peeters: A Large Set of Audio Features for Sound\nDescription in the CUIDADO Project , Tech. Rep., IR-\nCAM, Paris, France, 2004.\n[16] R. Plomp and W. J. M. Levelt: “Tonal Consonance and\nCritical Bandwidth,” Journal of the Acoustical Society\nof America , V ol. 38, No. 4, pp. 548–560, 1965.\n[17] J. Salamon and E. Gomez: “Melody Extraction from\nPolyphonic Music Signals Using Pitch Contour Char-\nacteristics,” IEEE Trans. on Audio, Speech and Lan-\nguage Processing , V ol. 20, No. 6, pp. 1759–70, 2010.\n[18] J. Smith et al.: “Design and Creation of a Large-\nScale Database of Structural Annotations,” Proc. of\nthe Int. Society for Music Information Retrieval Conf. ,\npp. 555–560, 2011."
    },
    {
        "title": "Design and Evaluation of Semantic Mood Models for Music Recommendation using Editorial Tags.",
        "author": [
            "Mathieu Barthet",
            "David Marston",
            "Chris Baume",
            "György Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418005",
        "url": "https://doi.org/10.5281/zenodo.1418005",
        "ee": "https://zenodo.org/records/1418005/files/BarthetMBFS13.pdf",
        "abstract": "In this paper we present and evaluate two semantic music mood models relying on metadata extracted from over 180,000 production music tracks sourced from I Like Music (ILM)’s collection. We performed non-metric multidimensional scaling (MDS) analyses of mood stem dissimilarity matrices (1 to 13 dimensions) and devised five different mood tag summarisation methods to map tracks in the dimensional mood spaces. We then conducted a listening test to assess the ability of the proposed models to match tracks by mood in a recommendation task. The models were compared against a classic audio contentbased similarity model relying on Mel Frequency Cepstral Coefficients (MFCCs). The best performance (60% of correct match, on average) was yielded by coupling the fivedimensional MDS model with the term-frequency weighted tag centroid method to map tracks in the mood space.",
        "zenodo_id": 1418005,
        "dblp_key": "conf/ismir/BarthetMBFS13",
        "keywords": [
            "semantic music mood models",
            "metadata from ILM",
            "non-metric multidimensional scaling",
            "mood stem dissimilarity matrices",
            "five different mood tag summarisation methods",
            "tracks in dimensional mood spaces",
            "listening test",
            "classic audio content-based similarity model",
            "Mel Frequency Cepstral Coefficients",
            "coupling five-dimensional MDS model"
        ],
        "content": "DESIGN AND EV ALUATION OF SEMANTIC MOOD MODELS FOR\nMUSIC RECOMMENDATION\nMathieu Barthet1, David Marston2, Chris Baume2, Gy¨orgy Fazekas1, Mark Sandler1\n1Centre for Digital Music, Queen Mary University of London\nffirstname.lastname g@eecs.qmul.ac.uk\n2BBC R&D London, ffirstname.lastname g@bbc.co.uk\nABSTRACT\nIn this paper we present and evaluate two semantic mu-\nsic mood models relying on metadata extracted from over\n180,000 production music tracks sourced from I Like Mu-\nsic (ILM)’s collection. We performed non-metric multi-\ndimensional scaling (MDS) analyses of mood stem dis-\nsimilarity matrices (1 to 13 dimensions) and devised ﬁve\ndifferent mood tag summarisation methods to map tracks\nin the dimensional mood spaces. We then conducted a\nlistening test to assess the ability of the proposed mod-\nels to match tracks by mood in a recommendation task.\nThe models were compared against a classic audio content-\nbased similarity model relying on Mel Frequency Cepstral\nCoefﬁcients (MFCCs). The best performance (60% of cor-\nrect match, on average) was yielded by coupling the ﬁve-\ndimensional MDS model with the term-frequency weighted\ntag centroid method to map tracks in the mood space.\n1. INTRODUCTION\nResearch on music and emotions is a burgeoning ﬁeld in\nthe Music Information Retrieval community, generating an\never-increasing number of studies [1]. Although the na-\nture of emotional responses to music is still a misunder-\nstood and controversial topic [6], several analyses of music\nconsumer needs and behaviors have highlighted the use-\nfulness of developing search engines which can organise\ntracks according to the moods they express (see e.g. [8]).\nEven before the advent of online music technologies and\nthe concurrent removal of physical media for music such\nas the CD, production music labels1created track compi-\nlations according to suggested moods; this music delivery\nmodel persists today (see e.g. the “Mood for Love” compi-\nlation from West One Music Group2). Production music\nhas, by tradition, been called “mood music” as it is often\n1Production music is recorded music for use in ﬁlm, television, radio\nand other media.\n2http://www.westonemusic.com/album?catno=WOM_\nRFM_0018\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.composed to convey mostly constant emotional responses\nfacilitating its association with a narrative, as opposed to\ncommercial music which often presents larger changes of\nperceived emotions over time. Creative producers search-\ning for background music for television, ﬁlm, or radio of-\nten use keywords which evoke emotions (e.g. “happy mu-\nsic for kids”), or emotion-eliciting situations (e.g. “bas-\nketball music”). Unfortunately, due to the complex and\nsubjective nature of music emotions there is no consen-\nsual agreement between production music libraries on the\ngenesis and organisation of mood-related metadata. This\nhinders the recommendation of tracks based on mood, es-\npecially on music search engines which aggregate several\nproduction music catalogues. In this paper we propose\ntwo different methods to represent the relationships be-\ntween mood tags (semantic mood models) based on analy-\nsis of mood-related metadata extracted from 183,176 pro-\nduction music tracks from I Like Music (ILM)’s aggre-\ngated catalogue (29 different production music libraries).\nWe then assess how well the various models perform in a\ntask of music recommendation when combined with ﬁve\ndifferent tag summarisation techniques. In a companion\nstudy, the mood model providing the best accuracy (a ﬁve-\ndimensional model derived from multidimensional scal-\ning analysis of mood tag co-occurences) was used for au-\ndio content-based music emotion recognition using sup-\nport vector regression (SVR).\n2. RELATED WORK\nOther data-driven semantic mood models have previously\nbeen proposed. In [9], latent semantic analysis (LSA) tech-\nniques were applied to 105 emotion words occurring in\ntags associated with 8,872 Last.fm tracks. The resulting\nlow-dimensional representation obtained after MDS anal-\nysis was in line with the traditional arousal and valence\ndimensions from Russell’s circumplex [12], and a third di-\nmension correlated with the spiritual ormeditative compo-\nnent of musical experience. The conclusions from [5] also\nfound mood spaces in agreement with the A V plane after\napplying MDS to co-occurrence counts of artists from the\nLast.fm dataset whose styles were described by 146 mood\ntags. The model from [9] was extended in [14] in which\nthe affective circumplex transform (ACT) was applied to\nLSA-MDS spaces in order to infer explicit conﬁgurations\nmatching Russell’s circumplex. The use of ACT providedbetter accuracy than the baseline LSA-MDS technique ac-\ncording to human judgements. The semantic mood models\nproposed in this paper (Section 3) differ from previous ap-\nproaches in several ways. First, to the best of our knowl-\nedge, no formal assessment of such models has previously\nbeen conducted in relation to music recommendation (Sec-\ntion 4). Second, the models are derived from mood an-\nnotations curated by music experts (production music li-\nbraries) rather than social tags which are more noisy and\nidiosyncratic by essence. The analysed mood tags come\nfrom the large-scale ILM dataset including a high num-\nber of unique mood-related tags (2,060). The comparative\nstudy [13] showed that mood models derived from this set\nof curated editorial mood tags signiﬁcantly outperformed\nmood models derived from Last.fm social tags, based on\nhuman judgements of arousal, valence and tension. Third,\nthe models which obtained the best performance rely on\na higher number of dimensions (5-D and 10-D) than the\nclassic 3-D emotion model.\n3. DESIGN OF SEMANTIC MOOD MODELS\n3.1 Data-driven Mood Model (MDS)\nIn the same way that a measure of similarity between tracks\ncan be derived from tag co-occurrence counts, a measure\nof similarity between tags can be derived from their co-\noccurrence over tracks [10].\n3.1.1 Processing of Mood Tags\nIn the case of curated editorial metadata, tracks are asso-\nciated with a list of unique tags judged to be the most ap-\npropriate by professional music experts. Hence, a given\ntag is only attributed once to a track, unlike for social tags\nfor which a large number of users apply tags to tracks.\nInitially the mood tags were cleaned by correcting mis-\nspellings (100 errors out of 2,398 mood tags), removing\nduplicates (338 duplicates yielding 2,060 unique tags), and\nstripping white spaces and punctuation marks (e.g. ‘.”, ‘!”).\nInstead of following a bag-of-words approach, in which\nthe meaning of certain tags consisting of a series of words\ncan be lost (e.g. “guilty pleasure”), we collated multiple\nwords together to further process them as single entities\n(using a hyphen between the words). The vocabulary used\nin the ILM editorial annotations is composed of conven-\ntional words and does not suffer from the idiosyncrasies\nof social tags which often include sentences, informal ex-\npressions (e.g. “good for dancing to in a goth bar”, cited\nas an example in [10]), or artists’ names. For this reason,\nwe did not have to tokenise the tags with a stop-list (for in-\nstance, removing words such as “it”, “and”, “the”). How-\never, we used a stemmer algorithm3to detect tags with\nsimilar base parts (e.g. ”joyful” and ”joy”), as these refer\nto identical emotional concepts. 1,873 mood-related stems\nwere obtained out of the 2,060 unique mood tags. In or-\nder to reduce the size of the stem vector while maintaining\nthe richness of the track descriptions, we only kept stems\n3The PorterStemmer algorithm from the Natural Language Toolkit\n(NLTK) package for Python was used.which were associated with at least 100 tracks in further\nanalyses. This stem ﬁltering process yielded a list of 453\nstems which provided at least one mood tag for each of the\n183,176 tracks from the ILM dataset. The associations be-\ntween tracks and stems are provided in a document-term\nmatrix X=fxijgwhere:\nxij=\u001a1 if stemjis associated with track i\n0 otherwise(1)\nThe stem pairwise co-occurrences over tracks cijare\nthen given by:\ncij=jfx\u000fig\\fx\u000fjgj (2)\nwherefx\u000figis the set of tracks annotated with stem iand\nj jis the cardinality operator. The measure of dissimilar-\nity between stems sijis computed as follows:\nsij= 1\u0000cij\nMax (cij)(3)\nwhereMax (cij)is the maximum of the pairwise stem co-\noccurrence in the ILM dataset (26,859).\n3.1.2 Multidimensional Scaling Analysis\nNon-metric multidimensional scaling (MDS) analyses were\nthen applied to the stem dissimilarity matrix, S=fsijg.\nFour outlier stems with a null or very small co-occurrence\nmeasure compared to all the other stems were discarded\nso as not to bias the MDS analysis (this yielded a list of\n449 stems). Figure 1 shows the evolution of Kruskal’s\nstress1 [7] as the number of dimensions (D) increases from\n1 to 13. Following a rule of thumb for MDS [3], accept-\nable, good and excellent representations are obtained for D\n= 3 (stress<0.2), D = 5 (stress <0.1) and D = 11 (stress\n<0.05). Interestingly, ﬁve dimensions yield a good rep-\nresentation (elbow of the scree plot). This result suggests\nthat more than three dimensions are required to accurately\ncategorise mood terms in the context of production music,\nwhich contrasts with the classic three-dimensional emo-\ntion model (arousal, valence and dominance) [11]. In fur-\nther analyses, we mapped the mood stems back to mood\ntags to uncover the meaning of the dimensions. We in-\nspected whether the organisation of the terms along each\ndimension of the MDS conﬁgurations was relevant accord-\ning to deﬁnable emotion-related concepts. For mood tags\ncommon to the list from the Affective Norm for English\nWords (ANEW) [2] (123 common tags were found out\nof our list of 449), we computed the correlations (Pear-\nson’sr) between the coordinates of the mood tags along\neach dimension of the MDS conﬁgurations and the ANEW\nmeasures of arousal, valence and dominance (see results in\nTable 1 in the case where D=5). Interestingly, three out\nof the ﬁve MDS dimensions are signiﬁcantly correlated\nwith the arousal and/or valence and/or dominance dimen-\nsions, showing that the 5-D MDS conﬁguration captures\naspects of the core emotion dimensions. However, some\nof the MDS dimensions are concurrently correlated with\nthe arousal, valence and dominance dimensions. This isFigure 1 . Kruskal’s stress1 as a function of the number of\ndimensions in the MDS analysis of the mood stem dissim-\nilarity matrix.\npartly due to the fact that these dimensions can co-vary for\ncertain emotions. For instance, the correlation between the\nANEW valence and dominance dimensions is highly sig-\nniﬁcant (r=.83, p <.001) which may explain why the sec-\nond MDS dimension is correlated with both valence and\ndominance. A positive, although weak, correlation (r=.20,\np<.05) was also found between the ANEW arousal and\ndominance dimensions. In contrast, no signiﬁcant corre-\nlation was found between the ANEW arousal and valence\ndimensions. In order to disambiguate some of the MDS di-\nmensions, and as MDS yields a solution which is invariant\nto rotation, we next applied a transformation to the MDS\nspace to align it according to existing mood models.\n3.1.3 Affective Circumplex Transform (ACT)\nThe affective circumplex transform (ACT) proposed in [14]\nwas applied to the ﬁve-dimensional MDS conﬁguration pre-\nviously described. The goal of the ACT is to match the\ntwo ﬁrst dimensions of the MDS conﬁguration with the di-\nmensions from Russell’s arousal/valence (A V) model [12].\nAs in [13], we ﬁrst determined mood terms common to\nboth the MDS and Russell’s A V spaces (37 terms were\nfound in common). The ACT maintains the relative dis-\ntances between mood terms in the initial MDS conﬁgura-\ntion, since it only allows translation, reﬂection, orthogo-\nnal rotation, and isotropic scaling. Measures of correlation\nbetween the transformed ﬁve-dimensional MDS conﬁgu-\nration and the arousal, valence and dominance measures\nfrom the ANEW dataset are reported in Table 1. The ﬁrst\ndimension of the MDS-ACT conﬁguration is strongly cor-\nrelated with valence (r=.56, p <.001) whereas before the\nACT, no signiﬁcant correlations with arousal, valence and\ndominance were found for that dimension. The strong rela-\ntionship between valence and dominance can explain why\nthe ﬁrst MDS-ACT dimension is also correlated with dom-\ninance (r=.30, p <.001). The second and third dimensions\nare only correlated with arousal (r=.35, p <.001) and dom-\ninance (r=.34, p <.001), respectively. Hence, the ACT can\ninfer an explicit representation according to the core emo-\ntion dimensions. Although no clear interpretations of thefourth and ﬁfth dimensions have yet been found, the 5-D\nmodel yields a tag conﬁguration which can be used to com-\npute track distance measures, as described in the next two\nsections.\n3.2 Tag Summarisation Methods\nWe devised several methods to summarise the tags of a\ntrack in a given multidimensional mood space. Let’s de-\nnote T=ftijgas the tag matrix representing the coordi-\nnates of the tags iof a track across the dimensions jof\nthe mood space. For the methods described in Sections\n3.2.1 to 3.2.4, the tag summary matrix Y=fyijgis ob-\ntained by multiplying the tag matrix with a weight matrix\nW=fwig.\n3.2.1 Tag of Maximum Term Frequency (MTF)\nThis method assumes that a track is best represented by\nthe tag from its set of tags which has the highest term fre-\nquency (TF) in the dataset. The weight wifor the N tags\nof a track are as follows:\nwi=\u001a1if TF (ti) =Max i=(1:::N )[TF(ti)]\n0 otherwise\n(4)\n3.2.2 Centroid (CEN)\nThis method summarises the tags of a track by their cen-\ntroid or geometrical mean in the mood space. The tag\nweights are hence given by wi=1\nN.\n3.2.3 Term-Frequency Weighted Centroid (TFW)\nThis method summarises the tags of a track by their cen-\ntroid after attributing to each tag a weight proportional to\nits term frequency (TF): wi=TF(ti)P(TF(ti)). Hence the cen-\ntroid is attracted by the tag with the highest term frequency.\n3.2.4 Inverse Term-Frequency Weighted Centroid (ITF)\nConversely, this method attributes more weight to the tag\nwith the lowest term frequency following the assumption\nthat this tag may convey more speciﬁc information about\nthe song:wi=1=TF (ti)P(1=TF (ti)).\n3.2.5 Mean and Variance (MVA)\nRather than summarising the tags of a track by a point in\nthe space, this method assumes that the tags can be repre-\nsented by a Gaussian distribution. The tag summary matrix\nYis given by the mean \u0016Tand variance \u001b(T)of the tag ma-\ntrix:Y=f\u0016T;\u001b(T)g.\n3.3 Model Derived from Mood Taxonomy (CLUST)\nPopular mood key words were added to an initial selec-\ntion provided by ILM to create a list of 355 mood words.\nOver 95% of the production music library contained at\nleast one of these 355 words. Each of these words were\nplaced in one of 27 categories, which became the start-\ning point for a cluster-based model. Each category was\ntreated as a cluster containing several mood words. ManyBefore ACT After ACT\nDimension Arousal Valence Dominance Arousal Valence Dominance\nMDS Dim 1 - - - - .56\u0003\u0003\u0003.30\u0003\u0003\u0003\nMDS Dim 2 - .31\u0003\u0003\u0003.36\u0003\u0003\u0003.35\u0003\u0003\u0003- -\nMDS Dim 3 -.33\u0003\u0003\u0003.47\u0003\u0003\u0003.19\u0003- - .34\u0003\u0003\u0003\nMDS Dim 4 - - - - - -\nMDS Dim 5 -.20\u0003-.24\u0003\u0003-.31\u0003\u0003\u0003- - -\nTable 1 . Correlation (Pearson’s r) between the dimensions of the ﬁve-dimensional MDS conﬁguration and the arousal,\nvalence and dominance dimensions, as characterised by the ANEW dataset. Only signiﬁcant correlations are reported.\u0003\u0003\u0003\np<.001,\u0003\u0003p<.01,\u0003p<.05\nof these clusters could be considered to overlap in their\nmood; some were clearly opposites while others had lit-\ntle in common. To convert these clusters into dimensions,\nthe overlapping ones were combined into single dimen-\nsions; any opposite clusters were converted into negative\n(-ve) values of the dimension they were opposite to; and\nthe non-overlapping clusters were treated as new dimen-\nsions. Using this method, the 27 clusters were converted to\n10 dimensions, giving each of the 355 mood words 10 di-\nmensional mood values. The choice of allocation of words\nto clusters and clusters to dimensions was performed based\non only one person’s opinion. The choice of 10 dimensions\nwas a compromise between combining clusters that were\ntoo dissimilar and having too sparse a model. To illustrate\nthe process, the ﬁrst three dimensions represent the follow-\ning mood clusters: 1) Conﬁdent (+ve scale), Cautious &\nDoubtful (-ve scale); 2) Sad & Serious (+ve scale), Happy\n& Optimistic (-ve scale); and 3) Exciting (+ve scale), Calm\n(-ve scale).\nAs each music track is associated with several mood\ntags which each mapped to 10 dimensional values, tags\nhad to be combined. The most simple and obvious way\nwould be to take the mean of all the mood values to gen-\nerate a single 10-dimensional value for a track. However,\nit was felt that a music track can be represented by moods\nthat differ signiﬁcantly, so combining them into a single\nmood would be too crude. Therefore, a method (denoted\nPEA) to generate two mood values per track was devised.\nThis method uses clustering of the 10-D scores where close\nscores are combined together. The means of the two most\nsigniﬁcant clusters are then calculated, resulting in two 10-\nD mood values for each track. A weight was assigned to\neach value according to the size of the cluster.\n3.4 Track Distance Measures in Mood Space\nFor the purposes of searching a database of tracks with\nmood values assigned to them, a distance measurement\nis required to ﬁnd which tracks most closely match each\nother. For the MDS-based models (Section 3.1), distances\nbetween tracks were obtained using either the Euclidean\ndistance between tag summary vectors (methods MTF, CEN,\nTFW, ITF), or the Kullback-Leibler (KL) divergence be-\ntween the Gaussian representations of the tags (method\nMV A). As the model described in Section 3.3 allocates two\n10-D mood values per track (method PEA), a weighted Eu-clidean measure was used which exploited the weighting\nvalues associated with each of the two 10-D mood values.\nThis is shown in Equation (5) where ms(i;k)is the mood\nof the seed track (where iis the value index, and kis the\ndimension index), mt(j;k)is the mood expressed by the\ntest track (where jis the value index), ws(i)is the seed\ntrack weighting, and wt(j)is the test track weighting.\nd=vuut1X\ni=01X\nj=09X\nk=0((ws(i)\u0001ms(i;k))\u0000(wt(j)\u0001mt(j;k)))2\n(5)\n4. LISTENING EXPERIMENT\n4.1 Corpus and Recommenders Tested\n5,000 production music tracks were picked up randomly\nfrom the ILM dataset according to two constraints: (i) the\ndurations of the tracks had to be at least 60 s (in order\nto discard short versions of the tracks), (ii) instrumental\nstems, i.e. individual tracks from multitrack recordings,\nwere discarded. Six main genres were represented (jazz,\ndance, rock, electronic, folk and orchestral). 18 differ-\nent recommenders were tested based on the two different\nmood models MDS and CLUST (Sections 3.1 and 3.3, re-\nspectively), the ACT transformation of the MDS model,\nMDS-ACT (Section 3.1.3), and a timbre-based model based\non 20 MFFCs. Model MDS was tested with three different\ndimensions (3, 5 and 11) and the ﬁve different tag sum-\nmarisation methods deﬁned in Section 3.2 (MTF, CEN,\nTFW, ITF, MV A). Models CLUST, MFCC and MDS-ACT\nwere tested with just one conﬁguration each. As the ACT\nmaintains the relative distances between mood terms in\nthe MDS space, it doesn’t affect track distances using the\nMTF, CEN, TFW, and ITF methods. The MDS-ACT model\nwas hence only tested with the MV A method.\n4.2 Procedure\nTo determine which mood model conﬁguration used in a\nrecommender gave the best matches according to human\nperception, a listening experiment was conducted. If a\nmood model is of any use it should ensure that a recom-\nmender generates tracks that closely match a seed track\naccording to mood.Figure 2 . Survey interface.\nThe requirements of the test were that it should be sim-\nple to perform, not require specialist software or equip-\nment, be accessible to enough people and not take too much\ntime. To achieve these aims, a simple web-page was de-\nveloped which made use of the audio tools in HTML5\n(see Figure 2), so anyone could access the survey on an\ninternet connection. The web-page presented the listener\nwith a randomly selected seed track (from the 5,000 track\ndataset), plus four more tracks for assessment. In order to\navoid potential causes of similarity between tracks due to\nthe fact that they belong to the same album, recommended\ntracks were restricted to belong to a different album than\nthat of the seed track. Of the four assessed tracks, one was\nfrom the recommender and the other three were randomly\nselected. The choice of recommender was also random.\nParticipants were required to listen to each track at least\nonce and select which one of the four tracks they felt most\nclosely matched the seed track. They could repeat the pro-\ncess as many times as they wished, though they were en-\ncouraged to do at least 10 minutes’ worth of testing.\n4.3 Statistical Analysis\nThe system counted how many times the participants se-\nlected the recommenders’ tracks. Therefore if a recom-\nmender generated recommendations no better than chance,\nthe baseline score would be 25%. To determine which\nmodel gave the best performance, the conﬁdence intervals\nfor the scores had to demonstrate whether recommenders’\nscores were signiﬁcantly different from each other. The\nscores(m)for a particular recommender mis shown in\nthe Equation 6, where r(m;n)is 1 for a correct selection\nof the recommender m, 0 for a false selection, and nis\nthe trial index. The number of times the recommender has\nbeen tested is N(m).\ns(m) =1\nN(m)N(m)X\nn=1r(m;n) (6)\nTo calculate the conﬁdence intervals, we decided to move\naway from the traditional parametric method which assumes\na Gaussian distribution, and use a non-parametric boot-\nstrapping method [4].\nFigure 3 . % of correct choice across recommenders. The\nrecommenders are labelled using abbreviations in the for-\nmat: ‘ModelDimensions TagSummarisationMethod’.\n5. RESULTS AND DISCUSSION\nThe simple web-based survey design meant that the sur-\nvey was easily accessible and simple to use, which allowed\nbetween 40-60 participants (estimated from email request\nlist and activity) to perform the survey. There were 971\nmarks given in total in the survey, which was enough to\ndetermine which recommenders we should avoid using,\nbut not enough to identify a clear leader. The scores for\neach model are shown in Figure 3, and are arranged in or-\nder with the best-scoring model at the top. The horizontal\nbars represent the conﬁdence interval, with the ﬁne ver-\ntical line representing the mean score. There is a line at\n25% which corresponds to the random baseline score, so\nany scores that overlap that line can be considered to be\nno better than chance (given the sample size). To com-\npare the performance of the different dimensions and of\nthe MDS model, the scores from the versions of the model\nwith the same dimensions were combined. To compare the\nmethods, the scores from the same method were combined.\nFigure 4 shows how 3, 5 and 11 dimensions compare, and\nhow the ﬁve tag summarisation methods perform. While\nthe number of results per recommender ranged from 29\nto 91 scores, the conﬁdence intervals still remained quite\nlarge for making comparisons. While it would have been\npreferable to sample more participants to reduce the con-\nﬁdence intervals, the length of the experience had to be\nconsidered. While the conﬁdence intervals do not neces-\nsarily allow a single recommender to be selected above\nall the others, they do still give us enough scope to elim-\ninate the worst recommenders. The results show that the\nseven worst recommenders’ conﬁdence intervals overlap\nthe 25% baseline score, although with some of those the\noverlap is small and a larger sample size could show theyare better than chance. For assessing the types of model,\nnumber of dimensions, method and distance measures, there\nwas little evidence to conclude that any of those factors in-\ndividually had a strong inﬂuence. However, the scores in-\ndicate the recommenders (for model MDS) with 5 dimen-\nsions could be stronger than 3 and 11 dimensions (though\nconﬁdence intervals overlap). The MTF (tag with maxi-\nmum term frequency) method yielded the smallest scores,\nwhich were signiﬁcantly smaller than those of the strongest\nmethod, TFW (term-frequency weighted centroid).\nThe only recommender which does not use metadata re-\nlies on the timbre-based similarity measurement (MFCC).\nThis recommender performed well, which can probably be\nexplained by the fact that within the 5,000 tracks in the\nsurvey, there were still enough tracks that sounded very\nsimilar to each other (despite a certain amount of manual\nﬁltering of the full track list), and which would therefore\nindicate a similar mood. In practice, with a recommender\nsearching over a million tracks, one that just returns very\nsimilar-sounding tracks may not ﬁt the requirement of a\nmood-based recommender offering a more diverse selec-\ntion.\nModel CLUST performed as well as the best combina-\ntions of models MDS and MFCC. However, it was only\ntested with a single conﬁguration, so it was not known\nwhether the 10-dimensional two-maxima method was the\nbest for it. It was also based on only one person’s opin-\nions of mood words, so could have beneﬁted from a more\nextensive multi-person survey to reﬁne the mood values.\nFigure 4 . Dimension and method scores of model MDS.\n6. SUMMARY AND CONCLUSIONS\nThe aim of the work was to design a mood model suit-\nable for a recommender that could work over a very large\ndatabase of music. It was felt that existing mood mod-\nels with two [12] or three dimensions [11] were both too\ngeneralised for music analysis and lacking in dimensions\nto discriminate over large quantities of music. The sur-\nvey conducted for this study assessed how various rec-\nommenders corresponded with human perception of mu-\nsic track matching based on perceived moods. The survey\ndemonstrated that higher dimension models are effectivein a recommender, and given the correct choice of sum-\nmarisation methods and distance measures, can be tuned\nfor better results. The results of the test showed that a ﬁve-\ndimensional model produced the best scores, although it\nwas not statistically the clear leader. This work used pro-\nduction music associated with manually generated mood\ntags, and thus provided a source of information on emo-\ntional responses to music without analysis of the audio\nsignal itself. Based on the results of the perceptual evalua-\ntion presented in this paper, a mood model (5-D MDS with\nterm-frequency weighted centroid) was selected to develop\nan audio-content-based music emotion recognition (MER)\nsystem. We will follow up this study by testing to what ex-\ntent this system can be used to assign mood tags in a robust\nmanner to commercial music tracks which do not possess\nmood metadata.\n7. ACKNOWLEDGEMENTS\nThis work was partly funded by the TSB project “Mak-\ning Musical Mood Metadata” (TS/J002283/1). The authors\nwould like to acknowledge the BBC Audio Research Part-\nnership from which this collaboration was started.\n8. REFERENCES\n[1] M. Barthet, G. Fazekas, and M. Sandler. Multidisciplinary perspec-\ntives on music emotion recognition: Recommendations for content-\nand context-based models. In Proc. of CMMR , pages 492–507, 2012.\n[2] M.M. Bradley and P.J. Lang. Affective norms for English words\n(ANEW): Instruction manual and affective ratings. Technical report\nc-2, University of Florida, Gainesville, FL, 2010.\n[3] W. R. Dillon and M. Goldstein. Multivariate Analysis . Wiley series\nin probability and mathematical statistics. John Wiley & Sons, New\nYork, 1984.\n[4] T. C. Hesterberg, D. S. Moore, S. Monaghan, A. Clipson, and R. Ep-\nstein. Bootstrap methods and permutation tests . 2005.\n[5] X. Hu. Music and mood: where theory and reality meet. In Proc. of\niConference , 2010.\n[6] P. Juslin and D. V ¨astf¨all. Emotional responses to music: The need\nto consider underlying mechanisms. Behavioral and brain sciences ,\n31:559–621, 2008.\n[7] J.B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt\nto a nonmetric hypothesis. Psychometrika , 29:1–27, 1964.\n[8] J. A. Lee and J. S. Downie. Survey of music information needs, uses,\nand seeking behaviors: preliminary ﬁndings. In Proc. of ISMIR , 2004.\n[9] M. Levy. Retrieval and Annotation of Music Using Latent Semantic\nModels . PhD thesis, Queen Mary University of London, 2012.\n[10] M. Levy and M. Sandler. A semantic space for music derived from\nsocial tags. In Proc. ISMIR , pages 411–416, 2007.\n[11] C. E. Osgood, G. J. Suci, and P. H. Tannenbaum. The Measurement\nof Meaning . University of Illinois Press, 1957.\n[12] J.A. Russell. A circumplex model of affect. Journal of personality\nand social psychology , 39(6):1161–1178, 1980.\n[13] P. Saari, M. Barthet, G. Fazekas, T. Eerola, and M. Sandler. Semantic\nmodels of musical mood: comparison between crowd-sourced and\ncurated editorial tags. In Proc. of IEEE ICME (Affective Analysis in\nMultimedia workshop) , San Jose, CA, 2013.\n[14] P. Saari and T. Eerola. Semantic computing of moods based on tags\nin social media of music. IEEE Transactions on Knowledge and\nData Engineering , manuscript submitted for publication available at\nhttp://arxiv.org/, 2013."
    },
    {
        "title": "Explicit Duration Hidden Markov Models for Multiple-Instrument Polyphonic Music Transcription.",
        "author": [
            "Emmanouil Benetos",
            "Tillman Weyde"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416088",
        "url": "https://doi.org/10.5281/zenodo.1416088",
        "ee": "https://zenodo.org/records/1416088/files/BenetosW13.pdf",
        "abstract": "In this paper, a method for multiple-instrument automatic music transcription is proposed that models the temporal evolution and duration of tones. The proposed model supports the use of spectral templates per pitch and instrument which correspond to sound states such as attack, sustain, and decay. Pitch-wise explicit duration hidden Markov models (EDHMMs) are integrated into a convolutive probabilistic framework for modelling the temporal evolution and duration of the sound states. A two-stage transcription procedure integrating note tracking information is performed in order to provide more robust pitch estimates. The proposed system is evaluated on multi-pitch detection and instrument assignment using various publicly available datasets. Results show that the proposed system outperforms a hidden Markov model-based transcription system using the same framework, as well as several state-of-theart automatic music transcription systems.",
        "zenodo_id": 1416088,
        "dblp_key": "conf/ismir/BenetosW13",
        "keywords": [
            "Automatic music transcription",
            "Multiple-instrument support",
            "Temporal evolution modeling",
            "Duration modeling",
            "Spectral templates",
            "Pitch-wise explicit duration hidden Markov models",
            "Convolutive probabilistic framework",
            "Note tracking information",
            "Multi-pitch detection",
            "Instrument assignment"
        ],
        "content": "EXPLICIT DURATIONHIDDENMARKOV MODELS FOR\nMULTIPLE-INSTRUMENT POLYPHONIC MUSICTRANSCRIPTION\nEmmanouilBenetos andTillman Weyde\nMusic Informatics ResearchGroup, Department of Computer Sci ence,City University London\n{emmanouil.benetos.1, t.e.weyde }@city.ac.uk\nABSTRACT\nIn this paper, a method for multiple-instrument automatic\nmusic transcription is proposed that models the temporal\nevolution and duration of tones. The proposed model sup-\nportstheuseofspectraltemplatesperpitchandinstrument\nwhich correspond to sound states such as attack, sustain,\nand decay. Pitch-wise explicit duration hidden Markov\nmodels(EDHMMs)areintegratedintoaconvolutiveprob-\nabilistic framework for modelling the temporal evolution\nand duration of the sound states. A two-stage transcrip-\ntionprocedureintegratingnotetrackinginformationispe r-\nformed in order to provide more robust pitch estimates.\nThe proposed system is evaluated on multi-pitch detection\nandinstrumentassignmentusingvariouspubliclyavailabl e\ndatasets. Results show that the proposed system outper-\nforms a hidden Markov model-based transcription system\nusing the same framework, as well as several state-of-the-\nart automatic music transcriptionsystems.\n1. INTRODUCTION\nAutomatic music transcription (AMT) is the process of\nconverting an acoustic musical signal into some form of\nmusic notation [13]. In the music information retrieval\nliterature, AMT typically involves the detection of mul-\ntiple concurrent pitches (multi-pitch detection), the est i-\nmation of note onsets and offsets (note tracking) and the\nestimation of instrument identities (instrument identiﬁc a-\ntion/assignment). It is generally considered to be an open\nproblem, especially for highly polyphonic music signals\nandmultipleinstruments. ForarecentreviewofAMTsys-\ntems, thereader isreferredto[12].\nA large part of AMT systems employ spectrogram fac-\ntorization methods for multi-pitch detection. These sys-\ntemsattempttodecomposeaninputtime-frequencyrepre-\nsentationasaseriesofspectralcomponentsandpitchacti-\nvations,usingavarietyofconstraints(regardingpolypho ny\nE. Benetos is supported by a City University London Research Fel-\nlowship.\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbear this noticeand thefull citation ontheﬁrst page.\nc/circlecopyrt2013International Society forMusic InformationRetrieval .level,instrumentidentities,spectralenvelopes,andtem po-\nral continuity among others); systems related to the pro-\nposed workwill bepresented below.\nIn [6], Dessein et al. propose an AMT system for pi-\nano music which uses non-negative matrix factorization\n(NMF) with beta-divergence and pre-extracted note tem-\nplates, which is able to transcribe pieces in real-time. Vin -\ncent et al. [16] propose a harmonic variant of NMF for\ndecomposing a spectrogram into a series of narrowband\nharmonic spectra, which are also smooth across frequency\n(also called the spectral smoothness assumption [13]). In\n[4], Carabias-Orti et al. propose a system for multi-pitch\ndetection and instrument identiﬁcation using NMF with\nsource-ﬁltermodelconstraints. GrindlayandEllis[11]ut i-\nlize a probabilistic variant of NMF called probabilistic la -\ntent component analysis (PLCA) for decomposing a spec-\ntrogram into a series of eigeninstrument templates, pitch\nactivations, and source contributions, and evaluate their\nmethod for multi-pitch detection and instrument assign-\nment. Yoshii and Goto [17] proposed a non-parametric\nmodel for music signal analysis which decomposes an in-\nput spectrogram as a series of source-ﬁlter templates de-\nrived from an autoregressive model. Finally, in [2] Bene-\ntosandDixonproposedavariantofconvolutivePLCAfor\nmodelling the evolution of notes using sound state tem-\nplates (such as attack, sustain, decay) with hidden Markov\nmodel-based constraints.\nInthispaper,weintegrateexplicitdurationhiddenMar-\nkovmodels(EDHMMs)[7,18]withinthespectrogramfac-\ntorizationframeworkof[2],inordertomodeltheduration\nof sound states within a note. Contrary to hidden Markov\nmodels (HMMs), where the state duration is (implicitly)\ngeometrically distributed, EDHMMs form a speciﬁc case\nof hidden semi-Markov models [18], where each state has\na variable duration. Alternatively, it can be viewed that\nan EDHMM can emit a sequence of observations instead\nof a single one. The additional information in EDHMMs\nis modelled through the use of a duration probability per\nstate. EDHMMs have been shown to overcome the limita-\ntions posed by HMMs regarding state durations and have\nbeensuccessfullyusedinavarietyofapplications(see[18 ]\nforareview).\nTheproposedmodelusespitch-wiseEDHMMsforcon-\nstraining the order of the sound states, while also support-\ningtheuse ofmultiple templates per pitchand instrument,\nandalsoshift-invarianceacrosslog-frequencyforsuppor t-\ning tuning changes and frequency modulations. In addi-AUDIOCQT MODEL POSTPROCESSINGOUTPUT\nTEMPLATES\nFigure 1. Proposed system diagram.\ntion,weproposeatwo-stagetranscriptionprocedureinor-\nder to eliminate spurious pitch detections. The proposed\nmodel is trained on note samples from the RWC database\n[10]andistestedusingrecordingsfromtheMAPSdatabase\n[8],theTRIOSdataset[9],andtheMIREXMultiF0record-\ning [1]. Multi-pitch detection and instrument assignment\nresults show that the proposed EDHMM-based system is\nable to model the durations of sound states and the overall\nevolution of notes, and its temporal constraints lead to im-\nproved performance compared to hidden Markov models\n(HMMs) used in the same framework. Finally, the pro-\nposedsystemoutperformsseveralAMTmethodsinthelit-\nerature forthe sameexperiments.\nTheoutlineofthispaperisasfollows. Section2presents\nthe proposed EDHMM-based transcription model, along\nwith the postprocessing steps. The datasets used for train-\ningandtesting,aswellastheevaluationmetricsandexper-\nimental results, are presented in Section 3. Finally, con-\nclusions are drawn and future directions are indicated in\nSection 4.\n2. PROPOSED METHOD\nIn this section, the proposed EDHMM-constrained auto-\nmatic transcription model is described, along with the up-\ndaterulesforestimatingthevariousmodelparametersand\nthestepsusedforpost-processing;theproposedsystemdi-\nagram can be seen inFig. 1.\n2.1 Model\nTheproposedmodelaimstoexpresstheevolutionofnotes\ninmultiple-instrumentpolyphonicmusicasasuccessionof\nsound state templates, further constrained by the ordering\nand the expected duration of each sound state. These tem-\nporal constraints areincorporated intoa model which sup-\nportsmultipletemplatesperpitchandinstrument,andalso\nsupports shift-invariance across log-frequency in order t o\nmodel tuning changes and frequency modulations. In or-\nder to achieve this, we integrate independent pitch-wise\nexplicit duration hidden Markov models (EDHMMs) [18]\ninto the HMM-constrained automatic transcription model\nof [2]. Thus, the proposed model can be called EDHMM-\nconstrained shift-invariant PLCA.\nMoreformally,thenormalizedmagnitudelog-frequency\nspectrogram Vω,t(ωdenotes log-frequency and tdenotes\ntime) which is used as input, is decomposed into a series\nof sound state spectral templates per instrument and pitch,\na time-varying pitch shifting parameter, a time-varying in -\nstrument contribution per pitch, a pitch activation, and ﬁ-nallyasoundstateactivationperpitch,whichiscontrolle d\nby its respective EDHMM. If we denote the collection of\nobservations for all time frames as ¯ω, the proposed model\nintermsof theobservations isgiven by:\nP(¯ω) =/summationdisplay\n¯q(1),···,¯q(P)/summationdisplay\n¯d(1),···,¯d(P)P(q(1)\n1)···P(q(P)\n1)\nP(d(1)\n1)···P(d(P)\n1)\n/parenleftbigg/productdisplay\ntP(q(1)\nt|q(1)\nt−1,d(1)\nt−1)P(d(1)\nt|q(1)\nt,d(1)\nt−1)/parenrightbigg\n···\n/parenleftbigg/productdisplay\ntP(q(P)\nt|q(P)\nt−1,d(P)\nt−1)P(d(P)\nt|q(P)\nt,d(P)\nt−1)/parenrightbigg\n/parenleftbigg/productdisplay\ntP(¯ωt|q(1)\nt,...,q(P)\nt)/parenrightbigg\n(1)\nwherep= 1,···,Pdenotes pitch, q(p)denotes the sound\nstate for the p-th pitch, d(p)denotes the duration distribu-\ntion for the p-th pitch, P(q(p)\n1)is the sound state prior for\nthep-th pitch, P(d(p)\n1)is the duration prior for the p-th\npitch,¯qis the sequence of draws of q,¯dis the sequence of\ndraws of d, and ﬁnally P(¯ωt|q(1)\nt,...,q(P)\nt)is the obser-\nvation probability for agiven observation ¯ωt.\nAn EDHMM has state transitions only at the end of a\nsegment, and its duration distributions generate segment\nlengths only at every stateswitch[7]:\nP(q(p)\nt+1|q(p)\nt,d(p)\nt) =/braceleftBigg\nδ(q(p)\nt+1,q(p)\nt), dt>1\nP(q(p)\nt+1|q(p)\nt),otherwise\nP(d(p)\nt+1|q(p)\nt+1,d(p)\nt) =/braceleftBigg\nδ(d(p)\nt+1,d(p)\nt−1), dt>1\nP(d(p)\nt+1|q(p)\nt+1),otherwise\nwhereP(q(p)\nt+1|q(p)\nt)isthepitch-wisesoundstatetransition\nmatrix, and P(d(p)\nt|q(p)\nt)is the pitch-wise sound state du-\nrationdistribution. Also, δ(x,y) = 1ifx=yand0other-\nwise.\nSince in the PLCA-based models Vω,trepresents the\nnumber of times ωhas been drawn at the t-th time frame,\ntheobservation probability iscalculated as:\nP(¯ωt|q(1)\nt,...,q(P)\nt) =/productdisplay\nωtPt(ωt|q(1)\nt,...,q(P)\nt)Vω,t\n(2)\nIn the proposed model, Pt(ωt|q(1)\nt,...,q(P)\nt)is decom-\nposed as:\nPt(ωt|q(1)\nt,...,q(P)\nt) =\n/summationdisplay\nst,pt,ftPt(pt)Pt(st|pt)P(ωt−ft|st,pt,q(pt)\nt)Pt(ft|pt)\n(3)\nwheresdenotestheinstrumentsource, fisthepitchshift-\ning parameter, Pt(pt)is the pitch activation, Pt(st|pt)is\nthe time-varying instrument contribution for each pitch,\nP(ω|s,p,q(p))are the sound state spectral templates persources, pitchp, and sound state q(p), andPt(ft|pt)is\nthe log-frequency shifting distribution per pitch over tim e.\nThe subscript tinft,ωt,st,ptdenotes the values of vari-\nablesf,ω,s,p taken at time t. The shifting parameter fis\nconstrainedtoasemitonerangearoundtheidealtuningpo-\nsitionofeachpitch. Sinceintheproposedsystemthetime-\nfrequency representation used is the constant-Q transform\n(CQT) with a log-frequency resolution of 60 bins/octave\nand a 40ms step [15], this implies that f∈[1,5]. We also\nset a maximum duration for each sound state: d∈[1,20],\nwhich means that the maximum duration of each sound\nstateis800ms.\n2.2 Parameter Estimation\nThe unknown model parameters of Section 2.1 can be es-\ntimated using the Expectation-Maximization (EM) algo-\nrithm [5]. For the E-step, the posterior for all hidden vari-\nables is:\nPt(ft,st,pt,q(1)\nt,...,q(P)\nt|¯ω) =\nPt(q(1)\nt,...,q(P)\nt|¯ω)Pt(ft,st,pt|ωt,q(1)\nt,...,q(P)\nt)(4)\nWe assume that the pitch-wise EDHMMs are indepen-\ndent,thusthejointprobabilityofallsoundstatesisdecom -\nposed as:\nPt(q(1)\nt,...,q(P)\nt|¯ω) =P/productdisplay\np=1Pt(q(p)\nt|¯ω)(5)\nwhere:\nPt(q(p)\nt|¯ω) =\n/summationtext\nτ<t/parenleftbig\nα∗(p)\nτ(q(p)\nτ+1)β∗(p)\nτ(q(p)\nτ+1)−α(p)\nτ(q(p)\nτ)β(p)\nτ(q(p)\nτ)/parenrightbig\n/summationtext\nq(p)\nt,τ<t/parenleftbig\nα∗(p)\nτ(q(p)\nτ+1)β∗(p)\nτ(q(p)\nτ+1)−α(p)\nτ(q(p)\nτ)β(p)\nτ(q(p)\nτ)/parenrightbig\n(6)\nwhereα∗\nτ(qτ+1),ατ(qτ)are the EDHMM forward vari-\nables and β∗\nτ(qτ+1),βτ(qτ)are the EDHMM backward\nvariables; all aforementioned forward-backward variable s\ncan be computed usingrecursive formulae[18].\nThe second term of (4) can be computed using Bayes’\ntheoremandthenotionthat P(ωt|ft,st,pt,q(pt)\nt) =P(ωt−\nft|st,pt,q(pt)\nt):\nPt(ft,st,pt|ωt,q(1)\nt,...,q(P)\nt) =Pt(ft,st,pt|ωt,q(pt)\nt) =\nPt(pt)P(ωt−ft|st,pt,q(pt)\nt)Pt(ft|pt)Pt(st|pt)\n/summationtext\npt,st,ftPt(pt)P(ωt−ft|st,pt,q(pt)\nt)Pt(ft|pt)Pt(st|pt)\n(7)\nFor the M-step, the update equations for the unknown\nparameters areas follows:\nPt(pt) =\n/summationtext\nωt,ft,st,q(1)\nt,···,q(P)\ntVω,tPt(ft,st,pt,q(1)\nt,...,q(P)\nt|¯ω)\n/summationtext\npt,ωt,ft,st,q(1)\nt,···,q(P)\ntVω,tPt(ft,st,pt,q(1)\nt,...,q(P)\nt|¯ω)\n(8)Pt(st|pt) =\n/summationtext\nωt,ft,q(1)\nt,···,q(P)\ntVω,tPt(ft,st,pt,q(1)\nt,...,q(P)\nt|¯ω)\n/summationtext\nst,ωt,ft,q(1)\nt,···,q(P)\ntVω,tPt(ft,st,pt,q(1)\nt,...,q(P)\nt|¯ω)\n(9)\nPt(ft|pt) =\n/summationtext\nωt,st,q(1)\nt,···,q(P)\ntVω,tPt(ft,st,pt,q(1)\nt,...,q(P)\nt|¯ω)\n/summationtext\nft,ωt,st,q(1)\nt,···,q(P)\ntVω,tPt(ft,st,pt,q(1)\nt,...,q(P)\nt|¯ω)\n(10)\nP(q(p)\n1) =P1(q(p)\n1|¯ω)\n/summationtext\nq(p)\n1P1(q(p)\n1|¯ω)(11)\nP(q(p)\nt+1|q(p)\nt) =/summationtext\ntα(p)\nt(q(p)\nt)P(q(p)\nt+1|q(p)\nt)β∗(p)\nt(q(p)\nt+1)\n/summationtext\nq(p)\nt+1,tα(p)\nt(q(p)\nt)P(q(p)\nt+1|q(p)\nt)β∗(p)\nt(q(p)\nt+1)\n(12)\nP(d(p)\nt|q(p)\nt) =\n/summationtext\ntα∗(p)\nt(q(p)\nt+1)P(d(p)\nt|q(p)\nt)βp\nt+d(q(p)\nt+d)/producttext\nτP(¯ωτ|q(p)\nτ)\n/summationtext\nd(p)\nt,tα∗(p)\nt(q(p)\nt+1)P(d(p)\nt|q(p)\nt)βp\nt+d(q(p)\nt+d)/producttext\nτP(¯ωτ|q(p)\nτ)\n(13)\nwhereτ=t+1,···,t+d.\nItshouldbenotedthatweconsiderthesoundstatetem-\nplates to be ﬁxed, so no update rule for P(ω|s,p,q(p))ex-\nisits. Using ﬁxed templates, 10-15 iterations using the up-\ndaterulespresentedinthepresentsectionaresufﬁcientfo r\nconvergence. Theoutputofthesystemisapitchactivation\nwhich isscaled by theenergy of thelog-spectrogram:\nPt(p)/summationdisplay\nωVω,t (14)\nInordertofurtherconstrainthemodelsothatitreaches\nmore meaningful solutions, sparsity is enforced in Pt(p)\nandPt(s|p), by modifying the update rules in (8) and (9),\nwhere a power greater than 1 is applied to the numerators\nand denominators, which leads to sharpened distributions,\nthusencouragingsparsity[2]. Eventhoughconvergenceis\nnot guaranteed, it is observed in practice. This procedure\nimpliesthatonlyfewpitchesneedtobeactiveateachtime\nframe, and also that for a note at a given time frame, only\nfew instruments areresponsible for producing it.\nAs an example of the learned EDHMM parameters us-\ningtheproposedsystem,Fig. 2showsthelearnedduration\ndistributions and sound state transitions for a D4 note, us-\ningapianorecordingasinputtothesystem. Itcanbeseen\nthatthedurationdistributionforthe1stsoundstate(whic h\ncorrespondstoanattackstate)favorsshortdurations,whi le\nthe duration distribution for the 2nd state (which corre-\nsponds to the steady state) favors much longer durations.\nAlso, the resulting transition matrix also shows the linear\nsuccession between thesound states.qtqt+1(c)\nωq(b)\ndq(a)\n1 2 3 100 200 300 400 500 5 10 15 201\n2\n3\n123\n1\n2\n3\nFigure 2. EDHMM parameters learned for note D4 using the ‘MAPS MUS-alb se2ENSTDkCl’ piece from the MAPS\ndatabase. (a) The duration distribution matrix P(d|q). (b) Pre-computed piano sound state templates for note D4. ( c) The\nsound statetransitionmatrix P(qt+1|qt).\n2.3 Post-processing\nSincetheresultingpitchactivationfrom(14)isnon-binar y,\na postprocessing procedure needs to take place in order to\nconvert it to a binary piano-roll or a MIDI-like representa-\ntion (this procedure is also called note tracking). As in the\nvastmajorityofspectrogramfactorization-basedautomat ic\ntranscription systems (e.g. [6,11]), we perform threshold -\ning on the pitch activation, followed by a process for re-\nmoving note events with a duration less than 80ms. We\nshould note that the HMM-based postprocessing method\nof [2] was found not to perform well on pieces with fast\ntempo and rapid note changes. An example of the output\nof the post-processing step compared with a ground-truth\ntranscription is given in Fig. 3, using a segment from a\npiano sonata.\nAsystemvariantisalsoproposed,whereafterdetecting\nall active pitches in the ﬁnal piano-roll, the update rules o f\nsubsection 2.2, are run again, but instead of setting pas to\ncover the entire pitch range, we only use the list of active\npitches estimated in the ﬁrst run. This two-stage process\nalsohelpsinfurtherconstrainingthesolutionbyremoving\nanypitchesthatmightappearin Pt(p)butarenevertheless\nremoved inthepostprocessing step.\n3. EVALUATION\n3.1 Training Data\nSound state templates are extracted for several orchestral\ninstruments, using isolated note samples from the RWC\ndatabase [10]. Speciﬁcally, we extract templates for bas-\nsoon, cello, clarinet, ﬂute, guitar, harpsichord, oboe, or -\ngan, piano,tenorsax,andviolin,usingtheCQTasatime-\nfrequencyrepresentation[15]. Thecompletenoterangeof\nthe instruments is used, given the available training data.\nThesoundstatetemplatesarecomputedinanunsupervised\nmanner, using a single-pitch and single-instrument varian t\nof the model of (3), where the number of sound states is\nset toQ(p)= 3.\np(MIDI scale)\nt(sec)(b)p(MIDI scale)(a)\n5 10 15 20 25 305 10 15 20 25 30\n3040506070809030405060708090\nFigure3. (a)Theground-truthpiano-rolloftheﬁrst30sec\nof W.A. Mozart’s Piano Sonata K.333, 2nd movement\n(from the MAPS database). (b) The piano-roll computed\nfromthe proposed transcriptionsystem.\n3.2 Test Data\nFortesting,weuserecordingsfromthreepubliclyavailabl e\ntranscription datasets. Firstly, we used thirty 30sec pian o\nsegments from the MAPS database [8], speciﬁcally from\nthe ‘ENSTDkCl’ subset that has been used in the past for\nmulti-pitchevaluation in[4,17].\nWe also utilized the woodwind quintet recording used\nasadevelopmentsetintheMIREXmultiF0andnotetrack-\ningtask[1]. Instrumentspresentincludebassoon,clarine t,\nﬂute,horn,andoboe,whilemanually-alignedgroundtruth\nforeach instrument track isavailable online [1].\nFinally, we used the TRIOS dataset [9], which includes\nﬁve multitrack recordings of trio pieces of classical and\njazz music. For the current experiments, we used the ex-\nisting mixes of the multitracks. Instruments included in\nthe dataset are: bassoon, cello, clarinet, horn, piano, sax -\nophone, trumpet, viola, and violin. The dataset includes\nmanually-aligned ground truth with instrument informa-\ntionperpitch. Totheauthors’knowledge, notranscription\nresultshave been reported for theTRIOS dataset.3.3 Metrics\nFor evaluating the performance of the proposed system\nfor multi-pitch detection, we employ two sets of metrics:\nframe-based and note-based ones. For note-based evalu-\nation, we use the onset-based transcription metrics which\nare used in the MIREX note tracking task [1]. A detected\nnote is considered correct if its pitch matches a ground\ntruth pitch and its onset is within a 50ms tolerance of a\nground-truthonset. Theresultingnote-basedprecision,r e-\ncall, and F-measure are deﬁned as:\nPren=Ntp\nNsysRecn=Ntp\nNrefFn=2RecnPren\nRecn+Pren\n(15)\nwhereNtpisthenumberofcorrectlydetectedpitches, Nsys\nis the number of pitches detected by the system, and Nref\nisthe number of reference pitches.\nFor the frame-based metrics, evaluations are performed\nin a 10ms step as in the MIREX multiF0 evaluations [1],\nandweusetheframe-basedprecision,recall,andF-measure ,\nwhich are deﬁned in a similar way to (15) and are denoted\nasPref,Recf,andFf, respectively.\n3.4 Results\nExperiments are performed using the proposed system of\nSection 2 using two variants; a one-stage version (using\nthe update rules and the note tracking step) and the two-\nstage version presented in subsection 2.3. The proposed\nEDHMM-basedsystemiscomparedwiththeHMM-based\nsystemof[2]usingthesametime-frequencyrepresentation\nand note tracking steps. In all cases, the Markov models\nwere initialized as ergodic, with uniform priors and state\ntransitionprobabilities.\nInTable1,multi-pitchdetectionresultsusingtheMAPS\nrecordingsareshown. Itcanbeseenthatusingbothsetsof\nmetrics,theEDHMM-basedsystemsoutperformtheHMM-\nbasedone. Itcanalsobeseenthatthetwo-stageversionof\nthe system makes a signiﬁcant improvement in terms of\nperformance. The differences in performance are not as\nclear using the frame-based metrics, but they are still evi-\ndent. Inallcases,theprecisionishighercomparedtorecal l\n(e.g. for the two-stage EDHMM case, Pren= 74.73%\nandRecn= 64.46%), which signiﬁes that there is a larger\nnumber of missed detections compared to the number of\nfalse alarms. When comparing the reported results with\nother methods using the same dataset, it can be seen that\ntheproposedsystemoutperformsboththeinﬁnitecompos-\nite autoregressive system of [17] (which reported Ff=\n48.4%) and the source-ﬁlter NMF model of [4], which\nreported Ff= 52.4%(where the best reported perfor-\nmancein[4]wasreportedusingtheusingtheSONICalgo-\nrithm[14],reaching Ff= 58.0%). Finally,thenote-based\naccuracymeasurefortheMAPSrecordingsis53.42%;the\naccuracy reported in [3] for the MAPS-Disklavier dataset\nwas 68.7%, although it should be stressed that in [3] the\ndataset was alsoused fortraining thesystem.\nResults using the MIREX woodwind quintet recording\nareshowninTable1;againitcanbeseenthattheEDHMM-Method/Instrument HMM-based EDHMM-based\nBassoon 47.72% 41.42%\nClarinet 64.33% 67.68%\nFlute 51.18% 57.53%\nHorn 39.86% 44.59%\nOboe 23.84% 22.17%\nMean 45.39% 46.68%\nTable2. Instrumentassignmentresults( Ff)usingtheﬁrst\n30sec of theMIREX MultiF0recording.\nbased system outperforms the HMM-based one. In the lit-\nerature, an experiment using the ﬁrst 30sec of the MIREX\nrecording was made in [16], where Ff= 62.5%. Using\nthe ﬁrst 30sec in the 2-stage EDHMM system, the frame-\nbased F-measure reaches 66.95%.\nAlso, using the TRIOS dataset, similar results are re-\nported,ascanbeseeninTable1. Itshouldbenotedthough\nthatthereisalargedifferencebetweenthenote-basedmet-\nrics and the frame-based metrics, which can be attributed\ntothefactthattheTRIOSdatasetcontainsnoteswithlong\ndurations, which get oversegmented in the proposed sys-\ntem(wheresmallgapsdonotsigniﬁcantlyaffecttheframe-\nbased metrics).\nFinally, we perform experiments on instrument identi-\nﬁcation using information from matrix Pt(s|p). In the in-\nstrument assignment task [11], a detected pitch is consid-\nered to be correct if, in addition to pitch and timing con-\nstraints, it is assigned to a correct instrument source. We\nperformedexperimentsusingtheMIREXwoodwindquin-\ntet, using a system variant which utilizes templates found\nin the recording (bassoon, clarinet, ﬂute, horn, oboe). The\noutput(forinstrument s)isgivenby Pt(p)Pt(s|p)/summationtext\nωVω,t.\nFor comparative purposes, we evaluated the ﬁrst 30sec of\nthe MIREX recording, as in [4], using the frame-based F-\nmeasure. InstrumentassignmentresultsareshowninTable\n2, where it can be seen that the proposed EDHMM-based\nmethodperformsbettercomparedtotheHMM-basedone.\nItcanbeseenthatthebestperformanceisreportedforclar-\ninet, which has a relatively different spectral shape com-\npared to the other instruments. It should be noted though\nthat the HMM-based method performs better for bassoon\nandoboe,whiletheEDHMM-basedmethodperformsbet-\nter for clarinet, ﬂute, and horn. The reported Fffor the\nmethod in [4] is 37.0%, which indicates that the proposed\nmethod (which uses pre-extracted spectral templates in-\nstead of source-ﬁlter models within a spectrogram factor-\nization framework) ismore appropriate forthe task.\n4. CONCLUSIONS\nIn this paper, we proposed a model for automatic music\ntranscriptionwhichmodelsthetemporalevolutionofnotes\nusing pitch-wise explicit duration hidden Markov models,\nwithin a spectrogram factorization framework supporting\nmultiple pitch and instrument templates, as well as shift-\ninvarianceacrosslog-frequency. Itwasshownthatthetem-Dataset MAPS ‘ENSTDkCl’ MIREX TRIOS\nMethod /Metric Fn Ff FnFf FnFf\nHMM-based 65.93% 66.41% 63.64% 66.01% 55.94% 67.76%\nEDHMM-based 67.12% 66.82% 65.14% 66.42% 56.95% 69.54%\nEDHMM-based (2-stage) 68.61% 67.99% 66.60% 66.98% 57.66% 71.17%\nTable 1. Multi-pitchdetection results(in FnandFf)using thethree employed datasets.\nporal constraints posed by the EDHMMs resulted in im-\nproved multi-pitch detection and instrument identiﬁcatio n\nperformance when compared to HMM-based constraints.\nEvaluationresultsoutperformedstate-of-the-artmulti- pitch\ndetection methods using the MAPS and MIREX datasets.\nFinally,aproposedtwo-stagetranscriptionprocedurehel ps\ninfurther eliminating transcriptionerrors.\nOne of the main drawbacks of the proposed method\nis its computational complexity. Even with independent\nEDHMMs, the proposed method performs about 60 ×real-\ntime, which is prohibitive for large-scale experiments or\nreal-timeapplications. Inthefuture,wewillattempttocr e-\natecomputationally-efﬁcientversionsoftheproposedsys -\ntem using more compact time-frequency representations\nand by replacing the expensive expectation-maximization\nalgorithm with variational Bayesian methods. Finally, we\nwill expand the existing spectrogram factorization frame-\nwork in order to introduce additional constraints via musi-\ncologicalmodels,forexampleintegratinginformationfro m\nchord and key detection for improving multi-pitch detec-\ntionperformance.\n5. REFERENCES\n[1] Music Information Retrieval Evaluation eX-\nchange (MIREX). http://music-ir.org/\nmirexwiki/ .\n[2] E. Benetos and S. Dixon. Multiple-instrument poly-\nphonic music transcription using a temporally-\nconstrainedshift-invariantmodel. J.AcousticalSociety\nof America , 133(3):1727–1741, March 2013.\n[3] S. B¨ock and M. Schedl. Polyphonic piano note tran-\nscription with recurrent neural networks. In IEEE Int.\nConf. Audio, Speech and Signal Processing , pages\n121–124, March2012.\n[4] J. J. Carabias-Orti, T. Virtanen, P. Vera-Candeas,\nN. Ruiz-Reyes, and F. J. Ca ˜nadas-Quesada. Musi-\ncal instrument sound multi-excitation model for non-\nnegative spectrogram factorization. IEEE J. Selected\nTopics in Signal Processing , 5(6):1144–1158, October\n2011.\n[5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-\nimum likelihood from incomplete data via the EM al-\ngorithm.J.RoyalStatisticalSociety ,39(1):1–38,1977.\n[6] A.Dessein,A.Cont,andG.Lemaitre. Real-timepoly-\nphonic music transcription with non-negative matrix\nfactorization and beta-divergence. In 11th Int. SocietyforMusicInformationRetrievalConf. ,pages489–494,\nAugust 2010.\n[7] M. Dewar, C. Wiggins, and F. Wood. Inference in hid-\nden Markov models with explicit state duration distri-\nbutions.IEEE Signal Processing Letters , 19(4):235–\n238, 2012.\n[8] V.Emiya,R.Badeau,andB.David.Multipitchestima-\ntion of piano sounds using a new probabilistic spectral\nsmoothnessprinciple. IEEETrans.Audio,Speech, and\nLanhuageProcessing ,18(6):1643–1654,August2010.\n[9] J. Fritsch. High quality musical audio source separa-\ntion.Master’sthesis,UPMC/IRCAM/Tel ´ecomParis-\nTech, 2012.\n[10] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWCmusicdatabase: musicgenredatabaseandmusi-\ncal instrument sound database. In Int. Conf. Music In-\nformation Retrieval , October 2003.\n[11] G. Grindlay and D. Ellis. Transcribing multi-\ninstrument polyphonic music with hierarchical\neigeninstruments. IEEE J. Selected Topics in Signal\nProcessing , 5(6):1159–1169, October 2011.\n[12] P. Grosche, B. Schuller, M. M ¨uller, and G. Rigoll. Au-\ntomatic transcription of recorded music. Acta Acustica\nunited withAcustica , 98(2):199–215, March 2012.\n[13] A. Klapuri and M. Davy, editors. Signal Process-\ningMethodsforMusicTranscription .Springer-Verlag,\nNew York, 2006.\n[14] M.Marolt.Aconnectionistapproachtoautomatictran-\nscriptionofpolyphonicpianomusic. IEEETrans.Mul-\ntimedia, 6(3):439–449, June 2004.\n[15] C. Sch ¨orkhuber and A. Klapuri. Constant-Q transform\ntoolbox for music processing. In 7th Sound and Music\nComputing Conf. , Barcelona, Spain, July2010.\n[16] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch es-\ntimation. IEEE Trans. Audio, Speech, and Language\nProcessing , 18(3):528–537, March 2010.\n[17] K. Yoshii and M. Goto. Inﬁnite composite autoregres-\nsive models for music signal analysis. In 13th Int. So-\ncietyforMusicInformationRetrievalConf. ,pages79–\n84, October 2012.\n[18] S.-Z.Yu.Hiddensemi-Markovmodels. ArtiﬁcialIntel-\nligence, 174(2):215–243, 2010."
    },
    {
        "title": "Local Group Delay Based Vibrato and Tremolo Suppression for Onset Detection.",
        "author": [
            "Sebastian Böck",
            "Gerhard Widmer"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416460",
        "url": "https://doi.org/10.5281/zenodo.1416460",
        "ee": "https://zenodo.org/records/1416460/files/BockW13.pdf",
        "abstract": "In this paper we present a new vibrato and tremolo suppression technique for onset detection. It weights the differences of the magnitude spectrogram used for the calculation of the spectral flux onset detection function on the basis of the local group delay information. With this weighting technique applied, the onset detection function is able to reliably distinguish between genuine onsets and spectral energy peaks originating from vibrato or tremolo present in the signal and lowers the number of false positive detections considerably. Especially in cases of music with numerous vibratos and tremolos (e.g. opera singing or string performances) the number of false positive detections can be reduced by up to 50% without missing any additional events. Performance is evaluated and compared to current state-of-the-art algorithms using three different datasets comprising mixed audio material (25,927 onsets), violin recordings (7,677 onsets) and solo voice recordings of operas (1,448 onsets).",
        "zenodo_id": 1416460,
        "dblp_key": "conf/ismir/BockW13",
        "keywords": [
            "vibrato",
            "tremolo",
            "onset detection",
            "magnitude spectrogram",
            "spectral flux",
            "group delay information",
            "onset detection function",
            "false positive detections",
            "music",
            "operatic singing"
        ],
        "content": "LOCAL GROUP DELAY BASED VIBRATO AND TREMOLO\nSUPPRESSION FOR ONSET DETECTION\nSebastian B ¨ock and Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University, Linz, Austria\nsebastian.boeck@jku.at\nABSTRACT\nIn this paper we present a new vibrato and tremolo sup-\npression technique for onset detection. It weights the dif-\nferences of the magnitude spectrogram used for the cal-\nculation of the spectral ﬂux onset detection function on\nthe basis of the local group delay information. With this\nweighting technique applied, the onset detection function\nis able to reliably distinguish between genuine onsets and\nspectral energy peaks originating from vibrato or tremolo\npresent in the signal and lowers the number of false posi-\ntive detections considerably. Especially in cases of music\nwith numerous vibratos and tremolos (e.g. opera singing\nor string performances) the number of false positive detec-\ntions can be reduced by up to 50% without missing any\nadditional events. Performance is evaluated and compared\nto current state-of-the-art algorithms using three different\ndatasets comprising mixed audio material (25,927 onsets),\nviolin recordings (7,677 onsets) and solo voice recordings\nof operas (1,448 onsets).\n1. INTRODUCTION AND RELATED WORK\nOnset detection is the process of ﬁnding the starting points\nof all musically relevant events in an audio performance.\nWhile the detection of percussive onsets can be considered\na solved problem,1softer onsets, vibrato and tremolo are\nstill a major challenge for existing algorithms.\nSoft onsets (e.g. bowed string or woodwind instruments)\nhave a long attack phase with a slow rise in energy, thus en-\nergy or magnitude-based approaches are not the best ﬁt to\ndetect these sort of onsets. In the past, special algorithms\nhave been proposed to solve the problem of soft onsets by\nincorporating (additionally) phase [3, 4, 10] or pitch infor-\nmation [9, 14, 15] or a combination thereof [12] to over-\ncome the shortcomings of energy or magnitude-based on-\nset detection algorithms. However, advances in magnitude-\nbased methods [6] show that these methods are now on par\n1F-measure values >0:95as obtained with state-of-the-art onset de-\ntection algorithms [1] can be considered to have solved the problem.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.with the before-mentioned methods but outperform them\non all sorts of percussive audio material.\nThe current state-of-the-art methods for online [5] and\nofﬂine [11] onset detection are based on a probabilistic\nmodel and incorporate a recurrent neural network with the\nspectral magnitude and its ﬁrst time derivative as input fea-\ntures. Especially the ofﬂine variant OnsetDetector shows\nsuperior performance on all sorts of signals [1]. Because\nof its bidirectional architecture, it is able to model the con-\ntext of an onset to both detect barely dicernible onsets in\ncomplex mixes and suppress events which are erroneously\nconsidered as onsets by other algorithms.\nVibrato is an artistic effect commonly used in classi-\ncal music and can be sung or played by (mostly) string\ninstruments. It reﬂects a periodic change of the played or\nsung frequency of the note. Vibrato is technically char-\nacterized by the amount of pitch variation (e.g. \u0006a semi-\ntone for string instruments and up to a complete tone in\noperas) and the frequency with which the pitch changes\nover time (e.g. 6 Hz). It is sometimes used synonymously\nas a combination with another effect: the tremolo, which\ndescribes the changes in volume of the note. Because it is\ntechnically hard for a human musician to play pure vibratos\nor tremolos, usually both effects are performed simultane-\nously. The resulting ﬂuctuations in loudness and frequency\nmake it very difﬁcult for onset detection algorithms to dis-\ntinguish correctly between new note onsets and an intended\nvariation of the note.\nSo far only a few publications have addressed the prob-\nlem of spuriously detected onsets music containing vibrato\nand tremolo. Collins [9] uses a vibrato suppression stage\nin his pitch-based onset detection method, which ﬁrst iden-\ntiﬁes vibrato regions that ﬂuctuate at most one semitone\naround the center frequency and collects the extrema in a\nlist. The region is expanded gradually in time to cover the\nwhole duration of the vibrato. After having identiﬁed the\ncomplete extent of the vibrato, all values within this win-\ndow are replaced by the mean of the extrema list. The onset\ndetection function is based on the concept of stable pitches\nand uses the change in pitches as cues for new onsets.\nSchleusing et al. [14] deploy a system based on the\ninverse correlation of Nconsecutive spectral frames cen-\ntered around the current location. Regions of stable pitch\nlead to low inverse correlation values, and pitch changes\nresult in peaks in the detection function. To suppress vi-\nbrato they deploy a warp compensation which cancels outsmall pitch changes within the considered window, leaving\ngenuine onsets mostly untouched.\nRecent research [7] applies a maximum ﬁlter to sup-\npress vibrato in audio signals. This method operates in\nthe spectral domain; speciﬁcally it only considers the mag-\nnitude spectrogram without incorporating any phase infor-\nmation. Like the common spectral ﬂux algorithm [13] it re-\nlies on the detection of positive changes in the energy over\ntime, but instead of calculating the difference between the\nsame frequency bin for the current and previous frames,\nit includes a special magnitude trajectory tracking stage\nwhich is able to suppress spurious positive energy frag-\nments.\nStill, all algorithms (apart from those relying solely on\nphase information) suffer from loudness variations, which\nmostly originate from the tremolo effect. This paper ad-\ndresses this problem by incorporating the phase – more\nspeciﬁcally the local group delay (LGD) information – to\ndetermine steady tones and suppress the spurious loudness\nvariations accordingly.\n2. PROPOSED METHOD\nIncorporating phase information is only feasible if each\nfrequency bin of the spectrogram is considered separately\nas in the methods described in [3, 4, 10]. However, these\nmethods have proven to perform poorly compared to cur-\nrent state-of-the-art algorithms [6]. Thus, our method is\nbased on the recently proposed SuperFlux [7] algorithm,\nwhich is an enhanced version on the common spectral ﬂux\nalgorithm [13]. It is already signiﬁcantly less sensitive to\nfrequency variations caused by vibrato, but adding a spe-\ncial local group delay based weighting technique to the dif-\nference calculation step, makes this method even more ro-\nbust against loudness variations of steady tones, e.g., those\ncaused by tremolo.\n2.1 SuperFlux\nThe system performs a frame-wise processing of the audio\nsignal (sample rate 44,1 kHz). The signal is divided into\noverlapping chunks of length N= 2048 samples and each\nframe is weighted with a Hann window of the same length\nbefore being transformed to the spectral domain via the\ndiscrete Fourier transform (DFT). Two adjacent frames are\nlocated 220.5 samples apart, resulting in a resolution of\n200 frames per second, which allows reportin of onsets to\nwithin 5 ms.\nIt has been found advantageous [6] to ﬁrst ﬁlter the re-\nsulting magnitude spectrogram jX(n;k)j(ndenotes the\nframe number and kthe frequency bin index) with a ﬁl-\nterbankF(k;m)(withmbeing the ﬁlter band number)\nbefore being processed further. The ﬁlterbank has M=\n138 ﬁlters aligned equally on the logarithmic frequency\nscale with quarter-tone spacing. To better match the human\nperception of loudness, the resulting ﬁltered spectrogram\nXF(n;m)is then transferred to a logarithmic magnitude\nscale, denoted XL;F(n;m)hereafter. Instead of calculat-\ning the bin-wise difference to the previous frame of the\nsame logarithmic ﬁltered spectrogram, a maximum ﬁlteralong the frequency axis is applied (i.e. the value of a bin\nis set to the maximum of the same bin and its direct neigh-\nbors on the frequency axis) and the difference is calculated\nwith respect to the \u0016-th previous frame of this maximum\nﬁltered spectrogram Xmax\nL;F(n;m)resulting in the follow-\ning equation for the difference calculation stage:\nD(n;m) =XL;F(n;m)\u0000Xmax\nL;F(n\u0000\u0016;m)(1)\nThe parameter \u0016depends on the frame rate fr, which is\nset to 200fps, resulting in \u0016= 2frames. The SuperFlux\nonset detection function is then deﬁned as the sum of all\npositive differences:\nSF(n) =m=MX\nm=1H(D(n;m)) (2)\nwithH(x) =x+jxj\n2being the half-wave rectiﬁer function.\nThe positive effect of these measures can be seen clearly\nin Figures 1a to 1c, which depict a 4 second recording of a\nviolin played with vibrato and tremolo. However, there are\nstill some spurious positive energy fragments left, which\ncan be eliminated with the approach described in the next\nsection. For a more detailed description of the SuperFlux\nalgorithm, please refer to [7].\n2.2 Local Group Delay based difference weighting\nUsing solely the magnitude information of the spectrogram\nenables onset detection algorithms to detect most onsets\nreliably, but also makes them susceptible to all kinds of\nloudness variations of steady tones. Using the phase as an\nadditional source of information helps to lower the impact\nof these loudness variations. However, the main problem\nof incorporating the phase information is that it can only\nbe combined easily with the magnitude spectrogram if all\nfrequency bins of the STFT are considered individually.\nBut since ﬁltering the magnitude spectrogram with a ﬁlter-\nbank (i.e. merging several frequency bins into a single one)\nprevious to the difference calculation yields much better\nperformance for almost all kinds of audio signals [6], the\nphase information of constituent frequency bins of a ﬁlter\nband have to be combined such that phase can be used in\nconjunction with the ﬁltered spectrogram.\nWe investigated different approaches for combining the\nphase information of several frequency bands into one, and\npropose the following simple but effective solution. Given\nthe phase\u001eof the complex spectrogram Xby:\n\u001e(n;k) =angle (X(n;k)); (3)\nwe can estimate the local group delay (LGD) of the spec-\ntrogram as:\nLGD (n;k) =\u001e\u0003(n;k)\u0000\u001e\u0003(n;k\u00001); (4)\nwith\u001e\u0003deﬁned as the 2\u0019-unwrapped (over the frequency\naxis) phase. The local group delay gives information as\nwhere the gravitational centre of the magnitude is located.\nThe spectrogram reassignment method [2] uses this infor-\nmation to gather a sharpened (reassigned) representation0 100 200 300 400\nt [frames]020406080100120f [bins](a) magnitude spectrogram\n0 100 200 300 400\nt [frames]020406080100120f [bins]\n(b) classical bin-wise positive difference\n0 100 200 300 400\nt [frames]020406080100120f [bins]\n(c) maximum ﬁltering trajectory tracking based difference\n0 100 200 300 400\nt [frames]020406080100120f [bins]\n(d) local group delay based difference weighting\nFigure 1 :(a) logarithmic magnitude spectrogram of a 5s\nviolin played with vibrato and tremolo, (b) the positive dif-\nferences calculated as in the spectral ﬂux algorithm, (c)\nwith applied maximum ﬁltering as in [7] and (d) the pro-\nposed local group delay based difference weighting ap-\nproach .of the magnitude spectrogram. Although this representa-\ntion is more exact, the process leads to areas with lower\nmagnitudes. The reassigned spectrogram looks a bit like a\n“scattered” version of the well known magnitude spectro-\ngram. Thus, using this representation directly to calculate\nthe spectral ﬂux showed worse performance, mostly be-\ncause of lots of smaller energy peaks, which we are trying\nto avoid.\nInstead of using the local group delay information to re-\nlocate the magnitudes of the spectrogram, the information\ncan be interpreted in a different way: regions with values\nclose to zero indicate stable tones (or percussive sounds if\nthey are aligned along the frequency axis) and regions with\nabsolute values greater than zero indicate a possible onset.\nHolzapfel et al. [12] use the average of all local group de-\nlay values along the frequency axis as a feature for their\nonset detection function. Instead of averaging the individ-\nual values, we determine the local minimum within each\nband of the ﬁlterbank F(k;m)for the SuperFlux calcula-\ntion, and use these values as a weighting function.\nCare has to be taken that the individual ﬁlters of the ﬁl-\nterbank do not cover too many frequency bins, as the like-\nlihood that there is a local group delay minimum that does\nnot belong to any steady tone increases accordingly. Fil-\nterbanks with 24 ﬁlters per octave yielded good results for\nall kinds of music material. The higher the expected ﬂuc-\ntuations in frequency, the lower should be the chosen num-\nber of ﬁlter bands. However, the fewer ﬁlter bands used,\nthe wider the individual ﬁlter bands become, and in turn,\nthis impacts the performance on percussive onsets. Percus-\nsive onsets have low local group delay values over a broad\nrange of the frequency axis, thus applying the local min-\nimum as a weighting would “erase” almost all percussive\nonsets.\nTo lower the impact of local group delay weighting on\npercussive sounds, we ﬁrst apply a maximum ﬁlter over\ntime which covers the range of 15 ms. For a frame rate\noffr= 200 fps, this equals to three frames and results in\na temporal maximum ﬁltered version of the LGD spectro-\ngram:\nLGD\u0003(n;k) = max (jLGD (n\u00001 :n+ 1;k)j)(5)\nAfter this ﬁrst ﬁltering step, we get the ﬁnal local group\ndelay based weighting by applying the previously described\nminimum ﬁlter, which sets the value of a bin to the local\nminimum of the region deﬁned by the ﬁlter band:\nW(n;m) = min\u0000\nLGD\u0003(n;kL(m):kU(m))\u0001\n(6)\nwithkL(m)representing the lower frequency bin index of\nthe ﬁlter band mof the ﬁlterbank F(k;m), andkU(m)the\nupper bound respectively. This function is then used to\nweight the difference of the SuperFlux (cf. Equation 1), re-\nsulting in the modiﬁed detection function:\nSF\u0003(n) =m=MX\nm=1H(D(n;m))\u0001W(n;m) (7)withH(x) =x+jxj\n2being the half-wave rectiﬁer function,\nnthe frame number and mthe frequency bin index. The ‘ \u0001’\noperator denotes the element wise multiplication of the two\nmatrices.\nThe effect of all proposed measures can be seen in Fig-\nure 1. Compared to the standard spectral ﬂux implemen-\ntation (1b), the difference with applied maximum ﬁltering\ntrajectory tracking (1c) already shows fewer positive en-\nergy components, which are further reduced by the pro-\nposed method, as can be seen in (1d). Figure 2 shows the\nsums of the positive differences. It is evident that the new\napproach lowers the overall noise in regions with vibrato\nand tremolo but keeps very sharp peaks at the onset posi-\ntions.\n0 100 200 300 400 500\nt [frames]0.00.20.40.60.81.0normalized onset detection function\nFigure 2 :Spectral ﬂux sum of the differences shown in\nFigure 1. The simple ﬁltered spectral ﬂux is shown as dot-\nted line, the SuperFlux as dashed line, and the proposed\nlocal group delay based difference weighting approach as\nsolid line.\nIt should be mentioned that the same weighting tech-\nnique could be used for unﬁltered magnitude spectrograms\n(i.e. the original spectral ﬂux implementation). Instead of\nusing the local maximum of all frequencies of a ﬁlter band,\nonly the same frequency bin and its direct neighbors should\nbe considered. Although the same positive impact on sig-\nnals containing vibrato and tremolo can be observed, the\noverall performance compared to the ﬁltered variants of\nthe spectral ﬂux (e.g. the LogFiltSpecFlux [6] or the Super-\nFlux [7]) is much lower, especially for polyphonic music.\n2.3 Peak-picking\nFor selecting the ﬁnal onsets of the weighted SuperFlux de-\ntection function we use the same peak-picking method as\nin [7]. Since the new onset detection function SF\u0003(n)has\na lower noise ﬂoor and shows sharper peaks than the orig-\ninal implementation (Equation 2), we had to alter the pa-\nrameters for the peak-picking method used in [7]. A frame\nnof the onset detection function SF\u0003(n)is selected as an\nonset if it fulﬁlls the following three conditions:\n1.SF\u0003(n) = max (SF\u0003(n\u0000!1:n+!2))\n2.SF\u0003(n)\u0015mean(SF\u0003(n\u0000!3:n+!4)) +\u000e\n3.n\u0000nprevious onset >! 5where\u000eis the tunable threshold. The other parameters\nwere chosen to yield the best performance on the complete\ndataset.!1= 30 ms,!2= 30 ms,!3= 100 ms,!4=\n70ms and the combination width parameter !5= 30 ms\nshowed good overall results. Parameter values must be\nconverted to frames depending on the frame-rate frused.\n3. EV ALUATION\nFor the evaluation of the algorithm, different datasets and\nsettings have been used to allow highest comparability with\nprevious publications.\n3.1 Performance measures and evaluation settings\nFor evaluating the performance of onset detection meth-\nods, commonly Precision, Recall, and F-measure are used.\nIf a detected onset is within the evaluation window around\nan annotated ground truth onset location, it is considered\na correctly identiﬁed onset. But every detected onset can\nonly match once, thus any detected onset within the eval-\nuation window of two different annotated onsets counts\nas one true positive and one false negative (a missed on-\nset). The same applies to annotations, i.e. all additionally\nreported onsets within the evaluation window of an anno-\ntation are counted as false positive detections. In order to\nkeep the comparability with other results, we match the\nevaluation parameters as follows:\nOur standard setting is the one used in [6], which com-\nbines all annotated onsets within 30 ms to a single onset\nand uses an evaluation window of \u000625 ms to identify cor-\nrectly detected onsets. Thus the combination width param-\neter!5of our peak-picking method is set to 30 ms as well.\nThe second set of parameters (denoted with an asterisk\nin Table 1) uses the same settings as in [14], where all on-\nsets within 50 ms are combined (i.e. !5= 50 ms) and an\nevaluation window of \u000670 ms is used.\nUnless otherwise noted, all given results are obtained\nby swiping the threshold parameter \u000eof the peak-picking\nstage and choosing the value that maximizes the F-measure\non the respective dataset.\n3.2 Datasets\nFor comparison with the former state-of-the-art algorithm\nfor pitched non-percussive music, the dataset from [14] is\nused. Unfortunately not all sound ﬁles and annotations\ncould be used for evaluation, since the authors were only\nable to provide part of this set. Still, we believe that the\nachieved results are comparable, because the dataset has\nover three quarters of the size of the original dataset (7,677\ninstead of 9,717 onsets) and an identical distribution of the\ndifferent playing styles (50% contain vibrato, some stac-\ncato etc.). This will be called the Wang dataset.\nTo show the ability to suppress tremolo and vibrato pre-\nsent in sung opera vocals, a second dataset introduced in [7]\nand consisting of solo singing rehearsal recordings of a\nHaydn opera is used. The set covers both male and fe-\nmale singers and has a total length of 10 minutes contain-\ning 1,448 onsets. It is called the Opera dataset.The biggest dataset used for evaluation is that described\nin [6], which consists mostly of mixed audio material cov-\nering different types of musical genres performed on var-\nious instruments. It includes the sets used in [3], [12],\nand [11]. The 321 ﬁles have a total length of approximately\n102 minutes and have 27,774 annotated onsets (25,927 if\nall onsets within 30 ms are combined). The main purpose\nof this set is to show how the new local group delay weight-\ning for the SuperFlux algorithm impacts the performance\non a general purpose dataset. This dataset is named B¨ock.\nBased on this set, we build a subset that contain violin and\ncello recordings played with vibrato and tremolo, but also\nfeature accompaniment instruments. These 16 ﬁles have\n849 onsets.\n3.3 Results & Discussion\nBecause the local group delay weighting technique is de-\nsigned especially for audio signals containing mostly vi-\nbrato and tremolo, the main focus should be put on the re-\nsults obtained on the Wang andOpera datasets. But since\nwe expect that it does not harm the overall performance of\nthe underlying SuperFlux algorithm too much when used\non other musical signals, the results given on the general\npurpose B¨ockdataset should not be neglected.\n3.3.1 Competitors\nBesides the former state-of-the-art algorithm for pitched\nnon-percussive music presented in [14] (for comparison on\ntheWang dataset), we chose the winning submissions of\nlast year’s MIREX evaluation [1] for comparison. We con-\nsider these submissions to be state-of-the-art, since they\nachieved the highest F-measure ever measured during the\nMIREX evaluation.\nTheOnsetDetector.2012 is an improved version of the\nmethod originally proposed in [11], which shows supe-\nrior performance in ofﬂine scenarios, and represents the\ngroup of probabilistic onset detection approaches. Since\ntheOnsetDetector.2012 was trained on the B¨ockdataset,\nthe results given in Table 3 and 4 for this algorithm were\nobtained with 8-fold cross-validation and parameters se-\nlected solely on the training set. Instead of the LogFilt-\nSpecFlux [6] algorithm, we chose the recently proposed\nSuperFlux algorithm [7], which shows better performance\non all datasets. The SuperFlux algorithm does not use any\nprobabilistic information and thus has much lower com-\nputational demands, marking the current upper bound of\nperformance of so-called “simple” algorithms.\nBecause the onset detection functions of the compared\nmethods show very different shapes and characteristics,\nand the choice of peak-picking methods and parameters\nhighly inﬂuence the ﬁnal results, we use ofﬂine peak-pick-\ning only. Since all algorithms yield their best performance\nin ofﬂine mode and are less sensitive to variations of pa-\nrameters, we consider this a valid choice. Nonetheless, all\nalgorithms can be used in online mode with slightly lower\nperformance.3.3.2 Wang set\nTable 1 shows the performance on violin music for the\nWang dataset. The new local group delay weighted Super-\nFlux method outperforms all other algorithms with respect\nto false positive detections by at least 25%. Compared\nside-by-side with the current state-of-the-art onset detec-\ntion algorithm, the OnsetDetctor , the weighted SuperFlux\nis able to achieve the same level of true positive detections,\nbut improves regarding false positive detections by an im-\npressive 56%.\nTP FP\nOnsetDetector.2012 [11] * 96.5% 15.5%\nSchleusing et.al. [14] * 91.2% 9.2%\nSuperFlux [7] * 94.7% 9.1%\nSuperFlux w/ LGD weighting * 97.0% 6.8%\nTable 1 :True and false positive rates of different onset\ndetection algorithms on the Wang dataset. Results for\nSchleusing’s algorithms were taken from [14]. Asterisks\nmark the evaluation method used in [14].\nSince the recordings in the Wang dataset are exclusively\nsolo recordings made in a sound absorbing room and con-\ntain only very few polyphonic parts, this result can be seen\nas the maximum possible performance boost that can be\nobtained with the local group delay weighting method for\nthis type of music.\n3.3.3 Opera set\nOn the Opera dataset with male and female opera rehearsal\nrecordings, the new method also shows its strength and\nis able to dramatically lower the number of false positive\ndetections. Compared with the original SuperFlux imple-\nmentation, the number of false detections go down from\n450 to 221 (which is a reduction by 51%), if the new lo-\ncal group delay based weighting technique is applied. The\nnew method even outperforms the current best-performing\nprobabilistic approach (with respect to F-measure), but it\nshould be noted that the neural network based method was\nnot trained on any opera material.\nP R F\nOnsetDetector.2012 [11] 0.576 0.777 0.662\nSuperFlux [7] 0.672 0.635 0.653\nSuperFlux w/ LGD weighting 0.806 0.635 0.711\nTable 2 :Precision, Recall and F-measure of different on-\nset detection algorithms on the Opera dataset.\n3.3.4 B ¨ock set\nIn Table 3 results for the full B¨ockdataset are given. With\nthe new difference weighting scheme, slightly lower per-\nformance can be observed. This was expected, since the\nnew approach is tuned speciﬁcally towards music with vi-\nbrato and tremolo but which otherwise contains only veryfew percussive sounds (as present in complex audio mixes\nlike pop songs). It could be argued, that the impressive\nperformance gains achievable for this special type of mu-\nsic justify the small performance penalty on this dataset.\nP R F\nOnsetDetector.2012 [11] 0.892 0.855 0.873\nSuperFlux [7] 0.883 0.793 0.836\nSuperFlux w/ LGD weighting 0.873 0.778 0.823\nTable 3 :Precision, Recall and F-measure of different on-\nset detection algorithms on the B ¨ock dataset.\nMore interesting are the results given in Table 4 for the\nstrings subset, which includes pieces with string instru-\nmentation that also feature accompaniment instruments –\nwhich make vibrato and tremolo suppression harder. As\ncan be seen, the local group delay weighted SuperFlux\nmethod also performs slightly worse than the original Su-\nperFlux implementation. Thus, it must be concluded that\nthe new weighting scheme is mainly suited for signals\nwhich feature numerous vibratos and tremolos but do not\ncontain many other instruments.\nP R F\nOnsetDetector.2012 [11] 0.834 0.820 0.827\nSuperFlux [7] 0.836 0.701 0.762\nSuperFlux w/ LGD weighting 0.777 0.710 0.742\nTable 4 :Precision, Recall and F-measure of different on-\nset detection algorithms on the strings subset of the B ¨ock\ndataset using the same parameters as used for the results\nin Table 3.\n4. CONCLUSIONS\nIn this paper a new method for vibrato and tremolo sup-\npression with local group delay based spectral weighting\nwas presented. The new weighting scheme can be applied\nto any spectral ﬂux like onset detection method and is able\nto reduce the number of false positive detections originat-\ning from vibrato and tremolo by up to 50% compared to\ncurrent state-of-the-art implementations.\nFor future versions of this weighting technique, the Con-\nstant-Q transform could be investigated. Using this trans-\nform instead of the Short-Time Fourier Transform would\nmake both the use of a ﬁlterbank for the magnitude spectro-\ngram and the rather simple combination technique for the\nphase information of several frequency bins into one obso-\nlete, but retain the beneﬁcial behavior of this approach.\n5. ACKNOWLEDGMENTS\nThis work is supported by the European Union Seventh\nFramework Programme FP7 / 2007-2013 through the\nPHENICX project (grant agreement no. 601166).6. REFERENCES\n[1] MIREX 2012 onset detection results. http://nema.\nlis.illinois.edu/nema_out/mirex2012/\nresults/aod/ , 2012, accessed 2013-03-27.\n[2] F. Auger and P. Flandrin. Improving the readability of\ntime-frequency and time-scale representations by the reas-\nsignment method. IEEE Transactions on Signal Processing ,\n43(5):1068–1089, May 1995.\n[3] J.P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. Davies,\nand M. Sandler. A tutorial on onset detection in music sig-\nnals. IEEE Transactions on Speech and Audio Processing ,\n13(5):1035–1047, September 2005.\n[4] J.P. Bello, C. Duxbury, M. Davies, and M. Sandler. On the use\nof phase and energy for musical onset detection in the com-\nplex domain. IEEE Signal Processing Letters , 11(6):553–\n556, June 2004.\n[5] S. B ¨ock, A. Arzt, F. Krebs, and M. Schedl. Online real-time\nonset detection with recurrent neural networks. In Proceed-\nings of the 15th International Conference on Digital Audio\nEffects (DAFx-12) , York, UK, September 2012.\n[6] S. B ¨ock, F. Krebs, and M. Schedl. Evaluating the online ca-\npabilities of onset detection methods. In Proceedings of the\n13th International Society for Music Information Retrieval\nConference (ISMIR 2012) , pages 49–54, Porto, Portugal, Oc-\ntober 2012.\n[7] S. B ¨ock and G. Widmer. Maximum ﬁlter vibrato suppression\nfor onset detection. In Proceedings of the 16th International\nConference on Digital Audio Effects (DAFx-13) , Maynooth,\nIreland, September 2013.\n[8] N. Collins. A comparison of sound onset detection algo-\nrithms with emphasis on psychoacoustically motivated detec-\ntion functions. In Proceedings of the AES Convention 118 ,\npages 28–31, Barcelona, Spain, May 2005.\n[9] N. Collins. Using a pitch detector for onset detection. In Pro-\nceedings of the 6th International Conference on Music In-\nformation Retrieval (ISMIR 2005) , London, UK, September\n2005.\n[10] S. Dixon. Onset detection revisited. In Proceedings of the 9th\nInternational Conference on Digital Audio Effects (DAFx-\n06), pages 133–137, Montreal, Quebec, Canada, September\n2006.\n[11] F. Eyben, S. B ¨ock, B. Schuller, and A. Graves. Universal on-\nset detection with bidirectional long short-term memory neu-\nral networks. In Proceedings of the 11th International Society\nfor Music Information Retrieval Conference (ISMIR 2010) ,\npages 589–594, Utrecht, Netherlands, August 2010.\n[12] A. Holzapfel, Y . Stylianou, A.C. Gedik, and B. Bozkurt.\nThree dimensions of pitched instrument onset detection.\nIEEE Transactions on Audio, Speech, and Language Process-\ning, 18(6):1517–1527, August 2010.\n[13] P. Masri. Computer Modeling of Sound for Transformation\nand Synthesis of Musical Signals . PhD thesis, University of\nBristol, UK, December 1996.\n[14] O. Schleusing, B. Zhang, and Y . Wang. Onset detection\nin pitched non-percussive music using warping-compensated\ncorrelation. In Proceedings of the IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP\n2008) , pages 117–120, April 2008.\n[15] R. Zhou, M. Mattavelli, and G. Zoia. Music onset detection\nbased on resonator time frequency image. IEEE Transactions\non Audio, Speech, and Language Processing , 16(8):1685–\n1695, November 2008."
    },
    {
        "title": "Essentia: An Audio Analysis Library for Music Information Retrieval.",
        "author": [
            "Dmitry Bogdanov",
            "Nicolas Wack",
            "Emilia Gómez",
            "Sankalp Gulati",
            "Perfecto Herrera",
            "Oscar Mayor",
            "Gerard Roma",
            "Justin Salamon",
            "José Ricardo Zapata",
            "Xavier Serra"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.6546714",
        "url": "https://doi.org/10.5281/zenodo.6546714",
        "ee": "https://zenodo.org/records/6546714/files/An Integrated Approach -Formatted Paper.pdf",
        "abstract": "EchoDB is an integrated search engine specifically to search songs, lyrics, albums, artists, writers, Record Labels, and all other music data across all major music platforms. The present generation which is Gen-Z has wide access to various information from around the globe right on their mobile phones. Entertainment being a day-to-day thing helped the industry boom within a few decades of usage of smartphones with the help of the internet. Music has become a daily chore and finding various genres is also now one of the drawbacks. EchoDB is an Integrated search engine that helps to access any song with suitable links provided along with the information about the song and the artist.",
        "zenodo_id": 6546714,
        "dblp_key": "conf/ismir/BogdanovWGGHMRSZS13",
        "keywords": [
            "EchoDB",
            "integrated search engine",
            "searches songs",
            "lyrics",
            "albums",
            "artists",
            "writers",
            "Record Labels",
            "music data",
            "all major music platforms"
        ],
        "content": "HBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 1 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nAn Integrated Approach of Music Search Engine  in Cross \nPlatforms – EchoDB  \n \nDr R.Poorvadevi * \nAssistant Professor,  CSE Department,  SCSVMV University,  Tamilnadu, India . \n*Corresponding Author  \nE-mail Id: -rpoorvadevi@kanchiuniv.ac.in  \n \nABSTRACT  \nEchoDB  is an integrated search engine specifically to search songs, lyrics, albums, artists, \nwriters, Record Labels, and all other music data across all major music platforms.  The \npresent generation which is Gen -Z has wide access to various information from arou nd the \nglobe right on their mobile phones. Entertainment being a day -to-day thing helped the \nindustry boom within a few decades of usage of smartphones with the help of the internet. \nMusic has become a daily chore and finding various genres is also now one  of the \ndrawbacks. EchoDB is an Integrated search engine that helps to access any song with \nsuitable links provided along with the information about the song and the artist.  \n \nKeywords :- Echo DB , Songs Search , Integrated Music search engine , echodb  music search , \nechodb songs search  \n \nINTRODUCTION  \nOverview  \nIn the modern era of music industry, where \ntechnology has eased things for anyone \nwho wants to pursue their music career by \nintroducing multiple sources to compose, \nhost and release their songs. Platforms like \nSpotify, Apple Music, Am azon music etc. \nto release. Platforms like DistroKid, \nAmuse, Indefy etc. to host the music and \napplications like FLStudios, GarageBand \netc. to create the music.  \n \nThese various platforms are making things \neasy for anyone who can afford computers \nand the internet nowadays. Since, \nplatforms like DistroKid and Amuse \nexpect you to pay lumps of money to host \nyour music in  order to reach higher \nengagement in top grossing platforms like \nSpotify and Apple music, artists \nsometimes might not always pay, thus, \nusually opting for a budget one or \nsometimes free. EchoDB helps artists to \nengage their audience by providing every \nuser the information they require when \nthey have heard  any kind of music and EchoDB does even provide the user with \nthe availability of the song on available \nplatforms and helps the user to experience \nevery music, just from a few clicks away. \nHowever, EchoDB in its initial stages \ndoesn't have any sophisticate d algorithms \nto recommend music for the user in order \nto make the UI more interactive,  \n \nObjectives  \n1. To provide Music Data across all \nmajor streaming platforms and \noptimize web search (SEO)  \n2. Analytics using ML & AI to \nunderstand the Trends in Music over \nthe ages \n3. Design efficient Algorithms to enable \ncomputers to recognize Human \nLanguages (Natural Language \nProcessing) better than ever  \n \nExisting Solutions  \nIt is intended to be integrated search \nengines for music but they are mostly \nbusiness -oriented. The existin g solutions \nare Google’s Search Engine and Shazam.  \n   \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 2 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nIn fact, Google’s search engine is still \nunder development in the case of music \ndata. Shazam is more business -oriented \nthan open -source music, and upon \nsearching this will result in only a few \nsongs that are further redirected to that \nsong’s search query in Spotify's database, \nwhich results are obviously being cashed \nin many ways.  \n \nDrawbacks of Existing Solutions  \n- Having multiple platforms would \nreduce the required engagement for \nunderrated artists.  \n- Having multiple platforms would \neven make the user urge to subscribe to \nmore than one platform, in order to listen \nto their favorite music.  \n- Few platforms like Deezer, Napster \nare not available in all the regions of the \nworld.  \n \nLITERATURE SURVEY  \nLiterature Survey  \nOnly a few research papers/articles were \navailable in the domain of music search \nengines. Here is the abstract of one of \nthose few papers available.  \n \nIn paper [1]  - Categories and Subject \nDescriptors: H.3.3 [Information Systems]: \nInformat ion Storage and  Retrieval; H.5.5 \n[Information Systems]: Information \nInterfaces and Presentation – Sound and \nMusic Computing  General . \n  \nAn approach is presented to automatically \nbuild a search engine for large -scale music \ncollections that can be queried thr ough \nnatural language. While existing \napproaches depend on explicit manual \nannotations and meta -data assigned to the \nindividual audio pieces, we automatically \nderive descriptions by making use of \nmethods from Web Retrieval And Music \nInformation Retrieval. Based on the ID3 \ntags of a collection of mp3 files, we \nretrieve relevant Web pages via  Google \nqueries and use the contents of these pages to characterize the music pieces and \nrepresent them by term vectors.  By \nincorporating complementary information \nabout acoustic similarity we are able to \nboth reduce the dimensionality of the \nvector space and improve the performance \nof retrieval  i.e. the quality of the results. \nFurthermore, the usage of audio similarity \nallows us to also characterize audio pieces \nwhen ther e is no associated information \nfound on the Web . \n \nProblem Statement  \nDeveloping a one -stop solution for the \nexisting problems in the field of music \nplatforms, with simple solutions like \nproviding links to access the songs along \nwith the details of the artists and Music \nvideos of that song.  \n \nFuture Projects (with EchoDB as base \nproject)  \nGraphene (Lightweight PHP MVC \nFramework) [Used in the 1st version of \nEchoDB]  \nPitchGrid (App Builder) [Converts any \nwebsite into an android or iOS \nApplication]  \n \nPROPOSED W ORK  \nProposed Method & Advantages  \nEchoDB is an open -source database for \nmusic, where people can search for any \nsong, album, lyrics, artists, and anything \nrelated to music across all streaming \nplatforms.  \n \nThis project can be a base for many \npotential researc h works and products in \nthe near future.  \n• Integrated Music Search Engine  \n• Data across all major streaming \nplatforms  \n• SEO Friendly & Sharable search \nresults  \n• Preview Music Video  \n• URL/URI for any song across all \nmajor streaming platforms  \n   \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 3 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nSYSTEM REQUIREMENTS  \nSoftware Environment  \nThe software requirement specification is \nan essential role in the software \ndevelopment stages. It defines the features \nand behavior of a software application to \nmeet the overall requirement for the \nSVMS. Hence, the specifications of the \nsoftware requirement are mentioned \nbelow.  \n• Linux Server  \n• 300 GB SSD Storage  \n• 12 GB RAM  • 6 CPU Cores  \n• SSL \n• CDN  \n• NOSQL Database (MongoDB)  \n \nTechnologies Used  \nFor implementing the project the following \ntechnologies were used:  \nFront -end: HTML, CS S, ReactJS, & \nBootstrap  \nBack -end: ExpressJS, Redux, MongoDB, \n& Nodejs  \nHosting: Netlify Cloud  \n \n System Architecture  \n \nFig.1:-System Architecture  \n \nSystem Planning and Design  \nWhen a user opens the application, they \ncan see the main search bar for Songs \nSearch. They can also see the Lyrics \nSearch  button on the top Navigation bar.  \n \nIn the Songs Search screen, they can type \nin the query and press Enter or click the \nsearch button to search songs for that \nquery. Then the query is further processed \nand the system sends requests to Spotify \nAPI, Apple Music APi, etc,.. and other 3rd \nparty APIs. The fetched data will be in the \nJSON format and will be further \nprocessed and merged togethe r. Then the \nprocessed data will be displayed in the \nResults View .  Now the users can click on listen to open \nthe Details View. Now they will have \naccess to the details of that particular song. \nThey can also find the links for that song \nto respective stream ing platforms like \nSpotify, Apple Music, Amazon Music, Jio \nSavan, YouTube, erc,... and they can click \nthat link to listen to that song on their \nfavorite streaming platform.  \n \n• Get data from Spotify, Apple Music \nand Musixmatch and store it in the \ndatabase  \n• Run ML algorithms on server and \nrefine the data  \n• Using Data Structures like linked lists \nperform Searching and Sorting \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 4 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nalgorithms for displaying results on \nthe client’s device  \n• Store search data and analyze the data using AI & Data Sciences to \nunderstand the tr ends and make the \nUser Experience (UX) better.  \n \n \nFig.2:-Cross Platform Compatibility  \n \nModules Description  \nModule description provides a detailed \nexplanation of the functionalities involved \nin the application.  \n \nThe following are the modules involved in \nthis application An API was built to merge \nSpotify’s API, Apple Music’s API, \nMusixmatch’s  API, and other 3rd party \nAPIs to connect results with all music \nstreaming services. Since this current \nphase of the project is more like an MVP, \nJavascript XML  HttpRequest was used to \nfetch the data from all the above -\nmentioned APIs.  \n• Songs Search  \n• Lyrics Search  \n• Data merging API  \n• Detail View  \n• Redirection API  \n \nSongs Search  \nThe main screen of the application is \nSongs Search. In this module, users can \nsearch for any song, album, artist, movie, \nlyricist, songwriter, etc .  \n \nWhen the user clicks the search b utton, it sends a request to all the APIs like Spotify, \nApple Music, Jio Savan, Amazon Music, \netc. Merges all the data together, and \ndisplays them in the results. Now users can \neither watch the Music Video or know the \ndetails of that song 1or listen to tha t song \non their favorite streaming service.  \n \nLyrics Search  \nUsers can navigate to Lyrics Search from \nthe main screen of the application by \nclicking the Lyrics Search button on the \ntop Navbar . In this module, users can \nsearch for any lyrics. When the user c licks \nthe search button, it sends a request to \nMusixmatch API and displays the data in \nthe results. Now users can click on the \nlyrics button on a song in the results list to \nget the lyrics of that song.  \n \nData merging API & Redirection API  \nData Merging API Module is the main \nfeature developed for this application. This \nModule basically sends requests to all the \nrespective APIs and fetches the data.  \n \nRedirection API Module fetches the \nURL/URI of a particular song using Data \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 5 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \nMerging API and wh en the user clicks the \nparticular URL/URI, it redirects the \napplication to the respective application.  \n \n3.6 Methodology    \nThe application for the integrated music \nsearch engine provides information about \nthe user interface and gives  a brief \nstatement abou t the app to give users an \ninstant understanding of the app. The \nsearch  allows a user to search for a song, \nalbum, movie, lyrics, music composer, \netc,...  and get the links and details of that \nsearch result.  \n \nFeatures Provided  \nThe verified MVP  includes features which \nplay some vital role in the whole model. \nBelow are the listed  features:  \nSearching for any song across all major \nstreaming platforms like Spot ify, Amazon Music, Apple Music, Jio Saavn, \nYouTube, etc. \n- Details of song Including \nTrackName, AlbumName, Artist, \nGenre, ReleaseDate, Language,  etc. \n- Providing Links to a particular \nsong to all major streaming platforms.  \n- Lyrics Search &  \n- Sharing a particular search result or \na song.  \n \nFuture Enhancements  \nIn further versions of this application, \nuser’s search data will also be collected  \n- To enhance and personalize the \nsearch results  \n- Understand and improvise Natural \nLanguage Processing  \n- Understand the Trends in Music \nover the ages  \n \nEXPERIMENTAL RESULTS  \n \n \nFig.3:-Home Screen on various devices  \n  \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 6 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \n \n \n \nFig.4:-Home Screen/ Songs Search View  \n \n \n                    \n Fig.5:-Result View                  Fig.6:-Details View                         Fig.7:-Share View  \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 7 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \n                         \n \n \n \n \n \nTelugu Old Songs Results  \n                     \n \nFig.11: - Telugu Old Song 1                  \n \n \n \n \n \nFig.8:-Lyrics Search  Fig.9:-Results View                            Fig.10: -Lyrics on \nMusixmatch  \n \nFig.12: - Telugu old Song 2               Fig.13: -Telugu Old Song 3    \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 8 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \n \nTelugu Old Songs Details  \n                \nFig.14: -Telugu Old Song 1    Fig.15: -Telugu old Song 2      Fig.16: -Telugu Old Song 3  \n \n \nTamil Old Songs Results  \n                  \nFig.17: -Tamil Old Song 1       Fig.18: -Tamil old Song 2     Fig.19: - Tamil Old Song 3  \n \nCONCLUSION  \nUpon completion of all the preprocessed \nreviews, this project has been successfully \ndeveloped and presented as the verified \nMVP (Minimum Viable Product) and a \nfully working model for the project as of \n02-04-2022, and this has been approved \nfor further documentation which concludes \nthis report.  \n FUTURE ENHANCEMENT  \nThe verified MVP  includes features which \nplay some vital role in the whole model. \nBelow are the listed  features:  \n \n- Searching for any song across all \nmajor streaming platforms like Spotify, \nAmazon Music, Apple Music, Jio Saavn, \nYouTube, etc. \n  \n \n \n \nHBRP Publication Page 1 -9 2022. All Rights Reserved                                                                Page 9 Research and Applications of Web Development and Design   \n Volume 5 Issue 1  \n \n- Details of song Including \nTrackName, Album  Name, Artist, \nGenre, Release  Date, Language,  etc. \n- Providing Links to a particular \nsong to all major streaming platforms.  \n- Lyrics Search  \n- Sharing a particular search result or \na song.  \n \nREFERENCES  \n1. Knees, P., Pohle, T ., Schedl, M., & \nWidmer, G. (2007, July). A music \nsearch engine built upon audio -based \nand web -based similarity measures. \nIn Proceedings of the 30th annual \ninternational ACM SIGIR conference \non Research and development in \ninformation retrieval  (pp. 447-454).   \n2. Aucouturier, J. J. (2006).  Ten \nexperiments on the modeling of \npolyphonic timbre  (Doctoral \ndissertation, Université Pierre et Marie \nCurie (Paris 6)).  \n3. Aucouturier, J. J., & Pachet, F. (2002, \nOctober). Music similarity measures: \nWhat's the use?. In  Ismir  (pp. 13 -17). \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 4. Aucouturier, J. J., Pachet, F., & \nSandler, M. (2005). \" The way it \nSounds\": timbre models for analysis \nand retrieval of music signals.  IEEE \nTransactions on Multimedia , 7(6), \n1028 -1035.  \n5. Yates , R., B., Neto  B.,R. . Modern \nInformation Retrieval. Addison -\nWesley, Reading, Massachusetts, \n1999. Google ScholarDigital Library"
    },
    {
        "title": "Evaluating The Quality of Generated Playlists Based on Hand-Crafted Samples.",
        "author": [
            "Geoffray Bonnin",
            "Dietmar Jannach"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418001",
        "url": "https://doi.org/10.5281/zenodo.1418001",
        "ee": "https://zenodo.org/records/1418001/files/BonninJ13.pdf",
        "abstract": "The automated generation of playlists represents a particular type of the music recommendation problem with two special characteristics. First, the tracks of the list are usually consumed immediately at recommendation time; second, tracks are listened to mostly in consecutive order so that the sequence of the recommended tracks can be relevant. A number of different approaches for playlist generation have been proposed in the literature. In this paper, we review the existing core approaches to playlist generation, discuss aspects of appropriate offline evaluation designs and report the results of a comparative evaluation based on different data sets. Based on the insights from these experiments, we propose a comparably simple and computationally tractable new baseline algorithm for future comparisons, which is based on track popularity and artist information and is competitive with more sophisticated techniques in our evaluation settings.",
        "zenodo_id": 1418001,
        "dblp_key": "conf/ismir/BonninJ13",
        "keywords": [
            "playlist generation",
            "music recommendation problem",
            "tracks consumed immediately",
            "sequential listening",
            "core approaches",
            "offline evaluation designs",
            "comparative evaluation",
            "different data sets",
            "simple and computationally tractable",
            "baseline algorithm"
        ],
        "content": "EVALUATING THE QUALITY OF PLAYLISTS BASED ON\nHAND-CRAFTED SAMPLES\nGeoffray Bonnin\nTU Dortmund, Germany\ngeoffray.bonnin@tu-dortmund.deDietmar Jannach\nTU Dortmund, Germany\ndietmar.jannach@tu-dortmund.de\nABSTRACT\nThe automated generation of playlists represents a parti-\ncular type of the music recommendation problem with two\nspecial characteristics. First, the tracks of the list are usu-\nally consumed immediately at recommendation time; se-\ncond, tracks are listened to mostly in consecutive order so\nthat the sequence of the recommended tracks can be rele-\nvant. A number of different approaches for playlist gene-\nration have been proposed in the literature. In this paper,\nwe review the existing core approaches to playlist gene-\nration, discuss aspects of appropriate ofﬂine evaluation de-\nsigns and report the results of a comparative evaluation\nbased on different data sets. Based on the insights from\nthese experiments, we propose a comparably simple and\ncomputationally tractable new baseline algorithm for fu-\nture comparisons, which is based on track popularity and\nartist information and is competitive with more sophisti-\ncated techniques in our evaluation settings.\n1. INTRODUCTION\nAmong the different application domains of recommender\nsystems (RS), music is often considered as being particu-\nlarly difﬁcult to deal with [5, 12]. One speciﬁc approach\nfor music recommendation and discovery is the automated\ngeneration and provision of playlists (mixes). This stra-\ntegy however induces additional challenges as the tracks\nare consumed in sequence and usually immediately at re-\ncommendation time. This means that the context of the\nprevious recommendations has some inﬂuence on user sa-\ntisfaction and should be taken into account in the playlist\ngeneration process.\nWhen our goal is to automatically generate playlists,\ni.e., lists of sequentially ordered tracks, one key question\nis the evaluation of their quality. In fact, there might be\na number of different factors that inﬂuence the perceived\nvalue of a playlist, including, e.g., the coherence of the list,\nor the variety or freshness of the songs [9]. Many playlist\ngeneration algorithms have been proposed in the literature,\nbut their quality is hard to compare as they often focus on\nparticular families of techniques and use different baseline\nalgorithms and evaluation criteria.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.In this paper, we review existing playlist generation ap-\nproaches and propose a comparably simple and computa-\ntionally efﬁcient new baseline algorithm for future compa-\nrisons which relies on track popularity and artist informa-\ntion. We discuss evaluation designs from the literature and\npresent experiments that use two evaluations metrics based\non comparisons with hand-crafted playlists, i.e., playlists\nmade by hand by music enthusiasts. The results show that\nour algorithm outperforms the other approaches on two out\nof three data sets when using hit rates as a metric. In ad-\ndition, the experiments reveal a considerable limitation of\nusing the log-likelihood metric as a means to compare the\nquality of automatically generated playlists.\n2. AUTOMATED PLAYLIST GENERATION\nIn the following, we review existing approaches to playlist\ngeneration and present our new approach. A playlist is usu-\nally deﬁned to be an ordered sequence of musical tracks.\nThe playlist generation problem typically consists in cre-\nating such a list given either some seed information or se-\nmantic description [3]. As another input, we might also\nhave some extra information for each track, e.g., the audio\nsignal, the composer, artists, lyrics, tags, ratings, etc.\nIn this work, we assume that the seed information con-\nsists of the listening history so far. Given this history,\nthe system presents recommendation lists of tracks to the\nuser, and each time a track is selected the process is re-\npeated [4]. Thus, the problem comes down to the compu-\ntation of the score of a candidate track tgiven a playlist\nhistoryh=ht1;t2;:::;t ii. The resulting scores – which in\nsome cases correspond to probability estimates – can then\nbe used to ﬁlter and rank the remaining tracks.\n2.1 Markov chains\nAttempting to recommend tracks that represent a smooth\ntransition from the previous track is an obvious approach.\nThis corresponds to the Markov property and leads to a\nﬁrst-order Markov model in which states correspond to\ntracks. Given a history hof a playlist and a candidate track\nt, the probability of tin such a model thus only depends\nonti, the last element of h. Examples of playlist model-\ning approaches based on this strategy include [14] and [6].\nIn [14], the authors compare a set of approaches to assign\ntransition probabilities to a Markov model including the\nuniform distribution, tags, audio signal and artist informa-\ntion. In [6], a more sophisticated Latent Markov Em-\nbedding (LME ) model in which tracks are represented byvectors in the Euclidean space is compared with the bigram\nmodel and a uniform distribution.\nThe major limitation of these models is that the assump-\ntions on which they are based may be too strong as the\nchoice of the next track by a user may or may not depend\nonly on the previous track. Although tracks are usually\nbeing listened one after the other and transitions between\ntracks surely have some importance, in practice, the rules\nusers follow to build playlists can be quite different and\noften contradict this assumption, see also [8].\n2.2 Frequent patterns\nAnother possibility to recommend tracks for playlist gene-\nration is to extract frequent patterns from playlists. The\ncommon techniques are association rule (AR) and sequen-\ntial pattern (SP) mining. An association rule [1] has the\nformA!C, whereAandCare two itemsets. Sequen-\ntial patterns are a sequential version of association rules\n[2] in which the order of the elements in the pattern is also\ntaken into account in the mining process. The additional\nconstraints of sequential patterns over association rules can\nin general lead to more accurate recommendations, but the\napproach has a higher computational complexity and re-\nquires a larger amount of training data. Another possible\nlimitation of this approach might be the comparably small\nconﬁdence values for the extracted patterns given the usu-\nally high sparsity of musical data sets.\n2.3 Neighborhood recommenders\nAnother way to exploit co-occurrences of tracks is to use a\nk-nearest-neighbors (kNN) recommender which is based\non the similarity between playlists. Such a kNN approach\nwas proposed in [11] and used as a basis for a more so-\nphisticated recommender which uses sequential patterns of\nlatent topics based on tags. Similar to association rules,\nthiskNN approach not only exploits information about the\ncollocation of items in playlists but also takes the number\nof shared items in each playlist into account when estima-\nting the probability. However, association rule mining is\nbased on counting the frequency of patterns for all users\nin an ofﬂine process. The kNN approach, in contrast, dy-\nnamically computes a “local” probability using the kmost\nsimilar playlists. In other words, the limitation mentioned\nin the previous subsection with respect to low conﬁdence\nvalues is reduced. The computation of neighborhoods and\nplaylist similarities is however computationally complex\nboth in terms of time and space, making the approach in-\ntractable when recommendations have to be made in real\ntime.\n2.4 Playlists as users\nIn principle, if we interpret the playlist generation problem\nto be similar to the item prediction problem in typical RS\nsettings by considering playlists to be users, existing RS al-\ngorithms for item recommendations can be applied inclu-\nding recent learning-to-rank techniques. In particular, the\nBPR-approach (Bayesian Personalized Ranking) from [15]has been included in previous comparative evaluations for\nplaylist or music recommendation, see e.g. [11] and [13].\nThe experiments in the last two papers however show that\nthe plain BPR method can be easily outperformed by other\nmethods in particular problem settings.\n2.5 Content-based approaches\nUsing additional information, one can try to enhance the\nconﬁdence of pattern-based approaches or avoid the comp-\nlexity of the kNN approach. Such additional information\ncan be the content of the tracks (lyrics, spectrum, etc.),\nthe similarity of musical features [10], user tags, or more\nsimple elements such as artist names. Some of the afore-\nmentioned approaches use some forms of content and meta-\ndata. For instance, the topic-aware hybrid recommender\nof [11] uses tags to determine topics, but does not solve\nthe scalability problem of the underlying kNN approach.\nAlso McFee and Lanckriet [14] experiment with Markov\nmodels that use tags, the audio signal and artist names.\nHowever, their approach does not solve the problem of the\nstrong assumption of the Markov property.\nRegarding the incorporation of additional information\ninto the recommendation process, we hypothesize that the\nuse of artist names in general is particularly promising as\nthis type of data is objective, easy to obtain and to process\n(as opposed to, for instance, information about the playlist\ntopic, genre or style).\n2.6 Popularity-based approaches\nIn many application domains for RS and in particular in\nthe music domain [5], we can observe a so called “long\ntail” distribution of items, meaning that a small subset of\nthe items accounts for the majority of transactions or inter-\nactions. This popularity bias results in the fact that simple\npopularity-based approaches, which present the same set\nof popular items to everyone, can represent a compara-\nbly hard baseline [7]. Given these observations, we in-\ncluded two approaches that are based on popularity com-\nbined with artist information in the experiments.\n“Same artists - greatest hits” (SAGH) : In [13], the\nauthors propose a baseline algorithm for music recommen-\ndation – not in the context of playlists – called “Same artists\n- greatest hits”, which simply recommends the most popu-\nlar songs of the artists appearing in the user’s listening his-\ntory. Their experiments on the Million Song data set shows\nthat higher prediction accuracy can be obtained with such\nan approach than when using, e.g., the above-mentioned\nBPR method. This method would thus be a hybrid that\nuses both additional information as well as popularity.\n“Collocated artists - greatest hits” (CAGH) : In this\npaper, we do not only apply the previous scheme, but pro-\npose an extension to it. Our assumption is that the different\nartists that are included in playlists by the users are not too\ndifferent from each other. We thus propose to recommend\ntracks based on the frequency of the collocation of artists.More precisely, we compute the similarity between two\nartistsaandbaccording to the following formula:\nsim a(a;b) =P\np(\u000ea;p\u0001\u000eb;p)qP\np\u000ea;p\u0001P\np\u000eb;p\nwith\u000ea;p= 1if playlistpcontainsaand0otherwise. The\nsimilarity thus depends on the collocations of artists within\nplaylists, which can be computed ofﬂine. Our proposed\nformula for the computation of the score of a next track t\nwith artistagiven a playlist beginning his as follows:\nscore CAGH (t;h) =X\nb2Ahsim a(a;b)\u0001counts (t)(1)\nwhereAhis the set of artist names of the tracks in hand\ncounts (t)is the number of occurrences of tin the data set,\nwhich corresponds to the greatest hits of the data set.\n3. A COMPARATIVE EVALUATION OF PLAYLIST\nGENERATION STRATEGIES\nThe presented playlist generation techniques follow diffe-\nrent strategies and exploit different types of inherent cha-\nracteristics of the playlists or rely on external information.\nThe goal of this study is to obtain a better understanding of\ndifferent aspects related to the generation and evaluation of\nplaylists. In particular, we aim to better understand the role\nof sequentiality and popularity, ﬁnd out if different metrics\nfollow the same trends in a comparative evaluation and if\nthe observations are consistent across different data sets.\n3.1 Data Sets\nWe used three data sets in our experiments. One is from\nArtofthemix, which is probably the most commonly used\ndata set for related research [11, 14]. The second was re-\ntrieved from last.fm1. The third was provided to us by\n8tracks2. In order to reduce the sparsity of the data, we\nused the web service of Musicbrainz3to correct artist and\ntrack misspellings. We also removed playlists of size 1.\nFor the last.fm data, we furthermore decided to select play-\nlists in a way that long-tail tracks are used at least twice.\nTable 1 shows the data set characteristics.\nNotice that the Artofthemix data does not contain user\nIDs. As implicitly done also by [11], we consider users as\nbeing equivalent to playlists, as they usually do not create\nlarge numbers of playlists. Regarding track occurrences,\nthe last.fm and 8tracks data sets have a similar average\ntrack usage count ( 5:5and5:3)4. This usage count is sig-\nniﬁcantly smaller for Artofthemix ( 2:7). Another related\ncharacteristic is the long tail distribution of track usages.\nTable 1 divides the corresponding distribution into three\nparts: head, middle and tail. The “head” contains tracks\nwhich appeared more than 20times in playlists, tracks in\nthe “middle” were included in playlist between 2and20\n1http://www.lastfm.com/api\n2http://8track.com\n3http://musicbrainz.org/ws/2/\n4The track/artist usage count means how often a track/artist was used\nin all playlistslast.fm Aotm 8tracks\nPlaylists 50,000 28,636 99,542\nUsers 47,603 – 51,743\nTracks 69,022 214,769 179,779\nAvg. tracks/playlist 7.6 20.1 9.7\nAvg. track usage count 5.5 2.7 5.3\nHead 4.8% 1.6% 4.5%\nMiddle 35.0% 18.5% 25.7%\nTail 60.1% 79.9% 69.8%\nArtists 11,788 47,473 29,352\nAvg. artists/playlist 4.5 17.3 8.9\nAvg. artist usage count 32.2 12.1 32.7\nArtist reuse rate 31.1% 21.8% 13.8%\nTable 1 . Properties of the data sets.\ntimes, and songs from the “tail” were only used once or\ntwice. These values are admittedly somewhat arbitrary\nbut allow us to roughly compare the respective distribu-\ntions. The resulting proportions reveal another difference\nbetween the last.fm and 8tracks data sets: although they\nhave a similar average track usage count, the size of the\nlong tail of 8tracks is much larger.\nRegarding artist-based recommendation approaches, Ta-\nble 1 shows that playlists usually contain fewer artists than\ntracks. The row “artist reuse rate” in the table represents\nthe percentage of cases when the artist of the last track\nof a playlist already appeared in the same playlist before.\nThe corresponding values are 31:1%for the last.fm data\nset,21:8%for the Artofthemix data set and 13:8%for the\n8tracks data set. This represents another difference be-\ntween the last.fm and 8tracks data sets: although they have\na similar average artist usage count, the artists are more\ndistributed across the playlists in the 8tracks data because\n8tracks’ license only allows for up to two songs from the\nsame artist per playlist. Overall, we think that these values\nrepresent a strong argument to emphasize on artist names\nas an additional information when recommending tracks,\nexcept maybe for the 8tracks data set.\nUsing three data sets with quite different characteristics\nshould allow us to analyze how the different algorithms\nperform in different situations. In general, generating re-\ncommendations based on the Artofthemix data set should\nbe much more difﬁcult than with the last.fm and 8tracks\ndata sets, as it is smaller and the individual tracks are less\noften used. Other factors may however play a major role\nas well, in particular the size of the long tail.\n3.2 Evaluation Metrics\nPlaylist generation is usually evaluated according to three\npossible strategies: semantic cohesion, human opinion sur-\nvey and comparison with hand-crafted playlists. Seman-\ntic cohesion corresponds to the assumption that a good\nplaylist is a playlist which tracks are as homogeneous as\npossible, e.g., in terms of genre or style, which can be con-\nsidered as a strong assumption. Human opinion surveys\ndo not have this drawback but are time consuming and dif-\nﬁcult to reproduce. We thus choose the third option andevaluate playlisters by comparing their output with hand-\ncrafted playlists. Two evaluation strategies are common,\nthe hit rate and the average log-likelihood.\n3.2.1 Measuring hit rates\nA ﬁrst way of measuring the accuracy of playlist gene-\nration for music recommendation is to use the hit rate ,\ni.e., the proportion of relevant predictions on a test set. An\nevaluation method of this type is used, e.g., by [11], who\nhide the last element of each given playlist, which has then\nto be recommended by the algorithm. In general, any sub-\nset of playlist elements could be hidden in such a protocol.\nRemoving the last one however is based on the assumption\nthat the sequential history of a playlist can be relevant.\nThe limitation of this evaluation metric is that it cor-\nresponds to the assumption that the actual next tracks in\nthe playlist are the only relevant tracks that can be rec-\nommended, although some other tracks may be relevant.\nIn other words, it is possible that hundreds of tracks are\nrelevant, but as the recommender has to select a subset of\nthem, the actual next tracks of the test playlists might not\nbe recommended. As it is impossible to know how many\ntracks are relevant for each situation, it is reasonable to\nanalyze the accuracy of a system using longer recommen-\ndation lists. Such lists could of course not be used in a\nreal framework, but our goal here is only to compare the\nalgorithms. The assumption is then that there is a corres-\npondence between the size of the recommendation lists and\nthe average number of relevant tracks. Still, the hit rate can\nonly be considered to be a lower bound for the accuracy.\n3.2.2 Measuring the average log-likelihood\nAnother way to measure accuracy is to use the average log-\nlikelihood . The average log-likelihood can be used to mea-\nsure how likely a system is to recommend the tracks of a\ngiven set of playlists through a weighted random process.\nMore precisely, given a test set of playlists, the average\nlog-likelihood can be determined by computing the proba-\nbility of observing each next track according to the corres-\nponding playlist history and some model learned on the\ntraining data. Research on music recommendation using\nplaylists that use this metric includes [6] and [14]. Obvi-\nously, the application of this measure requires that the out-\nput of a playlist recommender can be expressed as proba-\nbility values for each song, which can be easily obtained\nby a normalization over the prediction lists.\nIn contrast to the hit rate, which provides a realistic\nlower bound on the accuracy that is directly interpretable,\nthis metric is not interpretable on an absolute scale: the\npossible values vary between \u00001 (at least one track in the\ntest set has a corresponding 0probability in the model) and\n0(all probabilities in the model for all tracks in the test set\nare1). Thus, this metric does not tell us if a generative\napproach leads to good playlists, but allows us to compare\nthe results of different generative approaches. It can thus\nbe considered as a complementary measure to the hit rate.\nAs only one track having a 0probability is sufﬁcient to\ninduce a\u00001 average log-likelihood, 0probabilities must\nbe avoided. This requires an additional smoothing step,which might result in a strong bias. For instance, new\ntracks will always have such 0probabilities with a fre-\nquency-based approach. In that situation a combination\nwith the uniform distribution can be used, but then the\nweight of the uniform distribution may become too large\ngiven the long-tailed distribution of our data sets.\n3.2.3 Computational complexity\nAnother important fact that should be taken into account is\nthe computational complexity. Indeed, as opposed to, for\ninstance, movie recommendation, for which recommen-\ndations can be computed ofﬂine and updated regularly, mu-\nsic recommendation can be highly dynamic and contex-\ntual. Users usually listen to tracks in sequence, where\neach track lasts a few minutes. Therefore, a music re-\ncommender should be able to provide fast contextual re-\ncommendations. Moreover, as the number of tracks that\ncan be recommended is usually very high, the efﬁciency of\nthe training phase can become crucial. In the subsequent\nanalysis of algorithms, we will thus also brieﬂy discuss as-\npects of computational complexity.\n4. EXPERIMENTS\nIn the following evaluation, we use the two aforementioned\naccuracy metrics: hit rate and average log-likelihood. A\n10-fold cross-validation procedure was applied for both\nmetrics on the three data sets. Recall that the total number\nof tracks of the data set highly inﬂuences the hit rate va-\nlues. In [11], the results for prediction lists of size varying\nbetween 1and300given 21;783tracks are reported. This\ncorresponds to the selection of about 1:5%of the tracks.\nWe used a similar proportion in our experiments and set the\nmaximum size of the prediction lists to 1;000for last.fm,\n3;000for Artofthemix and 2;500for 8tracks.\n4.1 Evaluating Hit Rates\nFigure 1 shows the results of comparing ﬁve different re-\ncommendation approaches on the three data sets using the\nhit rate. The approaches include the three above-mentioned\nfrequent-pattern approaches AR and SP, a kNN recom-\nmender using 50and 100 neighbors, the SAGH recom-\nmender and our new baseline recommender CAGH5.\nWe can ﬁrst notice that all approaches lead to compa-\nrably low accuracy values for short recommendation lists.\nFor longer recommendation lists, our new CAGH recom-\nmender clearly outperforms the other approaches on the\nlast.fm and Artofthemix data, except for recommendation\nlists longer than 2;800for the frequent-patterns approach\non the Artofthemix data. On the data from 8tracks, the\nfrequent pattern approach clearly outperforms all other ap-\nproaches, followed by the kNN approach with 100neigh-\nbors, and the CAGH recommender. The reason for the\nlower performance of the CAGH recommender is proba-\nbly the better distribution of artists across playlists on this\nparticular data set due to the corresponding license restric-\ntions (see section 3.1).\n5The method of [11] is not included here but is comparable to the kNN\nmethod according to their measurements. 0 5 10 15 20 25 30 35 40\n 0  100  200  300  400  500  600  700  800  900  1000hit rate\nSize of the recommendation listlast.fm\nCAGH\nSAGH\nSP with n = 2 and w = 10\nkNN with k = 50\nkNN with k = 100\n 0 5 10 15 20 25\n 0  500  1000  1500  2000  2500  3000hit rate\nSize of the recommendation listArtofthemix\nCAGH\nSAGH\nAR with n = 3 and w = 100\nkNN with k = 50\nkNN with k = 100\n 0 5 10 15 20 25 30 35\n 0  500  1000  1500  2000  2500hit rate\nSize of the recommendation list8tracks\nCAGH\nSAGH\nSP with n = 3 and w = 100\nkNN with k = 50\nkNN with k = 100Figure 1 . Hit rates of the different approaches.\nIn general, using more neighbors enhances the accuracy\nof thekNN approach on the three data sets. The kNN\napproach may even outperform all the other approaches\nusing more than 100neighbors. However, both neighbor-\nhood sizes used in these experiments are already high and\nmake the recommendation algorithm not only intractable\nin terms of space requirements, but also in terms of run-\nning time. Still, kNN approaches lead to a lower accuracy\nvalues than both our new baseline approach and the fre-\nquent patterns method. More precisely, on the three data\nsets the accuracy of the kNN approach seems to be limited\nby the size of the recommendation lists it is able to build.\nThis is probably the reason why the frequent patterns out-\nperform this approach on the 8tracks data set, as it is close\nto akNN approach that uses all the neighbors.\nOther observations depend on the used data set. In parti-\ncular, for the last.fm data set, the SAGH recommender\nleads to results that are similar to those of the kNN re-commender with 100neighbors. For the Artofthemix data\nset, the SAGH recommender is clearly outperformed by all\nother approaches. For the 8tracks data set, it leads to results\nthat are similar to those of the kNN recommender with 50\nneighbors for recommendation lists longer than 750.\nBeside the results shown in Figure 1, we also experi-\nmented with models based on the Markov property, among\nthem the simple bigram model and the recent Latent Mar-\nkov Embedding (LME) model of [6]. Despite the long time\nthat can be required to train these models – e.g., several\nweeks for the LME model – these methods led to particu-\nlarly low accuracy values which were consistently below\n10% for recommendation lists of size 1;000for the last.fm\ndata set and 5%for recommendation lists of size 3;000for\nthe Artofthemix data set. We therefore omit these results\nin this paper. In general, given these comparably strong\ndifferences, assuming the Markov property might be too\nstrong for this problem setting. Furthermore, our results\nindicate that emphasizing on artist names can be particu-\nlarly promising for accurate track recommendation in the\ncontext of playlist generation.\n4.2 Evaluating Average Log-Likelihoods\nCAGHSAGH SP with n=2 and w=10KNN with k=100KNN with k=50UniformLME\n-12 -10 -8 -6 -4 -2  0\nAverage log-likelihoodlast.fm\nCAGHSAGHAR with n=3 and w=100KNN with k=100KNN with k=50Uniform\n-12 -10 -8 -6 -4 -2  0\nAverage log-likelihoodArtofthemix\nCAGHSAGHSP with n=3 and w=100KNN with k=100KNN with k=50Uniform\n-12 -10 -8 -6 -4 -2  0\nAverage log-likelihood8tracks\nFigure 2 . Av. log-likelihood of the different approaches\nFigure 2 shows the results of the generative versions of\nthe ﬁve approaches on the three data sets using the average\nlog-likelihood. The experimented methods all correspond\nto mixture models that combine the uniform distribution\nwith the approaches of the previous set of experiments.\nIn order to perform these evaluations, each training set of\nthe cross-validation process was further split into a lear-\nning set for obtaining model probabilities and a validation\nset to compute optimized weights using the Expectation-Maximization algorithm. We also provide the results of the\nLME model with standard parameters on the last.fm data\n(on both other data sets the training process lasted more\nthan a month).\nWe focus on the correspondences with the previous set\nof hit rate results. On the last.fm data set the mixture of the\nuniform distribution with the CAGH and SAGH recom-\nmenders leads to the best average log-likelihood values.\nIt is worth noting that both models provide similar results\naccording to this metric although on the same data set the\nCAGH recommender was a clear winner in terms of the\nhit rate. We also evaluated all the mixture models of this\nsection in terms of the hit rate and obtained a similar out-\nput: both CAGH and SAGH recommenders lead to a simi-\nlar hit rate when combined with the uniform distribution.\nMore precisely, this combination strongly lowers the hit\nrate of the CAGH recommender, but not that of the SAGH\nrecommender. This result conﬁrms the bias of using this\nmetric we mentioned in section 3.2: as a relatively impor-\ntant number of tracks are new in the test sets, EM-based\nmixtures tend to give more weight to tracks coming from\nthe uniform distribution than tracks coming from the simi-\nlar artists. This phenomenon also appears on the three data\nsets, although the CAGH and SAGH do not outperform\nthe other models. On the Artofthemix data set, it even ap-\nplies to all the models: the accuracy of all approaches is\nstrongly lowered when combined with the uniform distri-\nbution. These results indicate a limitation of using the ave-\nrage log-likelihood metric, i.e., smoothing the models have\nhighly lowered the accuracy, although in reality a RS does\nnot have to avoid 0probabilities.\nWhen testing the LME model on the last.fm data set, the\nexperiments showed that the model lead to lower results\nthan the uniform distribution. This conﬁrms the previous\nconclusion about the use of the Markov property.\n5. CONCLUSION\nThis paper proposes a classiﬁcation of existing approaches\nfor playlist generation and discusses limitations of typical\nexperimental designs, which for example do not take sca-\nlability aspects into account or are based on comparably\nstrong assumptions such as the Markov property. Based\non this discussion, we propose a new computationally ef-\nﬁcient recommendation scheme based on popularity and\nartist information. An experimental comparative evalu-\nation showed that our algorithm outperforms the other ap-\nproaches in terms of hit rate on two of three data sets. On\nthe remaining data set, our recommender is on a par with\nneighborhood-based approaches and was outperformed by\na frequent pattern technique. This difference is probably\ncaused by the high dispersion of artists among playlists due\nto license constraints. However, other factors may have in-\nduced this difference in accuracy, which we are investigat-\ning in our current work. Our evaluations also put forward\na strong limitation of using the average log- likelihood me-\ntric: it implies to smooth models in order to avoid 0proba-\nbilities which resulted in a strong degradation of the quality\nof the approaches.6. ACKNOWLEDGMENTS\nWe thank 8tracks for providing us their valuable data.\n7. REFERENCES\n[1] R. Agrawal, T. Imieli ´nski, and A. Swami. Mining\nAssociation Rules between Sets of Items in Large\nDatabases. In SIGMOD 1993 , pages 207–216, 1993.\n[2] R. Agrawal and R. Srikant. Mining Sequential Patterns.\nInProc. ICDE 1995 , pages 3–14, 1995.\n[3] L. Barrington, R. Oda, and G. Lanckriet. Smarter than\nGenius? Human Evaluation of Music Recommender\nSystems. In Proc. ISMIR 2009 , pages 357–362, 2009.\n[4] Dominikus Baur, Sebastian Boring, and Andreas Butz.\nRush: Repeated Recommendations on Mobile De-\nvices. In Proc. IUI 2010 , pages 91–100, 2010.\n[5]`O. Celma. Music Recommendation and Discovery -\nThe Long Tail, Long Fail, and Long Play in the Dig-\nital Music Space . Springer, 2010.\n[6] S. Chen, J.L. Moore, D. Turnbull, and T. Joachims.\nPlaylist Prediction via Metric Embedding. In Proc.\nKDD 2012 , pages 714–722, 2012.\n[7] P. Cremonesi, Y . Koren, and R. Turrin. Performance\nof recommender algorithms on top-n recommendation\ntasks. In ACM RecSys 2010 , pages 39–46, 2010.\n[8] S. Cunningham, D. Bainbridge, and A. Falconer.\n‘More of an Art than a Science’: Supporting the Cre-\nation of Playlists and Mixes. In Proc. ISMIR 2006 ,\npages 240–245, 2006.\n[9] B. Fields. ”Contextualize Your Listening: The Playlist\nas Recommendation Engine” . PhD thesis, Goldsmiths,\nUniversity of London, London, UK, April 2011.\n[10] A. Flexer, D. Schnitzer, M. Gasser, and G. Widmer.\nPlaylist Generation Using Start and End Songs. In IS-\nMIR 2008 , pages 173–178, 2008.\n[11] N. Hariri, B. Mobasher, and R. Burke. Context-Aware\nMusic Recommendation Based on Latent Topic Se-\nquential Patterns. In Proc. ACM RecSys 2012 , pages\n131–138, 2012.\n[12] P. Lamere and `O. Celma. Music Recommendation\nand Discovery Remastered, Tutorial at ACM RecSys\n2011. Online at http://www.slideshare.net/\nslideshow/embed_code/9860137 , 2011.\n[13] B. McFee, T. Bertin-Mahieux, D. Ellis, and G. Lanck-\nriet. The million song data set challenge. In Proc. Ad-\nMIRe’12 , 2012.\n[14] B. McFee and G. Lanckriet. The Natural Language of\nPlaylists. In Proc. ISMIR 2011 , 2011.\n[15] S. Rendle, C. Freudenthaler, Z. Gantner, and\nL. Schmidt-Thieme. BPR: Bayesian Personalized\nRanking from Implicit Feedback. In Proc. UAI , pages\n452–461, 2009."
    },
    {
        "title": "Audio Chord Recognition with Recurrent Neural Networks.",
        "author": [
            "Nicolas Boulanger-Lewandowski",
            "Yoshua Bengio",
            "Pascal Vincent"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418319",
        "url": "https://doi.org/10.5281/zenodo.1418319",
        "ee": "https://zenodo.org/records/1418319/files/Boulanger-LewandowskiBV13.pdf",
        "abstract": "In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task.",
        "zenodo_id": 1418319,
        "dblp_key": "conf/ismir/Boulanger-LewandowskiBV13",
        "keywords": [
            "audio chord recognition system",
            "recurrent neural network",
            "audio features",
            "deep neural network",
            "chromagram targets",
            "chord information",
            "acoustic and musicological models",
            "training objective",
            "efficient algorithm",
            "MIREX dataset"
        ],
        "content": "AUDIO CHORD RECOGNITION WITH RECURRENT NEURAL\nNETWORKS\nNicolas Boulanger-Lewandowski, Yoshua Bengio and Pascal Vincent\nDept. IRO, Universit ´e de Montr ´eal\nMontr ´eal, Qu ´ebec, Canada H3C 3J7\nfboulanni, bengioy, vincentp g@iro.umontreal.ca\nABSTRACT\nIn this paper, we present an audio chord recognition system\nbased on a recurrent neural network. The audio features\nare obtained from a deep neural network optimized with\na combination of chromagram targets and chord informa-\ntion, and aggregated over different time scales. Contrar-\nily to other existing approaches, our system incorporates\nacoustic and musicological models under a single train-\ning objective. We devise an efﬁcient algorithm to search\nfor the global mode of the output distribution while tak-\ning long-term dependencies into account. The resulting\nmethod is competitive with state-of-the-art approaches on\nthe MIREX dataset in the major/minor prediction task.\n1. INTRODUCTION\nAutomatic recognition of chords from audio music is an\nactive area of research in music information retrieval [16,\n21]. Existing approaches are commonly based on two fun-\ndamental modules: (1) an acoustic model that focuses on\nthe discriminative aspect of the audio signal, and (2) a mu-\nsicological, or language model that attempts to describe\nthe temporal dependencies associated with the sequence of\nchord labels, e.g. harmonic progression and temporal con-\ntinuity. In this paper, we design a chord recognition system\nthat combines the acoustic and language models under a\nuniﬁed training objective using the sequence transduction\nframework [7, 12]. More precisely, we introduce a proba-\nbilistic model based on a recurrent neural network that is\nable to learn realistic output distributions given the input,\nthat can be trained automatically from examples of audio\nsequences and time-aligned chord labels.\nFollowing recent advances in training deep neural net-\nworks [1] and its successful application to chord recog-\nnition [19], music annotation and auto-tagging [15], poly-\nphonic music transcription [24] and speech recognition [17],\nwe will exploit the power of deep architectures to extract\nfeatures from the audio signals. This pre-processing step\nwill ensure we feed the most discriminative features pos-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.sible to our transduction network. A popular enhancement\nthat we also employ consists in the use of multiscale aggre-\ngated features to describe context information [4, 10, 14].\nWe also exploit prior information [13] in the form of pitch\nclass targets derived from chord labels, known to be a use-\nful intermediate representation for chord recognition (e.g.\n[9]).\nRecurrent neural networks (RNN) [26] are powerful dy-\nnamical systems that incorporate an internal memory, or\nhidden state , represented by a self-connected layer of neu-\nrons. This property makes them well suited to model tem-\nporal sequences, such as frames in a magnitude spectro-\ngram or chord labels in a harmonic progression, by being\ntrained to predict the output at the next time step given\nthe previous ones. RNNs are completely general in that in\nprinciple they can describe arbitrarily complex long-term\ntemporal dependencies, which made them very successful\nin music applications [5–7,11,23]. While RNN-based mu-\nsical language models signiﬁcantly surpass popular alter-\nnatives like hidden Markov models (HMM) [6] and offer a\nprincipled way to combine the acoustic and language mod-\nels [7], existing inference procedures are time-consuming\nand suffer from various problems that make it difﬁcult to\nobtain accurate predictions. In this paper, we propose an\ninference method similar to Viterbi decoding that preserves\nthe predictive power of the probabilistic model, and that is\nboth more efﬁcient and accurate than alternatives.\nThe remainder of this paper is organized as follows. In\nSection 2, we present our feature extraction pipeline based\non deep learning. In Sections 3 and 4 we introduce the\nrecurrent neural network model and the proposed inference\nprocedure. We describe our experiments and evaluate our\nmethod in Section 5.\n2. LEARNING DEEP AUDIO FEATURES\n2.1 Overview\nThe overall feature extraction pipeline is depicted in Fig-\nure 1. The magnitude spectrogram is ﬁrst computed by\nthe short-term Fourier transform using a 500 ms sliding\nBlackman window truncated at 4 kHz with hop size 64 ms\nand zero-padded to produce a high-resolution feature vec-\ntor of length 1400 at each time step, L2normalized and\nsquare root compressed to reduce the dynamic range. Due\nto the following pre-processing steps, we found that a mel\nscale conversion was unnecessary at this point. We applyh1y(t)\nW0\nv(t)RBM 1Output\nh2\nW1 RBM 2Prediction~y(t) Audio Signal\nSpectrogram\nPCA\nFigure 1 . Pre-processing pipeline to learn deep audio fea-\ntures with intermediate targets z(t);~z(t). Single arrows rep-\nresent a deterministic function, double-ended arrows rep-\nresent the hidden-visible connections of an RBM.\nPCA whitening to retain 99% of the training data variance,\nyielding roughly 30–35% dimensionality reduction. The\nresulting whitened vectors v(t)(one at each time step) are\nused as input to our DBN.\n2.2 Deep belief networks\nThe idea of deep learning is to automatically construct in-\ncreasingly complex abstractions based on lower-level con-\ncepts. For example, predicting a chord label from an audio\nexcerpt might understandably prerequire estimating active\npitches, which in turn might depend on detecting peaks in\nthe spectrogram. This hierarchy of factors is not unique\nto music but also appears in vision, natural language and\nother domains [1].\nDue to the highly non-linear functions involved, deep\nnetworks are difﬁcult to train directly by stochastic gradi-\nent descent. A successful strategy to reduce these difﬁ-\nculties consists in pre-training each layer successively in\nan unsupervised way to model the previous layer expecta-\ntion. In this work, we use restricted Boltzmann machines\n(RBM) [27] to model the joint distribution of the previous\nlayer’s units in a deep belief network (DBN) [18] (not to\nbe confused with a dynamic Bayesian network).\nThe observed vector v(t)\u0011h0(input at time step t) is\ntransformed into the hidden vector h1, which is then ﬁxed\nto obtain the hidden vector h2, and so on in a greedy way.\nLayers compute their representation as:\nhl+1=\u001b(Wlhl+bl) (1)\nfor layerl,0\u0014l < D whereDis the depth of the net-\nwork,\u001b(x)\u0011(1+e\u0000x)\u00001is the element-wise logistic sig-\nmoid function and Wl;blare respectively the weight and\nbias parameters for layer l. The whole network is ﬁnally\nﬁne-tuned with respect to a supervised criterion such as the\ncross-entropy cost:\nL(v(t);z(t)) =\u0000NX\nj=1z(t)\njlogy(t)\nj+(1\u0000z(t)\nj) log(1\u0000y(t)\nj)\n(2)\nwherey(t)\u0011hDis the prediction obtained at the top-\nmost layer and z(t)2f0;1gNis a binary vector servingas a target at time step t. Note that in the general multi-\nlabel framework, the target z(t)can have multiple active\nelements at a given time step.\n2.3 Exploiting prior information\nDuring ﬁne-tuning, it is possible to utilize prior informa-\ntion to guide optimization of the network by providing dif-\nferent variables, or intermediate targets , to be predicted at\ndifferent stages of training [13]. Intermediate targets are\nlower-level factors that the network should learn ﬁrst in or-\nder to succeed at more complex tasks. For example, chord\nrecognition is much easier if the active pitch classes, or\nchromagram targets , are known. Note that it is straightfor-\nward to transform chord labels z(t)into chromagram tar-\ngets~z(t)and vice versa using music theory. Our strategy\nto encourage the network to learn this prior information is\nto conduct ﬁne-tuning with respect to ~z(t)in a ﬁrst phase\nthen with respect to z(t)in a second phase, with all pa-\nrametersWl;blexcept for the last layer preserved between\nphases.\nWhile a DBN trained with target z(t)can readily pre-\ndict chord labels, we will rather use the last hidden layer\nh(t)\nD\u00001as inputx(t)to our RNN in order to take temporal\ninformation into account.\n2.4 Context\nWe can further help the DBN to utilize temporal informa-\ntion by directly supplementing it with tap delays and con-\ntext information. The retained strategy is to provide the\nnetwork with aggregated features \u0016x;~x[4] computed over\nwindows of varying sizes L[14] and offsets \u001crelative to\nthe current time step t:\n\u0016x(t)=nb(L\u00001)=2cX\n\u0001t=\u0000bL=2cx(t\u0000\u001c+\u0001t);8(L;\u001c)o\n(3)\n~x(t)=nb(L\u00001)=2cX\n\u0001t=\u0000bL=2c(x(t\u0000\u001c+\u0001t)\u0000\u0016x(t)\nL;\u001c)2;8(L;\u001c)o\n(4)\nfor mean and variance pooling, where the sums are taken\nelement-wise and the resulting vectors concatenated, and\nL;\u001c are taken from a predeﬁned list that optionally con-\ntains the original input ( L= 1;\u001c= 0). This strategy is\napplicable to frame-level classiﬁers such as the last layer\nof a DBN, and will enable fair comparisons with temporal\nmodels.\n3. RECURRENT NEURAL NETWORKS\n3.1 Deﬁnition\nThe RNN formally deﬁnes the conditional distribution of\nthe outputzgiven the input x:\nP(zjx) =TY\nt=1P(z(t)jA(t)) (5)z(2) ...z(T)\n...z(1)\nh(1)h(2)h(T)h(0)WhhWhz\nWzh\nx(1)x(2)x(T)WxhWxz\n...\nFigure 2 . Graphical structure of the RNN. Single arrows\nrepresent a deterministic function, dotted arrows represent\noptional connections for temporal smoothing, dashed ar-\nrows represent a prediction. The x!zconnections have\nbeen omitted for clarity at each time step except the last.\nwhereA(t)\u0011fx;z(\u001c)j\u001c < tgis the sequence history at\ntimet,x\u0011fx(t)gandz\u0011fz(t)2Cgare respectively the\ninput and output sequences (both are given during super-\nvised training), Cis the dictionary of possible chord labels\n(jCj=N), andP(z(t)jA(t))is the conditional probability\nof observing z(t)according to the model, deﬁned below in\nequation (9).\nA single-layer RNN with hidden units h(t)is deﬁned by\nits recurrence relation:\nh(t)=\u001b(Wzhz(t)+Whhh(t\u00001)+Wxhx(t)+bh)(6)\nwhere the indices of weight matrices and bias vectors have\nobvious meanings. Its graphical structure is illustrated in\nFigure 2.\nThe prediction y(t)is obtained from the hidden units\nat the previous time step h(t\u00001)and the current observa-\ntionx(t):\ny(t)=s(Whzh(t\u00001)+Wxzx(t)+bz) (7)\nwheres(a)is the softmax function of an activation vec-\ntora:\n(s(a))j\u0011exp(aj)PN\nj0=1exp(aj0); (8)\nand should be as close as possible to the target vector z(t).\nIn recognition problems with several classes, such as chord\nrecognition, the target is a one-hot vector and the likeli-\nhood of an observation is given by the dot product:\nP(z(t)jA(t)) =z(t)\u0001y(t): (9)\n3.2 Training\nThe RNN model can be trained by maximum likelihood\nwith the following cost (replacing eq. 2):\nL(x;z) =\u0000TX\nt=1log(z(t)\u0001y(t)) (10)\nwhere the gradient with respect to the model parameters is\nobtained by backpropagation through time (BPTT) [26].\nWhile in principle a properly trained RNN can describe\narbitrarily complex temporal dependencies at multiple time\nscales, in practice gradient-based training suffers from var-\nious pathologies [3]. Several strategies can be used to helpreduce these difﬁculties including gradient clipping, leaky\nintegration, sparsity and Nesterov momentum [2].\nIt may seem strange that the z(t)variable acts both as\na target to the prediction y(t)and as an input to the RNN.\nHow will these labels be obtained to drive the network dur-\ning testing? In the transduction framework [7, 12], the ob-\njective is to infer the sequence fz(t)\u0003gwith maximal prob-\nability given the input. The search for a global optimum\nis a difﬁcult problem addressed in the next section. Note\nthat the connections z!hare responsible for temporal\nsmoothing by forcing the predictions y(t)to be consistent\nwith the previous decisions fz(\u001c)j\u001c <tg. The special case\nWzh= 0gives rise to a recognition network without tem-\nporal smoothing.\nA potential difﬁculty with this training scenario stems\nfrom the fact that since zis known during training, the\nmodel might (understandably) assign more weight to the\nsymbolic information than the acoustic information. This\nform of teacher forcing during training could have dan-\ngerous consequences at test time, where the model is au-\ntonomous and may not be able to recover from past mis-\ntakes. The extent of this condition can be partly controlled\nby adding the regularization terms \u000b(jWxzj2+jWxhj2) +\n\f(jWhzj2+jWhhj2)to the objective function, where the\nhyperparameters \u000band\fare weighting coefﬁcients. It is\ntrivial to revise the stochastic gradient descent updates to\ntake those penalties into account.\n4. INFERENCE\nA distinctive feature of our architecture are the (optional)\nconnections z!hthat implicitly tie z(t)to its history\nA(t)and encourage coherence between successive output\nframes, and temporal smoothing in particular. At test time,\npredicting one time step z(t)requires the knowledge of the\nprevious decisions on z(\u001c)(for\u001c < t ) which are yet un-\ncertain (not chosen optimally), and proceeding in a greedy\nchronological manner does not necessarily yield conﬁgu-\nrations that maximize the likelihood of the complete se-\nquence. We rather favor a global search approach analo-\ngous to the Viterbi algorithm for discrete-state HMMs.\n4.1 Viterbi decoding\nThe simplest form of temporal smoothing is to use an HMM\non top of a frame-level classiﬁer. The HMM is a directed\ngraphical model deﬁned by its conditional independence\nrelations:\nP(x(t)jfx(\u001c);\u001c6=tg;z) =P(x(t)jz(t)) (11)\nP(z(t)jfz(\u001c);\u001c <tg) =P(z(t)jz(t\u00001)) (12)\nwhere the emission probability can be formulated using\nBayes’ rule [17]:\nP(x(t)jz(t))/P(z(t)jx(t))\nP(z(t))(13)\nwhereP(z(t)jx(t))is the output of the classiﬁer and con-\nstant terms given xhave been removed. Since the resultingjoint distribution\nP(z(t);x(t)jfz(\u001c);\u001c <tg)/P(z(t)jx(t))\nP(z(t))P(z(t)jz(t\u00001))\n(14)\ndepends only on z(t\u00001), it is easy to derive a recurrence\nrelation to optimize z\u0003by dynamic programming, giving\nrise to the well-known Viterbi algorithm.\n4.2 Beam search\nAn established algorithm for sequence transduction with\nRNNs is beam search (Algorithm 1) [7, 12]. Beam search\nis a breadth-ﬁrst tree search where only the wmost promis-\ning paths (or nodes) at depth tare kept for future examina-\ntion. In our case, a node at depth tcorresponds to a sub-\nsequence of length t, and all descendants of that node are\nassumed to share the same sequence history A(t+1); con-\nsequently, only z(t)is allowed to change among siblings.\nThis structure facilitates identifying the most promising\npaths by their cumulative log-likelihood. Note that w= 1\nreduces to a greedy search, and w=NTcorresponds to\nan exhaustive breadth-ﬁrst search.\nAlgorithm 1 BEAM SEARCH\nFind the most likely sequence fz(t)2Cj1\u0014t\u0014Tggiven\nxwith beam width w\u0014NT.\n1:q priority queue\n2:q.insert( 0;fg)\n3:fort= 1:::T do\n4:q0 priority queue of capacity w?\n5: forzinCdo\n6: forl;sinqdo\n7:q0.insert(l+ logP(z(t)=zjx;s);fs;zg)\n8:q q0\n9:returnq.max()\n?A priority queue of ﬁxed capacity wmaintains (at most) the w\nhighest values at all times.\n4.3 Dynamic programming\nA pathological condition that sometimes occurs with beam\nsearch is the exponential duplication of highly likely quasi-\nidentical paths differing only at a few time steps, that quickly\nsaturate beam width with essentially useless variations. In\nthat context, we propose a natural extension to beam search\nthat makes a better use of the available width wand results\nin better performance. The idea is to make a trade-off be-\ntween an RNN for which z(t)fully depends onA(t)but\nexact inference is intractable, and an HMM for which z(t)\nexplicitly depends only on z(t\u00001)but exact inference is in\nO(TN2).\nWe hypothesize that it is sufﬁcient to consider only the\nmost promising path out of all partial paths with identical\nz(t)when making a decision at time t. Under this assump-\ntion, any subsequence fz(t)\u0003jt\u0014T0gof the global opti-\nmumfz(t)\u0003gending at time T0<T must also be optimal\nunder the constraint z(T0)=z(T0)\u0003. Note that relaxingthis last constraint would lead to a greedy solution. Set-\ntingT0=T\u00001leads to the dynamic programming-like\n(DP) solution of keeping track of the Nmost likely paths\narriving at each possible label j2Cwith the recurrence\nrelation:\nl(t)\nj=l(t\u00001)\nk(t)\nj+P(z(t)=jjx;s(t\u00001)\nk(t)\nj) (15)\ns(t)\nj=fs(t\u00001)\nk(t)\nj;jg (16)\nwithk(t)\nj\u0011Nargmax\nk=1\u0002\nl(t\u00001)\nk+P(z(t)=jjx;s(t\u00001)\nk)\u0003\n(17)\nand initial conditions l(0)\nj= 0;s(0)\nj=fg, where the vari-\nablesl(t)\nj;s(t)\njrepresent respectively the maximal cumu-\nlative log-likelihood and the associated partial output se-\nquence ending with label jat timet(Algorithm 2). It is\nalso possible to keep only the w\u0014Nmost promising\npaths to mimic an effective beam width and to make the\nalgorithm very similar to beam search.\nAlgorithm 2 DYNAMIC PROGRAMMING INFERENCE\nFind the most likely sequence fz(t)2Cj1\u0014t\u0014Tggiven\nxwith effective width w\u0014N.\n1:q priority queue\n2:q.insert( 0;fg)\n3:fort= 1:::T do\n4:q0 priority queue of capacity w\n5: forzinCdo\n6:l;s argmax(l;s)2q\u0002\nl+ logP(z(t)=zjx;s)\u0003\n7:q0.insert(l+ logP(z(t)=zjx;s);fs;zg)\n8:q q0\n9:returnq.max()\nIt should not be misconstrued that the algorithm is lim-\nited to “local” or greedy decisions for two reasons: (1) the\ncomplete sequence history A(t)is relevant for the predic-\ntiony(t)at timet, and (2) a decision z(t)\u0003at timetcan\nbe affected by an observation x(t+\u000et)arbitrarily far in the\nfuture via backtracking , analogously to Viterbi decoding.\nNote also that the algorithm obviously does not guarantee\na globally optimal solution z\u0003, but is referred to as DP due\nto its strong similarity to the Viterbi recurrence relations.\n5. EXPERIMENTS\n5.1 Setup\nThis section describes experiments conducted on the dataset\nused in the MIREX audio chord estimation task1. Ground\ntruth time-aligned chord symbols were mapped to the ma-\njor/minor andfull chord dictionaries comprising respec-\ntively 25 and 121 chord labels:\n\u000fCmajmin\u0011fNg[f maj, ming\u0002S,\n\u000fCfull\u0011fNg[f maj, min, maj/3, maj/5, maj6, maj7,\nmin7, 7, dim, augg\u0002S,\n1http://www.music-ir.org/mirex/wiki/2012:\nAudio_Chord_EstimationwhereSrepresents the 12 pitch classes and ‘N’ is the no-\nchord label [16, 21]. This allows us to evaluate our algo-\nrithm at different precision levels. Evaluation at the ma-\njor/minor level is based on chord overlap ratio (OR) and\nweighted average OR (WAOR), standard denominations\nfor the average frame-level accuracy [22, 25].\nResults are reported using 3-fold cross-validation. For\neach of the 3 partitions, 25% of the training sequences are\nrandomly selected and held out for validation. The hyper-\nparameters of each model are selected over predetermined\nsearch grids to maximize validation accuracy and we re-\nport the ﬁnal performance on the test set. In all experi-\nments, we use 2 hidden layers of 200 units for the DBN,\n100 hidden units for the RNN, and 8 pooling windows with\n1\u0014L\u0014120s during pre-processing.\nIn order to compare our method against MIREX pre-\ntrained systems, we also train and test our model on the\nwhole dataset. It should be noted that this scenario is strongly\nprone to overﬁtting: from a machine learning perspective,\nit is trivial to design a non-parametric model performing at\n100% accuracy. The objective is to contrast our results to\npreviously published data, to analyze our models trained\nwith equivalent features, and to provide an upper bound on\nthe performance of the system.\n5.2 Results\nIn Table 1, we present the cross-validation accuracies ob-\ntained on the MIREX dataset at the major/minor level us-\ning a DBN ﬁne-tuned with chord labels z(DBN-1) and\nwith chromagram intermediate targets ~zand chord labels z\n(DBN-2), in addition to an RNN with DP inference. The\nDBN predictions are either not post-processed, smoothed\nwith a Gaussian kernel ( \u001b= 760 ms) or decoded with\nan HMM. The HPA [25] and DHMM [9] state-of-the-art\nmethods are also provided for comparison.\nModel Smoothing OR WAOR\nNone 65.8% 65.2%\nDBN-1 Kernel 75.2% 74.6%\nHMM 74.3% 74.2%\nNone 68.0% 67.3%\nDBN-2 Kernel 78.1% 77.6%\nHMM 77.3% 77.2%\nRNN DP 80.6% 80.4%\nHPA [25] HMM 79.4% 78.8%\nDHMM [9] HMM N/A 84.2%y\nTable 1 . Cross-validation accuracies obtained on the\nMIREX dataset using a DBN ﬁne-tuned with chord la-\nbelsz(DBN-1) and with chromagram intermediate tar-\ngets~zand chord labels z(DBN-2), an RNN with DP in-\nference, and the HPA [25] and DHMM [9] state-of-the-art\nmethods.y4-fold cross-validation result taken from [9].\nIt is clear that optimizing the DBN with chromagram\nintermediate targets ultimately increases the accuracy of\nthe classiﬁer, and that the RNN outperforms the simpler\nmodels in both OR and WAOR. We also observe that ker-nel smoothing (a simple form of low-pass ﬁltering) sur-\nprisingly outperforms the more sophisticated HMM ap-\nproach. As argued previously [8], the relatively poor per-\nformance of the HMM may be due to the context informa-\ntion added to the input x(t)in equations (3-4). When the\ninput includes information from neighboring frames, the\nindependence property (11) breaks down, making it dif-\nﬁcult to combine the classiﬁer with the language model\nin equation (14). Intuitively, multiplying the predictions\nP(z(t)jx(t))andP(z(t)jz(t\u00001))to estimate the joint dis-\ntribution will count certain factors twice since both models\nhave been trained separately. The RNN addresses this issue\nby directly predicting the probability P(z(t)jA(t))needed\nduring inference.\nWe now present a comparison between pre-trained mod-\nels in the MIREX major/minor task (Table 2), where the\nsuperiority of the RNN to the DBN-2 is apparent. The\nRNN also outperforms competing approaches, demonstrat-\ning a high ﬂexibility in describing temporal dependencies.\nSimilar results can be observed at the full chord level with\n121 labels (not shown).\nMethod OR WAOR\nChordino [22] 80.2% 79.5%\nGMM + HMM [20] 82.9% 81.6%\nHPA [25] 83.5% 82.7%\nProposed (DBN-2) 89.5% 89.8%\nProposed (RNN) 93.5% 93.6%\nTable 2 . Chord recognition performance (training error) of\ndifferent methods pre-trained on the MIREX dataset.\nTo illustrate the computational advantage of DP infer-\nence over beam search, we plot the WAOR as a function\nof beam width wfor both algorithms. Figure 3 shows that\nmaximal accuracy is reached with a much lower width for\nDP (w\u0003'10) than for beam search ( w\u0003>500). The for-\nmer can be run in 10 minutes on a single processor while\nthe latter requires 38 hours for the whole dataset. While\nthe time complexity of our algorithm is O(TNw )versus\nO(TNw logw)for beam search, the performance gain can\nbe mainly attributed to the possibility of signiﬁcantly re-\nducingwwhile preserving high accuracy. This is due to\nan efﬁcient pruning of similar paths ending at z(t), pre-\nsumably because the hypothesis stated in Section 4.3 holds\nwell in practice.\n6. CONCLUSION\nWe presented a comprehensive system for automatic chord\nrecognition from audio music, that is competitive with ex-\nisting state-of-the-art approaches. Our RNN model can\nlearn basic musical properties such as temporal continu-\nity, harmony and temporal dynamics, and efﬁciently search\nfor the most musically plausible chord sequences when the\naudio signal is ambiguous, noisy or weakly discriminative.\nOur DP algorithm enables real-time decoding in live situa-\ntions and would also be applicable to speech recognition.100101102103\nbeam width w82848688909294WAOR (%)\nRNN + beam\nRNN + DPFigure 3 . WAOR obtained on the MIREX dataset with\nthe beam search and dynamic programming algorithms as\na function of the (effective) beam width w.\n7. REFERENCES\n[1] Y . Bengio. Learning deep architectures for AI. Foun-\ndations and Trends in Machine Learning , 2(1):1–127,\n2009.\n[2] Y . Bengio, N. Boulanger-Lewandowski, and R. Pas-\ncanu. Advances in optimizing recurrent networks. In\nICASSP , 2013.\n[3] Y . Bengio, P. Simard, and P. Frasconi. Learning long-\nterm dependencies with gradient descent is difﬁcult.\nIEEE Trans. on Neural Networks , 5(2):157–166, 1994.\n[4] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. K ´egl. Aggregate features and adaboost for mu-\nsic classiﬁcation. Machine Learning , 65(2-3):473–484,\n2006.\n[5] S. B ¨ock and M. Schedl. Polyphonic piano note tran-\nscription with recurrent neural networks. In ICASSP ,\npages 121–124, 2012.\n[6] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent. Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic\nmusic generation and transcription. In ICML 29 , 2012.\n[7] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent. High-dimensional sequence transduction. In\nICASSP , 2013.\n[8] P. Brown. The acoustic-modeling problem in automatic\nspeech recognition . PhD thesis, Carnegie-Mellon Uni-\nversity, 1987.\n[9] R. Chen, W. Shen, A. Srinivasamurthy, and P. Chor-\ndia. Chord recognition using duration-explicit hidden\nMarkov models. In ISMIR , 2012.\n[10] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-\ndependent pre-trained deep neural networks for large-\nvocabulary speech recognition. IEEE Transactions on\nAudio, Speech, and Language Processing , 20(1):30–\n42, 2012.\n[11] D. Eck and J. Schmidhuber. Finding temporal structure\nin music: Blues improvisation with LSTM recurrent\nnetworks. In NNSP , pages 747–756, 2002.[12] A. Graves. Sequence transduction with recurrent neural\nnetworks. In ICML 29 , 2012.\n[13] C ¸ . G ¨ulc ¸ehre and Y . Bengio. Knowledge matters: Im-\nportance of prior information for optimization. ICLR ,\n2013.\n[14] P. Hamel, Y . Bengio, and D. Eck. Building musically-\nrelevant audio features through multiple timescale rep-\nresentations. In ISMIR , 2012.\n[15] P. Hamel and D. Eck. Learning Features from Music\nAudio with Deep Belief Networks. In ISMIR , pages\n339–344, 2010.\n[16] C. Harte. Towards automatic extraction of harmony in-\nformation from music signals . PhD thesis, University\nof London, 2010.\n[17] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed,\nN. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen, T. N.\nSainath, and B. Kingsbury. Deep neural networks for\nacoustic modeling in speech recognition. Signal Pro-\ncessing Magazine , 29(6):82–97, 2012.\n[18] G. Hinton, S. Osindero, and Y .-W. Teh. A fast learn-\ning algorithm for deep belief nets. Neural Computa-\ntion, 18:1527–1554, 2006.\n[19] E. J. Humphrey and J. P. Bello. Rethinking automatic\nchord recognition with convolutional neural networks.\nInICMLA 11 , volume 2, pages 357–362, 2012.\n[20] M. Khadkevich and M. Omologo. Time-frequency re-\nassigned features for automatic chord recognition. In\nICASSP , pages 181–184. IEEE, 2011.\n[21] M. Mauch. Automatic chord transcription from audio\nusing computational models of musical context . PhD\nthesis, University of London, 2010.\n[22] M. Mauch and S. Dixon. Approximate note transcrip-\ntion for the improved identiﬁcation of difﬁcult chords.\nInISMIR , pages 135–140, 2010.\n[23] M. C. Mozer. Neural network music composition by\nprediction. Connection Science , 6(2):247–280, 1994.\n[24] J. Nam, J. Ngiam, H. Lee, and M. Slaney. A\nclassiﬁcation-based polyphonic piano transcription ap-\nproach using learned feature representations. In ISMIR ,\n2011.\n[25] Y . Ni, M. McVicar, R. Santos-Rodr ´ıguez, and\nT. De Bie. An end-to-end machine learning system for\nharmonic analysis of music. Audio, Speech, and Lan-\nguage Processing , 20(6):1771–1783, 2012.\n[26] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning internal representations by error propagation.\nInParallel Dist. Proc. , pages 318–362. MIT Press,\n1986.\n[27] P. Smolensky. Information processing in dynamical\nsystems: Foundations of harmony theory. In Parallel\nDist. Proc. , pages 194–281. MIT Press, 1986."
    },
    {
        "title": "Placing Music Artists and Songs in Time Using Editorial Metadata and Web Mining Techniques.",
        "author": [
            "Dimitrios Bountouridis",
            "Remco C. Veltkamp",
            "Jan Van Balen"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1414948",
        "url": "https://doi.org/10.5281/zenodo.1414948",
        "ee": "https://zenodo.org/records/1414948/files/BountouridisVB13.pdf",
        "abstract": "This paper investigates the novel task of situating music artists and songs in time, thereby adding contextual information that typically correlates with an artist’s similarities, collaborations and influences. The proposed method makes use of editorial metadata in conjunction with web mining techniques, aiming to infer an artist’s productivity over time and estimate the original year of release of a song. Experimental evaluation over a set of Dutch and American music confirms the practicality and reliability of the proposed methods. As a consequence, large-scale correlational analyses between artist productivity and other musical characteristics (e.g. versatility, eminence) become possible.",
        "zenodo_id": 1414948,
        "dblp_key": "conf/ismir/BountouridisVB13",
        "keywords": [
            "situating music artists",
            "time",
            "contextual information",
            "editorial metadata",
            "web mining techniques",
            "artist productivity",
            "original year of release",
            "experimental evaluation",
            "correlational analyses",
            "musical characteristics"
        ],
        "content": "PLACING MUSIC ARTISTS AND SONGS IN TIME USING EDITORIALMETADATA AND WEB MINING TECHNIQUESDimitrios Bountouridis, Remco C. Veltkamp, Jan Van BalenUtrecht University, Department of Information and Computing Sciences{d.bountouridis,r.c.veltkamp,j.m.h.vanbalen}@uu.nlABSTRACTThis paper investigates the novel task of situating musicartists and songs in time, thereby adding contextual in-formation that typically correlates with an artist’s similar-ities, collaborations and inﬂuences. The proposed methodmakes use of editorial metadata in conjunction with webmining techniques, aiming to infer an artist’s productiv-ity over time and estimate the original year of release ofa song. Experimental evaluation over a set of Dutch andAmerican music conﬁrms the practicality and reliabilityof the proposed methods. As a consequence, large-scalecorrelational analyses between artist productivity and othermusical characteristics (e.g. versatility, eminence) becomepossible.1. INTRODUCTIONMany real-world music collections show a lack of meta-data when it comes to placing their constituent music enti-ties in a semantic or quantitative context. As a result, con-tent management and disclosure become challenging tasks.Meanwhile, in the emerging ﬁeld of the digital human-ities, well-documented collections are becoming increas-ingly essential for high quality research.The lack of contextual information has been typicallyaddressed by content-based approaches, where knowledgeis extracted after the actual audio is processed and anal-ysed. Contrarily, Web-Music Information Retrieval (MIR)techniques exploit the “wisdom of the crowd” and use theWeb or music metadata hubs in order to estimate the de-sired information. Typical applications include artist sim-ilarity [3, 9], classiﬁcation [10], country of origin determi-nation [8, 11] and many more exceeding our scope.This paper focuses on quantifying artists’ productivityover time, and estimating the original release time of songs.The productive period of a music artist is important infor-mation that is typically highly correlated to his style, in-ﬂuences and similarities to other artists. Teitelbaum et.al. [12] have shown that the activity span is strongly as-sociated with artist collaborations. As such, the productivePermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page.c\u00002013 International Society for Music Information Retrieval.years constitute a reliable, additional feature for variousMIR tasks: as the authors of [13] argue, listeners typicallyshow a certain affection for music related to particular pe-riods of their lives, and therefore time information couldact as a basis for music recommendation.The practical applications of productivity proﬁles ex-ceed the MIR domain. According to [5], productivity inabsolute terms may be the most important factor for a com-prehensive understanding of the creativity in music. Basedon that, Kozbelt [1] investigated the correlation betweenproductivity and musical characteristics such as versatilityand eminence. From a musicological perspective, a quan-titative representation of productivity can offer valuable in-sights in music trends, signiﬁcant musical events, signiﬁ-cant social events, and the mutual inﬂuence that may existbetween them.1.1 Problem Deﬁnition and Related WorkWe deﬁne a song’s year of release as the year on which itwas ﬁrst released in a recording. We further deﬁne as anartist’s productivity proﬁle (APP) the distribution of yearsin which the artist was alive and musically active, mean-ing recording and releasing albums, singles, etc. The pro-ductivity of an artist for a given year corresponds to thenumber of recorded songs released throughout that year.Time information regarding an artist’s output is typi-cally provided by services and hubs such as MusicBrainz1,Last.fm2, Allmusic.com3etc. in the form of editorialmetadata, i.e. data related to prescriptive knowledge aboutthe music [6], in addition to domain-free sources such asWikipedia. However, when dealing with old and lesser-known artists, any provided information is highly proba-ble to be erroneous, incomplete or even non-existent. Forexample, the MusicBrainz proﬁle for the popular Dutchsinger Willy Derby (1886-1944) includes a series of com-pilation albums, released after his death, and only 16 sin-gles out of his huge catalogue. To the best of our knowl-edge, the only published work aiming at automatically pro-viding time information for artists and songs, although in-side a recommendation framework, is [4]. Bogdanov andHerrera address the issue of determining a record’s orig-inal epoch, meaning the years when the music was ﬁrstrecorded, produced and consumed. This task is handled by1musicbrainz.org2ww.last.fm3www.allmusic.comﬁnding the release on Discogs4with the earliest date andby propagating it using a decreasing weighting scheme.Similarly to MusicBrainz though, Willy Derby’s Discogsproﬁle is limited (2 albums, 3 singles and one compilation)and therefore this approach is destined to fail.The only content-based method we know [13] acts asa proof of the Million Song Dataset’s applicability, andaims at estimating a song’s year of release based on itsaudio features. Such an approach shows a small mean er-ror of 6 years but actually estimates the year that the songwould best ﬁt in and not its actual year of release. Con-sidering that audio data for old and lesser-known artists,such as Willy Derby, is typically hard to ﬁnd, this methodcan be rendered useless in this context. The same holdsfor audio ﬁngerprinting methods such as [7] and commer-cial services such as Shazam5, which in addition face themetadata scarcity problem.Based on the previous, the need for a reliable methodthat overcomes the scarcity of the editorial metadata oflesser-known artists and is based on high-level metadataonly (e.g. artist name - song title), becomes apparent.1.2 ContributionThe contribution of this paper is multiple. First, it intro-duces the novel task of accurately placing music entities,and especially artists, in a time context. In addition, weprovide a publicly available testset. Secondly, it employsa methodology that combines both editorial metadata andweb mining techniques, a conjunction rarely investigated.The fusion of the different sources is aided by musicallymeaningful heuristics offering room for research. Thirdly,our method incorporates generic techniques for birth-dateand death-date estimation that can ﬁnd applications outsidethe music context.The remainder of this paper is organized as follows.Sections 3 and 4 describe our proposed method for theartist productivity proﬁle and year of release estimation re-spectively. Sections 5 and 6 summarize the evaluation andexperiment results, while section 7 presents our conclu-sions.2. GENERAL FRAMEWORKOur generic method for determining a person’s productiv-ity proﬁle (singer, actor, author etc.) is performed in var-ious steps. Web search engines are initially mined to pro-vide the “preliminary” productivity proﬁle. Secondly, ed-itorial metadata hubs are queried for time data about theperson’s life and works. This generates what we call a“shaping” proﬁle. The information gathered from this pro-cess is merged and applied on the ﬁrst, to attenuate anynoise and shape the ﬁnal productivity distribution (see Fig-ure 1). In the possible case of absent lifespan editorialmetadata, our method estimates the birth and death datesbased on the preliminary proﬁle.4www.discogs.com5www.shazam.comEditorial)Metadata)Web)Mining)\nDiscography)Lifespan)Ac9ve)Period)+x)x)Heuris9cs)Final)Produc9vity)Proﬁle)Es9mate?)(2))(1))\nPreliminary)produc9vity)proﬁle)Shaping)proﬁles)Figure 1. A graphical representation of the artist produc-tivity proﬁle estimation process. The initial Web-minedproﬁle is subsequently noise-ﬁltered, based on editorial (orestimated) medatada and heuristics.3. DETERMINING ARTIST PRODUCTIVITYPROFILE3.1 Editorial Metadata RetrievalGivenAan artist name andSAa set of song titles cor-responding to the artist’s recordings, we ﬁrst try to matchany of the tupleshA, sjiwheresj2SAto the databasesof Last.FM, EchoNest and MusicBrainz. Besides beingwell established and previously used by MIR researches,the employed metadata hubs provide convenient APIs. Thefollowing pieces of information are desirable for each artist:a)discography,b)lifespan andc)active years period.Given that we have retrieved two values correspondingto the start and end years (s,e) of an artist’s activity, wecreate a 130-bin proﬁlePact2[0,1]1⇥130, spanning theyears 1880 to 2010, with bothPact(s)andPact(e)set to 1.Similarly, we create a proﬁlePlifefor the lifespan data.APdiscproﬁle is also created for the discography data,but populating it is more sophisticated. The discographydata comprises of album names, song titles accompaniedby their release date and “release group” information. Ourmethod assumes that some release groups (e.g. singles) aremore reliable than others (e.g. compilations) with regardto the original date determination. GivenD={(year1,weight1),...,(yearn,we i gh tn)}the retrieved data from Mu-sicBrainz, withyeari2{1880,. . ,2010}andweighti2[0,1], thePdiscgets the following values:Pdisc(y)=X8i:yeari=yweightiNrelease groupi(1)Nrelease groupis a normalization factor corresponding to thenumber of recordings belonging to that particular releasegroup.3.2 Web MiningThe second step is concerned with identifying the Webpages related to the artist under consideration. Our methodqueries Google and Bing with the scheme “A+music” asin [8], and retrieves the 100 and 80 top-ranked URLs, de-noted as setsGandBrespectively. Fetching and indexing1880189019001910192019301940195019601970198019902000201000.20.40.60.81LifespanActive periodYearConfidence  Discography\n1880189019001910192019301940195019601970198019902000201000.20.40.60.81\nYearConfidence  TruthEstimateFigure 2. A representation of the three proﬁles, de-rived from editorial metadata, for the artist Corry Brokken.Discography data correspond to just one single (1960) andone compilation (2003).the webpages inB[Gare performed by the Apache Nutchweb crawler6and Apache Lucene7respectively.3.2.1 Proﬁle ConstructionAt this stage we aim at generating a probability distribu-tion that would best model the artist’s productivity as it isdocumented on the Web. This is performed using the fol-lowing:•Returned counts: The number of returned Lucenepagespcqfor queryq, acts a draft estimate of thequery’s relevance inside the pool of retrieved docu-ments.•Co-occurrence analysis:Considering returned countsas probabilities, the conditional probability of a termt1to co-occur in the same page ast2can be writtenasp(t1|t2)=pct1,t2/pct1.•Relevance score:Apache Lucene employs a sophis-ticated variation of thetf*idfand Boolean mode tocalculate the similarity between a queryqand a doc-umentd, denotedsim(q,d). The details of the so-called Lucene Practical Scoring Function8, are omit-ted here.For each yearyj2{1880,1881,. . . ,2010}we queryLucene with “A+yj”. Incorporating a proximity factorto the query, ensures that whenAandyjare separated byover 300 words, the document containing them would beconsidered irrelevant. The value of 300 was chosen basedon a set of preliminary experiments.Our method assigns a score to each query “A+yj” usingthe following formula:s(A, yj)=score(A, yj)score(A)(2)where:score(q)=max1<k<pcq[sim(q,dk)]⇥pcq(3)A proﬁlePwebof 130 bins is populated such thatPweb(yj)=s(A, yj)8yj2Y. Often, however, time information isnot explicitly stated. For instance, it is quite common for6nutch.apache.org7lucene.apache.org/core8lucene.apache.org/core/oldversioneddocs/versions/350/api/all/org/apache/lucene/search/Similarity.htmlan artist that was active during the period 1980-1990, tobe considered and identiﬁed as an “80’s” artist. Based onthis, we have identiﬁed a set of termsTthat semanticallycorrespond to decades. For example, the terms “1960’s”,“sixties”, “60’s”, “jaren 60” and “jaren zestig” correspondto the period 1960-1970. The two latter country-speciﬁcterms are introduced manually but can be automated basedon a country of origin estimation process [8,11]. Thereforeﬁnally, we query Lucene with “A+tj”, wheretj2T, andthen increase the value of the ten corresponding bins bys(A, tj)⇥0.1.3.3 Proﬁle FusionBy the end of the previous process, the system has acquiredfour separate proﬁles (Pact,Pdisc,PlifeandPweb). We com-bine those pieces of information in a two-step procedure.It is very likely for the discography to be incomplete forapparent reasons. Our method tries to compensate for thatfact by smoothingPdiscwith a sized-5 Gaussian window.A proﬁlePfis later created such that its bin values hold theweighted sum of the normalizedPwebandPdisc.The system exploits lifespan and active years data bysetting all the coefﬁcients ofPfthat fall outside thePlife,Pactboundaries, to zero. Despite this process, thePfdatainside the lifespan or active period may still contain a sig-niﬁcant amount of noise. Further noise removal is based onthe observation that an artist’s productivity is usually max-imized during his ﬁrst 20 years of adulthood. This is sup-ported by the dotted line in Figure 3, which represents thedistribution of single-type releases across the artist’s lifes-pan, as generated from a Musicbrainz subset of 518, pre-1950’s artists. This behaviour is modelled by our methodas an envelope-probability density functionW(solid lineFigure 3). Our envelope’s decay slope is less steep in or-der to accommodate for non-single releases.Wis alignedwith the artist’s birth-date, as provided byPlife, and usedto weighPfwhich constitutes the ﬁnal artist productivityproﬁle estimate.\n010203040506070809010000.10.20.30.40.50.60.70.80.91\nYears after birthProbability of productivityFigure 3. (Solid line) the probability density functionWfor artist productivity. (Dotted line) the distribution ofsingle-type releases across lifespan.3.3.1 Birth-Date & Death-Date EstimationAs previously mentioned, it occurs very often that no lifes-pan information can be found. In this case we usePwebwhich ideally ampliﬁes important years in the life of anartist, represented as sharp peaks. For the birth-date esti-mate, the idea is to locate those and pick the one that bestﬁts our productivity probability assumptions, as modelledbyW. This is achieved by traversing and aligningWwitheach found peak and then multiplying it withPweb. Thepeak that yields the largest area under the proﬁle is consid-ered the birth-date estimate.The selection of the peak candidates is a critical pro-cedure, considering that the noise-to-signal ratio can besigniﬁcantly high. Our method aims at emphasising thosepeaks that exhibit high surprisingness or unexpectedness,in contrast to noise-generated peaks. This is achieved byscanning thePwebproﬁle from left to right; the coefﬁcientsof the surprisingness vectorSv2[0,1]1⇥130are then com-puted such that:Sv(j)=Pweb(j)Pj\u00001k=1Pweb(k)(4)Peak candidates are then selected using a simple threshold-ing function (Figure 4).\nFigure 4.Pweband its surprisingness vectorSv(solid line).The more data is processed (from left to right) the less sur-prising peaks become.Death-date estimation is based on the assumption thatthe artist’s productivity, as documented on the Web, willbe limited or non-existent after his death. Noise however,challenges this assumption; therefore our method uses apeak-picking technique that employs the following piecesof information and heuristics: 1) Birth-date, estimated orknown, 2) max life expectancy, set to 90 years and 3) aP0webproﬁle generated with a proximity factor of 10 (in-stead of 300) which aims at capturing co-occurrences in-stances of the type “Artist (Year of birth - Year of death)”.The peak picking is done as follows: by using a sim-ple thresholding function onP0web, we ﬁrstly pick the peakcandidatesp. Each one of them is assigned a conﬁdence-probability valueprob(p)based on its distance from thebirth-date; assuming that peaks far away from the birth-date have higher chance of corresponding to death-date.Ideally, the peak that maximizes the ratio between dis-tributions to its left and to its right should correspond tothe correct death-date. However, considering once againthe noise and its non-uniform distribution (e.g. album re-releases after the artist’s death), a more sophisticated tech-nique is required. Our method, aiming at capturing theshort and long term distributions around each peak, createsfour sub-proﬁles fromPweb, centred onp, with windowsizes of 40 and 10 (P40left,P40right,P10left,P10right). Each candidatepis then assigned a value such that:vp= mean(P40left)mean(P40right)+mean(P10left)mean(P10right)!⇥prob(p)(5)The candidate with the maximumvpis considered the death-date estimate.We evaluated our lifespan estimation method on a test-set of 100 MusicBrainz artists with known lifespans, rang-ing from 1880 to 2000. The mean error for the birth-dateand death-date was estimated at approximately 11.5 and10.6 years respectively. This might not be exactly accu-rate, yet the death-date estimation is not a goal in itself,rather a pragmatic strategy to remove some of the noise inthe proﬁles.4. DETERMINING YEAR OF RELEASEEstimating year of release is based on a similar approach asthe one employed for artist productivity proﬁles. The basicidea is to create four separate proﬁles for eachhA, sji:arelease information proﬁle from MusicBrainz, a productiv-ity proﬁle forAand two Web proﬁles forsjandA+sjrespectively. Processing and fusing them is the ﬁnal step.We ﬁrst query the metadatabases with tuples of the formhA, sjiin order to retrieve the discography. It is now easyto search into the discography ofAfor any of the songs inSA. If there is a match, the release group is “Single” andthe year of releaseyis available, then a proﬁlePdiscfor thesong in hand is populated, such thatPdisc(y)=1.During web-mining, two set of queries for each tuplehA, sjiare applied, and more speciﬁcally “sj+yeark”and “sj+A+yeark” . Eventually two proﬁles,Pweb,sandPweb,s+A, are calculated.Calculating the year of release estimate is based on theassumption that the input vectors correspond to mixturecomponents. Their conical sumPCSis:PCS=WvT⇥[Pweb,s,Pweb,s+A,Pf,A,Pdisc](6)withWv=[w1,w2,w3,w4]the weight vector andPf,Athe artist’s productivity proﬁle. Finding the optimal weightsis solved with the employment of a genetic algorithm on atraining set. The ﬁnal year of release estimate is just thePCScoefﬁcient with the maximum corresponding value.5. EXPERIMENT5.1 Test CollectionThere exists no standardized or previously used data setfor this kind of task, therefore we built one from scratch9.Note that the requirements are very speciﬁc: on the onehand, an evaluation dataset of artist productivity proﬁlesshould only include complete or near-complete discogra-phies, and on the other hand, should focus enough on themore challenging artists for which no complete cataloguesare readily available on the web. The low commercial dis-tribution and rarity of pre-60’s music was ideal for our9Available at www.projects.science.uu.nl/COGITCHpurposes; therefore, we manually gathered from our per-sonal music collection, books [2] and “deep” Web sources,639 Dutch and American song titles, corresponding to 15artists (see Table 1), accompanied by original release dates,ranging between the period of 1900 to 1959. Overall, 26832documents were downloaded and indexed for the year ofrelease and 3391 for artist productivity proﬁle includinga set of “noise” webpages (irrelevant to the artists them-selves).The difﬁculty in assessing the “obscurity” of an artistand the amount of effort required to gather and cross-checkhis complete discography without using the Web, resultedin a rather small artist productivity proﬁle test set. It shouldbe noted that a direct comparison of our method to oth-ers is unfortunately impossible. The work of [4] exploitsDiscogs which offers limited, or non-existent, data for ourparticular testset. Regarding year of release estimation, thework of [13] uses purely audio features which are almostimpossible to acquire, considering our method’s prerequi-site for music rarity and oldness.5.2 Evaluation MeasuresFor the artist productivity proﬁles the idea is to examinethe overlap between the ground truthAPPand estimatedAPP⇤distributions. This is achieved by using precision,recall and their harmonic mean, usually called F-measure.In addition, given the mean values of both proﬁles’ Gaus-sian ﬁts, denotedmandm⇤respectively, we computeEr=|m\u0000m⇤|as a measure of the error in terms of time contextplacement.GivenN, the number of songs inS,Yo Rsjthe truerelease date of the songsjandYo R⇤sjthe estimate, “Ac-curacy” for a year-window of sizexis deﬁned by the fol-lowing formula:Accuracyx=Psj2Sfx(Yo R⇤sj,Yo Rsj)N(7)wherefx(t, e)=⇢1for|t\u0000e|x0for|t\u0000e|>x(8)6. RESULTS AND EVALUATIONThe results for the artist productivity proﬁle task are pre-sented in Table 1. Our approach shows low error withregard to the time context placement. It is worth exam-ining certain illustrative cases, starting with “August DeLaat” (Figure 5), which presents one of the lowest preci-sions. The artist is well placed into the time context butour approach assumes a considerable amount of produc-tivity from 1940 to 1954. This misbehaviour relies on thefact that even after De Laat’s last recording in 1941, heremained active in non-music areas such as theatre10.In contrast to the previous case, “Bob Scholte” showsboth accurate time-context placement and distribution mod-elling (Figure 6). In the case of “Louis Davids” (1883-1939) presented in Figure 7, lifespan or active years infor-mation from the metadabases was unavailable. Our method10www.thuisinbrabant.nl/personen/l/laat,-august-deestimated the correct birth and death dates by performingthe peak picking algorithm presented in 3.3.1. Filtering theproﬁle by applying the productivity assumptions, as mod-elled byW, also attenuated a considerable amount of noiseright after the artist’s birth.Artist NamePrecision%Recall%F%ErAugust De Laat48.3683.9761.382.45Bob Scholte65.190.8275.841.15Kees Pruis73.8680.2176.91.45Lou Bandy51.997.9767.853.95Louis Davids50.299.8566.810.11Willy Derby90.2181.9385.871.25B. Schoepen52.7585.1165.136.02Cole Porter66.3388.275.721.94Corry Brokken81.8988.6585.141.21Eddy Christiani85.0678.2281.490.56George Gershwin61.3183.6470.762.66Harold Arlen81.668.3674.395.69Jerome Kern78.1163.1269.823.95Richard Rodgers46.7497.1363.117.55Wim Sonneveld55.7495.8570.491.01Mean65.586.13572.6652.56Table 1. Precision, recall and F-measure for the 15 artistsin the test set.Table 2 presents theAccuracyxfor the year-of-releaseestimation task for windows ranging from 1 to 5. The meanerror is 2.91 years. As a general evaluation measure weconsiderAccuracy2, assuming that this level of detail isappropriate for artists of the era 1900 - 1959. Therefore,for a 2-year window around 81% of the cases are identiﬁedas hits; signiﬁcantly outperforming the random, baselineestimation (mean error = 33.4,Accuracy2=8.92%).Window Size12345Accuracy0.660.8160.8560.8880.907Table 2. Accuracy for window size ranging from 1 to 5.7. CONCLUSIONSThe aim of this report has been to determine an artist’s pro-ductivity proﬁle and a song’s original year of release. Ourapproach is based on the exploitation of editorial metadatafrom sources such as MusicBrainz and EchoNest, in ad-dition to Web harvested data. The evaluation demonstratesthe strength of the proposed method for year of release esti-mation; around 81% of the estimates fall within a±2-yearwindow, with a mean error of 2.91 years.The results for determining productivity proﬁles are lessimpressive though it should be noted that the ground truthgeneration assumes complete knowledge of the artist’s discog-raphy, which is not always the case. In fact, in the cases forwhich we are most certain we have the complete discogra-phies, modelling the productivity distribution is accurate.With our novel methods, it is possible for the ﬁrst timeto perform large-scale, correlational analysis between pro-ductivity and various musical characteristics. Therefore,work such as [1, 5], which is based on manually gatheredclassical music, cannot only be signiﬁcantly aided but alsoexpanded to include artists from various eras and of vary-ing popularity.Given certain enhancements and modiﬁcations, our ap-proach can be generalized to accommodate non-music do-mains. Wikipedia instead of MusicBrainz or EchoNest canbe employed for determining an author’s productivity pro-ﬁle or the original dates of his publications. Metadata hubssuch as IMDB11can be used to extend our approach in themovies domain as well.\n1880189019001910192019301940195019601970198019902000201000.20.40.60.81Lifespan\nYearConfidence  WebDiscography\n1880189019001910192019301940195019601970198019902000201000.20.40.60.81\nYearConfidence  TruthEstimateFigure 5. (Top) the input proﬁles, (bottom) the groundtruth against the estimated artist productivity proﬁle (APP)for August de Laat.\n1880189019001910192019301940195019601970198019902000201000.20.40.60.81LifespanActive period\nYearConfidence  WebDiscography\n1880189019001910192019301940195019601970198019902000201000.20.40.60.81\nYearConfidence  TruthEstimateFigure 6. (Top) the input proﬁles, (bottom) the groundtruth against the estimated APP for Bob Scholte.8. ACKNOWLEDGMENTSThis research is supported by the NWO CATCH projectCOGITCH (640.005.004), and the FES project COMMIT/.The authors would like to thank Frans Wiering for his use-ful comments.9. REFERENCES[1] A. Kozbelt: “Performance time productivity and versa-tility estimates for 102 classical composers,”Psychol-ogy of Music, V ol. 37, No. 1, pp. 25-46, 2009.[2] A. Wilder:American Popular Song: The Great Innova-tors, 1900 - 1950, Oxford University Press, New York,1990.11www.imdb.com1880189019001910192019301940195019601970198019902000201000.20.40.60.81\nYearConfidence  WebDiscography\n1880189019001910192019301940195019601970198019902000201000.20.40.60.81\nYearConfidence  TruthEstimateFigure 7. (Top) the input proﬁles, (bottom) the groundtruth against the estimated APP for Louis Davids.[3] B. Whitman and S. Lawrence: “Inferring descriptionsand similarity for music from community metadata,”ICMC Conference Proceedings, pp. 591-598, 2002.[4] D. Bogdanov and P. Herrera: “Taking advantage of ed-itorial metadata to recommend music,”CMMR Con-ference Proceedings, pp. 618-632, 2012.[5] D. K. Simonton: “Creative productivity: a predictiveand explanatory model of career landmarks and trajec-tories,”Psycological Review, V ol. 104, No. 1, pp. 66-89, 1997.[6] F. Pachet, A. La Burthe, J. Aucouturier and A.Beurive: “Editorial metadata in electronic music dis-tribution systems: between universalism and isolation-ism,”Journal of New Music Research, V ol. 34, No. 2,pp. 173-184, 2005.[7] J. Haitsma and T. Kalker: “A highly robust audio ﬁn-geprinting system,”ISMIR Conference Proceedings,pp. 107-115, 2002.[8] M. Schedl, C. Schiketanz and K. Seyerlehner: “Coun-try of origin determination via web mining te-chiniques,”AdMIRe Proceedings, pp. 1451-1456,2010.[9] M. Schedl and D. Hauger: “Mining microblogs to infermusic artist similarity and cultural listening patterns,”WWW Conference Proceedings, pp. 877-886, 2012.[10] P. Knees, E. Pampalk and G. Widmer: “Artist classi-ﬁcation with web-based data,”ISMIR Conference Pro-ceedings, pp. 517-524, 2004.[11] S. Govaerts and E. Duval: “A web-based approach todetermine the origin of an artist,”ISMIR ConferenceProceedings, pp. 261-266, 2009.[12] T. Teitelbaum, P. Balenzuela, P. Cano, and J. M. Buldu:“Community structures and role detection in music net-works,”Chaos journal, V ol. 18, No. 4, pp. 043105,2005.[13] T. Bertin-Mahieux, D.P.W. Ellis, B. Whitman and P.Lamere: “The million song dataset,”ISMIR Confer-ence Proceedings, pp. 591-596, 2011."
    },
    {
        "title": "Source Separation of Polyphonic Music with Interactive User-Feedback on a Piano Roll Display.",
        "author": [
            "Nicholas J. Bryan",
            "Gautham J. Mysore",
            "Ge Wang 0002"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418247",
        "url": "https://doi.org/10.5281/zenodo.1418247",
        "ee": "https://zenodo.org/records/1418247/files/BryanMW13.pdf",
        "abstract": "The task of separating a single recording of a polyphonic instrument (e.g. piano, guitar, etc.) into distinctive pitch tracks is challenging. One promising class of methods to accomplish this task is based on non-negative matrix factorization (NMF). Such methods, however, are still far from perfect. Distinct pitches from a single instrument have similar timbre, similar note attacks, and contain overlapping harmonics that all make separation difficult. In an attempt to overcome these issues, we use a database of synthesized piano and guitar recordings to learn the harmonic structure of distinct pitches, perform NMF-based separation, and then extend the method to allow an end-user to interactively correct for errors in the output separation estimates by drawing on a piano roll display of the separated tracks. The user-annotations are mapped to linear grouping regularization parameters within a modified NMF-based algorithm and are then used to refine the separation estimates in an iterative manner. For evaluation, a prototype user-interface was built and used to separate several polyphonic guitar and piano recordings. Initial results show that the method of interactive feedback can significantly increase the separation quality and produce high-quality separation results.",
        "zenodo_id": 1418247,
        "dblp_key": "conf/ismir/BryanMW13",
        "keywords": [
            "polyphonic instrument",
            "pitch tracks",
            "non-negative matrix factorization",
            "timbre",
            "note attacks",
            "harmonics",
            "synthesized piano and guitar recordings",
            "end-user interaction",
            "piano roll display",
            "linear grouping regularization parameters"
        ],
        "content": "SOURCE SEPARATION OF POLYPHONIC MUSIC WITH\nINTERACTIVE USER-FEEDBACK ON A PIANO ROLL DISPLAY\nNicholas J. Bryan\nCCRMA, Stanford University\nnjb@ccrma.stanford.eduGautham J. Mysore\nAdobe Research\ngmysore@adobe.comGe Wang\nCCRMA, Stanford University\nge@ccrma.stanford.edu\nABSTRACT\nThe task of separating a single recording of a polyphonic\ninstrument (e.g. piano, guitar, etc.) into distinctive pitch\ntracks is challenging. One promising class of methods\nto accomplish this task is based on non-negative matrix\nfactorization (NMF). Such methods, however, are still far\nfrom perfect. Distinct pitches from a single instrument\nhave similar timbre, similar note attacks, and contain over-\nlapping harmonics that all make separation difﬁcult. In an\nattempt to overcome these issues, we use a database of syn-\nthesized piano and guitar recordings to learn the harmonic\nstructure of distinct pitches, perform NMF-based separa-\ntion, and then extend the method to allow an end-user to\ninteractively correct for errors in the output separation es-\ntimates by drawing on a piano roll display of the separated\ntracks. The user-annotations are mapped to linear grouping\nregularization parameters within a modiﬁed NMF-based\nalgorithm and are then used to reﬁne the separation esti-\nmates in an iterative manner. For evaluation, a prototype\nuser-interface was built and used to separate several poly-\nphonic guitar and piano recordings. Initial results show\nthat the method of interactive feedback can signiﬁcantly\nincrease the separation quality and produce high-quality\nseparation results.\n1. INTRODUCTION\nFor many audio editing and production tasks, it is desirable\nto separate a single recording of a polyphonic instrument\ninto its respective pitch tracks. One promising method to\ndo so is that of non-negative matrix factorization (NMF),\nwhich models audio spectrogram data as a linear combina-\ntion of prototypical frequency components or basis vectors\nover time. NMF can be deﬁned by\nV⇡WH (1)\nwhere V2RF⇥T\n+is an audio spectrogram, W2RF⇥K\n+\nis a dictionary or matrix of basis vectors (columns), and\nH2RK⇥T\n+is a matrix of activations or gain vectors (rows).\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\u00002013 International Society for Music Information Retrieval.\nFigure 1: Polyphonic source separation of a single pianorecording (blue) ofMary Had A Little Lambdisplayed on apiano roll. Given an initial separation, a user can annotateerrors (yellow overlays) in the separated outputs (red) anditeratively improve results. Note underneath the overlays,incorrect, residual energy is present.Given a spectrogramV, the matricesWand/orHcan thenbe computed via an optimization problemminimizeW,HD(V|WH)subject toW\u00000,H\u00000(2)that minimizes the distance betweenVandWH, whereDis a suitable divergence function (e.g. Euclidean,Kullback-Leibler, Itakura-Saito, etc.) and the inequalitiesare element-wise [5,6,10,11,14]. Note, (2) is non-convex,typically allowing us to only ﬁnd a local optima.When used to separate a single polyphonic recordinginto distinctive pitch tracks (e.g. 88 keys of the piano),typically supervised NMF is used. In this case, isolatedrecordings of distinct pitches are used to model the proto-typical frequency content of each pitch. The pre-learnedmodels are then collected together and used to estimate thecontribution of each pitch within an unknown mixture.These methods can sometimes produce high-quality sep-aration results. At the same time, however, these methodscan be frustrating in that the resulting separation output es-timates can contain errors that are audibly and/or visuallyobvious. These errors are typically caused by overlappingharmonics, similarities in timbre, similarities in note at-tack, and other such issues, limiting the general usefulnessof the method.To overcome these limitations, we propose an extension\nto supervised NMF-based source separation. In particu-\nlar, we allow an end-user to separate a single recording\ninto distinctive pitch tracks and then interactively annotate\nerrors in the output estimates by drawing on a piano roll\ndisplay of the separated tracks, as shown in Fig. 1. The\nuser-annotations are then mapped to linear grouping regu-\nlarization parameters in a modiﬁed NMF-based algorithm\nto reﬁne the separation estimates and iteratively improve\nresults through user-feedback.\nTo initially train our supervised NMF model, we lever-\nage a database of synthesized pianos and guitars to learn\nthe harmonic structure of distinct pitches. Using the entire\ndatabase, we learn a universal pitch model across all in-\nstruments and timbres. Additionally, we learn instrument-\nspeciﬁc pitch models, and instrument/timbre-speciﬁc mod-\nels that can be used in place of the universal model if needed.\nFor evaluation, we built a prototype user-interface and used\nit to separate several polyphonic guitar and piano record-\nings. Initial results shows that the proposed method signif-\nicantly improves separation quality and can produce high-\nquality separation estimates.\nThe complete proposed method consists of an initial\npre-computation step discussed in Section 2, and two core\nsteps in Section 3, and Section 4. Algorithmic issues, eval-\nuation, and related works are discussed in Section 5, Sec-\ntion 6, and Section 7, followed by acknowledgements, and\nconclusions in Section 9, and Section 8.\n2. LEARNING PITCH MODELS\nTo separate an unknown polyphonic instrument recording,\nwe must ﬁrst precompute or learn the prototypical frequency\ncontent for each pitch p21,. . . ,P we wish to separate. For\na given pitch p, we learn one or more ( Kp) prototypical\nspectra or basis vectors that capture the harmonic structure\nof that pitch. While this can be achieved by handcrafting\nspeciﬁc spectra, we instead learn this from data by the fol-\nlowing supervised NMF procedure:\n1.Given isolated training data of each pitch p, compute\nthe spectrogram Vp,8p21,. . . ,P via the short-\ntime Fourier Transform (STFT).\n2.Factorize each spectrogram Vpvia (2), and obtain\nthe basis vectors Wp2RF⇥Kp\n+ of each source,\nwhere Kpis the number of basis vectors per pitch.\nNormalize each column of Wpto sum to one. Dis-\ncard the activations Hp2RKp⇥T.\n3.Concatenate the basis vectors Wptogether to form\nthe complete pitch model or dictionary\nW=[W1W2...WP]2RF⇥K\n+, where\nK=PP\n1Kp.\nFor this work, we deﬁne Dto be the Kullback-Leibler (KL)\ndivergence and use the multiplicative NMF-update algo-\nrithm of Lee and Seung [6] to solve (2). Note, alterna-\ntive divergence functions can be used instead, such as the012345678910Frequency (kHz)\nStudent Version of MATLABFigure 2 : One octave of acoustic guitar pitch-based basis\nvectors for Kp=1. Notice the harmonic structure.\nItakura-Saito divergence. An example set of guitar basis\nvectors for one octave ( Kp=1) is shown in Fig. 2.\nFor training data, we used 13 distinct piano synthesiz-\ners and 11 distinct guitar synthesizers and recorded two\nmeasures of quarter notes ( ⇡5 seconds) for each of the 88\npiano pitches of each of the 24 synthesizers. The different\nsynthesizers have contrasting timbres, reverberation, and\nother effects and correspond to the guitar and piano pre-\nsets of the Logic Pro software package. The piano timbres\ninclude: electric piano, grand piano, grand piano on stage,\ngrand piano punchy, smokey clav, smooth clav, studio pop\npiano, swirling electric piano, whirly, yamaha piano club,\nyamaha piano hall, yamaha piano room, yamaha piano stu-\ndio. The guitar timbres include: acoustic guitar, big elec-\ntric lead, classical acoustic, clean electric, crunchy muted\ndelays, electric tremolo, fuzzy synth guitar, heavy metal\nguitar, los freakos, nylon shimmer, steel string acoustic.\nGiven the collection of recordings, we then use vari-\nous subsets of the data and the aforementioned supervised\nNMF procedure several times to compute various pitch\nmodels. By learning a pitch model on the complete set of\nrecordings across all instruments and timbres, for example,\nwe learn a form of universal pitch model (U) that general-\nizes across instruments and instrument timbre, similar in\nmotivation to the work of Reynolds et al. [9] and more re-\ncently Sun and Mysore [12]. When using such a model to\nperform separation, however, the results might be less than\nideal because of the difference in harmonic structure be-\ntween instruments (e.g. guitar vs. piano). This motivates\nthe ability to train more speciﬁc pitch models on subsets of\nour training data. As a result, in addition to computing a\ngeneral, universal pitch model, we additionally compute a\nuniversal guitar model (UG), a universal piano model (UP),\nand all 24 instrument/timbre-speciﬁc models (T).\n3. MIXTURE SEPARATION\nGiven a particular pitch model, we can proceed to sepa-\nrate an unknown polyphonic mixture sound. This involves\nusing a single, complete pitch model Wto estimate the\nweights or activations Hof each pitch from the unknown\nrecording spectrogram V. This is done via\nminimize\nHD(V|WH)\nsubject to H\u00000(3)Student Version of MATLABFigure 3 : User-annotated penalty weight parameters ⇤\nfrom Fig. 1, depicted as an image. White represents an-\nnotated regions. Black represents unannotated regions.\nwhere we only optimize over H2RK⇥T\n+. Assuming D\nis convex, this optimization problem is convex, allowing us\nto ﬁnd a global minimizer. We again use the KL divergence\n(which is convex) and the multiplicative update algorithm\nof [6] to solve (3), holding Wﬁxed. We then use the given\npitch model and corresponding activations to estimate the\nmagnitude spectrogram of each pitch within the mixture.\nThe estimated pitch spectrograms are then converted to the\ntime-domain using the mixture phase and inverse STFT ac-\ncording to standard practice (see Section 5).\n4. INTERACTIVE USER-FEEDBACK\nOnce an initial separation is performed, we allow an end-\nuser to interactively reﬁne the separated output estimates\nby annotating a piano roll display of the results. To do\nso, we 1) instruct an end-user to draw on regions of each\npitch track that are incorrectly separated, 2) incorporate the\nannotations to update the separation estimates, 3) present\nthe updated results back to the user, and 4) repeat until\nsatisﬁed. This form of interaction is done as a result of the\nobservation that it is much easier for people to iteratively\ncorrect for errors after an initial result is presented, rather\nthen pre-annotate time regions of one source or another.\nThis is similar to the observations discussed in [3], where\nuser-feedback is used to improve a clustering algorithm.\nThe speciﬁc type of drawing interaction can be done in\nseveral ways, such as 1) annotating a type of amplitude en-\nvelope for each pitch track, where height is used as a mea-\nsure of conﬁdence/strength of the error annotation or 2) al-\nlowing a user to paint over the errors with a colored brush,\nwhere opacity is used as a measure of conﬁdence/strength.\nThe drawing annotations are then collected into a sin-\ngle matrix ⇤2RP⇥T, where each row corresponds to\nthe penalties for a given pitch. This matrix is then used to\npenalize the activations of the incorrectly activated pitches\nfrom the initial separation estimates. As a result, we can re-\nmove errors caused by incorrectly activated notes and re-\nallocate the incorrectly assigned energy to the remaining\npitch tracks in an optimal way. This is in contrast to di-\nrectly using the annotations to down-weight the appropri-\nate elements of Hwithout recomputing the factorization,\nwhich would not reassign the incorrectly allocated energy.\nAn example penalty matrix ⇤is shown in Fig. 3, which\nFigure 4: Reﬁned polyphonic separation ofMary Had ALittle Lambwith user-guided interactive feedback. Noticeincorrectly activated notes are eliminated and the note at-tacks are more sharply outlined.embodies the user annotations, shown as yellow overlays,in Fig. 1. Note that while Fig. 3 appears binary in nature,⇤is real-valued (the beneﬁt of real-valued annotations isdiscussed below).The user-annotation matrix⇤is then incorporated intoour NMF model viaminimizeHD(V|WH)+\u0000⌦(H;⇤)subject toH\u00000(4)⌦(H;⇤)is an appropriately chosen penalty that is a func-tion ofHand parameterized by⇤and\u00002R+is a scalarused to decrease or increase the overall weight of the userannotation penalty. The penalty function⌦(H;⇤)dis-courages the activations of speciﬁc pitches, dependent onthe user-annotations. For our case, we use a simple lineargroup penalty⌦(H;⇤)=TXt=1PXp=1pKpXk=(p\u00001)Kp+1⇤(p,t)H(k,t)(5)=1KT(\u0000⇤\u0000H)1T(6)where the matrix subscripts are used to index the rows orcolumns of the given matrices,\u00002RK⇥Pis\u0000=266641Kp0...001Kp...0.........00 01Kp37775,(7)K=PP1Kp, and1N2RN⇥1is a column vector ofNones. While alternative penalties are possible, (6) is rel-atively straightforward, adds minimal computation com-plexity, and results in a compact multiplicative update al-gorithm for solving (4) as discussed below in Section 5.Also note, due to linearity, we can absorb\u0000into the userannotation matrix⇤and only use⇤for user-tuning.We can see the immediate beneﬁt of the interactive user-feedback in two demonstrative examples. First, Fig. 4 il-lustrates the output result of including the user-annotationsStudent Version of MATLAB\nStudent Version of MATLAB\nStudent Version of MATLAB\nStudent Version of MATLABFigure 5 : (First Row) Mixture spectrogram of E and D\npiano pitches. (Second Row) Initially separated results (E-\nleft, D-right) using supervised NMF. (Third Row) Initially\nseparated results (E-left, D-right) using supervised NMF\nwith overlaid annotations (red-line) depicting incorrectly\nseparated note transient (yellow box). (Third Row) The re-\nﬁned separation estimates with the transient error removed.\ndisplayed in Fig. 1, which are used to clean up incorrectly\nseparated regions of each pitch track. We can notice that\nboth the incorrect note activations are removed and the\nnote transients of the remaining (correct) notes are sharp-\nened as a result of reallocating the incorrectly assigned\nnote energy to the remaining pitch tracks.\nSecond, we can view waveform visualizations in Fig. 5,\nwhich show the beneﬁt of having the ability to annotate\na level of conﬁdence or strength (real-valued annotations)\nwhen correcting for errors. In this example, we separate\ntwo overlapping piano pitches (E and D). Using standard,\nsupervised NMF-based separation, the energy from the tran-\nsient attack of the second note gets incorrectly assigned to\nthe ﬁrst note, causing a ghost-like effect. After annotation\n(red line), the transient error is reduced and the separation\nquality is improved.\n5. ALGORITHM\nUsing a suitably deﬁned divergence function D(V|WH),\nour choice of ⌦(H), and an appropriate pitch model W,\nwe need to derive an efﬁcient algorithm to actually com-\npute the unknown activations Hfor each pitch. As be-\nfore, we deﬁne Dto be the Kullback-Leibler divergence\nand follow the mathematical justiﬁcation of Lee [6] to de-\nrive a Majorization-Minimization optimization algorithm\nto solve (4), resulting in a multiplicative update algorithm\nthat incorporates our user-guided constraints.\nGiven the modiﬁed multiplicative NMF update equa-\ntions, we outline the complete interactive separation algo-\nrithm in Algorithm 1. We deﬁne the forward and inverse\nshort-time Fourier transform as ( V,6V) STFT( x) and\nx ISTFT( V,6V),1to be an appropriately sized ma-\ntrix of ones, \u0000is element-wise multiplication, the divisionAlgorithm 1 Interactive Polyphonic Separation\nProcedure INTERACTIVE-POLY-SEPARATION (\nx, // time-domain mixture signal\nW, // pitch basis vectors (model)\nKp, // basis vectors per pitch\n)\ninitialize: ⇤=0\nprecompute:\n(V,6V) STFT( x)\nrepeat\ninput: user-annotated penalties\n⇤2RP⇥T\ninitialize: feasible H2RK⇥T\n+\nrepeat\nH H\u0000WT(V\nWH)\nWT1+\u0000⇤(8)\nuntil convergence\nfor all p21,...,P do\nˆVp V\u0000W(p)H(p)\nWH(9)\nxp ISTFT( ˆVp,6V) (10)\nend for\nuntil satisﬁed\nreturn: time-domain signals xp,8p2{1,. . . ,P }\nis element-wise, and use the subscript notation (p)to pick\noff the elements of Wand/or Hthat correspond to pitch p.\nAt each point within the feedback-loop, the entire NMF-\nbased separation is re-run from scratch, displayed to the\nuser, and used as a starting point for further iterations.\n6. RELATED WORK\nThere are several related works that leverage some form\nof user-guidance to aid the source separation process. One\nof the most similar works to our proposed approach is dis-\ncussed in Ozerov et al. [8]. In this work, segmental infor-\nmation indicating the time activations of particular sources\nis used in a multichannel nonnegative tensor factorization\nmodel to improve separation quality. While similar to our\nproposed work, this work only allows for binary annota-\ntions that are used to zero-initialize elements of Hand does\nnot allow a user to specify a conﬁdence or strength level.\nAs a result, there is no mechanism to guide the separation\nprocess within regions where two or more sources overlap,\nsuch as our example of Fig. 5.\nOther user-guided approaches include the work of Dur-\nrieu et al. [4], Lef `evre et al. [7], and Bryan and Mysore\n[1, 2], which each use some form of time-frequency dis-\nplay to elicit user-annotations. In all such cases, however,\nthe interaction process is limited to separating two sound\nsources at a time (as oppose to Ppitches). In addition,\nthese works require end-users to annotate time-frequency\ndisplays of sound, which can be difﬁcult to interpret even\nfor expert users, motivating the proposed approach.Table 1 : Piano results (in dB) averaged across songs, tim-\nbre, and active notes.\nKp=1 T T+ UP UP+ U U+\nSDR 12.5 13.2 9.4 11.5 9.8 11.6\nSIR 19.7 22.9 15.6 21.0 15.9 21.1\nSAR 16.2 16.0 15.5 14.5 15.6 14.5\nKp=5 T T+ UP UP+ U U+\nSDR 13.1 13.3 10.5 12.8 10.1 12.6\nSIR 19.0 23.0 14.4 22.3 14.2 22.3\nSAR 17.0 16.0 17.2 15.5 17.0 15.4\nTable 2 : Guitar results (in dB) averaged across songs, tim-\nbre, and active notes.\nKp=1 T T+ UG UG+ U U+\nSDR 12.7 12.5 8.2 10.4 7.8 10.2\nSIR 19.2 22.2 15.0 20.3 14.5 20.1\nSAR 16.2 15.3 15.0 13.7 15.4 13.5\nKp=5 T T+ UG UG+ U U+\nSDR 12.8 12.6 9.6 11.5 8.7 11.2\nSIR 18.2 22.4 13.5 21.1 12.3 20.4\nSAR 17.1 15.3 16.6 14.6 16.6 14.3\n7. EVALUATION\nWe built a C++ prototype user-interface similar to Fig. 1.\nWe then used the interface to test our proposed method on\nseveral polyphonic piano and guitar recordings with vari-\nous pitch models. We generated the test material from ﬁve\nshort MIDI ﬁles, including “Mary Had A Little Lamb” by\nSarah Josepha Hale/Lowell Mason, “The Blue Danube” by\nJohann Strauss, “Super Mario Bros” by Nintendo Games,\n“Yesterday” by The Beatles, and “Maple Leaf Rag” by\nScott Joplin, using the 24 different synthesizers discussed\nin Section 2, resulting in 120 different ground truth record-\nings. For each of the ﬁve unique songs, the user-interface\nwas used to initially separate each song into pitch tracks\nand then interactively reﬁne the outputs over of the course\nof 30 minutes. The ﬁve user-annotations were then saved\nand used to test the method across the different ground-\ntruth recordings and pitch models.\nThe BSS-EV AL metrics were then used to compute the\nSignal-to-Distortion ratio (SDR), Signal-to-Interference ra-\ntio (SIR), and Signal-to-Artifact ratio (SAR) to measure\nthe separation quality [13]. The SIR measures the level of\nsuppression of the unwanted pitch sources, the SAR mea-\nsures the level of artifacts introduced by the separation pro-\ncess, and the SDR gives an average measure of separation\nquality that considers both the suppression of the unwanted\nsources and level of artifacts compared to ground truth.\nThe results were computed for each instrument before\nand after user-interaction, averaged across song and tim-\nbre, for various pitch models and values of Kp. When eval-\nuating the simultaneous separation of 88 different sound\nsources, however, the standard approach of comparing all\ncombinations of the estimated sources and known sources\nbecomes computationally prohibitive. As a result, we take\nthe approach of reducing the problem into 88 two-source\nevaluations that compare the separation quality of each in-\ndividual pitch pvs. the remaining 87 pitches. In addi-\ntion, we partition the results for active and inactive pitchesTable 3 : Piano results (in dB) averaged across songs, tim-\nbre, and inactive notes.\nKp=1 T T+ UP UP+ U U+\nSDR -6.5 133.9 -12.3 134.5 -2.3 135.2\nSIR -79.2 61.5 -84.6 61.7 -74.2 62.1\nSAR 4.9 90.0 1.8 90.3 7.3 90.7\nKp=5 T T+ UP UP+ U U+\nSDR -5.7 135.1 -11.7 135.0 -10.9 134.7\nSIR -79.6 62.3 -86.3 62.0 -86.2 61.9\nSAR 4.8 90.7 0.8 90.6 0.8 90.4\nTable 4 : Guitar results (in dB) averaged across songs, tim-\nbre, and inactive notes.\nKp=1 T T+ UG UG+ U U+\nSDR 1.2 133.9 -7.4 135.5 -11.1 135.3\nSIR -84.0 59.4 -91.1 60.6 -94.6 59.9\nSAR 7.7 91.4 4.5 92.3 2.5 92.1\nKp=5 T T+ UG UG+ U U+\nSDR 2.7 135.1 -13.0 134.7 -13.4 134.7\nSIR -83.9 60.1 -99.2 59.8 -99.4 59.6\nSAR 7.7 92.1 0.0 91.8 0.0 91.8\n(completely zero signals), allowing for a more detailed and\ncareful analysis of the results shown below. Without this\npartitioning, the results are extremely skewed in favor of\nthe proposed method due to averaging the inactive signal\nresults, limiting interpretability.\nThe results for active pitches are shown in Table 1 for\npiano and Table 2 for guitar. The results for inactive pitches\nare shown in Table 3 for piano and Table 4 for guitar.\nThe different pitch models include: universal (U), univer-\nsal guitar (UG), universal piano (UP), and each of the 24\ninstrument/timbre-speciﬁc models (T). Items denoted with\na plus ( +) indicate user-interaction was used. Note, be-\ncause of the partitioning of active vs. inactive pitches,\ncomparison of SDR vs. SAR vs. SIR should only be done\nwithin like rows.\nFrom these results, we have two initial observations.\nFirst, we compare the results with and without interac-\ntion. For active pitches, the SDR and SIR improve by\nseveral decibels for almost all pitch models and values of\nKp. When using the timbre-based pitch model (T) for a\ngiven set of recordings, however, there are cases where\nuser-interaction (T+) slightly decreases the SDR for active\npitches. For inactive pitches, the SDR, SAR, and SIR all\nimprove by an extremely large amount (often more than\n100 dB) caused by the zero signal pitch tracks in both the\nestimated and true recordings, motivating our decision to\nseparate the results for inactive and active pitch sets. This\nshows us that 1) a vast majority of the SDR increase due\nto user-interaction is caused by simply annotating inactive\npitch tracks, 2) while the user-annotations can occasion-\nally decrease the SDR for active pitches, in most cases,\nuser-interaction increases the SDR results for both active\nand inactive pitches. This is interesting in that it demon-\nstrates the idea that annotating incorrectly activated notes\ncan improve the separation quality of all other pitches on\naverage.\nSecondly, we can compare the results between the uni-versal pitch model (U), the instrument-speciﬁc pitch mod-\nels (UG, UP), and the instrument/timbre-speciﬁc pitch mod-\nels (T). We can see that the instrument/timbre-speciﬁc pitch\nmodels (T) perform the best, followed by the instrument-\nspeciﬁc pitch models (UG, UP), and then the universal\npitch model (U) as expected. When we vary the value of\nKpand incorporate user-interaction ( +), however, this ef-\nfect is signiﬁcantly reduced or eliminated. In the case of\npiano, the universal model with interaction outperformed\nthe timbre-based pitch model without interaction. This is\nsigniﬁcant in that it gives hope to the use of “universal”\npitch models, which eliminates the need for speciﬁc train-\ning data for particular instruments and timbres.\nFinally, because it is difﬁcult to evaluate the sound qual-\nity of the proposed method via numerical comparison, au-\ndio and video examples of our prototype can be found at\nccrma.stanford.edu/ ˜njb/research/pitch .\n8. CONCLUSIONS\nIn an attempt to overcome common, frustrating, and limit-\ning problems in supervised non-negative matrix factoriza-\ntion approaches to polyphonic single-channel source sepa-\nration, we propose an extension that allows a user to cor-\nrect for errors (with a conﬁdence value) in the separation\nresults by annotating a piano roll visualization of sound.\nThe user-annotations are mapped to linear grouping reg-\nularization parameters within a modiﬁed NMF-based al-\ngorithm, and used to reﬁne the separation estimates and\nimprove results. In addition, a database of piano and gui-\ntar recordings was used to learn a generalized pitch model,\ninstrument-speciﬁc pitch models, and instrument/timbre-\nspeciﬁc models. A prototype user-interface was built and\nused to separate several polyphonic guitar and piano record-\nings and initial results show that 1) user-interaction can sig-\nniﬁcantly increase separation quality and 2) make the use\nof generalized universal pitch models more viable.\n9. ACKNOWLEDGEMENTS\nThis work was generously supported by Adobe Research.\n10. REFERENCES\n[1]N. J. Bryan and G. J. Mysore. An efﬁcient posterior\nregularized latent variable model for interactive sound\nsource separation. In International Conference on Ma-\nchine Learning , June 2013.\n[2]N. J. Bryan and G. J. Mysore. Interactive reﬁnement\nof supervised and semi-supervised sound source sepa-\nration estimates. In IEEE International Conference on\nAcoustics, Speech, and Signal Processing , May 2013.\n[3]D. Cohn, R. Caruana, and A. Mccallum. Semi-\nsupervised clustering with user feedback. In Sugato\nBasu, Ian Davidson, and Kiri Wagstaff, editors, Con-\nstrained Clustering: Advances in Algorithms, Theory,\nand Applications . Chapman & Hall/CRC, 2008.[4]J.-L. Durrieu and J.-P. Thiran. Musical audio source\nseparation based on user-selected f0 track. In The 10th\nInternational Conference on Latent Variable Analy-\nsis and Signal Separation (LVA/ICA) , pages 438–445,\n2012.\n[5]C. F´evotte and J. Idier. Algorithms for nonnegative ma-\ntrix factorization with the \u0000-divergence. Neural Com-\nputation , 23(9):2421–2456, 2011.\n[6]D. D. Lee and H. S. Seung. Algorithms for non-\nnegative matrix factorization. In Advances in Neural\nInformation Processing Systems (NIPS) , pages 556–\n562. MIT Press, 2001.\n[7]A. Lef `evre, F. Bach, and C. F ´evotte. Semi-supervised\nnmf with time-frequency annotations for single-\nchannel source separation. In In the Proceedings of The\nInternational Society for Music Information Retrieval\n(ISMIR) Conference , 2012.\n[8]A. Ozerov, C. F ´evotte, R. Blouet, and J.-L. Durrieu.\nMultichannel nonnegative tensor factorization with\nstructured constraints for user-guided audio source\nseparation. In IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 257 –260, May 2011.\n[9]D. A. Reynolds, T. F. Quatieri, and R. B. Dunn.\nSpeaker veriﬁcation using adapted gaussian mixture\nmodels. In Digital Signal Processing , page 2000, 2000.\n[10] P. Smaragdis and J.C. Brown. Non-negative matrix fac-\ntorization for polyphonic music transcription. In IEEE\nWorkshop on Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA) , pages 177 – 180, Oct.\n2003.\n[11] P. Smaragdis, B. Raj, and M. Shashanka. Supervised\nand semi-supervised separation of sounds from single-\nchannel mixtures. In International Conference on In-\ndependent Component Analysis and Signal Separation ,\npages 414–421, Berlin, Heidelberg, 2007. Springer-\nVerlag.\n[12] D. L. Sun and G. J. Mysore. Universal speech models\nfor speaker independent single channel source separa-\ntion. Proceedings of the International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\n2013.\n[13] E. Vincent, R. Gribonval, and C. Fevotte. Performance\nmeasurement in blind audio source separation. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 14(4):1462 –1469, July 2006.\n[14] T. Virtanen. Monaural Sound Source Separation by\nNonnegative Matrix Factorization With Temporal Con-\ntinuity and Sparseness Criteria. IEEE Transactions\non Audio, Speech and Language Processing (TASLP) ,\n15(3):1066–1074, March 2007."
    },
    {
        "title": "Hooked: A Game For Discovering What Makes Music Catchy.",
        "author": [
            "John Ashley Burgoyne",
            "Dimitrios Bountouridis",
            "Jan Van Balen",
            "Henkjan Honing"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417599",
        "url": "https://doi.org/10.5281/zenodo.1417599",
        "ee": "https://zenodo.org/records/1417599/files/BurgoyneBBH13.pdf",
        "abstract": "Although there has been some empirical research on earworms, songs that become caught and replayed in one’s memory over and over again, there has been surprisingly little empirical research on the more general concept of the musical hook, the most salient moment in a piece of music, or the even more general concept of what may make music ‘catchy’. Almost by definition, people like catchy music, and thus this question is a natural candidate for approaching with ‘gamification’. We present the design of Hooked, a game we are using to study musical catchiness, as well as the theories underlying its design and the results of a pilot study we undertook to check its scientific validity. We found significant di↵erences in time to recall pieces of music across di↵erent segments, identified parameters for making recall tasks more or less challenging, and found that players are not as reliable as one might expect at predicting their own recall performance.",
        "zenodo_id": 1417599,
        "dblp_key": "conf/ismir/BurgoyneBBH13",
        "keywords": [
            "earworms",
            "musical hook",
            "catchy",
            "gamification",
            "Hooked",
            "recall tasks",
            "scientific validity",
            "pilot study",
            "time to recall",
            "challenge parameters"
        ],
        "content": "HOOKED: A GAME FOR DISCOVERING WHAT MAKES MUSIC CATCHY\nJohn Ashley Burgoyne⇤Dimitrios Bountouridis†Jan Van Balen†Henkjan Honing⇤\n⇤Music Cognition Group, University of Amsterdam, the Netherlands\n†Department of Information and Computing Sciences, Utrecht University, the Netherlands\n{j.a.burgoyne,honing}@uva.nl {d.bountouridis,j.m.h.vanbalen}@uu.nl\nABSTRACT\nAlthough there has been some empirical research on ear-\nworms , songs that become caught and replayed in one’s\nmemory over and over again, there has been surprisingly\nlittle empirical research on the more general concept of the\nmusical hook , the most salient moment in a piece of mu-\nsic, or the even more general concept of what may make\nmusic ‘catchy’. Almost by deﬁnition, people like catchy\nmusic, and thus this question is a natural candidate for ap-\nproaching with ‘gamiﬁcation’. We present the design of\nHooked, a game we are using to study musical catchiness,\nas well as the theories underlying its design and the results\nof a pilot study we undertook to check its scientiﬁc validity.\nWe found signiﬁcant di ↵erences in time to recall pieces of\nmusic across di ↵erent segments, identiﬁed parameters for\nmaking recall tasks more or less challenging, and found that\nplayers are not as reliable as one might expect at predicting\ntheir own recall performance.\n1. INTRODUCTION\n‘Aha! Yes, it’s thatsong!’ Many music listeners, even cas-\nual listeners, have had the pleasant experience of recalling\na song to memory after hearing a few seconds of its ‘hook’.\nLikewise, many casual listeners can tell almost immediately\nupon hearing a new song whether it will be ‘catchy’. Des-\npite the prevalence of these musical instincts, musicology\n(in the broadest sense, encompassing music cognition and\nmir) can provide only a limited understanding of why cer-\ntain pieces music are catchy and what is distinctive about\nthe hooks within these pieces of music. The concepts of the\nhook and of catchiness are vital to understanding human\nmusical memory, but they also have implications outside\nof music cognition. Charles Kronengold, a musicologist,\nThe Netherlands Organisation for Scientiﬁc Research (NWO) funded\nthis study under grant 640.005.004, ‘Cognition-Guided Interoperability\nBetween Collections of Musical Heritage (COGITCH)’. In addition to\ncomments from those who tested the prototype, we received helpful sug-\ngestions from many colleagues, among them Fleur Bouwer, Aline Hon-\ningh, Berit Janssen, Jaap Murre, Johan Oomen, Carlos Vaquero, Remco\nVeltkamp, Lourens Waldorp, and Frans Wiering. The data are available to\ndownload from http://mcg.uva.nl/ .\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\u00002013 International Society for Music Information Retrieval.has posited that the characteristics of hooks might vary\nacross genres and, a fortiori , that di ↵erent assortments of\nhook characteristics might constitute a working deﬁnition\nof genre [ 11]. In mir, a better understanding of hooks and\ncatchiness would be useful for music recommendation (all\nelse being equal, the catchier of two tunes is probably the\nbetter recommendation), measuring musical similarity (as\nestimating similarity between the hooks of two pieces of\nmusic may be closer to human perception than estimating\nsimilarity over complete pieces), generating satisfying seg-\nmentations of pieces of music (as hooks tend to mark the\nstart of new sections), and to some extent, ﬁngerprinting (as\nhooks are the ﬁngerprints for the brain’s retrieval system).\nThe boundaries between catchiness, hooks, and some\nother musical concepts are fuzzy. One related concept that\nhas attracted a certain amount of empirical research is the\nearworm , songs that are so catchy that they become invol-\nuntarily stuck in one’s mind [ 3,7,19]. Earworms are a\nmuch narrower phenomenon than catchiness, too narrow,\nwe believe, for many mir applications: Few users are look-\ning for playlists comprising nothing but earworms. Another\nrelated concept is so-called hit-song science, which aims\nto predict the popularity of songs based on their musical\ncontent [ 6,16]. This area of study, in contrast, is broader\nthan our area of inquiry. Although catchiness is certainly\ncorrelated with popularity, many popular songs are quite\nforgettable, and we are most interested in music that re-\nmains in listeners’ memories for the long term. This level\nof cognitive information seems to be right for contributing\nto the widest variety of tasks in mir [9].\nThe deﬁnition of a hook itself is also fuzzy, and as musi-\ncologist Don Traut has observed, ‘When we go further and\nask not only “What is a hook?”, but “What is it about the\nmusic that makes this a hook?”, the picture gets even more\nblurry’ [ 17]. From a cognitive point of view, we deﬁne\na hook to be the most salient, easiest-to-recall fragment\nof a piece of music [ 9]; likewise, we deﬁne catchiness as\nlong-term musical salience, the degree to which a musical\nfragment remains memorable after a period of time. By\nour deﬁnitions, every piece of music will have a hook –\nthe catchiest part of the piece, whatever that may be – but\nsome pieces of music clearly have much catchier hooks\nthan others. In principle, a piece of music may also have\nmultiple hooks: two or more fragments of equivalent sa-\nlience that are nonetheless more salient than all others in\nthe piece. There is agreement in the literature that hooks\nstart at points of considerable structural change, or in otherwords, at points that we in mir would consider to be the\nbeginnings of new sections for the purposes of a segment-\nation algorithm [ 4,15]. There is more debate about the\nduration of hooks. While songwriters will often speak of\nthe hook as the entire chorus, in fact, only a few seconds\nare necessary for most listeners to recall a catchy song to\nmemory; one study has shown that after only 400 ms, listen-\ners can identify familiar music with a signiﬁcantly greater\nfrequency than one would expect from chance [12].\nWe have designed an experiment that we believe will\nhelp to quantify the e ↵ect of catchiness on musical memory.\nBecause we consider catchiness to be long-term rather than\nshort-term salience, this design posed some important chal-\nlenges. First, we needed to be able to work with well-known\nrecordings of well-known music in order to capture frag-\nments that have in fact remained in participants’ memories\nfor potentially long periods of time. Individual listening\nhistories vary widely, however, and thus this constraint also\nentailed the ability to use quite a large set of musical stimuli,\non the order of 1000 or more. Moreover, listening histor-\nies vary with respect not only to what music participants\nhave heard before but also to how well they know particular\npieces; as such, in order to obtain reliable statistics, we\nalso needed to be able to support a much larger number\nof participants than a traditional psychological experiment.\nNext to becoming a serious alternative to a certain class of\nlab-based experiments, Internet-based experiments can po-\ntentially reach a much larger, more varied and intrinsically\nmotivated participant pool, positively inﬂuencing the eco-\nlogical validity of the results [10]. Furthermore, given that\nmost listeners enjoy catchy music, our question seems nat-\nurally suited for ‘gaming with a purpose’, which has already\nproven successful for certain tasks in mir and for machine\nlearning in general [ 1,13]. By framing the experiment as\na game, we believe we will be able to collect enough data\nabout catchiness to support a robust analysis of recall from\nmusical memory and also to open new possibilities for using\ncontent-based mir to predict musical catchiness.\n2. DESIGNING HOOKED\nHooked, as we have named the game, comprises three es-\nsential tasks: a recognition task, a veriﬁcation task, and a\nprediction task. Each of them responds to a scientiﬁc need\nin what we felt was the most entertaining fashion possible.\nIn this way, we hope to be able recruit the largest number\nof subjects possible without sacriﬁcing scientiﬁc quality.\n2.1 Recognition Task\nThe recognition task is the heart of the game. It stems from\nthe idea that the deﬁning aspect of catchiness its e ↵ect on\nlong-term memory. In particular, the easier a fragment of\nmusic is to recall after a long period of time, the catchier it\nshould be. Thus, a ‘drop-the-needle’ style quiz, whereby\na piece of music starts playing from a point in the middle\nand players are asked to recognise it, seemed to be appropri-\nate. As noted above, there is a consensus in the theoretical\nliterature that the hook should start at the beginning of anew structural section (possibly including the beginning\nof the piece itself), and we extended this idea to limit the\nnumber of starting points to a statistically tractable subset:\nMusic will always start playing from the beginning of a\nstructural section. Then the amount of time it takes a player\nto recognise the piece is a proxy for how easy that section\nis to recall, or in short, how catchy it is.\nFigure 1a illustrates the recognition game as implemen-\nted in our current iOS prototype. A piece of music starts\nplaying from the start of a structural section, and players\nhave several seconds to decide whether they know it. While\nplayers are listening, points are counting down; the faster\nplayers recognises the piece, the more points they can win.\n2.2 Veriﬁcation Task\nIn a controlled laboratory environment, it might be justi-\nﬁable to trust subjects to be honest in claiming to have\nrecognised a piece of music. In a competitive game environ-\nment, it is not. We needed a task to verify that players have\ntruly recognised the music at the moments they claim so.\nMost music trivia games, e.g., SongPop,1would ask play-\ners to identify the title, composer, artist, or year of release,\nbut this type of question would cause serious problems for\nthe scientiﬁc goals of Hooked. Many listeners may know a\npiece of music rather well without knowing its exact title or\nthe name of the performing artist; moreover, even for those\nusers who do know such trivia, the extra cognitive load in\nrecalling it in addition to recognising the music itself would\nhave an unpredictable e ↵ect on reaction time.\nIdeally, the veriﬁcation task would be strictly musical,\nbut precisely because we expect players to know the mu-\nsical material fairly well, ﬁnding a strictly musical task was\nchallenging. Playing a new fragment of music and asking\nthe player whether it came from the same song, for example,\nwould likely be far too easy to be a reliable test. Using any\nkind of audio degradation to make the task harder would\nlikely make it too di \u0000cult in cases where the player genu-\ninely did know the song. Using mir tools to extract melod-\nies, beats, or some other feature would bias the general\nnotion of catchiness unduly toward catchiness as limited to\nwhat such an mir tool can extract.\nIn the end, we were inspired by the idea that once players\nhave fully recalled a piece of music to memory, they should\nbe able to follow along with the song in their heads for some\ntime even after the music stops playing. Moreover, there is\nevidence that absolute tempo is part of musical memory, al-\nthough the error distribution is somewhat skewed in favour\nof overly fast tempi [ 14]. In Hooked, as soon as players\nclaim to know a song, playback mutes for a few seconds.\nDuring the mute, players are asked to imagine mentally or\nsing along actively for a few seconds (Figure 1b). When the\nsound returns, half the time the music returns the correct\nplace (i.e., the mute was genuinely only a mute) and half the\ntime the playback is o ↵set by a few seconds (i.e., an invis-\nible DJ ‘scratched the record’ during the mute). The player\nmust answer whether the music is in the right place. We\nbelieve that over mutes of the duration we are considering\n1http://www.songpop.fm/(a) Recognition\n (b) Veriﬁcation\n (c) Prediction\nFigure 1 : Screenshots from the Hooked prototype. (a) The recognition task is the heart of the game: A song starts from\nthe beginning of an internal musical section, chosen at random, and the player must guess the song as quickly as possible.\n(b) The sound then mutes for a few seconds while players try to follow along in their heads. When the sound comes back,\nplayers must verify that the song is playing back from the correct place. (c) Occasionally, players instead must do the reverse:\npredict which of two sections is catchiest, in order to store bonus rounds for themselves.\nfor Hooked, players who truly remember a song should be\ncapable of following along well enough to identify whether\nthe music has returned in the correct place. The primary\nchallenge is ﬁnding empirical evidence for the optimal mute\ntime: not so short that one can judge the continuation on\nthe basis of common-sense musical knowledge or timbral\ncharacteristics (type 2 error) but also not so long that it\nwould interfere with the speed of imagining that might well\nbe faster than in singing (type 1 error).\n2.3 Prediction Task\nWe have argued here that because the notion of catchiness\ninherently invokes musical memory, a scientiﬁc deﬁnition\nof the term must involve ease of recall. The recognition\ngame seeks to quantify listeners’ behaviour on this axis. We\nwould also like to know how well this formal deﬁnition cor-\nresponds to listeners’ informal intuitions for what it catchy\nand what is not. As such, we decided to include periodic\nrounds of the game where we turn the recognition task on\nits head and ask players to choose which of two fragments\nfrom the same song is catchier. An image of such a round\nin our prototype appears in Figure 1c.\nAs a survey question, this task is pleasant enough, but it\nwas a challenge to integrate it meaningfully into the game-\nplay. One idea we may explore in the future is adding a\nsocial element. For example, we might ask players to try\nto fool online opponents by predicting which will be the\nlesscatchy members of each pair and sending those predic-\ntions to those opponents for a recognition task; we would\nthen award prediction players the inverse of the number\nof points their opposing recognition player earns. For the\nmoment, however, we wanted a self-standing game with an\nintrinsic reward for the prediction task. Our solution was\nbonus rounds. Each time players complete a prediction task,\nthe chosen fragment is saved in a special bu ↵er for eachplayer. Periodically, the recognition task will enter a bonus\nround for double points, with a guarantee that the fragment\nselected comes from the special bu ↵er of prediction frag-\nments. Thus, users who spend time to do a thorough job\nwith prediction tasks can potentially earn many extra points.\n3. TESTING SCIENTIFIC SOUNDNESS\nWe developed a prototype of Hooked on iOS and undertook\na pilot study to identify the best values of the free para-\nmeters in the design (the maximum time allowed for the\nrecognition task, the length of the mute, and length of the\no↵set used for false returns in the veriﬁcation task) and to\nensure that the scientiﬁc assumptions underlying the design\nwere correct. We recruited 26 testers from within our aca-\ndemic networks, 18 men and 8 women, between the ages of\n20 and 70. Most participants spent about 45 minutes testing,\nsome at home and some in their o \u0000ces, some on their own\niOS devices and some on ours.\n3.1 Musical Material\nAlthough we designed Hooked to accommodate a very large\ncorpus of music, our pilot study required a more constrained\nset of musical material. We chose 32 songs at random from\nthe 2012 edition of a list of the ‘greatest songs of all time’\nfrom a popular annual radio programme. In order to avoid\nlicensing problems as the scope of the game expands, we\nused Spotify’s iOS libraries to stream all audio and require\na Spotify Premium membership to play.2The Echo Nest\nhas a partnership with Spotify that includes a convenient\nweb service for applying the Echo Nest Analyzer to tracks\nin Spotify’s catalogue,3and we used this service to obtain\nestimates of the start times of the major structural sections\n2http://www.spotify.com/\n3http://developer.echonest.com/in each song. For the 9 songs that were ranked highest on\nthe list, we retained all sections but the ﬁrst and last (which\noften contain silence); with these songs, we hoped to be\nable to show that there is indeed signiﬁcant variation in\nrecognition time across di ↵erent sections of the same song.\nFor the next 8 highest ranked, we retained a random sample\nconstituting half of the sections, a compromise position.\nFor the remaining 15 songs, we retained only a random pair\nof sections; with these songs, we hoped primarily to be able\nto introduce some variety for the participants so that they\nwould have a better sense of how the game would feel with\na full-sized corpus. In total, this procedure yielded 160 song\nsections to use for the recognition task. From among these\nsections, we selected just one random pair from each of the\n32 songs to use for testing the prediction task.\n3.2 Method\nDuring a testing session, testers worked through recognition\ntasks for each of 160 sections in a random order. For the\nﬁrst 80 sections, we asked testers to play as they would in\nthe real world. For the remaining 80 sections, in order to\ntest the limits of the veriﬁcation task, we asked testers to try\nto cheat the system by claiming that they recognised every\nsection as soon as possible, ideally before they actually\nknew the song. We recorded the reaction times and whether\nthe responses were correct for each tester and each section.\nThroughout a testing session, testers also had a 20 percent\nchance of being asked to perform a prediction task instead\nof a recognition task for any given round and a 10 percent\nchance that a recognition round would be a bonus round.\nDuring test runs, we changed some parameters of the\ngame after every 10 recognition tasks. Overall, there were\neight possible conﬁgurations of the parameters, which we\npresented in a random order to each tester during both\nthe ﬁrst, ‘honest’ half of the test run and again during the\nsecond, ‘dishonest’ half. Speciﬁcally, each of three para-\nmeters took one of two distinct values, which we chose\nbased on preliminary testing prior to the pilot. The max-\nimum time allowed for recognition was either 10 s or 15 s;\nthis parameter primarily a ↵ects the feel of the gameplay,\nbut it has some scientiﬁc consequences in the rare cases\nwhere players need more than 10 s to decide whether they\nrecognise a fragment. The mute time was either 2 s or 4 s;\nthis parameter in principle a ↵ects the di \u0000culty of the veri-\nﬁcation task. The o ↵set for false returns in the veriﬁcation\ntask was either 15 s or \u000015 s; this parameter likewise a ↵ects\nthe di \u0000culty of the veriﬁcation task. Testers were informed\nwhen either the maximum recognition time or mute time\nchanged so that they could comment on their preferences;\ntesters were not informed about changes in the o ↵set time\nfor false returns so as not to give extra information they\ncould have used to cheat the veriﬁcation task.\n4. RESULTS\nDue to personal time constraints, not all participants were\nable to complete the pilot in its entirety: 4 made it less\nthan halfway through and a further 5 made it less than 8012.638.991.3109.9133.9179.9Section Start Time (s)Mean Response Time (s)0246810\nFigure 2 : Mean response times from the recognition task\non di ↵erent sections of Adele’s ‘Rumour Has It’. Error bars\nreﬂect standard error. Controlling for multiple comparisons,\nthere are signiﬁcant di ↵erences ( p<.05) in response times\nbetween the bridge (133.9 s) or the verse at 91.3 s, and the\ninitial entry of the vocals (12.6 s) or the pre-chorus at 38.9 s.\npercent through. Nonetheless, because we randomised the\npresentation of sections and parameter settings for each\nsubject, we have no reason to believe that the missing data\nshould exhibit any systematic bias.\nFor the recognition task, the Box-Cox procedure sug-\ngests a log transform on response time, and we assume that\nresponse times are log-normally distributed. Regressing\nthus across all song sections, the average response time for\nsuccessfully veriﬁed claims to know a song is 5.2 s. anova\nconﬁrms that there are signiﬁcant di ↵erences between the\nresponse times for di ↵erent sections within a song even\nafter accounting for the variation in average response time\nfor di ↵erent participants: F(128,964)=1.55,mse =39.06,\np<.001. Figure 2 illustrates the variation in response\ntimes for Adele’s ‘Rumour Has It’.4After correcting for\nmultiple comparisons with Tukey’s test, there are signi-\nﬁcant di ↵erences ( p<.05) between either of the initial\nentry of the vocals (12.6 s, ‘She, she ain’t real’) and the\npre-chorus at 38.9 s (‘Bless your soul, you’ve got your head\nin the clouds’) and either of the bridge (133.9 s, ‘All of these\nwords whispered in my ear’) and the second verse (91.3 s,\n‘Like when we creep out’). The di ↵erences in response time\nare as high as 4 s.\nIn order to tune the veriﬁcation task, we needed to de-\ntermine the best values to use for maximum recognition\ntime, the time limit on the recognition task; mute time ; and\nthedistractor o ↵set, the o ↵set to use on the occasions when\nthe sound returns from the mute in the wrong place. More\nspeciﬁcally, the distractor o ↵set could be either a forward\no↵setof 15 s ahead of where the song should have been\nplaying or a backward o ↵setof 15 s before where the song\nshould have been playing. We also needed to ensure that\nthere is a su \u0000ciently large beneﬁt to playing honestly over\nrandom guessing. Using the player, maximum recognition\ntime, mute time, the distractor o ↵set, and whether the player\nwas in the ‘honest’ or ‘dishonest’ portion of the pilot, we\nused a stepwise selection procedure on logistic regression\nmodels for the probability of answering the validation ques-\ntion correctly. Akaike’s Information Criterion (aic)prefers\n4spotify:track:50yHVBbU6M4iIfqBI1bxWxRecognition Time p1 95% CI p2 95% CI\nDistractor O ↵set:\u000015 s\n10 s .66 [.59, .73] .26 [.21, .31]\n15 s .72 [.64, .79] .19 [.15, .24]\nDistractor O ↵set:+15 s\n10 s .54 [.47, .61] .33 [.28, .39]\n15 s .67 [.60, .74] .33 [.28, .38]\nTable 1 : Probability of type 1 and type 2 errors for the\nvalidation task (i.e., answering the validation question cor-\nrectly for an unknown song or answering it incorrectly for\na known song) under di ↵erent values of the design paramet-\ners. The ideal combination of parameters would minimise\nboth types of error, but some trade-o ↵s will be necessary.\na model including only the player, maximum recognition\ntime, the distractor o ↵set, and the ‘honesty’ variable with no\ninteractions. A maximum recognition time of 15 s vs. 10 s\nimproved a player’s odds of answering the validation ques-\ntion correctly by 31 percent on average (95% CI [5, 62]), a\ndistractor o ↵set of -15 s vs. +15 s improved a player’s odds\nof guessing correctly by 57 percent on average (95% CI\n[27, 93]), and playing honestly improved a player’s odds of\nguessing correctly by 64 percent on average (95% CI [29,\n111]). Table 1 summarises the veriﬁcation data from the\npilot in the more traditional language of type 1 and 2 errors.\nIn order to analyse the data from the prediction task,\nwe use the fact that after completing a full test run, testers\nin the pilot had also completed recognition tasks for all\nfragments o ↵ered to them as choices in prediction task.\nWe compared the choices made in prediction tasks to the\ndi↵erence in response times for the same fragments when\nthey appeared in recognition tasks. Although there is a\nstatistically signiﬁcant relationship ( p=.02), the e ↵ect is\nsmall: For each second of di ↵erence in response times, the\nodds of a player choosing the faster-recognised member\nof a pair during the prediction task increased by only 6\npercent (95% CI [1, 12]). Moreover, the variance is quite\nhigh. Figure 3a shows the distribution of response-time\ndi↵erences where players chose the ﬁrst fragment in the\nprediction task and Figure 3b shows the distribution where\nthey chose the second. Although these distributions are\neach skewed to the appropriate side, it is clear that players\nare not necessarily consistent with their behaviour in the\nrecognition task when making predictions.\n5. DISCUSSION\nThe results of our pilot of the recognition task conﬁrm that\ndi↵erent fragments of music, even within the same song,\ndi↵er measurably in their ability to trigger musical memory.\nIn a context where average response time is just over 5 s,\nthe 4-s e ↵ect size is substantial. Moreover, this magnitude\nof response time sets us comfortably in the realm of musico-\nlogical theories about hooks: something rather longer than\nKrumhansl’s 400-ms ‘plinks’ [ 12] but also rather shorter\nthan a complete refrain chorus, say 5 to 10 s. Historically,\nmir has worked rather less with musical fragments of thisscale, more often tending to consider audio frames that are\nshorter even than plinks or attempt to classify complete\npieces. Having shown in this pilot study how important\nthese 5-to-10-s fragments are to human musical memory,\nwe would like to suggest that they might be especially prof-\nitable when tuning the granularity of algorithms for predict-\ning musical similarity or recommending new music, a claim\nthat is consistent some recent mirresearch on segmentation\nand annotation [2, 5, 8]).\nThe most important limitation to this result arises from\nthe quality of automatically generated audio segments. If, as\nmusicological theory suggests, hooks are tied to moments\nof change in the musical texture, any error in the estimation\nof segment boundaries will propagate throughout the ana-\nlysis. For a study of this size, it would have been possible\nto choose the segments by hand, thereby eliminating this\nsource of error, but because our purpose was to test the feas-\nibility of a larger-scale study where it will not be possible\nto choose segments by hand, we felt it was important to\nuse automatic segmentation for our pilot, too. The analytic\ntechniques available for larger-scale data, most notably the\ndrift-di ↵usion model [ 18], will allow us to identify ‘lag\ntime’ in segments that begin playing a bit too early, but for\nthis study, we have to assume that such lags are noise.\nFor our veriﬁcation task, we have arrived at the classical\ntrade-o ↵between type 1 and type 2 errors, perhaps more\noften encountered in mir when trying to optimise preci-\nsion and recall: Because we found no signiﬁcant interaction\nbetween parameter settings and honest play, choosing set-\ntings to make the game easier for honest players also will\nmake it easier for cheaters. Conversely, the large beneﬁt\nto playing honestly – again, a 64 percent improvement in\nthe odds of answering the veriﬁcation correctly – suggest\nthat we may feel comfortable that players have an incentive\nto play honestly regardless of the parameter settings and\nthus can focus on making the game as pleasant for honest\nplayers as possible. As such, we intend to allow 15 s for\nrecognition and use the \u000015-s distractor o ↵set.\nWe were surprised that the distractor o ↵set had such a\nstrong e ↵ect on the players’ accuracy, and the idea that dis-\ntractors from the past are easier to identify as incorrect than\ndistractors from the future is especially intriguing from a\ncognitive perspective: Is it easier to rewind musical memory\nthan it is to fast-forward? Another possibility, perhaps sim-\npler, is that the forward distractor is more likely to be in the\nsame structural section as the original fragment, whereas\nbecause we have chosen our fragments always to start at the\nbeginnings of new sections, the backward distractor will al-\nways be in a di ↵erent one. Assuming that structural sections\nmaintain some degree of timbral consistency, the backward\ndistractor may more often o ↵er timbral clues to the player\nthat something is not right when the sound returns.\nThe data for the prediction task do not lend strong sup-\nport our hypothesis that recognition time is a proxy for\nsocial intuitions about catchiness. This lack of support is\nespecially surprising given that our concern had originally\nbeen more that players would somehow learn to choose\nfragments that optimised gameplay without touching onDifference in Response Time (s)Relative Frequency−10−505100.000.050.100.15\n(a) Choosing the First FragmentDifference in Response Time (s)Relative Frequency−10−505100.000.050.100.15\n(b) Choosing the Second Fragment\nFigure 3 : Distributions of response-time di ↵erences from the recognition task on pairs presented during prediction tasks.\nThere are slight di ↵erences in the distribution of di ↵erences when the ﬁrst member of the pair is chosen as opposed to the\nsecond, but overall, players do not appear to be consistent with their recognition behaviour when making predictions.\ntheir personal feelings about catchiness; in fact, just the\nreverse seems to be true. Akin to Williamson and Müllen-\nsiefen’s work on earworms and Burns’s more speculative\nwork, [ 4,19], as we roll Hooked out to larger audience\nand thereby generate a larger database, we plan to ﬁnd sets\naudio features that correlate with recognition and predic-\ntion performance. The di ↵erence between these two sets\nwill help clarify this divergence between listeners’ actual\nlong-term musical memories and their expectations of them.\n6. REFERENCES\n[1]L. van Ahn and L. Dabbish: ‘Designing Games with a\nPurpose’, Communications of the \u0000\u0000\u0000, Vol. 51, No. 8,\npp. 58–67, 2008.\n[2]L. Barrington, A. B. Chan, and G. Lanckriet: ‘Mod-\neling Music as Dynamic Texture’, \u0000\u0000\u0000\u0000 Transactions\non Audio, Speech, and Language Processing , Vol. 18,\nNo. 3, pp. 602–12, 2010.\n[3]C. P. Beaman and T. I. Williams: ‘Earworms (“Stuck\nSong Syndrome”): Towards a Natural History of Intrus-\nive Thoughts’, British Journal of Psychology , V ol. 101,\nNo. 4, pp. 637–53, 2010.\n[4]G. Burns: ‘A Typology of “Hooks” in Popular Records’,\nPopular Music , V ol. 6, No. 1, pp. 1–20, 1987.\n[5]E. Coviello, A. B. Chan, and G. Lanckriet: ‘Time Series\nModels for Semantic Music Annotation’, \u0000\u0000\u0000\u0000 Trans-\nactions on Audio, Speech, and Language Processing ,\nV ol. 19, No. 5, pp. 1343–59, 2011.\n[6]R. Dhanaraj and B. Logan: ‘Automatic Prediction of Hit\nSongs’, Proc. 6th \u0000\u0000\u0000\u0000\u0000 , pp. 488–91, London, England,\n2005.\n[7]A. R. Halpern and J. C. Bartlett: ‘The Persistence of\nMusical Memories: A Descriptive Study of Earworms’,\nMusic Perception , V ol. 28, No. 4, pp. 425–32, 2011.\n[8]P. Hamel, S. Lemieux, Y. Bengio, and D. Eck: ‘Tem-\nporal Pooling and Multiscale Learning for Automatic\nAnnotation and Ranking of Music Audio’, Proc. 12th\n\u0000\u0000\u0000\u0000\u0000 , pp. 729–34, Miami, FL, 2011.[9]H. J. Honing: ‘Lure(d) into Listening: The Potential\nof Cognition-Based Music Information Retrieval’, Em-\npirical Musicology Review , Vol. 5, No. 4, pp. 121–26,\n2010.\n[10] H. J. Honing and O. Ladinig: ‘The Potential of the\nInternet for Music Perception Research: A Comment\non Lab-Based Versus Web-Based Studies’, Empirical\nMusicology Review , V ol. 3, No. 1, pp. 4–7, 2008.\n[11] C. Kronengold: ‘Accidents, Hooks and Theory’, Popu-\nlar Music , V ol. 24, No. 3, pp. 381–97, 2005.\n[12] C. L. Krumhansl: ‘Plink: “Thin Slices” of Music’,\nMusic Perception , V ol. 27, No. 5, pp. 337–54, 2010.\n[13] E. Law, K. West, M. Mandel, M. Bay, and J. S. Downie:\n‘Evaluation of Algorithms Using Games: The Case of\nMusic Tagging’, Proc. 11th \u0000\u0000\u0000\u0000\u0000 , Utrecht, the Nether-\nlands, 2010.\n[14] D. J. Levitin and P. R. Cook: ‘Memory for Musical\nTempo: Additional Evidence That Auditory Memory\nIs Absolute’, Perception and Psychophysics , Vol. 58,\nNo. 6, pp. 927–35, 1996.\n[15] P. Mercer-Taylor: ‘Two-and-a-Half Centuries in the Life\nof a Hook’, Popular Music and Society , V ol. 23, No. 2,\npp. 1–15, 1999.\n[16] F. Pachet: ‘Hit Song Science’, Music Data Mining ,\nT. Li, M. Ogihara, and G. Tzanetakis, Eds., pp. 305–26,\nChapman & Hall /CRC, Boca Raton, FL, 2012.\n[17] D. Traut: ‘“Simply Irresistible”: Recurring Accent Pat-\nterns as Hooks in Mainstream 1980s Music’, Popular\nMusic , V ol. 24, No. 1, pp. 57–77, 2005.\n[18] J. Vandekerckhove and F. Tuerlinckx: ‘Fitting the\nRatcli ↵Di↵usion Model to Experimental Data’, Psycho-\nnomic Bulletin and Review , V ol. 14, No. 6, pp. 1011–26,\n2007.\n[19] V. J. Williamson and D. Müllensiefen: ‘Earworms\nfrom Three Angles: Situational Antecedents, Personal-\nity Predisposition, and the Quest for a Musical Formula’,\nProc. 12th \u0000\u0000\u0000\u0000\u0000 , pp. 1124–32, Thessaloniki, Greece,\n2012."
    },
    {
        "title": "Robotaba Guitar Tablature Transcription Framework.",
        "author": [
            "Gregory Burlet",
            "Ichiro Fujinaga"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415042",
        "url": "https://doi.org/10.5281/zenodo.1415042",
        "ee": "https://zenodo.org/records/1415042/files/BurletF13.pdf",
        "abstract": "This paper presents Robotaba, a web-based guitar tablature transcription framework. The framework facilitates the creation of web applications in which polyphonic transcription and guitar tablature arrangement algorithms can be embedded. Such a web application is implemented, and consists of an existing polyphonic transcription algorithm and a new guitar tablature arrangement algorithm. The result is a unified system that is capable of transcribing guitar tablature from a digital audio recording and displaying the resulting tablature in the web browser. Additionally, two ground-truth datasets for polyphonic transcription and guitar tablature arrangement are compiled from manual transcriptions gathered from the tablature website ultimate-guitar.com. The implemented transcription web application is evaluated on the compiled ground-truth datasets using several metrics.",
        "zenodo_id": 1415042,
        "dblp_key": "conf/ismir/BurletF13",
        "keywords": [
            "Robotaba",
            "web-based",
            "guitar tablature transcription",
            "framework",
            "polyphonic transcription",
            "guitar tablature arrangement",
            "web application",
            "digital audio recording",
            "web browser",
            "ground-truth datasets"
        ],
        "content": "ROBOTABA GUITAR TABLATURE TRANSCRIPTION FRAMEWORK\nGregory Burlet and Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Technology\nMcGill University, Montr ´eal, Qu ´ebec, Canada\ngregory.burlet@mail.mcgill.ca, ich@music.mcgill.ca\nABSTRACT\nThis paper presents Robotaba , a web-based guitar tabla-\nture transcription framework. The framework facilitates\nthe creation of web applications in which polyphonic tran-\nscription and guitar tablature arrangement algorithms can\nbe embedded. Such a web application is implemented, and\nconsists of an existing polyphonic transcription algorithm\nand a new guitar tablature arrangement algorithm. The re-\nsult is a uniﬁed system that is capable of transcribing gui-\ntar tablature from a digital audio recording and display-\ning the resulting tablature in the web browser. Addition-\nally, two ground-truth datasets for polyphonic transcrip-\ntion and guitar tablature arrangement are compiled from\nmanual transcriptions gathered from the tablature website\nultimate-guitar.com . The implemented transcription\nweb application is evaluated on the compiled ground-truth\ndatasets using several metrics.\n1. INTRODUCTION\nTablature has become the primary form of communication\nbetween guitarists on the Internet. Guitar tablature is a mu-\nsic notation system with a six-line staff that represents the\nstrings on a guitar. A numeric entry on a line represents\nthe fret to depress on a particular string (Figure 1).\nManually transcribing guitar tablature from an audio\nrecording is a difﬁcult and laborious task, even for expe-\nrienced guitarists. In response to the time-consuming pro-\ncess of manual transcription, automatic music transcrip-\ntion systems aim to extract a symbolic music score from\nan acoustical signal [5]. Speciﬁcally, automatic guitar tab-\nlature transcription systems transform a digital guitar sig-\nnal into tablature notation. The task of automatic guitar\ntablature transcription can be decomposed into two sub-\nproblems: polyphonic transcription and guitar tablature ar-\nrangement. Polyphonic transcription algorithms extract the\npitch, onset time, and duration of notes occurring in an au-\ndio recording. Guitar tablature arrangement algorithms as-\nsign a string and fret combination to each note occurring\nin an input music score. Adding more ambiguity to the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.\nFigure 1 . Tablature notation depicting six different string\nand fret combinations to perform the note E4 on a 24-fret\nguitar in standard tuning.\ntranscription process, the guitar can produce the same note\nin several ways. For example, there exists six string and\nfret combinations that can produce the note E4 on a 24-fret\nguitar in standard tuning (Figure 1).\nWhile several polyphonic transcription and guitar tab-\nlature arrangement algorithms have been proposed in the\nliterature, no frameworks have been developed to facilitate\nthe combination of these algorithms to produce an auto-\nmatic guitar tablature transcription system. Moreover, af-\nter a new polyphonic transcription or guitar tablature ar-\nrangement algorithm is developed, the code has no imme-\ndiately available vessel to be used by music researchers and\nthe large community of guitarists on the Internet.\nA web-based guitar tablature transcription framework,\nentitled Robotaba , has been designed and implemented to\nfacilitate the creation of guitar tablature transcription web\napplications in which polyphonic transcription and guitar\ntablature arrangement algorithms can be embedded. An\nexisting polyphonic transcription algorithm and a new gui-\ntar tablature arrangement algorithm is implemented; these\nalgorithms are embedded in a transcription web application\nusing the Robotaba framework. Two ground-truth datasets\nhave been compiled to evaluate the performance of the im-\nplemented guitar tablature transcription system.\nThe structure of this paper is as follows: The next sec-\ntion provides an overview of polyphonic transcription and\nguitar tablature arrangement algorithms. Section 3 presents\nthe design of Robotaba, followed by a description of the\nimplemented transcription web application in Section 4.\nSection 5 presents the compiled ground-truth datasets and\ntheir use in the evaluation of the implemented polyphonic\ntranscription and guitar tablature arrangement algorithms.\n2. LITERATURE REVIEW\nThough the transcription of monophonic musical passages\nis considered a solved problem [5], the transcription ofpolyphonic music is still an open problem [1]. Several\ntechniques have been proposed to accomplish the task of\npolyphonic transcription, including human audition mod-\nelling [12]; salience methods, which apply transformations\nto the input audio signal in order to emphasize the underly-\ning fundamental frequencies [17]; iterative and joint funda-\nmental frequency estimation algorithms followed by note\ntracking algorithms [2]; and a variety of machine learning\nmethods such as non-negative matrix factorization [14],\nsupport vector machines [8], neural networks [6], and hid-\nden Markov models [10].\nSeveral algorithms have also been proposed to generate\ntablature from a symbolic music score according to crite-\nria that minimizes the performance difﬁculty of the tab-\nlature arrangement. Heijink and Meulenbroek [4] estab-\nlished that guitarists have a disposition toward instrument\nﬁngering positions that are biomechanically easy to per-\nform. Speciﬁcally, guitarists favour hand positions near\nthe head of the guitar and avoid composing arrangements\nthat require extensive hand repositioning and large ﬁnger\nspans. Shortest-path graph search algorithms [13], con-\nstraint satisfaction algorithms [9], neural networks [15],\nand genetic algorithms [16] have been used to search for\ntablature arrangements with minimal performance difﬁculty.\n3. FRAMEWORK DESIGN\nThe Robotaba transcription framework is composed of three\nmodules: a polyphonic transcription module, a guitar tab-\nlature arrangement module, and a guitar tablature engrav-\ning module (Figure 2).\nThree beneﬁts arise from this modular design: First,\neach module can be used independently or together. Used\nindependently, an input ﬁle is sent directly to a module for\nprocessing, which returns a result instead of passing the\noutput to the next module in the workﬂow. Using each\nmodule in sequence, guitar tablature can be generated from\nan input audio ﬁle and displayed in the web browser. Sec-\nond, the modular design facilitates algorithm interchange-\nability. Assuming an algorithm produces valid output, it\ncan be inserted into a module without disturbing the func-\ntionality of surrounding modules. As a result, the tran-\nscription framework can accommodate new state-of-the-art\npolyphonic transcription or guitar tablature arrangement\nalgorithms without substantial changes to the web appli-\ncation. Third, the use of a single symbolic music ﬁle for-\nmat for data interchange between modules promotes poly-\nphonic transcription and tablature arrangement algorithms\nto adhere to a common interface. Robotaba uses the 2012\nrelease of the music encoding initiative (MEI): an extensi-\nble markup language (XML) ﬁle format that encodes sym-\nbolic music notation in a hierarchical fashion [11].\n3.1 Polyphonic Transcription Module\nThe polyphonic transcription module accepts an audio ﬁle\nas input, which is passed to the polyphonic transcription\nalgorithm embedded in the module. The polyphonic tran-\nscription algorithm is responsible for generating an MEI\nFigure 2 . Modular architecture of the Robotaba guitar tab-\nlature transcription framework.\nﬁle containing the estimates of note events occurring in the\ninput audio ﬁle. The polyphonic transcription module op-\ntionally postprocesses the output symbolic music ﬁle by\nlimiting the number of simultaneous notes to six, and dis-\ncarding or transposing estimated notes that are outside of\nthe range of a speciﬁc guitar. Properties of the guitar (num-\nber of frets, tuning, and capo position)1and postprocess-\ning options are speciﬁed by the user of the web application.\n3.2 Guitar Tablature Arrangement Module\nThe guitar tablature arrangement module accepts an MEI\nﬁle as input, which is mandatorily preprocessed in an iden-\ntical manner as the postprocessing step of the polyphonic\ntranscription module described in the previous section. The\npreprocessed MEI ﬁle is subsequently passed to the guitar\ntablature arrangement algorithm embedded in the module,\nwhich is responsible for assigning a guitar string and fret\ncombination to each note occurring in the MEI ﬁle.\n3.3 Guitar Tablature Engraving Module\nThe guitar tablature engraving module is responsible for\nparsing an MEI ﬁle containing a sequence of note events\nthat have each been assigned a guitar string and fret com-\nbination and displaying the encoded tablature in the web\nbrowser. Robotaba uses the digital guitar tablature engrav-\ning library AlphaTab2to render tablature symbols on the\nhypertext markup language (HTML) canvas element. Al-\nphaTab parses drawing scripts called AlphaTex, in which\nstructured keywords inform the rendering engine about the\ncontents of the tablature and how it should be displayed.\nWhen an MEI ﬁle is passed to the tablature engraving mod-\nule, the contents of the ﬁle are converted to an AlphaTex\ndrawing script to be rendered by AlphaTab. A rendered\ntablature score can be seen in Figure 1.\n3.4 Framework Implementation\nRobotaba is implemented using Django,3a Python web\nframework that facilitates rapid development of database-\ndriven web applications by automatically translating Python\nclasses called models into relational database tables. Mod-\nels are created for audio and symbolic music ﬁles, as well\nas their associated metadata. Additionally, the database\n1A capo is a device that is clipped onto the fretboard of a guitar and\nraises the pitches of the open strings.\n2www.alphatab.net\n3www.djangoproject.comstores the user-speciﬁed parameters used to generate a tran-\nscription, such as the guitar properties and ﬁle pre and post-\nprocessing options, to allow reproducibility of results.\n4. TRANSCRIPTION WEB APPLICATION\nUsing the Robotaba framework, a web application for gui-\ntar tablature transcription is developed that incorporates\nthe polyphonic transcription and guitar tablature arrange-\nment algorithms described in this section.4\n4.1 Polyphonic Transcription Algorithm\nA polyphonic transcription application, which uses the state-\nof-the-art algorithm proposed by Zhou and Reiss [17], is\nimplemented. This algorithm was selected for several rea-\nsons: First, this algorithm ranked highest out of the poly-\nphonic transcription algorithms evaluated in the Music In-\nformation Retrieval Evaluation eXchange (MIREX) on the\npiano dataset from 2007–2012 when considering the ac-\ncuracy of pitch and note onset times only. Second, the\nauthors tuned underlying parameters of the algorithm ac-\ncording to a dataset composed of both piano and guitar\nrecordings [17]. Third, this algorithm is capable of per-\nforming polyphonic transcriptions in realtime. Finally, the\nsource code of this algorithm is open source.\nThe aforementioned polyphonic transcription algorithm\nis distributed as a Vamp plugin written in the C++ pro-\ngramming language. A Vamp plugin is an audio feature\nextraction module that must be “plugged into” a host ap-\nplication.5In order to integrate the polyphonic transcrip-\ntion Vamp plugin into Robotaba, the plugin is ﬁrst divorced\nfrom the host to produce a standalone application. A Python\ninterface is created using the Boost.Python library6to ac-\ncess the standalone application from Robotaba. A Python\napplication is implemented, which sets parameters of the\npolyphonic transcription algorithm, imports an audio ﬁle,\nsends the audio data to the Python bindings of the poly-\nphonic transcription Vamp plugin, and generates an MEI\ndocument containing the resulting note event estimates.\n4.2 Guitar Tablature Arrangement Algorithm\nA new guitar tablature arrangement algorithm entitled A-\nstar-guitar is developed, written in the Python program-\nming language, and embedded in the Robotaba guitar tab-\nlature arrangement module. Extending the approach pro-\nposed by Sayegh [13], which uses the Viterbi algorithm\nto search for an optimal path through a weighted graph of\ncandidate fretboard locations for notes in a monophonic\nmusical passage, A-star-guitar uses the popular A* pathﬁnd-\ning algorithm [3] to search for an optimal tablature ar-\nrangement of a sequence of notes and chords in a poly-\nphonic musical passage.\nThe A* pathﬁnding algorithm searches for an optimal\npath through a directed graph, in which vertices represent\ncandidate string and fret combinations for a note or chord\n4ddmal.music.mcgill.ca/robotaba\n5www.vamp-plugins.org\n6www.boost.org/libs/python/docin a symbolic music score. Candidate string and fret com-\nbinations are calculated by considering the number of frets\non the user’s guitar, the tuning, and optional fret position\nof a capo. Vertices that correspond to adjacent notes or\nchords in the music score are connected by an edge.\nThe weight of an edge wij2Nbetween vertices iand\njrepresents the biomechanical difﬁculty associated with\nthe transition between the two hand positions on the fret-\nboard. Following the study of left-hand movements of pro-\nfessional guitar players by Heijink and Meulenbroek [4],\nthe edge weight between two vertices is the cumulation of\nthree biomechanical complexity factors: the fretwise dis-\ntance that the fretting hand must move to accommodate\nnote transitions, the fretwise ﬁnger span required to per-\nform chords, and a penalty of one if the fretting hand sur-\npasses the seventh fret. The values of this penalty and fret\nthreshold number were chosen on the basis of preliminary\ntests and encourage tablature arrangements near the begin-\nning of the fretboard. In the event of a note played by de-\npressing fret number f, followed by a chord comprised of\nmultiple notes with the set of fret numbers g, the fretwise\ndistance is calculated by the expression\nabs\u0012\nf\u0000\u0012max(g)\u0000min(g)\n2\u0013\u0013\n: (1)\nThe A* algorithm uses a heuristic function that returns\nan estimate of the summation of edge weights from a given\nvertex to the goal vertex. This must be an admissible heuris-\ntic: the cost of traversing the graph from a vertex to the\ngoal vertex must not be overestimated. Using the pro-\nposed biomechanical complexity factors to assign weights\nto edges, the heuristic function is zero for all vertices and\nthe resulting algorithm is effectively the Dijkstra pathﬁnd-\ning algorithm [3]. To illustrate why the heuristic function\nis zero, consider a music score with a sequence of identi-\ncal notes that may be performed by repeatedly plucking an\nopen string on the guitar.7In this case, each edge connect-\ning the open-string vertices has a weight of zero: there is\nno fretwise distance between notes, there are no chords in\nthe music score, and no frets are depressed.\nFigure 3 displays a directed acyclic weighted graph of\ncandidate string and fret combinations for an example mu-\nsic score consisting of four notes above the tablature ar-\nrangement produced by the A* search algorithm.\n5. TRANSCRIPTION EVALUATION\nThis section focuses on the procedure for evaluating the\nimplemented transcription web application, speciﬁcally the\npolyphonic transcription and guitar tablature arrangement\nalgorithms embedded within.\n5.1 Ground-truth Datasets\nFor the purposes of evaluating polyphonic transcription and\nguitar tablature arrangement algorithms, two new ground-\ntruth datasets are presented. The datasets were compiled\n7The term open string refers to a pluck whereby no fret is depressed.Figure 3 . A directed acyclic weighted graph of candidate\nstring and fret combinations for a music score consisting of\nfour notes. The bold edges in the graph identify the optimal\npath found by the A* search algorithm. Edge weights are\nomitted for space considerations.\nby harvesting the wealth of community-moderated and pub-\nlicly available manual transcriptions of popular musical\nworks uploaded to the Ultimate Guitar tablature website.8\nManual transcriptions were downloaded as Guitar Pro\nsymbolic music ﬁles: a proprietary ﬁle format that is read\nand manipulated by the Guitar Pro desktop application.9\n75 Guitar Pro ﬁles were selected from the Ultimate Gui-\ntar Top 100 list and the Ultimate Guitar Fresh Tabs list.\nThe Ultimate Guitar Top 100 list sorts every Guitar Pro\nﬁle uploaded to the website by its rating—from one to ﬁve\nstars—and displays the top 100 ﬁles. The Ultimate Gui-\ntar Fresh Tabs list is a catalogue of Guitar Pro ﬁles that\nhave recently been uploaded to the website and is sorted by\nnumber of views. Only uploaded tablature with a ﬁve-star\nrating agreed upon by at least ten unique users was consid-\nered for selection. Tablature was selected on the basis of\nmusical genre, average polyphony, and tempo, in order to\ncompile a set of pieces with a variety of different attributes.\n5.1.1 Polyphonic Guitar Transcription Dataset\nTo form the polyphonic guitar transcription dataset, sev-\neral preprocessing steps were ﬁrst applied to each Guitar\nPro ﬁle. These symbolic music ﬁles often contain multi-\nple instrument tracks, many of which are not guitar tracks.\nExtraneous tracks containing instruments such as the bass\nguitar, drums, and vocals were removed. Finally, all tempo,\nvolume, and pan automations were removed from the re-\nmaining guitar track.\nEach guitar track was then synthesized using Guitar Pro’s\nclean anddistortion guitar model to create two audio ﬁles.10\nSubsequently, a ﬁle listing the pitch, onset time, and dura-\ntion of notes occurring in the synthesized audio ﬁles was\ngenerated. This ground-truth ﬁle was created by ﬁrst ex-\nporting the Guitar Pro ﬁle as a MusicXML ﬁle and re-\ntrieving pertinent information about the music score from\na MusicXML parser program, which was originally devel-\noped to gather information from symbolic music scores for\nthe purpose of ﬁnding patterns in the relationships between\n8www.ultimate-guitar.com\n9www.guitar-pro.com\n10Clean guitar refers to a guitar signal with no audio effects applied.melody, lyrics, and instrumentation [7].\nThe polyphonic guitar transcription dataset consists of\n75 isolated guitar tracks; 125,192 note events; 30,914 chords;\nand an average tempo of 112 beats per minute. There are\napproximately ﬁve and a half hours of clean guitar record-\nings and ﬁve and a half hours of distortion guitar record-\nings, yielding approximately eleven hours of synthesized\naudio in total.\n5.1.2 Guitar Tablature Arrangement Dataset\nThe ground-truth dataset for guitar tablature arrangement\nwas compiled using the same 75 Guitar Pro ﬁles that were\ncollected and preprocessed for the polyphonic guitar tran-\nscription dataset. Excerpts were selected from the sym-\nbolic music ﬁles on the basis of overall length (no one\nexcerpt exceeds eight measures), the number of times the\nexcerpt occurred throughout the entire piece, and the aver-\nage polyphony of the excerpt. Each excerpt in Guitar Pro\nwas exported as a MusicXML ﬁle, which was subsequently\nconverted to an MEI ﬁle using an open-source MusicXML\nto MEI conversion application written in Python.11\nThe ground-truth dataset for guitar tablature arrange-\nment consists of 75 tablature arrangements—with 4,845\nnotes and 1,143 chords—encoded in both the MEI and Mu-\nsicXML symbolic music ﬁle formats.\n5.2 Polyphonic Transcription Evaluation\nThe implemented polyphonic transcription algorithm is eval-\nuated using the compiled ground-truth dataset for poly-\nphonic guitar transcription. Using an evaluation procedure\nsimilar to the MIREX multiple fundamental frequency es-\ntimation and tracking task, the output of the polyphonic\ntranscription algorithm on each audio recording in the dataset\nis compared to the corresponding ground-truth note events\nusing the metrics of precision, recall, and f-measure. Pre-\ncision describes the ratio of correctly transcribed note events\nto the total number of estimated note events. Recall de-\nscribes the ratio of correctly transcribed note events to the\ntotal number of ground-truth note events. The f-measure\nstatistic combines precision and recall into a single metric.\nAn estimated note event is deemed to be correctly tran-\nscribed if the fundamental frequency is within 50 cents of\nthe ground-truth note event and the onset time is within a\n50-millisecond range of the ground-truth note event.\nFour experiments were conducted: Experiment 1 re-\nports the precision, recall, and f-measure of the polyphonic\ntranscription algorithm on the clean synthesized guitar\nrecordings, considering the accuracy of note pitch and on-\nset time only. Experiment 2 is identical to the ﬁrst ex-\nperiment, except that octave errors are ignored. Experi-\nment 3 and 4 are identical to Experiment 1 and 2, respec-\ntively, except that the polyphonic transcription algorithm\nis evaluated on the distortion synthesized guitar recordings\nin the ground-truth dataset. The results of these experi-\nments are presented in Table 1, alongside the results of the\npolyphonic transcription algorithm in the 2008 MIREX ex-\nperiments. In MIREX 2008, the polyphonic transcription\n11github.com/gburlet/musicxml-mei-conversionPRECISION RECALLf-MEASURE\nExperiment 1 0.71 0.42 0.50\nExperiment 2 0.75 0.45 0.53\nExperiment 3 0.48 0.36 0.39\nExperiment 4 0.56 0.42 0.46\nMIREX 2008 0.74 0.78 0.76\nTable 1 . Results of four experiments evaluating the imple-\nmented polyphonic transcription algorithm, alongside the\nresults of the same algorithm in MIREX 2008.\nalgorithm was evaluated on a relatively small dataset of pi-\nano recordings and considered the accuracy of note pitch\nand onset time.\nAnalyzing the results, the performance of the polyphonic\ntranscription algorithm on clean guitar recordings is signif-\nicantly better than on distortion guitar recordings. A possi-\nble contributing factor to the reduced transcription perfor-\nmance on distortion guitar recordings could be the modi-\nﬁcation of the relative amplitudes of harmonics in the fre-\nquency domain of the guitar signal caused by the distortion\naudio effect. In line with past research, the performance\nof the polyphonic transcription algorithm improves when\nignoring octave errors. The polyphonic transcription algo-\nrithm receives inferior results on the compiled dataset in\ncomparison to the piano dataset used in the 2008 MIREX.\nApart from containing pieces of a different genre, one pos-\nsible explanation for the degraded performance is that the\nsynthesized guitar recordings in the compiled dataset con-\ntain ornamentation such as slides, bends, hammer-ons,\nhammer-offs, palm-muting, and dead notes. The piano is\nincapable of replicating many of these ornaments.\n5.3 Guitar Tablature Arrangement Evaluation\nThe implemented guitar tablature arrangement algorithm is\nevaluated using the compiled ground-truth dataset for gui-\ntar tablature arrangement. Since there is no standardized\nmethod for evaluating guitar tablature arrangement algo-\nrithms, a new evaluation method is used. The evaluation is\nperformed by calculating a ﬁtness value\nf=1\nfGT\u0010\n1 +PN\ni=1wi\u0011 (2)\nfor each generated tablature arrangement that is normal-\nized with respect to the ﬁtness of the corresponding ground-\ntruth tablature arrangement fGT. In Equation 2, Nis the\nnumber of vertices in the optimal path returned by the A*\nsearch algorithm and wiis the weight of the ithedge along\nthe optimal path. The normalized ﬁtness value f2R+is\ninterpreted as the biomechanical ease of performing a gen-\nerated tablature arrangement relative to the ground-truth\ntablature arrangement.\nBy the central limit theorem, it can be assumed that the\ndistribution of normalized ﬁtness values for guitar tabla-\nture arranged using the A* search algorithm approximately\nFigure 4 . Gaussian distribution N(^\u0016= 1:11;^\u001b= 0:17)\nof normalized ﬁtness values for guitar tablature arranged\nusing the A* search algorithm.\nfollows a normal distribution. The median ^\u0016and stan-\ndardized median absolute deviation ^\u001bare robust descrip-\ntive statistics that are used to describe the central tendency\nand dispersion, respectively, of this distribution. Robust\ndescriptive statistics are used because they are less sen-\nsitive to outliers present in small datasets in comparison\nto the sample mean and sample standard deviation. The\nresulting Gaussian distribution N(^\u0016= 1:11;^\u001b= 0:17)of\nnormalized ﬁtness values is displayed in Figure 4.\nHaving a median normalized ﬁtness value of ^\u0016= 1:11,\nthe A* search algorithm generates guitar tablature arrange-\nments that, on average, are of similar performance difﬁ-\nculty as tablature arranged by humans. By the empirical\nrule, 68.2% of tablature arrangements generated by the A*\nsearch algorithm are estimated to have normalized ﬁtness\nvalues that lie within one standard deviation of the mean\n(0.94–1.28).\n6. CONCLUSION\nAn open-source and web-based guitar tablature transcrip-\ntion framework, entitled Robotaba , is designed and im-\nplemented.12The framework facilitates the rapid devel-\nopment of guitar tablature transcription web applications,\nproviding a vessel for music researchers to publicize their\npolyphonic transcription and guitar tablature arrangement\nalgorithms, while allowing researchers to focus on algo-\nrithm development instead of application development.\nAs a proof of concept, a guitar tablature transcription\nweb application is developed using the Robotaba frame-\nwork. An open-source polyphonic transcription applica-\ntion is implemented, which uses the state-of-the-art algo-\nrithm proposed by Zhou and Reiss [17].13A new guitar\ntablature arrangement algorithm, which uses the popular\nA* pathﬁnding algorithm, is proposed and implemented as\nan open-source application.14\n12github.com/gburlet/robotaba\n13github.com/gburlet/zhoutranscription\n14github.com/gburlet/astar-guitarTwo ground-truth datasets are compiled from manual\ntablature transcriptions downloaded from the Ultimate Gui-\ntartablature website. The polyphonic guitar transcription\ndataset consists of 150 synthesized guitar recordings to-\ntalling approximately 11 hours of audio, which have been\nsemi-automatically annotated. The guitar tablature arrange-\nment dataset consists of 75 hand-arranged tablature scores\nencoded in the MEI ﬁle format. It is hoped that these\ndatasets will stimulate future research in the area of au-\ntomatic guitar tablature transcription.\nUsing the compiled ground-truth datasets, the imple-\nmented polyphonic transcription and guitar tablature ar-\nrangement algorithms are evaluated. Several experiments\nillustrate that the polyphonic transcription algorithm ex-\nhibits reduced precision and recall on guitar recordings\nwith a distortion audio effect applied. Moreover, in com-\nparison to the quality of transcriptions produced by the\npolyphonic transcription algorithm on piano recordings in\nthe 2008 MIREX evaluation, the algorithm receives infe-\nrior results on the compiled dataset of synthesized guitar\nrecordings. An evaluation of the proposed guitar tabla-\nture arrangement algorithm indicates that the algorithm ar-\nranges tablature that, on average, is of similar performance\ndifﬁculty as human-arranged tablature.\nNow that a framework exists to combine and evaluate\npolyphonic transcription and guitar tablature arrangement\nalgorithms, more work can be done to improve the algo-\nrithms themselves.\n7. ACKNOWLEDGEMENTS\nThis research was supported by the Social Sciences and\nHumanities Research Council of Canada. Special thanks\nare owed to the individuals who have uploaded manual\ntablature transcriptions to the Ultimate Guitar website, as\nwell as Ruohua Zhou and Joshua Reiss for the open-source\npolyphonic transcription algorithm used in this research.\n8. REFERENCES\n[1] Benetos, E., S. Dixon, D. Giannoulis, H. Kirchoff,\nand A. Klapuri. 2012. Automatic music transcription:\nBreaking the glass ceiling. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference , Porto, Portugal, 1002–7.\n[2] Chang, W., A. W. Su, C. Yeh, A. Roebel, and X.\nRodet. 2008. Multiple-F0 tracking based on a high-\norder HMM model. In Proceedings of the International\nConference on Digital Audio Effects , Espoo, Finland.\n[3] Hart, P., N. Nilsson, and B. Raphael. 1968. A formal\nbasis for the heuristic determination of minimum cost\npaths. IEEE Transactions on Systems Science and Cy-\nbernetics 4 (2): 100–7.\n[4] Heijink, H., and R. Meulenbroek. 2002. On the com-\nplexity of classical guitar playing: Functional adapta-\ntions to task constraints. Journal of Motor Behavior 34\n(4): 339–51.[5] Klapuri, A. 2004. Automatic music transcription as we\nknow it today. Journal of New Music Research 33 (3):\n269–82.\n[6] Marolt, M. 2004. A connectionist approach to auto-\nmatic transcription of polyphonic piano music. IEEE\nTransactions on Multimedia 6 (3): 439–49.\n[7] Nichols, E., D. Morris, S. Basu, and C. Raphael. 2009.\nRelationships between lyrics and melody in popular\nmusic. In Proceedings of the International Society for\nMusic Information Retrieval Conference , Kobe, Japan,\n471–6.\n[8] Poliner, G., and D. Ellis. 2007. Improving generaliza-\ntion for polyphonic piano transcription. In Proceedings\nof the IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics , New Paltz, NY , 86–9.\n[9] Radicioni, D., and V . Lombardo. 2005. Guitar ﬁnger-\ning for music performance. In Proceedings of the In-\nternational Computer Music Conference , Barcelona,\nSpain, 527–30.\n[10] Raphael, C. 2002. Automatic transcription of piano\nmusic. In Proceedings of the International Society\nfor Music Information Retrieval Conference , Paris,\nFrance, 1–5.\n[11] Roland, P. 2002.The music encoding initiative (MEI).\nInProceedings of the International Conference on Mu-\nsical Applications using XML , Milan, Italy, 55–9.\n[12] Ryyn ¨anen, M., and A. Klapuri. 2005. Polyphonic mu-\nsic transcription using note event modeling. In Pro-\nceedings of the IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics , New Paltz, NY ,\n319–22.\n[13] Sayegh, S. 1989. Fingering for string instruments with\nthe optimum path paradigm. Computer Music Journal\n13 (3): 76–84.\n[14] Smaragdis, P., and J. Brown. 2003. Non-negative ma-\ntrix factorization for polyphonic music transcription. In\nProceedings of the IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , New Paltz,\nNY , 177–80.\n[15] Tuohy, D., and W. Potter. 2006. An evolved neural\nnetwork/HC hybrid for tablature creation in GA-based\nguitar arranging. In Proceedings of the Computer Mu-\nsic Conference , New Orleans, LA, 576–9.\n[16] Tuohy, D., and W. Potter. 2006. GA-based music\narranging for guitar. In Proceedings of the IEEE\nCongress on Evolutionary Computation , Vancouver,\nBC, 1065–70.\n[17] Zhou, R., and J. Reiss. 2008. A real-time poly-\nphonic music transcription system. In the Mu-\nsic Information Retrieval Evaluation eXchange ,\nwww.music-ir.org/mirex/abstracts/\n2008/F0_zhou.pdf ."
    },
    {
        "title": "Basic Evaluation of Auditory Temporal Stability (Beats): A Novel Rationale and Implementation.",
        "author": [
            "Zhouhong Cai",
            "Robert J. Ellis",
            "Zhiyan Duan",
            "Hong Lu 0001",
            "Ye Wang 0007"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415096",
        "url": "https://doi.org/10.5281/zenodo.1415096",
        "ee": "https://zenodo.org/records/1415096/files/CaiEDLW13.pdf",
        "abstract": "The accurate detection of pulse-level temporal stability has important practical applications; for example, the creation of fixed-tempo playlists for recreational exercise (e.g., jogging), rehabilitation therapy (e.g., rhythmic gait training), or disc jockeying (e.g., dance mixes). Although there are numerous software algorithms which return simple point estimate statistics of “overall” tempo, none has operationalized the beat-to-beat stability of an interbeat interval series. We propose such a method here, along with several novel summary statistics. We illustrate this approach using a public data set (the 10,000-item subset of the Million Song Dataset) and outline a series of future steps for this project.",
        "zenodo_id": 1415096,
        "dblp_key": "conf/ismir/CaiEDLW13",
        "keywords": [
            "accurate detection",
            "pulse-level temporal stability",
            "practical applications",
            "fixed-tempo playlists",
            "rehabilitation therapy",
            "disc jockeying",
            "beat-to-beat stability",
            "interbeat interval series",
            "public data set",
            "Million Song Dataset"
        ],
        "content": "BASIC EVALUATION OF AUDITORY TEMPORAL STABILITY (BEATS):  \nA NOVEL RATIONALE AND IMPLEMENTATION \nZhuohong Cai1, Robert J. Ellis 1, Zhiyan Duan 1, Hong Lu 2, and Ye Wang 1 \n \n  \n \n1  School of Computing  \nNational University of Singapore  \n{ a0109706,ellis,  zhiyan, wangye} \n@comp.nus.edu   2  School of Computer Science \n   Fudan University \n  honglu@fudan.edu.cn  \n \nABSTRACT \nThe accurate detection of pulse0level temporal stab ility \nhas important practical applications; for example, the  \ncreation of fixed0tempo playlists for recreational exercise \n(e.g., jogging), rehabilitation therapy (e.g., rhyt hmic gait \ntraining), or disc jockeying (e.g., dance mixes). A lthough \nthere are numerous software algorithms which return  \nsimple point estimate statistics of “overall” tempo , none \nhas operationalized the beat0to0beat stability  of an inter0\nbeat interval series. We propose such a method here , \nalong with several novel summary statistics. We ill ustrate \nthis approach using a public data set (the 10,0000i tem \nsubset of the Million Song Dataset) and outline a s eries \nof future steps for this project.  \n \n1.  INTRODUCTION \nMotor synchronization with an auditory beat has bee n \ndeemed a human cultural universal [20] and a “diagn ostic \ntrait of our species” [16]. Even infants show perce ptual \nsensitivity to and motor coordination with musical \nrhythms [26,28]. A temporally stable beat facilitat es \nrhythmic human movement during leisure activities s uch \nas exercise (for recent reviews, see [10,11]). It a lso serves \nas the basis for a class of gait rehabilitation the rapies \nknown as “Rhythmic Auditory Stimulation” or “Rhyth0\nmic Auditory Cueing” for Parkinson’s disease (for r e0 \nviews, see [12,22]), stroke [25], and others [27]. \nNumerous beat tracking algorithms have been devel0 \noped which return a time series of detected beats f or a \ngiven audio input (for reviews, see [5,18]), return ing a \nsimple “beats per minute” point estimate of tempo. None \nof these algorithms, however, has attempted to oper atio0 \nnalize the beat0to0beat stability  of that tempo over time, \nother than occasional efforts to note whether multi ple ex0 \ncerpts taken from the same audio file have the same  ap0 \nproximate tempo. Such a coarse estimate of tempo st abili0 ty does not have the necessary precision for the ty pe of \nclinical applications cited above applications, whi ch not \nonly need to know if a given audio file is stable, but the \nprecise time indices at which it is stable (so as t o preserve \nthat information in the playlist). \nTo address these issues, we present a novel analysi s \ntool: “Basic Evaluation of Auditory Temporal Stabil ity” \n(BEATS) for Matlab (version ≥ 7.8). BEATS is not  a beat \ntracking algorithm; instead, it uses the output of an exist0 \ning beat tracking algorithm (i.e., beat and barline  onset \ntimestamps) to provide a full set of outcome statis tics. \nHere, we focus on the “Million Song Subset” of 10,0 00 \nmetadata files selected from the Million Song Datas et  [1] \n(http://labrosa.ee.columbia.edu/millionsong/), with  all \naudio files processed using the proprietary “Analyz e”  \nalgorithm [9] developed by The Echo Nest \n(www.echonest.com). Compatibility with this data so urce \nhas long0term advantages, as the full Echo Nest lib rary \ncontains over 34 million analyzed audio files.  \n \n2.  METHODS \n2.1  Data inputs \nBEATS pulls four Echo Nest fields from each metadat a \nfile: beats_start  and bars_start  (the estimated \nonsets of successive beats and barlines, respective ly); \nand tempo  and time_signature . Next, the \nbeats_start  and bars_start  vectors are trans0 \nformed into an inter0beat interval (IBeI) series an d an \ninter0bar interval (IBaI) series, respectively, by taking the \nfirst0order difference of each vector.   \n \n2.2  Initialization Thresholds \nBEATS requires the user to specify three Initializa tion \nThresholds: \n(1) “Local Stability Threshold”, θLocal : a percentage \nvalue (default = 5.0%) used to define temporal stab ility \nat the level of individual and successive IBeIs (de tailed \nbelow). \n(2) “Run Duration Threshold”, θRun : the minimum du0 \nration (default = 10 s) of a set of consecutive IBe Is (i.e., \na “Run”) that fall below θLocal .   \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page.  \n© 2013 International Society for Music Information Retrieval    \n \n(3) “Gap Duration Threshold”, θGap : the maximum du0 \nration (default = 2 s) between the last timestamp o f Run j \nand the first timestamp of Run j+1 .    \n \n2.3  Internal Calculations \nThe first statistic calculated by BEATS is the cent ral \ntendency, λ (for “location”) of the IBeI series: th e single \nvalue which best characterizes the predominant IBeI . \nObtaining an “optimal” value for λ can more challen ging \nthan simply taking the median or mode of a series. Con0 \nsider the hypothetical IBeI series S shown in Figure 1, \nwhich exhibits two tempo changes (at the 21st and 4 1st \nIBeIs). In Matlab, median (S) = 0.869 and mode (S) = \n0.477. (mode  is known to be problematic for both non0\ndiscrete and non0quantized data.) Neither statistic  effec0 \ntively captures the central tendency of S.   \nTo address this, we define an iterative loop in whi ch \nthe range of S is divided into an increasing odd number of \nbins k from 1 to 15. The loop stops at the largest value of \nk in which the most0frequent bin contains just over one0\nthird of the data (or quits when k = 15). λ is then defined \nas the median value within the most0frequent bin. U nder \nthis definition, S has a λ = 0.993, which better captures its \ncentral tendency. \nHaving derived λ, the longest “Stable Segment” with in \nan IBeI series can be identified. The first step in  this \nprocess is to quantify local temporal stability in two \nways: local deviations  (from λ) and local differences  (ad0 \njacent IBeIs). Local deviations are quantified by a n abso0 \nlute deviation from λ  (ADL), calculated for each e lement \ni of IBeI series S: \n/g1845ADL ,/g2191= 100×|/g3P2P/g2191/g2879/g2971|\n/g2971 .               (1)  \nLocal differences are quantified by a first0order a bsolute \nsuccessive difference (ASD), calculated for each el ement \ni of S:  \nSASD ,/g2191= 100×|/g3P2P/g2191/g2879/g3P2P/g3284/g3127/g3117|\n/g2868./g2873×/g4666/g3P2P/g3284/g2878/g3P2P/g3284/g3127/g3117/g4667\t,                  (2)  \nwhere SASD,1  = 0 to preserve the series indexing. Both \nSADL  and SASD  are expressed relatively (i.e., as percentag0 \nes) to facilitate comparisons across IBeI sequences  in dif0 \nferent tempo ranges.  \nNext, a binarized version of S (SBin ) is created:  \n/g1845Bin,/g3P36=/g46881, /g342P/g1845ADL,/g3P36\t\t≤\tθ/g2896/g2925/g2913/g2911/g2922 \n/g1845ASD,/g3P36\t≤\tθ/g2896/g2925/g2913/g2911/g2922 /g1\n0, otherwise /g1\t .                 (3) \nSBin  identifies the locations of Runs (i.e., strings of  1s) \nand Gaps (strings of 0s) within the IBeI series its elf. Fi0 \nnally, the Stable Segment is defined as the longest  se0 \nquence of {Run j, Gap j, Run j+1, … Gap n–1, Run n}, where each Run has a duration ≥ θRun , each intervening Gap has \na duration ≤ θGap , and the median IBeI value of each pair \nof neighboring Runs has a percent difference of ≤ θLocal . \n2.4  Outcome Statistics \nBEATS calculates six statistics from each Stable Se g0 \nment: \n(1) “Stable Duration” (in seconds): the time betwee n \nthe first and last time stamps of the longest run.  \n(2) “Stable Percentage”: Stable Duration relative t o \nthe duration of the entire IBeI series.  \n(3) “Estimated Tempo” (in beats per minute, BPM): \nthe median IBeI value within the Stable Segment, mu l0 \ntiplied by 60.  \n(4) “Estimated Meter”: a more precise definition th an \nthe typical beats0per0bar value. Specifically, for a Stable \nSegment with a bar timestamp series { ri, ri+1 , …} and \nbeat timestamp series { bj, bj+1 , …}, let Bi be the number \nof beat timestamps for which ri ≤ b j < r i+1 . Estimated \nMeter is then taken as the mean of all Bi. Only in the case \nwhen all Bi have the same value will an integer value re0 \nsult (e.g., 4.00), providing a simple way to identi fy the \npresence of a changing meter within the Stable Segm ent. \n(5) “Percentile of Absolute Deviations from λ” \n(PADL P): a statistically robust alternative to a percen0 \ntage0based coefficient of variation (CV), used exte nsive0 \nly in the gait literature (for a review, see [6]). For an IBeI \nseries S, CV is defined as the standard deviation of S di0 \nvided by the mean of S, and multiplied by 100. Because \nit makes use of the standard deviation, CV is susce ptible \nto inflation by high0value outliers (e.g., a beat t hat is \n“dropped” by the beat tracking algorithm). By contr ast, \nPADL P offers a more robust formulation:  \nPADM P=prc /g4666SADL ,/g1842/g4667\t,                (4) \n Figure 1 . A hypothetical IBeI series, with IBeI values \n(y0axis) plotted ordinally ( x0axis). The usual estimators \nof median and mode are both non0optimal. The newly0\nproposed λ statistic provides a better match.  \nIBeI (s) \nOrdinal IBeIs median = 0.869 \nmode = 0.477 λ = 0.993   \n \n \n \nFigure 2.  Visual illustration of the Outcome Statistics calc ulated by BEATS. Panel A shows the IBeI series ( y0axis) as a \nfunction of real time ( x0axis), with λ shown as a horizontal gray line (at y = .631) and the Stable Segment highlighted in \nfilled circles. Panel B shows the Stable Segment in  isolation; the best0fitting line (gray line) revea ls a lack of temporal \ndrift. Panel C shows all the number of beats per ba r for all detected downbeats; Estimated Meter is co nsistent at four \nbeats per bar throughout the Stable Segment. \n \n \n \n \nFigure 3.  Histogram summaries of the six Outcome Statistics across the full 10,0000item dataset. \nAA AA\nCC CCStable Duration = \nStable Percentage = 60.9 s \n40.6 % \nBB BB\nPADL 90 = \nPASD 90 = 3.1 % \n1.4 % \nCount Count Count Count Count Count Stable Duration (s) Stable Percentage (%) \nEstimated Meter (beats per bar) Estimated Tempo (BPM) \nPADL 90 (%) PASD 90 (%) AA AA BB BB\nCC CC DD DD\nEE EE FF FF  \n \nwhere prc (SADL ,P) is the Pth percentile of the SADL  vec0 \ntor (Eq. 1).  \n (6) “Percentile of Absolute Successive Differences ” \n(PASD P): a robust alternative to the root0mean0square of \nsuccessive differences (rMSSD), widely used in stud ies \nof heart rate variability to quantify beat0to0beat fluctua0 \ntions (for a review, see [24]. However, just as squ aring \ndeviations from the mean make the standard deviatio n \nsusceptible to outliers,  squaring successive diffe rences \nyields a potentially inflated RMSSD. By contrast, \nPASD P is defined as:   \n\tPASD/g3P17=prc /g4666|SASD |,/g1842/g4667 .               (5) \nFor both PADL P and PASD P, BEATS uses P = 90 as \nits default. In practice, however, any value of P  \nbetween 0 and 100 may be used. \n \n2.5  Implementation \nBEATS was run on the 10,0000item dataset using its de0 \nfault Initialization Thresholds (Section 2.2). (The  ratio0 \nnale behind θLocal  = 5.0% is explained in Section 4.1.) \n \n3.  RESULTS AND DISCUSSION \nFigure 2  presents a visual illustration of the six Outcome \nStatistics calculated by BEATS, in a single file fr om the \nMillion Song Subset (Track TRAHNHL128F14A4DDD: \n“In the Hall of the Mountain King” by Edvard Grieg,   \nperformed by the Staatskapelle Dresden; available a t \nhttp://open.spotify.com/track/2cTXwtIFEeCNa0ZtbI97z h ). \nThis work is famous for its accelerando , which can be \nseen in the IBeI plot of Figure 2A (albeit with som e con0 \nfusion on the part of the Echo Nest “Analyze” algor ithm \n[9], a point discussed further in Section 4.1). Suc h a re0 \ncording would be of limited use for a constant0temp o ex0 \nercise  paradigm. A temporally stable segment, howe ver \n(using θLocal  = 5.0%), can in fact be found between the \n0′08″ and 1′09″, which can be more clearly apprecia ted in \nFigure 2B. The identified Stable Segment has a PADL 90  \n= 3.1% and a PASD 90  = 1.4%, markedly different than if \nthose statistics are calculated from the entire  IBeI series \n(PADL 90  = 23.3% and a PASD 90  = 3.1%). Finally, Figure \n2C shows the number of beats per bar within the Sta ble \nSegment; this yields an Estimated Meter = 4. \nFigure 3 presents a histogram for each of the six \nBEATS Output Statistics across the full 10,0000file  data \nset. Of particular note is Figure 2B, which indicat es that \nStable Percentage varied widely across the data set . In0 \ndeed, only 18.6% of files were deemed temporally st able \n(i.e., as defined by θLocal = 5.0%) over their entire dura0 \ntion (i.e., Stable Percent = 100). In other words, the prob0 \nability that a song randomly selected from the MSD can \nbe played it in its entirety as part of a rhythmic movement paradigm (i.e., has a moderately stable perceptual tempo \nwith less than 5.0% local tempo variability) is < 2 0%. \nFigure 4 presents a slightly different picture, plo tting \nthe percentage of files ( y0axis) with a Stable Duration ≥ \nthe x0axis value. Allowing BEATS to identify the Stable \nSegment within each audio file (if present) yields a higher \npercentage of files available for exercise playlist s; for ex0 \nample, 55.7% of files are temporally stable over a dura0 \ntion of ≥ 90 s within the file—three times the numb er of \nfiles that are stable over their entire duration.   \n  \n \n \n \nFigure 4.  The percentage of files in the 10,0000item  \ndataset which have a Stable Duration ≥ the correspo nding  \nx0axis value. \n \n The power of BEATS lies in the flexible way its Ou t0 \ncome Statistics may be combined to deliver a stimul us set \noptimized to a user’s specific needs. For example, a gait \ntraining paradigm requiring highly stable music mig ht \nuse the following set of inclusion criteria: Stable  Duration \n≥ 120 seconds, Tempo between 50 and 150 BPM,   \nEstimated Meter = 4.0, PADL 90  ≤ 2.5, and PASD 90  ≤ 2.5. \nThis combination of inclusion criteria retains 24.2 % of \nthe 10,0000file dataset (again, using θLocal  = 5.0%). Al0 \nthough this percentage may seem low, it is scalable . That \nis, assuming that the remainder of the Million Song  Data0 \nset yields similar distributions for the six Outcom e Statis0 \ntics, nearly 250,000 candidate songs could be made avail0 \nable for rhythmic synchronization paradigms (using these \nsame inclusion criteria), and more still if the ent ire  \n340million0item Echo Nest library were leveraged. \n4.  CONCLUSIONS, CAVEATS, AND  \nFUTURE DIRECTIONS \nWe present a novel tool to evaluate auditory tempor al \nstability (BEATS). An important departure that BEAT S \nmakes from previous methods is that it seeks to ide ntify \nthe most temporally stable  segment within an inter0beat \ninterval (IBeI) series for an entire audio file, ra ther than 0 30 60 90 120 150 180 210 240 270 Percentage of available files \nStable Duration (s) 7090\n50 \n30 \n10   \n \nderive a point estimate of tempo for the entire IBe I series.  \nThis increased flexibility enables BEATS to identif y a \ngreater number of candidate pieces of music that sa tisfy \nthe requirements of rhythmic exercise applications.   \n \n4.1  Caveats \nFor ease of illustration in the present report, a s ingle  \nLocal Stability Threshold ( θLocal  = 5.0%) was used to in0 \nstantiate BEATS and generate the associated figures . This \nvalue was chosen based on prior studies which have ex0 \nplored just0noticeable differences (JNDs) for chang es in \ntempo (e.g., [3,8,19,23]), with reported values ran ging \nfrom 10% (for single pairs of intervals) to 2% (for  longer \nsequences). The stimuli in each of these cited stud ies, \nhowever, were all (1) isochronous (i.e., all interv als \nequally spaced in time), and (2) contained no more than \n10 temporal intervals per sequence.  Both factors l imit the \ngeneralizability of these studies to actual extende d ex0 \ncerpts of music, which is frequently (1) non0isochr onous \nand (2) of a longer duration (enabling a stronger r eference \ntempo to be formed, and thus a more finely tuned ab ility \nto detect change). θLocal  = 5.0% was chosen as a compro0 \nmise, but warrants further experimental validation.   That \nis, determining a threshold for “perceptually stabl e” in a \nnon0isochronous IBeI series with varying degrees of  local \nand global variability across trials (and across di fferent \ntempo ranges) would greatly increase the utility of  \nBEATS. \n Another issue, highlighted by Figure 2, concerns t he \naccuracy of the beat tracking algorithm itself. Tha t is, \nBEATS is ignorant of the fidelity of the algorithm used to \nderive an inter0beat and inter0bar interval series.  In the \ncase of Figure 2, the derived IBeI series (as deriv ed by \nthe Echo Nest “Analyze” algorithm [9]) does not mat ch \nthe steady acceleration of tempo present within the  audio \nfile. Furthermore, preliminary exploration of the 1 0,0000\nitem dataset suggests that highly complex or multi0\nlayered rhythm loops that have an underlying percep tual \npulse may nevertheless flummox a beat tracking algo 0 \nrithm.  \n Although this may mean that BEATS is conservative \n(in that it will classify some pieces of music as “ tempo0 \nrally unstable” when they in fact may not be), such  con0 \nservativeness may be beneficial in practice, as it will rule \nout pieces of music that may in fact be too challen ging  \nfor listeners to synchronize with.  \n Alternatively, research from another sub0domain of  \naudio content analysis, score–performance matching  \n(e.g., [7,21]), may provide techniques to more robu stly \nquantify changes in tempo over time, enhancing the abili0 \nty of BEATS to detect excerpts of tempo stability. \n \n 4.2  Future Directions \nBy summarizing temporal stability using simple summ ary \nstatistics, the output of BEATS can become the inpu t to \nsearch engines for which tempo is a key feature (e. g., \n[4,14,15]). In its current state, however, BEATS is  a work \nin progress. Our own future goals for this project include \n(1) implementing BEATS on much larger datasets (suc h \nas the entire Million Song Dataset, or even larger Echo \nNest datasets), and (2) developing a high0quality w eb0\nbased user interface (“iBEATS”) that will offer vis ualiza0 \ntions (box plots, scatter plots) and flexible param eter set0 \ntings (buttons and sliders) to efficiently sort and  sift \nthrough large amounts of metadata (including artist , re0 \nlease date, and genre tags) to create customized pl aylists \nfor clinical (e.g., gait rehabilitation) or commerc ial (e.g., \nrhythmic exercise) applications. \n5.  ACKNOWLEDGMENT \nWe thank three anonymous reviewers for helpful com0\nments. This research is supported by the Singapore Na0 \ntional Research Foundation under its International  \nResearch Centre @ Singapore  funding initiative, and  \nadministered by the Interactive Digital Media Progr amme \nOffice.  \n6.  REFERENCES \n[1]  T. Bertin0Mahieux, D. P. Ellis, B. Whitman, and P. \nLamere, “The million song dataset,” in ISMIR 2011: \nProceedings of the 12th International Society for \nMusic Information Retrieval Conference , October \n24028, 2011, Miami, Florida, 2011, pp. 591–596. \n[2]  S. Dixon, “An interactive beat tracking and \nvisualisation system,” in Proceedings of the \nInternational Computer Music Conference , Havana, \nCuba, 2001, pp. 215–218. \n[3]  C. Drake and M. C. Botte, “Tempo sensitivity in \nauditory sequences: evidence for a multiple0look \nmodel,” Percept. Psychophys. , vol. 54, no. 3, pp. \n277–286, Sep. 1993. \n[4]  F. Gouyon, “Dance music classification: A tempo0\nbased approach,” in Proceedings of the International \nConference on Music Information Retrieval , \nBarcelona, 2004. \n[5]  F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. \nTzanetakis, C. Uhle, and P. Cano, “An experimental \ncomparison of audio tempo induction algorithms,” \nAudio Speech Lang. Process. Ieee Trans ., vol. 14, \nno. 5, pp. 1832–1844, 2006. \n   \n \n[6]  J. M. Hausdorff, “Gait dynamics in Parkinson’s \ndisease: common and distinct behavior among stride \nlength, gait variability, and fractal0like scaling, ” \nChaos Woodbury N , vol. 19, no. 2, p. 026113, Jun. \n2009. \n[7]  H. Heijink, P. Desain, H. Honing, and L. Windsor, \n“Make me a match: An evaluation of different \napproaches to score—performance matching,” \nComput. Music J. , vol. 24, no. 1, pp. 43–56, 2000. \n[8]  R. B. Ivry and R. E. Hazeltine, “Perception and \nproduction of temporal intervals across a range of \ndurations: Evidence for a common timing \nmechanism,” J. Exp. Psychol. Hum. Percept. \nPerform. , vol. 21, no. 1, pp. 3–18, 1995. \n[9]  T. Jehan, “Analyzer Documentation.” 2011. \nhttp://developer.echonest.com/docs/v4/_static/ \nAnalyzeDocumentation.pdf \n[10]  C. I. Karageorghis and D.0L. Priest, “Music in the \nexercise domain: a review and synthesis (Part I),” \nInt. Rev. Sport Exerc. Psychol ., vol. 5, no. 1, pp. 44–\n66, Mar. 2012. \n[11]  C. I. Karageorghis and D.0L. Priest, “Music in the \nexercise domain: a review and synthesis (Part II),”  \nInt. Rev. Sport Exerc. Psychol ., vol. 5, no. 1, pp. 67–\n84, Mar. 2012. \n[12]  I. Lim, E. Van Wegen, C. De Goede, M. Deutekom, \nA. Nieuwboer, A. Willems, D. Jones, L. Rochester, \nand G. Kwakkel, “Effects of external rhythmical \ncueing on gait in patients with Parkinson’s disease : a \nsystematic review,” Clin. Rehabil ., vol. 19, no. 7, pp. \n695–713, 2005. \n[13]  J. Langner and W. Goebl, “Visualizing expressive \nperformance in tempo0loudness space,” Comput. \nMusic J ., vol. 27, no. 4, pp. 69–83, 2003. \n[14]  Z. Li, Q. Xiang, J. Hockman, J. Yang, Y. Yi, I. \nFujinaga, and Y. Wang, “A music search engine for \ntherapeutic gait training,” in Proceedings of the \ninternational conference on Multimedia , 2010, pp. \n627–630. \n[15]  Z. Li and Y. Wang, “A domain0specific music \nsearch engine for gait training,” in Proceedings of \nthe 20th ACM international conference on \nMultimedia , New York, NY, USA, 2012, pp. 1311–\n1312. \n[16]  B. H. Merker, G. S. Madison, and P. Eckerdal, “On \nthe role and origin of isochrony in human rhythmic \nentrainment,” Cortex , vol. 45, no. 1, pp. 4–17, 2009. \n[17]  M. F. McKinney and D. Moelants, “Ambiguity in \ntempo perception: What draws listeners to different  metrical levels?,” Music Percept ., vol. 24, no. 2, pp. \n155–166, 2006. \n[18]  M. F. McKinney, D. Moelants, M. E. P. Davies, and \nA. Klapuri, “Evaluation of audio beat tracking and \nmusic tempo extraction algorithms,” J. New Music \nRes ., vol. 36, no. 1, pp. 1–16, 2007. \n[19]  N. S. Miller and J. D. McAuley, “Tempo sensitivity \nin isochronous tone sequences: the multiple0look \nmodel revisited,” Percept. Psychophys. , vol. 67, no. \n7, pp. 1150–1160, 2005. \n[20]  B. Nettl, “An ethnomusicologist contemplates \nuniversals in musical sound and musical culture,” i n  \nThe origins of music , B. Wallin, B. Merker, and S. \nBrown, Eds. Cambridge, MA: MIT Press, 2000, pp. \n463–472. \n[21]  A. Robertson, “Decoding Tempo and Timing \nVariations in Music Recordings from Beat \nAnnotations.,” in ISMIR , 2012, pp. 475–480. \n[22]  T. C. Rubinstein, N. Giladi, and J. M. Hausdorff, \n“The power of cueing to circumvent dopamine \ndeficits: a review of physical therapy treatment of  \ngait disturbances in Parkinson’s disease,” Mov. \nDisord ., vol. 17, no. 6, pp. 1148–1160, 2002. \n[23]  H. H. Schulze, “The perception of temporal \ndeviations in isochronic patterns,” Percept. \nPsychophys. , vol. 45, no. 4, pp. 291–296, 1989. \n[24]  Task Force of the European Society of Cardiology \nand the North American Society of Pacing and \nElectrophysiology, “Heart Rate Variability: \nStandards of Measurement, Physiological \nInterpretation, and Clinical Use,” Circulation , vol. \n93, no. 5, pp. 1043–1065, Mar. 1996. \n[25]  M. H. Thaut, G. C. McIntosh, and R. R. Rice, \n“Rhythmic facilitation of gait training in hemipare tic \nstroke rehabilitation.,” J. Neurol. Sci ., vol. 151, no. \n2, pp. 207–212, Oct. 1997. \n[26]  I. Winkler, G. P. Háden, O. Ladinig, I. Sziller, an d \nH. Honing, “Newborn infants detect the beat in \nmusic,” Proc. Natl. Acad. Sci ., vol. 106, no. 7, pp. \n2468–2471, 2009. \n[27]  J. E. Wittwer, K. E. Webster, and K. Hill, \n“Rhythmic auditory cueing to improve walking in \npatients with neurological conditions other than \nParkinson’s disease0what is the evidence?,” Disabil. \nRehabil ., vol. 35, no. 2, pp. 164–176, 2013. \n[28]  M. Zentner and T. Eerola, “Rhythmic engagement \nwith music in infancy,” Proc. Natl. Acad. Sci . \nU.S.A., Mar. 2010."
    },
    {
        "title": "Social-EQ: Crowdsourcing an Equalization Descriptor Map.",
        "author": [
            "Mark Cartwright",
            "Bryan Pardo"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415792",
        "url": "https://doi.org/10.5281/zenodo.1415792",
        "ee": "https://zenodo.org/records/1415792/files/CartwrightP13.pdf",
        "abstract": "We seek to simplify audio production interfaces (such as those for equalization) by letting users communicate their audio production objectives with descriptive language (e.g. “Make the violin sound ‘warmer.’”). To achieve this goal, a system must be able to tell whether the stated goal is appropriate for the selected tool (e.g. making the violin “warmer” with a panning tool does not make sense). If the goal is appropriate for the tool, it must know what actions need to be taken. Further, the tool should not impose a vocabulary on users, but rather understand the vocabulary users prefer. In this work, we describe SocialEQ, a webbased project for learning a vocabulary of actionable audio equalization descriptors. Since deployment, SocialEQ has learned 324 distinct words in 731 learning sessions. Data on these terms is made available for download. We examine terms users have provided, exploring which ones map well to equalization, which ones have broadly-agreed upon meaning, which term have meanings specific small groups, and which terms are synonymous.",
        "zenodo_id": 1415792,
        "dblp_key": "conf/ismir/CartwrightP13",
        "keywords": [
            "audio production interfaces",
            "descriptive language",
            "communication",
            "audio production objectives",
            "tool selection",
            "appropriate goal",
            "tool understanding",
            "vocabulary learning",
            "web-based project",
            "actionable audio equalization descriptors"
        ],
        "content": "SOCIAL-EQ: CROWDSOURCING AN EQUALIZATION DESCRIPTOR\nMAP\nMark Cartwright\nNorthwestern University\nEECS Department\nmcartwright@u.northwestern.eduBryan Pardo\nNorthwestern University\nEECS Department\npardo@northwestern.edu\nABSTRACT\nWe seek to simplify audio production interfaces (such as\nthose for equalization) by letting users communicate their\naudio production objectives with descriptive language (e.g.\n“Make the violin sound ‘warmer.’”). To achieve this goal,\na system must be able to tell whether the stated goal is\nappropriate for the selected tool (e.g. making the violin\n“warmer” with a panning tool does not make sense). If\nthe goal is appropriate for the tool, it must know what ac-\ntions need to be taken. Further, the tool should not impose\na vocabulary on users, but rather understand the vocabulary\nusers prefer. In this work, we describe SocialEQ, a web-\nbased project for learning a vocabulary of actionable audio\nequalization descriptors. Since deployment, SocialEQ has\nlearned 324 distinct words in 731 learning sessions. Data\non these terms is made available for download. We exam-\nine terms users have provided, exploring which ones map\nwell to equalization, which ones have broadly-agreed upon\nmeaning, which term have meanings speciﬁc small groups,\nand which terms are synonymous.\n1. INTRODUCTION\nMuch of the work of audio production involves ﬁnding\nthe mapping between the terms in which a musician de-\nscribes an acoustic concept (e.g. “Make the violin sound\n‘warmer.’”) and the tools available to manipulate sound\n(e.g. the controls of a parametric equalizer). Often, map-\npings are non-obvious and require signiﬁcant work to ﬁnd.\nWe seek to simplify audio production interfaces (such\nas those for equalization) by letting users communicate\ntheir audio production objectives with descriptive language\n(“Make the violin sound ‘warmer.’”). To achieve this goal,\nthe system must be able to tell whether the stated goal\nis achievable for the selected tool (e.g. making the violin\n“warmer” with a panning tool does not make sense). It\nmust also know what actions need to be taken, given the\ncorrect tool (“Use the parametric equalizer to boost the 2-\n4 kHz and the 200-500 Hz bands by 4 dB”). Further, the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.tool should be aware of possible variations in the mapping\nbetween word and audio among users (Bob’s “warm” 6=\nSarah’s “warm”), and the tool should be aware of which\nwords are synonymous.\nIn this work, we describe SocialEQ, a project to crowd-\nsource a vocabulary of audio descriptors, grounded in per-\nceptual data that can be mapped onto concrete actions by\nan equalizer (EQ) to effect meaningful change to audio\nﬁles. SocialEQ has, to date, been taught 324 distinct words\nin 731 learning sessions. Through our analysis of the data\ncollected, we address the following questions:\n1. What audio descriptors are actionable by an audio\nequalizer?\n2. How widely-agreed-upon is the meaning of an EQ\ndescriptor?\n3. What EQ descriptors are true audio synonyms?\n2. BACKGROUND AND RELATED WORK\nThere is currently no universal dictionary of audio termi-\nnology that is deﬁned both in terms of subjective experien-\ntial qualities and measurable properties of a sound. Tools\nbuilt from text co-occurrence, lexical similarity and dic-\ntionary deﬁnitions (e.g. WordNet [13]) are fundamentally\ndifferent in their underpinnings and do not address the is-\nsue of how words map to measurable sound features.\nThere are some terms relating to pitch (high, low, up,\ndown) and loudness (soft, loud) that have relatively well-\nunderstood [6, 18] mappings onto measurable sound char-\nacteristics. Some terms of art used by recording engineers\n[8] describe effects produced by recording and production\nequipment, and are relatively easy to map onto measurable\nproperties. These include “compressed” (i.e., a compressor\nhas been applied to reduce the dynamic range of the audio)\nand “clipped” (i.e., the audio is distorted in a way charac-\nteristic of an overloaded digital recorder). These terms are\nnot, however, widely understood by either musicians or the\ngeneral public [21].\nNumerous studies have been performed over the last\nﬁfty years in the hopes of ﬁnding universal sound descrip-\ntors that map onto a set of canonical perceptual dimen-\nsions [4, 11, 20, 22]. Also, in the last decade or so, many\nresearchers coming from backgrounds such as recording\nengineering [8], music composition [19] and computer sci-\nence [16] have studied the space of terminology, seeking auniversal set of English terms for timbre. Researchers in\nacoustic signal processing and perception continue to seek\nEnglish taxonomies for timbre descriptors. Some seek gen-\neral taxonomies of sound descriptors [16]. Others are fo-\ncused on timbre adjectives for particular instruments [2,9].\nSuch studies typically start by determining a set of nat-\nural descriptors by performing a survey. These descriptors\nare then used in a second study, where participants evaluate\nsounds in terms of the descriptors. These are then mapped\nonto machine-measurable parameters, such as spectral tilt,\nsound pressure level, or harmonicity. Commonalities in\ndescriptor mappings are found between participants in the\nstudies and then some small set of descriptive terms are\nproposed as relatively universal.\nIn audio engineering, there has been work directly map-\nping equalizer parameters to commonly used descriptive\nwords using a ﬁxed mapping [12, 14]. A problem with\nthese approaches is that mappings are very time-consuming\nto develop, the former requires a painstaking training pro-\ncess for each user, and the latter is brittle with respect to\nthe variability that exists across users.\nIn contrast, [15] establishes the effectiveness of an ap-\nproach to automatically learning mappings of audio adjec-\ntives onto actionable controllers in a small study using 4\nresearcher-selected descriptors and 19 participants. This\nstudy was, however, quite small in the number of partici-\npants and the number of adjectives learned.\nDespite the efforts of all these groups, a universal map\nof descriptive terms for audio remains an unachieved goal.\nWe believe this is because the terms used range from\nwidely-agreed-upon (e.g. loud) to ones that have agree-\nment within groups but not between groups (e.g. warm ) to\nidiosyncratic terms meaningful solely to individuals (e.g.\nsplanky ).\nIn this work, rather than seek a set of universal underly-\ning dimensions of timbre or the universal meaning of a de-\nscriptive terms in all auditory contexts, we seek an under-\nstanding of descriptors in a speciﬁc context: audio equal-\nization.\n3. THE SOCIALEQ TASK\nSocialEQ.org is a web-based application that learns an\naudio equalization curve associated with a user-provided\naudio descriptor. To deploy the software to a large audi-\nence for our data collection, we have implemented it as a\nweb application using Adobe Flex and Drexel University’s\nAudio Processing Library for Flash (ALF) [17].\nAfter agreeing to participate, we ask participants to “en-\nter a descriptive term in the language in which you are\nmost comfortable describing sound (e.g. ‘warm’ for En-\nglish, ‘claro’ in Spanish, or ‘grave’ in Italian), pick a sound\nﬁle which we will modify to achieve the descriptive term,\nthen click on ‘Begin’.”\nParticipants are then given the choice of three different\nsource audio ﬁles to manipulate: “Electric Guitar”, “Pi-\nano”, and “Drums”. All ﬁles were sampled at 44.1 kHz\nand 16 bits. The ﬁles all had constant sound (i.e. no breaks\nFigure 1 . SocialEQ ratings screen. The user rates how\nwell each audio example corresponds to the descriptive\nterm they selected at the prior stage. Users rate an example\nby dragging the associated circle to the desired horizontal\nposition. Vertical position does not matter.\nFigure 2 . The equalization curve and control slider learned\nfor “bright” in a single session.\nin the audio) and were presented in loops that were 8, 10,\nand 14 seconds long, respectively.\nOnce a participant selects a descriptive term and a sound\nﬁle, they are asked to “rate how ‘your word’ that sound is”\nusing the interface shown in 1. The participant then rates\n40 examples of the chosen audio ﬁle, each modiﬁed with\nan equalization curve. Fifteen curves are repeats of prior\nexamples, to test for rating consistency. Figure 1 shows the\ninterface for rating examples.\nFrom these rated examples, the system learns the rela-\ntive boost/cut to apply each of 40 frequency bands. These\nbands have equivalent rectangular bandwidth (ERB) [3]\nderived center frequencies. We use the method from [15]\nto learn the EQ curve from user responses. This method\ntreats each frequency band as independent and correlates\nchanges in user ratings with changes in gain for that band.Positive correlation indicates a boost, and negative corre-\nlation a cut. The result is a 40-band EQ curve for the\ndescriptor learned from the participant. The system uses\nthe learned equalization curve to build a slider that lets the\nparticipant manipulate the sound in terms of the descriptor\n(Figure 2).\nAfter teaching SocialEQ an audio descriptor and trying\nthe control slider, participants were asked to complete a\nquestion survey that assessed their background, listening\nenvironment, and experience using SocialEQ.\n4. DATA COLLECTION\nFor a data collection of this size, an on-site data collection\nwas not feasible. We instead recruited participants through\nAmazon’s Mechanical Turk. We had 633 participants who\nparticipated in a total of 1102 training sessions (one ses-\nsion per learned word). We paid participants $1.00 (USD)\nper session, with the possibility of up to a $0.50 bonus,\ndetermined by the consistency of their equalized audio ex-\nample ratings. While the quality of loudspeakers could not\nbe controlled, over 92% of the participants reported listen-\ning over either headphones or large speakers (rather than\nsmall/laptop speakers).\n4.1 Inclusion Criteria for Sessions\nBefore analyzing the results, we ﬁrst removed sessions by\nparticipants who seemed to not put effort into the task. The\nmean time to teach the system the deﬁnition of a single\ndescriptor was 292 seconds (SD=237). We removed all\nsessions where the participant completed the task in less\nthan 60 seconds. We also removed all sessions where the\nparticipant gave the default rating for more than 5 out of\nthe 40 examples. We also removed any session where the\nparticipant responded “no” to the survey question: “Was\nthe listening environment quiet?”.\nRecall that 15 of the 40 examples were repeats in any\nsession. This let us test for consistency of user responses\nto audio examples when teaching SocialEQ. We measured\nconsistency using Pearson correlation between the ratings\nof the test and repeated examples. The median consistency\nacross sessions was 0.41 (95% CI [0.39, 0.44]).\nOnly sessions with consistency above 0 were retained.\nThis left 481 participants who taught the machine in 731\nsessions (Individual participants were allowed to teach more\nthan one descriptor to the system. The maximum number\nof descriptors a participant taught was 9.).\n5. RESULTS\nSince we have data on hundreds of words taught by hun-\ndreds of participants, we are not able to describe it all in de-\ntail here. We have made the data available for use by the re-\nsearch community at http://socialeq.org/data .\n5.1 Deﬁnitions\ndescriptor: An adjective (e.g. warm ) taught to the system\nin at least one session.session: A participant teaches SocialEQ a descriptor, tries\nthe learned controller, and completes the survey.\nrelative spectral curve (RSC): A set of relative gains (i.e.\nboosts or cuts) on the 40 ERB frequency bands. To\nmake RSC comparable, every RSC is normalized by\nputting gain values in standard deviations from the\nmean value of the 40 bands.\nuser-concept: The RSC learned in a single session (e.g.\nthe session where Bob teaches ’warm’ to SocialEQ).\ndescriptor deﬁnition The set of user-concepts that all\nshare a descriptor. A deﬁnition may be vague or\nprecise depending on how much agreement there is\nbetween user-concepts that share the descriptor. Fig-\nure 3 shows deep andsharp .\n5.2 The descriptors\nIn the 731 sessions, there were 324 unique descriptors. The\ndescriptors taught most frequently are listed in Table 1. Of\nthe 324 words, 91 occurred two or more times. The most\npopular descriptor was warm , but note that there was a bias\nfor participants to teach the system warm due to its use as\nthe example in the instructions (see Section 3).\nTable 1 . The 10 most common descriptors contributed\nRank Descriptor Sessions\n1 warm 57\n2 cold 25\n3 soft 24\n4 loud 22\n5 happy 19\n6 bright 16\n7 harsh 15\n8 soothing 14\n9 heavy 11\n10 cool 11\n5.3 Representing equalization concepts\nIn this paper, we make the assumption that participants\njudged each equalization example relative to the unpro-\ncessed source rather than judging the absolute spectrum of\neach equalization example. We therefore have chosen to\nrepresent equalization concepts in terms of relative changes\nin each frequency band rather than the resulting spectrum\nafter equalization. This allows us to compare equaliza-\ntion concepts from varying source material played on vary-\ning loudspeakers. In Figure 3, we show the distribution\nof RSCs collected for two example descriptors: deep and\nsharp. Each column shows the distribution of learned val-\nues for the corresponding ERB-spaced frequency band.\nNote that except for one outlier participant, there was\nfairly high agreement for the meaning of sharp . This is\ninteresting, since most musicians are taught that sharp re-\nlates to relative pitch, rather than the spectral character-\nistics of a sound. Our data indicate sharp also has other\nconnotations that relate to timbre.Relative Spectral Curve for deep, N=6 Relative Spectral Curve for sharp, N=8Figure 3 . Per frequency band boxplots of RSCs for the descriptors deep andsharp , learned in 6 and 8 sessions respectively.\nIn each ERB-spaced frequency band, the center line is the median, the box represents 50% of the data, the whiskers\nrepresent the remaining 50%. The pluses represent outliers.\n5.4 Actionable equalization descriptors\nTable 2 . Top 10 descriptors taught to the system by at least\n4 people, ranked by mean slider rating , which ranges from\n-3 (Strongly Disagree) to 3 (Strongly Agree)\nRank Word Mean Response\n1 relaxing 2.75\n2 quiet 2.60\n3 hot 2.50\n4 hard 2.50\n5 heavy 2.36\n6 smooth 2.33\n7 deep 2.33\n8 bright 2.31\n9 soothing 2.31\n10 mellow 2.29\nAs stated in Question 1 in Section 1, one one goal of this\npaper is to determine what audio descriptors describe goals\nachievable by an audio equalizer (i.e. the audio descriptor\nis an equalization descriptor). One way to answer this is to\nsimply look at the mean slider rating : the mean response\nto the survey statement, “The ﬁnal (control) slider captured\nmy target audio concept.” Participants were asked to re-\nspond on a 7-level Likert scale coded from -3 (Strongly\nDisagree) to 3 (Strongly Agree). Table 2 shows the 10 de-\nscriptors with the highest mean response to this question\nthat were contributed by at least 4 participants.\nThe learning approach used by SocialEQ [15] has an\ninherent bias toward learn smooth equalization curves and\nhas difﬁculty learning curves with narrow boosts or cuts or\nfrequency relationships that are non-linear or dependent.\nTherefore, mean slider rating is a sufﬁcient but not neces-\nsary condition to determine whether the descriptor is ac-\ntionable by an equalizer.5.5 Agreed upon equalization descriptors\nTable 3 . Top 10 descriptors ranked by the agreement score\ndescribed in Section 5.5\nRank Word Agreement Score\n1 tinny 0.294\n2 pleasing 0.222\n3 low 0.219\n4 dry 0.210\n5 metallic 0.195\n6 quiet 0.188\n7 deep 0.164\n8 hollow 0.160\n9 light 0.131\n10 warm 0.130\nThe second question we would like to answer is “How\nwidely-agreed-upon is the meaning of an EQ descriptor?”.\nFor some words, the meaning of a descriptor, as embodied\nin the RSC learned in a particular session, may vary sig-\nniﬁcantly from person to person. We want to ﬁnd which\ndescriptors vary the least from person to person, or rather\nwhich descriptors have the most widely-agreed-upon mean-\nings. To answer the question we looked at the total vari-\nance (i.e. the trace of the covariance matrix) of RSCs within\na descriptor deﬁnition. This can be written simply as the\nsum of the variance in each RSC frequency-band for the\nuser-concepts in a descriptor deﬁnition:\ntrace(\u0006)descriptor =1\nN39X\nk=0N\u00001X\nn=0(xn;k\u0000\u0016k)2(1)\nwhereNis the number of user-concepts in the descriptor\ndeﬁnition,kis the index of the frequency band, xis the\nRSC for user-concept n, and\u0016kis the mean of frequencyFigure 4 . 2-dimensional multi-dimensional scaling\n(MDS) plot of the descriptors. Their font size positively\ncorrelates to their agreement score from Equation 2.\nDistance\nFigure 5 . Hierarchical clustering of the 15 descriptors with\nthe highest agreement scores.\nbandkover theNuser-concepts in the descriptor deﬁni-\ntion.\nIf we then divide the natural logarithm of the number of\nuser-concepts ( log(N)) by this value, i.e.:\nagreementscore =log(N)\ntrace(\u0006)descriptor(2)\nwe have an agreement score that takes into account both\ntotal variance and the popularity of the descriptor. We used\nlog(N)to linearize the number of user-concepts since the\nfrequency with which a descriptor was taught the system\nwas distributed similarly to Zipf’s law [10]. When we rank\nthe descriptors by this score, we discover which descriptors\nhave more agreement amongst the participants. The top ten\ndescriptors ranked by this score are shown in Table 3.\n5.6 Audio descriptor synonyms\nTo answer the last question, “What EQ descriptors are true\naudio synonyms”, we compared the descriptor deﬁnitionsTable 4 . Synonyms of high agreement descriptors (see Ta-\nble 3) found through comparing descriptor deﬁnitions\ndescriptor synonyms\nlight tinny, crisp\ntinny hollow, crisp, light, shrill, bright, cold,\nraspy\ndeep throbbing, dark\nhollow tinny, dry, shrill, pleasing\nusing a distance function.\nTo compare learned descriptor deﬁnitions, we wanted\na distance measure that would: 1) allow for varying num-\nber of user-concepts per descriptor deﬁnition; 2) allow for\nmulti-modal distributions; and 3) take into account the un-\ncertainty of the descriptor deﬁnition. Therefore, we mod-\nelled each descriptor deﬁnition as a probability distribution\nover the user-concepts for the descriptor, and we then com-\npared descriptor deﬁnitions using an approximation of the\nsymmetric KL-divergence.\nThe steps to calculate the distance between two descrip-\ntor deﬁnitions are:\n1. Model each user-concept as a Gaussian distribution,\nN(\u0016i;\u0006i), where\u0016iis the RSC of the user-concept\nand\u0006is a diagonal covariance matrix in which the\nvariance for each frequency-band is set by \u001b2\ni;k=\n(\u001bk\u0000\u001bkri)2where\u001bkis the sample standard devia-\ntion of frequency-band kfor RSCs of alldescriptors,\nandriis the ratings consistency for the session that\nlearned user-concept i. Here we are using the ratings\nconsistency as a measure of the uncertainty of the\nuser-concept, mapping a consistency range of [0;1]\nto a per-frequency-band variance range of [\u001bk;0].\n2. Then model each descriptor deﬁnition as follows:\nP(x) =1\nNN\u00001X\ni=0N(\u0016i;\u0006i) (3)\nwhereN(\u0016i;\u0006i) is the distribution for the ithuser-\nconcept.\n3. We then use a symmetric Monte Carlo approximate\nKL-divergence [7] to compare two descriptor deﬁni-\ntion models.\nUsing this distance measure, we computed the distance\nbetween every pair of descriptor deﬁnitions. With these\ndistances, we can map and visualize the relationships of\ndescriptor deﬁnitions. In Figure 4, we placed the descrip-\ntor deﬁnitions in a two-dimensional space by using metric\nmulti-dimensional scaling [1], each descriptor deﬁnition is\nscaled by its agreement score so that well-deﬁned descrip-\ntors are larger. To get a better sense of the relationships\nof the 15 high agreement descriptor deﬁnitions listed in\nTable 3, we performed agglomerative hierarchical cluster-\ning using the “group average” algorithm [5] and plotted the\ndendrogram in Figure 5. From this you can see two clustersformed with descriptors one might typically associates as\nopposites: bright/dark ,quiet/loud ,light/heavy . From this\nwe can also see relationships of descriptors which may not\nhave been obvious such as pleasing being closely associ-\nated with dry.\nIn Table 4, we list a few high agreement descriptors\nalong with their synonyms through comparing descriptor\ndeﬁnitions. We considered two words synonyms if their\ndistance was within the ﬁrst percentile of all the pairwise\ndistances. Also, only descriptors deﬁnitions that consisted\nof at least two user-concepts and had at least a 1.0 mean\nslider rating (as described in Section 5.4) were included.\n6. CONCLUSION\nIn this work, we analyzed the equalization descriptors that\nwere taught to a web-based equalization learning system,\nSocialEQ. We developed methods to determine which de-\nscriptors map well to equalization, which have broadly-\nagreed upon meaning, and which are synonymous. During\nanalysis we found both expected and unexpected relation-\nships and deﬁnitions of descriptors, but more importantly\nwe developed the ground work of an intelligent audio pro-\nduction system that responds to the descriptive language\nof the user. With a large collection of equalization descrip-\ntors and the techniques described in this paper, we have a\nmap of equalization descriptors which a system could use\nto determine whether a goal is achievable with an equalizer\nand whether it needs to learn an individual’s equalization\nconcept or can use an agreed upon concept.\nThis work was supported by grant by National Science\nFoundation Grant No. IIS-1116384.\n7. REFERENCES\n[1] I. Borg and P. Groenen. Modern multidimensional scal-\ning: Theory and applications . Springer Verlag, 2005.\n[2] A. Disley and D. Howard. Spectral correlates of timbral\nsemantics relating to the pipe organ. Speech, Music and\nHearing , 46:25–39, 2004.\n[3] B. Glasberg and B. Moore. Derivation of auditory ﬁl-\nter shapes from notched-noise data. Hearing Research ,\n47(12):103–138, 1990.\n[4] J. Grey. Multidimensional perceptual scaling of musi-\ncal timbres. The Journal of the ASA , 61(5):1270–1277,\n1977.\n[5] T. Hastie, R. Tibshirani, and J. Friedman. The elements\nof statistical learning . Springer New York, 2001.\n[6] H. Helmholtz and A. Ellis. On the sensations of tone\nas a physiological basis for the theory of music . Dover,\nNew York, 2d english edition, 1954.\n[7] J. Hershey and P. Olsen. Approximating the kullback\nleibler divergence between gaussian mixture models.\nInProc. ICASSP , 2007 .[8] D. Huber and R. Runstein. Modern recording tech-\nniques . Focal Press/Elsevier, Amsterdam ; Boston, 7th\nedition, 2010.\n[9] E. Lukasik. Towards timbre-driven semantic retrieval\nof violins. In Proc. of International Conference on In-\ntelligent Systems Design and Applications, 2005 .\n[10] C. Manning, P. Raghavan, and H. Schtze. Introduction\nto information retrieval . Cambridge University Press\nCambridge, 2008.\n[11] S. McAdams, S. Winsberg, S. Donnadieu, G. Soete,\nand J. Krimphoff. Perceptual scaling of synthesized\nmusical timbres: Common dimensions, speciﬁcities,\nand latent subject classes. Psychological Research ,\n58(3):177–192, 1995.\n[12] S. Mecklenburg and J. Loviscach. subjeqt: controlling\nan equalizer through subjective terms. In Proc. of CHI\n’06 Extended Abstracts on Human Factors in Comput-\ning Systems, 2006 .\n[13] G. Miller. Wordnet: a lexical database for english.\nCommun. ACM , 38(11):39–41, 1995.\n[14] D. Reed. Capturing perceptual expertise: a sound\nequalization expert system. Knowledge-Based Sys-\ntems, 14(12):111–118, 2001.\n[15] A.T. Sabin, Z. Raﬁi, and B. Pardo. Weighting-function-\nbased rapid mapping of descriptors to audio processing\nparameters. Journal of the AES , 59(6):419–430, 2011.\n[16] M. Sarkar, B. Vercoe, and Y . Yang. Words that describe\ntimbre: a study of auditory perception through lan-\nguage. In Proc. of Language and Music as Cognitive\nSystems Conference, 2007 .\n[17] J. Scott, R. Migneco, B. Morton, C. Hahn, P. Diefen-\nbach, and Y . Kim. An audio processing library for\nmir application development in ﬂash. In Proc. ISMIR,\n2010 .\n[18] R. Shepard. Geometrical approximations to the\nstructure of musical pitch. Psychological Review ,\n89(4):305–333, 1982.\n[19] D. Smalley. Spectromorphology: explaining sound-\nshapes. Organised Sound , 2(02):107–126, 1997.\n[20] L. Solomon. Search for physical correlates to psycho-\nlogical dimensions of sounds. The Journal of the ASA ,\n31(4):492–497, 1959.\n[21] E. Toulson. A need for universal deﬁnitions of audio\nterminologies and improved knowledge transfer to the\naudio consumer. In Proc. of The Art of Record Produc-\ntion Conference, 2003 .\n[22] A. Zacharakis, K. Pastiadis, G. Papadelis, and J. Reiss.\nAn investigation of musical timbre: Uncovering salient\nsemantic descriptions and perceptual dimensions. In\nProc. of the ISMIR, 2011 ."
    },
    {
        "title": "A Deterministic Annealing EM Algorithm for Automatic Music Transcription.",
        "author": [
            "Tian Cheng 0001",
            "Simon Dixon",
            "Matthias Mauch"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417014",
        "url": "https://doi.org/10.5281/zenodo.1417014",
        "ee": "https://zenodo.org/records/1417014/files/ChengDM13.pdf",
        "abstract": "In the past decade, non-negative matrix factorisation (NMF) and probabilistic latent component analysis (PLCA) have been used widely in automatic music transcription. Despite their successes, these methods only guarantee that the decomposition converges to a local minimum in the cost function. In order to find better local minima, we propose to extend an existing PLCA-based transcription method with the deterministic annealing EM (DAEM) algorithm. The PLCA update rules are modified by introducing a “temperature” parameter. At higher temperatures, general areas of the search space containing good solutions are found. As the temperature is gradually decreased, distinctions in the data are sharpened, resulting in a more finegrained optimisation at each successive temperature. This process reduces the dependence on the initialisation, which is otherwise a limitation of NMF and PLCA approaches. The method was tested on two standard multi-instrument transcription data sets (MIREX and Bach10). Experimental results show that the proposed method significantly outperforms a state-of-the-art reference method, according to both frame-based and note-based metrics. An additional analysis of instrument assignment results shows that instrument spectra are typically modelled as mixtures of templates from several instruments.",
        "zenodo_id": 1417014,
        "dblp_key": "conf/ismir/ChengDM13",
        "keywords": [
            "non-negative matrix factorisation",
            "probabilistic latent component analysis",
            "automatic music transcription",
            "deterministic annealing EM",
            "temperature parameter",
            "PLCA update rules",
            "search space",
            "good solutions",
            "finegrained optimisation",
            "instrument assignment"
        ],
        "content": "A DETERMINISTIC ANNEALING EM ALGORITHM FOR \nAUTOMATIC MUSIC TRANSCRIPTION \nTian Cheng, Simon Dixon and Matthias Mauch \nCentre for Digital Music, Queen Mary University of London \n{tian.cheng, simon.dixon, matthias.mauch }@eecs.qmul.ac.uk \nABSTRACT \nIn the past decade, non-negative matrix factorisation (NMF) \nand probabilistic latent component analysis (PLCA) have \nbeen used widely in automatic music transcription. De- \nspite their successes, these methods only guarantee that\nthe decomposition converges to a local minimum in the \ncost function. In order to ﬁnd better local minima, we \npropose to extend an existing PLCA-based transcription \nmethod with the deterministic annealing EM (DAEM) al- \ngorithm. The PLCA update rules are modiﬁed by intro- \nducing a “temperature” parameter. At higher temperatures, \ngeneral areas of the search space containing good solutions \nare found. As the temperature is gradually decreased, dis- \ntinctions in the data are sharpened, resulting in a more ﬁne- \ngrained optimisation at each successive temperature. This \nprocess reduces the dependence on the initialisation, which \nis otherwise a limitation of NMF and PLCA approaches. \nThe method was tested on two standard multi-instrument \ntranscription data sets (MIREX and Bach10). Experimen-\ntal results show that the proposed method signiﬁcantly out- \nperforms a state-of-the-art reference method, according to \nboth frame-based and note-based metrics. An additional \nanalysis of instrument assignment results shows that in- \nstrument spectra are typically modelled as mixtures of tem- \nplates from several instruments. \n1. INTRODUCTION \nAutomatic music transcription is the process of transcrib- \ning audio into a symbolic music representation. To date, \nnon-negative matrix factorisation (NMF) [15] and its prob- \nabilistic counterpart, probabilistic latent component analy-\nsis (PLCA) [17], have been used extensively for this task. \nThese methods treat the spectrogram as a matrix, and de- \ncompose it into spectral bases, gain functions, and in- \nstrument distributions (when considering different instru-\nments). Although not yet providing the best transcription\nresults, they provide a powerful mathematical model which \ncan lead to a meaningful decomposition, using the con- \nstraints of non-negativity and sparsity. Another advantage \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for proﬁt or commercial advantage and that copies \nbear this notice and the full citation on the ﬁrst page. \nc/circlecopyrt2013 International Society for Music Information Retrieval. of these methods is that they are easy to extend, by formu- \nlating a more complex model, adding variables or combin- \ning them with other models.\nOne obvious problem of non-negative matrix decompo- \nsition methods (such as NMF and PLCA) is that they are \ninitialisation-sensitive and tend to converge to a local mini- \nmum. Training instrument templates is an effective way to \ninitialise the spectral bases. By ﬁxing the templates during \nthe updating, we obtain a stable output for the gain func- \ntion, independent of its initialisation. But when the model \nbecomes more complicated, as by introducing an instru- \nment variable into the model, which is used widely nowa- \ndays, it is not possible for us to ﬁnd good initialisations for \nall variables. \nIn this paper, we tackle the local minimum problem \nby introducing an optimisation method. When using non- \nnegative matrix decomposition methods, the transcription \nresult is related to the cost function, the update rules and \nalso the constraints. Here, we particularly focus on PLCA, \nwhich utilises the Kullback-Leibler (KL) divergence as the \ncost function and derives the update rules based on the EM \nalgorithm [16]. To address the local minimum problem of \nthe EM algorithm, we make use of the deterministic an- \nnealing EM algorithm [18] by introducing a temperature \nparameter into an existing PLCA-based model [2]. The \nproposed method is tested on the Bach10 dataset [5] and \nthe MIREX multi-F0 development dataset [1]. In com- \nparison to the original PLCA-based model, the proposed \nmethod improves the results of multi-F0 estimation and \nnote tracking, while the instrument assignment results vary \nfor each individual instrument.\nAlthough not much attention has been paid to the local \nminimum problem of automatic music transcription meth- \nods, there is still some related work. Bertin et al. [3] used \na tempering scheme to favour the convergence of Itakura- \nSaito (IS) divergence to global minima. Experiments on \nmusic transcription show that IS-NMF can provide a good \nresult by choosing a suitable temperature parameter. Hof- \nmann [9] proposed a model based on the tempered EM al- \ngorithm to avoid overﬁtting in probabilistic latent semantic \nanalysis. Kameoka et al. [11] introduced the DAEM al- \ngorithm into the harmonic-temporal-structured clustering\n(HTC) model for audio feature extraction. The HTC model \nis represented by a Gaussian kernel, and the DAEM algo- \nrithm is used to optimise the parameter convergence. Itaya \net al. [10] used the DAEM algorithm to estimate the pa- \nrameters of Gaussian mixture models (GMMs) and hidden Markov models (HMMs). Experiments on speaker recog- \nnition and speech recognition show that DAEM is an ef- \nfective method for GMM- and HMM-based acoustic mod- \neling. Finally, Smaragdis et al. [16] stated that it is more \nlikely to get “meaningful” decompositions and quick con- \nvergence by using “annealing” in PLCA. \nThe rest of this paper is organised as follows. In Section \n2, we describe the PLCA model and the local minimum \nproblem of this model. In Section 3, the update rules of a \nPLCA-based model are modiﬁed according to the DAEM \nalgorithm. The results for three transcription subtasks are\npresented in Section 4. Finally discussion and conclusions \nare indicated in Section 5 and 6, respectively. \n2. PLCA AND SHIFT-INV ARIANT PLCA \nTwo basic PLCA models, PLCA and Shift-invariant \nPLCA, are presented in [17]. For automatic music tran- \nscription, the spectrogram is formulated by PLCA as: \nV(ω,t )≈P(ω,t ) = P(t)/summationdisplay\npP(ω|p)P(p|t)(1)\nwhereV(ω,t )is the input spectrogram, P(ω,t )the ap-\nproximated spectrogram, ωis the frequency bin, tthe\nframe number. P(t)is the energy of each time frame, \nP(ω|p)is the spectral bases corresponding to pitch p, and \nP(p|t)the gain function. \nTo build a shift-invariant PLCA model, the spectrogram \nneeds to be presented on a logarithmic frequency scale, \nsuch as the constant-Q transform. Assuming that the en- \nergy distributions of adjacent pitches are similar for any \ngiven instrument, the spectral basis can be shifted in fre- \nquency very easily, as the pattern of partial spacings is the \nsame for all pitches, due to the logarithmic frequency axis. \nThe spectrogram is formulated as: \nV(ω,t )≈P(ω,t ) = /summationdisplay\nzP(z)P(ω|z)∗ωP(f,t |z)\n=/summationdisplay\nzP(z)/summationdisplay\nfP(ω−f|z)P(f,t |z)\n(2)\nwhereP(ω|z)andP(f,t |z)are the spectral templates\nand time-dependent shifted variant fof component z, and \nP(z)is the prior distribution of the components. \nIn many recent systems the PLCA model is extended \nby introducing an instrument distribution, with templates \ntrained per pitch per instrument. The spectrogram is for- \nmulated as:\nV(ω,t )≈P(ω,t ) = P(t)/summationdisplay\np,s P(ω|s,p )P(s|p,t )P(p|t)\n(3)\nwhereP(ω|s,p )represents the spectral templates corre-\nsponding to each instrument sand pitch p,P(s|p,t )the\ninstrument contribution to each pitch in the tth frame, and\nP(p|t)the pitch probability distribution for each frame.\nThe parameters of the PLCA models are estimated by \niteratively decreasing the KL divergence of the input spec- \ntrogramV(ω,t )and the synthetic spectrogram P(ω,t )us- \ning the EM algorithm. The KL divergence is convex in one variable, but not convex in multiple variables [12]. In \nthis case, the EM algorithm can only guarantee to ﬁnd a \nlocal minimum for these parameters, so the results depend \non the initialisation. The use of instrument templates is \nan effective way to deal with the initialisation sensitivity \nof the algorithm. Taking the model described in Eqn. (1) \nfor example, if the templates are ﬁxed as a constant, the \ngain function will be convex. This means that when we \nformulate the model as the product of the spectral bases \nand a gain function, we obtain a unique gain function cor- \nresponding to a ﬁxed set of templates. On the one hand, \nthe templates lead to a stable decomposition for automatic \nmusic transcription; on the other hand, the templates also \nlimit the performance of the transcription. However, when \nencountering the extended model as described in Eqn. (3), \nthe instrument contribution and the pitch contribution still\nface the risk of converging to local minima, even with ﬁxed \ntemplates. \n3. PROPOSED METHOD \nTo deal with the local minimum problem of PLCA models, \nwe derive the update rules according to the deterministic \nannealing EM algorithm [18], which introduces a temper- \nature parameter into the EM algorithm. The temperature \nparameter is employed on the posterior probability density \nin the E-step. Then by gradually reducing the temperature, \nthe EM steps are iteratively executed until convergence at \neach temperature, leading the result to a global or better lo- \ncal minimum. We apply this method to a baseline PLCA- \nbased model proposed in [2]. Since the templates are kept \nﬁxed, the temperature parameter is applied to the posterior \nprobability density of the instrument distribution. In this \nway, we can enjoy the beneﬁts of the DAEM algorithm \nand the templates.\n3.1 The Baseline PLCA Model \nBenetos and Dixon [2] proposed a model that adds an in- \nstrument distribution variable to shift-invariant PLCA. The \ntime-frequency representation of the input signal was com- \nputed with the Constant-Q Transform [14] using 120 bins \nper octave. Templates were trained for 10 instruments al- \nlowing shifts within a semitone range, in order to deal with \narbitrary tuning and frequency modulation. The model is \nformulated as:\nP(ω,t ) = P(t)/summationdisplay\np,s P(ω|s,p )∗ωP(f|p,t )P(s|p,t )P(p|t)\n(4)\nwhereP(ω,t )is the approximated spectrogram, P(t)is \nthe energy distribution of spectrogram. P(ω|s,p )are the\ntemplates of instrument sand pitch p,P(f|p,t )is the shift- \ned variant for each p,P(s|p,t )is the instrument contribu- \ntion for each pitch, and P(p|t)is the pitch probability dis- \ntribution for each time frame. The templates P(ω|s,p )are \ntrained using the MAPS dataset [6] and RWC dataset [7]. \nThe update rules are derived from the EM algorithm. instrument lowest note highest note \n1 Bassoon 34 72 \n2 Cello 26 81 \n3 Clarinet 50 89 \n4 Flute 60 96 \n5 Guitar 40 76 \n6 Horn 41 77 \n7 Oboe 58 91 \n8 Piano 21 108 \n9 Tenor Sax 44 75 \n10 Violin 55 100 \nTable 1 : Instrument ranges, adapted from [1] \nFor the E-step, the posterior probability density is: \nP(p,f,s |ω,t ) = \nP(ω−f|s,p )P(f|p,t )P(s|p,t )P(p|t)/summationtext\np,f,s P(ω−f|s,p )P(f|p,t )P(s|p,t )P(p|t)(5)\nFor the M-step, each parameter is estimated. \nP(f|p,t ) = /summationtext\nω,s P(p,f,s |ω,t )P(ω,t )/summationtext\nf,ω,s P(p,f,s |ω,t )P(ω,t )(6)\nP(s|p,t ) = (/summationtext\nω,f P(p,f,s |ω,t )P(ω,t )) α1\n/summationtext\ns(/summationtext\nω,f P(p,f,s |ω,t )P(ω,t )) α1(7)\nP(p|t) = (/summationtext\nω,f,s P(p,f,s |ω,t )P(ω,t )) α2\n/summationtext\np(/summationtext\nω,f,s P(p,f,s |ω,t )P(ω,t )) α2(8)\nThe templates P(ω|s,p )are not updated as they are pre- \nviously trained and kept ﬁxed. The parameters α1andα2\nused in Eqn. (7) and (8) are used to enforce sparsity, where \nα1,α 2>1. We set α1= 1 .3andα2= 1 .1. The ﬁnal \npiano-roll matrix P(p,t )and the pitches assigned to each \ninstrument P(p,t,s )are given by: \nP(p,t ) = P(p|t)P(t) (9)\nP(p,t,s ) = P(s|p,t )P(p|t)P(t) (10)\nFor post-processing, instead of using an HMM, the note \nevents are extracted by performing thresholding on P(p,t )\nand using minimum-length pruning (deleting notes shorter\nthan50 ms ). The instrument-wise note events are detected \nin the same way using P(p,t,s ).\n3.2 The DAEM-based Model \nTo modify the update rules according to the DAEM algo- \nrithm, in the E-step, the posterior probability density in \nEqn. (5) is modiﬁed by introducing a temperature parame- \nterτ1:\nPτ(p,f,s |ω,t ) = \n(P(ω−f|s,p )P(f|p,t )P(s|p,t )P(p|t)) 1/τ \n/summationtext\np,f,s (P(ω−f|s,p )P(f|p,t )P(s|p,t )P(p|t)) 1/τ \n(11)\nAnd the update rules are extended by adding a τ-loop:\n1The parameter used in [18] is β, and the temperature is indicated \nby 1/β . The reason of using τhere is because we want to indicate the \ntemperature directly by τand distinguish the proposed method from the\nβ-divergence. •Setτ←τmax(τmax>1) .\n•Iterate the following EM-steps until convergence: \nE-step: calculate Pτ(p,f,s |ω,t ).\nM-step: estimated P(f|p,t ),P(s|p,t )andP(p|t)\nby replacing P(p,f,s |ω,t )withPτ(p,f,s |ω,t ).\n•Decrease τ.\n•If τ≥1, repeat from step 2; otherwise stop. \nBy gradually decreasing τ, the temperature is cooling \ndown. At higher temperatures, the distributions are \nsmoothed and general areas of the search space containing \ngood solutions are found. As the temperature is gradually \ndecreased, distinctions in the data are sharpened, resulting \nin a more ﬁne-grained optimisation at each successive tem- \nperature.\nConsidering the properties of this particular model, we \nsimplify the posterior probability density to:\nPτ(p,f,s |ω,t ) = \nP(ω−f|s,p )P(f|p,t )P(s|p,t )1/τ P(p|t)/summationtext\np,f,s P(ω−f|s,p )P(f|p,t )P(s|p,t )1/τ P(p|t)\n(12)\nThe convolution of the templates and the pitch impulse dis- \ntribution, giving the terms P(ω−f|s,p )P(f|p,t ), works \nas the shift-invariant templates here. These are not mod- \niﬁed by the temperature parameter, as the templates are \nﬁxed during the iterative process 2. In addition, having \nobserved that the pitch distribution P(p|t)is dependent on \nthe instrument distribution P(s|p,t )in this model, we only \nneed to modify P(s|p,t )in the posterior probability den- \nsity.\nIn the experiment, the parameter τtook the values \n10 /i, i ∈ { 8,9,10 }. When τﬁnally decreases to 1, the \nupdate rules agree with the original ones. \n4. EV ALUATION \n4.1 Datasets\nWe used the Bach10 Dataset [5] and the MIREX Multi- \nF0 Development Dataset (MIREX dataset) [1] to test the \nperformance of the proposed method. The Bach10 dataset \nconsists of 10 quartet recordings performed on violin, clar- \ninet, saxophone and bassoon. The MIREX dataset is an \nexcerpt from a woodwind quintet recording, played on bas- \nsoon, clarinet, ﬂute, horn, oboe.\n4.2 Evaluation Metrics \nThe performance of the proposed system is evaluated on \nthree subtasks of automatic music transcription. The ﬁrst \ntwo, multiple F0 estimation and note tracking, are very \ncommonly used. The third subtask, instrument assign- \nment, evaluates the algorithms’ ability to assign the notes \nto corresponding instruments. \n2This was also conﬁrmed by test experiments where the power 1/τ \nwas also applied to the pitch impulse distribution P(f|p,t ), giving simi- \nlar transcription results.Dataset Methods P R F Acc EtotEsubsEmissEfa \nBach10 BD(2012) 0.784 0.791 0.787 0.650 0.311 0.116 0.093 0.102\nProposed 0.819 0.796 0.807 0.677 0.282 0.098 0.106 0.078\nMIREX BD(2012) 0.748 0.537 0.625 0.455 0.486 0.158 0.305 0.023\nProposed 0.769 0.561 0.649 0.480 0.461 0.146 0.292 0.023\nBoth BD(2012) 0.781 0.768 0.772 0.632 0.327 0.120 0.112 0.094\nProposed 0.814 0.775 0.793 0.659 0.299 0.102 0.123 0.074\nTable 2 : Multiple F0 estimation results (see text for explanation of symbols). \nIn the multiple F0 estimation subtask, performance is \nevaluated frame by frame with an interval of 10 ms . The \naccuracy metrics are precision ( P), recall ( R), F-measure \n(F) [19] and the overall accuracy ( Acc) [4], deﬁned as fol- \nlows: \nP=Ntp \nNsys, R =Ntp \nNref , F =2·R·P\nR+P(13)\nAcc=Ntp \nNtp +Nfp +Nfn (14)\nwhereNtp is the number of true positives, NsysandNref \ndenote the number of the detected pitches and the ground- \ntruth pitches, Nfp andNfn are the number of false positives \nand false negatives respectively. The error metrics are the \nrates of total error ( Etot), substitution error ( Esubs), missed \ndetections ( Emiss) and false alarms ( Efa ). See the deﬁni- \ntions in [13]. \nFor the note tracking task, a note is considered cor- \nrectly detected if the note is within the following ranges \nof ground truth. \npitch range±3% \nonset range±50 ms \noffset range±max{20% of the duration, 50 ms }\nThe algorithms are evaluated in terms of onset-only and \nonset-offset accuracies, which are denoted by Pon ,Ron ,\nFon ,Acc on andPoff ,Roff ,Foff ,Acc off respectively. \nThe instrument assignment task assesses whether the\ntranscription not only identiﬁes the correct pitch, but also \nthe correct instrument. First, pitches are detected for each\nindividual instrument. Then instruments actually occurring\nin the piece are evaluated according to the frame-based F- \nmeasure (13), whereas for the other instruments we calcu- \nlate the false positive rate. \n4.3 Results\nWe compare the performance of the proposed method to \nthat of the baseline PLCA model introduced in Section 3.1 \n(mentioned as BD(2012) below). Here, we provide results \nfor three subtasks on two different datasets. \n4.3.1 Multiple F0 Estimation \nThe results for multiple F0 estimation using the Bach10 \nand MIREX datasets for two methods are shown in Table \n2. It can be seen that the proposed method outperforms the Dataset Methods Pon Ron Fon Acc on \nBach10 BD(2012) 0.319 0.339 0.328 0.197\nProposed 0.399 0.354 0.374 0.231\nMIREX BD(2012) 0.628 0.420 0.503 0.336\nProposed 0.690 0.459 0.551 0.380\nBoth BD(2012) 0.347 0.346 0.344 0.209\nProposed 0.427 0.364 0.391 0.245\n(a) onset-only accuracy\nDataset Methods Poff Roff Foff Acc off \nBach10 BD(2012) 0.217 0.230 0.223 0.126\nProposed 0.281 0.249 0.263 0.152\nMIREX BD(2012) 0.487 0.326 0.391 0.243\nProposed 0.537 0.357 0.429 0.273\nBoth BD(2012) 0.242 0.239 0.238 0.137\nProposed 0.305 0.259 0.279 0.163\n(b) onset and offset \nTable 3 : Note-tracking results \nBD(2012) method in terms of accuracy ( Acc) on both in- \ndividual datasets by at least 2.5 percentage points, leading \nto an increased overall accuracy of 0.659 (up 2.7 percent- \nage points). The total error decreases by 2.8 percentage \npoints. On the Bach10 dataset improvements are mainly \ndue to a reduced false alarm rate ( Efa ), which decreases \nfrom10 .2% to 7.8% . This is also reﬂected by increased \nprecision ( P) and stable recall ( R). The improvement for \nthe MIREX dataset mainly comes from reduction in both \nsubstitution error ( Esubs) and missed detection error ( Emiss)\nrates, leading to higher precision and recall. \nIn order to determine if the increase in accuracy ( Acc )\nis signiﬁcant we ran a Friedman test for this subtask. The \nresulting p-value of 0.0009<0.01 indicates that the dif-\nference is highly signiﬁcant. The distribution of Accof the \nten ﬁles in the Bach10 dataset is shown in Figure 1a. \n4.3.2 Note Tracking \nFor the note tracking subtask, we found that the F-measure \nwas improved by almost 5 percentage points for onset-only \nevaluation and around 4 percentage points for onset-offset \nevaluation for both datasets, as shown in Table 3. We ran a \nFriedman test with regard to the F-measures ( Fon andFoff )\nfor this subtask. For both onset-only and onset-offset met- \nrics, thep-values are less than 0.01 , showing that—here, BD(2012) Proposed 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7 0.71 \n(a)AccBD(2012) Proposed 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 0.46 \n(b)Fon BD(2012) Proposed 0.18 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 \n(c)Foff \nFigure 1 : Box-and-whisker plots of (a) accuracy; (b) \nonset-only F-measure; and (c) onset-offset F-measure; for\nthe Bach10 dataset.\ntoo—the differences are signiﬁcant. The distributions of \nFon andFoff for the Bach10 dataset are shown in Figures 1b \nand 1c.\nThe note tracking evaluation shows that both meth- \nods under consideration perform better on the MIREX \ndataset, whereas according to the frame-based evaluation \n(see Section 4.3.1) they perform better on the Bach10 \ndataset. This result is in line with the results from other \nmethods on the same data, 3and is likely to stem from the \nunusual co-occurrence of trills and legato notes that domi- \nnates the MIREX piece.\n4.3.3 Instrument Assignment\nThe results for instrument assignment for the two datasets \nare shown in Table 4. In this subtask, we cannot identify a \nsystematic advantage of either method, with the F-measure \nmeans over all instruments being very close (20.7% and \n20.9% on the Bach10 dataset, and 35.1% and 34.3% on \nthe MIREX dataset). Slight differences between the meth-\nods for particular instruments do not show a consistent ad- \nvantage of one method either; we will therefore focus on \nthe proposed method in the rest of the discussion. The \nmost obvious differences in F-measure occur between in- \nstruments. For example, the results for the Bach10 dataset \nshow that instrument assignment works better for the clar- \ninet and bassoon than for the violin and saxophone. Also,\nsince the note templates include instruments not present\nin the pieces, false positives occur for these instruments, \nwith the largest ratio of false positives occurring for horn \n(18.6%) and piano (16.4%). The problem instrument in \nthe MIREX dataset is the oboe, to which few notes are \nassigned, leading to a low F-measure of around 12-13%. \nNotes are detected in three instruments that do not feature \nin the music, with the largest ratio of false positives found \nin the piano (47.9%) and guitar (34.5%). No false positives \nwere detected for saxophone or violin. \nThe discrepancy between the satisfactory multiple F0 \n3as published on the MIREX website [1]. F-measure Violin Clarinet Saxophone Bassoon Mean \nBD(2012) 0.175 0.313 0.092 0.246 0.207\nProposed 0.190 0.275 0.127 0.243 0.209\n(a) Bach10\nF-measure Bassoon Clarinet Flute Horn Oboe Mean \nBD(2012) 0.292 0.444 0.485 0.409 0.125 0.351\nProposed 0.294 0.420 0.489 0.385 0.129 0.343\n(b) MIREX\nTable 4 : Instrument assignment results \nestimation results and the comparatively low results for in- \nstrument assignment is due to the fact that often the correct \npitch is detected, but assigned to a wrong instrument or \ncombination of instruments. That is, note templates from \ndifferent instruments are combined to approximate the ob- \nserved spectra. The proposed method provides a better re- \nconstruction of the observed data using combinations of \ntemplates at the correct pitches, resulting in better perfor- \nmance for frame level and note tracking tasks. \n5. DISCUSSION \nThe use of the temperature parameter τthat is central to \nthe DAEM algorithm in Eqn. (11) is similar to the use of \nthe sparsity parameters in Eqn. (7) and Eqn. (8). In fact, \nthe sparsity method used here is related to the Tempered \nEM algorithm [8]. Both the DAEM and sparsity equations \n‘put an exponent on a distribution’. When the exponent \nis larger than one, the distribution becomes sharper and \nsparser; when the exponent is smaller than one, the dis- \ntribution is smoothed, as in the case of high-temperature \nstages of DAEM. \nSo far we have used DAEM with only one conﬁguration \nof three temperature steps. In the future, we would like \nto explore different conﬁgurations to see whether we can \nfurther improve the results of multiple F0 estimation and \nnote tracking.\nWe have shown that DAEM can improve the per- \nformance of an EM-based model, but further investiga- \ntions are needed to show how well this result generalises. \nFor example, preliminary tests have shown that applying \nDAEM directly in the standard PLCA model in Eqn. (1) \nwithout templates, fails to provide better results. \nWe observe that the previously-trained templates are \nvery important and work as an excellent initialisation for \nthe spectral bases in the PLCA models. On the other hand, \nthey also inﬂuence the result of the gain functions, which \nmeans that the transcription result will be poor if we use \npoor or inappropriate templates. The risk of updating the \ntemplates during the iteration is that an updated template \nmight no longer accord with its labels (pitch, instrument). \nDue to the different ways a note can be played and differ- \nences in sound transmission, templates will never match \nobservations precisely. Spectral decomposition algorithms \ncompensate for this mismatch by ﬁnding mixtures of tem- \nplates which provide a better approximation of the data (see Section 4.3.3). In order to capture the variations of \ninstrument sounds in a single model, we intend to explore \nphysical modelling for time-varying templates in future \nwork. \n6. CONCLUSIONS \nIn this paper, we modiﬁed a baseline PLCA model for \nautomatic music transcription. The model’s update rules \nwere changed according to the DAEM algorithm to tackle \nthe local minimum problem. The DAEM algorithm intro- \nduces a temperature parameter to the update rules and leads \nthe decomposition to converge to a global or better local \nminimum by gradually lowering the temperature. The pro- \nposed method was tested using two standard transcription \ndatasets, the Bach10 dataset and the MIREX dataset. The\nresults show that the proposed method signiﬁcantly outper- \nforms the baseline method in multiple F0 estimation (accu- \nracy increases by 2.7 percentage points) and note tracking \n(F-measure increases by 4 percentage points). Although \nresults on an additional instrument assignment task show \nno signiﬁcant difference between the methods, they reveal \nthat both methods use mixtures of instrument templates to \napproximate observed spectra in the test data. We noted \nseveral aspects that call for further study: DAEM temper- \nature conﬁgurations, the extension of DAEM to more gen- \neral PLCA models, and the use of physical modelling to \ngenerate more ﬂexible instrument templates. \n7. ACKNOWLEDGEMENTS \nTian Cheng is supported by China Scholarship Council \n(CSC)/Queen Mary Joint PhD scholarships. We would like \nto thank Emmanouil Benetos and Roland Badeau for their \ncomments on this paper. \n8. REFERENCES \n[1] Music Information Retrieval Evaluation eXchange \n(MIREX). http://www.music-ir.org/mirex/\nwiki/MIREX_HOME .\n[2] E. Benetos and S. Dixon. A shift-invariant latent vari- \nable model for automatic music transcription. Com- \nputer Music Journal , 36(4):81–94, 2012. \n[3] N. Bertin, C. F ´evotte, and R. Badeau. A tempering \napproach for itakura-saito non-negative matrix fac- \ntorization. with application to music transcription. In \nIEEE International Conference on Acoustics, Speech \nand Signal Processing (ICASSP09) , pages 1545–1548, \nApr. 2009. \n[4] S. Dixon. On the computer recognition of solo piano \nmusic. In Proceedings of Australasian Computer Mu- \nsic Conference , pages 31–37, 2000. \n[5] Z. Duan, B. Pardo, and C. Zhang. Multiple fundamen- \ntal frequency estimation by modeling spectral peaks \nand non-peak regions. IEEE Transactions on Audio, \nSpeech, and Language Processing , 18(8):2121 – 2133, \n2010.[6] V . Emiya, R. Badeau, and B. David. Multipitch esti- \nmation of piano sounds using a new probabilistic spec- \ntral smoothness principle. IEEE Transactions on Au- \ndio, Speech, and Language Processing , 18(6):1643 – \n1654, 2010.\n[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. \nRWC Music Database: Music Genre Database and \nMusical Instrument Sound Database. In Proceedings of \nthe 4th International Conference on Music Information \nRetrieval (ISMIR03) , pages 229–230, 2003. \n[8] G. Grindlay and D. Ellis. A probabilistic subspace \nmodel for multi-instrument polyphonic transcription.\nIn Proceedings of the 11th International Society \nfor Music Information Retrieval Conference , Utrecht, \nNetherlands, August 9-13, 2010. \n[9] T. Hofmann. Probabilistic latent semantic analysis. In \nProc. of Uncertainty in Artiﬁcial Intelligence, UAI99 ,\npages 289–296, 1999.\n[10] Y . Itaya, H. Zen, Y . Nankaku, C. Miyajima, K. Tokuda, \nand T. Kitamura. Deterministic annealing EM algo- \nrithm in acoustic modeling for speaker and speech \nrecognition. IEICE Transactions , 88-D(3):425–431, \n2005.\n[11] H. Kameoka, T. Nishimoto, and S. Sagayama. \nHarmonic-temporal structured clustering via determin- \nistic annealing EM algorithm for audio feature extrac- \ntion. In Proceedings of the 6th International Confer- \nence on Music Information Retrieval (ISMIR05) , pages \n115–122, Sep. 2005.\n[12] D. Lee and H. Seung. Algorithms for non-negative ma- \ntrix factorization. In NIPS , pages 556–562. MIT Press, \n2001.\n[13] G. Poliner and D. Ellis. A discriminative model for \npolyphonic piano transcription. EURASIP Journal on \nAdvances in Signal Processing , pages 154–162, 2007. \n[14] C. Schoerkhuber and A. Klapuri. Constant-q transform \ntoolbox for music processing. In the 7th Sound and\nMusic Computing Conference , 2010. \n[15] P. Smaragdis and J. Brown. Non-negative matrix fac- \ntorization for polyphonic music transcription. In IEEE\nWorkshop on Applications of Signal Processing to Au- \ndio and Acoustics , pages 177–180, 2003. \n[16] P. Smaragdis and B. Raj. Shift-invariant probabilistic \nlatent component analysis. Technical report, 2007. \n[17] P. Smaragdis, B. Raj, and M. Shashanka. Sparse and \nshift-invariant feature extraction from non-negative \ndata. In IEEE International Conference on Acous- \ntics, Speech and Signal Processing (ICASSP08) , pages \n2069–2072, Apr. 2008. \n[18] N. Ueda and R. Nakano. Deterministic annealing EM \nalgorithm. Neural Networks , 11(2):271 – 282, 1998. \n[19] E. Vincent, N. Bertin, and R. Badeau. Adaptive har- \nmonic spectral decomposition for multiple pitch esti- \nmation. IEEE Transactions on Audio, Speech, and Lan- \nguage Processing , 18(3):528 – 537, 2010."
    },
    {
        "title": "A Distributed Model For Multiple-Viewpoint Melodic Prediction.",
        "author": [
            "Srikanth Cherla",
            "Tillman Weyde",
            "Artur S. d&apos;Avila Garcez",
            "Marcus T. Pearce"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415682",
        "url": "https://doi.org/10.5281/zenodo.1415682",
        "ee": "https://zenodo.org/records/1415682/files/CherlaWGP13.pdf",
        "abstract": "The analysis of sequences is important for extracting information from music owing to its fundamentally temporal nature. In this paper, we present a distributed model based on the Restricted Boltzmann Machine (RBM) for melodic sequences. The model is similar to a previous successful neural network model for natural language [2]. It is first trained to predict the next pitch in a given pitch sequence, and then extended to also make use of information in sequences of note-durations in monophonic melodies on the same task. In doing so, we also propose an efficient way of representing this additional information that takes advantage of the RBM’s structure. In our evaluation, this RBM-based prediction model performs slightly better than previously evaluated n-gram models in most cases. Results on a corpus of chorale and folk melodies showed that it is able to make use of information present in longer contexts more effectively than n-gram models, while scaling linearly in the number of free parameters required.",
        "zenodo_id": 1415682,
        "dblp_key": "conf/ismir/CherlaWGP13",
        "keywords": [
            "analysis",
            "sequences",
            "information",
            "music",
            "temporal",
            "nature",
            "distributed",
            "model",
            "Restricted",
            "Boltzmann"
        ],
        "content": "A DISTRIBUTED MODEL FOR MULTIPLE-VIEWPOINT MELODIC\nPREDICTION\nSrikanth Cherla1,2, Tillman Weyde1,2, Artur d’Avila Garcez2and Marcus Pearce3\n1Music Informatics Research Group, Department of Computer S cience, City University London\n2Machine Learning Group, Department of Computer Science, Ci ty University London\n3Centre for Digital Music, Queen Mary University of London\n{srikanth.cherla.1, t.e.weyde, a.garcez }@city.ac.uk\nmarcus.pearce@eecs.qmul.ac.uk\nABSTRACT\nThe analysis of sequences is important for extracting in-\nformation from music owing to its fundamentally temporal\nnature. In this paper, we present a distributed model based\non the Restricted Boltzmann Machine (RBM) for melodic\nsequences. The model is similar to a previous successful\nneural network model for natural language [2]. It is ﬁrst\ntrained to predict the next pitch in a given pitch sequence,\nand then extended to also make use of information in se-\nquences of note-durations in monophonic melodies on the\nsame task. In doing so, we also propose an efﬁcient way\nof representing this additional information that takes ad-\nvantage of the RBM’s structure. In our evaluation, this\nRBM-based prediction model performs slightly better than\npreviously evaluated n-gram models in most cases. Re-\nsults on a corpus of chorale and folk melodies showed that\nit is able to make use of information present in longer con-\ntexts more effectively than n-gram models, while scaling\nlinearly in the number of free parameters required.\n1. INTRODUCTION\nSequential structure in music inﬂuences our notions of mu-\nsical style, similarity and the emotions we associate with\nit. The analysis of sequences in musical scores and equiv-\nalent symbolic representations of music is an integral part\nof Music Information Retrieval, with applications such as\nmusic classiﬁcation [6], computational musicology [26],\nmusic creation [19], and music source separation [10]. In\nthe past, this analysis has often been carried out using mu-\nsic generation systems [1, 4, 8, 13, 18].\nThe present research is based around previous work that\nadopted ideas proposed in information theory to music [7].\nThere, Multiple-viewpoint Systems for Music Prediction\nwere introduced as a detailed re-interpretation of the key\nideas of information theory [22] in music, through an anal-\nogy between language and musical style. In that work and\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage an d that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieva l.what followed [21], Markov models were employed for\nlearning melodic subsequences. While this is a reason-\nable choice, Markov models are often faced with a prob-\nlem related to data sparsity known as the curse of dimen-\nsionality . This refers to the exponential rise in the num-\nber of model parameters with the length of the modelled\nsubsequences. Recent research in language modelling has\ndemonstrated that neural networks can be a suitable al-\nternative to more widely used n-gram and variable-order\nMarkov models [2, 5, 17]. There have been some initial\nresults on the success of such models in music [3, 24].\nIn this paper, we present a model for melody predic-\ntion based on one such neural network — the Restricted\nBoltzmann Machine (RBM) [23]. The choice is motivated\nby the following. Firstly, the inherent non-linearity of th e\nRBM makes it a suitable candidate for learning complex\nstructures in data, such as those occurring in musical se-\nquences. There exist efﬁcient algorithms for training this\nmodel [11, 25]. The RBM, with its straightforward exten-\nsibility to deep networks [12], has become a vital building\nblock for creating models that are capable of learning fea-\ntures from the data at multiple levels of abstraction.\nWe describe here a model for ﬁxed-length subsequences\nof musical pitch, which compares favourably to n-gram\nmodels that were previously evaluated with a prediction\ntask on a corpus of monophonic MIDI melodies [21]. This\npitch-only version of the model is then adapted to also\nmake use of note-durations in the melodies, on the same\npitch-prediction task. In doing so, we also propose an efﬁ-\ncient way to represent this additional information, which\ntakes advantage of the RBM’s structure and thus limits\nmodel complexity. The structure of the proposed model\nensures that it scales only linearly with the length of sub-\nsequences to be learned and with the number of symbols\nin the data. We demonstrate an improvement of results by\ncombining the two models in a manner similar to [7] us-\ning the arithmetic mean of their individual probability es-\ntimates. An implementation of the model in Python, along\nwith scripts used to generate the results in this paper, are\navailable upon request.\nThe remainder of this paper is organized as follows.\nThe next section introduces music prediction and multiple\nviewpoint systems as a framework for music prediction.\nSection 3 explains the RBM and its discriminative inter-pretation which make up the basis for the model proposed\nin this paper. This is followed by a description of the model\nitself in Section 4. An evaluation of the the model and its\ncomparison with previously evaluated n-gram models is\npresented in Section 5, followed by discussion on possible\ndirections for future research in Section 6.\n2. MUSIC PREDICTION WITH\nMULTIPLE-VIEWPOINT SYSTEMS\nIn order to explain music prediction with multiple view-\npoints, the analogy to natural language is used here. In\nstatistical language modelling, the goal is to build a model\nthat can estimate the joint probability distribution of sub se-\nquences of words occurring in a language L. A statistical\nlanguage model (SLM) can be represented by the condi-\ntional probability of the next word wTgiven all the previ-\nous ones[w1,...,w (T−1)](written here as w(T−1)\n1 ), as\nP(wT\n1) =T/productdisplay\nt=1P(wt|w(t−1)\n1). (1)\nThe most commonly used SLMs are n-gram models, which\nrely on the simplifying assumption that the probability of a\nword in a sequence depends only on the immediately pre-\nceding(n−1)words [16]. This is known as the Markov\nassumption, and reduces (1) to\nP(wT\n1) =T/productdisplay\nt=1P(wt|w(t−1)\n(t−n+1)). (2)\nFollowing this approach, musical styles can be inter-\npreted as vast and complex languages [7]. In music pre-\ndiction, one is interested in learning the joint distributi on\nofmusical event sequences sT\n1in a musical language S.\nMuch in the same way as an SLM, a system for music pre-\ndiction models the conditional distribution p(st|s(t−1)\n1), or\nunder the Markov assumption p(st|s(t−1)\n(t−n+1)). For each\nprediction, context information is obtained from the event s\ns(t−1)\n(t−n+1)immediately preceding st. Musical events have a\nrich internal structure and can be expressed in terms of di-\nrectly observable or derived musical features such as pitch ,\nnote duration, inter-onset interval, or a combination of tw o\nor more such features. The framework of multiple-view-\npoint systems for music prediction [7] was proposed in or-\nder to efﬁciently handle this rich internal structure of mu-\nsic by exploiting information contained in these different\nmusical feature sequences, while at the same time limiting\nthe dimensionality of the models using these features. In\nthe interest of brevity, we limit ourselves to an informal\ndiscussion of multiple-viewpoint systems for monophonic\nmusic prediction and refer the reader to [7] for the under-\nlying mathematical formulation.\nA musical event srefers to the occurrence of a note in\na melody. A viewpoint type (henceforth written as type)\nτrefers to any of a set of musical features that describe\nan event. The domain of a type, denoted by |τ|is the set\nof possible values of that type. A basic type is a directly\nobservable or given feature such as pitch, note duration,h1h2h\nv1v2v3v4vW\nFigure 1 . A simple Restricted Boltzmann Machine with\nfour visible, two hidden, and no bias units.\nkey-signature ortime-signature . Aderived type can be de-\nrived from any of the basic types or other derived types.\nAlinked viewpoint type is created by taking the Cartesian\nproduct over two or more types, thus “linking” them.\nAmultiple-viewpoint system (MVS) is a set of mod-\nels, each of which is trained on subsequences of one type,\nwhose individual predictions are combined in some way\nto inﬂuence the prediction of the next event in a given\nevent sequence. Given a context s(t−1)\n(t−n+1)and an event st,\neach viewpoint τin an MVS must compute the probabil-\nitypτ(st|s(t−1)\n(t−n+1)). While originally n-gram models were\nproposed to be used with the multiple viewpoints frame-\nwork, we demonstrate how a distributed model such as the\nRBM used here can serve as a scalable alternative.\n3. RESTRICTED BOLTZMANN MACHINE\nThe Restricted Boltzmann Machine (RBM) is an undirected\ngraphical model consisting of a set of rvisible units vand\na set ofqhidden units h. These make up the visible and\nhidden layers of the RBM respectively. The two layers\nare fully inter-connected but there exist no connections be -\ntween any two hidden units, or any two visible units. In its\noriginal form, the RBM has binary, logistic units in both\nlayers. Additionally, the units of each layer are connected\nto a bias unit whose value is always 1.\nThe edge between the ithvisible node and the jthhid-\nden node is associated with a weight wji. All these weights\nare together represented in a weight matrix Wof sizeq×r.\nThe weights of connections between visible units and the\nbias unit are contained in an r-dimensional visible bias\nvectorb. Likewise, for the hidden units there is a q-dimen-\nsional hidden bias vectorc. The RBM is fully character-\nized by the parameters W,bandc. Figure 1 shows a sim-\nple RBM with four visible and two hidden units, without\nthe bias unit to better illustrate its bipartite structure.\nThe activation probabilities of the units in the hidden\nlayer given the visible layer (and vice versa) are given by\nthe logistic sigmoid function as p(hj= 1|v) =σ(cj+\nWj·v), andp(vi= 1|h) =σ(bi+W′\ni·h)respectively. Due\nto the RBM’s bipartite structure, the activation probabili -\nties of the nodes within one of the layers are independent,\nif the activation of the other layer is given, i.e.\np(h|v) =q/productdisplay\nj=1p(hj|v) (3)\np(v|h) =r/productdisplay\ni=1p(vi|h). (4)The RBM is a special case of the Boltzmann Machine,\nwhich is an energy-based model for representing probabil-\nity distributions [15]. In such energy-based models, prob-\nability is expressed in terms of an energy function. In the\ncase of the RBM, this function is expressed as\nEnergy(v,h) =−b⊤v−c⊤h−h⊤Wv.(5)\nLearning in energy-based models can be carried out in a\ngenerative fashion, by updating the weights and biases in\norder to minimize the overall energy of the system with re-\nspect to the training data. This amounts to maximizing the\nlog-likelihood function of the joint probability distribu tion\np(v), which is given by\np(v) =e−FreeEnergy (v)\nZ, (6)\nwithZ=/summationtext\nve−FreeEnergy (v), where\nFreeEnergy (v) =−log/summationdisplay\nhe−Energy (v,h).(7)\nWhile computing the exact gradient of the log-likeli-\nhood function for p(v)is not tractable, an approximation\nof this gradient called the Contrastive Divergence (CD)\ngradient has been found to be a successful update rule for\ntraining RBMs [11]. With the CD update, the RBM can be\ntrained efﬁciently.\nThe RBM described above models the joint probability\np(v)of the set of visible units v. However, as described in\nSection 2, we are interested in a conditional distribution o f\nthe formp(y|x). It has been demonstrated in [14] how an\nRBM can be used for a discriminative task such as classiﬁ-\ncation. The posterior class probability distribution of su ch\nan RBM has the form\np(y=ec|x) =/summationdisplay\nhp(y=ec,h|x) (8)\n=e−FreeEnergy (x,ec)\n/summationtext\nc′=1...Ce−FreeEnergy (x,ec′)(9)\nwherexis the input vector, and yis a vector that is a 1-of-\nCrepresentation of the class (also known as one-hot en-\ncoding), with Cbeing the number of classes. If xbelongs\nto a class c, theny=ec, whereecis a vector with all val-\nues set to 0except at position c. With respect to the RBM,\nxandytogether make up the visible layer v.\nAssuming a training set Dtrain={(xi,yi)}wherexi\nandyi∈ {1,...,C}are thei-th input vector and target\nclass respectively, training the RBM generatively involve s\nminimizing the negative log-likelihood\nLgen(Dtrain) =−|Dtrain|/summationdisplay\ni=1logp(xi,yi). (10)\nThe RBM thus used in a discriminative manner, forms\nthe basis of the prediction model described in the next sec-\ntion.4. A DISTRIBUTED MODEL FOR USE WITH\nMULTIPLE VIEWPOINTS\nThe prediction model we present in this paper models the\nconditional distribution p(st|s(t−1)\n(t−n+1)). It places no re-\nstrictions on the types associated with events in the con-\ntexts(n−1)\n(t−n+1)(input type ), or the predicted event st(target\ntype). In the simplest case, both are the same. In the case\nwhere they are different, the performance of the model de-\npends on how informative the input types are of the target\ntype. In the present work, we demonstrate this model with\ntwo cases where (1) both the input and target viewpoint\ntypes are musical pitch, and (2) the input types are pitch\nand duration, and the target type pitch. The choice of the\nadditional input type in the second case was motivated by\nsimplicity and to lay emphasis on the representation.\nFor each monophonic melody (in MIDI format) in a\ngiven dataset, sequences of the relevant input and target\ntypes are ﬁrst extracted using the MIDI Toolbox [9]. These\nvalues are encoded as binary 1-of-|τ|vectors, where |τ|is\nthe size of the domain of type τ. In the case where more\nthan one input type exists, their corresponding vectors are\nsimply concatenated. Such an idea is similar to that of the\nlinked viewpoint type proposed in [7]. There are however,\ntwo important distinctions between the two. Firstly, the\ninput and target types must be identical in the case of the\nn-gram models originally proposed for use with multiple-\nviewpoint systems, whereas this is not a requirement for\nthe RBM model. Secondly, a linked viewpoint between\ntwo arbitrary types τ1andτ2of domain sizes |τ1|and|τ2|\nrespectively, would have a domain of size |τ1| × |τ2|in\nthe case of the n-gram models. Thus, for subsequences of\nlengthn, the number of free parameters to be estimated\nare(|τ1|×|τ2|)nin the worst case. In contrast, the number\nto be estimated in case of the RBM model, with qhid-\nden units and rvisible units, is (q×r) +q+r, where\nr= (n−1)×[(|τ1|+1)+(|τ2|+1)]+|τ3|, andτ3the\ntarget type. The additional visible unit added to the repre-\nsentation of each of the input types τ1andτ2in the con-\ntext is1when the corresponding event is absent at the start\nof a melody. Such a model only scales linearly with the\nlength of the learned subsequences as well as the domain\nsize of each of the involved viewpoint types (assuming q\nis constant). Its structure is depicted in Figure 2. Here we\nconsidered only those cases with a single target type.\n...h\n... ......... ...v\ns(t−n+1)s(t−n+2)...s(t−1) s(t)W\nFigure 2 . The structure of the prediction model. The set\nof nodes in the visible layer grouped together on the left\nmake up the context s(t−1)\n(t−n+1)of the input type(s). The set\nof nodess(t)to the far right corresponds to the target type.To train the model generatively, a subsequence st\n(t−n+1)\nis clamped to all the nodes in the visible layer. Training\nis done using the ﬁrst instantiation of the Contrastive Di-\nvergence learning algorithm (CD-1). This simply means\nthat the model parameters are updated after a single step\nof Gibbs sampling [11]. During prediction, the probabil-\nity of each of the possible pitches in the prediction space\nis determined using (9). The distribution generated in this\nway does not require any kind of smoothing operation for\nunseen subsequences unlike n-gram models, where in [21]\nan empirical evaluation of different smoothing techniques\nwas found necessary to establish the most reliable one.\n5. EV ALUATION\nIn order to evaluate the proposed prediction model, we\nmake a comparison to a previous study of n-gram models\nfor music prediction in [21]. There, cross-entropy was used\nto measure the information content of the models. This\nquantity is related to entropy , which is deﬁned as\nH(p) =−/summationdisplay\ns∈Sp(s)log2p(s). (11)\nwherep(s∈S) =p(χ=s)is the probability mass func-\ntion of a random variable χdistributed over a discrete al-\nphabetS={s1,...,s k}such that the individual proba-\nbilities are independent and sum to 1. The value of en-\ntropy, with reference to a prediction model, is a measure of\nthe uncertainty of its predictions. A higher value reﬂects\ngreater uncertainty. In practice, one rarely knows the true\nprobability distribution of the stochastic process and use s\na model to approximate the probabilities in (11). An es-\ntimate of the goodness of this approximation can be mea-\nsured using cross-entropy ( Hc) which represents the diver-\ngence between the entropy calculated from the estimated\nprobabilities and the source model. This quantity can be\ncomputed over all the subsequences of length nin the test\ndataDtest, as\nHc(pmod,Dtest) =−/summationtext\nsn\n1∈Dtestlog2pmod(sn|s(n−1)\n1)\n|Dtest|\n(12)\nwherepmodis the probability assigned by the model to the\nlast pitch in the subsequence given its preceding context.\nCross-entropy approaches the true entropy as the number\nof test samples ( |Dtest|) increases.\nEvaluation was carried out on a corpus of monophonic\nMIDI melodies that cover a range of musical styles. The\ncorpus is a collection of 8datasets containing a total of\n54,308musical events and was also used to evaluate n-\ngram models for music prediction in [21]. There, two dif-\nferent models were evaluated both individually and in com-\nbination. The ﬁrst of these was a Long-Term Model (LTM),\nthat was governed by structure and statistics induced from\na large corpus of sequences from the same musical style.\nAnd the other was a Short-Term Model (STM) which re-\nlied on structure and statistics particular to the melody be -\ning predicted. The prediction model presented here dealsonly with long-term effects that are induced from a cor-\npus, and is thus compared with the two best performing\nLTMs in [21] of unbounded order (labelled there as C*I)\nand order bound 2respectively. To facilitate a direct com-\nparison between the two approaches, the melodies are not\ntransposed to a default key.\nFor the RBM model, different hyperparameters were\nevaluated through a grid search over the learning rate λ=\n{0.01,0.05}, the number of hidden units nhid={100,\n200,400}, and the weight-cost wcost={0.0001,0.0005}.\nEach model was trained using mini-batch gradient descent\nover500epochs with a batch size of 100samples. The\nmomentum µ, was set to 0.5during the ﬁrst ﬁve epochs\nand then increased to 0.9for the rest of the training. Each\nmodel was evaluated with 10-fold cross-validation.\nWe carry out three types of evaluation. The ﬁrst mea-\nsures the information content of the pitch-only version of\nthe proposed model using cross-entropy, and compares it\nto then-gram models of [21]. It was observed that the\nRBM model compares favourably with the best of the n-\ngram models by making better use of information in longer\ncontexts. In the second evaluation, we compare a variant\nof the model with input types pitch and duration and tar-\nget type pitch to its pitch-only counterpart. And lastly, we\ncombine these two models using mixture-of-experts and\ndemonstrate how this can further improve the model per-\nformance in comparison to the individual models.\nThe ﬁrst evaluation is carried out with cross-validation\nseparately for each of the individual datasets. The con-\ntext length is varied between 1and8. It was found that\nthe RBM models with context length greater than 2per-\nform better than corresponding n-gram models on aver-\nage. This is illustrated in Figure 3. An RBM model of\nsuitable context length perform marginally better than the\nbest-performing n-gram model — that of unbounded or-\nder. The same is the case with the best bounded-order n-\ngram model (of context length 2) and the RBM model of\nthe same context length. While it was found that the perfor-\nmance of bounded order n-gram models tends to worsen\non further increasing the context length, the performance\nof RBM models continues to improve until a context length\nof4. The value of nwhere the RBM model performs better\nthan then-gram models of unbounded order is different on\ndifferent datasets, and typically occurs between n= 3and\nn= 7. The best average model cross-entropy of 2.819is\nreached for a context length of 4. For models using longer\ncontexts an increase in training performance was accom-\npanied by a slight worsening of test performance, indicat-\ning overﬁtting. We suspect that the overall performance\nof the RBM models can be further improved with an op-\ntimized grid-search strategy in the hyper-parameter space ,\nbut leave this to be explored in the future. The optimal\nnumber of hidden units in our search was 100across all\ndatasets for almost all context lengths, leading to a linear\nincrease in model size with context length.\nIn the second evaluation, we compared the cross-entropies\nof the single and multiple input type models (pitch and\npitch with duration respectively) using the same target typ e\n(pitch), on the Bach chorale subset of the corpus. The re-0 1 2 3 4 5 6 7 8 9 102.62.72.82.933.13.23.33.43.5\nContext-lengthCross-entropy\n  \nn-gram (bounded)\nn-gram (unbounded)\nRBM\nFigure 3 . Variation in average cross-entropy of the prediction mode ls with context length l(with standard deviation across\nfolds for the RBM model). The cross-entropy of the RBM models progressively decreases until l= 4, while that of the\nn-gram models evaluated in [21] is minimal at l= 2 and increases thereafter. The performance of the n-gram model of\nunbounded order is indicated by the dashed line.\nsults are shown in Table 1. The choice of adding duration\nwas motivated by simplicity but the results show that it was\nnot ideal for improving predictions. This conclusion is als o\nsupported by a similar trend observed with n-gram models,\nwhere a small deterioration in performance was observed\non adding duration. The RBM model shows small per-\nformance improvements for some context lengths. This\nindicates that the representation for multiple input types\nproposed in Section 4 as an alternative to the linked view-\npoints may indeed be effective.\nl 1 2 3 4\nn-gram (p) 2.737 2.565 2.505 2.473\nn-gram (p + d) 2.761 2.562 2.522 2.502\nRBM (p) 2.698 2.530 2.490 2.470\nRBM(p + d) 2.660 2.512 2.481 2.519\nRBM (combined) 2.663 2.486 2.462 2.413\nTable 1 . Cross-entropies of the single (pitch) and mul-\ntiple (pitch, duration) input type RBM models and their\ncombination over a range of context lengths lon the Bach\nchorales dataset. The individual RBM models compare\nfavourably with corresponding n-gram models.\nTo illustrate the application of the proposed RBM model\nto multiple viewpoints for music prediction, we combine\nthe pitch-only and the pitch & duration models. We use a\nsimple mixture-of-experts model, i.e., take the arithmeti c\nmean of the distributions each of the two models predicts\nfor pitch. The results of this are listed in the third row of\nTable 1 and show an improvement over individual models.6. CONCLUSIONS & FUTURE WORK\nWe presented a distributed model based on the Restricted\nBoltzmann Machine for multiple-viewpoint music predic-\ntion. It was demonstrated how such a model can be a\nscalable alternative to n-gram models for simultaneously\nmodelling sequences of multiple musical features. The\nproposed model was evaluated in comparison with n-gram\nmodels and was found to compare favourably with them.\nIt is able to make better use of information in longer event\ncontexts than n-gram models, and also scales linearly with\ncontext length.\nIn the future, we would ﬁrst like to address some of the\nissues left open in the present research. These include ex-\nperiments with more promising viewpoint-type combina-\ntions as reported in [7] and [20], the use of alternative data\nfusion techniques like the weighted mixture- and product-\nof-experts [20], and further optimization of the existing\nmodel parameters. Previous research suggests that com-\nbining the LTM and STM improves prediction performance\n[7, 20] and, in fact, the combined n-gram model reported\nin [20] (mean cross-entropy: 2.479for all datasets; 2.342\nfor the chorale dataset) outperforms the long-term RBMs\nexamined here. Given the improved performance of these\nlong-term RBMs, we expect adding a short-term compo-\nnent will yield the best prediction performance yet observe d\nfor this corpus. Extensions of the present model to handle\npolyphony and higher-level musical structure will also be\nexplored. We would also like to apply the prediction model\ndescribed here to some of the MIR tasks listed in Section\n1. The present model can be potentially extended into a\ndeep network, as demonstrated in [11], which is expected\nto improve its performance further.7. ACKNOWLEDGEMENTS\nSrikanth Cherla is supported by a Ph.D. studentship from\nCity University London. The authors would like to thank\nSon Tran for many useful discussions on RBMs, and the\nreviewers for their valuable feedback on the paper.\n8. REFERENCES\n[1] Charles Ames. The Markov Process as a Compo-\nsitional Model: A Survey and Tutorial. Leonardo ,\n22(2):175–187, 1989.\n[2] Yoshua Bengio, R´ ejean Ducharme, Pascal Vincent,\nand Christian Jauvin. A Neural Probabilistic Lan-\nguage Model. Journal of Machine Learning Research ,\n3:1137–1155, 2003.\n[3] Greg Bickerman, Sam Bosley, Peter Swire, and Robert\nKeller. Learning to Create Jazz Melodies using Deep\nBelief Nets. In International Conference On Computa-\ntional Creativity , 2010.\n[4] John Biles. Genjam: A genetic algorithm for gener-\nating jazz solos. In Proceedings of the International\nComputer Music Conference , pages 131–131, 1994.\n[5] Ronan Collobert, Jason Weston, L´ eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural\nlanguage processing (almost) from scratch. The Jour-\nnal of Machine Learning Research , 12:2493–2537,\n2011.\n[6] Darrell Conklin. Multiple viewpoint systems for mu-\nsic classiﬁcation. Journal of New Music Research ,\n42(1):19–26, 2013.\n[7] Darrell Conklin and Ian H Witten. Multiple viewpoint\nsystems for music prediction. Journal of New Music\nResearch , 24(1):51–73, 1995.\n[8] David Cope. Experiments in musical intelligence , vol-\nume 12. AR Editions Madison, WI, 1996.\n[9] Tuomas Eerola and Petri Toiviainen. MIR in Matlab:\nThe Midi Toolbox. In Proceedings of the International\nConference on Music Information Retrieval , pages 22–\n27. Universitat Pompeu Fabra Barcelona, 2004.\n[10] Joachim Ganseman, Paul Scheunders, Gautham J\nMysore, and Jonathan S Abel. Evaluation of a Score-\ninformed Source Separation System. In International\nSociety for Music Information Retrieval Conference\n(ISMIR) , pages 219–224, 2010.\n[11] Geoffrey E Hinton. Training products of experts by\nminimizing contrastive divergence. Neural computa-\ntion, 14(8):1771–1800, 2002.\n[12] Geoffrey E Hinton, Simon Osindero, and Yee-Whye\nTeh. A Fast Learning Algorithm for Deep Belief Nets.\nNeural Computation , 18:1527–1554, 2006.[13] Robert M Keller and David R Morrison. A Grammat-\nical Approach to Automatic Improvisation. In Sound\nand Music Computing Conference , pages 11–13, 2007.\n[14] Hugo Larochelle and Yoshua Bengio. Classiﬁcation\nusing discriminative restricted Boltzmann machines.\nInInternational Conference on Machine Learning\n(ICML) , pages 536–543. ACM Press, 2008.\n[15] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,\nand F Huang. A tutorial on energy-based learning. Pre-\ndicting Structured Data , 2006.\n[16] Christopher D Manning and Hinrich Sch¨ utze. Founda-\ntions of statistical natural language processing . MIT\npress, 1999.\n[17] Andriy Mnih and Geoffrey E Hinton. A scalable hierar-\nchical distributed language model. In Advances in neu-\nral information processing systems , pages 1081–1088,\n2008.\n[18] Michael C Mozer. Connectionist music composition\nbased on melodic, stylistic and psychophysical con-\nstraints. Music and connectionism , pages 195–211,\n1991.\n[19] Francois Pachet. The continuator: Musical interactio n\nwith style. Journal of New Music Research , 32(3):333–\n341, 2003.\n[20] Marcus Pearce. The Construction and Evaluation of\nStatistical Models of Melodic Structure in Music Per-\nception and Composition . PhD thesis, 2005.\n[21] Marcus Pearce and Geraint Wiggins. Improved meth-\nods for statistical modelling of monophonic music.\nJournal of New Music Research , 33(4):367–385, 2004.\n[22] Claude E. Shannon. A Mathematical Theory of\nCommunication. The Bell System Techincal Journal ,\n27(July):379–423, 623–656, 1948. Reprinted in ACM\nSIGMOBILE Mobile Computing and Communications\nReview , 5(1):3–55, 2001.\n[23] Paul Smolensky. Parallel distributed processing: ex-\nplorations in the microstructure of cognition, vol. 1.\nchapter Information processing in dynamical systems:\nfoundations of harmony theory, pages 194–281. MIT\nPress, Cambridge, MA, USA, 1986.\n[24] Athina Spiliopoulou and Amos Storkey. Compar-\ning probabilistic models for melodic sequences. In\nMachine Learning and Knowledge Discovery in\nDatabases , pages 289–304. 2011.\n[25] Tijmen Tieleman. Training restricted boltzmann ma-\nchines using approximations to the likelihood gradient.\nInProceedings of the 25th international conference on\nMachine learning , pages 1064–1071. ACM, 2008.\n[26] Raymond Whorley, Christophe Rhodes, Geraint Wig-\ngins, and Marcus Pearce. Harmonising melodies: Why\ndo we add the bass line ﬁrst? In International Confer-\nence on Computational Creativity , pages 79–86, 2013."
    },
    {
        "title": "The Use Of Melodic Scales In Bollywood Music: An Empirical Study.",
        "author": [
            "Monojit Choudhury",
            "Ranjita Bhagwan",
            "Kalika Bali"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418235",
        "url": "https://doi.org/10.5281/zenodo.1418235",
        "ee": "https://zenodo.org/records/1418235/files/ChoudhuryBB13.pdf",
        "abstract": "Hindi film music, which is commonly referred to as “Bollywood” music, is one of the most popular forms of music in the world today. One of the reasons for its popularity has been the willingness of Bollywood composers to adopt and be influenced by various musical forms including Western pop, jazz, rock, and classical music. However, till date, we are unaware of any systematic quantitative analysis of how this genre has changed and evolved over the years since its inception in the early 20th century. In this paper, we study the evolution of Bollywood music with respect to the use of melodic scales. We analyse songs composed over seven decades using a database of top-lists, which reveals many interesting patterns. We also analyze the scale usage patterns in the music of some of the most popular composers, which clearly brings out certain idiosyncrasies and preferences of each of them.",
        "zenodo_id": 1418235,
        "dblp_key": "conf/ismir/ChoudhuryBB13",
        "keywords": [
            "Bollywood music",
            "pop",
            "jazz",
            "rock",
            "classical music",
            "evolution",
            "melodic scales",
            "database of top-lists",
            "patterns",
            "composers"
        ],
        "content": "THE USE OF MELODIC SCALES IN BOLLYWOOD MUSIC:\nAN EMPIRICAL STUDY\nMonojit Choudhury Ranjita Bhagwan Kalika Bali\nMicrosoft Research Lab India\nfmonojitc, bhagwan, kalikab g@microsoft.com\nABSTRACT\nHindi ﬁlm music, which is commonly referred to as\n“Bollywood” music, is one of the most popular forms of\nmusic in the world today. One of the reasons for its popu-\nlarity has been the willingness of Bollywood composers to\nadopt and be inﬂuenced by various musical forms includ-\ning Western pop, jazz, rock, and classical music. How-\never, till date, we are unaware of any systematic quantita-\ntive analysis of how this genre has changed and evolved\nover the years since its inception in the early 20thcentury.\nIn this paper, we study the evolution of Bollywood mu-\nsic with respect to the use of melodic scales . We analyse\nsongs composed over seven decades using a database of\ntop-lists, which reveals many interesting patterns. We also\nanalyze the scale usage patterns in the music of some of the\nmost popular composers, which clearly brings out certain\nidiosyncrasies and preferences of each of them.\n1. INTRODUCTION\nThe Mumbai based Hindi language ﬁlm industry, which is\npopularly referred to as Bollywood1, produces more than\n800 movies every year2. Almost all Bollywood movies\nfeature several songs that are very popular in India, and\nhave often been termed as the heartline of Indian popu-\nlar culture [1, 11]. In fact, Bollywood songs are one of\nthe most searched items on the Web from India3. Over\ndecades, this music has inﬂuenced lives and cultures not\njust in India, but across all of Asia, Africa, Eastern Europe,\nand more recently, North America [7]. Several scholars\nin the past [2] have attributed this universal popularity of\nBollywood music to its great ability in assimilating vari-\nous styles of music from around the world and churning\nout compositions of global appeal. The cultural history of\nBollywood, the emergence of musical styles, genres and\nsub-genres, and corresponding ethnomusicological aspects\nhave been well studied by social scientists [5, 9, 10]. How-\n1http://en.wikipedia.org/wiki/Bollywood\n2http://geography.about.com/od/culturalgeography/a/bollywood.htm\n3http://www.google.com/intl/en/press/zeitgeist2010/regions/in.html\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.ever, as far as we know, till date there are no quantitative\nand statistical analysis of musical styles of Bollywood, ex-\ncept for a recent study that analyzes Bollywood song lyrics\nthrough computational linguistic techniques [3].\nIn this paper, we present a quantitative study of the rela-\ntive usage and popularity of various melodic scales in Bol-\nlywood over a period of seven decades. We analyzed the\nscales used in 310 songs from 1953 to the present day, all\nof which have been in the top 50 hit list for the year they\nappeared in. Our results show that some scales have waned\nin popularity over the years, while some other scales have\nalways dominated the mix. We also identiﬁed ﬁve inﬂu-\nential music composers of Bollywood from different time\nperiods and analyzed the usage of musical scales in their\ncompositions. Again we noticed several interesting com-\nposer speciﬁc trends. For instance, some composers like\nAllahrakka Rahman and Rahul Dev Burman, show great\ndiversity in the usage of musical scales in their compo-\nsitions, while others such as Shankar-Jaikishan or Pritam\nChakraborty preferred to exploit the melodic patterns of a\nfew popular scales.\nThe study of melodic scales in Bollywood music is par-\nticularly interesting because since its inception in the early\n20thcentury and until the 1960s, Hindi ﬁlm songs were\nalmost exclusively based on or inspired by Indian Classi-\ncal Music (ICM). A characteristic feature of ICM is its use\nof a large number of scales. Hence, we expect Hindi ﬁlm\nmusic to exhibit a similar diversity of scale usage than that\nfound in the popular music of other cultures. However, the\ndecreasing inﬂuence of ICM, and the gradually increasing\ninﬂuence of folk, rock, pop and blues on this genre can\nsigniﬁcantly alter the scale usage pattern over time. Thus,\nby studying the evolution of scale usage patterns in Bol-\nlywood, we can, in essence, objectively characterize the\ninﬂuence of various musical cultures as well as individual\ncomposers on Bollywood.\n2. MELODIC SCALE OF A SONG\n2.1 Deﬁnition of a Scale\nBollywood music, along the lines of ICM, uses the stan-\ndard twelve note (in an octave) Western chromatic scale .\nAny given composition uses only a subset of these twelve\nnotes, which can be loosely referred to as the melodic scale\nof the composition. This is a relative scale ormode -based\nsystem, where the intervals between the adjacent notes are\nﬁxed, but not the notes themselves. Once the tonic , theNICM Sa kom. Re Re kom. Ga Ga Ma teevra Ma Pa kom. Dha Dha kom. Ni Ni\nNotation S r R g G m M P d D n N\nWestern C C # orDbD D # orEbE F F # orGbG G # orAbA A # orBbB\nTable 1 . Names and notations of the notes in NICM along with their Western counterparts. kom. stands for komal meaning\na ﬂat note. teevra is a sharp note.\nﬁrst note of the octave referred to as Sain Indian musical\ntraditions (equivalent to Doin Solfaggio), is set to a spe-\nciﬁc note (say CorAb) the rest of the notes in the scale\ngets automatically deﬁned. Table 1 introduces the names\nof the notes used in Indian music and the notation that we\nwill use in this paper along with the corresponding note in\nWestern music when the tonic is on C.\nLike Western music, the most common scale types in\nICM are heptatonic , i.e., consist of seven notes. There-\nfore, the two dominant classical musical traditions preva-\nlent in India today, the Hindustani or the North Indian\nClassical Music (NICM) and the Carnatic or the South In-\ndian Classical Music (SICM), deem the heptatonic scales\nto be fundamental; all other scales – pentatonic, hexatonic\nand (rarely) octatonic are considered to be derived from\na parent heptatonic scale. Musicologists in both the tradi-\ntions [4,6] have extensively studied and proposed the deﬁn-\ning features of the heptatonic scales used in the respective\nsystems. A detailed discussion on these theories is beyond\nthe scope of this paper, though it sufﬁces to mention here\nthat in NICM tradition, from which Bollywood music has\nderived its main genetic material, a basic heptatonic scale\ncan have only one of the two Re’s,Ga’s,Ma’s,Dha ’s\nandNi’s. This leads to 32 possible fundamental scales,\nof which only ten are traditionally identiﬁed as important\nor fundamental for deriving other scales and ragas4[4].\nThese scales are referred to as thaats . We shall refer to\nthese scales by their corresponding thaat names. However,\nthere are some more scales that are presently used in NICM\n(often borrowed from popular ragas, e.g., Charukeshi and\nKirwani , in SICM system). Table 2 lists all these popu-\nlar heptatonic scales used in NICM. We shall refer to these\nscales by the name of the thaat if it is one of the 10 thaats\ndescribed by Bhatkhande, else we will refer to them by the\nname of a popular raga based on this scale. One should\nnote that a raga and a scale are not equivalent, it is just\na naming convention that we will follow in this paper, and\nhenceforth all the raga names should be interpreted as the\ncorresponding scale (set of notes) used by the raga.\n2.2 Identifying the Scale of a Song\nAlmost all the Bollywood songs are composed in the scales\nmentioned in Table 2, though some of the compositions\nmight use a pentatonic or hexatonic scale derived from\none of these heptatonic scales, and sometimes accidentals\n(notes that do not belong to the parent scale) might be em-\nployed at a certain point in the composition. Since the\n4Araga is based on an ascending and descending scale, but is charac-\nterized using many other features and evades a formal deﬁnition. See [4]\nfor a detailed exposition.Scale NICM name Western name\nS R G m P D N Bilawal* Major Diatonic\nS R g m P D N Patdeep Melodic Minor\nS r G m P d N Bhairav* Major Gypsy\nS R g m P d N Kirwani Harmonic Minor\nS R G m P D n Khamaj * Mixolydian\nS R g m P D n Kaﬁ* Dorian\nS R G m P d n Charukeshi Major Minor\nS R g m P d n Asavari * Natural Minor\nS r g m P d n Bhairavi * Phrygian\nS R G M P D N Kalyan* Lydian\nS r G M P D N Marwa* Not known\nS r G M P d N Purvi* Chromatic Hypo-\nlydian\nS r g M P d N Todi* Chromatic Lydian\nInverse\nTable 2 . List of heptatonic scales used in NICM. The\nThaat names are marked with *.\n(staff) notations are not readily available for Bollywood\nsongs5, we resort to music experts to identify the notes\nand hence the scale of a song. When the set of notes used\ncan be easily identiﬁed as a heptatonic scale (which almost\nalways maps to one of the scales listed in Table 2), we\nunambiguously mark the scale of the song with its corre-\nsponding name. Whenever a note and its ﬂatter or sharper\nversion are used in the same song (less than 10% in our\ndataset), one of the notes is marked as accidental based on\nthe expert’s intuition. This often involves looking at the\nfrequency of usage, as the accidentals are used much less\nfrequently than the other note which is a part of the scale.\nOnce the accidentals are so identiﬁed, it is easy to map the\nrest of the notes into one of the heptatonic scales.\nThere are a substantial number of Bollywood compo-\nsitions that use scales with fewer than seven notes. For\ninstance, the Major Pentatonic scale – ( S R G P D ), that\nis used in ragas Bhupali ,Deshkar etc., has been used in\nmany Bollywood songs. This scale can be thought to be a\nderivative of Kalyan ,Bilawal orKhamaj . Due to this am-\nbiguity, we do not map pentatonic and hexatonic scales to\na parent heptatonic scale; rather, we refer to those scales by\nthe name of a popular raga that uses the scale. Some of the\ncommon pentatonic and hexatonic scales that we have en-\ncountered in Bollywood music (along with the names we\n5This is again a common feature of Indian musical traditions, where\nimprovisations abound in all genres of classical and semi-classical music,\nand consequently a ﬁxed notation is rarely used to describe the composi-\ntion.will refer to them with): S R G P D (Bhupali or the Ma-\njor Pentatonic scale), S R m P n (Megh or the Suspended\nPentatonic scale), S R G P N (Hamsadhvani ),S R g P\nD(Shivaranjani ) and S R g m P n (Nayaki or the Minor\nHexatonic scale).\nIt is important to note that Bollywood songs are usually\nstructured with a beginning mukhda (analagous to a cho-\nrus) followed by anywhere between one and four antaras\n(analogous to verses). Sometimes, a song may use differ-\nent melodic scales in the chorus and the verses. For the\npurpose of this study, we study the scales used only in the\nchorus, since this is the most important and remembered\npart of a song, on which the success or popularity of a song\nprimarily depends.\n3. DATA COLLECTION METHOD\nIn order to study the usage pattern of scales in Bollywood\nsongs and its evolution over the years, we want to build a\ndataset of songs along with their scales. Information about\nthe year of release of the song and the composer are also\nnecessary because our objective is to do a trend analysis\nand identify inﬂuences of composers on scale usage, if any.\nThus, we ﬁrst identify the songs that we would like to an-\nalyze, identify the scales of those songs, and then identify\na set of composers who are well represented in our dataset\nand hence can be meaningfully studied. The dataset is pub-\nlicly available at: http://bit.ly/18Edp7Y\n3.1 Selection of Songs\nThere are no published authentic statistics on the number\nof Bollywood songs produced to date. Some crude esti-\nmates tell us that this number should be between 100,000\nand 200,000. Therefore, it is impossible to manually iden-\ntify the scales for even a small subset, say a randomly se-\nlected 10%, of Bollywood songs. We decided to select a\nsubset of popular songs from every year that appeared in\nthe year-speciﬁc top-lists. There are two reasons for this\nchoice. First, the top songs represent the choice of the peo-\nple or what was popular at a particular point of time. Sec-\nond, top songs probably also have an inﬂuence on the com-\nposers and their future compositions. We selected songs\nfrom Binaca Geetmala6charts, which published yearly\ntop-lists from 1953 to 1994 and were the authoritative re-\nsource at the time. The lists have somewhere between 12\nto 40 songs per year with more than 1000 songs in all. We\nchose two to three years per decade and included the top\nsongs from those years in our dataset. We collected top-\nlists for 2000s from other online sources7.\n3.2 Selection of Composers\nFrom the list of selected songs, we identiﬁed the com-\nposers for whom we already had a substantial number of\nsongs. These composers are (active year range for our\n6http://en.wikipedia.org/wiki/Binaca Geetmala\n7http://shaileshkapoor.com/2012/01/14/top-50-bollywood-songs-of-\n2011/, http://www.bollywoodmusicradio.com/announcements/2617-top-\n40-songs-2005-a.htmlDecade 50 60 70 80 90 2000 2011-\n#Songs 45 31 76 32 40 51 35\nTable 3 . Number of songs selected for analysis per decade.\nTotal number of songs is 310.\ndataset is given in parentheses): Shankar-Jaikishan8(1953-\n71), Kalyanji-Anandji9(1960-81), R. D. Burman10(1971-\n81), A. R. Rahman11(1992-2013, composer and Academy\naward winner for Slumdog Millionaire) and Pritam Chak-\nraborty12(2004-2013). All of these have composed many\nhit songs over more than a decade and are considered as\nvery inﬂuential composers who have introduced new mu-\nsical styles and genres in Bollywood music [1]. Moreover,\nthey composed during different periods of time. Hence, by\nstudying their scale usage preferences it might be possible\nto gain interesting insights into the evolution of Bollywood\nmelodies. In order to make the analysis more reliable, we\nthen selectively included several other songs in our dataset\nthat were composed by these ﬁve composers and that fea-\ntured in the yearly top-lists.\nThe ﬁnal dataset of songs, that was thus created, has 310\nsongs composed between 1953 and 2013. Table 3 reports\nthe number of songs selected per decade in our dataset.\n3.3 Scale Identiﬁcation\nWe asked two experts, who have 10+ years of training in\nNICM and Bollywood music, to independently identify the\nscales for a randomly selected 50 songs from our dataset\naccording to the rules speciﬁed in Sec. 2.2. The experts\nagreed on the scales for all the 50 songs, which indicates\nthat scales for the songs are unambiguously identiﬁable,\nand both the experts were good at it. Hence, ﬁnally we\nasked only one of the two experts to identify the scales for\nrest of the songs in the dataset. The process takes, on an\naverage, approximately 2 to 5 minutes per song. We intend\nto make this dataset publicly available for future research.\n4. FINDINGS\nA dataset of 310 songs spread over seven decades does not\nprovide sufﬁcient information to conduct a sophisticated\nper-year statistical analysis. Therefore, here we will report\noverall statistics on the temporal distribution of melodic\nscales, which reveal several interesting trends. Then we\nwill move beyond gross statistical analysis, and focus on a\ncase-by-case study of several issues that we could observe\neven from this limited dataset.\n4.1 Overall Distribution of Scales\nFig. 1 shows the percentage of songs in the entire dataset\nthat uses a particular melodic scale, represented as a pie-\n8http://en.wikipedia.org/wiki/Shankar Jaikishan\n9http://en.wikipedia.org/wiki/Kalyanji Anandji\n10http://en.wikipedia.org/wiki/R.D. Burman\n11http://en.wikipedia.org/wiki/A. R.Rahman\n12http://en.wikipedia.org/wiki/Pritam ChakrabortyFigure 1 . Distribution of Scales in our dataset.\nchart. We observe that almost a quarter of the songs are\ncomposed in Asavari and another quarter in Bilawal . This\nis not entirely surprising because these two heptatonic scales\nare known to be the most popular major and minor scales\nacross the globe. However, these statistics are indeed sur-\nprising if one considers the fact that in NICM as well as\nIndian folk traditions, though Bilawal is used quite often,\nuse of Asavari and its related ragas is not so common. In\nfact, Kalyan andKaﬁ are by far the more popular scales\npresently used in NICM rather than Asavari .\nBhairavi ,Kaﬁ andKhamaj together cover another quar-\nter of the songs, each being used almost equally frequently.\nIt is surprising to see that only 4%of the songs are com-\nposed in Kalyan , which is otherwise a very popular scale in\nNICM. On the other hand, Nayaki andKirwani , which are\nrarely used in NICM, are as well represented as Kalyan .\nPentatonic scales are quite popular (8%) in Bollywood,\nof which the more common ones are Bhupali (4%), Shiv-\naranjani (1%) and Megh (1%). We did notice a few cases\nwhere the scale used had only 3 or 4 notes, and sometimes\na pentatonic or hexatonic scale that is not used in NICM or\nWestern music. Together, these cases account for 10 songs\nin our dataset.\nIt is interesting to note that in our dataset we observe no\ncompositions in Bhairav ,Marwa andTodi, and only one\ninPurvi . These four scales enjoy the status of a thaat in\nNICM and are very popular in both NICM and SICM. This\nperhaps can be explained by the fact that these heptatonic\nscales have less symmetric tetrachords (see [8] for an in-\ndepth discussion) than the six other thaat scales that are\nmuch more frequently used in Bollywood.\n4.2 Temporal Dynamics of the Scales\nIs there any signiﬁcant change in the scale usage pattern\nover the years? To study the temporal dynamics of the\nscales, we computed the distribution of the scales for each\ndecade as the fraction of the songs in a given decade that\nwere composed in a particular scale. We observe that the\nscales can be broadly divided into three categories based\non their temporal trends: (a) scales which did not show\nany drastic change in their usage pattern and have always\nFigure 2 . Change in the use of the scales over time.\nbeen popular in Bollywood songs (i.e., always popular);\n(b) scales which did not show any drastic change in the\ntrends, but were always less commonly used; and (c) scales\nwhich show drastic changes in their usage. Fig. 2 shows\nthe trends for these three classes of scales in three different\nbar-charts. From the top chart one can easily see that the\nscales Asavari andBilawal were always commonly used\nand together they accounted for half of the songs com-\nposed in any decade. The middle chart shows the trends\nforKaﬁ,Khamaj and the pentatonic scales. There are no\nspeciﬁc observable trends (except for a drop in the use of\nKaﬁ as compared to the 1950s) and the minor variations\nare statistically insigniﬁcant. However, it is reasonable to\nconclude that these scales have always been in use in Bol-\nlywood songs, but only sparingly. The third set of scales\nrepresented in the lowermost chart shows several interest-\ning trends.\nWe observe that the use of Bhairavi is steadily gain-\ning popularity over the years, whereas the use of Kalyan ,\nwhich was quite popular in 1950s and 60s, has steadily\ngone down and there is not a single composition in Kalyan\nin the dataset beyond 1980s. Kirwani made its appear-\nance in 1970s and enjoyed some popularity over the next\ntwo decades, and then disappeared. Finally, Nayaki , which\nhad made a brief appearance in 1970s, gained popularity in\nthe 90s, which sustained over the next decade and beyond.\nClearly, these drastic changes in the usage patterns of somescales cannot be explained away as mere random ﬂuctua-\ntions. These could be either due to the inﬂuence of speciﬁc\nmusic composers, cross-cultural inﬂuences, or both.\n4.3 Composer-speciﬁc Trends\nTable 4 reports the number of songs by a composer that\nuses a particular melodic scale. As one would expect, all\nthe ﬁve composers have composed a signiﬁcant number of\nsongs in Bilawal andAsavari , the two most popular scales\nused in Bollywood. However, there are a couple of inter-\nesting observations that we would like to highlight here.\nFirst, the number of distinct scales used by Kalyanji-\nAnandji (K-A), R. D. Burman and A. R. Rahman is much\nmore than what we observe for the compositions of Pritam\nand Shankar-Jaikishan (S-J). In fact, we also noticed that\neven when using well-known scales like Asavari andBi-\nlawal , R. D. Burman and A. R. Rahman have extensively\nused various accidentals. Hence, from the perspective of\nscale-usage, one can say that some of the famous Bolly-\nwood composers were inclined to experiment with scales\nand introduce new melodic patterns, whereas other equally\nfamous composers focussed their attention on other com-\nponents of the composition (e.g., the rhythm).\nSecond, the drastically changing trends for the usage of\ncertain scales is evident from the composer-speciﬁc analy-\nsis as well. For instance, we notice an increase in the use\nofBhairavi by Pritam, which partly explains the rise of\nits popularity in the 2000s and beyond. Similarly, Nayaki\nseems to have been introduced by R. D. Burman and then\nextensively used by more recent composers like A. R. Rah-\nman and Pritam. On the other hand, Kaﬁ, which was used\nto some extent by S-J and K-A has not been used by R. D.\nBurman or Pritam. This explains the slight drop in its use,\nthough it is due to Rahman that it still shows up in some\ncompositions.\nThere are also interesting composer-speciﬁc observa-\ntions such as the strong preference towards Bilawal by Pri-\ntam or Khamaj by A. R. Rahman, which deviates from the\nBollywood norms. Likewise, K-A’s nearly equal prefer-\nence given to a large number of scales highlights a unique\ncharacteristic of this composer duo. We will discuss the\npossible causes and repercussions of these ﬁndings in the\nnext section.\n5. INTERPRETATIONS AND CONJECTURES\nBollywood is a melting pot of cultures and so its music\nincorporates numerous inﬂuences from musical traditions\nall over the world. The present study on the use of melodic\nscales can actually provide various interesting insights about\nthese cultural, and at times individual, inﬂuences on the\nmusic of Bollywood. Here we discuss some of our inter-\npretations of the observed trends in scale usage, and make\nsome conjectures.\nDecreasing Inﬂuence of ICM. The scale usage pattern\nof Bollywood differs signiﬁcantly from that of ICM. The\ngradually decreasing popularity of Kalyan and to some ex-\ntent Kaﬁ, both of which are extremely popular in ICM,\nclearly indicates that Bollywood is moving away from ICM.Scales S-J K-A RDB ARR PC\nAsavari 9 4 9 8 6\nBilawal 7 3 9 2 12\nKhamaj 2 3 6 5 2\nBhairavi 3 4 2 6\nKaﬁ 1 3 4\nKalyan 1 1\nKirwani 2 1 3 2\nNayaki 3 3 5\nOther 2 7 3 4 1\nTotal 23 25 38 30 32\nDistinct 6 11 10 10 6\nTable 4 . Scale usage of ﬁve inﬂuential composers: S-J –\nShankar-Jaikishan, K-A – Kalyanji-Anandji, RDB – R. D.\nBurman, ARR – A. R. Rahman, PC – Pritam Chakraborty.\nTotal and Distinct respectively refers to the total number\nof scales analyzed for the composer and number of distinct\nscales found in those compositions.\nOn the other hand, we note that the scale usage pattern of\nthe 1950s and 60s ( Khamaj ,Kaﬁ andKalyan were used\nequally and only slightly less than Bilawal ) follow a trend\nthat one would expect under a strong inﬂuence of ICM. In-\ndeed, music by earlier composers such as Naushad, were\nheavily inﬂuenced by ICM13. Nevertheless, it is important\nto note that ICM-based songs are still occasionally used in\nthe Bollywood movies, especially in period ﬁlms, and we\ncan expect this trend to continue.\nIncreasing Inﬂuence of Western Music. Since 1970s,\nWestern popular music has had a steady inﬂuence on Bol-\nlywood. R. D. Burman is known for extensive use of ideas\nfrom Jazz and Blues in his compositions. His introduction\nofNayaki – the Minor Hexatonic scale that is popularly\nused in Blues and other Western music – and subsequent\npopularity of this scale till the present day is a clear ev-\nidence of the increasing inﬂuence of the Western music.\nWhile Bilawal andAsavari are popular in ICM, they are\nprevalent in Western pop music as well. A deeper analysis\nof the recent songs composed in these scales shows that it\nis indeed the inﬂuence of the latter which has led to the\nrecurrent usage of these scales.\nInﬂuence of other Musical Cultures. Bhairavi or the\nPhrygian scale, though popular in ICM, is also prevalent in\nthe folk songs of the Middle East, some East European re-\ngions, and the Punjab and Kashmir regions of India. Most\nBollywood compositions in Bhairavi hardly bear any re-\nsemblance to the raga Bhairavi or its derivatives. R. D\nBurman’s famous songs of the 1970s and 80s in this scale\nrather reﬂect the inﬂuence of Middle Eastern folk songs.\nThe music of the Middle East has been and still continues\nto be a strong source of inspiration for Bollywood com-\nposers. R. D. Burman’s compositions in Kirwani or the\nHarmonic Minor scale, another popular scale of the region,\n13http://chandrakantha.com/raga raag/ﬁlm song raga.html lists a large\nnumber of Bollywood songs based on ragas. Most of these songs were\ncomposed between 1940s and 1970s.also show a distinct Middle Eastern inﬂuence. However,\nKirwani is also used in Russian folk songs and early usage\nof this scale in Bollywood, promoted by S-J, can be traced\nback to its Russian inspiration. We also notice that the\nrecent upsurge in the use of Bhairavi , especially in com-\npositions by Pritam, is not related to Middle Eastern folk\nmusic, but is inspired by Punjabi folk songs. Interestingly,\nwe observe several other Bhairavi -based songs composed\nby Pritam which do not have any apparent resemblance to\neither Punjabi or Middle Eastern folk songs. Nevertheless,\nthe melodic structure of the songs clearly reveals an under-\nlying Punjabi inspiration.\nInspiration Patterns for Composers. We observe that\nsome composers, such as R. D. Burman, use a scale ex-\ntensively within a short period of time for a large number\nof apparently (melodically) unrelated compositions. Such\ncomposers seem to be consciously or subconsciously guided\nby the same melodic inspiration, which results in composi-\ntions based on the same scale, even though the situational\ncontext of the song in the ﬁlm may not demand a speciﬁc\ngenre or regional character in the music. For instance, out\nof 16 songs in our dataset composed by R. D. Burman be-\ntween 1971 and 73, 13 are in Asavari (7),Bilawal (4) and\nNayaki (1) and all are clearly inﬂuenced by Blues, Rock\nand Western Pop. The rest are in Khamaj (2) and Bhairavi\n(1), and are based on South Indian and Punjabi folk songs.\nOn the other hand, between 1980 and 81, out of the 16\ncompositions by him, 7 are in Kirwani andBhairavi all re-\nﬂecting distinctive Middle Eastern inﬂuence. On the other\nhand, for A. R. Rahman we do not see any such temporal\nclustering of scales between 1992 and 2005, but between\n2006 and 2009, we observe a preference towards Asavari\n(5 out of 9). Surprisingly, these compositions have been in-\nspired by Middle Eastern music and Suﬁ music which also\nuse this scale, and not so much by Western pop, which is\nwhat Asavari is most commonly used for in Bollywood.\nDiversity of Scales. Finally, we note that the number\nof scales used during each decade in Bollywood has been\nmore or less constant throughout its history. To conﬁrm\nthis, we computed the entropy of the distribution of the\nscale usage for each decade, which turns out to be more or\nless constant. Thus, the commonly held opinion that “all\nBollywood songs sound pretty much the same these days”,\ni.e., the melodic diversity was greater previously than it is\ntoday, is not supported by our data. This diversity is an\neffect of multiple inﬂuences drawn from various cultures.\nOf course, the sources of inspiration change over time, and\nso do the scale usage and melodic patterns; but the scale\ndiversity remains high at any given point of time.\n6. CONCLUSION\nIn this work, we created a small dataset of 310 Bollywood\nsongs sampled from the yearly top lists and identiﬁed the\nmelodic scales of the songs. Through an analysis of this\ndataset, we identiﬁed and quantiﬁed various cultural in-\nﬂuences on Bollywood music and its composers over the\nyears. Even though 310 songs are not sufﬁcient for mak-\ning any strong claims or observing complex cultural inﬂu-ences, we believe that our analysis revealed some striking\nfacts about the scale usage patterns and has helped us to\nformulate interesting conjectures which can be statistically\nveriﬁed if one had more data.\nThere has been a similar study for rhythmic patterns\nand other features of compositions in Western popular mu-\nsic [12], but we do not know of any study that analyzes\nmelodic scales for a given genre. Our next step is to study\nscale usage in various other musical cultures around the\nworld as well as regional ﬁlm industries and parallel musi-\ncal traditions extant in India and extend the current dataset\nto include more Bollywood songs.\n7. ACKNOWLEDGMENTS\nWe would like to thank Ms. Nandini Kamath for helping\nus with identiﬁcation of the scales.\n8. REFERENCES\n[1] G. Anantharaman. Bollywood Melodies-A history of\nthe Hindi ﬁlm song . Penguin Books India, 2008.\n[2] Aison E. Arnold. Hindi ﬁlm git: On the history of in-\ndian popular music. PhD. Thesis completed at the Uni-\nversity of Illinois at Urbana-Champagne, 1991.\n[3] A. Behl and M. Choudhury. A corpus linguistic study\nof Bollywood song lyrics. In Proceedings of ICON .\n2011.\n[4] V . N Bhatkhande. Hindustani Sangit Paddhati . Sakhi\nPrakashan, 1990.\n[5] P. Chatterjee. When melody ruled the day. In Vasude-\nvan, editor, Frames of mind: Reﬂections on Indian cin-\nema. UBPSD.\n[6] T. Christensen, editor. The Cambridge History of West-\nern Music Theory . Cambridge University Press, 2001.\n[7] S. Gopal and S. (ed) Moorti. Global Bollywood: The\nTransnational Travels of Hindi Song and Dance . Uni-\nversity of Minnesota, 2008.\n[8] N. A. Jairazbhoy. The Rags of North Indian Mu-\nsic: Their Structure and Evolution . Popular Prakashan,\n1995.\n[9] P. Manuel. Cassette Culture: popular music and tech-\nnology in north India . University of Chicago Press,\n1993.\n[10] S. Marcus. Recycling Indian Film-Songs: Popular Mu-\nsic as a Source of Melodies for North Indian Folk Mu-\nsicians. Asian Music , 24(1), 1993.\n[11] A. Morcom. Hindi Film Songs and the Cinema. SOAS\nMusicology Series , 2007.\n[12] J. Serr ´a, A. Corral, M. Bogu ˜n´a, M. Haro, and J. L. Ar-\ncos. Measuring the evolution of contemporary Western\npopular music. Nature Scientiﬁc Reports , 2(521), 2012."
    },
    {
        "title": "SIARCT-CFP: Improving Precision and the Discovery of Inexact Musical Patterns in Point-Set Representations.",
        "author": [
            "Tom Collins",
            "Andreas Arzt",
            "Sebastian Flossmann",
            "Gerhard Widmer"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416622",
        "url": "https://doi.org/10.5281/zenodo.1416622",
        "ee": "https://zenodo.org/records/1416622/files/CollinsAFW13.pdf",
        "abstract": "The geometric approach to intra-opus pattern discovery (in which notes are represented as points in pitch-time space in order to discover repeated patterns within a piece of music) shows promise particularly for polyphonic music, but has attracted some criticism because: (1) the approach extends to a limited number of inexact repetition types only; (2) typically geometric pattern discovery algorithms have poor precision, returning many false positives. This paper describes and evaluates a solution to the inexactness problem where algorithms for pattern discovery and inexact pattern matching are integrated for the first time. Two complementary solutions are proposed and assessed for the precision problem, one involving categorisation (hence reduction) of output patterns, and the second involving a new algorithm that calculates the difference between consecutive point pairs, rather than all point pairs.",
        "zenodo_id": 1416622,
        "dblp_key": "conf/ismir/CollinsAFW13",
        "keywords": [
            "geometric approach",
            "intra-opus pattern discovery",
            "notes represented as points",
            "pitch-time space",
            "repeated patterns within a piece of music",
            "polyphonic music",
            "criticism",
            "inexact repetition types",
            "algorithmic precision",
            "pattern discovery and inexact pattern matching"
        ],
        "content": "SIARCT-CFP: IMPROVING PRECISION AND THE DISCOVERY OF\nINEXACT MUSICAL PATTERNS IN POINT-SET REPRESENTATIONS\nTom Collins, Andreas Arzt, Sebastian Flossmann, and Gerhard Widmer\nDepartment of Computational Perception, Johannes Kepler University Linz\nftom.collins ,andreas.arzt ,sebastian.flossmann ,gerhard.widmer g@jku.at\nABSTRACT\nThe geometric approach to intra-opus pattern discovery (in\nwhich notes are represented as points in pitch-time space\nin order to discover repeated patterns within a piece of mu-\nsic) shows promise particularly for polyphonic music, but\nhas attracted some criticism because: (1) the approach ex-\ntends to a limited number of inexact repetition types only;\n(2) typically geometric pattern discovery algorithms have\npoor precision, returning many false positives. This pa-\nper describes and evaluates a solution to the inexactness\nproblem where algorithms for pattern discovery and inex-\nact pattern matching are integrated for the ﬁrst time. Two\ncomplementary solutions are proposed and assessed for the\nprecision problem , one involving categorisation (hence re-\nduction) of output patterns, and the second involving a new\nalgorithm that calculates the difference between consecu-\ntive point pairs, rather than all point pairs.\n1. INTRODUCTION\nThe discovery of repeated patterns within a piece of music\nis an activity that manifests itself in a range of disciplines.\nIn music psychology, for example, listeners’ emotional re-\nsponses to a piece exhibit distinctive behaviour at the be-\nginning of repeated sections [11]. In music analysis, an\nawareness of the locations of motifs, themes, and sections,\nand their relation to one another, is a prerequisite for writ-\ning about the construction of a piece [3]. Last but not least,\nin music computing, algorithmic pattern discovery can be\nused to deﬁne compressed representations [13] (e.g., the\nnumeric pitch sequence 67, 68, 67, 69, 69, 66, 67, 66, 68,\n68 can be encoded as 67, 68, 67, 69, 69, and a translation\noperation “-1”) and can act as a guide for the algorithmic\ngeneration of new music [9]. In the interests of supporting\nthese multiple manifestations, it is important that the ﬁeld\nof music information retrieval continues to develop and re-\nﬁne algorithms for the discovery of repeated patterns, and\ncontinues to evaluate these against each other and human-\nannotated ground truths.\nThere are two main representations in use for discov-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.ering repeated patterns within a piece of music (hereafter\nintra-opus discovery [8]): (1) viewpoints [9] involve en-\ncoding multiple aspects of the music as strings of symbols\n(such as the numeric pitches mentioned above, or dura-\ntions, intervals between notes, etc.). This approach has\nbeen applied mainly to monophonic music; (2) the geomet-\nric approach [14] involves converting each note to a point\nin pitch-time space (see the pitch-time pairs in Figures 1A\nand B). Higher-dimensional spaces are also possible (e.g.,\nincluding dimensions for duration or staff number). The\ngeometric approach is well-suited to handling polyphonic\nmusic, where few attempts have been made to apply view-\npoints. This paper focuses on the geometric approach;\nspeciﬁcally, ontime and morphetic pitch number [14] (C4\n= C]4 = 60, D [4 = D \\4 = D ]4 = 61, E [4 = E4 = 62, etc.).\nBefore getting into more details of related work, it is\nhelpful to distinguish the terms pattern matching andpat-\ntern discovery . Typically in pattern matching, there is a\nshort musical query and a longer piece (or pieces) of music,\nand the aim is to match the query to more or less exact in-\nstances in the piece(s) [2,17]. In intra-opus pattern discov-\nery there is no query, just a single piece of music, and the\nrequirement to discover motifs, themes, and sections that\nare repeated within the piece [8, 14]. (One could say that\nthe purpose of a pattern discovery algorithm is to create\nanalytically interesting but hitherto unknown queries.) Pat-\ntern discovery and pattern matching have been discussed\nin the same papers [13], but nobody to our knowledge has\nintegrated discovery and inexact matching components in\none algorithm before. This full integration is one of the\ncontributions of the current work, and the other consists of\ntwo complementary methods for improving the precision\nof pattern discovery algorithms. The paper is organised\naround describing and evaluating components of a new al-\ngorithm called SIARCT-CFP, beginning at the end of the\nacronym with “FP” for ﬁngerprinting, then “C” for cate-\ngorisation, and ﬁnally SIARCT, which stands for Structure\nInduction Algorithm for rsuperdiagonals and Compact-\nness Trawler, which has been deﬁned before [5] and for\nwhich a Matlab implementation has been released.1\n2. THE INEXACTNESS PROBLEM\nIn reviewing the Structure Induction Algorithm (SIA) and\nother geometric pattern discovery algorithms (see [14] or\n[7] for details), Lartillot and Toiviainen noted that “this ge-\n1http://www.tomcollinsresearch.net1 2 3 4 5 6 7 800.20.40.60.8\nBar NumberMusical Similarity in [0, 1]  \nD!\"\n!\"Andante grazioso\n#$$$1\n!%&%%%%'%&%%%%'%%(%%'% %%%%'%%(\n%'\n)$$$% %(%%&%%%%(%%'%(%%&%%%%(%%'%%(%%'%%(%%'%%'%&\n#$$$1\n!%& %&%%%%'*%%%%% %&%%%%'*%%%%% %&%%%% %&%%%%%&%&%% %'% %(\n%'\n)$$$\nlegat o%%$%%%%%%%%%%%%%%%%%%$%%%%%%%%%%%%%%%%%%$%%%%%%%%%+%%%%%%%%%%%%%\n%%%%%%%%%%%%\" \"\n#$$$5\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%(*\n)$$$ %,\n%%*%,\n%%* %$,\n%% * %,\n%% * %-,%-,\n%% % %,\n%,%%% %$,%,\n%% %\n%,\n%%*P2\nFP\nP2 Top Staff\nFP Top Staff(0, 67)(1½, 68)\n(2, 67) (3, 69)\n(0, 67)(1, 67)\n(2, 68) (3, 69)(2½, 67) (2¾, 68)A\nBC\nPFigure 1 . (A) Bars 1-4 of the Theme from the ﬁrst movement of Piano Sonata no.11 in A major K331 by Wolfgang\nAmadeus Mozart (1756–1791). Labels give the ontime and morphetic pitch of the indicated note, and the box contains the\ntop-rated pattern output by SIARCT; (B) Bars 1-8 of Variation II from the same movement; (C) Symbolic musical similarity\nof the pattern in (A) to the passage in (B), for two algorithms applied separately to the full texture and top staff only.\nometrical strategy did not apply to melodic repetitions that\npresented rhythmic variations” [10, pp. 290-291]. To illus-\ntrate this problem we use a theme by Mozart, from “one\nof the most overanalyzed pieces in the history of music\ntheory” [15, p. 160]. We are not particularly interested in\nadding to discussions of the structure of the theme itself,\nrather in the relation of the theme to a subsequent vari-\nation. If the passage in Figure 1B were appended to the\npassage in Figure 1A and SIA applied to the single result-\ning point set, there would be little in the output to sug-\ngest that the ﬁrst two bars of Figure 1B contain a varia-\ntion on the bounded pattern Pin Figure 1A. The points\nf(0;67);(3;69);(6;66);(9;68),(12;65)gwould appear in\nthe same output maximal translatable pattern (MTP, [14]),\nas they occur under the same translation in Figure 1B, but\nintervening points in the bounded pattern do not.\nThe pattern matching algorithm P2 [17] struggles with\nrhythmic variation also: for a given pattern Pand a larger\npoint set D, it returns all vector-frequency pairs (w; m)\nsuch that m\u00151points of Poccur translated by win\nD. We implemented P2 and used it to match P(from\nFigure 1A) to partial occurrences in D(Figure 1B). A sum-\nmary of the output is plotted in Figure 1C, for both full-\ntexture versions of PandDand a restriction to the right\nhand only (dashed and solid lines respectively). The maxi-\nmal frequency Mfor pairs (w1; m1)i2f1;2;:::;sgcorrespond-\ning to each crotchet-note ontime in Dis plotted, normalised\nby the number of points in P, to give a measure of the sym-\nbolic musical similarity of PtoDover time. While there\nare local maxima in the grey lines at bars 1, 2, and 5 (in the\nsecond case because P2 is transposition-invariant and thereis a transposed pattern within P), in general they have a rel-\natively small range, reﬂecting P2’s struggle to distinguish\ngenuine rhythmic variation from less related material.\nSubsequent work on geometric pattern matching im-\nproves upon P2 in terms of capturing rhythmic variation,\nby representing durations as line segments [12, 17], by us-\ning the Hausdorff metric [16], or by converting to a tonal\nspace representation [1]. A recent ﬁngerprinting (FP) ap-\nproach [2] has the advantage of not relying on durational\ninformation, and has options for transposition, time-shift,\nand scale-factor invariance, as well as tolerance for the\namount by which the inter-onset interval of a pair of notes\nis permitted to differ, compared to a corresponding note\npair in the original. The output of FP is a time series\nS=St:t2T, where the set Tof successive time points\nmay or may not be uniformly spaced. The magnitude of\nSt, called the matching score , indicates the extent to which\nan occurrence of the query begins at time t. In the trans-\nposition-invariant version, calculation of the matching score\ntime series begins by creating ﬁngerprint tokens\n[yj\u0000yi; xj\u0000xi]; t; (1)\nfor locally constrained combinations of successive ontime-\npitch pairs (xi; yi);(xj; yj), in both a query pattern Pand\nthe larger point set D. The pair in brackets in (1) is the\nhash key, and t=xiis a time stamp. A scatter plot of\nthe time stamps of matching hash keys for PandDcan be\nused to identify regions of high similarity, which appear as\napproximately diagonal lines. The matching score is cal-\nculated by applying an afﬁne transformation to the scatter\nplot and binning (for details, see [2, 18]).An implementation of the FP algorithm was used to\nmatch exact/inexact occurrences of Pfrom Figure 1A to\nDin Figure 1B, and the results are plotted in Figure 1C as\nblack lines. It can be seen that FP outperforms P2 at distin-\nguishing the rhythmic variation in bars 1-2 of Figure 1B.\nThe use of locally constrained combinations of ontime-\npitch pairs, rather than one candidate translation vector ap-\nplied to all points in P, is what enables the FP algorithm to\nﬁnd a stronger match than P2.\nProgress has been made in geometric pattern match-\ningtechniques, but Lartillot and Toiviainen’s [10] criticism\nof the discovery approach still stands, as nobody to our\nknowledge has integrated an inexact matching technique\nwithin a pattern discovery approach. We do so now, ac-\ncording to the following steps, which deﬁne the “FP” part\nof SIARCT-CFP:\n1. Let P1; P2; : : : ; P Mbe the output of a pattern dis-\ncovery algorithm, each Pihaving at least one trans-\nlationally exact repetition (two occurrences) in D;\n2. For i= 1;2; : : : ; M , run the FP algorithm [2] on\nPiandD, returning time points tPi\n1; tPi\n2; : : : ; tPimat\nwhich there may be further exact/inexact occurrences\nofPi, according to whether the value at tPi\njis greater\nthan some similarity threshold c2[0;1).\nUnderlying this integration of pattern discovery and pat-\ntern matching is the following assumption, which we call\nthetranslationally exact once (TEO) hypothesis:\nIf a piece of music contains multiple inexact\noccurrences of a perceptually salient or ana-\nlytically interesting pattern, then for some ma-\njority subset of the pattern (i.e., a subset con-\ntaining at least half of the points), there ex-\nists at least one translationally exact repetition\n(i.e., at least two occurrences).\nIf the discovery algorithm outputs such a majority subset,\nthen the matching algorithm may be relied upon to output\nfurther exact/inexact occurrences of the pattern.\nAs a case study, the new algorithm SIARCT-CFP was\nrun on the Nocturne in E major op.62 no.2 by Fr ´ed´eric\nChopin (1810–1849).2This is a sensible choice of piece,\nas it contains multiple variations of the opening theme (c.f.\nFigures 2B and D for instance). Fourteen patterns were\noutput in total, one of which Qis bounded in Figure 2A,\nand occurs translated three times (bars 27–28, 58–59, and\n60–61). These occurrences are rated as very similar to Q,\nwith normalised matching scores close or equal to 1. The\ntime series output by the FP has mean .264 and standard\ndeviation .173, suggesting that the occurrence in Figure 2C\nis not distinguishable from other unrelated material. This\nmakes sense, as although the contour and rhythm of the\nmelody are as in Q, the pitch intervals are different (see ar-\nrows) and so is the accompaniment. We note, however, that\n2The ﬁrst part of the algorithm, SIAR, ran with parameter r= 1 .\nSecond, the compactness trawler (CT) ran with compactness thresh-\nolda= 4=5, cardinality threshold 10, and lexicographic region type\n[7]. Third, the categorising and ﬁngerprinting (CFP) ran with similarity\nthreshold c= 1=2.\n.889\n.653\n.264\n.458\n1.000\n.944\n.889!\n!Lento\n\"####(1)\nsostenuto$$%$&$$$$'$%$&\n' $$$$'\n(####)$$$$$$$$$$\n$$$$$$$$$$$$\n$$$$$*\n$$$$\n\"####(9)\n$$$$%$&$$$$$$$$$$'$%$&\n$$$$%$$$'\n! \"\n(####$$$\n$$$$$$$$$$\n$$$$$\n$$#\n# $$$$$\n$$$$$*\n$$$$\n\"####(17)\n)\n#$%$&$$$$*'# $)$#%$+\ncresc.$$$$', $$*$$\n(####\n(25)$$$ $*$$$*\n$$$$\n$$$$$\n$$$$$$$$$\n$$#\n#$$\n$$$$#\n$$*\n*$$$\n\"####\n#$%$$% $&$$$$$$$$$$$$$$$$$$'$% $*&\n$%&$\"\n(####$\n$$$$$$$$$$\n$$$$$\n$$$\n\"####\n##$*%$*&$*$$*$'*$*%\ncresc.$&\n'# $$#$$# $$'\n(####\n$*$$$$*\n*$$\n$*$$**$$$\n$$\n*$$#\n$$\n$$\n$$$$*\n$$$$$*\n\"####(58)\n#$% $&$$$$ ''\n(####dim.$*\n$*$*-\n$$$$# $$$$$\n$$$$# $$$$$$**\n$*.\n\"####\n##$%$*&$$$*$'$%$&\n'\ncresc.$$#$$, $$'\n(####\n$$$$$*$$$$$$*\n$$$$$$$$$$**\n$$$$$#\n$$\n$$\n$$#\n#$$$*\n$$$$$A\nB\nC\nD\nEQFigure 2 . Excerpts from the Nocturne in E major op.62\nno.2 by Chopin. Dashed lines in (A) bound a pattern Q\ndiscovered by SIARCT, which is used to match other inex-\nact occurrences, with degree of exactness indicated in the\nﬁgure by numbers in [0, 1]. Pedalling omitted for clarity.\nthe FP algorithm could be extended further to incorporate\ncontour (up, down, same), as well as other viewpoints [9],\nbecause of its use of locally constrained comparisons.\n3. THE PRECISION PROBLEM\n3.1 Categorisation by Pattern Matching\nNow that we have integrated some inexact pattern match-\ning techniques into our pattern discovery approach, it is\npossible to employ them for the purposes of categorisa-\ntion, based on the idea that P2 [17] or FP [2] can be used\nto compare two discovered patterns PiandPjin exactly\nthe same way as if Pi=Pwas a query and Pj=Dwas a\npoint set (or vice versa, as the measures are symmetric).\nThe second “C” in SIARCT-CFP stands for a categoris-\nation process, which will be described now. The purpose\nof categorisation is to reduce an overwhelming amount of\ninformation (e.g., output patterns) to a more manageable\nnumber of exemplars. Here categorisation does not meanclassifying patterns into an accepted/interesting category\nversus a rejected/uninteresting category; rather it means\ngrouping similar patterns and representing each group with\none exemplar pattern. Our motivation for categorising the\noutput of SIARCT is to improve its precision: while the\nprecision and recall of pattern discovery algorithms has\nbeen shown to beneﬁt from compactness trawling, the pre-\ncision is still quite poor [7]. For example, SIARCT out-\nputs 76 patterns when run on Chopin’s op.62 no.2, which\ncan be reduced to fourteen patterns by using the following\ncategorisation process:\n1. Let P1; P2; : : : ; P Mbe the output of a pattern dis-\ncovery algorithm, sorted descending by a rating of\nperceived pattern importance [6], or some other or-\ndering. Let J=f1;2; : : : ; Mgindex the patterns\nthat are uncategorised currently;\n2. For the most important uncategorised pattern, index\ni= min( J), calculate the maximum normalised mat-\nching scores s(Pi; Pj)for each j2J,j6=i;\n3. For each similarity score s(Pi; Pj)that is greater\nthan some speciﬁable similarity threshold c2[0;1),\nplace pattern Pjin the category for which Piis the\nexemplar, and remove jfromJ;\n4. Repeat steps 2 and 3 until either Jhas one element\nk, in which case deﬁne Pkto be an exemplar with\ncategory membership Pk, or otherwise Jis empty;\n5. For the purposes of algorithm evaluation, return only\nthe exemplars Pi(1); Pi(2); : : : ; P i(m).\nDepending on the choice of c,m\u001cM. The categorisa-\ntion process can be visualised with two similarity matrices\n(Figure 3). The matrix in Figure 3A contains the maxi-\nmum normalised matching scores for each pair of 76 out-\nput patterns for Chopin’s op.62 no.2, ordered as in step 1\nabove. The matrix in Figure 3B is a permutation of 3A,\nshowing the categorised patterns ( c=:5) in their four-\nteen categories, bounded by white squares. The fourth\nsquare from top-left in Figure 3B represents the category\nfor which Qin Figure 2A is the exemplar. The ﬁvefold\n(5:43\u001976=14) reduction in output achieved by pattern-\nmatching categorisation may well improve precision: as\ndiscussed, the theme annotated in Figure 2A survives the\ncategorisation process, and so do all of the repetitions in\nthis piece lasting four or more bars (results not shown).\nPattern-matching categorisation also constitutes a novel and\ninteresting use of the FP algorithm [2]. It should be noted\nthat choosing too low a value for ccould lead to over-\nreduction and ﬁltering out of analytically interesting pat-\nterns. For instance, the ﬁrst two squares in Figure 3B show\nconsiderable variegation, suggesting that some interesting\nsubcategories may be overlooked.\n3.2 Consecutive Points and Conjugate Patterns\nThe ﬁnal novel contribution of this paper is to evaluate the\nSIARCT pattern discovery algorithm [5] against a collec-\ntion of music containing repeated sections, and to com-\nDiscovered Pattern Permuted IndexDiscovered Pattern Permuted Index\n10 20 30 40 50 60 7010\n20\n30\n40\n50\n60\n70\nDiscovered Pattern IndexDiscovered Pattern Index\n10 20 30 40 50 60 7010\n20\n30\n40\n50\n60\n70A\nBFigure 3 . (A) Pairwise symbolic musical similarities\n(ranging from white for dissimilar to black for identical)\nfor 76 patterns discovered by SIARCT in Chopin’s op.62\nno.2, ordered by a rating formula for perceived salience;\n(B) Permutation of the above matrix, with white lines indi-\ncating the results of categorising into fourteen groups.\npare its performance (especially precision) to SIA [14] and\nSIAR [5]. SIA outputs thousands of patterns for Chopin’s\nop.62 no.2 (and other pieces of music [7]), so it is nec-\nessary to develop a more parsimonious pattern discovery\nalgorithm for use as input to the categorisation and ﬁnger-\nprinting components described above (e.g., SIARCT out-\nputs only 76 patterns for Chopin’s op.62 no.2).\nIt has long been thought that in order to discover re-\npeated patterns within a geometric representation Dof a\npiece, it is necessary to calculate the difference between\neach pair of npoints ( n[n\u00001]=2calculations in total), as in\nSIA [14]. Unlike SIA, the ﬁrst step of SIARCT is to calcu-\nlate the difference between consecutive pairs of points only\n(n\u00001calculations). Some exhaustive pairwise compar-\nisons are still made in the second step, but for small, non-\noverlapping subsets of D, meaning that the total number\nof difference calculations performed by SIARCT is far less\nthann[n\u00001]=2, in all but one degenerate case.3The third\nstep of SIARCT makes use of a concept known as conju-\ngate patterns [5]: if a pattern containing lpoints occurs m\ntimes in a point set, then there exists in the same point set\na pattern consisting of mpoints that occurs ltimes. The\nfourth step calculates MTPs for each vector in a list L. As\na consequence of manipulating conjugate patterns, the vec-\ntors corresponding to repeated sections should be at or near\n3Please see [5] for the algorithmic details.the top of L. So for this step we could: (1) distribute each\nMTP calculation to parallel processors, and/or; (2) output\nMTPs dynamically for the user to browse, whilst calcula-\ntion of the remaining MTPs continues. The main claim is\nthat SIARCT will have much smaller output than SIA, with\nminimal or no negative impact on its performance as mea-\nsured by precision, recall, and robust versions of these met-\nrics [4]. The compactness trawler (CT) part of SIARCT is\nexactly as in [7], so is not addressed again here.\nSIA, SIAR, and SIARCT were run on point-set repre-\nsentations of movements by Ludwig van Beethoven (1770–\n1827) and Chopin listed in Figure 4A. SIARCT ran with\ncompactness threshold a= 1, and points threshold b= 50 .\nThis means that only patterns containing 50 points or more\nwere returned, and they had to have maximal compactness\nof 1. The parameter values make sense in terms of try-\ning to discover repeated sections. To make the evaluation\nfair, we also ﬁltered the results of SIA and SIAR, returning\nonly those patterns that contained 50 points or more. In the\nresults, these versions of SIA and SIAR are referred to as\nSIA (50+) and SIAR (50+).\n3.3 Evaluation Results\nFigure 4B shows the log of the total number of patterns\noutput by each algorithm for each movement/piece. It sup-\nports the claim that SIAR has a much smaller output than\nSIA. It is difﬁcult to see from Figure 4B, but the same ob-\nservation applies to the ﬁltered versions of each algorithm,\nSIAR (50+) and SIA (50+). The number of patterns output\nby SIARCT is several orders of magnitude less than that\nof any other algorithm. Figure 4C and Figure 4E show that\ncompared with SIA’s performance, SIAR is not negatively\nimpacted by restricting calculations to consecutive pairs of\npoints. The establishment precision and establishment re-\ncall for SIAR and SIA are comparable across all pieces.\nOverall, the most effective algorithm is SIARCT (see\nFigure 4C and Figure 4E). For half of the pieces, it dis-\ncovers all ground truth patterns exactly (Figure 4F). When\nSIARCT fails to discover a ground truth pattern exactly, of-\nten this is due to a difference between the repeated section\nas written in the score, and the repeated pattern as heard\nin a performance. For instance, in the fourth movement\nof Beethoven’s op.7, bars 65–70 are marked as a repeated\nsection, and this is included in the ground truth. The re-\npeated notes extend beyond these bars in both directions,\nhowever, creating a longer repeated pattern in a perfor-\nmance. SIARCT discovers the latter, performed pattern,\nwhich reduces exact precision and recall. The more ro-\nbust establishment metrics are not much reduced (e.g., see\nFigure 4E), and arguably discovering the performed pat-\ntern is preferable from a music-perceptual point of view.\n4. DISCUSSION AND FUTURE WORK\nThis paper identiﬁes two valid reasons why the geomet-\nric approach to intra-opus pattern discovery has attracted\nsome criticism—namely (1) the approach extends to a lim-\nited number of inexact repetition types only, and (2) typ-ically geometric pattern discovery algorithms are impre-\ncise, returning many false positives results. A new algo-\nrithm called SIARCT-CFP has been described and evalu-\nated component-wise, in an attempt to address these crit-\nicisms. It is the ﬁrst geometric pattern discovery algo-\nrithm to fully integrate an inexact pattern matching compo-\nnent (the ﬁngerprinting algorithm of [2]), and this match-\ning component was shown to be effective for retrieving\ninexact occurrences of themes in pieces by Mozart and\nChopin. The comparison of the FP algorithm [2] to a base-\nline pattern matching algorithm P2 [17] demonstrated that\nthe former was superior for a particular example. In gen-\neral it may be preferable to have two or more pattern match-\ners at one’s disposal, however, as the number of variation\ntechniques is large, and trying to account for them all with\none algorithm will likely produce false positive matches.\nThe precision metrics were of particular interest to us\nin the comparative evaluation of SIARCT [5], SIAR [5],\nand SIA [14], as we claimed that SIARCT could achieve\nlevels of precision comparable to SIA and SIAR, without\nharming recall. This claim was supported by the evaluation\nresults, although in future work it will be necessary to see\nif similar results are achieved for ground truths containing\nshorter patterns than repeated sections.\nOur translationally exact once (TEO) hypothesis (see\nSection 2) was borne out in the case study of Chopin’s\nop.62 no.2, where Q(Figure 2A) occurred exactly under\ntranslation (bars 27–28, 58–59, and 60–61), and its con-\ntents were sufﬁcient for use as a query to retrieve less exact\nversions such as in bars 9 (Figure 2B) and 25 (Figure 2D).\nFor the case study of the Theme section and Variation II\nfrom Mozart’s K331, SIARCT was able to discover per-\nceptually salient patterns such as Pin Figure 1A, which\nrecurs in bars 5-7 of the Theme section (not shown). As the\nTEO hypothesis holds in both cases, future work should\nfocus on ﬁnding counterexample pieces, as this will help\nto reﬁne and improve our underlying assumptions and en-\nsuing algorithms. Future work will also attempt to show\nusers/developers the differences between themes and par-\ntial matches, and to identify variation techniques (triplets,\nminore, etc.) automatically.\n5. ACKNOWLEDGEMENTS\nThis paper beneﬁted from the use of Kern Scores, and help-\nful discussions with David Meredith. We would like to\nthank four anonymous reviewers for their comments. This\nwork is supported by the Austrian Science Fund (FWF),\ngrants Z159 and TRP 109.\n6. REFERENCES\n[1] T. Ahonen, K. Lemstr ¨om, and S. Linkola:\n“Compression-based similarity measures in symbolic,\npolyphonic music,” Proceedings of the Interna-\ntional Symposium on Music Information Retrieval ,\npp. 91–96, 2011.\n[2] A. Arzt, S. B ¨ock, and G. Widmer: “Fast identiﬁcation\nof piece and score position via symbolic ﬁngerprint-00.20.40.60.81Recall00.050.10.150.20.250.3Precision\n  00.20.40.60.81Establishment Recall  \n00.20.40.60.81\nMovement or PieceEstablishment Precision\n  \nMovement or PieceA C E\nB D F2 4 6 8 10 12 2 4 6 8 10 12\nMovement or Piece2 4 6 8 10 12\nMovement or Piece2 4 6 8 10 121. Beethoven op.2 no.1 mvt.1\n2. Beethoven op.2 no.1 mvt.3\n3. Beethoven op.2 no.2 mvt.3\n4. Beethoven op.2 no.2 mvt.4\n5. Beethoven op.2 no.3 mvt.1\n6. Beethoven op.7 mvt.1\n7. Beethoven op.10 no.1 mvt.1\n8. Beethoven op.10 no.1 mvt.3\n9. Chopin op.6 no.2\n10. Chopin op.6 no.3\n11. Chopin op.7 no.1\n12. Chopin op.7 no.4\n2468101214\nMovement or PieceLog Total No. Patterns Returned\n  \nSIASIA (50+)SIARSIAR (50+)SIARCT2 4 6 8 10 12SIA (50+)\nSIAR (50+)\nSIARCT\nSIA (50+)\nSIAR (50+)\nSIARCT\nSIA (50+)\nSIAR (50+)\nSIARCTFigure 4 . Evaluation metrics for three algorithms, run on eight movements by Beethoven and four pieces by Chopin.\ning,” Proceedings of the International Symposium on\nMusic Information Retrieval , pp. 433–438, 2012.\n[3] I. Bent and A. Pople: “Analysis,” The new Grove dic-\ntionary of music and musicians , Macmillan, London,\n2001.\n[4] T. Collins: “Discovery of repeated themes\nand sections,” Retrieved 4th May 2013, from\nhttp://www.music-ir.org/mirex/wiki/2013:\nDiscovery ofRepeated Themes %26Sections\n[5] T. Collins: Improved methods for pattern discovery in\nmusic, with applications in automated stylistic compo-\nsition , PhD thesis, Faculty of Mathematics, Computing\nand Technology, The Open University, 2011.\n[6] T. Collins, R. Laney, A. Willis, and P. Garthwaite:\n“Modeling pattern importance in Chopin’s mazurkas,”\nMusic Perception , V ol. 28, No. 4, pp. 387–414, 2011.\n[7] T. Collins, J. Thurlow, R. Laney, A. Willis, and\nP. Garthwaite: “A Comparative Evaluation of Al-\ngorithms for Discovering Translational Patterns in\nBaroque Keyboard Works,” Proceedings of the Inter-\nnational Symposium on Music Information Retrieval ,\npp. 3–8, 2010.\n[8] D. Conklin and C. Anagnostopoulou: “Representation\nand discovery of multiple viewpoint patterns,” Pro-\nceedings of the International Computer Music Confer-\nence, pp. 479–485, 2001.\n[9] D. Conklin and I. Witten: “Multiple viewpoint systems\nfor music prediction,” Journal of New Music Research ,\nV ol. 24, No. 1, pp. 51-73, 1995.\n[10] O. Lartillot and P. Toiviainen: “Motivic matching\nstrategies for automated pattern extraction,” Musicae\nScientiae , Discussion Forum 4A, pp. 281–314, 2007.[11] S. Livingstone, C. Palmer, and E. Schubert: “Emo-\ntional response to musical repetition,” Emotion ,\nV ol. 12, No. 3, pp. 552–567, 2012.\n[12] A. Lubiw and L. Tanur: “Pattern matching in poly-\nphonic music as a weighted geometric translation prob-\nlem,” Proceedings of the International Symposium on\nMusic Information Retrieval , pp. 154–161, 2004.\n[13] D. Meredith: “Point-set algorithms for pattern discov-\nery and pattern matching in music,” Proceedings of the\nDagstuhl Seminar on Content-Based Retrieval , 2006.\n[14] D. Meredith, K. Lemstr ¨om, and G. Wiggins: “Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music,” Journal\nof New Music Research , V ol. 31, No. 4, pp. 321–345,\n2002.\n[15] E. Narmour: “Some major theoretical problems con-\ncerning the concept of hierarchy in the analysis of tonal\nmusic,” Music Perception , V ol. 1, No. 1, pp. 129–199,\n1983.\n[16] C. Romming and E. Selfridge-Field: “Algorithms for\npolyphonic music retrieval: the Hausdorff metric and\ngeometric hashing,” Proceedings of the International\nSymposium on Music Information Retrieval , pp. 457–\n462, 2007.\n[17] E. Ukkonen, K. Lemstr ¨om, and V . M ¨akinen: “Geomet-\nric algorithms for transposition invariant content-based\nmusic retrieval,” Proceedings of the International Sym-\nposium on Music Information Retrieval , pp. 193–199,\n2003.\n[18] A. Wang: “An industrial strength audio search algo-\nrithm,” Proceedings of the International Symposium on\nMusic Information Retrieval , pp. 7–13, 2003."
    },
    {
        "title": "Influences of ISMIR and MIREX Research on Technology Patents.",
        "author": [
            "Sally Jo Cunningham",
            "Jin Ha Lee 0001"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417303",
        "url": "https://doi.org/10.5281/zenodo.1417303",
        "ee": "https://zenodo.org/records/1417303/files/CunninghamL13.pdf",
        "abstract": "Much of the current Music Information Retrieval (MIR) research aims to contribute to the field by creating practical music applications or algorithms that can be used as part of such applications. Understanding how academic research results influence and translate to commercial products can be useful for MIR researchers, especially when we try to measure the impact of our research. This study aims to improve our understanding of the commercial influence of academic MIR research by analyzing the patents citing publications from ISMIR (International Society for Music Information Retrieval) Conference proceedings and its associated MIREX (Music Information Retrieval Evaluation eXchange) MIR algorithm trials. In this paper, we provide our preliminary analyses of the relevant patents as well as the ISMIR publications that are referenced in those patents.",
        "zenodo_id": 1417303,
        "dblp_key": "conf/ismir/CunninghamL13",
        "keywords": [
            "Music Information Retrieval",
            "academic research",
            "commercial influence",
            "patents",
            "ISMIR Conference proceedings",
            "MIREX MIR algorithm trials",
            "understanding",
            "practical music applications",
            "impact measurement",
            "academic MIR research"
        ],
        "content": "INFLUENCES OF ISMIR AND MIREX RESEARCH  \nON TECHNOLOGY PATENTS  \nSally Jo Cunningham  Jin Ha Lee  \nDepartment of Computer Science  \nUniversity of  Waikato  \nsallyjo@waikato.ac.nz  Information School  \nUniversity of Washington  \njinhalee@uw.edu  \nABSTRACT  \nMuch of the current Music Information Retrieval (MIR) \nresearch aims to contribute to the field by creating pract i-\ncal music applications or algorithms that can be used as \npart of such applications. Understanding how academ ic \nresearch results influence and translate to commercial \nproducts can be useful for MIR researchers, especially \nwhen we try to measure the impact of our research. This \nstudy aims to improve our understanding of the comme r-\ncial influence of academic MIR res earch by analyzing the \npatents citing publications from ISMIR (International S o-\nciety for Music Information Retrieval) Conference pr o-\nceedings  and its associated MIREX (Music Information \nRetrieval Evaluation eXchange) MIR algorithm trials . In \nthis paper, we provide our preliminary analyse s of the \nrelevant patents as well as the ISMIR publications that \nare referenced in those patents.  \n1. INTRODUCTION  \nThe ISMIR (International Society for Music Information \nRetrieval) conference started as a small -scale symposium \nin 2000  and continued to grow over the past decade as the \nfield of Music Information Retrieval (MIR) matured. \nISMIR has been one of the most important MIR confe r-\nences  since the early establishment of the field, and \nserve s as a key venue for dissemination of MIR r esearch. \nThe associated MIREX (Music Information R etrieval \nEvaluation eXchange) event, first run in 2005, has simi-\nlarly grown to be  a focus for MIR system and a lgorithm \nevaluation.  One of the key objectives of MIR research is \nto make practica l contrib ution s toward the develop ment \nof commercial music applications and services to i m-\nprove users’ interaction and experience with music. There \nis anecdotal evidence that our research results inform the \ndevelopment of new music applications and services, e s-\npecially since ISMIR conferences and MIREX trials have \ncontinued to attract participants from the commercial se c-\ntor. However, to date no research has been conducted to \nsystematically investigate the extent of the  practical im-\npact of academic MIR research  published through the ISMIR conferences and MIREX trials .  \nTo this end, we first identify  patents that reference \npublications from ISMIR and MIREX (Section 3). We \nthen perform an informetric analysis over these patents \nand the referenced publications drawn from ISMIR and \nMIREX, to discover  patterns of influence of ISMI R and \nMIREX on patented MIR tec hnology  (Section 4).  \n2. PREVIOUS WORK  \nPrevious investigations of the characteri stics of MIR r e-\nsearch have focused exclusively on the field as viewed \nthrough academic publication. The r esearch methods \nused were prim arily bibliometric —that is, quantitative \nmeasures such as citation analysis, based on data drawn \nfrom the metadata and text of the ISMIR and MIREX \nproceedings. These techniques have been used to paint \nrich pictures of the state of ISMIR acade mic research  at \nvarious stages in the history of ISMIR and MIREX  [1], \n[2], [3], with the emphasis  on scholarly  publishing.  \nThis present paper applies these bibliometric tec h-\nniques to a set of patent filings rather than academic p a-\npers. ‘Patent bibliometrics ,’ the natural extension of bi b-\nliometric techniques to collections of patent metadata \nand texts, has seen widespread use in technology -related \nfields since its introduction in 1994 [8]. The i ntroduction \nof online, free -to-search patent databases has further e n-\ncouraged patent bibliometric investigation s [6]; the most \ncomprehensive and widely used databases are provided \nby the Unites States P atent and Trademark Office \n(USPTO) and the European Patent Organisation  (EPO).  \nOne natural topic of interest has been the re lationship \nbetween academic publications and patents in a given \nfield. Data regarding this relationship should be straigh t-\nforward to draw from a patent database, as each patent \nfiling includes the equivalent of the bibliographic cit a-\ntions in the form of re ferences to prior art, and the prior \nart can include both earlier patents and relevant confe r-\nence and journal papers.  \nUnfortunately, patent databases index only the ‘front \npage’ prior art citations, and these are almost exclusively \nlimited to patents. References to prior art in the form of \nrelevant academic publications are typically found in the \nbody of the patent —which is not indexed by the data-\nbases —and, though they may appear on the patent’s \nfront page as “Other Publications ,” they are not indexed \nin the USPTO and EPO patent search engines [7].    \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2013  International Society for Music Information Retrieval    \n \nGiven these hurdles to identifying linkages between \npatents and academic research directly from the patent \ndatabases, researchers have had to draw in  evidence from \nadditional sources, or to use crude proxy measures for \nthe linkages. For example, a study of academics in No r-\nway compared the publishing behavior of matched sets \nof academic patent inventors and non -inventors —but \nsince the databases often did not include inventor a ffilia-\ntions, that had to be determined through expensive, error -\nprone, and time -consuming surveys of the instit utions \nthemselves [5]. Meyer [6] estimates the interactions b e-\ntween a country’s academic and patented outputs \nthrough the coarse mechanism of comparing patent and \npublication rates for that country in a small number of \nnarrowly focused fields.  \nThe introduction of search over the full text of a p a-\ntent by Google Patent Search (GPS) 1 supports patent \nbibliometric investigations to a far greater degree than \nhas been possible previously, in particular supporting \ncitation analysis of references to published scientific li t-\nerature in patents. To our knowledge, this present paper \nis the first to use this  facility to directly explore the i n-\nfluence of academic research on the patents.  \n3. DATA COLLECTION  \nWe used Google Patent Search in order to find all the \npatents containing  references  to ISMIR and MIREX. \nGoogle Patent Search  is a specialized Google search \nengine that indexes patents  and patent applications  drawn \nfrom public domain patent databases of the USPTO  and \nthe EPO . While both the USPTO and EPO offer search \nfacilities for their individual collections, the Google \nPatent Search presents a unified interface to both \ndatabases. More significantly for this present \ninvestigation, Google Patent Search indexes the entire \npatent for search, where the USPTO and EPO supp ort \nsearch only over the basic patent metadata (title, \nclassification code, publication date, inventor, etc.). \nThrough GPS, we can link back to the scientific literature \nsupporting a patent, via the references cited in the textual \ndescription of the backgr ound to the invention and of the \ninvention itself.  \n Searches for the initial patent datasets were conducted \nover Google Patent Search in April 2013 , using the terms \n‘ISMIR music’ and ‘MIREX music’.  Where multiple \npatent filings under the same inventor nam e and title were \nfound, we retained the earliest filing, and the granted \npatent record over the application.  As a consequence of \npreferring the record with the earlier filing date , in most \ncases the US patent record was retained rather than \nEuropean  (a common pattern in technology patents is to \nfile first in the US ). As a result, we found a total of 141 \npatents citing ISMIR papers , and 13 patents citing \nMIREX .  \n                                                           \n1 https://www.google.com/patents   For each of these patents, we identified:  the year of fi l-\ning; whether this patent instance was an application or \nhad be en granted; if granted, the year; the inventor(s) and \nassignee(s); and the number of references to ISMIR and \nMIREX in each patent .  For each reference to ISMIR or \nMIREX, we collected basic bibliographic data  (title, a u-\nthors, and year of publication).  \n4. DATA AND DISCUSSION  \nThis section presents an analysis of both sets of patents —\nthose referencing ISMIR publications, and those referen c-\ning MIREX . Given the disparity in the size of these two \ndatasets (141 and 13,  respectively), we present the results \nseparately.  \n4.1 Analysis of the Patents  \nIn this section, we summarize the broad characteristics of \nthe 141 patent documents that reference ISMIR public a-\ntions : distribution of patents by year of a pplication, \nunique inventors and assignees, ISMIR refe rences inclu d-\ned in the patents, and patent topics.  \n4.1.1 Year of Application  \nOf the 141 patents identified as referencing ISMIR publ i-\ncations, 102 h ave been issued as of  the date of our dataset \ngathering  (April 2013 ), and 39 exist as applications.  Fig-\nure 1 shows the number of patents by application year.  \n \nFigure 1. Number of patents by year of applic a-\ntion \nExamining the dates of application, we see a peak of \npatent applications referencing ISMIR publications in \n2007, with a sharp drop -off in 2011 and 2012. The pre s-\nence of only a single ISMI R-referencing patent filing in \n2012 may be partly explained by a time delay in updates \nto the Google Patent Search and the underlying USPTO \nand EPO databases; according to the USPTO website, \nmost patent applications filed on or after November 29, \n2000, are published 18 months after the filing date of the \napplication2. Also the EPO website states that the patent \napplication is published 18 months after the date of filing \n                                                           \n2 http://www.uspto.gov/faq/patents.jsp  3 3 2 10 19 22 24 \n21 \n16 \n13 \n7 \n1 \n051015202530\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012Number of patents  \nYear of application    \n \nor the priority date1. Still, the steady decline remains; this \nmay be due to the fact th at technologies in some areas are \nsaturated and new developments are appearing in a slo w-\ner pace than they used to in the earlier stage of the MIR \nfield. We consider possible explanations for this trend as \nwe examine the characteristics of the ISMIR referen ces \nthemselves (Section 4.2).  \n4.1.2 Anal ysis of Inventors and Patent Distribution  \nThere are 241 unique inventors in the ISMIR dataset, \nwith an average number of 1.39 patents per inventor . All \ninventors are natural persons (US patents were preferred \nin selecting  between multiple filings, and under US law \ncorporate entities cannot be registered as inventors). The \ndistribution of patents over inventors (Figure 2) indicates \na strong skew towards single -filing inventors; here, a p-\nproximately 80% of inventors are assoc iated with one fi l-\ning. We found this a bit  surprising as one would assume \nthat typically a team  of researchers  is involved in deve l-\noping new alg orithms/ technologies  (as evidenced by a \ngrowing trend toward co-authorship  in ISMIR  [1]).  \n \nFigure 2. Number of patents sorted by number of \ninventors  \nThe four inventors associated with the largest number \nof patents are presented in Table 1. Of the four, Mas ataka \nGoto and Brian A. Whitman have published in ISMIR \nproceedings.  \nNo. of patents  Inventors  \n9 Louis B. Rosenberg  \n6 Thomas Kemp  \n5 Masataka Goto  \nBrian A. Whitman  \nTable 1. Inventors with the largest numbers of p a-\ntents referencing ISMIR  \n4.1.3 Analysis of a ssignees and Patent Distribution  \nLooking at assignees, there are 120 unique assignees, \nwith an average of 1.18 patents per assignee. Of the 120 \nassignees, 21 are individuals, and 99 are corporate ent i-\n                                                           \n1 http://www.epo.org/applying/basics.html  ties (primarily commercial organizations and univers i-\nties).  \nFigure 3 shows the number of patents per  assignee , \nwhich ranged from 1 to 12 ; there were 120 unique a s-\nsignees in total . Most of the assignees  were associated \nwith a single  patent  (58.3%). Looking at the top 10 p a-\ntent-holding assignees (Table 2), we see  a mix of  large IT \ncorporat ions (Google, Apple, M icrosoft, Yahoo!), two \nelectronics corporations (Philips, Sony), a music  and vi d-\neo metadata specialist company (Gracenote), and two or-\nganizations specializing in patent acquisitions  (Colwood \nTechnology, Outland Research).  \n \nFigure 3. Number of assignees per patent refe r-\nencing ISMIR  \n \nNo. of patents  Assignees  \n12 Google Inc.  \n11 Apple Inc.  \n9 Colwood Technol ogy, Llc  \n9 Outland Research, Llc  \n8 Strands, Inc.  \n7 Microsoft Corp.  \n6 Gracenote, Inc.  \n6 Koniklijke Philips Ele c-\ntronics N.V.  \n6 Sony Cor p. \n6 Yahoo! Inc.  \nTable 2. Top 10 assignees by number of patents  \n4.1.4 Number of ISMIR References per Patent  \nThere were a total of 213 references to ISMIR public a-\ntions in the patents we analyzed. The average number of \nISMIR references per patent was 1.5.  Unfortunately we \ncannot estimate the proportion of ISMIR references to all \nreferences included in the patents; the lack of standard i-\nzation in patent descriptions and background formats, \ncompounded by errors in the Google process for identif y-\ning patent metadata (including references and patent cit a-\ntions) preclude this type of analysis.   192 \n27 9 9 2 1 0 0 1 \n050100150200250\n1 2 3 4 5 6 7 8 9Number of inventors  \nNumber of patents associated with inventor  70 \n27 \n11 \n0 2 4 1 1 2 0 1 1 \n01020304050607080\n1 2 3 4 5 6 7 8 910 11 12Number of patents  \nNumber of assignees    \n \n \nFigure 4. Number of ISMIR reference per patent  \n4.1.5 Topics of the Patents  \nIn order to get an overview of the patent topics, we ma n-\nually identified and categorized the main topics re pre-\nsented by each patent. Table 3 shows the top 10 most \ncommon topics represented in the patents analyzed.  \nTopics  Number of \npatent s \nAudio fingerprinting  14 \nIdentification of similar \nsongs  12 \nMusic recommendation  9 \nAutomatic playlist  \ngeneration  9 \nAudio/Music analysis  6 \nMusic player/interface  6 \nMusic search and display  5 \nMusic visualization  5 \nMusic classification  5 \nMusic retrieval methods  4 \nTable 3. Top 10 patent topics  \nThe most common topic was audio fingerprinting; 14 \npatents dealt with various technologies related to audio \nfingerprinting or thumbprinting. This is followed by 12 \npatents about using audio similarity algorithms to identify \nsongs similar to a sample song from a music collection. \nMusic recommendation and automatic playlist generation \nwere also popular topics. This is probably related to the \nincrease in the popularity of streaming services and \nemergence of new types of services such as music ident i-\nfication as all of these technologies are commonly used in \npopular music services [1], for instance, music streaming \nservices such as Pandora and Spotify, and music identif i-\ncation services such as Shazam or Soundhound. Topics \nsuch as music analysis, search, display, classifica tion, and \nretrieval methods that are important components of music \ndigital libraries/applications also appeared multiple times \nin the patents. Different techniques for music display and \nvisualization were also found multiple times. Some e x-\namples of other t opics that appeared two times include: \nmusic metadata, composition, audio encoding/decoding, associating music and geographic information, and social \nratings .   \n4.2 Analysis of ISMIR Publications Cited in Patents  \nIn this section, we examine the ISMIR publ ications cited \nin the patents: specifically, the distribution of citations \nfrom the ISMIR conference series, the ISMIR public a-\ntions most frequently referenced in the patent dataset, and \nthe overlap between ISMIR authors and inventors . \n4.2.1 Year of ISMIR Public ations  \nFigure 5  shows the year of publication of ISMIR papers \nreferenced in the patent dataset. The most striking aspect \nof this figure is the peak at 2002; just over a (34.9%) of \nthe ISMIR papers referenced were drawn from the 2002 \nISMIR conference procee dings. This may be due to the \nfact that  topics related to  content -based retrieval  (e.g., au-\ndio music similarity , automatic generation of playlists) \nstarted to become quite popular around that time.  \n \nFigure 5. Number of patents citing ISMIR publ i-\ncations , by publication year of ISMIR paper  \n4.2.2 Most Highly Cited ISMIR Publications in Patents  \nTable 4 lists the top 10 most highly cited ISMIR publ ica-\ntions in the patents we analyzed, sorted by the number of \ntimes cited. These publications date primarily from the \nearly years of the ISMIR conference series ; this  skew  is \nto be expected, given that the older publications have \nmore time to accumulate citati ons and given the distrib u-\ntion of ISMIR -referencing patent filings (Figure 1).  \nAuthor s \n(pub. year)  Title  Freq  \nHaitsma  J, \nKalker T  \n(2003)  A highly robust audio finge r-\nprinting system  21 \nCano  P,  \nKaltenbrunner \nM, Gouyon F, \nBatlle E  \n(2002)  On the use of FastMap for au-\ndio retrieval and browsing  10 \nLogan  B \n(2002)  Content -based playlist genera-\ntion: exploratory  \nexperiments  9 106 \n15 14 \n1 0 4 \n020406080100120\n1 2 3 4 5 6Number of patents  \nNumber of references to ISMIR papers  \n15 13 74 \n18 27 28 \n11 14 \n4 7 \n0 1 0 \n01020304050607080\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012Number of patents citing  \nISMIR papers  \nPublication year of ISMIR paper    \n \nMcKinney  MF, \nMoeland D  \n(2004)  Extracting the perceptual tem-\npo from music 9 \nPauws  S,  \nEggen B  \n(2002)  PATS: realization and user \nevaluation of an automatic \nplaylist generator  9 \nAucouturier  J-\nJ, Pachet F  \n(2002)  Music similarity measures: \nWhat's the use?  7 \nPampalk  E, \nFlexer A, \nWidmer G  \n(2005)  Improvements of audio -based \nmusic similarity and genre \nclassification  5 \nLogan  B \n(2000)  Mel frequency cepstral coeff i-\ncients for music modeling  5 \nBerenzweig  A, \nLogan B,  \nEllis DPW, \nWhitman B  \n(2004)  A large-scale evaluation of \nacoustic and subjective music \nsimilarity measures  4 \nLiu D, Lu L  \n(2003)  Automatic mood detection \nfrom acoustic music data  4 \nTzanetakis  G, \nEssl G, Cook P  \n(2002)  Automatic musical genre cla s-\nsification of audio signals  4 \nWest K, Cox S  \n(2005)  Finding an optimal segment a-\ntion for audio genre classific a-\ntion 4 \nPaulus J, \nKlapuri A  \n(2002)  Measuring the  similarity of \nrhythmic patterns  4 \nOliver N , \nKreger -\nStickles L  \n(2006)  PAPA: Physiology And Pu r-\npose-Aware automatic playlist \ngeneration  4 \nTable 4. Ten most highly cited ISMIR public a-\ntions by patents  \nFour of the papers in Table 4 are also listed among the \nmost highly cited ISMIR papers in a 2009 informetric \nanalysis of the ISMIR conference series [1]: specifical ly, \nAucouturier  & Pachet (2002), Music similarity measures: \nWhat's the use? ; Logan (2000), Mel frequency cepstral \ncoefficients for music modeling ; Tzanetakis  et al (2002), \nAutomatic musical genre classification of audio signals ; \nand Paulus &  Klapuri (2002), Measuring the similarity of \nrhythmic patterns .  \n4.2.3 Inventors Who P ublish in ISMIR  \nA total of  49 inventors have published in ISMIR procee d-\nings; Table 5 presents a list of  most prolific ISMIR a u-\nthors among the  the inventors. Masataka Goto (National \nInstitute  of Advanced Industrial Science and Technology) \nemerged as one of the  key figure s for co nnecting the ac a-\ndemic research and commercial develo pment s, followed \nby Brain Whitman  (The Echo Nest)  and Malcolm Slaney  \n(Microsoft) .  Author  No. of ISMIR \npublications  No. of patents \ninvented  \nMasataka Goto 29 5 \nDaniel P. W. Ellis  28 1 \nElias Pampalk  15 1 \nFrancois Pachet  15 1 \nTim Pohle  14 2 \nHiroshi Okuno  13 2 \nKazuyoshi Yoshii  12 2 \nDominik Schnitzer  10 1 \nDouglas Eck  9 1 \nMitsunori Ogihara  9 1 \nPaul B. Lamere  8 1 \nBrian A. Whitman  7 5 \nMalcolm Slaney  7 3 \nKristopher C. West  7 2 \nJosep -Lluis Arcos  6 1 \nTable 5. Top 15 inventors who are also ISMIR a u-\nthors (sorted by the number of ISMIR publications)  \n4.3 Analysis  of MIREX References in Patents  \nThirteen patents were identified that referenced MIREX :  \n9 had been granted, and 4 were applications . Table 6 \nsummarizes the filing dates and, for the granted patents, \nthe year in which they were issued . Of these  thirteen , six \nalso referenced ISMIR pap ers. A significant degree of \noverlap could be expected, given that the research of \nMIREX participants is also frequently published in the \nassociated ISMIR conference . Indeed, t he relationship \nbetween the MIREX and ISMIR events  may explain the \nrelatively small number of pat ents including MIREX re f-\nerences:  entries to the  MIREX trials are frequently a c-\ncompanied by more detailed submissions regarding the \nalgorithms to the associated ISMIR. Further,  MIREX \nproceedings are informally published and can be difficult \nto locate and cite [1]—additional  reasons why an ISMIR \npaper might be cited in preference to a similar MIREX \npublication.  \nYear  No. of Applications  No. Issued  \n2006  3  \n2007  5  \n2008  2 1 \n2009  1  \n2010  2 5 \n2011   1 \n2012   2 \nTable 6. Summary by year of MIREX patents  \nThese thirteen patents included 24 references to \nMIREX: 21 references to 11 unique papers in the MIREX \nproceedings, 2 references to an ISMIR paper providing an \noverview of MIREX results, and 3 more general refe r-\nences to the MIREX trials as a whole. These latter point \nto the significance of algorithms by relevance to their re p-\nresentation and relative performance in the MIREX trials \n(e.g., “ …systems us ing this technique regularly rank in   \n \nthe very top places in the yearly MIREX Automatic M u-\nsic Recommendation evaluations …” [1]). \nThe thirteen patents had 22 unique inventors (one i n-\nventor was named in two patents). Of these 22 inventors, \n11 had published in at least one ISMIR conference (See  \nFigure 6, MIREX inventor s publishing in ISMIR by \nyear)—yielding an average of 2.91 inve ntors who cited \nMIREX in attendance at each ISMIR, 2002 – 2012.  \n \nFigure 6. MIREX inventors publishing in ISMIR \nby year  \n5. CONCLUSION AND FUTUR E WORK  \nIn this paper, we have examined  the influence  of academ-\nic MIR research from the ISMIR conference series and \nMIREX on patents, as viewed through citation links  from \npatents to the academic publications . We identified  over \nhundred  references to ISMIR and MIREX research in the \npatents , most prominently  in early 2000s , and  concerning \nvarious content -based retrieval technologies  such as au-\ndio fingerprinting, recommendations, automatic playlist \ngeneration , and so on . Th e investigation indicates the \npresence of strong and ongoing personal links b etween \nacademic and commercial MIR research, as ev idenced by \nthe number of individuals who produce both academic \npublications and patents in MIR (Section 4.2.3). It is also \nencouraging to see significant propor tions of the ISMIR/ \nMIREX references in patents not filed by inve ntors with \ndirect connections to ISMIR (Sections 4.2.1, 4.2.3).   \nIronically, it would not be straightforward to  invest i-\ngate the opposite flow of influence —ISMIR /MIREX \npublications citing patents —because the ISMIR and \nMIREX events are not associated with the primary co m-\nputer science and engineering professional societies and \nconsequently are not included in the societies’ digital l i-\nbrary portals (i .e., the ACM Digi tal Library and IEEE \nXplore). Further, the MIREX papers are incompletely \nrepresented in Google Scholar  [1], and both MIREX and \nISMIR are inconsistently indexed  [1], [3]—to the extent \nthat it is not poss ible to be assured of complete coverage \nof ISMIR and MIREX publications through Google \nScholar searches.  Two digital libraries have been deve l-\noped and mai ntained by individuals within the MIR \ncommunity to pr ovide improved access to ISMIR and \nMIREX public ations: Michael Fingerhut’s ISMIR net  \n(http://www.ismir.net/ ) for ISMIR  publications , and D a-vid Bainbridge’s library  (http://music -ir.org/mirex -\ndl/library ) for MIREX . However, it lacks full -text sea rch \ncapabilities, and the latter does provide complete, stan d-\nardized metadata —and so their utility for informetric i n-\nvestigations is reduced . More seriously , the visibility of \nthe ISMIR and MIREX series as a whole is diminish ed. \nIn our future work, w e plan to conduct a topic analysis \nof ISMIR publications, to identify shifts in focus over the \nconference series and for comparison to topics identified \nin the patents (Section 4.1.3).  Also f urther investigation is \nrequired to tease out the factors contributing to the d e-\ncline in the number of patents referencing ISMIR. One \npossibility is that the research interests of the academic \nand commercial communities have indeed diverged \n(though the overlap between academic publishers and i n-\nventors argues against this).  Ultimately, we  hope to e x-\npand our search and identify the patents citing any publ i-\ncations related to MIR in multiple publication venues, not \nlimited to ISMI R conference proceedings. This may help \nreveal a direction for new research that can make strong \nimpact in the everyday life of music users.    \n6. REFERENCES  \n[1] S. J. Cunningham, D. Bainbridge, and J. S . Downie : \n“The impact of MIREX on scholarly research,” \nProc . of the ISMIR , pp. 259-264, 2012 . \n[2] J. Futrelle , J. S. Downie: “Interdisciplinary  Co m-\nmunities  and Research Issues in Music  Info rmation \nRetrieval ,” Proc . of the ISMIR , pp. 215 -221, 2002.  \n[3] J. H. Lee, M. C. Jones, J. S. Downie : “An analysis \nof ISMIR proceedings: Patterns of authorship, topic, \nand citation,” Proc . of the ISMIR , pp. 57-62, 2009.  \n[4] J. H. Lee and N. M. Waterman: “Understanding user \nrequirements for music information services,” Proc . \nof the ISMIR , pp. 253–258, 2012.  \n[5] A. Klitkou and M. Gulbrandsen: “The re lationship \nbetween academic patenting and scientific \npublishing in Norway”, Scientometrics , Vol. 82, pp. \n93-108, 2010.  \n[6] M. Meyer: “Patent citations in a novel field of \ntechnology —what can they tell about interactions \nbetween emerging communities of science and \ntechnology? ” Scientometrics,  Vol. 48, No. 2, pp. \n151-178, 2000.  \n[7] M. Meyer : “What is special about patent cit ations? \nDifferences between scientific and patent cit ations,” \nScientometrics , Vol. 49, No. 1, pp. 93-123, 2000 . \n[8] F. Narin: “Patent bibliometrics ,” Scientometrics , \nVol. 30, pp. 147–155, 1994 . \n[9] D. Schnitzer : “A method and a system for  \nidentifying similar audio tracks. ” European Patent \nNo. EP 2273384. 12 Jan. 2011.  1 2 3 3 \n1 5 5 \n3 3 4 \n2 \n0123456\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012Number of ISMIR papers by \nMIREX citing inventors  \nYear of publication"
    },
    {
        "title": "AutoMashUpper: An Automatic Multi-Song Mashup System.",
        "author": [
            "Matthew E. P. Davies",
            "Philippe Hamel",
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416050",
        "url": "https://doi.org/10.5281/zenodo.1416050",
        "ee": "https://zenodo.org/records/1416050/files/DaviesHYG13.pdf",
        "abstract": "This paper describes AutoMashUpper, an interactive system for creating music mashups by automatically selecting and mixing multiple songs together. Given a userspecified input song, the system first identifies the phraselevel structure and then estimates the “mashability” between each phrase section of the input and songs in the user’s music collection. Mashability is calculated based on the harmonic similarity between beat synchronous chromagrams over a user-definable range of allowable key shifts and tempi. Once a match in the collection for a given section of the input song has been found, a pitch-shifting and time-stretching algorithm is used to harmonically and temporally align the sections, after which the loudness of the transformed section is modified to ensure a balanced mix. AutoMashUpper has a user interface to allow visualisation and manipulation of mashups. When creating a mashup, users can specify a list of songs to choose from, modify the mashability parameters and change the granularity of the phrase segmentation. Once created, users can also switch, add, or remove sections from the mashup to suit their taste. In this way, AutoMashUpper can assist users to actively create new music content by enabling and encouraging them to explore the mashup space.",
        "zenodo_id": 1416050,
        "dblp_key": "conf/ismir/DaviesHYG13",
        "keywords": [
            "interactive system",
            "music mashups",
            "automatically selecting",
            "mixing multiple songs",
            "phrase-level structure",
            "harmonic similarity",
            "pitch-shifting",
            "time-stretching",
            "loudness modification",
            "user interface"
        ],
        "content": "AUTOMASHUPPER: AN AUTOMATIC MULTI-SONG MASHUP SYSTEM\nMatthew E. P. Davies, Philippe Hamel, Kazuyoshi Yoshii and Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\nfmatthew.davies, hamel.phil, k.yoshii, m.goto g[at] aist.go.jp\nABSTRACT\nThis paper describes AutoMashUpper , an interactive\nsystem for creating music mashups by automatically se-\nlecting and mixing multiple songs together. Given a user-\nspeciﬁed input song, the system ﬁrst identiﬁes the phrase-\nlevel structure and then estimates the “mashability” be-\ntween each phrase section of the input and songs in the\nuser’s music collection. Mashability is calculated based on\nthe harmonic similarity between beat synchronous chro-\nmagrams over a user-deﬁnable range of allowable key\nshifts and tempi. Once a match in the collection for a given\nsection of the input song has been found, a pitch-shifting\nand time-stretching algorithm is used to harmonically and\ntemporally align the sections, after which the loudness of\nthe transformed section is modiﬁed to ensure a balanced\nmix. AutoMashUpper has a user interface to allow visu-\nalisation and manipulation of mashups. When creating a\nmashup, users can specify a list of songs to choose from,\nmodify the mashability parameters and change the granu-\nlarity of the phrase segmentation. Once created, users can\nalso switch, add, or remove sections from the mashup to\nsuit their taste. In this way, AutoMashUpper can assist\nusers to actively create new music content by enabling and\nencouraging them to explore the mashup space.\n1. INTRODUCTION\nMashups form a key part of the remix culture in mu-\nsic production and listening. Created by mixing together\nmultiple songs, or elements within songs, music mashups\nhold strong potential for entertaining and surprising lis-\nteners by bringing together disparate musical elements in\nunexpected ways. Until recently, the process for creating\nmashups relied on two elements: ﬁrst, the requisite mu-\nsical imagination (and access to a large and varied music\ncatalogue) to determine which songs to mix together, and\nsecond, the technical ability to use a Digital Audio Work-\nstation to produce high quality results.\nDue to the high popularity of mashups, some commer-\ncial systems and online tools have become available to as-\nsist users (both professional DJs and amateurs) in mixing\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.\nFigure 1 . The concept of multi-song mashups.\nsongs and creating mashups. DJ Mix Generator1is an\nonline database of 30,000 songs, where users can search\nby tempo, key and genre to ﬁnd compatible songs to mix\ntogether. The Harmonic Mixing Tool2offers similar func-\ntionality, but instead of presenting results through an on-\nline search engine, it analyses a user’s collection to iden-\ntify song compatibility and can create a “harmonic fade”\nwhen mixing between songs. Mashup3also provides a\nharmonic compatibility measure between songs, which is\ndetermined using a key signature detection algorithm and\nrelationships in the circle of ﬁfths. To allow the manual\ncreation of mashups, Mashup has an advanced audio edit-\ning interface.\nGiven the existence of these commercial mashup tools\nand their use of MIR techniques such as key detection, beat\ntracking and tempo estimation, it is quite surprising that so\nfew research papers exist on this topic. Of those which do,\ntheir scope appears limited to using just a handful of mu-\nsical excerpts, and they focus on the engineering aspects\nof time-stretching multiple songs simultaneously [7] or on\nvisualisation as part of the mashup making process [12].\nWhile these elements are certainly important, we believe\nthat there are many opportunities for the development of\nMIR techniques within the ﬁeld of music mashups. In-\ndeed, mashup creation was recently highlighted as one of\nthe “grand challenges” of MIR [5, p.222].\nIt is within this light that we propose AutoMashUpper ,\na system for making automatic multi-song mashups, as\nshown in Figure 1. The main novelty of our system lies\n1http://www.djprince.no/site/DMG.aspx\n2http://www.idmt.fraunhofer.de/en/Service_\nOfferings/technologies/e_h/harmonic_mixing_tool.\nhtml\n3http://mashup.mixedinkey.com/Figure 2 . Screenshot of the AutoMashUpper user interface. Additional descriptions of the functionality are overlaid.\nin the estimation of what we term “mashability” - a mea-\nsure of how well two songs ﬁt, or mash together. Look-\ning beyond the functionality of existing mashup systems\nwhich guide users to songs with matching key signatures\nand similar tempi, we incorporate a measure of harmonic\nsimilarity of beat synchronous chromagrams. This allows\nus to look for deeper matches than those possible from key-\nsignature alone. Furthermore our measure of mashability\ncan identify matches between songs in completely differ-\nent key-signatures, by directly exploiting the knowledge\nthat songs can be pitch-shifted by some number of semi-\ntones to “force” a match.\nBy identifying the phrase-level structure of a given in-\nput song, AutoMashUpper can determine the mashability\nbetween each phrase section of the input and songs in a\nuser’s music collection. In this way, multiple songs can\nbe used in the mashup at different times and this can radi-\ncally increase the range and variety of possible mashups.\nTo produce the mashup, we use existing techniques for\npitch-shifting and time-stretching for harmonic and tem-\nporal alignment respectively, and loudness adjustment to\ncreate a balanced sound mixture.\nIn addition to the fully automatic mode, AutoMashUp-\nper has a user interface to allow visualisation and manipu-\nlation of mashups, as shown in Figure 2. When creating a\nmashup, users can specify a list of songs to choose from,\nmodify the mashability parameters and change the granu-\nlarity of the phrase segmentation. Once created, users can\nalso switch, add, or remove sections from the mashup to\nsuit their taste, and additionally save the results for re-use.\nThe remainder of this paper is structured as follows. In\nSection 2 we describe the phrase-level segmentation used\nto partition songs in AutoMashUpper. In Section 3 we\npresent our method for mashability estimation and describe\nhow we produce the mashups. We then present the inter-\nface of AutoMashUpper in Section 4 and illustrate its main\nmodes of operation. Finally, in Section 5, we discuss the\npotential impact of our system, towards motivating furtherresearch into mashup generation, and present some areas\nfor future work.\n2. PHRASE-LEVEL SEGMENTATION\nA central component of our mashup system, and the key\nto enabling multiple songs to be used to produce the mu-\nsical result, is a phrase-level segmentation of the input.\nWhile much research has been conducted into structural\nsegmentation of music signals (e.g., [11, 13]) their goal is\nto identify boundaries of long sections corresponding to\nintro, verse and chorus, and to apply labels to these sec-\ntions to identify repetitions. For our purpose in creating\nmashups, we require a similar type of analysis, however\nour concern is not directly in labelling the sections, but\nrather in precisely identifying temporal boundaries. Fur-\nthermore we wish to identify shorter sections correspond-\ning to musical phrases, rather than longer time scale struc-\nture such as verse or chorus.\nThrough informal experimentation with existing seg-\nmentation algorithms with publicly available implementa-\ntions (e.g., [13]) we discovered that it was not trivial to\nreliably sub-divide the estimated sections into downbeat\nsynchronous phrases. On this basis, and in the interest of\navoiding multiple separate stages of processing the input\nsignal, we devise our own method for phrase segmenta-\ntion, adapting elements from existing systems to suit our\nneeds. Since an important element of mashups is the har-\nmonic compatibility of the mixed music signals, we base\nour phrase-level segmentation on a harmonic representa-\ntion of the input.\nTo generate the harmonic signal representations for\nphrase-level segmentation and the subsequent estimation\nof mashability, we use the NNLS Chroma plugin [8] within\nSonic Annotator [2]. Given an input audio signal, we\nextract three results from the NNLS Chroma plugin: the\nglobal tuning, t, of the input, an 84-bin (7 octave), tuned\nsemitone spectrogram, S, and a 12-dimensional chroma-\ngram (the distribution of energy across the chromatic pitchclasses in a musical octave), C. All outputs are extracted\nusing the default parameters. To create beat-synchronous\nversions ofSandC, we use the QM Vamp beat tracking\nplugin in Sonic Annotator, and take the median across the\ntime frames per beat. For ease of notation, we will continue\nto useSandCto refer to the beat-synchronous versions.\nTo simplify our approach, we make the following as-\nsumptions about the songs to be used for mashups: all\nphrase sections are a whole number of measures, all songs\nhave a constant 4/4 time-signature, and the input tempo is\napproximately constant.\nTo determine the phrase boundaries, we group the\nbeat-synchronous frames of Sinto measures, to create a\ndownbeat-synchronous semitone spectrogram. To identify\nthe downbeats we used a modiﬁed version of the method\nby Davies and Plumbley [3] with Sas the main input. As\nshown in [13] it can be beneﬁcial for segmentation per-\nformance to “stack” beat frames together when estimat-\ning section boundaries. In this way, we group sets of\nfour consecutive beat frames (starting at each downbeat\nand without overlap) to create a downbeat-synchronous\nstacked semitone spectrogram.\nGiven the beats and downbeats, we then follow the\nclassical approach of Foote [4] for structural segmen-\ntation. We calculate a self-similarity matrix from the\ndownbeat-synchronous stacked semitone spectrogram us-\ning the Itakura-Saito distance [6] and slide a Gaussian\ncheckerboard kernel along the main diagonal to generate\na novelty function to emphasise section boundaries. As\nshown in [4] the size of this kernel has a direct impact on\nthe level of the segmentation and temporal precision of the\nboundaries. Since our interest is in ﬁnding short phrase-\nlevel sections, we use a small kernel of size eight down-\nbeats. To obtain an initial set of phrase boundaries we\npeak-pick the resulting novelty function. We then employ\na technique derived from [11] to maximise the regularity\nof the detected phrase boundaries. A graphical example is\nshown in Figure 3 along with a ﬂow chart in Figure 4(a).\n3. MAKING MASHUPS\nThis section describes how the mashability is estimated be-\ntween beat-synchronous chromagrams for each phrase sec-\ntion of the input song, and the songs in a music collection.\nThen we address the requisite processing to physically cre-\nate the mashup itself. A graphical overview of the com-\nplete mashup creation process is shown in Figure 4.\n3.1 Estimating Mashability\nOnce the set of phrase segment boundaries has been deter-\nmined, we turn our attention to ﬁnding a match for each\nphrase section of the input with songs in the users’ music\ncollection by estimating what we refer to as “mashability”.\nFor each song in the collection, we pre-calculate a beat-\nsynchronous chromagram using the techniques described\nin Section 2 prior to estimating the mashability.\nIn contrast to existing systems which guide users to-\nwards mixing songs with matching key signature and have\ntime (beats)Semitone binsBeat−synchronous\nsemitone spectogram\n100 200 300 40020406080\ntime (downbeats)DownbeatsDownbeat−synchronous\nself−similarity matrix\n20 40 60 8010020406080100\ntime (downbeats)Stacked semitone binsDownbeat−synchronous\nsemitone spectrogram\n20 40 60 80100100200300\n20 40 60 8010001020304050\ntime (downbeats)Signal noveltyNovelty functionFigure 3 . Phrase-level segmentation overview. (top left)\na beat synchronous tuned semitone spectrogram. (bottom\nleft) a downbeat-synchronous spectrogram, where groups\nof four beat frames are stacked into measures. (top right)\na self-similarity matrix generated from the downbeat-\nsynchronous semitone spectrogram. (bottom right) a nov-\nelty function whose peaks highlight likely phrase bound-\naries. The vertical dotted lines show raw phrase bound-\naries and the solid grey lines show the result of regularity-\nconstrained realignment.\nsimilar tempi, we argue that there is a much wider scope of\npotential matches (and potentially more interesting musi-\ncal results) by considering mashups between songs in dif-\nferent keys and tempi. In effect, our approach is not only\nto look for matches according to the existing properties of\nsongs, but also to look for matches in a kind of “trans-\nform” domain, in the knowledge that we can subsequently\nuse time-stretching to temporally align songs, and pitch-\nshifting (by some number of semitones) to create, or in-\ndeed “force” a harmonic alignment.\nWe base our estimation of mashability around the har-\nmonic similarity between beat-synchronous chromagrams.\nFor the current phrase-section pof lengthKbeats from the\ninput song,i, we isolate the beat-synchronous chromagram\nCi;p(a 12-by-K matrix). To facilitate the search across dif-\nferent key shifts, we rotate the chroma bins of Ci;pacross\na range of integer semitone shifts, r, which can be set from\n0to\u00066semitones according to user preference. For each\nkey-shifted chroma section of the input, Ci;p;r we mea-\nsure its harmonic similarity across each rotational shift, r\nto all possible beat increments k, (forK-beat frame chro-\nmagrams) for each song nin the user’s song collection us-\ning the Cosine similarity,\nHn(r;k) =Ci;p;r\u0001Cn;p;k\njjCi;p;rjjjjCn;p;kjj(1)Figure 4 . Overview of the mashup creation process. (a) Pre-processing and phrase level segmentation, (b) mashability\nestimation, and (c) mashup creation.\nwhere high harmonic similarity will have Hclose to unity\nand low similarity will have Hclose to zero. A graphical\noverview is shown in Figure 4(b).\nTo move from harmonic similarity to mashability, M,\nwe include an additional term which rewards songs whose\ntempo,Tnis within a user speciﬁed ratio, \u0011, of the input\ntempo,Ti, such that:\nMn(r;k) =(\nHn(r;k) +\r;ifj1\u0000j(Ti=Tn)jj\u0014\u0011\nHn(r;k); otherwise\n(2)\nwhere\r=0.2 was found to give favourable results. Note\nthat the greater the value of \u0011the more permissive the sys-\ntem in terms of allowable tempo matches between the input\nand songs in the collection.\nOnce the mashability has been calculated across the\nsong collection we ﬁnd the song, n, beat increment, k,\n(i.e. starting point) and rotational shift, r, which lead to\nthe highest mashability for the current input phrase sec-\ntion,Ci;p;r. We apply the rotations of chroma to the input\nchromagram and not to the songs in the database, which\nremain unaltered by the search across mashability space.\nTherefore, when we come to implement any required key-\nshift to match the selected song with the input, we mustpitch-shift the selected song by \u00001\u0003rsemitones.\nBy measuring the harmonic similarity across all beat in-\ncrementskand rotational shifts rwe create a large search\nspace, which in turn gives the highest possibility for ﬁnd-\ning regions of high harmonic similarity. Furthermore we\nhave found matching between chroma matrices at incre-\nmental beat shifts, rather than looking at individual chroma\nframes, we can implicitly capture aligned chord changes\nbetween songs – a factor we have found improves the qual-\nity of the resulting mashup.\n3.2 Mashup Creation\nThe ﬁnal part of the automatic mashup creation process is\nto transform the selected section and mix it with the input,\nas shown in Figure 4(c).\nTo create this mix, several steps are required. First, we\nuse the open-source Rubberband time-stretching and pitch-\nshifting library4to temporally align (or “beat-match”) the\nmatching section with the current phrase section of the in-\nput song. This is achieved using the mapfile function\n(within Rubberband) which speciﬁes a set of anchor points,\ni.e., the beats of the song to be transformed, and a cor-\n4http://breakfastquay.com/rubberband/responding set of target times – the beats of the current\nphrase section of the input.\nOnce aligned in time, the matching section is then har-\nmonically aligned to the input phrase using the pitch-\nshifting functionality of Rubberband. This harmonic align-\nment addresses two areas: pitch-shifting by the required\nnumber of semitones, r, to match the key-signature of the\nsongs, and a tuning correction, identiﬁed as the ratio be-\ntween the estimated tuning for the input song and selected\nmatching song. In the event that both a tuning and pitch\ncorrection are required, we combine these factors into a\nsingle processing call to Rubberband. In the event that the\ntwo songs are already matched in key, (i.e. r=0) and the\ndifference in tuning is less than 1Hz, then the mashup can\nbe made by beat-matching alone.\nThe ﬁnal stage in mixing the sections of the two songs\ntogether, is to address any imbalance in the loudness be-\ntween the current input section and the transformed match.\nTo this end, we estimate the perceptual loudness in the in-\nput phrase section and transformed signal using the Replay\nGain algorithm [10]. While traditionally used to equalise\nloudness between songs, we wish to give greater promi-\nnence in the mix to the input song, hence we scale the am-\nplitude of transformed section to have 90% of the loudness\nof the input phrase section.\n4. AUTOMASHUPPER\nUp to this point we have described the backend process-\ning to enable the automatic creation of mashups. To al-\nlow users of AutoMashUpper to be involved in the mashup\ncreation process, we have built a user interface, which is\nshown in Figure 2. To illustrate the functionality of the\nuser interface and to provide sound examples, demonstra-\ntion videos are available here5.\n4.1 Interface Overview\nThe layout of the interface is split into three sections. On\nthe left hand side there are two main panels, the top for\nvisualising the waveform of the input song and the esti-\nmated phrase level section boundaries. Below this is the\nmashup visualizer which shows the songs currently used\nin the mashup. In addition, a set of playback controls are\nincluded for listening to the input song and the mashup.\nOn the right hand side is a panel containing the list of pre-\nanalysed songs in the music collection. Below this list-\nbox are buttons to select songs from the library to use in\nthe mashup. In the central panel we have a listbox which\nshows the current songs selected for use in the mashup, as\nwell a set of sliders for manipulating the parameters of the\nmashabilty calculation. These specify the range of allow-\nable key-shifts, and the preferred tempo range. Beneath\nthe list box are buttons for creating the automatic mashup\nand then subsequent manipulation of the result. This func-\ntionality is described in the following subsection.\n5http://www.youtube.com/user/automashupper4.2 User interaction\nThe typical scenario we envisage for AutoMashUpper is\nas follows. The user loads a song of their choice into the\nsystem, after which a waveform of the input song appears\nin the top left panel along with vertical bars to indicate the\nestimated phrase section boundaries. The user can listen to\nthe input song and click on different sections for the play-\nback to jump directly to these parts of the song. In addi-\ntion the user can explore ﬁner segmentations where phrase\nsections can be sub-divided into 16, 8 or 4-beat units using\nthe segmentation level drop-down menu. Having selected a\nsegmentation level, the user can then choose a set of songs\nfrom the song library on the right hand side of the inter-\nface. For this, three options are available: i) to manually\nselect a sub-set of their choice; ii) to select all of the songs\nin the library; or iii) to pick ten random songs.\nWhen manually choosing a subset, we have found that\nonly picking songs from the same artist, or the same album,\ni.e.,artist-level-mashups oralbum-level-mashups , can lead\nto very pleasing results due to high timbral compatibility.\nThe songs chosen by the user then appear in the listbox\nof selected songs in the middle of the interface. Using the\nsliders above this listbox, the user can specify how wide a\nrange of key shifts and tempi to allow in the mashability es-\ntimation. Specifying a small range of key shifts and tempi\ncan lead to somewhat conservative results, whereas allow-\ning a wide range of possibilities in the mashup space can\nfacilitate better matches, but perhaps at the cost of creat-\ning more unusual results, for example where a transformed\nsong could be pitch-shifted up or down by ﬁve semitones\nor radically changed in speed.\nOnce AutoMashUpper has been parameterised, the user\ncan then hit the auto mashup! button to generate a\nmashup. Or, the user may simply hit this button right af-\nter loading the input song. As each section is identiﬁed\nand added to the mashup it appears in the lower left hand\npanel, where each song is displayed in a different colour.\nWhen the processing has ﬁnished the user can listen to the\nresult – once again with the ability to navigate between\nphrase sections by clicking in the appropriate region of the\nwaveform representation or the mashup visualizer panel.\nDuring playback, a red vertical line indicates the currently\nplaying phrase section of the input song.\nClicking a particular bar in the mashup visualizer will\nhighlight the name of the chosen song in the selected songs\nlistbox. It will also re-order the remaining songs in de-\nscending order of mashability. At this point the user can\nmake a subjective judgement over whether they like the\nmashup as it is or which to change it. The user has three\noptions: ﬁrst, they can delete the currently used section\nfrom the mashup, second, they can choose a different song\nfrom the selected songs listbox to replace it or third, they\ncan choose to add another song from the list to the mashup.\nIf the user is pleased with the resulting mashup they can\nuse save button, which will create time-stamped .wav ﬁles\nfor the input song, the generated mashup by itself and the\nmixture of the input and mashup. In addition it records a\nscreenshot of the interface to show the list of songs usedand mashability parameters.\n5. DISCUSSION\nIn this paper we have presented AutoMashUpper , a system\nfor the creation of multi-song mashups. Our main contri-\nbution in this work is a method for mashability estimation\nwhich enables the automatic creation of music mashups.\nOur work forms part of the emerging ﬁeld of creative-\nMIR, where music analysis and transformation techniques\nare used within real applications towards the transfer of\nknowledge outside the MIR research community. We have\ndesigned AutoMashUpper with the aim of assisting users\n(who might lack music composition skills) to become mu-\nsic creators through simple interactions with a user inter-\nface. Our hope is that AutoMashUpper will encourage\nusers to explore a wide space of mashup possibilities by\nmanipulation of the mashability parameters, perhaps even\ncreating a new genre of “auto” mashups.\nWe believe a particular advantage of the automatic ap-\nproach to searching for mashability within a large collec-\ntions of songs is that such a system can uncover musical\nrelationships which might otherwise never be found. This\nis especially relevant if we consider the size of the search\nspace when allowing for matches at the phrase-level of\nsongs. Our current system uses a catalogue of around 300\nsongs from which we have been able to create many in-\nteresting mashups, with minimal effort. Furthermore, even\nwhen the phrase-level segmentation has errors, this has the\npotential to create unusual and unexpected results.\nWe have been particularly surprised by the quality of re-\nsults achieved when using the “pick ten random songs” op-\ntion in AutoMashUpper. This indicates that many hidden\nrelationships exist between different sections of songs, and\ndiscovering them in the context of a music mashup appears\na particularly good way to enjoy them. In this sense, the\npossibilities when applying this system to a very large mu-\nsic collection could be almost endless. However, the tran-\nsition from a medium-sized collection to a very large one\npresents many challenges due to scalability and increased\ncomputational cost, and would require a much faster search\ntechnique, (e.g., [1]). We intend to explore this area within\nour future work as well as investigating source separation\nmethods (e.g., [9]) to offer users even greater mashup cre-\nation possibilities.\nSince mashups, by deﬁnition, contain multiple songs\nplaying at once, they represent an interesting category of\nmusic from an auditory scene analysis perspective, where\nit is listeners’ familiarity with songs in the mashup which\nallow them to understand a musical scene which might oth-\nerwise be too complex to process and hence appreciate [5].\nTo further explore these ideas and to address the lack of\na formal evaluation of AutoMashUpper, we plan to under-\ntake subjective listening tests to explore listeners’ levels of\nmusical engagement and understanding of mashups.\nLooking beyond the current version of AutoMashUp-\nper, we recognise that mashability is not fully explained by\nharmonic similarity alone, and we can envisage many ad-\nditional uses of MIR techniques for creating more sophis-ticated measures of mashability, e.g. by exploring rhyth-\nmic and spectral compatibility. On this basis we strongly\nencourage other researchers to explore mashup creation\nmethods to expand the ﬁeld of creative MIR.\n6. ACKNOWLEDGMENTS\nThis work was supported by OngaCREST, CREST, JST.\n7. REFERENCES\n[1] T. Bertin-Mahieux and D. Ellis. Large-scale cover song\nrecognition using the 2D Fourier transform magnitude. In\nProceedings of 13th International Society for Music Infor-\nmation Retrieval Conference , pages 241–246, 2012.\n[2] C. Cannam, M. O. Jewell, C. Rhodes, M. Sandler, and\nM. d’Inverno. Linked data and you: Bringing music research\nsoftware into the semantic web. Journal of New Music Re-\nsearch , 39(4):313–325, 2010.\n[3] M. E. P. Davies and M. D. Plumbley. A spectral difference\napproach to extracting downbeats in musical audio. In Pro-\nceedings of the 14th European Signal Processing Conference\n(EUSIPCO) , 2006.\n[4] J. Foote. Automatic audio segmentation using a measure of\naudio novelty. In Proceedings of IEEE International confer-\nence on multimedia and expo , pages 452–455, 2000.\n[5] M. Goto. Grand challenges in music information research. In\nM. Muller, M. Goto, and M. Schedl, editors, Multimodal Mu-\nsic Processing , pages 217–225. Dagstuhl Publishing, 2012.\n[6] F. Itakura and S. Saito. Analysis synthesis telephony based\non the maximum likelihood method. In Proceedings of the\nInternational Congress on Acoustics , pages 17–20, 1968.\n[7] G. Grifﬁn Y . E. Kim and D. Turnbull. Beat-sync-mash-\ncoder: A web application for real-time creation of beat-\nsynchronous music mashups. In Proceedings of IEEE Inter-\nnational Conference on Acoustics Speech and Signal Pro-\ncessing (ICASSP) , pages 437–440, 2010.\n[8] M. Mauch and S. Dixon. Approximate note transcription for\nthe improved identiﬁcation of difﬁcult chords. In Proceed-\nings of the 11th International Society for Music Information\nRetrieval Conference , pages 135–140, 2010.\n[9] Z. Raﬁi and B. Pardo. REpeating Pattern Extraction Tech-\nnique (REPET): A simple method for music/voice separation.\nIEEE Transactions on Audio, Speech and Language Process-\ning, 21(1):71–82, 2013.\n[10] D. Robinson. Perceptual model for assessment of coded au-\ndio. PhD thesis, Department of Electronic Systems Engineer-\ning, University of Essex, 2002.\n[11] G. Sargent, F. Bimbot, and E. Vincent. A regularity-\nconstrained viterbi algorithm and its application to the struc-\ntural segmentation of songs. In Proceedings of the 12th Inter-\nnational Society for Music Information Retrieval Conference ,\npages 483–488, 2011.\n[12] N. Tokui. Massh!: A web-based collective music mashup sys-\ntem. In Proceedings of the 3rd International Conference on\nDigital Interactive Media in Entertainment and Arts , pages\n526–527, 2008.\n[13] R. J. Weiss and J. P. Bello. Identifying repeated patterns\nin music using sparse convolutive non-negative matrix fac-\ntorization. In Proceedings of the 11th International Society\nfor Music Information Retrieval Conference , pages 123–128,\n2010."
    },
    {
        "title": "Multiscale Approaches To Music Audio Feature Learning.",
        "author": [
            "Sander Dieleman",
            "Benjamin Schrauwen"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416676",
        "url": "https://doi.org/10.5281/zenodo.1416676",
        "ee": "https://zenodo.org/records/1416676/files/DielemanS13.pdf",
        "abstract": "Content-based music information retrieval tasks are typically solved with a two-stage approach: features are extracted from music audio signals, and are then used as input to a regressor or classifier. These features can be engineered or learned from data. Although the former approach was dominant in the past, feature learning has started to receive more attention from the MIR community in recent years. Recent results in feature learning indicate that simple algorithms such as K-means can be very effective, sometimes surpassing more complicated approaches based on restricted Boltzmann machines, autoencoders or sparse coding. Furthermore, there has been increased interest in multiscale representations of music audio recently. Such representations are more versatile because music audio exhibits structure on multiple timescales, which are relevant for different MIR tasks to varying degrees. We develop and compare three approaches to multiscale audio feature learning using the spherical K-means algorithm. We evaluate them in an automatic tagging task and a similarity metric learning task on the Magnatagatune dataset.",
        "zenodo_id": 1416676,
        "dblp_key": "conf/ismir/DielemanS13",
        "keywords": [
            "features",
            "regressor",
            "classifier",
            "K-means",
            "restricted Boltzmann machines",
            "autoencoders",
            "sparse coding",
            "multiscale representations",
            "spherical K-means",
            "Magnatagatune dataset"
        ],
        "content": "MULTISCALE APPROACHES TO MUSIC AUDIO FEATURE LEARNING\nSander Dieleman and Benjamin Schrauwen\nElectronics and Information Systems department, Ghent University\nfsander.dieleman, benjamin.schrauwen g@ugent.be\nABSTRACT\nContent-based music information retrieval tasks are typi-\ncally solved with a two-stage approach: features are ex-\ntracted from music audio signals, and are then used as in-\nput to a regressor or classiﬁer. These features can be engi-\nneered or learned from data. Although the former approach\nwas dominant in the past, feature learning has started to\nreceive more attention from the MIR community in re-\ncent years. Recent results in feature learning indicate that\nsimple algorithms such as K-means can be very effective,\nsometimes surpassing more complicated approaches based\non restricted Boltzmann machines, autoencoders or sparse\ncoding. Furthermore, there has been increased interest in\nmultiscale representations of music audio recently. Such\nrepresentations are more versatile because music audio ex-\nhibits structure on multiple timescales, which are relevant\nfor different MIR tasks to varying degrees. We develop\nand compare three approaches to multiscale audio feature\nlearning using the spherical K-means algorithm. We evalu-\nate them in an automatic tagging task and a similarity met-\nric learning task on the Magnatagatune dataset.\n1. INTRODUCTION\nContent-based music information retrieval (MIR) techni-\nques can be used to solve a variety of different problems,\nsuch as genre classiﬁcation, artist recognition and music\nrecommendation. They typically have a two-stage archi-\ntecture: ﬁrst, features are extracted from music audio sig-\nnals to transform them into a more meaningful representa-\ntion. These features are then used as input to a regressor or\na classiﬁer, which is trained to perform the task at hand.\nAlthough machine learning techniques such as support\nvector machines and neural networks have traditionally\nbeen popular for the second stage, the features extracted\nfrom the audio are typically engineered rather than learned.\nFeature engineering is a complex and time-consuming pro-\ncess. Furthermore, these features are usually designed with\na particular task in mind and are likely not optimally suited\nfor other tasks. A prime example of this are the popular\nmel-frequency cepstral coefﬁcients (MFCCs), which were\noriginally designed for speech processing. MFCCs mainly\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.encode timbre and discard pitch information, which is of-\nten undesirable when working with music audio. Despite\nthis, they are still very commonly used for this purpose.\nIn recent years, feature learning has started to receive\nmore attention from the MIR community. This can be at-\ntributed at least in part to a surge of interest in feature learn-\ning in general, ever since the emergence of deep learning\nin the mid-2000s [12]. Simply put, the idea of deep learn-\ning is to learn a hierarchy of features, organized in layers\nthat correspond to different levels of abstraction. Higher-\nlevel features are deﬁned in terms of lower-level features.\nA similar evolution has taken place in computer vi-\nsion and speech processing, where approaches based on\ndeep learning are now commonplace, after improving on\nthe previous state of the art by a large margin [6, 14]. In\nlight of this, Humphrey et al. [13] advocate the use of\ndeep architectures to solve MIR problems. However, re-\ncent results in feature learning indicate that simple, shal-\nlow (single-layer) feature learning techniques such as the\nK-means algorithm can also be quite competitive, some-\ntimes surpassing more complicated approaches based on\nrestricted Boltzmann machines, autoencoders and sparse\ncoding [5]. These models are typically much more difﬁ-\ncult to tune than the K-means algorithm, which only has\none parameter (the number of means). They are often an\norder of magnitude slower to train as well.\nAnother recent development in MIR research is the in-\ncreased interest in multiscale architectures [1,8,10]. Music\naudio exhibits structure on many different timescales: at\nthe lowest level, signal periodicity gives rise to pitch. Peri-\nodicity at longer timescales emerges from rhythmic struc-\nture, repeated motifs and musical form. These timescales\nare relevant for different tasks to varying degrees.\nSome researchers have explored architectures that are\nboth deep and multiscale: for example, in convolutional\nneural networks, subsampling layers are often inserted be-\ntween the convolutional layers, so that the features at each\nsuccessive level take a larger part of the input into account\n[7, 16]. Indeed, it is not unreasonable to assume that more\ncomplex features will typically span longer timescales.\nHowever, these concepts need not necessarily be inter-\ntwined; for example in multiresolution deep belief net-\nworks [19], both hierarchies are separated.\nIn this paper, we endeavor to build a versatile feature\nlearning architecture for music audio that operates on mul-\ntiple timescales, using the spherical K-means algorithm.\nWe compare three multiscale architectures and evaluate the\nresulting features in an automatic tagging task and a sim-\nilarity metric learning task on the Magnatagatune datasetPCA whitening\nK-means\nPooling\nfeaturespooling window1, 2 or 4 consecutive frames\nFigure 1 : Schematic overview of the feature extraction\nprocess at a single timescale. The mel-spectrogram is di-\nvided into large pooling windows. Smaller windows con-\nsisting of 1, 2 or 4 consecutive frames are extracted convo-\nlutionally and PCA-whitened, and then K-means features\nare extracted. The features are pooled by taking the maxi-\nmum across time over the pooling windows.\n[15]. The rest of the paper is structured as follows: in Sec-\ntion 2 we outline the feature learning algorithm we used.\nA description of the multiscale architectures we investi-\ngated follows in Section 3. Our experiments and evalua-\ntion methods are detailed in Section 4 and the results are\ndiscussed in Section 5.\n2. FEATURE LEARNING\nTo learn features from music audio, it is typically con-\nverted into a time-frequency representation ﬁrst, such as\na mel-spectrogram (see Section 4.1 for details). Fea-\nture learning algorithms can then be applied to individ-\nual spectrogram frames or windows of consecutive frames.\nWe used the following feature extraction pipeline, which\nis visualized schematically in Figure 1: ﬁrst, the mel-\nspectrograms are divided into large pooling windows, sev-\neral seconds in length. Smaller windows consisting of 1, 2\nor 4 consecutive frames are then extracted convolutionally\nand PCA-whitened, and K-means features are extracted\nfrom the whitened windows. The features are pooled by\ntaking the maximum across time over the pooling win-\ndows. Each part of the pipeline is decribed in more detail\nbelow. For now, we will assume a typical single-timescale\nsetting. In Section 3, we extend our approach to multiscale\ntime-frequency representations.\n2.1 PCA whitening\nFirst, we randomly sample a set of windows from the data,\nand whiten them with PCA. We keep enough components\nto retain 99% of the variance. The whitened windows\nare then used to learn the dictionary. It has been shown\nthat this whitening step considerably improves the features\nlearned by the K-means algorithm [5].2.2 Spherical K-means\nWe then use spherical K-means to learn features from\nthe whitened windows. The spherical K-means algorithm\ndiffers from more traditional variants in the fact that the\nmeans are constrained to have a unit L2 norm (they must\nlie on the unit sphere). This is achieved by adding a nor-\nmalization step in every iteration after the means are up-\ndated. For a detailed overview of the algorithm, we refer\nto Coates et al. [4].\nK-means has often been used as a dictionary learning\nalgorithm in the past, but it has only recently been shown\nto be competitive with more advanced techniques such as\nsparse coding. The one-of-K coding (i.e., each example\nbeing assigned to a single mean) is beneﬁcial during learn-\ning, but it turns out to be less suitable for the encoding\nphase [3]. By replacing the encoding procedure, the fea-\ntures become signiﬁcantly more useful. For spherical K-\nmeans, it turns out that using a linear encoding scheme\nworks well: the feature representation of a data point is\nobtained by multiplying it with the dictionary matrix. To\nextract features from mel-spectrograms with a sliding win-\ndow, we have to whiten the windows and extract features,\nwhich can be implemented as a single convolution with the\nproduct of the whitening matrix and the dictionary matrix.\nWe can also skip the K-means step altogether and use\nthe PCA components obtained after whitening as features\ndirectly. These features were referred to as principal mel-\nspectrum components (PMSCs) by Hamel et al. [11]. They\nare in fact very similar to MFCCs: if the windows consist\nof single frames, replacing the PCA whitening step with a\ndiscrete cosine transform (DCT) results in MFCC vectors.\nBoth transformations serve to decorrelate the input.\n2.3 Pooling\nThe representation we obtain by extracting features con-\nvolutionally from mel-spectrograms is not yet suitable as\ninput to a regressor or classiﬁer. It needs to be summarized\nover the time dimension ﬁrst. We pool the features across\nlarge time windows several seconds in length. Although\na combination of the mean, variance, minimum and maxi-\nmum across time has been found to work well for this pur-\npose [11], we use only the maximum, because it was found\nto be the best performing single pooling function. This re-\nduces the size of the feature representation fourfold, which\nspeeds up experiments signiﬁcantly while having only a\nlimited impact on performance. We used non-overlapping\npooling windows for convenience, but our approach does\nnot preclude the use of overlapping windows. Figure 1\nshows a schematic overview of the feature extraction pro-\ncess.\n3. MULTISCALE APPROACHES\nWe explore three approaches to obtain multiscale time-\nfrequency representations from mel-spectrograms: mul-\ntiresolution spectrograms, Gaussian pyramids and Lapla-\ncian pyramids [2]. An example of each is given in Figure(a) Multiresolution spectrograms\n (b) Gaussian pyramid\n (c) Laplacian pyramid\nFigure 2 : Three different 4-level multiscale time-frequency representations of music audio. Level 0 is at the bottom, higher\nlevels correspond to coarser timescales. This ﬁgure is best viewed in color.\n2. To arrive at a multiscale feature representation, we sim-\nply apply the pipeline described in Section 2 to each level\nseparately, and concatenate the resulting feature vectors.\n3.1 Multiresolution spectrograms\nThe most straightforward approach to learning music au-\ndio features at multiple timescales is to extract mel-\nspectrograms with different window sizes. This approach\nwas previously explored by Hamel et al. [10]. For each\nconsecutive level, we simply double the spectrogram win-\ndow size of the previous level. Although we could also\ndouble the hop size to reduce the size of the higher level\nrepresentations, this was found to decrease performance\nconsiderably, so it is kept the same for all levels. Figure 2a\nshows an example of a set of multiresolution spectrograms.\n3.2 Gaussian pyramid\nThe Gaussian pyramid is a very popular multiscale repre-\nsentation in image processing. Each consecutive level of a\nGaussian pyramid is obtained by smoothing and then sub-\nsampling the previous level by a factor of 2 in the time\ndimension. To our knowledge, it has not previously been\napplied to time-frequency representations of music audio.\nIn this case, mel-spectrograms are extracted only at the\nﬁnest timescale and all higher levels can be obtained from\nthem. Higher-level representations in a Gaussian pyramid\nwill be smaller in size because of the subsampling step.\nThis means that the pooling windows used in our feature\nextraction pipeline must be shrunk accordingly. An exam-\nple of a Gaussian pyramid is shown in Figure 2b. Note that\nthe lowest level of the Gaussian pyramid is identical to that\nof the multiresolution spectrograms.\n3.3 Laplacian pyramid\nThe Laplacian pyramid can be derived from the Gaus-\nsian pyramid by taking each level and subtracting an up-\nsampled version of the level above it. The top level re-\nmains the same. The result is that the representations at\nﬁner timescales will not contain any information that is al-\nready represented at coarser timescales. This reduces re-\ndundancy between the levels of the pyramid. An example\nof a Laplacian pyramid is shown in Figure 2c.3.4 Modeling local temporal structure\nFrames taken from multiresolution spectrograms will auto-\nmatically model longer-range temporal structure at higher\nlevels, because the spectrogram window size is increased.\nFor the Gaussian and Laplacian pyramids however, this\nis not the case: any temporal structure present at longer\ntimescales is lost by the downsampling operation that is\nrequired to construct the higher levels of the pyramid. Al-\nthough frames at higher levels of the pyramid are affected\nby a larger region of the input, they do not reﬂect tempo-\nral structure within this region. To allow for this structure\nto be taken into account by the feature learning algorithm,\nwe can instead use windows of a small number of consec-\nutive frames as input. This is not useful for multiresolution\nspectrograms because the temporal resolution is the same\nat all levels, i.e. higher levels do not have coarser temporal\nresolutions as is the case for the pyramid representations.\nFigure 3 shows a random selection of features learned\nfrom windows of 4 consecutive mel-spectrogram frames\nat different levels in a 6-level Gaussian pyramid, with\nPCA whitening (top) and with spherical K-means (bot-\ntom). Some features are stationary across the time di-\nmension, others resemble percussive events and changes\nin pitch. Many of the K-means features seem to reﬂect\nharmonic structure. This is especially the case for level 1\nwhere the features span roughly half a second, which is\naround the average duration of a musical note. This type\nof structure is less pronounced in the PCA features.\n4. EXPERIMENTS\n4.1 Dataset\nWe used the Magnatagatune dataset [15], which contains\n25863 29-second audio clips taken from songs by 230\nartists sampled at 16 kHz, along with metadata and tags.\nIt comes in 16 parts, of which we used the ﬁrst 12 for\ntraining, the 13th for validation and the remaining 3 for\ntesting. We extracted log-scaled mel-spectrograms with\n200 components, with a window size of 1024 frames (cor-\nresponding to 64 ms) and a hop size of 512 frames (32\nms). For the multiresolution spectrogram representation,\nthe spectrogram window size was doubled for each con-(a) level 0\n (b) level 1\n (c) level 2\n (d) level 3\n (e) level 4\n (f) level 5\nFigure 3 : A random selection of features learned with PCA whitening (top) and spherical K-means (bottom) from windows\nof 4 consecutive mel-spectrogram frames, at different levels in a Gaussian pyramid. The frequency increases from bottom\nto top.\nsecutive level. We used 6 levels for all three multiscale\nrepresentations, numbered 0 to 5. This means that frames\nat the highest level span 2 seconds and are spaced 1 sec-\nond apart. We used pooling windows of 128 spectrogram\nframes at the lowest level (about 4 seconds).\n4.2 Tag prediction\nThe main task we used to evaluate the proposed feature\nlearning architectures is a tag prediction task. This allows\nus to evaluate the versatility of the representations, because\ntags describe a variety of different aspects of the music:\ngenre, presence (or absence) of an instrument, tempo and\ndynamics, among others. All clips in the dataset are an-\nnotated with tags from a set of 188. We only used the 50\nmost frequently occurring tags for our experiments, to en-\nsure that enough training data is available for each of them.\nWe trained a multilayer perceptron (MLP) on the pro-\nposed multiscale feature representations to predict the\npresence of the tags, with a hidden layer consisting of 1000\nrectiﬁed linear units [17]. We used minibatch gradient de-\nscent with weight decay to minimize the cross entropy be-\ntween the predictions and the true tag assignments, and\nstopped training when the performance on the validation\nset was no longer increasing. Hyperparameters such as the\nlearning rate and the weight decay constant were optimized\nby performing a random search on the validation set. We\ntrained the MLP on feature vectors obtained from pooling\nwindows. Tag predictions for an entire clip were computed\nby taking the average of the predictions across all pool-\ning windows. We computed the area under the ROC-curve\n(AUC) for all tags individually and then took the average\nacross all tags to get a measure of the predictive perfor-\nmance of the trained models.\nWe evaluated four different feature learning setups:\nPCA whitening, and spherical K-means with 200, 500 and\n1000 means. We also compared 7 different multiscale ap-\nproaches: Gaussian and Laplacian pyramids with windows\nof 1, 2 and 4 consecutive frames, and multiresolution spec-\ntrograms. This yields 28 different architectures in total.4.3 Similarity metric learning\nWe also used the features to learn an audio-based mu-\nsic similarity metric, to further assess their versatility.\nUsing Neighborhood Components Analysis (NCA) [9],\nwe learned a linear projection of the features into a 50-\ndimensional space, such that similar clips are close to-\ngether in terms of Euclidean distance. Each clip is mapped\ninto this space by projecting the feature vectors corre-\nsponding to each pooling window and then taking the mean\nacross all pooling windows. The linear projection matrix is\nthen optimized with minibatch gradient descent to project\nclips by a given artist into the same region. This approach\nto learning a music similarity metric was previously ex-\nplored by Slaney et al. [18].\nNCA is based on a probabilistic version of K-nearest\nneighbor classiﬁcation, where neighbors are selected prob-\nabilistically proportionally to their distance and each data\npoint inherits the class of its selected neighbor. The objec-\ntive is then to maximize the probability of correct classiﬁ-\ncation. We report this probability on the test set.\n5. RESULTS\n5.1 Architectures\nThe results for the tag prediction task obtained with each\nof the 28 different architectures are shown in Figure 4. All\nreported results are averaged across 10 MLP training runs\nwith different initializations. Unfortunately we cannot di-\nrectly compare our results with those of Hamel et al. [10],\nbecause they used a different version of the dataset.\nThe ﬁrst thing to note is that using features learned with\nspherical K-means almost always yields increased perfor-\nmance, although the difference between using 500 or 1000\nmeans is usually small. Interestingly, the best perform-\ning architecture uses a Laplacian pyramid, with features\nlearned from single frames. This is somewhat unexpected,\nbecause it implies that grouping consecutive frames into\nwindows so that the feature learning algorithm can capture\ntemporal structure is not necessary for this type of multi-multiresolution\nspectrogramsGaussian\n1 frameGaussian\n2 framesGaussian\n4 framesLaplacian\n1 frameLaplacian\n2 framesLaplacian\n4 frames0.8860.8880.8900.8920.8940.8960.8980.900Average AUCPCA\nK-means (200)\nK-means (500)\nK-means (1000)Figure 4 : Results for the tag prediction task, for 28 different multiscale feature learning architectures. All reported results\nare averaged across 10 MLP training runs with different initializations. Error bars indicate the standard deviation across\nthese 10 runs.\nmultiresolution\nspectrogramsGaussian\n1 frameGaussian\n2 framesGaussian\n4 framesLaplacian\n1 frameLaplacian\n2 framesLaplacian\n4 frames0.9100.9150.9200.9250.9300.9350.940Prob. of correct classification\nFigure 5 : Results for the similarity metric learning task, for 28 different multiscale feature learning architectures. All\nreported results are averaged across 10 training runs with different initializations. Error bars indicate the standard deviation\nacross these 10 runs.\nscale representation. This does seem to help when using a\nGaussian pyramid, however.\nResults for the similarity metric task are shown in Fig-\nure 5. Here, a Gaussian pyramid with windows of 2 con-\nsecutive frames works best. Using 1000 means is notice-\nably worse than using 500 means. This can be attributed\nto the fact that the NCA objective seems to be very prone\nto overﬁtting when using a large amount of input features\n(6000 in this case, for 6 levels), despite our use of weight\ndecay and early stopping for regularization.\n5.2 Relevant timescales\nTo assess which timescales are relevant for different tags,\nwe took the best architecture from the previous experiment\nand tried to predict tags from each level individually. Al-\nthough a combination of all levels performs best for all\ntags, it is not always obvious precisely which timescales\nare the most relevant ones for a given tag. Figure 6 shows\na selection of tags where some patterns can be identiﬁed.\nTwo tags describe the tempo of the music: slow and\nfast. As expected, the highest level seems to be the most\nimportant one for slow. For fast, level 1 (corresponding to\na frame size of 128 ms) performs best. Both tags beneﬁt\nquite a lot from the multiscale representation: a combina-\ntion of all levels performs much better than any level in-\ndividually. Tags describing dynamics, such as loud,quiet\nandsoft, seem to rely mostly on the top level, correspond-ing to the coarsest timescale. This may also be because the\ntop level is the only level in the Laplacian pyramid that is\nnot a difference of two levels in the Gaussian pyramid.\nTags related to vocals can be predicted most accurately\nfrom intermediate levels, as evidenced by the results for\nvocal ,female ,singing andvocals . Of these, female is the\neasiest to predict, being the most speciﬁc tag. Finally, the\nﬂute tag is somewhat atypical among the tags describing\ninstruments, in that it is the only one that relies mostly on\nthe coarsest timescale (results for other instruments are not\nshown). A possible reason for this could be that the instru-\nment lends itself well to playing longer, drawn out notes.\nA quick examination of the dataset reveals that many ex-\namples tagged ﬂute in the dataset feature such notes.\n6. CONCLUSION AND FUTURE WORK\nWe have proposed three approaches to building multiscale\nfeature learning architectures for music audio, and we have\nevaluated them using two tasks designed to demonstrate\nthe versatility of the learned features. Although learning\nfeatures with the spherical K-means algorithm consistently\nimproves results over just using PCA components, there is\nno clear winner among the proposed multiscale architec-\ntures. However, it is clear that learning features at multi-\nple timescales improves performance over single-timescale\napproaches. We have also shown that different kinds of\ntags tend to rely on different timescales. Finally, we haveslow fast loud quiet soft vocal female singing vocals flute0.750.800.850.900.951.00AUCall levels\nlevel 0 (finest)\nlevel 5 (coarsest)Figure 6 : Results for a selection of individual tags, when using features from all levels combined and when using features\nfrom each level individually. The reported AUCs are for K-means with 500 means on the Laplacian pyramid with features\nlearned from single frames.\nobserved that special care has to be taken to prevent overﬁt-\nting when using large feature representations, which is al-\nmost unavoidable if a multiscale representation is desired.\nIn future work, we would like to improve the fea-\nture learning pipeline, by learning multiple layers of fea-\ntures for each timescale and investigating other encod-\ning schemes. We would also like to evaluate the pro-\nposed architectures on an extended range of tasks, includ-\ning content-based music recommendation and artist recog-\nnition, and on multiple datasets.\n7. REFERENCES\n[1] Joakim And ´en and St ´ephane Mallat. Multiscale scattering for\naudio classiﬁcation. In Proceedings of the 12th International\nConference on Music Information Retrieval (ISMIR) , 2011.\n[2] P.J. Burt and E.H. Adelson. The laplacian pyramid as a com-\npact image code. IEEE Transactions on Communications ,\n31:532–540, 1983.\n[3] Adam Coates and Andrew Y . Ng. The importance of encod-\ning versus training with sparse coding and vector quantiza-\ntion. In Proceedings of the 28th International Conference on\nMachine Learning (ICML) , 2011.\n[4] Adam Coates and Andrew Y . Ng. Learning feature represen-\ntations with k-means. Neural Networks: Tricks of the Trade,\nReloaded , 2012.\n[5] Adam Coates, Andrew Y . Ng, and Honglak Lee. An analy-\nsis of single-layer networks in unsupervised feature learning.\nJournal of Machine Learning Research - Proceedings Track ,\n15:215–223, 2011.\n[6] G. E. Dahl, Dong Yu, Li Deng, and A. Acero. Context-\ndependent pre-trained deep neural networks for large-\nvocabulary speech recognition. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 20(1):30–42, January\n2012.\n[7] Sander Dieleman, Phil ´emon Brakel, and Benjamin\nSchrauwen. Audio-based music classiﬁcation with a\npretrained convolutional network. In Proceedings of the 12th\nInternational Conference on Music Information Retrieval\n(ISMIR) , 2011.\n[8] R ´emi Foucard, Slim Essid, Mathieu Lagrange, Ga ¨el Richard,\net al. Multi-scale temporal fusion by boosting for music clas-\nsiﬁcation. In Proceedings of the 12th International Confer-\nence on Music Information Retrieval (ISMIR) , 2011.[9] Jacob Goldberger, Sam Roweis, Geoff Hinton, and Ruslan\nSalakhutdinov. Neighbourhood components analysis. In Ad-\nvances in Neural Information Processing Systems 17 , pages\n513–520, 2004.\n[10] Philippe Hamel, Yoshua Bengio, and Douglas Eck. Building\nmusically-relevant audio features through multiple timescale\nrepresentations. In Proceedings of the 13th International\nConference on Music Information Retrieval (ISMIR) , 2012.\n[11] Philippe Hamel, Simon Lemieux, Yoshua Bengio, and Dou-\nglas Eck. Temporal pooling and multiscale learning for auto-\nmatic annotation and ranking of music audio. In Proceedings\nof the 12th International Conference on Music Information\nRetrieval (ISMIR) , 2011.\n[12] Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A\nfast learning algorithm for deep belief nets. Neural Computa-\ntion, 18(7):1527–1554, 2006.\n[13] Eric J. Humphrey, Juan P. Bello, and Yann LeCun. Moving\nbeyond feature design: Deep architectures and automatic fea-\nture learning in music informatics. In Proceedings of the 13th\nInternational Conference on Music Information Retrieval (IS-\nMIR) , 2012.\n[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-\nsiﬁcation with deep convolutional neural networks. In Ad-\nvances in Neural Information Processing Systems 25 , 2012.\n[15] Edith Law and Luis von Ahn. Input-agreement: a new mech-\nanism for collecting data using human computation games. In\nProceedings of the 27th international conference on Human\nfactors in computing systems , 2009.\n[16] Honglak Lee, Peter Pham, Yan Largman, and Andrew Ng.\nUnsupervised feature learning for audio classiﬁcation using\nconvolutional deep belief networks. In Advances in Neural\nInformation Processing Systems 22 . 2009.\n[17] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units im-\nprove restricted boltzmann machines. In Proceedings of the\n27th International Conference on Machine Learning (ICML-\n10), 2010.\n[18] Malcolm Slaney, Kilian Q. Weinberger, and William White.\nLearning a metric for music similarity. In Proceedings of the\n9th International Conference on Music Information Retrieval\n(ISMIR) , 2008.\n[19] Yichuan Tang and Abdel rahman Mohamed. Multiresolu-\ntion deep belief networks. In Proceedings of the 15th Inter-\nnational Conference on Artiﬁcial Intelligence and Statistics\n(AISTATS) , 2012."
    },
    {
        "title": "Swara Histogram Based Structural Analysis And Identification Of Indian Classical Ragas.",
        "author": [
            "Pranay Dighe",
            "Harish Karnick",
            "Bhiksha Raj"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417841",
        "url": "https://doi.org/10.5281/zenodo.1417841",
        "ee": "https://zenodo.org/records/1417841/files/DigheKR13.pdf",
        "abstract": "This work is an attempt towards robust automated analysis of Indian classical ragas through machine learning and signal processing tools and techniques. Indian classical music has a definite heirarchical structure where macro level concepts like thaats and raga are defined in terms of micro entities like swaras and shrutis. Swaras or notes in Indian music are defined only in terms of their relation to one another (akin to the movable do-re-mi-fa system), and an inference must be made from patterns of sounds, rather than their absolute frequency structure. We have developed methods to perform scale-independent raga identification using a random forest classifier on swara histograms and achieved state-of-the-art results for the same. The approach is robust as it directly works on partly noisy raga recordings from Youtube videos without knowledge of the scale used, whereas previous work in this direction often use audios generated in a controlled environment with the desired scale. The current work demonstrates the approach for 8 ragas namely Darbari, Khamaj, Malhar, Sohini, Bahar, Basant, Bhairavi and Yaman and we have achieved an average identification accuracy of 94.28% through the framework.",
        "zenodo_id": 1417841,
        "dblp_key": "conf/ismir/DigheKR13",
        "keywords": [
            "Indian classical ragas",
            "machine learning",
            "signal processing",
            "scale-independent identification",
            "random forest classifier",
            "swara histograms",
            "Youtube videos",
            "partly noisy recordings",
            "state-of-the-art results",
            "8 ragas"
        ],
        "content": "SWARA HISTOGRAM BASED STRUCTURAL ANALYSIS AND\nIDENTIFICATION OF INDIAN CLASSICAL RAGAS\nPranay Dighe1, Harish Karnick1, Bhiksha Raj2\n1. Indian Institute of Technology, Kanpur, India. 2. Carnegie Mellon University, Pittsburgh PA, USA\npranay.dighe@idiap.ch, hk@cse.iitk.ac.in, bhiksha@cs.cmu.edu\nABSTRACT\nThis work is an attempt towards robust automated analysis\nof Indian classical ragas through machine learning and sig-\nnal processing tools and techniques. Indian classical mu-\nsic has a deﬁnite heirarchical structure where macro level\nconcepts like thaats and raga are deﬁned in terms of mi-\ncro entities like swaras and shrutis. Swaras or notes in\nIndian music are deﬁned only in terms of their relation to\none another (akin to the movable do-re-mi-fa system), and\nan inference must be made from patterns of sounds, rather\nthan their absolute frequency structure. We have devel-\noped methods to perform scale-independent raga identiﬁ-\ncation using a random forest classiﬁer on swara histograms\nand achieved state-of-the-art results for the same. The ap-\nproach is robust as it directly works on partly noisy raga\nrecordings from Youtube videos without knowledge of the\nscale used, whereas previous work in this direction often\nuse audios generated in a controlled environment with the\ndesired scale. The current work demonstrates the approach\nfor 8 ragas namely Darbari, Khamaj, Malhar, Sohini, Ba-\nhar, Basant, Bhairavi and Yaman and we have achieved\nan average identiﬁcation accuracy of 94:28% through the\nframework.\n1. INTRODUCTION\nMusic information retrieval(MIR) is an active and grow-\ning ﬁeld of research as most music today whether available\nover the internet or otherwise is in digital form. The im-\nportant feature of classical music, both western and Indian,\nthat distinguishes it from other kinds of music is that it is\nsupported by a proper well established theory and rules.\nThere has been a lot of work on content analysis of west-\nern classical music in terms of information retrieval, genre\ndetection and instrument/singer identiﬁcation etc. for ex-\nample (see [1], [3], [2]). Though Indian Classical music\nis also a major form of music, the current literature related\nto it is very limited, in comparison to its western coun-\nterpart. Indian classical music is known for its technical\nsoundness and its well deﬁned structure. A basic musi-\ncal performance unit, akin to a song, is a raga. A raga\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.has layers of additional ﬁner structure that can help in con-\ntent identiﬁcation and classiﬁcation, yet these aspects have\nbeen under-utilised by the music analysis community.\nWhile an expert in Indian Classical music can identify a\nraga just by noticing the unique constituent patterns such as\nswaras (notes), arohan, avarohan andpakad in the perfor-\nmance (explained later), developing computational mod-\nels for the same has been a challenging task for music re-\nsearchers. The freedom that Indian classical music pro-\nvides to an artist to give his/her own personal ﬂavour to a\nraga makes it harder for a novice to identify two different\nperformances of the same raga. Another key challenge is\nthe fact that swaras are deﬁned only in terms of their rela-\ntion to one another, and inferences must be made from pat-\nterns of sounds, rather than their absolute frequency struc-\nture. This motivated us to move away from diretly trying\nto identify notes as features and conceptualize various fea-\ntures based on swaras and their structural form.\n2. APPLICATIONS\nAutomatic Tagging/Annotation : Continued digitisation\nof old archives of classical music and the sole availabil-\nity of newer classical performances in digital form has re-\nsulted in huge digital databases of music. Automatic con-\ntent tagging of this unorganised media is important to gen-\nerate metadata for available data and thus facilitate creation\nof easily accessible databases. Bertin-Mehiuex et al. have\nalready created a tool [7] for music tagging, but the work\ndoes not include Indian ragas.\nMusic Recommendation System : Based on an user’s ear-\nlier choices of music, a music recommendation system ﬁl-\nters out and recommends music through tags or content\nanalysis. In the Indian system ragas are associated with\nemotions, time of the day, and seasons of the year. For\nqueries based on such criteria, a proper recommendation\nsystem can make appropriate choices using raga identiﬁ-\ncation as a base.\nMusic Tutoring/Correctness Detection System : A prob-\nabilistic raga identiﬁcation system can be modiﬁed to a tu-\ntoring [9] or correctness detection system as well. While\nachieving this for a complex professional musical perfor-\nmance might be difﬁcult, yet using it for cleaner and sim-\npler instrumental versions of a raga may give positive re-\nsults. Mistakes like skipping some swaras or a particular\nrule not being followed, can be identiﬁed by such a tutor-\ning system.Raga Generation : Within some broad rules, Indian clas-\nsical music allows a performer to modify the components\nof ragas according to his creativity to create his own per-\nsonalised performance of that raga. Generative models in-\nduced from human raga performances can be used to syn-\nthesize new raga performances that can be made unique or\npersonalized by injecting controlled randomness or varia-\ntions.\n3. RELATED WORK\nFor a novice human ear, the basic way of recognising a raga\nis to correlate two tunes on the basis of how similar they\nsound. A trained expert on the other hand looks for char-\nacteristic phrases like arohanam, avrohanam, pakad and\nconstituent swaras, gamakas etc. to arrive at a conclusion\nabout the raga. This well deﬁned manner of raga identiﬁca-\ntion using the above properties has motivated researchers\nto conceptualize computational models for them.\nSahasrabuddhe and Upadhye [10] (1992) modelled a\nraga as a ﬁnite state automaton based on the swara con-\nstituent sequential patterns. Pandey et al. (2003) [11] ex-\ntended this idea of swara sequence in the “Tansen” raga\nrecognition system where they worked with Hidden Markov\nModels on swara sequences. These swara sequences were\nextracted using two methods- hill peak heuristic and note\nduration heuristic. They also employed two separate pakad\nmatching algorithms that improved the HMM based re-\nsults. The ﬁrst algorithm used substring matching for pakad\nidentiﬁcation and the second algorithm was based on count-\ning occurences of n-grams of frequencies in the pakad.\nTansen was able to perform with an accuracy of 87% on\na dataset of two ragas.\nIn [13] Sreedhar and Geetha created a database of ragas\nand used the scale of the raga performance as the similarity\nmetric. Within a scale, notes are matched with the exist-\ning sets of notes in the ragas in the database. The closest\nraga in the database is given as the output for the test raga.\nChrodia and Rae [12](2007) derived Pitch-class Distribu-\ntions (PCD) and Pitch-class Dyad Distributions (PCDD)\nfrom Harmonic Pitch Class Proﬁles (HPCP) and used these\ndistributions for classiﬁcation using SVMs. They achieved\nstate-of-the-art results with accuracies of 78% for PCDs\nand 97.1% for PCDDs. The dataset they had used con-\nsisted of 17 ragas played by a single artist.\nInspired by the use of HMMs over swara sequences and\nPCDs and PCDDs in [11] and [12], we propose a new ap-\nproach to raga identiﬁcation using swara based features ex-\ntracted from chromagrams. Similar to HPCP features, we\nextract feature vectors from chromagram patterns (details\nlater). But instead of learning probability distributions and\nusing their parameters for an SVM (support vector ma-\nchine) based classiﬁcation as in [12], we modify them to\nextract Swara based features that are then used for raga\nmodelling and identiﬁcation.\nThe novelty in our approach is in performing raga iden-\ntiﬁcation without the knowledge of the scale of the perfor-\nmance. We employ swara based features extracted from\nthe chromagram using the concept of vadi (explained later)and perform random forest classiﬁer based classiﬁcation to\nachieve state-of-the-art results.\n4. INDIAN CLASSICAL MUSIC: BASIC\nSTRUCTURAL ELEMENTS\nThe theory of Indian classical music discussed in this chap-\nter is based on the texts [8] of well known Indian musicol-\nogist and scholar Vishnu Narayan Bhatkhande. We ﬁrst\ndiscuss some important concepts of Indian classical mu-\nsic that are needed to understand the methods used in this\nwork.\n4.1 Swaras\nSwaras (or notes) correspond to the frequencies being per-\nformed vocally or by a musical instrument. Collectively\nthe seven Swaras are symbols used for a set of frequency\nvalues. They are the constituent units of a raga and thus act\nas an alphabet. They are closely related to the solfege (do\nre mi fa so la ti) in western music. Basically, there are 7\nswaras in Indian classical music: Shadja (Sa), Rishab (Re),\nGandhara (Ga), Madhyama (Ma), Panchama (Pa), Dhai-\nvata (Dh), and Nishad (Ni) . These swaras are related to\neach other by the ﬁxed ratio of absolute frequency values\nthey denote. Similar to notes in western music, we get the\nsame swara one octave above or below a particular swara .\nThe notes Sa, Re, Ga, Ma, Pa, Dha and Ni are today\nknown as ”shuddha svaras” or ”pure notes”. This expres-\nsion is used in contrast to ”vikrta svaras” or ”altered notes”.\nThus Re may be ﬂattened to get the vikrta swara ”Komal\nRe” and Ma can be ”sharpened” to get the vikrata swara\n”tivra” Ma. SaandPaonly have the Shuddha (pure) form,\nwhile the rest have variants in Tivra (sharp) or Komal (soft)\nforms. Table 1 describes relations between the various\nswaras and similarity to western notes if C is labelled as\nSa.\nTable 1 . Scale of 12 Swaras used in Ragas (if the note C is\nlabelled as the swara Sa)\nHindustani Name (Symbol) Solfa Scale of C Ratio to Sa\nShadja (Sa) Doh C 1\nKomal Rishabh (Re) C#,Db 256/243\nShuddha Rishabh (Re) Re D 9/8\nKomal Gandhr (Ga) D#,Eb 32/27\nShuddha Gandhr (Ga) Mi E 5/4\nShuddha Madhyam (Ma) Fa F 4/3\nTvra Madhyam (M) F#,Gb 45/32\nPancham (Pa) Sol G 3/2\nKomal Dhaivat (Dha) G#,Ab 128/81\nShuddha Dhaivat (Dha) La A 5/3\nKomal Nishd (Ni) A#,Bb 16/9\nShuddha Nishd (Ni) Ti B 15/8\nShadja (Sa) Doh C’ 2\nThe swaras are related to each other through frequency\nvalue ratios. If the swara Sa is assigned some frequency\nvalue in hertz, the rest of the swaras are spread around Sa\nas per the ratios governing them. For example the swara\nPais always3\n2times Sain terms of actual frequencies.\nThe 12-note system which is mostly used with Saas the\n1stnote and Nias12this described in table 4.1 in terms of\nthe frequency ratios.4.2 Raga\nA raga (Sanskrit meaning - ”color/hue”) is a complex mel-\nodic construction over swaras meant to induce a speciﬁc\nemotional response i.e. color the mind of the listener with\na particular emotion.\nA raga is not just a simple collection of swaras , but is\ntypically identiﬁed by swara patterns, the presence or ab-\nsence of certain swaras , the possible ornamentation ( alank-\nars) applied on these swaras and their relative importance\nwhile playing the raga.\nHere we discuss some relevant raga related terminology.\n4.2.1 Arohan, Avrohan and Pakad\nThough two ragas may have the same constituent swaras ,\na unique differentiation between them is the ascending and\ndescending sequences of swaras termed as arohan andava-\nrohan respectively. Pakad is a small sequence of swaras in\na raga that acts as a signature for the raga and an artist often\nvisits and revisits the pakad over a performance.\n4.2.2 Vadi and Samvadi\nVadi is the most prominent swara in a raga and is often de-\nscribed as the King of swaras for that raga. The swara sec-\nond to vadi in importance is called the samvadi . An artist\nstays at the vadi andsamvadi for signiﬁcant durations and\nemphasises them in a performance. The swaras other than\nvadi andsamvadi which constitute the raga are called anu-\nvadi swaras whereas the swaras which are totally absent\nare called vivadi .\n4.2.3 Scale of the Raga - Tonic Frequency\nAn artist always tunes his whole raga performance around\na ﬁxed tonic frequency which she chooses according to her\nown comfort. This frequency deﬁnes the scale in which\nthe raga will be performed. Irrespective of the absolute\nvalue of this tonic frequency, it is termed as the swara Sa.\nThe rest of the swaras get aligned in accordance with their\nratios with the swara Sa.\n4.2.4 Jati- Audhav/Shadav/Sampoorna\nRagas are classiﬁed based on the number of swaras in the\narohan and avrohan. Sampoorna is all 7 swaras, shadhav\nis 6 swaras, audhav is 5 swaras and Surtar is 4 swaras. So,\nan audhav-audhav raga has 5 swaras in its arohan and 5\nswaras in its avrohan.\n5. FEATURE EXTRACTION\n5.1 Chromagram\nA chromagram [6] is a visual representation of energies\nin the 12 semitones(or chromas) of the western musical\noctave namely C, C#, D, D#, E, F, F#, G, G#, A, A# and\nB. So, it basically depicts the distribution of energy in the\ntwelve pitch classes.\nThe western semitones are such that they are ﬁxed with\nrespect to absolute frequency values and the musical oc-\ntave has the property that the semitone one octave below or\nabove is equivalent to the current semitone. For example,\nFigure 1 .Chromagram and Swara sequence for an arohan\nof Raga Bhairavi\nif one note has a frequency of 440 Hz, the note an octave\nabove it is at 880 Hz, and the note an octave below is at\n220 Hz. Since the semitones get repeated in each octave\nabove and below, the energies in the chromagram for each\nsemitone(chroma) is computed by wrapping and adding it\nup over different octaves.\nFigure 1 depicts a chromagram generated from an aro-\nhanof raga Bhairavi. The arohan for raga bhairavi is:\nSa Re-Kom Ga-Kom Ma Pa Dh-Kom Ni-Kom Sa\nTheSa swara ofBhairavi coincides with the semitone\nG and the rest of the swaras in the arohan get aligned with\nthe semitone pattern in the chromagram. Note that, though\nthis notes-to-swara alignment is not perfect and energies\nmay spill over to the adjacent bins, still a signature pattern\nvery close to the Bhairavi arohan is visible. This alignment\nis deduced using the swara and western seminotes equiva-\nlence discussed above and in Table 4.1.\nThese observations and previous use of chromagrams in\naudio analysis [14] and chord recognition [15] motivated\nus to use the chromagram to extract information about swa-\nras. For processing chromagrams, the MIRToolbox [16],\nan open source toolbox for musical feature extraction was\nused to extract features.\n5.2 Swara Histograms\nFrom the chromagram, we extract the semitone with max-\nimum energy in each frame and get a sequence of semi-\ntones for the raga. Though these sequences of semitones\nmight have some identifying information about the raga,\nusing them for raga identiﬁcation is not appropriate since\nragas are deﬁned over patterns of swaras that in turn may\nget associated with different semitones depending on the\ntonic frequency selected by the artist for a particular per-\nformance.\nIn our approach, we assume that we do not have in-\nformation about the tonic frequency of the raga perfor-\nmance. We must therefore ﬁnd the mapping from the ab-\nsolute frequency scale employed by the chromagram to the\nrelative scale of the musical piece, so that the swara se-\nquence can be identiﬁed. To do so, we use the concept of\nvadi discussed earlier to convert the semitone sequence to\naswara sequence. We compute the most frequently occur-ring semitone from the semitone sequence and associate it\nwith the vadi of a raga which is known for each raga. For\nexample vadi for raga Khamaj is swara “Ga”. In an audio\nof Khamaj, if the semitone C# is most prominent, we label\nswara “Ga” at semitone C# and then convert the rest of the\nsemitone sequence into a swara sequence.\nThe above procedure is raga speciﬁc, i.e.the conver-\nsion from semitone sequence to swara sequence utilizes\nthe identity of the raga-speciﬁc vadi swara. Assume that\nwe are building a system for nragas and the actual raga\nfor the test audio is not known, then for the given audio,\nwe must compute separate swara transcriptions for each\nof the nragas. They are combined into an uniﬁed rep-\nresentation in the algorithm below. The concatenation of\n12-dimensional normalized frequency vectors N0\nisis done\nso as to capture the underlying raga’s behavior even in the\ncases when it is being transcripted using a vadi of some\nother raga.\nAlgorithm 1 Computation of Swara based Features\nStep 1: From a raga audio, extract 3 minutes long snip-\npets.\nStep 2: Compute the snippet’s western semitone tran-\nscription Tfrom the chromagram by assigning each\nframe the highest energy semitone.\nStep 3: Find the most frequently occuring semitone sin\ntranscription T.\nStep 4: Compute the snippet’s swara transcription Si\nfor each raga Riwhere i= 1::nby matching semitone\nsto vadi viof raga Ri.\nStep 5: From each transcription Si, create a 12-\ndimensional normalized frequency vector Ni, that is a\nhistogram of swaras over the 3 minute snippet.\nStep 6: Concatenate all such Ni’s to get k-dimensional\nfeature vector where k= 12\u0002n. We call this feature the\nswara histogram and use classiﬁcation trees to identify\nthe raga.\n6. EXPERIMENTS\n6.1 Raga Dataset\nThere does not exist any standard database for raga iden-\ntiﬁcation yet and the current dataset has been collected\nfrom YouTube videos. We include both vocal and instru-\nmental performances by artists. In terms of quality, the\nrelatively newer raga performances whose digital record-\nings are available have cleaner audio on YouTube. The\nolder recordings do not have very clear vocal or instrument\nsounds. We believe that the methods used here should be\nunaffected by these. We demonstrate our system on 8 ragas\nwhose details are in Table 2.\nRaga Number of Recordings Time (in minutes)\nBahar 16 109\nBasant 13 125\nBhairavi 18 162\nDarbari 14 125\nKhamaj 13 91\nMalhar 13 124\nSohini 14 121\nYaman 16 151\nTable 2 .Details of the Raga Dataset Used6.2 Random Forest Classiﬁer over Swara Histograms\nData: Swara histograms were created for raga audios us-\ning Algortihm 1. For each snippet of 3 minutes, a hop\nfactor of 0.025 seconds was used which gave 7200 frames.\nEach frame was tagged with a swara. We created a his-\ntogram of the 12 swaras over these 7200 frames and then\nnormalised it to get swara frequencies that added up to 1.\nAs explained earlier, for each snippet, we got 8 swara tran-\nscriptions corresponding to each possible raga. After con-\ncatenation of histograms from each such transcription, we\nﬁnally got a 96 dimensional histogram feature for the snip-\npet.\nRandom Forest Conﬁguration: An ensemble of 100\ntrees was grown using the random forest algorithm [18].\nFor each tree, if there are N datapoints in the data, we\nchoose N datapoints from them with repetition. This is\ncalled a bootstrap sample. Within each tree, when splitting\ncriteria for a decision at a node are being calculated, a ﬁxed\nnumber of dimensions are randomly sampled from the 96\ndimensions. This number has to be less than the total num-\nber of dimensions and the best split on these dimensions\nis used to split the node. We take the square root of the\nnumber of dimensions i.e. dp\n96e= 10 for this.\n7. RESULTS AND ANALYSIS\n7.1 Results for Random Forest Classiﬁer Experimrent\nA 10-fold cross validated experiment has been done and\nthe average accuracy attained for the 8 ragas is 94:28% .\nApart from Khamaj, all ragas have been identiﬁed with an\naccuracy of more than 90%.\nRaga Da Kh Ma So Ba Bh Bs Ya\nDa 95.96 0.83 0.77 0.00 0.00 0.00 0.83 1.60\nKh 1.11 88.00 2.22 0.00 0.00 2.11 0.00 6.56\nMa 0.77 0.00 91.86 0.00 8.00 4.17 0.00 3.21\nSo 0.00 0.00 0.00 96.67 0.00 1.11 0.00 2.22\nBa 0.00 0.91 0.91 0.00 96.36 0.00 0.00 1.82\nBh 4.23 0.00 0.00 0.00 0.59 94.56 0.00 1.82\nBa 0.00 0.00 0.00 1.11 1.11 1.11 95.56 1.11\nYa 0.67 2.67 0.67 0.00 0.00 1.33 0.00 94.67\nTable 3 .Confusion Matrix for the 8 ragas\nWe make some important observations here:\n1. Raga Malhar is being confused as Bahar and Bhaira-\nvi with 8%and 4%misclassiﬁcations respectively.\nThe fact that all three of them have the same vadi\n(Ma) and samvadi (Sa) is the likely reason this hap-\npens.\n2. Similarly, Khamaj and Yaman where 6:5%of Kha-\nmaj test cases are misclassiﬁed. Again, both have\nthe same vadi (Ga) and samvadi (Ni) pair.\n3. Raga Sohini is the only raga among the three which\nemploys 6 swaras ) (a class of ragas called shadav )\nwhereas the rest are sampoorna and have 7 notes.\nThis is a possible reason for its superior performance.\n7.2 Plots for Swara Histograms\nThe swara histogram features have been plotted for all the\n8 ragas and they reveal exact correspondence of our swara(a) Swara Histogram for Bahar\nSwara Actual Plot\nVadi-Sam Ma-Sa Ma-Sa\nAnuvadi Re,Ga-k,Pa,Dh,Ni-k Ni,Ga,Ga-k,Ni-k,Dh-k,Re\nVivadi Re-k,Ga,Ma-t,Dh-k Ma-t,Re-k\n(b) Plot Observations vs. Actual Swaras in Bahar\n(c) Swara Histogram for Basant\nSwara Actual Plot\nVadi-Sam Sa-Pa Sa-Pa\nAnuvadi Re-k,Ga,Ma-t,Pa,Dh-k,Ni Dh-k,Ma-t,Ma,Re-k\nVivadi Re,Ga-k,Ma,Dh,Ni-k Re,Ga-k,Ga,Dh,Ni-k,Ni\n(d) Plot Observations vs. Actual Swaras in Basant\n(e) Swara Histogram for Bhairavi\nSwara Actual Plot\nVadi-Sam Ma-Sa Ma-Sa\nAnuvadi Re-k,Ga-k,Pa,Dh-k,Ni-k Ni,Re-k,Pa,Ga,Ga-k\nVivadi Re,Ga,Ma-t,Dh,Ni ReDh-k\n(f) Plot Observations vs. Actual Swaras in Bhairavi\n(g) Swara Histogram for Darbari\nSwara Actual Plot\nVadi-Sam Re-Pa Re-Ga\nAnuvadi Sa,Ga-k,Ma,Dh-k,Ni Re-k,Dh,Ni-k,Ni,Ga-k,Pa,Ma-t,Sa\nVivadi Re-k,Ga,Ma-t,Dh,Ni Dh-k,Ma\n(h) Plot Observations vs. Actual Swaras in Darbari\nFigure 2 .Swara Histogram Plot for Bahar, Basant, Bhairavi\nand Darbari\n(a) Swara Histogram for Khamaj\nSwara Actual Plot\nVadi-Sam Ga-Ni Ga-Dh\nAnuvadi Sa,Re,Ma,Pa,Dh,Ni-k Ni,Dh,Re-k,Dh-k,Ma-t,Re,Pa,Sa\nVivadi Re-k,Ga-k,Ma-t,Dh-k Ni-k,Ma\n(b) Plot Observations vs. Actual Swaras in Khamaj\n(c) Swara Histogram for Malhar\nSwara Actual Plot\nVadi-Sam Ma-Sa Ma-Sa\nAnuvadi Re,Ga,Pa,Dh,Ni-k,Ni Ga-k,Ni,Ni-k,Pa,Ga\nVivadi Re-k,Ga-k,Ma-t,Dh-k Re-k,Re,Ma-t,Dh\n(d) Plot Observations vs. Actual Swaras in Malhar\n(e) Swara Histogram for Sohini\nSwara Actual Plot\nVadi-Sam Dh-Ga Dh-Ma\nAnuvadi Sa,Re-k,Ma-t,Ni Ma,Dh-k,Re-k,Ga,Ga-k,Ni-k\nVivadi Re,Ga-k,Ma,Dh-k,Pa,Ni-k Sa,Re,Ma-t,Pa,Ni\n(f) Plot Observations vs. Actual Swaras in Sohini\n(g) Swara Histogram for Yaman\nSwara Actual Plot\nVadi-Sam Ga-Ni Ga-Ni\nAnuvadi Sa,Re,Ma-t,Pa,Dh Ga-k,Ma-t,Dh-k,Dh,Re-k\nVivadi Re-k,Ga-k,Ma,Dh-k,Ni-k Sa,Re,Pa,Ni-k\n(h) Plot Observations vs. Actual Swaras in Yaman\nFigure 3 .Swara Histogram Plot for Darbari, Malhar, Sohini\nand Yamanextraction approach to the musical theory of vadi, sam-\nvadi, anuvadi and vivadi. The plots are bar graphs of the\nswaras’ relative frequency with respect to each other for\neach graph. The plots also strongly suggest the use of clas-\nsiﬁcation methods based on swara histograms. We have\ncompared the observations we made from the plots with\nwhat the actual theory says about that raga in the tables\nbelow the histogram plots.\nWe note that while creating swara histograms, we had\nforcefully aligned the most frequent note with the vaadi.\nSo, in all the plots, the vadi suggested by the plot matches\nwith the actual vadi of the raga. What is important is that\nin most ragas, there is signiﬁcant resemblence between the\nplot observations and the actual samvadi, anuvadi and vi-\nvadi. The discrepancies can be again attributed to the fact\nthat actual raga performances are much more complex and\ncontain variations that are not present in the written form\nof the raga. Since these discrepancies are present in the\ntraining as well as test data, it should have only a small\nimpact on the results.\n8. CONCLUSION AND FUTURE WORK\nOur approach to raga identiﬁcation relies heavily on the\ntheoretical base for Indian classical music. The whole work\nconﬁrms the premise that Indian classical music has a very\nwell deﬁned structure. The whole concept of a complex\nraga being built out of small substructures of arohan, avro-\nhan and pakad which in turn are made up of swaras can be\nefﬁciently modelled efﬁciently if treated and analysed us-\ning these hierarchies. Breaking the ragas into swaras and\nsuccessful identiﬁcation can help to develop highly accu-\nrate applications for automatic tagging, raga tutoring, mu-\nsic recommendation, and possibly, raga generation.\nWe achieved an average accuracy of 94:28% which is\nthe best current result for scale independent raga identiﬁ-\ncation beating the previous best by Chordia [12]. We have\nachieved good accuracies in an experiment which excludes\ncommon audio features like MFCC, supports our initial hy-\npothesis that the ﬁner swara based substructure contains\nthe deﬁning information about ragas.\nMost earlier approaches use raga audios which are per-\nformed with a speciﬁc tonic frequency by artists in con-\ntrolled environments usually using only instrumental mu-\nsic. In comparison, we have attempted raga recognition\nwithout the knowledge of the tonic frequency, on perfor-\nmances downloaded from YouTube videos that have a good\nmixture of vocal (male and female) as well as instrumental\nmusic.\nCompeting with expert human identiﬁcation is the ﬁnal\ngoal in raga identiﬁcation and it can always be assumed\nthat professional artists can perform this task with 100%\naccuracy. So, there is still scope for improvement in the re-\nsults. Future work also lies in fusing the above approaches\nto achieve more efﬁciency and expanding our dataset to\nmany more ragas. We also wish to investigate structure dis-\ncovery through minimum entropy or maximum-structure\nlearning methods. Chroma and swara features can also be\ntested for classiﬁcation of genres and taals (rhythms).9. REFERENCES\n[1] Martin, Keith D., Eric D. Scheirer, and Barry L. Vercoe. “Music con-\ntent analysis through models of audition.” Proc. ACM Multimedia\nWorkshop on Content Processing of Music for Multimedia Applica-\ntions. 1998.\n[2] Tzanetakis, G.; Cook, P., “Musical genre classiﬁcation of audio sig-\nnals,” Speech and Audio Processing, IEEE Transactions on , vol.10,\nno.5, pp.293,302, Jul 2002.\n[3] G. Poliner, D. Ellis, A. Ehmann, E. Gmez, S. Streich, B. Ong,\n“Melody Transcription from Music Audio: Approaches and Eval-\nuation,” IEEE Transactions on Audio, Speech, Language Processing\n2006.\n[4] Phon-Amnuaisuk, S and Keh-Siong Chee, “Interactivities in mu-\nsic intelligent tutoring system”, In proc. of Fifth IEEE International\nConference on Advanced Learning Technologies, 2005.\n[5] Schulze, W. “Music Generation with Markov Models”, In IEEE Mul-\ntiMedia, March 2011, V ol 18 ,Issue 3, Page 78 - 85.\n[6] Dan Ellis, “Chroma Feature Analysis and Synthesis”, Resources\nof Laboratory for the Recognition and Organization of Speech and\nAudio-LabROSA,\n[7] T. Bertin-Mahieux, D. Eck and M. Mandel, Automatic tagging of au-\ndio: The state-of-the-art. In Wenwu Wang, editor, Machine Audition:\nPrinciples, Algorithms and Systems. IGI Publishing, 2010.\n[8] Vishnu Bhatkhande, “A comparative Study of the Music Systems of\nthe 15th, 16th, 17th and 18th Centuries”.\n[9] S. Belle, R. Joshi, and P. Rao, Raga Identiﬁcation by using Swara\nIntonation, Journal of ITC Sangeet Research Academy, 2009, vol.\n23.\n[10] H. Sahasrabuddhe and R. Upadhye, “On the computational model\nof raag music of india.” in Workshop on AI and Music: European\nConference on AI, 1992.\n[11] G. Pandey, C. Mishra, and P. Ipe, “Tansen: A system for automatic\nraga identiﬁcation” in Proc. of Indian International Conference on\nArtiﬁcial Intelligence , 2003, pp. 1350-1363.\n[12] P. Chordia and A. Rae, “Raag recognition using pitch-class and\npitch-class dyad distributions,” in Proc. of ISMIR, 2007, pp. 431-\n436.\n[13] R. Sridhar and T. Geetha, “Raga identiﬁcation of carnatic music for\nmusic information retrieval,” International Journal of Recent trends\nin Engineering, vol. 1, no. 1, 2009, pp. 571-574.\n[14] Xiaoqing Yu; Jing Zhang; Junwei Liu; Wanggen Wan; Wei Yang,\n“An audio retrieval method based on chromagram and distance met-\nrics,” International Conference on Audio Language and Image Pro-\ncessing (ICALIP), Nov. 2010.\n[15] K.Lee, “Automatic Chord Recognition from Audio Using Enhanced\nPitch Class proﬁle,” in Proc. of the International Computer Music\nConference, 2006.\n[16] Olivier Lartillot, Petri Toiviainen, “A Matlab Toolbox for Musical\nFeature Extraction From Audio”, International Conference on Digi-\ntal Audio Effects, Bordeaux, 2007.\n[17] G. Koduri, S. Gulati and P. Rao,“A Survey Of Raaga Recognition\nTechniques And Improvements To The State-Of-The-Art,” Sound\nand Music Computing, 2011.\n[18] Leo Breiman and E. Schapire, ”Random forests”, Machine Learn-\ning,2001,pages 5–32."
    },
    {
        "title": "Improved Audio Classification Using a Novel Non-Linear Dimensionality Reduction Ensemble Approach.",
        "author": [
            "Stéphane Dupont",
            "Thierry Ravet"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417705",
        "url": "https://doi.org/10.5281/zenodo.1417705",
        "ee": "https://zenodo.org/records/1417705/files/DupontR13.pdf",
        "abstract": "Two important categories of machine learning methodologies have recently attracted much interest in classification research and its applications. On one side, unsupervised and semi-supervised learning allow to benefit from the availability of larger sets of training data, even if not fully annotated with class labels, and of larger sets of diverse feature representations, through novel dimensionality reduction schemes. On the other side, ensemble methods allow to benefit from more diversity in base learners though larger data and feature sets. In this paper, we propose a novel ensemble learning approach making use of recent non-linear dimensionality reduction methods. More precisely, we apply t-SNE (t-distributed Stochastic Neighbor Embedding) to a large feature set to come up with embeddings of various dimensionality. A k-NN classifier is then obtained for each embedding, leading to an ensemble whose estimates can then be combined, making use of various ensemble combination rules from the literature. The rationale of this approach resides in its potential capacity to better handle manifolds of different dimensionality in different regions of the feature space. We evaluate the approach on a transductive audio classification task, where only part of the whole data set is labeled. We confirm that dimensionality reduction by itself can improve performance (by 40% relative), and that creating an ensemble through the proposed approach further reduces classification error rate by about 10% relative.",
        "zenodo_id": 1417705,
        "dblp_key": "conf/ismir/DupontR13",
        "keywords": [
            "unsupervised",
            "semi-supervised",
            "dimensionality reduction",
            "ensemble methods",
            "t-SNE",
            "k-NN classifier",
            "ensemble combination",
            "manifolds",
            "classification error rate",
            "transductive audio classification"
        ],
        "content": "IMPROVED AUDIO CLASSIFICATION USING A NOVEL NON-LINEAR\nDIMENSIONALITY REDUCTION ENSEMBLE APPROACH\nSt´ephane Dupont\nUniversity of Mons\nstephane.dupont@umons.ac.beThierry Ravet\nUniversity of Mons\nthierry.ravet@umons.ac.be\nABSTRACT\nTwo important categories of machine learning method-\nologies have recently attracted much interest in classiﬁca -\ntion research and its applications. On one side, unsuper-\nvised and semi-supervised learning allow to beneﬁt from\nthe availability of larger sets of training data, even if not\nfully annotated with class labels, and of larger sets of di-\nverse feature representations, through novel dimensional -\nity reduction schemes. On the other side, ensemble meth-\nods allow to beneﬁt from more diversity in base learners\nthough larger data and feature sets. In this paper, we pro-\npose a novel ensemble learning approach making use of\nrecent non-linear dimensionality reduction methods. More\nprecisely, we apply t-SNE (t-distributed Stochastic Neigh -\nbor Embedding) to a large feature set to come up with em-\nbeddings of various dimensionality. A k-NN classiﬁer is\nthen obtained for each embedding, leading to an ensemble\nwhose estimates can then be combined, making use of var-\nious ensemble combination rules from the literature. The\nrationale of this approach resides in its potential capacit y\nto better handle manifolds of different dimensionality in\ndifferent regions of the feature space. We evaluate the ap-\nproach on a transductive audio classiﬁcation task, where\nonly part of the whole data set is labeled. We conﬁrm\nthat dimensionality reduction by itself can improve perfor -\nmance (by 40% relative), and that creating an ensemble\nthrough the proposed approach further reduces classiﬁca-\ntion error rate by about 10% relative.\n1. INTRODUCTION\nFeature transformation and dimensionality reduction\napproaches have attracted a lot of interest as pre-processo rs\nin classiﬁcation problems, including in the area of multi-\nmedia information retrieval. In general, they are able to\nThis research has received funding from R´ egion Wallonne (B el-\ngium) through the Numediart long-term research programme ( grant nbr.\n716631) and the MediaWorkﬂows project (grant nbr. 1117549) , and\nfrom the European Union Seventh Framework Programme throug h the\ni-Treasures project (grant agreement no. 600676).\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage an d that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieva l.reduce correlation of feature dimensions as well as noise.\nThey also help to tackle the issues related to the curse of\ndimensionality, to avoid over-ﬁtting on the training data,\nand to reduce the computational cost of the classiﬁcation\nscheme.\nUnsupervised non-linear dimensionality reduction\nschemes have shown their beneﬁt in semi-supervised\nlearning problems for classiﬁcation [11, 23]. This follows\nfrom the so-called cluster and manifold assumptions. In\nthe ﬁrst, it is assumed that data samples are organized into\ndistinct clusters and that samples from different classes\nbelong to different clusters. In the second, it is assumed\nthat data samples from different classes occupy distinct\nmanifolds of lower dimensionality in the original feature\nspace. If one of these assumptions holds, the more the\navailable data can be used, the better these cluster or\nmanifold structures can be discovered, to the beneﬁt of\nclassiﬁcation accuracy.\nEnsemble approaches constitute another popular re-\nsearch theme in machine learning and classiﬁcation prob-\nlems. They consists in training a set of diverse estimators\n(referred to as base learners) for the same problem, and\ncombine their estimates or decisions when new data sam-\nples have to be classiﬁed [24]. These have become popular\nwith approaches such as bagging [2] and boosting [18] (f.i.\nAdaBoost), to name a few.\nIntuitively, in order to gain accuracy when combining\nsuch estimators, these have to be different. This has led\nto research into generating base learners that are as divers e\nas possible, by acting on one or more of the factors that\nwill have an effect on the end result of the learning pro-\ncess [4, 24]. There is literature on using different subsets\nof the training data for each member of the ensemble, on\nmanipulating the parameters involved in the training (up\nto the architecture of the individual learners), or on using\ndifferent output representations.\nMethods for altering the input feature space have also\nbeen researched. In this paper, we propose to make use\nof recent developments in the area of unsupervised non-\nlinear dimensionality reduction in order to alter the featu re\nspace and extract low-dimensional embedding whose di-\nmensions can be used to deﬁne multiple classiﬁers. Their\nestimates are then combined through an ensemble scheme.\nPrevious attempts and popular methods in creating en-\nsemble diversity through feature transformations and di-\nmensionality reduction will ﬁrst be summarized in Sec-\ntion 2. Section 3 will then describe the proposed approach,and in particular introduce the non-linear dimensionality\nreduction algorithm that have been applied (t-SNE, or t-\ndistributed Stochastic Neighbor Embedding), the way it\nis used to obtain multiple classiﬁers, and the combina-\ntion rules used to obtain the ensemble decision. We then\napply this method on a use case of interest to the cre-\native community concerned with the classiﬁcation of mu-\nsical instrument loops used in rhythmic music composi-\ntion/production. Section 4 presents the experimental pro-\ntocol and evaluation metrics, as well as the experimental\nresults, together with a discussion. We conclude the paper\nin Section 5.\n2. ENSEMBLE METHODS AND FEATURE\nMANIPULATIONS\nIf earlier proposals in ensemble learning relied on select-\ning different subset of the training data (bagging or boost-\ning) for each ensemble member, subsequent research has\nexplored various approaches for transforming and manipu-\nlating the available feature set, including feature select ion\nor more generally supervised and unsupervised dimension-\nality reduction. This is summarized in chronological order\nin the following paragraphs.\nWith the Random Subspace (RS) method [9, 17], ran-\ndom subsets of the original features are presented to the\nclassiﬁers. This was followed by a more general approach\ncalled Random Forests (RF) [3], combining with the idea\nof bagging, to end up with ensembles of classiﬁers (ini-\ntially decision trees, hence the name ”forest”) constructe d\nfrom random samplings on both features and data.\nSelecting features after PCA has been proposed in [12],\nwith ensembles where each constituent classiﬁer is trained\non a user-determined number of principal components. Su-\npervised dimensionality reduction approaches have also\nlead to some ensemble learning trials. In [13], Input Dec-\nimation (ID) is proposed. Its goal is to decouple the clas-\nsiﬁers by exposing them to different features. The method\ndoes so by training Nclassiﬁers ( Nbeing the number of\nclasses of the problem) and selecting for each the input fea-\nture dimensions (a user-determined number of them) hav-\ning the highest absolute correlation to the presence or ab-\nsence of the corresponding class. In [14], the ID-based\napproach was shown to compare favorably with the PCA-\nbased approach. One explanation is that unsupervised di-\nmensionality reduction approaches such as PCA are not\nwell suited for ﬁnding features useful for classiﬁcation as\nthey totally disregard class information. Remember how-\never that random or unsupervised feature selection also\nwork in some contexts.\nRather than selecting feature dimensions randomly as\nin RS, or making use of feature transformations, simply\nprojecting on randomly deﬁned axes as also been proposed\nwith the Random Projections (RP) approach [7, 20].\nIn [16], the ideas of RS and PCA are combined to lead\nto Rotation Forests (RotF), where the feature set is ran-\ndomly split into a number of subsets and PCA is applied to\nthese. Diversity-error diagrams revealed that RotF-based\nensembles construct individual classiﬁers which are moreaccurate than these in AdaBoost and RF, and more diverse\nthan these in Bagging, sometimes more accurate as well.\nMore recently [1], it is proposed to make use of Diffu-\nsion Maps (DM) [10], a non-linear dimensionality reduc-\ntion scheme, and to develop classiﬁers based on the trans-\nformed space dimensions. The approach is compared to\nRP, RS and RF methods cited above, as well as Bagging\nand Boosting (through the AdaBoost algorithm). A multi-\nstrategy approach combining DM and Boosting was shown\nto be superior to other algorithms in many cases.\nEnsemble methods can sometimes look like an art. This\nis without accounting for the theoretical considerations a nd\ndevelopments that participate to this research area. One\narea concerns the study of so-called diversity metrics and\ndiversity generation approaches. All these are however out\nof the scope of this paper, and the interested reader may\nrefer to recent literature for more information [19, 24].\n3. PROPOSED APPROACH\nIn this paper, we propose to make use of recent develop-\nments in non-linear dimensionality reduction approaches\nin order to obtain several sets of features enabling the de-\nvelopment of an ensemble of classiﬁers. This hence fol-\nlows up on the literature summarized in the previous sec-\ntion. An earlier proposal was indeed reported in [1], with\nthe use of Diffusion Maps. Here, we will make use of t-\nSNE (t-distributed Stochastic Neighbor Embedding), a re-\ncent method. As explained in [22], t-SNE is less suscep-\ntible than other classical approaches (including Diffusio n\nMaps) to assigning much higher importance to modeling\nthe large pairwise distances than the small ones. Hence, it\nis better at retaining the local structure, which is deﬁnite ly\nthought to be beneﬁcial in visualization but also classiﬁca -\ntion problems.\nIn a previous paper [6], we showed on two semi-\nsupervised classiﬁcation tasks that t-SNE (even when re-\nducing to a very low dimensional space) can perform as\nwell and sometimes even better than classiﬁcation in the\noriginal high-dimensional feature space.\n3.1 Dimensionality Reduction using t-SNE\nThe popularity of approaches derived from Multidimen-\nsional Scaling (MDS) has inspired variants, in particular\nthrough methods attempting to preserve local properties of\nthe data in a ”softer” probabilistic fashion. In particular ,\nSNE (Stochastic Neighbor Embedding) tries to preserve\nneighborhood identity [8]. It does so using a cost function\nthat favors the probability distributions of points belong -\ning to the neighborhoods of other points to be similar in the\nhigh-dimensional space and in its low-dimensional embed-\nding. In the original formulation, a Kullback-Leibler (KL)\ndivergence is used to measure that similarity, and probabil -\nities for a sample to belong to a neighborhood of another\none is based on Gaussian distributions.\nMore recently, a symmetric version of SNE has been\nproposed. It has also been proposed to use a Student-\nt distribution rather than a Gaussian distribution to com-pute the similarity between pairs of samples in the low-\ndimensional space. These modiﬁcations have lead to the t-\nSNE [22] method. The t-Student heavy-tailed distribution\nin the low-dimensional space signiﬁcantly alleviate the so -\ncalled ”crowding” problem observed with SNE where far\naway data samples, for instance low density areas in be-\ntween natural clusters, come close together in the low-\ndimensional embedding.\nIn details, we ﬁrst estimate the (symmetric) probability\nthat sample xiin the high-dimensional space would pick\nsamplexjas its neighbor using the following expression:\npij=exp/parenleftBig\n−/bardblxi−xj/bardbl2\n2σ2\ni/parenrightBig\n/summationtext\nk/negationslash=lexp/parenleftBig\n−/bardblxk−xl/bardbl2\n2σ2\ni/parenrightBig (1)\nwhereσiis the standard deviation of a Gaussian centered\nonxi. Similarly, we model the probability that yi, the low\ndimensional counterpart of xi, would take yjas its neigh-\nbor using the following (symmetric) expression:\nqij=/parenleftBig\n1+/bardblyi−yj/bardbl2/parenrightBig−1\n/summationtext\nk/negationslash=l/parenleftBig\n1+/bardblyk−yl/bardbl2/parenrightBig−1(2)\nwhere the model of proximity is Student-t distributed. t-\nSNE then proposes to ﬁnd a representation for which the\nprobabilities qijare faithful to pij. This is achieved by\nminimizing the mismatch between qijandpijmeasured\nusing a KL-divergence:\nC=KL(P/bardblQ) =/summationdisplay\ni/summationdisplay\njpijln/parenleftbiggpij\nqij/parenrightbigg\n(3)\nIfPirepresents the probability distribution of pijover\nall data points given point xi, t-SNE ﬁrst performs a bi-\nnary search for the value of σiproducing a Piwith a ﬁxed\nperplexity speciﬁed by the user, where the perplexity is de-\nﬁned based on the Shannon entropy of Pimeasured in bits.\nThe minimization of the cost function in Equation 3 is\nperformed using a gradient descent method.\nIn our experiments, the perplexity of the conditional\nprobability distribution was set to 20; and we performed\n2000 iterations of gradient descent. Also, we used the re-\nﬁnements proposed in [22], including a momentum term in\nthe gradient descent, as well a tricks referred to as ”early\ncompression” and ”early exageration” in [22].\n3.2 Ensemble of t-SNE Features\nBy varying the parameters involved in dimensionality re-\nduction through t-SNE, it is possible to come with several\ndiverse feature representations of the data samples, and to\nobtain a classiﬁer for each of these. In particular, it is pos -\nsible to either (1) alter some of the meta-parameters of t-\nSNE learning, and in particular the size of the local neigh-\nborhood (perplexity of the conditional probability distri -\nbution), (2) alter the number of dimensions preserved by\nt-SNE, (3) alter the high-dimensional input features used\nas input to t-SNE, and in particular select different subset sfrom the initial feature set, for instance, as in the Random\nProjection approach, or through more principled feature\ngroupings.\nHere, we have been using the later two approaches.\nClassiﬁcation tasks can beneﬁt from dimensionality reduc-\ntion, which is sometimes presented as enabling the reduc-\ntion of noise and unimportant details in the data, while pre-\nserving the multi-dimensional manifold structures. There\nis however a tradeoff between more denoising through\nlower dimensional target spaces, and better preservation\nof the inherent dimensions of the data. The optimal choice\nmay depend on the selected class of the problem, or on the\nconsidered regions of the space. One assumption is that\nensemble methods making use of classiﬁers obtained from\nvarious choices of target space dimensionality are able to\nmitigate this tradeoff. Experimental results on the pro-\nposed classiﬁcation task will show that some classes in-\ndeed strongly beneﬁt from dimensionality reduction, while\nothers do much less, or not at all. Combining the obtained\nclassiﬁers leads to improvement over the single best one.\nThe details of the experimental setup and of the way the\nmultiple classiﬁers are obtained are provided in Section 4.\n3.3 Combination Rules\nAs soon as the different classiﬁer are available, several ap -\nproaches are possible for ”combining” the individual esti-\nmations they provide. Suppose we have Kclassiﬁers avail-\nable. In this work, we consider classiﬁers that provide esti -\nmates of the posterior probabilities of each class. The ”sum\nrule” consists in averaging these posteriors for each class ,\npossibly using a weight dependent on the classiﬁer. It fol-\nlows from considering classiﬁcation as a regression prob-\nlem on posterior probability estimates, and beneﬁt from the\nliterature on ensemble combination through the averaging\nof various estimates [24]:\nP(q|x) =K/summationdisplay\nk=1αkPk(q|x) (4)\nwhereqis the class label, xthe feature vector, and Pk(q|x)\nthe posterior probability for class qassigned by classiﬁer\nk.\nThe ”product rule” consists of a product of probability\nestimates for each class. If follows from an independence\nassumption. When one has available several classiﬁers\nmaking use of distinct and statistically independent featu re\ndescriptors, the posterior probability of classes can eas-\nily be computed from the a priori probabilities of classes\nand posteriors estimated by the different classiﬁers. Let\nx= (x1,...,xK)be the feature vector built from Kinde-\npendent sub-vectors. Bayes rules tells us:\nP(q|x) =P(x|q)P(q)\nP(x)(5)\nAssuming that the different subparts of the feature vectorare statistically independent, we successively get:\nP(q|x) =P(q)\nP(x)/producttextK\nk=1P(xk|q)\n=P(q)\nP(x)/producttextK\nk=1P(q|xk)P(xk)\nP(q)\n=/bracketleftBig/producttextK\nk=1P(xk)\nP(x)/bracketrightBig/bracketleftBig/producttextK\nk=1P(q|xk)\n(P(q))(K−1)/bracketrightBig(6)\nThe ﬁrst term is independent of k, and if the initial inde-\npendence assumption holds, its value will be 1. In prac-\ntice, the posterior probability will be estimated based on\nthe second term normalized in such a way that the sum of\nestimates for all classes is the unity.\nBesides averaging, the use of order statistics has also\nbeen proposed in the literature [21]. The ”maximum rule”\nconsists in approximating the posterior probability for ea ch\nclass using the maximum of the various classiﬁer estimates\nfor this class:\nP(q|x) =Kmax\nk=1Pk(q|x) (7)\nThe ”minimum rule” follows a similar principle:\nP(q|x) =K\nmin\nk=1Pk(q|x) (8)\nFinally, the ”median rule” is expressed as:\nP(q|x) =medK\nk=1Pk(q|x) (9)\nThese ﬁve approaches have been compared in this work.\nMajority voting is another popular approach, but it has not\nbeen used in this work as it can not beneﬁt from posterior\nestimates.\n4. EXPERIMENTS\nAs data set, we used a production music library (ZeroG\nProPack). This library contains more than ten thousand\n”loops” and samples of various instruments and music\nstyles. Each soundﬁle is typically a few seconds long of\nmonophonic or polyphonic sound (f.i. in the case of gui-\ntars). We manually annotated the ﬁles within 7 classes of\ninstruments: Brass, Drums, V ocals, Percussion, Electric\nBass, Acoustic Guitar and Electric Guitar. After discardin g\nmore complex sounds or effects, we ended up with 4380\nsamples to be used in our evaluations.\nThe experimental work that follows is based on a trans-\nductive classiﬁcation task. It hence considers a closed dat a\nset that has to be classiﬁed with minimal effort. Part of the\ndata is hence annotated with class labels to guide the su-\npervised machine learning, but all the data set can be used\nin an unsupervised mode. Transductive learning has many\ninteresting applications [5].\nhttp://www.zero-g.co.uk/\nData sets deﬁnitions and labels available as supplementary material\nat http://www.numediart.org/tools/mediacycle/4.1 Low-level Features for Audio and Music\nA large body of recent work in the music information re-\ntrieval literature has been devoted to the design of fea-\nture extraction algorithms for the purpose of character-\nizing, analyzing, searching or classifying audio content.\nHere, we consider timbral properties. Audio analysis ap-\nproaches for extracting feature descriptors rely on isolat -\ning and analyzing short-term windows of temporal signal\n(typically around 30 ms long), to end up with one feature\nvector per window. For representing and be able to clas-\nsify longer-term signals as used in our experiments, we ex-\ntracted statistics (up to order 4) from the short-term win-\ndow feature vectors. From previous research, we ended-up\nusing two groups of features, covering the spectral enve-\nlope and the noisiness of the sounds, both being important\nfor characterizing the perceived timbre. The state-of-the -\nart feature set that we used contains:\n•Mel-Frequency Cepstral Coefﬁcients (MFCC) as\nused in [15], computed using 30 ms frames every\n10 ms, using a ﬁlterbank of 20 ﬁlters covering the\naudible frequency range, and keeping the ﬁrst 12 co-\nefﬁcients. To be able to capture the temporal char-\nacteristics and statistics of the MFCCs, we actually\nused as features the MFCCs means along the sample\nduration, as well as their standard deviation, skew-\nness and kurtosis; the means of the ﬁrst order tempo-\nral derivatives of the MFCCs, as well as their stan-\ndard deviation, and the means of the second order\ntemporal derivatives of the MFCCs, as well as their\nstandard deviation.\n•Spectral Flatness (SF), which is a correlate of the\nnoisiness (opposite of sinusoidality) of the spectrum\ncomputed on the same audio frames as MFCCs. It\nis computed as the ratio between the geometric and\narithmetic means of the spectrum energy values. As\nproposed in [15], the spectrum was divided into 4\nsub-bands for computing the ﬂatness: 250-500Hz,\n500-1000Hz, 1000-2000Hz and 2000-4000Hz. Here\ntoo, we used the mean of the SF over the sound\nextract duration, as well as its standard deviation,\nskewness and kurtosis.\n4.2 Experimental Protocol\nWe are interested in semi-supervised transductive classiﬁ -\ncation, where only part of the whole corpus can be anno-\ntated with the desired class labels. We hence performed ex-\nperiments with different percentages of randomly selected\nlabeled data (from 10% to 50%, f.i. 10% means that only\n438 samples have been labeled using their instrument class\nin the production music database). Being unsupervised, t-\nSNE is always making use of the whole data set however.\nFor each training condition, we ran 100 different training\nand evaluation batches (with selected labeled data random-\nized for each of them) for each classiﬁcation system (either\nsingle classiﬁer, or various ensemble conﬁgurations). The\nclassiﬁers (either individual classiﬁers or classiﬁers ta king\npart in the ensembles) are using k-NN (with k=5).(a) (b) (c) (d) (e)00.10.20.30.40.5Classification error rateClassification results with Ensemble Methods on the Propack database\n  \n(a)&(b)\nSum rule\nProduct rule\n Maximum rule\n Minimum rule\nMedian rule\nFigure 1 . Classiﬁcation error rates when making use of\n10% of labeled data: (a) using the full high-dimensional\nfeature set (b) using a 5-dimensional feature set obtained\nthrough t-SNE on the full high-dimensional feature set\n(best single classiﬁer from different t-SNE based classi-\nﬁers with various target space dimensionalities) (c) En-\nsemble composed of 5 classiﬁers obtained using 1 to 5-\ndimensional feature sets obtained through t-SNE on the\nfull high-dimensional feature set; various combination\nrules (d) Ensemble classiﬁer composed of 15 classiﬁers: 5\ntarget dimensionalities x 3 sub features set (mean, standar d\ndeviation, and high-order statistics of the baseline high-\ndimensional feature set) (e) Ensemble classiﬁer composed\nof 20 classiﬁers: 5 target dimensionalities x 4 sub features\nset (MFCCs, MFCCs ﬁrst and second derivatives, SF).\nThe baseline classiﬁcation system uses the high-\ndimensional features set described earlier. We normalized\neach feature to zero-mean and unity-variance. We then cre-\nated various classiﬁers used standalone, or involved in en-\nsemble conﬁgurations according to three principles:\n•creating various classiﬁers by altering the dimen-\nsionality of the t-SNE embedding from 1 to 5. This\nenabled the design of an ensemble of 5 classiﬁers.\n•creating various classiﬁers by altering both the di-\nmensionality of the t-SNE embedding and the input\nfeatures of t-SNE. Rather than selecting random sub-\nset of feature dimensions as done in the RD or RS\napproaches, we partitioned the full feature set ac-\ncording to the order of the statistics used to repre-\nsent the sound ﬁles. More precisely, three subspaces\nwere obtained, one gathering the means of the raw\nfeature vectors, a second one for the standard devi-\nations, and a third one gathering the skewness and\nkurtosis. This enabled the design of an ensemble of\n15 classiﬁers: 5 target dimensionalities x 3 subsets\nof features.\n•similar to the previous approach but where the cat-\negory of the feature is use to deﬁne the feature par-\ntitions. More precisely, we split the individual fea-\ntures into four groups: MFCCs, MFCCs ﬁrst deriva-\ntives, MFCCs second derivatives, and SF. This en-\nabled the design of an ensemble of 20 classiﬁers: 5\ntarget dimensionalities x 4 subsets of features.Brass Drums Vocals Percussion El. bass Ac. Guitar El. guitar00.10.20.30.40.50.6Classification error rateClassification by instrument type\n  \nHigh dimension\ntSNE 5D\nEnsemble Method: tSNE 1D−5D \n(maximum rule)\nFigure 2 . Classiﬁcation error rates when using 10% of la-\nbeled data on each instrument class. Comparison between:\n(1) using the full high-dimensional feature set, (2) using a\n5-dimensional feature set obtained through t-SNE on the\nfull high-dimensional feature set, (3) Ensemble composed\nof 5 classiﬁers obtained using 1 to 5-dimensional feature\nset obtained through t-SNE on the full high-dimensional\nfeature set; maximum rule for combination.\n0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.500.020.040.060.080.10.120.140.160.180.2\nlabelled data ratioClassification error ratekNN test on the ProPack database (5 nearest neighbours)\n  \nHigh dimension\ntSNE 5D\nEnsemble Method: tSNE 1D 5D(maximum rule)\nFigure 3 . Classiﬁcation error rates for various propor-\ntions of labeled data. Comparison between: (1) us-\ning the full high-dimensional feature set, (2) using a 5-\ndimensional feature set obtained through t-SNE on the full\nhigh-dimensional feature set, (3) Ensemble composed of 5\nclassiﬁers obtained using 1 to 5-dimensional feature set ob -\ntained through t-SNE on the full high-dimensional feature\nset; maximum rule for ensemble combination.\n4.3 Results and Discussion\nIn Figure 1, we present classiﬁcation results for the case\n10% of the whole data set is labeled. It shows results for\nthe baseline system, for a system where features are ﬁrst\nprocessed using t-SNE (with dimensionality of 5), as well\nas for various ensembles and combination rules. We can\nobserve that on this kind of data, an efﬁcient dimension-\nality reduction scheme is a useful pre-processing step for\nsemi-supervised classiﬁcation. Classiﬁcation performan ce\non the reduced dimensional space is indeed better. A 40%\nrelative reduction of the error rate is obtained.\nThe ﬁrst proposed ensemble approach yields a further\nerror rate reduction of 10% relative. All ﬁve combina-\ntion rules bring some improvement, but best results are\nobtained using the simple maximum rule, followed by the\nsum rule. The two other proposed ensembles are uncon-\nclusive, and the product and minimum combination rules\nperform notably much worse. This can be explained bythe discrepancy in classiﬁcation performance of the ensem-\nble members: base classiﬁers using the standard deviations\nof features in the ﬁrst case, and using SF features alone\nin the second are much worse than the other base classi-\nﬁers (detailed results not reported here). More complex or\nweighted combination rules may help in those cases.\nOverall, the best single classiﬁer is using a 5-\ndimensional feature set obtained through t-SNE on the full\nhigh-dimensional feature set. The best ensemble classi-\nﬁer is composed of 5 classiﬁers obtained using 1 to 5-\ndimensional feature sets obtained through t-SNE on the\nfull high-dimensional feature set; and a maximum rule\nfor ensemble combination. We then present more de-\ntailed comparisons of these two with the baseline clas-\nsiﬁer In Figure 2, we observe that some classes indeed\nstrongly beneﬁt from dimensionality reduction, while oth-\ners do much less, or not at all. As suggested earlier in\nthe text, the tradeoff between reducing the feature space\ndimension and preserving its representativity may depend\non the class and on the region of the feature space. This\nalso suggests further theoretical and empirical work in en-\nsemble approaches that could account for non-uniform in-\ntrinsic dimensionalities of the data set manifolds. In Fig-\nure 3, we present results for various proportions of labeled\ndata, showing that our conclusions hold when one can ob-\ntain labels for a larger part of the data set. With 50% of\nlabeled date, t-SNE allows to reduce the error rate by 18%\nrelative over high-dimensional features, and the ensemble\napproach further reduces it by 14% relative.\n5. CONCLUSIONS\nIn this paper, we presented a new method for designing\nmultiple classiﬁers system relying on non-linear dimen-\nsionality reduction through t-SNE, together with an experi -\nmental study of its performance on an audio-based musical\ninstruments transductive classiﬁcation task. We ﬁrst ob-\nserved that classiﬁcation performance can be boosted when\napplying t-SNE as a pre-processing step, even when going\ndown to as low as a few dimensions. Designing multiple\nclassiﬁers by altering the dimensionality of the t-SNE em-\nbedding and combining them using a simple combination\nrule further improved the results.\nThese promising initial results invite further work, in\nparticular in the application of other dimensionality re-\nduction schemes and more complex ensemble combina-\ntion rules, as well as in understanding how ensembles can\nbe used for mitigating the tradeoff between denoising and\nfeature preservation properties. The application of the pr o-\nposed approach to larger scale data sets can also be the sub-\nject of future work, together with experimental evaluation\non non-transductive tasks using out-of-sample extensions .\n6. REFERENCES\n[1] Amir Amit. Ensemble Classiﬁcation via Dimensionality R eduction.\nMaster’s thesis, Eﬁ Arazi School of Computer Science, Israe l, 2011.\n[2] Leo Breiman. Bagging predictors. Technical Report 421, Department\nof Statistics, University of California, Berkeley, Califo rnia, Septem-\nber 1994.[3] Leo Breiman. Random forests. Mach. Learn. , 45(1):5–32, October\n2001.\n[4] Gavin Brown, Jeremy L. Wyatt, Rachel Harris, and Xin Yao. Diver-\nsity creation methods: a survey and categorisation. Information Fu-\nsion, 6(1):5–20, 2005.\n[5] O. Chapelle, B. Sch¨ olkopf, and A. Zien, editors. Semi-Supervised\nLearning . MIT Press, Cambridge, MA, 2006.\n[6] St´ ephane Dupont, Thierry Ravet, C´ ecile Picard, and Ch ristian Fris-\nson. Nonlinear dimensionality reduction approaches appli ed to music\nand textural sounds. In Proceeding of IEEE International Conference\non Multimedia and Expo (ICME) , San Jose, California, jul 2013.\n[7] Xiaoli Zhang Fern and Carla E. Brodley. Random projectio n for\nhigh dimensional data clustering: A cluster ensemble appro ach. In\nICML’03 , pages 186–193, 2003.\n[8] Geoffrey Hinton and Sam Roweis. Stochastic neighbor emb edding.\nAdvances in neural information processing systems , 15:833–840,\n2003.\n[9] Tin Kam Ho. The random subspace method for constructing d ecision\nforests. IEEE Trans. Pattern Anal. Mach. Intell. , 20(8):832–844, Au-\ngust 1998.\n[10] S. Lafon and A.B. Lee. Diffusion maps and coarse-graini ng: a uni-\nﬁed framework for dimensionality reduction, graph partiti oning, and\ndata set parameterization. Pattern Analysis and Machine Intelligence,\nIEEE Transactions on , 28(9):1393 –1403, sept. 2006.\n[11] John A. Lee and Michel. Verleysen. Nonlinear dimensionality reduc-\ntion. Springer, New York; London, 2007.\n[12] Christopher J. Merz and Michael J. Pazzani. A principal components\napproach to combining regression estimates. Mach. Learn. , 36(1-\n2):9–32, July 1999.\n[13] Nikunj C. Oza and Kagan Tumer. Dimensionality reductio n through\nclassiﬁer ensembles. Technical Report NASA-ARC-IC-1999- 124,\nNational Aeronautics and Space Administration, Moffett Fi eld, CA,\n1999.\n[14] Nikunj C. Oza and Kagan Tumer. Input decimation ensembl es:\nDecorrelation through dimensionality reduction. In LNCS , pages\n238–247. Springer, 2001.\n[15] Geoffroy Peeters. A large set of audio features for soun d description\n(similarity and classiﬁcation). in the CUIDADO project. Pa ris, IR-\nCAM, 2004.\n[16] Juan J. Rodriguez, Ludmila I. Kuncheva, and Carlos J. Al onso. Rota-\ntion forest: A new classiﬁer ensemble method. IEEE Trans. Pattern\nAnal. Mach. Intell. , 28(10):1619–1630, October 2006.\n[17] Niall Rooney, David Patterson, Alexey Tsymbal, and Sar ab An. Ran-\ndom subspacing for regression ensembles. Technical report , Depart-\nment of Computer Science, Trinity College Dublin, Ireland, 2004.\n[18] Robert E. Schapire. The strength of weak learnability. Mach. Learn. ,\n5(2):197–227, July 1990.\n[19] Robert E. Schapire and Yoav Freund. Boosting: Foundations and Al-\ngorithms . The MIT Press, 2012.\n[20] Alon Schclar and Lior Rokach. Random projection ensemb le classi-\nﬁers. In Joaquim Filipe and Jos´ e Cordeiro, editors, Enterprise Infor-\nmation Systems, 11th International Conference, ICEIS 2009 , Milan,\nItaly, May 6-10, 2009. Proceedings , volume 24 of Lecture Notes in\nBusiness Information Processing , pages 309–316. Springer, 2009.\n[21] Kagan Tumer and Joydeep Ghosh. Linear and order statist ics com-\nbiners for pattern classiﬁcation. CoRR , cs.NE/9905012, 1999.\n[22] Laurens van der Maaten and G.E. Hinton. Visualizing hig h-\ndimensional data using t-sne. Journal of Machine Learning Research ,\n9:2579–2605, 2008.\n[23] L.J.P. van der Maaten, E. O. Postma, and H. J. van den Heri k. Dimen-\nsionality reduction: A comparative review. Technical Repo rt TiCC-\nTR 2009-005, Tilburg University, 2009.\n[24] Zhi-Hua Zhou. Ensemble Methods: Foundations and Algorithms .\nChapman and Hall/CRC, 2012."
    },
    {
        "title": "Modelling the Speed of Music using Features from Harmonic/Percussive Separated Audio.",
        "author": [
            "Anders Elowsson",
            "Anders Friberg",
            "Guy Madison",
            "Johan Paulin"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1414928",
        "url": "https://doi.org/10.5281/zenodo.1414928",
        "ee": "https://zenodo.org/records/1414928/files/ElowssonFMP13.pdf",
        "abstract": "One of the major parameters in music is the overall speed of a musical performance. In this study, a computational model of speed in music audio has been developed using a custom set of rhythmic features. Speed is often associated with tempo, but as shown in this study, factors such as note density (onsets per second) and spectral flux are important as well. The original audio was first separated into a harmonic part and a percussive part and the features were extracted separately from the different layers. In previous studies, listeners had rated the speed of 136 songs, and the ratings were used in a regression to evaluate the validity of the model as well as to find appropriate features. The final models, consisting of 5 or 8 features, were able to explain about 90% of the variation in the training set, with little or no degradation for the test set.",
        "zenodo_id": 1414928,
        "dblp_key": "conf/ismir/ElowssonFMP13",
        "keywords": [
            "speed",
            "music",
            "computational model",
            "rhythmic features",
            "tempo",
            "note density",
            "spectral flux",
            "audio separation",
            "regression",
            "validation"
        ],
        "content": "MODELLING THE SPEED OF MUSIC USING FEATURES \nFROM HARMONIC/PERCUSSIVE SEPARATED AUDIO \n   Anders Elowsson    Anders Friberg        Guy Madison   Johan Paulin \nKTH Royal Institute of Technology,  \nCSC, Dept. of Speech, Music and Hearing  \n          elov@kth.se   afriberg@kth.se  Department of Psychology, Umeå University \n{Guy.Madison,Johan.Paulin}@psy.umu.se  \nABSTRACT \nOne of the major parameters in music is the overall speed \nof a musical performance. In this study, a computational \nmodel of speed in music audio has been developed using \na custom set of rhythmic features. Speed is often associ-ated with tempo, but as shown in this study, factors such as note density (onsets per second) and spectral flux are \nimportant as well. The original audio was first separated \ninto a harmonic part and a percussive part and the fea-tures were extracted separately from the different layers. In previous studies, listeners had rated the speed of 136 \nsongs, and the ratings were used in a regression to evalu-\nate the validity of the model as well as to find appropriate features. The final models, consisting of 5 or 8 features, were able to explain about 90% of the variation in the \ntraining set, with little or no degradation for the test set. \n1. INTRODUCTION \nThis study is focused on one of the major parameters in \nmusic, the overall speed of a musical performance. From \na music theoretic background we are used to associate \nspeed with the tempo of the music. However, as suggest-ed earlier, the perceived speed is related to the tempo but \nmay also depend on other aspects like the note density \n(number of onsets per second) [9]. An indirect indication of this was provided in [2] where it was found that the \nnote density (and not the tempo) was constant for a cer-\ntain emotional expression across different music exam-\nples. Madison & Paulin [12] asked listeners to rate the \nspeed for 50 music examples spanning a variety of musi-cal styles and rhythms. They found that speed correlated \nwith tempo but that there must also be other aspects in-\nvolved in the perceptual judgment of speed. In earlier works [11, 15, 16] it has been shown that a classification \nof songs as fast or slow has helped to improve the accu-\nracy of tempo estimation algorithms. \nThe current work is part of an ongoing study about per-\nceptually determined features in music information re-trieval. In a previous study it was shown that speed could be modeled by a combination of tempo and different note \ndensities of the instruments using symbolic data [7]. The \nexplained variation was about 90 % using linear regres-sion. This indicates that a similar result could in theory be obtained using audio data provided that the appropriate \nlow-level audio features coul d be extracted. Unfortunate-\nly, audio features extracted with the MIRToolbox [14] as well as the VAMP plugins available in the Sonic Annota-\ntor\n1 did not map well to the perception of speed, high-\nlighting the need for new features to be developed [7].  \nThe purpose of the current study was to develop a \ncomputational model of speed in music audio restricted to examples containing percussive elements (e.g. drums). A \nset of rhythmic features were computed, mainly from \ndetected onsets of the music.  An important idea was that \na relevant model should expl oit the characteristics of each \nonset to better understand the music. As indicated in [7], \ngood results can be achieved by tracking both percussive \nand harmonic onsets. Therefore, these parts were ana-lyzed separately in the current model. As a first step, source separation was used to separate harmonic content \nand percussive content in the audio. Onsets and features \nwere computed from both the percussive and the harmon-ic part as well as from the original audio. A flowchart of the processes used is shown in Figure 1.  \nTo find appropriate features as well to evaluate the va-\nlidity of the model, regression was used, in which the \naudio features were mapped against ground truth data consisting of listener ratings of speed. \n2. SOURCE SEPARATION AND ONSET             \nDETECTION \n2.1 HP-Separation \nSource separation has been used in the past in computa-\ntional models related to rhythm [1]. For this study, the \nsource separation method proposed by FitzGerald [6] was \nused to separate harmonic and percussive content. The basic idea of the method is that percussive sounds are broadband noise signals with short duration and that \nharmonic sounds are narrow band signals with longer \nduration. The audio is first transformed to the spectral domain by using a short-time Fourier transform  (STFT). \nBy applying a median filter across each frame in the frequency direction, harmonic sounds are suppressed. By \napplying a median filter acro ss each frequency bin in the \ntime direction percussive sounds are suppressed. After median filtering, the signal is transformed back to the time domain again using the inverse STFT.  \nWith the STFT it is possible to accurately detect per-\ncussive content in the music.  The frequency resolution in \n                                                           \n1 http://www.omras2.org/SonicAnnotator Permission to make digital or hard copies of all or part of this work fo r\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for prof it or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page.  \n© 2013 International Society for Music Information Retrieval the lower frequencies is however not sufficient to detect \nharmonic content there. Thus, to further suppress har-monic content in the percussive waveform a second sepa-\nration stage incorporates a constant-Q transform (CQT) \n[17].  The CQT can be understood as an STFT with loga-rithmically spaced frequency bi ns, accomplished by vary-\ning the length of the analysis window. With the CQT, an appropriate frequency resolution can be achieved at all \nfrequencies, at the expense of a poor time resolution in \nthe low frequencies. The frequency resolution of the CQT was set to 60 bins per octave and each frame was median filtered across the frequency di rection. After filtering, the \npercussive signal was transformed back to the time do-\nmain using an inverse CQT. Notice that the phase infor-mation is retained in the transformation back to the time domain. It can be regarded as a mapping that connects a \nfrequency bin to a certain point in time. The percussive \nand harmonic waveforms are shown in Figure 2. \n2.2 Onset Detection \nAudio features were computed from all three waveforms \n(original, harmonic and percussive) by the scheme shown in Figure 1. The first step, independent of feature and \nwaveform, was to compute a spectral flux (SF) [3], where \nspectral fluctuations along th e time-domain are detected. \nThe SF was computed several times in numerous differ-\nent ways. Some shared steps will be described here, with \nunique steps described in Sections 3.1-3.8.  The power \nspectrum was computed with a CQT or a STFT and con-\nverted to sound level. A range of 30 dB was used. Thus, the maximum sound level of each band was set to 0 dB \nand sound levels below -30 dB were set to -30 dB. Let \nL(n, i) represent the sound level at the  ith frequency \nbin/band of the nth frame. The SF is given by\n \n()\n1() (,) (( ,)b\niSF n H L n i L n s i\n==− −         (1) \nwhere b is the number of bins/bands. The variable s is the  \nstep size and H is a half-wave rectifier function, or for the \npercussive SF: \n    \n0.if 0()if 2 0xHxxx\nx> =≤                       (2) \nThe implication of Eq. 2 is that negative spectral fluctua-\ntions have a slight influence on the onset detection func-tion. Onsets were detected by peak picking on a low-pass \nfiltered curve of the spectral flux (see Figure 2). \n2.3 Clustering \nOnsets were clustered based on sound level in 8 frequen-\ncy bands, spaced approximately an octave apart. An addi-\ntional band was based on the RMS sound level. As the \nappropriate number of clusters was unknown beforehand, three K-means clusterings we re carried out, with the \nnumber of clusters k, set to 2, 3 and 4. The fit of each \nclustering attempt was defined by the smallest Euclidian distance between any two clusters, where a large smallest \ndistance gave a higher fit. When choosing k, a higher \nnumber of clusters were premiered over a lower if their fit was similar. The result of the clustering is a separation \nof onsets into different groups as shown in Figure 2.  \n3. FEATURE EXTRACTION \nA total of 8 audio features were computed, 2 from the \noriginal waveform, 5 from the percussive waveform and \n1 from the harmonic waveform. These features are shown in the flowchart in Figure 1 and described in Sections \n3.1-3.8, with one subsection for each feature.  An in-\ndepth visualization of the processes involved to compute the features is shown in Figure 2. For conversion to onset \ndensity , the length of each song was set as the distance \nbetween the first and last onset. \n3.1 Onset Density – Harmonic \nOnsets were tracked from the original waveform, using \nthe SF of a CQT. To avoid false onset detections at pitch \nglides, deviations in a peak by 20 cents (one bin), without an increase in sound level, we re restricted from affecting \nthe SF. This was accomplished by subtracting the sound \nlevel of each bin of the new frame, by the maximum \nsound level of the adjacent bins in the old frame. \n3.2 Onset Density – Bass \nTo comply with the bass feature in [7], onsets in the low \nregister (40 Hz - 210 Hz) were tracked using the SF of the lower bins of a STFT. The frequency bins were \nsummed to a single band before the SF. \n3.3 Onset Density – Perceptual weighting \nPercussive onsets were tracked using the SF of a STFT \non the percussive waveform. The bins of the frequencyAudio (Original)\nHarmonic Percussive\nPercussiveHP-separation (STFT)\nHP-separation (CQT)SF-CQT\nSF-STFT\nSF-CQT\nSF-STFTOn Det.\nOn Det.\nSF-CQT\nOn Det. ClusteringOn Dens. BassOn Dens.  Harmonic\nOn Dens. Perceptual\nOn Dens. Strong\nStrong Cluster IOITempo \nPercussivenessAudio\nProcess\nFeature\nFigure 1.  Flowchart of the processes used to compute audio features  for the speed in music. The audio is filtered to sepa-\nrate harmonic and percussive content, onsets are detected from a spectral fl ux, and audio features are computed. domain representation were divided into 13 non-\noverlapping frequency bands (half-octave spacing). Sub-band processing for onset detection has been described in [13], and can be motivated by its similarity to human \nhearing [4]. The strength of  each detected onset was cal-\nculated based on the average sound level of the first 22 23 24 25 26 2700.51\n 22 22.5 23 23.5 24 24.5 25 25.5 26 26.5 27\n22 22.5 23 23.5 24 24.5 25 25.5 26 26.5 27Original\nOnsets Harmonic\nOnsets Bass\nPercussive\n22 22.5 23 23.5 24 24.5 25 25.5 26 26.5 27\n22 22.5 23 23.5 24 24.5 25 25.5 26 26.5 27Percussiveness\nOnsets Perceptual\nTempo Onsets Strong & Strong Cluster IOICluster 1 (Kick)\nCluster 2 (Claps)\nCluster 2 (Toms)\nCluster 4 (Shakers)\n22 22.5 23 23.5 24 24.5 25 25.5 26 26.5 27\nSF - CQTHarmonictime (sec)Melody 1\nMelody 2\nBass\nShakers\nToms\nClaps\nKickGround Truth\nFigure 2.  The process of estimating the perceived speed of a piece of music. The example is a 5-second section of the \nsong  Candy Shop , by 50 cent . In the top pane we see the ground truth of the audio file. The melodic lines have been \nconsolidated into a single row to convey only onset times. In the next pane  we see the processes involved in extracting \naudio features from the original waveform. In the third pane the percussive audio is used. Notice that the clustering of the audio matches the ground truth. Tempo  is detected as the IOI between kick an d handclaps. Finally the integral of the \nspectral flux is used from the harmonic waveform in the fourth pane. 50 ms from the onset positio n, where lower frequencies \nwere given a higher impact.  \nTo further determine the perceived strength of the on-\nsets, each onset was compared to the surrounding onsets \nwithin 1.5 seconds. This time span was defined as the perceptual present of the pa rticular onset. By comparing \nit with the strongest onset within the perceptual present \nits strength could be altered to represent its perceptual impact. The onset was given a higher strength if there \nwere no significantly stronger onsets within the perceptu-\nal present. If there were onsets that were significantly \nstronger, its strength was lowered. The height of the clus-\nter-bars in Figure 2 represents the perceptual strength. To derive at a measure of onsets density, the sum of the \nperceptual strength of the onsets was used. \n3.4 Onsets Density – Strong \nThe strongest clusters of the clustering contributed to two \nfeatures. The first feature wa s simply the number of on-\nsets, belonging to a strong cluster, per second. The idea \nbehind this feature is that prominent percussive elements such as the kick drum and the snare drum likely influence \nthe perception of speed in a different way than the less \nprominent elements su ch as the hi-hat. \n3.5 Strong Cluster IOI \nThe second feature derived from the strong clusters was \ndeveloped to catch the assumed perception of a slow \nspeed, when the interonset intervals (IOIs) of onsets be-longing to the same strong cluster are long. As an exam-ple, a song with equally spaced drum onsets consisting of \n“Kick, Snare, Kick, Snare, etc..” was assumed to have a \nhigher  perceived speed than a song where the drums \ninstead plays “Kick, Kick, Sn are, Kick, etc..”. This is \naccounted for in the Tempo feature as well, because the \ntempo in the second example would be half the tempo of \nthe first example. Cluster IOIs shorter than 750 ms were discarded based on the idea that they can both represent a drum fill in a slow song or represent a regular part of the drum pattern in a fast song.  \n3.6 Tempo \nThe tempo detection algorithm is part of an ongoing \nproject, and a detailed description is in preparation. All distances between onsets within 5 seconds from each other are used to detect the tempo. The histogram in Fig-\nure 3 is based on the song presented in Figure 2.  \nFirst, the period length of the percussive waveform is \ndetected. A histogram of onset distances is generated, where the contribution of each  onset-pair is increased \nwith increasing similarity  in spectrum as well as increas-\ning onset strength. The leftmost peak in the low pass \nfiltered histogram, within 92 % of the highest peak, is chosen as the period length.  \nSecondly, the tempo (beat le ngth) is detected. A histo-\ngram over onset distances is once again generated, where the contribution of each onset- pair is increased with in-\ncreasing dissimilarity  in spectrum as well as increasing \nonset strength. The final probability distribution for tem-po (Figure 3) is the Hadamard product of the histogram \nand several filters. One filter is based on the determined \nperiod length. The idea is that the beat will be a simple \nratio of the period length, so Hanning windows are pro-duced at the positions given by \n0,1, 2,.11 1,23.2nn\nlen lenPP n  ×× ×  =\n   (3) \n   Another filter is based on IOIs within strong clusters as \ndescribed in Section 3.5 and several filters are based on \nonset density. The highest peak in the final probability \ndistribution is chosen as the tempo. In compliance with the findings that speed is a shallower function of tempo for fast and slow music [12], differences in tempo be-\ntween 60 and 160 BPM are given the highest impact. \n \nFigure 3.  The histogram used to determine tempo.  \n3.7 Percussiveness \nAn estimate of how percussive the music is was comput-\ned as well. This estimate is derived from the height h of \nthe peaks in the SF of the percussive waveform, as shown \nin Figure 2. Equation 4 gives the mean peak height when \np is 0, an estimate closer to the lowest peaks when p is \nnegative, and an estimate cl oser to the highest peaks \nwhen p is positive. In this study p was set to 0.4. \n1p\n1\np\n1(i)\n(i)n\ni\nn\nih\nPercussiveness\nh+\n=\n==\n                      \n3.8 SF CQT \nWhen extracting information from the harmonic wave-\nform the integral of the SF wa s used; indicated as the area \nin the bottom pane of Figure 2. Onset detection was avoided as the HP-separation had removed all transients from the harmonic waveform. \n4. PREDICTING SPEED FROM THE FEATURES \n4.1 Speed Data and Audio Examples \nThe music examples were taken from two earlier studies. \nTo ensure that the songs c ontained percussive elements, \nsongs where the RMS of the percussive waveform was \nless than 1/8 of the RMS of the harmonic waveform were not included in the data sets. The training set was 89 \npopular songs, originally in MIDI format and converted 0 500 1000 1500 200001   ← Beat length: 0.613 seconds (97.9 BPM)\nLength (ms)Probability\n(4) to audio in a previous experiment [7, 10]. The speed \nestimations were previously determined using 20 listeners \nwho rated speed for each example on a quasi-continuous \nscale marked slow-fast (range 1-9). The test set consisted \nof 47 real audio examples previously used for studying \nthe relation between tempo a nd speed [12]. They were \nselected for exhibiting a large variation of tempi and genres within popular music styles. The speed was previ-\nously estimated in a similar way to the training set using \ncontinuous scales (range 0-10) . Due to a difference in the \ndesign of the original experiment [12], the medium tempo \nexamples were rated by 60 listeners while the fast and \nslow examples were rated by 12 listeners. \n4.2 Modelling Speed of the Training Set \nTwo regression techniques were used to analyze the \nmapping between the computed audio features and the \nlistener ratings. First, a multiple linear regression (MLR) \nwas used, justified by a predictor-to-case ratio higher \nthan 1:10. Secondly, partial least square regression (PLS) \nwas used. PLS regression carries out data reduction, whilst maximizing covariance between features and pre-\ndicted data [5]. It constructs new predictor variables \n(components), as linear combinations of the features.     The MLR prediction of listener ratings from computed \naudio features is presented in Table 1. As shown, a linear \ncombination of the computed audio features was able to explain more than 90 % of the variability. In comparison, \nthe agreement among the listeners, estimated by the mean \nintersubject correlation was 0.71 and Cronbach’s alpha 0.98 [7]. \n8 Features R\n2 = 0.909 Adjusted R2 = 0.900 \nVariable beta sr2 p-value \nOn Dens. - Harmonic 0.205  0.033 0.000*** \nOn Dens. - Bass 0.130  0.007 0.016*   \nOn Dens. - Perceptual 0.302  0.018 0.000*** \nOn Dens. - Strong -0.155  0.010 0.004**  \nStrong Cluster IOI 0.127  0.006 0.021*   \nTempo 0.430  0.056 0.000*** \nPercussiveness -0.095  0.005 0.041*   \nSF CQT  0.107    0.004 0.053    \n5 Features  R2 = 0.887  Adjusted R2 = 0.880\nOn Dens. - Harmonic 0.239 0.049 0.000*** \nOn Dens. - Perceptual 0.224 0.020 0.000*** \nStrong Cluster IOI 0.132 0.007 0.027* \nTempo 0.404 0.053 0.000*** \nSF CQT 0.225 0.032 0.000*** \nTable 1.  MLR prediction of the perceptual feature speed  \nfrom computed audio features. The variable sr2 is the \nsquared semipartial correlation coefficient. \nFor the model based on 8 features, 2 features ( Onset \nDensity – Strong and Percussiveness ) gave a negative \ncontribution. Notice that the difference in explained vari-\nance is only about 2 % between the two models, indicat-ing that the features in the 5-feature model may contain \nalmost all relevant information. \n    The PLS regression of the 8 features is shown in Ta-ble 2. With 3 components, the cross-validated adjusted R\n2 indicates that just below 90 % of the variability could be \nexplained. Note also that the cross-validation procedure \nonly lowers the result marginally, supporting the validity \nof the present features.  \nPLS Regression – Speed (3 PLS-components) \nR2 = 0.907 Adj. R2 = 0.903 Adj. R2 cv = 0.878 \nComponent Explained variance Cum. variance \n1 0.845 0.845 \n2 0.052 0.897 \n3 0.017 0.914 \nTable 2.  PLS prediction of the perceptual feature speed  \nfrom computed audio features. The squared correlation coefficient R\n2 was derived using PLS, including 10-fold \ncross validation (“cv”). Also, R2 as a function of the \nnumber of components is shown. \nThe fitted values of the linear regression from Table 1 \n(8-feature model) are shown in Figure 4 below. As seen \nin the Figure, the deviations from the target are rather \nevenly distributed across th e range and with a maximal \ndeviation of about one unit. \n \nFigure 4.  The fitted values in the MLR prediction of \nperceptual speed, where hi gher means faster. For each \nsong, the x-axis represents the estimated speed and the y-\naxis represents the ground truth (derived from listeners). \n4.3 Predicting Speed of the Test Set \nTwo linear models of speed (5 and 8 features) were de-\nrived from the multiple linear regression analysis of the \ntraining set shown in Table 1. The models were applied \nto the test set and the squared correlation between rated speed and computed speed is shown in Table 3.  \nNo. of Features/Regression coefficients R\n2 \n5 0.934 \n8 0.894 \nTable 3.  The prediction of the perceptual feature speed  \nfrom a linear model using computed audio features.  \nThe 5-feature model’s prediction of speed for each \nsong of the test set is shown in Figure 5. Computed speed 1 2 3 4 5 6 7 8 9123456789\n1\n236\n789\n1112\n1314\n151617\n1920\n21222324\n252627\n2829\n30\n3132\n333435\n363738\n39\n4041\n434445\n46\n4748\n4950\n51\n5253\n5455\n56\n5758\n59\n6061\n62 63\n646566\n676869\n70\n7172\n7375\n7677\n787980\n8283\n848586\n87 88899091\n92949698\n99\nComputed speedRated speedis approximately 1 unit higher than rated speed and this is \nprobably due to the difference s in the music examples of \nthe databases. Furthermore, different scales were used for \nthe listener ratings in the two data sets (1-9 and 0-10).  \n \nFigure 5.  The prediction of the perceptual feature speed, \nwhere higher means faster. For each song, the x-axis represents the estimated speed  and the y-axis represents \nthe ground truth (derived from listeners).  \n5. CONCLUSIONS AND DISCUSSION \nThe models were able to expl ain about 90 % of the varia-\nbility in listener ratings. The most important features were tempo together with onset densities for different \nlayers of the music as well as spectral fluctuations in the \nharmonic part of the audio. The validity of the features was supported by cross-validation, and verified by using \nthe extracted regression coeffi cients from the training set \nto accurately predict speed in the test set.   \nThe results show that it was possible to reach the same \nhigh explained variance on audio data as on the symbolic data in [7] using similar features. This indicates that the \nappropriate low-level audio f eatures have been extracted, \nwhich is reassuring for the ongoing study. The model based on 5 features was able to explain more of the vari-\nance in the test set than th e model based on 8 features. \nThis indicates that the 8-feature model was overfitting the training set. \nThe segmentation of audio (HP-separation and cluster-\ning) seems to be a promising path forward. By clustering onsets we can detect onsets belonging to the same source \nand thus use the rhythmic pattern of this source in the \nmodel. By using several onset detection functions on separate parts of the audio, different aspects of the music \ncan be captured. Source separation can be motivated from \nan ecological perspective; it seems reasonable to assume that listeners distinguish between sounds from different \nsources to better understand the soundscape. A drawback \nwith the proposed system is that the computation of sev-\neral STFTs and CQTs is relatively time consuming.  \nIn future work we intend to include songs without per-\ncussive elements. We also inte nd to investigate other high \nlevel rhythmic features such as rhythmic complexity and \ndynamics. We expect the audio segmentation to be a fruitful way forward. Data from this study is freely avail-\nable for research purposes\n2.   \n6. ACKNOWLEDGEMENT \nThis work was supported by the Swedish Research Coun-\ncil, Grant Nr. 2009-4285 and 2012-4685. \n7. REFERENCES \n[1] M. Alonso, G. Richard and B. David: “Accurate Tempo \nEstimation Based on Harmoni c + Noise Decomposition,” \nJournal on Advances in Signal Processing , 2007. \n[2] R. Bresin, and A. Friberg: “Emotion Rendering in Music: \nRange and Characteristic Values of Seven Musical Variables,” Cortex , Vol. 47, No. 9, pp. 1068-1081, 2011. \n[3] S. Dixon: “Onset dete ction revisited,” In Proc. of DAFx , \npp. 133–137, 2006.  \n[4] C. Duxbury, J. P. Bello, M. Sandler, and M. Davies: “A \nComparison between Fixed and Multiresolution Analysis for Onset Detection in Musical Signals,” In Proc. of \nDAFx,  pp. 207-212, 2004.  \n[5] T. Eerola, O. Lartillot, P. Toiviainen: ”Prediction of \nMultidimensional Emotional Ratings in Music from Audio using Multivariate Regression Models,” In Proc. of  \nISMIR,  pp. 621-626, 2009. \n[6] D. FitzGerald: “Harmonic/Pe rcussive Separation Using \nMedian Filtering,” In Proc. of DAFx,  2010. \n[7] A. Friberg, E. Schoonderwaldt , A. Hedblad, M. Fabiani, \nand A. Elowsson: “Perceptually  derived features can be \nused in music information retrieval,” submitted. \n[8] A. Friberg, E. Schoonderw aldt, and A. Hedblad: \n“Perceptual Ratings of Musi cal Parameters,” In von H. \nLoesch, and S. Weinzierl (Eds.), Gemessene Interpretation \n- Computergestützte Aufführ ungsanalyse im Kreuzverhör \nder Disziplinen,  pp. 237-253, Mainz: Schott, 2011. \n[9] A. Gabrielsson: Studies in rhythm , doctoral dissertation, \nUppsala University , 1973. \n[10] A. Hedblad: Evaluation of musical feature extraction tools \nusing perceptual ratings . Master thesis, KTH Royal \nInstitute of Technology, 2011. \n[11] J. Hockman and I. Fujinaga: Fast vs Slow: Learning \nTempo Octaves from User Data. In  Proc. of ISMIR , pp. \n231-236, 2010. \n[12] G. Madison, and J. Paulin: “Ratings of Speed in Real \nMusic as a Function of both Original and Manipulated Beat Tempo.” Journal of the Acoustical Society of \nAmerica , Vol. 128, No. 5, pp. 3032-3040, 2010. \n[13] A. Klapuri: “Sound Onset Detection by Applying \nPsychoacoustic Knowledge,” In Proc. IEEE Conf. \nAcoustics, Speech and Signal Processing , 1999. \n[14] O. Lartillot and P. Toiviainen: A Matlab Toolbox for \nMusical Feature Extraction from Audio. In Proc. of DAFx , \npp. 237-244, 2007. \n[15] M. Levy: Improving Perceptu al Tempo Estimation With \nCrowd-Sourced Annotations. In Proc. of ISMIR , pp. 317-\n322, 2011. \n[16] G. Peeters and J. Flocon-Cholet: Perceptual Tempo \nEstimation Using GMM Regression. In  Proc. of ACM \nMIRUM , pp. 45-50, Japan, November 2012. \n[17] C. Schörkhuber and A. Klapur i: “Constant-Q Transform \nToolbox for Music Processing,” In 7th Sound and Music Conference, Barcelona, 2010. \n \n                                                           \n2 www.speech.kth.se/music/speed 3 4 5 6 7 8 9 10 11123456789\n1236\n789\n111213\n14\n1516\n17\n19 2021222324\n2526\n2728\n2930 3132\n3334\n3536\n37383940\n41\n43\n444546\n4748\n495051\nComputed speedRated speed"
    },
    {
        "title": "An Extended Audio Fingerprint Method with Capabilities for Similar Music Detection.",
        "author": [
            "Sébastien Fenet",
            "Yves Grenier",
            "Gaël Richard"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418255",
        "url": "https://doi.org/10.5281/zenodo.1418255",
        "ee": "https://zenodo.org/records/1418255/files/FenetGR13.pdf",
        "abstract": "Content-based Audio Identification consists of retrieving the meta-data (i.e. title, artist, album) associated with an unknown audio excerpt. Audio fingerprint techniques are amongst the most efficient for this goal: following the extraction of a fingerprint from the unknown signal, the closest fingerprint in a reference database is sought in order to perform the identification. While being able to manage large scale databases, the recent developments in fingerprint methods have mostly focused on the improvement of robustness to post-processing distortions (equalization, amplitude compression, pitch-shifting,...). In this work, we describe a novel fingerprint model that is robust not only to the classical set of distortions handled by most methods but also to the variations that occur when a title is re-recorded (live vs studio version in particular). As a result our fingerprint method is able to identify any signal that is an excerpt of one of the references from the database or that is similar to one of the references. The issue that we cover thus lies at the intersection of audio fingerprint and cover song detection, meaning that the functional perimeter of our method is substantially larger than the classical audio fingerprint approaches.",
        "zenodo_id": 1418255,
        "dblp_key": "conf/ismir/FenetGR13",
        "keywords": [
            "content-based audio identification",
            "meta-data retrieval",
            "audio fingerprint techniques",
            "database management",
            "robustness to distortions",
            "live vs studio versions",
            "fingerprint model",
            "cover song detection",
            "functional perimeter",
            "audio fingerprint approaches"
        ],
        "content": "Firstname.Lastname@telecom-paristech.fr"
    },
    {
        "title": "Chord-Sequence-Factory: A Chord Arrangement System Modifying Factorized Chord Sequence Probabilities.",
        "author": [
            "Satoru Fukayama",
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417701",
        "url": "https://doi.org/10.5281/zenodo.1417701",
        "ee": "https://zenodo.org/records/1417701/files/FukayamaYG13.pdf",
        "abstract": "This paper presents a system named ChordSequenceFactory for automatically generating chord arrangements. A key element of musical composition is the arrangement of chord sequences because good chord arrangements have the potential to enrich the listening experience and create a pleasant feeling of surprise by borrowing elements from different musical styles in unexpected ways. While chord sequences have conventionally been modeled by using Ngrams, generative grammars, or music theoretic rules, our system decomposes a matrix consisting of chord transition probabilities by using nonnegative matrix factorization. This enables us to not only generate chord sequences from scratch but also transfer characteristic transition patterns from one chord sequence to another. ChordSequenceFactory can assist users to edit chord sequences by modifying factorized chord transition probabilities and then automatically re-arranging them. By leveraging knowledge from chord sequences of over 2000 songs, our system can help users generate a wide range of musically interesting and entertaining chord arrangements.",
        "zenodo_id": 1417701,
        "dblp_key": "conf/ismir/FukayamaYG13",
        "keywords": [
            "ChordSequenceFactory",
            "automatic generation",
            "chord arrangements",
            "musical composition",
            "chord sequences",
            "nonnegative matrix factorization",
            "musical styles",
            "unexpected borrowing",
            "pleasant feeling",
            "musically interesting"
        ],
        "content": "CHORD-SEQUENCE-FACTORY: A CHORD ARRANGEMENT SYSTEM\nMODIFYING FACTORIZED CHORD SEQUENCE PROBABILITIES\nSatoru Fukayama Kazuyoshi Yoshii Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\nfs.fukayama, k.yoshii, m.goto g@aist.go.jp\nABSTRACT\nThis paper presents a system named ChordSequenceFac-\ntoryfor automatically generating chord arrangements. A\nkey element of musical composition is the arrangement of\nchord sequences because good chord arrangements have\nthe potential to enrich the listening experience and create\na pleasant feeling of surprise by borrowing elements from\ndifferent musical styles in unexpected ways. While chord\nsequences have conventionally been modeled by using N-\ngrams, generative grammars, or music theoretic rules, our\nsystem decomposes a matrix consisting of chord transi-\ntion probabilities by using nonnegative matrix factoriza-\ntion. This enables us to not only generate chord sequences\nfrom scratch but also transfer characteristic transition pat-\nterns from one chord sequence to another. ChordSequence-\nFactory can assist users to edit chord sequences by modi-\nfying factorized chord transition probabilities and then au-\ntomatically re-arranging them. By leveraging knowledge\nfrom chord sequences of over 2000 songs, our system can\nhelp users generate a wide range of musically interesting\nand entertaining chord arrangements.\n1. INTRODUCTION\nChord sequences are essential when composing and ar-\nranging music. Different songs, composers, arrangers, and\nmusical genres have different tendencies to use chord se-\nquences, which contributes to increased variety in music.\nEach song could have different natural chord sequences\nthat give different impressions. Although an arranger can\nchange ( i.e., arrange) chord sequences of a song to alter its\nmood, this is very difﬁcult for people who lack knowledge\nof chord sequences to arrange the chords in an appropriate\nway. The goal of this research is to assist people to gen-\nerate variations of chord sequences from an input original\nsequence by leveraging knowledge from a large number of\nother existing chord sequences called references .\nChord sequence arrangement is a promising approach\nto create derivative works from existing songs. Although\namateur creators could create such derivative works them-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.\n=+C D m G7 Am\nmajor seventh minor minor +7 +2 +5 +2 \n0.3 x 0.7 x \nC F M7 D7 GM7 major seventhmajor \nseventh+7 +5 +9 +5 FACTORIZATION ARRANGEMENTbases \nbases POOL OF BASES 0.6 \nmajor \nseventh0.2 0.2 0.1 0.5 0.4 0.2 0.2 0.6 0.1 0.5 0.4 \n0.6 0.2 0.2 0.1 0.5 0.4 0.2 0.2 0.6 0.1 0.5 0.4 Input :\nArranged :AM7 Em7 B9E\nCdim Fm7 Esus4 Gm7 Figure 1 . Overview of generating chord arrangements\nwith ChordSequenceFactory .\nselves, known as user-generated content (UGC) on video\nsharing services, the majority of users are generally music\nlisteners who do not create much themselves. We aim to\nencourage such people to create more derivative works by\nchanging chord sequences. This enables music listeners to\npersonalize songs by customizing chord sequences without\nany special training [4]. Such content personalization was\nachieved by Drumix [15], a system for arranging the drum\ntrack in music audio signals, but content personalization\nusing chord arrangements has not yet been studied.\nTo generate chord arrangements, it is necessary to model\nand manipulate chord sequences. Conventionally, mod-\nels for generating chord sequences have been represented\nby probabilistic models or N-grams [1, 6, 8, 9, 11, 12, 14],\ngenetic algorithms [2], generative grammars [10, 13], and\nexploiting examples or templates [3, 7]. Although these\nexisting models can easily be used to generate new chord\nsequences from scratch [2], it is difﬁcult to arrange chord\nsequences of an existing song while referring to other ex-\nisting songs.\nIn this paper we propose a new mathematical formu-\nlation of chord sequences using nonnegative matrix fac-\ntorization (NMF) and use it to build a system, Chord-\nSequenceFactory, that enables users to arrange chord se-\nquences of existing songs (Fig. 1). The chord sequence of\neach song (a sequence of symbols) is ﬁrst converted into\nachord differential matrix that represents chord transitions\nin short regions. The matrix is then decomposed into a set\nof bases (characteristic chord transition patterns) and a set\nof corresponding temporal activations using NMF. Chord\narrangements can be achieved by interpolating the bases of\na song with similar bases obtained from other songs while\npreserving the activations. An arranged chord sequence isCC#/Db \nD\nD#/Eb \nE\nFF#/Gb GG#/Ab AA#/Bb BCC#/Db \nD\nD#/Eb \nE\nFF#/Gb GG#/Ab AA#/Bb BCC#/Db \nD\nD#/Eb \nE\nFF#/Gb GG#/Ab AA#/Bb B\n+2 \n+5 +2 +2minor +5seventh +2minorC D m G7 AmChord Sequence\nChord Transition\nSequence { x}tt=1 TFigure 2 . The representation of a sequence of a chord tran-\nsition used in our method. Each transition is represented as\na combination of the interval between the root notes of ad-\njacent chords and the type of the latter chord.\nﬁnally generated from the reconstructed chord differential\nmatrix as the product of the resulting bases and the original\nactivations.\nThe remainder of this paper is structured as follows. In\nSection 2, we present the analysis and synthesis framework\nof chord symbol sequences based on the chord differential\nmatrix . In Section 3, we present the formulation of chord\narrangements. In Section 4, we present experimental re-\nsults to illustrate the performance of the system ChordSe-\nquenceFactory . Finally in Sections 5 and 6 we include dis-\ncussion and conclusions, which summarize the main con-\ntributions of the paper.\n2. ANALYSIS AND SYNTHESIS OF SYMBOLIC\nCHORD SEQUENCES\n2.1 Chord transition sequence\nChord names typically consist of up to three labels: root\nnote, chord type, and bass note speciﬁcation [5]. Since the\nbass notes are omitted in many cases, we treat chord infor-\nmation as a combination of the ﬁrst two labels. In general,\nchord transitions are considered to be more important than\nthe absolute pitch of the root note of each chord. This is\nsupported by the fact that it is possible to transpose chord\nsequences into other tonalities without affecting the func-\ntions of chord idioms used in these sequences.\nIn this paper we do not deﬁne a vocabulary of chord\nnames but rather, a vocabulary of chord “transitions” for\nmodeling how adjacent chords are arranged in music. Let\nfcngN\nn=1be the vocabulary of chord transitions, where N\nis the size of the vocabulary and each cnis deﬁned as fol-\nlows:\ncn\u0011str(INTERVAL) + str(TYPE) ; (1)\nwhere INTERV AL indicates the difference in semitones\nbetween the root notes of a target chord and the previ-\nous chord and TYPE indicates the type of the target chord.\nAn example chord transition sequence is shown in Fig. 2.\nSince we do not need to know the direction of root changes,\nthe value of INTERV AL is restricted to a nonnegative inte-\nger. The operation str (\u0001)+str(\u0001)means the string conjunc-\ntion ( i.e., concatenation). Once a given sequence of chords\nFigure 3 . Representation of a chord transition sequence\nwith the chord differential matrix representation.\nis converted into a sequence of chord transitions accord-\ning to the deﬁned vocabulary, we can recover the original\nsequence if the ﬁrst chord of the sequence is given. This\nrepresentation has also been used in related work [6] for\nthe same purpose of normalizing the tonality of chord se-\nquences.\nAn advantage of this representation is that it can reduce\nthe number of parameters to be dealt with. The typical ap-\nproach to modeling chord sequences is to calculate a tran-\nsition probability matrix over chord names deﬁned in a vo-\ncabulary. This approach, however, requires a large number\nof parameters, i.e., we need to deal with M\u0002Mparame-\nters if Mkinds of chord names are contained in the vocab-\nulary. Since the chord type seems to be less dependent on\nthe type of the previous chord name, we directly focus on\nthe root-note interval and the current chord type.\n2.2 Analysis of a chord transition sequence based on a\nchord differential matrix\nWe now explain the probabilistic representation of a chord-\ntransition sequence fxtgT\nt=1, where xt2 fcngN\nn=1andT\nis the length of the sequence. We analyze the sequence\non frame-by-frame basis using an exponentially-decaying\nwindow as shown in Fig. 3. This window is designed based\non our assumption that the characteristics of chord tran-\nsitions remain the same for some period of time because\nmusical pieces are usually composed so that each section\ngives a coherent impression. The window is moved one\nby one from the ﬁrst chord of the given sequence. In each\nframe t, we calculate a probability vector vt2RNsuch\nthat the elements of the vector sum to unity. More specif-\nically, vtnis a ratio of chord transition cnto all possible\ntransitions in frame t, which is given by\nvtn=P\n0\u0014\u001c\u0014\u0015\u000ecnxt+\u001ce\u0000\u001c\nP\n0\u0014\u001c\u0014\u0015e\u0000\u001c; (2)\nwhere \u000eijis the Kronecker delta, \u0015is the window length,\nande\u0000\u001cis a temporally-decaying weight. Note that vtin-\ndicates a co-occurrence relationship between chord transi-Figure 4 . Regeneration of a chord transition sequence\nfrom a chord differential matrix representation by means\nofVand the bi-gram probability obtained a priori from\nthe data.\ntions in the vicinity of frame t. The process of representing\nthe chord transition sequence as a chord differential matrix\nis shown in Fig. 3.\nThis frame-based vectorial representation of chord tran-\nsition probabilities has useful properties for chord arrange-\nment as follows:\n\u000fMood coherence in short durations\nAlthough chords often change at bar boundaries, the\nmood does not change in such a short time span be-\ncause we use an exponentially-decaying window of\nlength \u0015.\n\u000fReproducibility of chord sequences\nWe can approximately reconstruct the original se-\nquence fxtgT\nt=1from a sequence of probability vec-\ntorsfvtgT\nt=1in a principled manner.\n2.3 Synthesis of a chord transition sequence\nTo regenerate a chord transition sequence from a chord dif-\nferential matrix, we need to consider the transition between\nsuccessive chord transitions xtandxt\u00001. Using the transi-\ntion probability P(xtjxt\u00001)trained from the data, we can\ncalculate the probability of observing fxtgT\nt=1as follows:\nP\u0000\nfxtgT\nt=1\u0001\n=TY\nt=1(\u0018vt(xt) + (1 \u0000\u0018)P(xtjxt\u00001));\n(3)\nwhere \u0018is an interpolation coefﬁcient such that 0\u0014\u0018\u00141.\nThe chord transition sequence fx\u0003\ntgT\nt=1is reconstructed\nby maximizing P\u0000\nfxtgT\nt=1\u0001\nas follows:\nfx\u0003\ntgT\nt=1= argmax\nfxtgT\nt=1P\u0000\nfxtgT\nt=1\u0001\n: (4)\nSince there are Npossibilities for each xt, it is computa-\ntionally infeasible to test all NTpossible sequences with\nthe naive method of exhaustive search. Fortunately, we can\nobtain the solution fx\u0003\ntgT\nt=1withO(N)using dynamic\nprogramming. The process for generating a sequence is\nshown in Fig. 4.\nChord Differential Matrix Vocabulary of Chord Transitions \n( n = 1, 2, … , N ) \nTime ( t = 1, 2, … , T ) V\nTemporal Activations Characteristic Chord \nTransition Patterns \n(Bases)\nx\nTime ( t = 1, 2, … , T ) Bases ( k = 1, 2, … , K ) \nBases ( k = 1, 2, … , K ) Vocabulary of Chord Transitions \n( n = 1, 2, … , N ) H WFigure 5 . Factorizing the chord deferential matrix: Anal-\nysis of the frequent combinations and the temporal occur-\nrences of chord transitions.\n3. FORMULATION OF\nCHORD-SEQUENCE-FACTORY\n3.1 Factorization of a chord differential matrix\nAn overview of ChordSequenceFactory is illustrated in\nFig. 6. The mood of music varies according to sections\nof a song. For instance, some sections often use dominant\nintervals and other sections tend to use chords with more\ntension notes. Therefore we aim to identify the characteris-\ntic patterns of chord transitions that affect the mood of each\nsection. We represent a probability vector vtat each time\ntas a convex combination of multiple bases as follows:\nvt=KX\nk=1hktwk; (5)\nwhere wk;(k= 1;\u0001 \u0001 \u0001; K)denotes a characteristic chord\ntransition pattern and hktis its weight. In order to de-\ncompose fvtgT\nt=1using shared bases fwkgK\nk=1, we exploit\nnonnegative matrix factorization (NMF) (Fig. 5), i.e.,\nV= (v1\u0001 \u0001 \u0001vT) (6)\nis decomposed into matrices W(N\u0002K)andH(K\u0002T)\nas:\nV'WH; (7)\nwhere Wis the matrix of bases:\nW= (w1\u0001 \u0001 \u0001wK) (8)\nandHis the matrix of activations:\nH=0\nB@h11\u0001 \u0001 \u0001h1T\n.........\nhK1\u0001 \u0001 \u0001hKT1\nCA: (9)FACTORIZATION ARRANGEMENTPOOL OF BASES\nFACTORIZATION\n+5 sus2 +2 major +2 minor +10 major +5 major . . .\nCONVERTED\nPreservedChord Transi/g415on Sequence to  Chord Diﬀeren/g415al Matrix\n(ANALYSIS)\n(SYNTHESIS)\nChord Diﬀeren/g415al Matrix toChord Transi/g415on Sequence\nChord Diﬀeren/g415al Matrix\nV\nChord Diﬀeren/g415al Matrix\nVCONVERTED≃≃ x\nx\nx =\n*\n+3 dim7 +2 seventh +7 minor +11 sus2 +2 major7 . . .ORIGINAL REFERENCE\n+a ( 1 – a)\n=\nARRANGEDINTERPOLATION OF BASES\nTemporal Ac/g415va/g415ons\nH\nCharacteris/g415c Chord \nTransi/g415on Pa/g425erns\n(Bases)\nW*Temporal Ac/g415va/g415ons\nHCharacteris/g415c Chord \nTransi/g415on Pa/g425erns\n(bases)\nWFigure 6 . Overview of the process executed by ChordSequenceFactory for generating chord arrangements.\n3.2 Interpolation\nWe want to modify the Vof a target song by referring\nto chord transition patterns used in another song. More\nspeciﬁcally, to obtain a reconstructed chord differential ma-\ntrixV\u0003, we reuse the Hof the target song and use a set of\nmodiﬁed vectors W\u0003= (w\u0003\n1\u0001 \u0001 \u0001w\u0003\nK)as follows:\nV\u0003= (w\u0003\n1\u0001 \u0001 \u0001w\u0003\nK)H (10)\n=W\u0003H: (11)\nTo obtain each modiﬁed vector w\u0003\ni, we interpolate a ref-\nerence vector wref\niof another song with the original vector\nwi. Since the original bases fwkgK\nk=1and reference bases\nfwref\nkgK\nk=1are not guaranteed to have their index aligned,\nwe associate each wiwith a reference wref\nj(i)such that wi\nis closest to wref\nj(i),i.e.,\nj(1);\u0001 \u0001 \u0001; j(K) = argmax\nj(1);\u0001\u0001\u0001;j(K)KX\nk=1D\u0010\nwikwref\nj(i)\u0011\n;\n(12)\nwhere Dis a distance measure based on the symmetric\nKullback Leibler divergence:\nD\u0010\nwikwref\nj(i)\u0011\n=X\nkwiklogwik\nwref\nj(i)k\n+X\nkwref\nj(i)klogwref\nj(i)k\nwik:(13)\nUsing the aligned indicies j(1);\u0001 \u0001 \u0001; j(K), we can cal-culate the modiﬁed vector w\u0003\nias follows:\nw\u0003\ni=awi+ (1\u0000a)wref\nj(i); (14)\nwhere ais the interpolation parameter such that 0\u0014a\u00141.\nThe value of arepresents how the mood of the original\nsong is preserved through the chord arrangement.\n3.3 Generation of chord arrangements\nAs discussed in Section 2.3, we can generate an arranged\nchord transition sequence from the modiﬁed chord differ-\nential matrix V\u0003. We interpolate the transition probability\nobtained from all songs in the database Pall(xtjxt\u00001)and\nthat obtained from the reference song Pref(xtjxt\u00001)with\nthat obtained from the original song Porg(xtjxt\u00001)as fol-\nlows:\nP\u0003(xtjxt\u00001) =\u00181Pall(xtjxt\u00001) +\u00182Porg(xtjxt\u00001)\n+\u00183Pref(xtjxt\u00001); (15)\nwhere f\u0018ig3\ni=1are the interpolation coefﬁcients that sum to\nunity, i.e.,P\ni\u0018i= 1. Using P\u0003(xtjxt\u00001), we can calcu-\nlate the probability of observing fxtgT\nt=1as follows:\nP\u0000\nfxtgT\nt=1\u0001\n=TY\nt=1(\u0018v\u0003\nt(xt)\n+ (1\u0000\u0018)P\u0003(xtjxt\u00001)): (16)\nThe arranged chord transition sequence is then obtained by\nmaximizing P\u0000\nfxtgT\nt=1\u0001\n(see Section 2.3).a = 1.0 a = 0.7\na = 0.5 a = 0.3 a =  0.0\nBases ( k = 1, 2, … , K ) Bases ( k = 1, 2, … , K ) Bases ( k = 1, 2, … , K ) Vocabulary of Chord Transitions \n( n = 1, 2, … , N ) Vocabulary of Chord Transitions \n( n = 1, 2, … , N ) Figure 7 . Examples of the bases interpolated with bases\nof a reference song: we can see that the chord usages orig-\ninated in two different songs are combined, depending on\nthe value of interpolation factor a.\n4. EVALUATION\n4.1 Experimental conditions\nTo evaluate our system, we used 2,123 lead sheets down-\nloaded from the Wikifonia (www.wikifonia.org) website.\nAll lead sheets we used were formatted in the MusicXml\nformat including information of chord sequence. Each\nchord name in a ﬁle included the step (C, D, ...), alternation\n(#,[) of the root note, and the chord type.\nTo deﬁne a vocabulary fcngN\nn=1, we extracted chord\ntransitions that appeared more than 10 times in the data. A\nspecial symbol “Unknown” 2 fcngN\nn=1was used for repre-\nsenting the rest. The vocabulary size was N= 163 . Using\nthe vocabulary, we represented each song as a chord tran-\nsition sequence fxtgT\nt=1and calculated a chord differen-\ntial matrix from that sequence by using an exponentially-\ndecaying window of length \u0015= 6, corresponding to the\nnumber of chord changes. The transition probabilities be-\ntween fcngN\nn=1were calculated in advance for all 2,123\nsongs and for each song, respectively. The chord differ-\nential matrix was decomposed using NMF based on the\nEuclidean distance with K= 5.\nWe interpolated characteristic chord transition patterns\n(basis vectors) of a reference song with those of an origi-\nnal song according to an interpolation coefﬁcient a= 1:0,\n0:7,0:5,0:3, or0:1. A chord transition sequence was syn-\nthesized from a modiﬁed chord differential matrix. Since\nthe vocabulary of chord transitions only holds information\nof relative root positions, we set the root note of the initial\nchord to the original root note.\n4.2 Experimental results and discussions\nAs shown in Fig. 7, we can combine two different charac-\nteristic chord transition patterns by controlling the interpo-\nlation factor aon the user interface of ChordSequenceFac-\ntory (Fig. 8). The generated examples of chord arrange-\nments are shown in Table. 1. We conﬁrmed that the mood\nof an original song can be modiﬁed while incorporating the\nmood of a reference song if these songs have some simillar\ncharacteristic chord transition patterns.\nThrough informal evaluation and listening tests we\nFigure 8 . User interface of ChordSequenceFactory : users\ncan change the interpolation factor with sliders corre-\nsponding to each base.\nfound our system was, in many cases, able to provide mu-\nsically coherent chord sequence arrangements (Table. 1).\nHowever, our evaluation also revealed some limitations\nthat should be addressed in our future work. Since the vo-\ncabulary of chord transitions deﬁned for the system does\nnot include the absolute pitch for the root note, the gener-\nated results tended to exhibit transposition frequently. In\naddition, ﬁnding the optimal number of bases when de-\ncomposing the probability should be investigated for better\nperformances. Finally, a slider for changing the activation\ncan bring about more effects in the generated sequence.\nAlthough simply adding a seventh note to chord se-\nquences (rather than using our approach) has a similar ef-\nfect on chord arrangements, dissonance may appear in the\nconnection between the chords. In contrast, with our sys-\ntem, dissonant chords can be avoided by using the con-\nstraints given by the transition probabilities. Furthermore,\nthere are no restrictions in terms of combining two songs\nthat have different structures, since the interpolation is done\nbetween the decomposed basis.\nConventional methods based on N-grams cannot con-\ntrol the dynamic characteristics of chord transitions and\nneed label sequences to reﬂect human intention. In our\nmethod, we can see that the probability is changed for each\ntimetand that the activations at time tplay the same role\nas the label sequences in the conventional methods.\n5. CONCLUSIONS\nWe have described a system ChordSequenceFactory that\ncan assist a user to arrange chord symbol sequences of\na song by ﬁnding latent frequent patterns of chord tran-\nsitions and modifying them on the basis of other songs.\nWe proposed a new analysis and synthesis framework for\nchord symbol sequences, where the temporal changes of\nchord transition sequences are represented as a chord dif-\nferential matrix. NMF is used to decompose this matrix\ninto bases corresponding to the frequent patterns of chord\ntransitions and activations corresponding to their temporal\noccurrences. The matrix can then be updated for a new ar-\nrangement by modifying these bases by mixing them withORIGINAL Cmin B [maj E [sus2 Fmaj Gmin Fmaj B [maj Gmaj\nreference a\nARRANGED A 0.7 Cmaj7 Fmaj7 B [7 E [7 A [maj6+9 G [maj Bmaj Gmin6\n0.6 Cmaj B [maj E [7 A [7 D [maj6+9 Bmaj Emaj Cmin6\n0.5 Cmaj B [maj Cmaj Dmaj Cmaj B[maj E [maj A [min\nB 0.7 Cdim E [dim E [7 E [7 F#dim Adim D7 Gmin6\n0.6 Cdim E [dim E [7 E [7 F#dim Adim D7 Gmin6\n0.5 Cmaj B [maj Cmaj Dmaj Cmaj B[maj E [maj B [min6\nC 0.7 Cmaj Fmaj F7 F7 B [maj E [maj A [maj Gsus4\n0.6 Cmaj Fmaj F7 F7 B [maj E [maj A [maj Gsus4\n0.5 Cmaj Fmaj F7 F7 B [maj E [maj A [maj Gsus4\nD 0.7 Cmin7 Dmin7 Dmaj7 Dmaj7 Emin7 Amaj7 Dmaj7 Cmaj7\n0.6 Cmin7 Dmin7 Dmaj7 Dmaj7 Emin7 Amaj7 Dmaj7 Cmaj7\n0.5 Cmaj B [maj Cmaj Dmaj Cmaj B[maj E [maj7 A [min\nE 0.7 Cmaj6 Fmaj6 B [sus4 E [sus4 A [maj6 D [maj6 F#7 Bmin\n0.6 Cmaj6 Fmaj6 B [sus4 E [sus4 A [maj6 D [maj6 F#7 Bmin\n0.5 Cmaj6 Fmaj6 B [sus4 E [sus4 A [maj6 D [maj6 F#major Bmin\nTable 1 . Generated results of ChordSequenceFactory with ﬁve different reference songs (A,B,C,D,E). For each reference\nsong, three values of interpolation factor a= 0:7;0:6;0:5were used. The arranged chord sequences were decoded from\nthe chord transition sequence fxtgT\nt=1, by setting the root note of the ﬁrst chord to C as in the original sequence.\nsimilar bases in the pool of bases obtained from more than\n2000 songs. The updated matrix is ﬁnally used to generate\na new re-arranged chord sequence using dynamic program-\nming. In our experience, ChordSequenceFactory gener-\nated musically interesting and entertaining chord arrange-\nments. In the future, we plan to extend our framework to\nconsider melody lines as constraints on arranged chord se-\nquences. We will also include audio signal processing so\nthat the chord differential matrix can be used directly on\nmusic audio signals.\nAcknowledgement\nThis work was supported in part by OngaCREST, CREST,\nJST.\n6. REFERENCES\n[1] C. Ames: “The Markov Process as a Compositional\nModel: A Survey and Tutorial,” Leonardo Music Jour-\nnal, Vol. 22, No. 2, pp. 175–187, 1989.\n[2] J. Biles: “GenJam: A Genetic Algorithm for Generat-\ning Jazz Solos,” Proceedings of ICMC , pp. 131–137,\n1994.\n[3] D. Cope: Experiments in Musical Intelligence , A-R\nEditions, 1996.\n[4] M. Goto: “Active Music Listening Interfaces based on\nSignal Processing,” Proceedings of ICASSP , pp. 1441–\n1444, 2007.\n[5] C. Harte, M. B. Sandler, S. A. Abdallah, E. G ´omez:\n“Symbolic Representation of Musical Chords: A Pro-\nposed Syntax for Text Annotations,” Proceedings of\nthe ISMIR conference , pp. 66–71, 2005.\n[6] M. Mauch, S. Dixon, C. Harte, M. Casey, B. Fields:\n“Discovering Chord Idioms through Beatles and RealBook Songs,” Proceedings of the ISMIR conference ,\npp. 255–258, 2007.\n[7] F. Pachet: “Surprising Harmonies,” International Jour-\nnal of Computing Anticipatory Systems , Vol. 4 1999.\n[8] J. Paiement, D. Eck, S. Bengio: “A Probabilistic Model\nfor Chord Progressions,” Proceedings of the ISMIR\nconference , pp. 312–319, 2005.\n[9] H. Papadopoulos, G. Peeters: “Large-scale study of\nchord estimation algorithms based on chroma repre-\nsentation and hmm,” Proceedings of the ISMIR con-\nference , pp 225-258, 2007.\n[10] M. Rohermeier: “Towards a generative syntax of tonal\nharmony,” Journal of Mathematics and Music , Vol. 5,\nNo. 1, pp. 35–53, 2011.\n[11] R. Scholz, E. Vincent, F. Bimbot: “Robust modeling of\nmusical chord sequences using probabilistic N-grams,”\nProceedings of the ICASSP , pp. 53–56, 2009.\n[12] A. Sheh, D. Ellis: “Chord Segmentation and Recogni-\ntion using EM-Trained Hidden Markov Models,” Pro-\nceedings of the ISMIR conference , pp. 185–191, 2003.\n[13] M. J. Steedman: “A Generative Grammar for Jazz\nChord Sequences,” Music Perception , 2:1 pp. 52–77,\n1984.\n[14] D. Temperley: Music and Probability , The MIT Press,\n2007.\n[15] K. Yoshii, M. Goto, K. Komatani, T. Ogata and H. G.\nOkuno: “Drumix: An Audio Player with Real-time\nDrum-part Rearrangement Functions for Active Music\nListening,” IPSJ Digital Courier , Vol. 3, pp. 137–144,\n2007."
    },
    {
        "title": "Sparse Music Decomposition onto a MIDI Dictionary Driven by Statistical Music Knowledge.",
        "author": [
            "Boyang Gao",
            "Emmanuel Dellandréa",
            "Liming Chen 0002"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416994",
        "url": "https://doi.org/10.5281/zenodo.1416994",
        "ee": "https://zenodo.org/records/1416994/files/GaoDC13.pdf",
        "abstract": "The general goal of music signal decomposition is to represent the music structure into a note level to provide valuable semantic features for further music analysis tasks. In this paper, we propose a new method to sparsely decompose the music signal onto a MIDI dictionary made of musical notes. Statistical music knowledge is further integrated into the whole sparse decomposition process. The proposed method is divided into a frame level sparse decomposition stage and a whole music level optimal note path searching. In the first stage note co-occurrence probabilities are embedded to generate a sparse multiple candidate graph while in the second stage note transition probabilities are incorporated into the optimal path searching. Experiments on real-world polyphonic music show that embedding music knowledge within the sparse decomposition achieves notable improvement in terms of note recognition precision and recall.",
        "zenodo_id": 1416994,
        "dblp_key": "conf/ismir/GaoDC13",
        "keywords": [
            "music signal decomposition",
            "note level representation",
            "semantic features",
            "music analysis tasks",
            "MIDI dictionary",
            "sparse decomposition",
            "optimal note path searching",
            "note co-occurrence probabilities",
            "note transition probabilities",
            "statistical music knowledge"
        ],
        "content": "SPARSE MUSIC DECOMPOSITION ONTO A MIDI \nDICTIONARY DRIVEN BY STATISTICAL MUSIC \nKNOWLEDGE  \nBoyang Gao, Emmanuel Dellandr éa and Liming Chen  \nUniversité de Lyon, CNRS,  \nEcole centrale  de Lyon, LIRIS, UMR5205, F -69134, France.   \nE-mail:{Boyang.Gao, Emmanuel.Dellandrea, Liming.Chen}@ec -lyon.fr \nABSTRACT  \nThe general goal of music signal decomposition is to rep-\nresent  the music structure  into a note level to  provide va l-\nuable semantic features for further  music analysis tasks. \nIn this paper , we propose a new method to sparsely d e-\ncompose  the music signal onto a MIDI dictionary  made \nof musical notes. Statistical music knowledge is  further \nintegrat ed into the whole sparse decomposition process . \nThe proposed  method is divided into a frame level sparse \ndecomposition  stage  and a whole music  level optimal \nnote path searching . In the first stage  note co -occurrence \nprobabilit ies are embedded to generate  a sparse multiple \ncandidate graph while  in the second stage  note transition \nprobabilit ies are incorporated into the optimal path \nsearching. Experiment s on real -world polyphonic music \nshow that  embedding music knowledge within the sparse \ndecomposition achieves  notable  improvement in terms of \nnote recognition  precision and recall.  \n1. INTRODUCTION  \nLarge amount s of digitalized music available drive the  \nneed for the development of  automatic music analysis , for \nexample automatic genre classification, mood detection \nand similarity measurement. Most  of the tasks rely on e f-\nfective features extracted from music signal s. Among \nvarious features , music notes , denoted by MIDI notes in \nthis paper,  provide the most comprehensive information , \nsince music is indeed  sound poetry comprised  of notes \nplayed by instruments . If notes are accurately recovered \nfrom music signal, automatic music analysis can be grea t-\nly improved. However, mixing different instrument pla y-\ning is trivial while decomposing is quite challenging  due \nto the intrinsic complexity of polyphonic  music.  \nRecovering note s from a music wave signal is usually \nreferred to multiple F0 estimation. The approaches in lit-\nerature can be roughly sorted into two categories: para m-\neterized like statistical model based methods and non -\nparameterized like non -negative matrix factorization \n(NMF) based methods. Parameterized approaches usually \nassume that multiple F0 can be described by particular \nmodel s with a small number of free parameters that can  be estimated from  the signal. For example , in [1] \nKameoka  et al. propose  a mul ti-pitch analyzer named  the \nharmonic temporal str uctured clustering (HTC) method  \nthat jointly estimates pitch, intensity, onset  and duration. \nHTC decomposes the power spectrum time series into \ndistinct clusters such that each cluster has originated from \na single source modeled by a Gaussian Mixture Model \n(GMM). The p arameters of the source model are compu t-\ned thanks to maximum  a posteriori (MAP) estimation . In \n[2], Wu et al. extend Kameoka 's work  to propose a flex i-\nble harmonic temporal timbre model to decompose the \nspectral energy of the signal in the time -frequency d o-\nmain into individual pitched notes. Each note is modeled \nwith a 2 -dimensional Gaussian kernel . Parameters of \nGaussian mixtures are then estimated by expectation \nmaximization (EM) algorithm  with a global Kullback –\nLeibler (KL) divergence cost function.  \nUnlike parameterized approach es, non-parameterized \nmethods like NMF focus on recovering pitch combin a-\ntions from the signal data itself without presuming any \nunderlying model form s. For example,  NMF  [3] based \nmethods try to decomposes the multiple pitch spectrum \nmatrix   into two ma trices   and   [4].   contains va r-\nious harmonic patterns and   consists of activation  be-\nhaviors  so that     . In [5], Hoyer  extends  the origi-\nnal NMF by adding a regulation term to make   sparse. \nSparseness property is quite helpful  especially for music \nnote estimation, since a short period music can only  con-\ntain a few  notes play ed together , compared with  all po s-\nsible notes .  \nNMF is such an extensible framework that it largely \ndominates non -parameter methods. F or example , in [6] \nZafeiriou  adds a linear discriminant analysis (LDA) stage \nto the activities extracted by NMF. In [6-8], fisher -like \ndiscriminant constraints are embed ded inside the deco m-\nposition. In [9], Lewandowski  propose s a supervised \nmethod with two discriminative criteria that maximize \ninter-class scatter and quantify the predictive potential of \na given decomposition. In order to extract features that \nenforce the separability between pitch labels, pi tch i n-\nformation present in time -aligned musical scores is fused \nin sparse NMF. In [10], Sakaue  combines Bayesian infe r-\nence with NMF to propose a Bayesian non -negative ha r-\nmonic -temporal factorization (BNHTF). BNHTF models \nthe harmonic and temporal structure s separately with \nGaussian mixture model s. In [11] , a music sparse deco m-\nposition approach  is proposed  using high quality MIDI Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2013  International Society fo r Music Information Retrieval    \n \ndictionary. This work is a variant of sparse NMF  and use s \nnon-negative match ing pursuit to solve sparse NMF . Un-\nlike NMF  that process es the entire signal , this work con-\nstructs the activity matrix   column by column.  It is still \nworth mentioning the work i n [12] where Leveau  et al. \npropose to learn instrument specified note atoms  with a \nmodified matching pursuit  and a track ing of  the played \ninstrumental note s by searching an optimal path with r e-\nspect to the reconstruction error . \nThus, p revious works in  the literature have demo n-\nstrated the effectiveness of various approaches in mult i-\nple-F0 estimation , especially NMF based  methods . How-\never,  under the NMF framework , the entire  music spe c-\ntrum series   are treated as a whole object to be reco n-\nstructed . Most of the algorithms  focus on reducing the \nspectrum  reconstruction  error  so as to  overlook the com-\npatibility  in concurrent  and consec utive  notes . This batch \nprocessing style  make s it hard to  fuse note co -occurrence \nand transition information  to guide note detection  during \nthe matrix factorization . Indeed,  after the signal spectr um \nmatrix is factorized,   and   are new represents of the \nmusic , which have lost signal context information for \npost-processing to correct possible error . Even in [12], \nthe Viterbi algorithm is used to search the optimal path \nonly with respect to a minimum reconstruction error  and \nneglect s the underlying note  relations . Nevertheless cor-\nrelation between concurrent and consecutive notes co n-\ntains significant heuristics that can help to correct the de-\ncomposition error introduced by a signal level analysis.   \nTherefore, to employ note statistical information to \nhelp music  sparse  decomposition, we propose in this p a-\nper a two-stage sparse decomposition approach  integrated \nwith music knowledge . In frame level decomposition  \nstage, note co -occurrence probabilit ies are embedded to \nguide atom selection in modified matching pursuit alg o-\nrithm with a MIDI dictionary . A spars e multiple cand i-\ndate graph is then constructed  to provide backup choices \nfor later selection s. In the global optimal path searching  \nstage,  note transition probabilit ies are incorporated  to-\ngether with a goodn ess measure of frame decomposition . \nIts principle is to guide the local sparse music decompos i-\ntion with co -occurred notes information and decode the \nglobal optimal decomposition path with consecutive note \nknowledge.  Due to the Gabor limit, time and frequen cy \nresolution cannot be well satisfied at the same time. Thus, \nwe emphasize the frequency resolution aspect rather than \nthe exact time location, since correct note recognition  is \nmore important  for our following  classification  task. \nThe rest  of this  paper is organized as follow s: Section \n2 introduces our two-stage approach  in detail . Section 3 \nshows experiment al result s on real -world music signal s. \nThe conclusion is drawn in the final section.   \n2. SPARSE DECOMPOSITION  WITH NOTE \nSTATISTICS  \nOur proposed m ethod consists of two main steps . In the \nfirst step, the entire music signal is framed  and a  modified orthogonal matching  pursuit algorithm is performed on \neach frame to generate decomposition candidates. In the \nsecond step, decomposition candidates are co nnected to \nform a directed graph. An optimal path is  then construc t-\ned to produce the final decomposition  result.  \n2.1 Frame Level Sparse Decomposition  \nFormer  study shows that e laborating an appropriate mus i-\ncal dictionary is a key issue since this set of atoms has to \nbe rich enough to characterize the varieties of real word  \nmusic . Although [ 12] has developed sophisti cated met h-\nod to learn  atoms from instrument recordings, it is still \nimpractical to apply to a large instrument set.  There fore, \nin order to get ade quate instru ment note sound s, we pro-\npose to make  use of a MIDI synthesizer.  Logic Pro 9  is \nemployed in our  approach  to generate the MIDI note di c-\ntionary because of its huge instrumental library and the \nhigh sound  quality . Unlike pre -installed MIDI synthesizer \nwith sound card, Logic Pro 9 uses a large number of real \ninstrument recordings to make synthesized wave signal as \nnatural  as possible.  \nTo build the MIDI dictionary, we choose the first 80 \nrealistic  instruments as in general MIDI level 1 set and 31 \npercussion instrument sets including 1860 per cussion \nsounds . For each in strument, we keep 60 notes f rom note \n31 to note 90. The 60 notes span 5 octaves from low to \nhigh, covering most instrument al playing range. The d u-\nration of each note is set to 186ms, which are 4096 sa m-\nples under a sample rate of 22050Hz. This duration is \nlong enough to hold one attack -decay -sustain -release \n(ADSR)  envelope  and leads to 5.38Hz in terms of fr e-\nquency resolution, which is su fficient to discriminate a d-\njacent note s in piano roll . Our MIDI note wave is then \nconverted into a single -sided power spectrum  obtained  by \napplying the short time Fourier transform (STFT) with a \nHamming window. The final MIDI dictionary thus co n-\ntains 6660 2048 -dimensional vectors.  \nThe a doption of the sparse representation is based  on \nthe hypothesis that during a 186ms time slot , there will \nnot be many notes play ed together . Therefore,  concurrent \nnotes are sparse  within one frame . Sparse representation \n[13] is originated from finding the solution    of an u n-\nderdetermined linear system      so that    contains \nas few non -zero components as possible. In most cases \n     is hard to satisfy, thus in practice      and \n        are minimized simultaneously instead.  \nArmed w ith our MIDI dictionary, the classical matc h-\ning pursuit algorithm like orthogonal matching  pursuit \n(OMP) [ 17] must be modified because the single -sided \npower spectrum words in MIDI dictionary impose an in-\nheren t positive cons traint on sparse solutions. In other \nword s, any negative component of a sparse solution is \nprohibited, as negative appearance of certain note s is im-\npossible. To solve this problem we adopt  a positive co n-\nstraint matching pursuit (PCMP) algori thm that is me n-\ntioned in [11][14]. The difference between OMP and   \n \nPCMP is in updating a provi sional solution step: f or \nOMP, least mean square (LMS) suffices to solve the mi n-\nimization resulting in residual signal s orthogonal to su p-\nport set. For PCMP, howeve r, after the positive constraint \nminimization, orthogonality is not always guaranteed, \nthus the algo rithm is turned to a weak orthogonal  matc h-\ning pursuit.   \nWhen scrutinizing  the decomposition  result s of PCMP \nwithin one frame , we found  a number of  irregular note \ncombinations. This  is due to PCMP ’s over-fitting  target \nsignal s without considering any compatibility of concu r-\nrent notes. In fact, atom selection in each iteration of  or-\nthogonal matching pursuit algorithm is very important. \nOMP guarantees t hat expending support set with any li n-\near independent atoms will decrease  the reconstruction \nerror and at the same time keep  the residual signal o r-\nthogonal to the new expanded support set.  Any atom s e-\nlected in  the support set  will permanently reside. Ther e-\nfore previously selected atom s have  a great influence on  \nfollowing ones and alter  the overall OMP performance . \nAlthough PCMP does not always hold orthogonal prope r-\nty, the principle remains  the same . \nSelecting  a new atom in dictionary is thus the very \nplace  where concurrent note heuristic information should \nbe embedded. To formulate concurrent note information , \nBayes model is employed  in our approach  to approximate  \nthe posterior probability of pote ntial note given observed \nnotes   \n                    \n   \n              \n                               \nwhere              denote s   observed notes o b-\ntained by first   PCMP iterations,   represents a pote n-\ntial co -occurred note with  . The note prior probability \n     and the note co -occurrence posterior pr obability \n       are estimated from ou r classical music  MIDI  da-\ntabase. To obtain       , a joint distribution        is \nfirstly estimated by accounting the frequency with ove r-\nlap degree of the concurrent note   and    Then        \nis obtained by nor maliz ing        over    Although \nequation (1) provides instructive information to help s e-\nlect appropriate note combinations, it is still risky to only \nconsider  the best note decomposition , since the second \nbest one may be more appropriate in adjacent n ote co n-\ntext.  To avoid the one best bias, we propose to preserve \nmultiple candidates to give top -N best decompositions \nchances to recover in optimal path searching.  \nOrthogonal m atching pursuit  is a greedy algorithm . In \neach iteration  only the best atom will be added into su p-\nport set. This can be  risky in some cases , since o nce a \n“bad” atom is selected , this error cannot be corrected in \nthe future. In [15], it has been shown  that it is possible to \nselect “bad” atom initially so as to t rap OMP from reco n-\nstructing target signal s. Methods like OCMP in [ 19] are \nproposed to overcome the problem.  However, i n music \ndecomposition the same note in different octave or from the same kind of instruments shares the similar harmonic \npattern . Therefor e it is hazardous to rule out  a suboptimal \ndecomposition too early before adjacent note compatibi l-\nity is ch ecked.   \nTo overcome this drawback  of OMP , we propose to \nkeep   best candidates in each iteration  instead of only \none. To measure the goodness of frame decomposition \nwe define                          , where   is \nsparse note decomposition  vector ,    is decomposition \nresidual signal,                     \n     denotes \nnote concurrent probability,    is a free parameter that \nbalance s concurrent probability term and reconstruction \nerror term . As an  example shown in Figure 1  we keep  the \ntop 3 decomposition candidates in every iteration. In the \nfirst iteration (C), (E), (G) are kept. In the second iter a-\ntion, (C, D), (E, F) and (E,  G) are  obtained  according to \nthe reconstruction error and concurrent probability. Note \nthat (G) selected in the first iteration is eliminated be-\ncause  its descendant combinations (G,*) are inferior to \nothers’ . After 3 iteration s, combinatio ns of (C,  D, E), (C, \nD, G) and (E,  F, B) survive , as shown in orange .  \n \nFigure 1. Multiple candidate selection  example  \nWhen sparse decomposition terminates , the top   note \ncandidates  are derived  for every signal frame . The best \none can be treated  as the decomposition result  of the cu r-\nrent frame . Besides, all candidates are preserved  for co n-\nstructing the optimal decomposition path when we further \ninvestigate  inter-frame relations. The multiple candidate  \nPCMP  algorithm  that we propose  is summarized in Algo-\nrithm 1.  \n2.2 Global Level Optimal Note Path Searching  \nAll previous steps  in section  2.1 focus on improving  \nsparse note decomposition within one signal frame.  When \nfurther scrutinizing the PCMP decomposition between \nconsecu tive frames , we can still find a number of  disco n-\ntinuous note decompositions , in which  the note sequence \nhas sudden abnormal jumps in adjacent frames, including \noctave shift or sharp/flat drift. This is due to a lack of \nnote transition regulation  and becau se the sparse deco m-\nposition only minimize s reconstruction error in current \nframe without con sidering any neighbor  frame contexts .  \nBeside s the co -occurred ones, consecutive notes bear \nstrong correla tions  which convey various melody, te m-\nporal and  dynamic information of music. It is reasonable \nto incorpo rate such sequential knowledge  of notes as to \nsuppress  the discontinuous note error.  \n  \n \nTask : Approximate the solution of problem:         , \nsubject to         . \nInput : Dictionary  , signal  , max iteration number  , top \n   candidates to keep, balance parameter   , note posteri or \nprobability        and error threshold    . \nOutput:  sparse solution:    \nInitialization : \nInitial residual:      . \nInitial support:        . \nInitial candidate queue:                } \nMain  iteration :  \nfor             \nfor          \n Compute            according to equation (1)  \n Compute error:                            \n                 all   using the optimal choice \n                   . \n Find top   minimizers of      to form   \n                              , and push \n                                       into \n    \nend \nfor each         \n Compute   that minimizes          subject to \n                  . \n Compute residual         . \nend \nAscending ly sort    according to           \n             and keep  the first   items.  \nIf any            break.  \nend \nOutput result:       . \nAlgorithm 1. Positive constraint matching pursuit pro-\nducing  multiple candidates  \nWe thus apply  transition  probabilities  to model  rela-\ntions between two decomposition candidates in adjacent \nframe s. To formulate note transition s, Bayes model is \nadopted so that conditional probability can be approx i-\nmated  from individual note pair s. Since at most   cand i-\ndates remain in one frame the posterior probability of \ncandidate   in frame   given candidate   in frame     is \ncalculated  as \n                                      \n   \n                     \n                         \nwhere                                  denotes the decompos i-\ntion candidate   in frame   containing   notes. \n            is calculated similarly  as in equation (1).   \nThanks to  the multiple decomposition candidates ge n-\nerated by the modified PCMP previously, an inter -\ndecomposition directed graph is further constructed  to \nhelp determin ing the  optimal decomposition path through all frames , as illustrated in Fig ure 2. In this directed \ngraph, each decomposition candidate forms a node and \noutgoing edge denotes the transition probability compu t-\ned by equation (2). The nodes are disconnected within the \nsame frame  indexed with   . \n  \nFigure  2. Optimal path decoding example  \nIn order t o connect transition probabilities  with the \nsparse decomposition candidates , the decomposition \ngoodness measure is converted into corresponding proba-\nbilities  as                  , since  the frame signal    \nand atoms  in   have been normalized to unit vectors. The \nconversion also reflects a reasonable assumption that the \nreconstructed signal is approximately Gaussian distribu t-\ned around original one. Treat ing the decomposition ca n-\ndidates as hidden states of a Hidden Markov Model \n(HMM), Viterbi algorithm decodes the optimal decomp o-\nsition candidate path   : \n         \n                                          \n  \n  \n       \nwhere   is the frame index,   is the total number of \nframes,   is a balance parameter to adjust emphasis,    \ndenotes decomposition candidate index in frame   along \npath  . Initially                  =1. \n3. EXPERIMENT AND RESUL TS  \nTo evaluate the decomposition quality of the proposed \nPCMP with note statistics, a  multi -timbral music with \ntime domain note reference has been used, which is pro-\nvided in Mirex2007 multiF0 development data  [16]. The \nmusic is a recording of the fifth variation from L. van \nBeethoven Variations from String Quartet Op.18 N.5, \nlasting for 54s. 5 instruments are included in to the music . \nEach ins trument was recorded separately and then  mixed \nto a mono 44.1 kHz 16 bits wave file. The whole music is \ntested by our system  (PCMP with mu lti-candidate and \nViterbi) against its ground truth MIDI file.  \nAnother widely used data set adopted in our exper i-\nments is MUS , provided in MAPS [ 18]. MUS contains \n270 pieces of classical and traditional music , recorded in \ndifferent conditions which vary in piano instruments and \nsurroundings. For each piano music piece , as in [9], first \n30 seconds are tested by our  system  against ground truth \nMIDI  files. \nFigure 3 shows precision and recall scatter diagram of \nthe proposed decomposition  that improve original PCMP,  \nnoted as PCMPMC and PCMPMCV . Table 1 display s the \n  \n \ncompari sons in terms of precision, recall and F-measure \nbetween  our proposed method  and the state of the art r e-\nsults.  F-measure is defined  as the harmonic mean of pr e-\ncision and recall.  Statistic of note recognition precision \nand recall has been made upon consecut ive 186ms  \nframes . For ground truth MIDI, if 70% of some note lies \nin the frame the note is accounted and  there is  no fr e-\nquency tolerance . Threshold for drawing the diagram is \nimposed on sparse solution vector in each frame to filter \ninsignificant note dete ction according to its sparse sol u-\ntion value.  Different thresholds result in scatter points  in \nFigure 3. Two free parameters   and   are set to 0.8 and \n1.3 to balance reconstruction error and note statistics.  \n Prec. (%)  Rec. (%)  F-meas. (%)  \nNMF[4 ] 41.1 46.6 45.3 \nHTC[1 ] 57.4 51.3 54.2 \n JHT[2]  59.7 61.4 60.5 \nPCMPMCV  51.8 72.0 60.3 \nTable 1. Average multiple pitch estimation perfo r-\nmance on MIREX2007 dataset . \nFrom Fig ure 3 we can see that when co -occurrence \nnote information is integrated into PCMP , the precision \nincrease s about 6% whi le recall increase s by 2%~3%. \nWhen the note transition information is fused and  the op-\ntimal path decoding is applied , the precision and recall \nare further improved by 5% and 2% approximately.  From \nTable 1 and Figure 3 w e can find that if no threshold is \nimposed on  the sparse solution  of PCMPMCV , 72% of \nthe notes can be recalled while the precision is 5 1.8% re-\nsulting in an F-meas ure of  60.3 %. The recall of our best \nconfiguration outperform s state of the art result in [2] by \nmore than 10% while the precision is 8% lower, resulting \nin an F-measure 0.2% lower than  that reported in [2] . \n Prec. (%)  Rec. (%)  F-meas. (%) \nSpectral co n-\nstraints [20] 71.6 65.5 67.0 \nIsolated note \nspectra  [20] 68.6 66.7 66.0 \nDNMF -LV[9] 68.1 65.9 66.9 \nDNMF -AE[9] 66.8 68.7 67.8 \nSONIC [21] 74.5 57.6 63.6 \nPCMPMCV   60.7 77.3 68.0 \nTable 2. Average multiple pitch estimation perfo r-\nmance on MUS dataset . \nTable 2  shows the precision, recall and F-measure r e-\nsults on MUS data set. All parameters and setups are the \nsame as used in previous experiment except for the cond i-\ntional probability estimation. In this experiment the rest \ndata other than first 30 seconds are used to estimate co n-\nditional probabilities       . From Table 2 we can o b-\nserve that the proposed approach achieve s the highest r e-\ncall and F-measure of 77.3%  and 68%,  although  obtains \nthe lowest precision of 60.7%.  \nFigure 3. Note precision vs. recall of the two i m-\nprovements  \nFrom the two experiments, we can find that with st a-\ntistical musical knowledge sparse decomposition is i m-\nproved in terms of both precision and recall.  The pr o-\nposed  approach tends to obtain  superior  recall and F-\nmeasures but lower precision s compared with variant \nNMF and other methods . Higher recall means the more \ninformation is preserved in the decomposition results. \nSince our final aim of the decomposition is to provide \ndecent features for music classifications, the performance \nof our system is actually preferred.  Our higher recalls and \nF-measures are attribute d to the quality  of MIDI dictio n-\nary as well as statistical music knowledge  fused in sparse \ndecomposition . Longer analysis window is another i m-\nportant fac tor.  \nWhen comparing decomposition with the g round truth,  \nwe found numbers of instrument errors  even with correct \nnote detections , which  is likely  caused by mismatch es \nbetween the MIDI dictionary  and the real-world data. In \nsome  cases c oncurrent and transition probability of note s \ncan even  make incor rect compensation to original PCMP , \nwhich is probably  due to the  limitation of the naive Bayes \nmodel . To overcome these drawbacks, dictionary adapt a-\ntion techniques and sophisticated graphical model s will \nbe proposed and investigated in our future  work . \n4. CONCLUSION  \nWe have proposed in this paper a novel sparse music d e-\ncomposition approach  driven by music knowledge. It \nemploy s note statistical information to improve sparse \ndecomposition with a MIDI dictionary. In the frame  lev-\nel, music signal s are  decompos ed onto a MIDI dictionary \nwith a note co -occurrence heuristic. Transition probabil i-\nties are then computed between adjacent decomposition \ncandidates through  the whole frame sequence. The final \noptimal decomposition path is  then constructed by the \nViterbi al gorithm. Experiment al results show that  embe d-\nding concurrent note statistics in PCMP and applying a \nnote sequence heuristic allows improving  the note reco g-\nnition precision and re call. \n  \n \n5. ACKNOWLEDGMENT  \nThis work is partly supported by the French ANR under \nthe project VideoSense ANR -09-CORD -026. \n6. REFERENCES  \n[1] H. Kameoka, T. Nishimoto, and S. Sagayama, “A \nmulti -pitch analyzer based on harmonic temporal \nstructured clustering, ” IEEE Transactions on Audio, \nSpeec h, and Lan guage Processing  , vol. 15, no. 3, \npp. 982 –994, 2007 . \n[2] J. Wu, E. Vincent, S. A. Raczynski, T. Nishimoto, \nN. Ono, and S. Sagayama , “Multipitch estimation by \njoint modeling of harmonic and transient sounds, ” in \nProceedings of 2011 IEEE International  Conference \non Acoustics, Speech and Signal Processing \n(ICASSP) , pp. 25 –28, 2011 . \n[3] D. D. Lee and H. S. Seung , “Algorithms for non -\nnegative ma trix factorization, ” Advances in neural \ninformati on pro cessing systems , vol. 13, pp. 556 -\n562, 2001.  \n[4] S. A. Raczyn ski, N. Ono, and S. Sagayama, \n“Multipitch analysis with harmoni c nonnegative \nmatrix approxima tion,” in Proceedings of 8th \nInternational Conference on Music Information \nRetrieval  (ISMIR ), 2007.  \n[5] P. O. Hoyer, “Non -negative sparse coding,”  in \nProceedings of 12th IEEE Workshop on Neural \nNetworks for Signal Processing , pp. 557 – 565, 2002.  \n[6] S. Zafeiriou, A. Tefas, I. Buciu, and I. Pitas, \n“Exploiting discriminant information in nonnegative \nmatrix fac torization with application to frontal face \nverifi cation, ” IEEE Transactions on Neural \nNetworks , vol. 17, no. 3, pp. 683 –695, 2006.  \n[7] N. Guan, D. Tao, Z. L uo and B. Yuan, “Manifold \nregularized discriminative nonnegative matrix \nfactorization with fast gradient descent, ” IEEE \nTransactions on Ima ge Processing , vol. 20, no. 7, \npp. 2030 –2048, 2011.  \n[8] Y. Wang , Y. Jia,  C. Hu and M. Turk,  “Fisher non -\nnegative matrix fac torization for learning local \nfeatures, ” in Proceedings  of Asian Conference  on \nComp uter Vision  (ACCV) , 2004.  \n[9] N. Boulanger -Lewandowski, Y. Bengio, and P . \nVincent, “Discriminative non -negative matrix \nfactorization for  multiple pitch estimation, ” in \nProceedings of 13th In ternational Conference on \nMusic Information Retrieval  (ISMIR) , 2012  \n[10] D. Sakaue, T. Otsuka, K. Itoyama, and H.G. Okuno , \n“Bayesian non -negati ve harmonic -temporal \nfactorization and its ap plication to multipitch \nanalysis,” in  Proceedings  of 13th International Conference on Music Information Retrieval  \n(ISMIR ), 2012.  \n[11] B. Gao, E. Dellandr éa, and L. C hen, “Music sparse \ndecomposition onto a midi dictionary of musical \nwords and its application to music mood \nclassification, ” in Proceedings  of 10th IEEE  \nInternational Workshop on  Content -Based \nMultimedia Indexing (CBMI) , pp. 1–6, 2012.  \n[12] P. Leveau, E. Vincent,  G. Richard and L. Daudet, \n“Instrument -speci fic harmonic atoms for mid -level \nmusic repre sentation, ” IEEE Transact ions on Audio, \nSpeech, and Lan guage Processing , vol. 16, no. 1, pp. \n116-128, 2008.  \n[13] M. Elad, “Sparse and Redundant Representations  \nfrom Theory to Applications in Signal and Image \nProcessing ,” Springer -New York, 2010.  \n[14] A.M. Bruckstein, M. Elad, and M. Zibulevsky, \n“Sparse non -negative solution of a linear system of \nequations is unique, ” in Proceedings of 3rd IEEE  \nInternational Sympo sium on Communic ations, \nControl and Signal Pro cessing  (ISCCSP) , pp. 762 –\n767, 2008.  \n[15] S. S. Chen, D.  L. Donoho, and M.  A. Saunders, \n“Atomic decomposition by basis pursuit, ” SIAM \nreview , vol. 43, no. 1, pp. 129 –159, 2001.  \n[16] http://www.music -\nir.org/mirex/wiki/2007:Multiple_Fundamental_Freq\nuency_Estimation_%26_Tracking  \n[17] Y. C. Pati, R. Rezaiifar, P . S. Krishnaprasad, \n“Orthogonal matching pursuit: Recursive function \napproximation with applications to wavelet \ndecomposition. ” In Proc eedings of IEEE Signals, \nSystems and Computers , vol. 1, pp. 40 -44, 1993  \n[18] V. Emiya, R. Badeau, and B. David , “Multipitch \nestimation of piano sounds using a new probabilisti c \nspectral smoothness principle,” IEEE Transactions \non Audio, Speech, and Language Processing , vol. 18 , \nno. 6, pp.  1643 -1654 , 2010 . \n[19] G. Rath, and C. Christine,  “A complementary \nmatching pursuit alg orithm for sparse \napproximation,” in  Proc eedings of  European Signal \nProcess. Conf., Lausanne, Switzerland . 2008.  \n[20] E. Vincent, N. Bertin, and R. Badeau , “Adaptive \nharmonic spectral decomposition for multiple pitch \nestimation,” IEEE Transactions on Audio, Speech, \nand Language Processing , vol. 18, no. 3:pp. 528 –\n537, 2010.  \n[21] M. Marolt, “A connectionist approach to automatic \ntranscription of polyphonic piano m usic,” IEEE \nTransactions on Multimedia , vol. 6, no. 3, pp. 439 –\n449, 2004."
    },
    {
        "title": "Tempo Detection of Urban Music Using Tatum Grid Non Negative Matrix Factorization.",
        "author": [
            "Daniel Gärtner"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415552",
        "url": "https://doi.org/10.5281/zenodo.1415552",
        "ee": "https://zenodo.org/records/1415552/files/Gartner13.pdf",
        "abstract": "High tempo detection accuracies have been reported for the analysis of percussive, constant-tempo, Western music audio signals. As a consequence, active research in the tempo detection domain has been shifted to yet open tasks like tempo analysis of non-percussive, expressive, or non-western music. Also, tempo detection is included in a large range of music-related software. In DJ software, features like beat-synching or tempo-synchronized sound effects are widely accepted in the DJ community, and their users rely on correct tempo hypothesis as their basis. In this paper, we are evaluating both academic and commercial tempo detection systems on a typical dataset of an urban club music DJ. Based on this evaluation, we identify octave errors as a problem that has not yet been solved. Further, an approach based on non-negative matrix factorization is presented. In its current state it can compete with the state of the art. It further provides a foundation to tackle the octave error issue in future research.",
        "zenodo_id": 1415552,
        "dblp_key": "conf/ismir/Gartner13",
        "keywords": [
            "percussive",
            "constant-tempo",
            "Western music",
            "tempo detection",
            "non-percussive",
            "expressive",
            "non-western music",
            "DJ software",
            "beat-synching",
            "tempo-synchronized sound effects"
        ],
        "content": "TEMPO DETECTION OF URBAN MUSIC USING TATUM GRID\nNON-NEGATIVE MATRIX FACTORIZATION\nDaniel G ¨artner\nFraunhofer Institute for Media Technology IDMT\ndaniel.gaertner@idmt.fraunhofer.de\nABSTRACT\nHigh tempo detection accuracies have been reported for\nthe analysis of percussive, constant-tempo, Western mu-\nsic audio signals. As a consequence, active research in\nthe tempo detection domain has been shifted to yet open\ntasks like tempo analysis of non-percussive, expressive, or\nnon-western music. Also, tempo detection is included in\na large range of music-related software. In DJ software,\nfeatures like beat-synching or tempo-synchronized sound\neffects are widely accepted in the DJ community, and their\nusers rely on correct tempo hypothesis as their basis. In\nthis paper, we are evaluating both academic and commer-\ncial tempo detection systems on a typical dataset of an ur-\nban club music DJ. Based on this evaluation, we identify\noctave errors as a problem that has not yet been solved.\nFurther, an approach based on non-negative matrix factor-\nization is presented. In its current state it can compete with\nthe state of the art. It further provides a foundation to tackle\nthe octave error issue in future research.\n1. INTRODUCTION\nTempo detection on percussive music with constant tempo\nhas been extensively investigated by the music informa-\ntion retrieval community throughout the last 30 years, and\nhigh accuracies have been reported. Therefore, researchers\nhave moved on to related tasks like tempo detection of non-\npercussive music, dealing with soft onsets, or tempo/beat\ntracking of expressive performances, that are more difﬁcult\nto analyze correctly.\nSeveral comparative evaluations of tempo detection al-\ngorithms have been published. The results of the ISMIR\ntempo induction contest of 2004 are summarized in [9].\nIn a recent study [19], another 12 algorithms are investi-\ngated. In both studies, [11] outperforms the competing al-\ngorithms in most of the cases. Another comparative study\nis presented in [14], where algorithms of seven groups are\nanalyzed. It is shown, that the genre has an effect on the\ntempo detection performance. Also, algorithms performed\nquite differently within different tempo ranges. Further-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.more, some algorithms performed much worse on songs\nwith ternary meter compared to songs with binary meter,\nwhile in general, percussive music returned higher scores\nthan non-percussive music.\nTo motivate our work, a pre-study has been conducted,\nin which several algorithms have been evaluated on a urban\nclub music dataset of 1000 songs size (more details can be\nfound in Section 3). This study revealed, that the leading\nacademic tempo-detection algorithms reach up to 70% of\naccuracy on urban club music, which is less than expected\n(100% on reggae, soul, and rap are reported in [1], [8]\nachieved over 95% on constant rock and pop music).\nSeveral metrics are commonly used for tempo detection\nevaluation. The fraction of songs, for which the tempo has\nbeen correctly identiﬁed, is an intuitive measure. Often, an\nadditional metric is used [9, 19], in which tempo estimates\nthat are an integer multiple or divisor of the ground-truth\ntempo are also counted as correct estimates. This metric\nis motivated by the fact that even human listeners will not\nagree on a single tempo (another approach dealing with\nthis fact is the metric used in [14]). This is surely true for\na large quantity of music from several styles. However, as\nreported in [13], there is a high agreement in tempo per-\nception of urban club music amongst listeners, and it can\nbe assumed that the agreement is even higher amongst ur-\nban club music DJs, since it is their job to mix songs with\nthe same tempo. However, this assumption remains to be\nproven.\nFrom the perspective of the user of a DJ software, it\nis absolutely mandatory that the tempo is annotated cor-\nrectly. The so called octave errors are unacceptable. Al-\nthough not really related to the origin of the word octave\nin music, they refer to tempo estimates that are different\nfrom the correct tempo by a factor that is a power of two.\nSongs need to have the same tempo in order to be mixed\nwith clean transitions, which is requested by the dancing\naudience. Further, many audio effects also rely on correct\ntempo hypotheses. And of course, additional processing\nlike beat tracking, which can be used for automatic mix-\ning, also strongly depends on a correct tempo estimation.\n[13] shows that there is a signiﬁcant effect of music\ngenre on the most salient tempo, which is consolidated in\n[16], where a style detection method is used to improve\ntempo detection of ballroom dance songs. Further, [4] lists\nseveral cues that beat-tracking might proﬁt from a style-\nspeciﬁc analysis.\nBesides a limited tempo range of about 60 to 140 bpm,urban club music is percussive, has a constant tempo, and\nis composed of repeating drum patterns. Further, the drums\nare often sampled and triggered by a sequencer. This re-\nduces tempo ﬂuctuations and the variety in sound of mul-\ntiple occurrences of a certain drum.\nThe remainder of this paper is organized as follows. In\nSection 2, the proposed system is explained, followed by\nthe experimental setup and results section (3). The paper\nconcludes with a summary and outlook in Section 4.\n2. APPROACH\nAs [3] states, ”Perceptually, musical metric structure com-\nprises beats and hierarchies. Beats constitute the frame-\nwork in which successive musical events are perceived”.\nThe number of beats per minute is used to quantify the\ntempo of a musical piece, the beat grid speciﬁes the po-\nsition of the beats in a musical piece. The beat grid can\nbe further subdivided, leading to the tatum grid (or just\ntatum), which is the ”lowest regular pulse train that a lis-\ntener intuitively infers from the timing of perceived musi-\ncal events” [10]. Also, beats can be grouped in bars. In\nurban music, a bar mostly consists of four beats. On each\nmetrical level we will use the term grid to address the po-\nsitions of the pulses, and the term period for the time or\nframe interval between two successive pulses.\nAn overview of our approach is shown in Figure 1. From\nthe spectrogram of the audio data, quantized event bands\nare calculated from which the bar period is estimated, that\ndirectly leads to the tempo hypothesis. In order to extract\nthe quantized event bands, a tatum tracker is applied to an\nonset detection function. Next, the spectrogram is sampled\nat tatum grid positions and then factorized in event bands.\nThis is done by means of non-negative matrix factoriza-\ntion (NMF [12]), aiming at also isolating different drum\nclasses (bass drum, snare drum) in separate bands. Cal-\nculating NMF on just a subset of spectrogram frames is a\nmajor difference to our approach in [7].\nFigure 1 . System Overview.\n2.1 Onset Detection Function\nAt the beginning of the analysis, events that are supposed\nto contribute to the perception of beats in the musical piece\nare determined. As this study focuses on urban music, per-\ncussive drum events are investigated, and an onset detec-tion function that is sensitive to percussive events is cho-\nsen. An absolute spectrogram Sis calculated from the\n44.1 kHz audio input data, using a window-size of 4096\nsamples size with a hop-size of 512 samples, which corre-\nsponds to a spectrogram sample rate of 86.1 Hz.\nFrom S, an onset detection function dois calculated.\nThe onset components detection function developed in [8]\nis used as onset detection function.\n2.2 Constant Tempo Segment\nIn this component, the longest segment with approximately\nconstant tempo is identiﬁed. Analyzing audio data with\nseveral distinct tempi will harm the tatum period detection.\ndois convolved with a set of 601 comb grids, which cor-\nrespond to pulse trains of 20 s length from quarter notes be-\ntween 40 bpm and 640 bpm (sixteenth notes at 160 bpm).\nNext, each convolved function is cut into segments of 1\ns length, and the maximum value inside each segment is\nstored in a so called comb response matrix.\nTime [s]Tempo [bpm]\n100 200 300 400200\n400\n600\nFigure 2 . Comb response matrix of a song with multiple\ntempi, including the Viterbi path.\nAn example of a comb response matrix is shown in Fig-\nure 2. The tempo is constant over the ﬁrst half of the song,\nand then changes to another tempo. A strong response for\nthe grid corresponding to about 380 bpm, which equals a\npulse train of 16th notes at 95 bpm, can be observed in the\nﬁrst half. The second half is a little slower.\nNext, the entries in the comb response matrix are nor-\nmalized to unit sum for each column. Now, each entry in\nthe matrix can be seen as likelihood for observing a bpm\nclass at a given time in a hidden Markov model (HMM).\nThe Viterbi algorithm [18] is then used to track the most\nlikely path through the comb response matrix of observa-\ntions. From the path, the segment where the tempo stays\nroughly constant for the longest time is determined and fur-\nther investigated during tatum period detection.\n2.3 Tatum Period Detection\nSince dois an onset detection function sensitive to percus-\nsive events, it can be used for tatum period detection in the\nfollowing way. An accent function gois calculated from\nthe constant-tempo excerpt of doby keeping only the local\nmaxima of dowhile setting all other values to zero, and\nconvolving the resulting signal with a hann window (11\nsamples width). The tatum period detection function gtiscalculated: gt=R(R(go)), where R(f)is the autocorrela-\ntion of function f. The ﬁrst 400 values of gtcan be seen in\nFigure 3. Now, for each local maximum position on the lag\n0 100 200 300 40000.51\nLag [frames]gt\nFigure 3 . The tatum period detection function gt. Values\nclose to 1 indicate a high periodicity with a period of the\ncorresponding lag axis index. Solid vertical lines denote\nlocal maxima positions, dotted vertical lines denote multi-\nples of the tatum period.\naxis as tatum period candidate ci, we determine the small-\nest multiple miofcithat does not correspond to a local\nmaximum in gt. The tatum period candidate ciwith the\nlargest miis chosen as tatum period if it is also the local\nmaximum with the smallest lag. Otherwise, it’s lag value\nis divided by 2 until it is smaller than 14, which still allows\ntracking sixteenth notes at 180 bpm. This additional step is\nrequired, since not even all the rhythmic events necessarily\nfall exactly on the tatum grid. If a certain drum event in a\nbar is systematically played early or late, this will lead to\nslightly shifted locations of local maxima in gt. This can\nalso be observed in Figure 3, where the odd local maxima\n(solid lines) do not exactly overlap with the tatum period\nand its multiples (dotted lines).\n2.4 Tatum Grid\nAfter having determined the tatum period, the onset detec-\ntion function dois convolved with a comb grid where the\ncombs are tatum period spaced. This will strengthen ac-\ncents in dothat lie on the not yet determined tatum grid,\nand extenuate all other accents. Now, all dominant local\nmaxima positions and their pairwise differences are deter-\nmined. Only those pairs, for which the difference is ap-\nproximately 1, 2, or 3 times the tatum period, are kept.\nThen, dynamic programming [2] is used to ﬁnd the longest\npath over the remaining pairs. Using this approach, vari-\nations around a center tempo can be compensated. How-\never, if a song contains multiple different tempi, the path\nwill only cover the region that has a beat period that is\napproximately a multiple of the tatum period hypothesis.\nIn [6], dynamic programming is used for beat tracking.\nThe approaches differ mostly in the fact that we only al-\nlow local maxima in the underlying detection function as\npotential tatum grid anchors.\n2.5 Signal Decomposition\nThe determined tatum grid positions are reﬁned to the lo-\ncations of close local maxima in do. This way, also early\nand late played events can be incorporated. Then, the spec-\ntrogram is subsampled at the tatum grid positions, and theresulting matrix is factorized using NMF [12]. In NMF,\na non-negative matrix Vis factorized in matrix factors W\nandH,V\u0019WH . Applied to an absolute spectrogram,\nit is factorized in a set of components where the character-\nistic spectrum of each component is stored in Wand the\nactivation of each component is stored in H. From factor-\nization, we expect to separate different drum classes in dif-\nferent bands, and also separate additional, sources in addi-\ntional bands (e.g., separate bass drum and bass even though\nthey might be overlapping in the spectrogram). Since the\ndrum tracks in urban club music are often generated using\ndrum machines, samplers, and sequencers, NMF seems to\nbe a good choice for decomposition, since drum sounds are\nnot supposed to vary over time. In [7] we observed better\nresults in tempo recognition on urban music using NMF\nbased decomposition compared to a ﬁlter bank approach.\nNMF is used to factorize the subsampled spectrogram in\n24 components. Both bases and activations are randomly\ninitialized, and 50 iterations of multiplicative update rules\nusing Kullback-Leibler divergence are performed. These\nparameters have not yet been quantitatively optimized in\nany way.\nNMF has been shown to be able to separate drum events\nin, e.g., [15], where NMF is used for drum transcription in\npolyphonic music.\nFactorizing a spectrogram that is subsampled at tatum\ngrid positions instead of the full spectrogram reduces the\ncomputational load. A more general advantage of working\non a tatum grid level lies in removing tempo variations.\nHowever there are disadvantages as well. The decom-\nposition is dependent on the quality of the tatum grid. If\nthe tatum grid does not contain the drum event onsets, the\ndrums will not be analyzed at all. Further, in cases where\ndifferent drum events fall, for example, on the same beat\nbut at slightly different times, the spectral frame that is\nmeant to capture these events might not capture them all.\n2.6 Bar Period Detection\nThe 24 bands obtained by the NMF analysis are again an-\nalyzed using a comb grid. Each of the bands is convolved\nwith a set of comb grid with combs spaced 1, 2, ... , n,\nwhere n corresponds to the largest possible distance be-\ntween 4 beats at 65 bpm (see Section 3.5.2), considering\nthe underlying tatum grid. For each band, from the corre-\nsponding n ﬁltered functions, the index of the one with the\nlargest variance is determined.\nFrom all 24 collected indices, a histogram is calculated\nand the most frequent index is chosen as tatum-period-to-\nbar-period factor z. The bar period is calculated by mul-\ntiplying Zand the tatum period. As ﬁnal steps, the de-\ntermined bar period is ﬁrst divided by four to retrieve the\nbeat period, since 4/4 is the most common measure in ur-\nban club music. As a ﬁnal step, the retrieved beat period is\ntransformed in the bpm range of 65 bpm to 130 bpm (see\nSection 3.5.2) by doubling or halving.3. EVALUATION\nIn this section we describe the experiments, present the re-\nsults, and discuss them.\n3.1 Measures\nFor each evaluation run, we report the relative number of\nsongs with a tempo hypothesis that differs less then 4%\nfrom the ground-truth tempo (Acc1). To gather further in-\nsights on the performance of the evaluated algorithms, we\nfurther report the relative number of songs with two times\nor three times the correct tempo (Acc2), and one half or\none third of the correct tempo (Acc3). The fraction of the\nremaining songs is denoted Acc4.\n3.2 Dataset\n2324 songs have been collected from an urban music pro-\nmotion platform exclusively for DJs. Labels provide songs\nto DJs over this kind of platforms at no charge, in return the\nDJs will help to promote these songs and make them popu-\nlar by playing them in their sets in the club. It is an authen-\ntic set that well represents the kind of music, urban club\nmusic DJs are working with. Each of the songs has been\ntempo-annotated by an experienced urban music DJ. The\nset has been randomly split into a development set (1000\nsongs, denoted dev) and a test set (1324 songs, denoted\ntest).\nDevhas been used for the development of the algorithm,\nwhich includes the design of the components and param-\neter setting. The ﬁnal algorithm has then been evaluated\nusing test, the results for devare also reported.\nFigure 4 shows a histogram over the observed tempi in\ndev.\n50 100 150050\nBPMOccurrences\nFigure 4 . Histogram over the ground truth tempi in dev\nAlthough it has been argued, that urban club music in\ngeneral is strongly percussive and songs have a constant\ntempo, there are exceptions. A few songs in the dataset\ndo not contain any or only soft percussions. Some are\nplayed by live bands (e.g., The Roots) which leads to vary-\ning tempo. Another source for varying tempo is the use of\nsampling in the music, where the samples vary according\nto their tempo. And there are even a few songs that are cut\ntogether in a way that the segments are not concatenated\non beat, or even the tempo is different for the segments.\nFurther, a few songs in the database do not belong to the\nurban music genre, like some pop rock songs, that are also\nresponsible for the upper tempo outliers in Figure 4. We\ndecided to keep them in the database, since we wanted to\nkeep it exactly the way it was obtained. However, genredetection algorithms could be used to identify songs like\nthat prior to tempo analysis.\n3.3 Preprocessing\nThe data consists of MP3 ﬁles of different sample rates\nand bit rates. FFmpeg has been used to convert the ﬁles to\n44.1 kHz mono PCM wav ﬁles.\n3.4 Algorithms\nThe approaches from Dixon [5], Ellis [6], and Klapuri [11]\nhave been selected to represent the academic systems. [6]\nreturns two tempo hypotheses, from which the stronger one\nis selected. [11], and [5] return a beat grid. We calculated\nhistograms over inter-beat-intervals, and then determined\nthe mean of all inter-beat-intervals that contributes to the\nmost frequent inter-beat-interval, from which the tempo in\nbpm can be derived. All implementations were obtained\nfrom the authors.\nIn addition, several commercial DJ systems have been\nevaluated. Each one offers ways to parameterize the tempo\nanalysis algorithm. Cross 1.7.0, denoted Cross1, offers\nthree different tempo ranges for bpm analysis, each one\ncovering exactly one octave. 75-150 has been selected for\nanalysis. Scratch Live 2.4.1, denoted SL2, offers ﬁve dif-\nferent tempo ranges for analysis, each one covering one\noctave. 68 - 135 has been selected for analysis. Torq\n2.0.3, denoted Torq3, offers several genre-speciﬁc tempo\nranges. In our experiments, the default settings have been\nused, returning bpm values from 60 to 160 bpm. Traktor\nPro 2.6.0, denoted TraA+B4, offers 9 different octaves,\nfrom which 68-135 (TraB) has chosen. Further, Traktor\nalso offers a single range covering more than a octave (60-\n200), which will be denoted TraA. Virtual DJ Home 7.0.5,\ndenoted VDJ, offers an option to allow also bpm values\nsmaller than 80 bpm, which was activated. It returned\ntempi between 60 and 170 bpm. It is worth noticing, that\nsome of the investigated tools only offer tempo ranges of\nexactly one octave, which is a simple but (as can be seen in\nthe Section 3.5) working approach to reduce octave errors\nat least for urban club music, since a large amount of urban\nclub music is located inside a single octave.\n3.5 Results\nIn this section the results of the conducted experiments are\npresented and discussed. All experiments have been per-\nformed in Matlab.\n3.5.1 Evaluation of the reference systems\nTable 1 lists the results returned from the evaluation of the\nstate of the art.\nDirectly comparing the results for devandtest, one can\nsee that the performances are similar, which indicates that\nboth sets are comparably difﬁcult. This is also true for the\n1http://www.mixvibes.com\n2http://serato.com/scratchlive\n3http://www.torq-dj.com/\n4http://www.native-instruments.com/traktorAcc1 Acc2 Acc3 Acc4\n1 2+3 1/2+1/3 other\nCross 73.2/75.6 23.2/22.9 1.2/0.5 2.4/1.0\nSSL 89.4/89.4 8.2/8.5 1.3/1.2 1.1/1.0\nTorq 85.6/84.3 2.9/4.4 4.3/3.4 7.2/7.9\nVDJ 81.0/78.5 17.0/20.4 1.1/0.9 0.9/0.2\nTraA 77.6/79.1 15.3/14.7 5.1/4.5 2.0/1.7\nTraB 90.3/90.7 6.1/6.2 1.5/1.4 2.1/1.7\nDixon 25.3/24.5 69.8/70.1 0.0/0.0 4.9/5.4\nEllis 57.5/51.5 4.5/5.4 19.3/26.5 18.7/16.5\nKlapuri 68.7/71.7 28.8/27.2 1.8/0.8 0.7/0.3\nTable 1 . Results for the state of the art algorithms on dev/\ntest, accuracies in %.\nresults of the proposed approach, listed in Table 2, which\nshows, that even though it has been optimized using devit\nstill generalizes well.\nWith an accuracy of 73.2%, Cross is the worst of the\ncommercial algorithms. This is mainly caused by the lim-\nited choices of the tempo range, of which no one really ﬁts\nour data well. Since all commercial algorithms do a pretty\ngood job in choosing the correct tempo (as long as octave\nerrors are still accepted as correct), the performance mainly\ndepends on the prior choice of the bpm-analyzing octave.\nTraktor offers both a large tempo range (60-200, TraA) and\na suitable octave tempo range (68-135, TraB). TraB has an\naccuracy of about 12.7% higher than TraA, which shows,\nthat automatically picking the right tempo octave is still an\nopen issue.\nDixon often returns twice the correct tempo (69.8%)\nand could be simply tuned by halving the returned tempo\nestimate. For Ellis, almost 20% of the songs in devare\nneither correct nor do they belong to one of the octave er-\nror classes. The most common cases in ”other” of Ellis\nare 4/3 (15%) and 2/3 (4%). In accordance with the men-\ntioned MIREX benchmarks, Klapuri is the best performing\nacademic algorithm.\nIn the ﬁrst category (Acc1), the commercial approaches\noutperform the academic ones. In this category, the com-\nmercial approaches can beneﬁt by the fact that most of the\ndata is located in a single octave. However, in the fourth\ncategory (other), Klapuri’s algorithm is among the best.\n3.5.2 Best Tempo Octave\nBased on the data from dev, an experiment has been con-\nducted to determine the best tempo octave settings, assum-\ning that an algorithm performs perfectly but transforms its\nresults in a speciﬁed tempo range of exactly one octave by\ndoubling or halving the tempo several times. The accuracy\ndepending on the given tempo range is plotted in Figure 5.\nThe best performance (92.2%) is achieved for a bpm range\nof 65 - 130 bpm. Therefore, in the presented approach, a\ntempo hypothesis is transformed in this range by doubling\nand halving.\n3.5.3 Evaluation of the proposed approach\nTable 2 contains the results for the evaluation of the pre-\nsented algorithm. For both devandtest, the algorithm re-\nturns the highest accuracies retrieved in the whole study.\n50 100 15000.51\nLower Tempo Octave Border [bpm]AccuracyFigure 5 . Accuracy of a perfect algorithm with octave re-\nstriction depending on the allowed range.\nAcc1 Acc2 Acc3 Acc4\n1 2+3 1/2+1/3 other\nOwn rel. 91.9/92.5 4.9/4.5 2.9/2.7 0.3/0.2\nOwn abs. 919/1225 49/60 29/36 3/3\nTable 2 . Results for the presented algorithm on dev/test,\naccuracies in %, and absolute number of songs in each cat-\negory.\nFor both devandtest, only three songs return a tempo\nthat is not 2ntimes the correct tempo. The three failing\nsongs in devall have a 3/4 measure, but a 4/4 measure is as-\nsumed when going from bar period to beat period. Two of\nthe failing songs in testhave no or almost no drum tracks,\nthe third one has also a measure of only 3 beats length.\nThe chosen tempo range (that has been determined from\nthe bpm distribution in dev) induces the assignment to one\nof the accuracies Acc1, Acc2 and Acc3. An Acc1 of 92.5%\nconﬁrms the choice of the bpm range.\n1 2 3 4 6 8 912 other\ndev 1188 8761 632 1 3 0\ntest 0279 7992 541 0 0 0\nTable 3 . Distribution of the songs in tatum classes.\nTable 3 shows the performance of the tatum period detec-\ntion component. The songs are differentiated into tatum\nclasses, where the class name denotes the ratio of ground\ntruth beat period and determined tatum period. For all in-\nstances, the beat period is an multiple of the tatum period.\nThe most common multiple is 4, which corresponds to 16th\nnotes. Assuming 16th notes as tatum for each song, and\ntherefore estimating the beat period as four times the tatum\nperiod, the tempo could be correctly determined for 76.1%\n(dev) and 74.9% ( test) of the songs without making any as-\nsumptions on a bpm range, which outperforms any of the\nacademic approaches.\nThe distribution of the songs in bar classes is listed in\nTable 4. For both sets, for most of the songs a bar-length\nof 4 beats is returned. Assuming the determined bar period\nto be four times the beat period, the tempo could be deter-\nmined correctly for 81.5% ( dev) and 82.0% ( test) respec-\ntively, without making any assumptions on a bpm range,\nwhich again outperforms any of the academic approaches.\nBoth tables reveal the strength of the algorithm, but at\nthe same time also show limits regarding to octave deter-\nmination, as also shown in [7].1 2 3 4 6 8 other\ndev 4166 3 815 012 0\ntest 0214 01085 122 2\nTable 4 . Distribution of the songs in bar classes.\n4. CONCLUSION AND OUTLOOK\nIn this paper, it is shown that ﬁnding the correct octave is\nstill an issue for even urban club music. This claim is con-\nsolidated by evaluating several academic and commercial\ntempo detection algorithms on a urban club music data set.\nThe presented algorithm, developed speciﬁcally for urban\nmusic, outperforms all the other algorithms evaluated in\nthis study, estimating the correct tempo for 92.5% of the\ntestset. The remaining songs are a 2nmultiple of the cor-\nrect tempo except for 3 out of 1324 songs.\nTherefore, the proposed algorithm provides a good ba-\nsis for further processing, in which the correct octave has\nto be determined. In the current approach, all tempo val-\nues are forced to be in the range of 65 to 130 bpm. Further\nexperiments will be conducted where the octave is chosen\nbased on the musical structure. A ﬁrst investigation on the\ntatum-quantized activations indicated, that they still cap-\nture the dominant drum events, contributing to the rhythm\nof a song. Therefore, drum pattern analysis, as as per-\nformed in, e.g., [17] could be carried out on the activations,\nand then be incorporated in tempo detection. Finding the\ncharacteristic drum pattern of a song also offers additional\nopportunities like drum pattern similarity or urban music\nsub-genre classiﬁcation.\n5. ACKNOWLEDGEMENT\nWe want to thank Simon Dixon, Daniel P.W. Ellis, and\nAnssi Klapuri for providing implementations of their al-\ngorithms.\n6. REFERENCES\n[1] Miguel Alonso, Bertrand David, and Gael Richard.\nTempo and beat estimation of musical signals. In Pro-\nceedings of the 5th International Conference on Music\nInformation Retrieval (ISMIR) , 2004.\n[2] Richard Ernest Bellman. Dynamic Programming .\nPrinceton University Press, 1957.\n[3] Jeff A. Bilmes. Timing is of essence . PhD thesis, Mas-\nsachusetts Institute of Technology, 1993.\n[4] Nick Collins. Towards a style-speciﬁc basis for compu-\ntational beat tracking. In Proceedings of the 9th Inter-\nnational Conference on Music Perception & Cognition ,\n2006.\n[5] Simon Dixon. Evaluation of the audio beat tracking\nsystem beatroot. Journal of New Music Research , (36),\n2007.[6] Daniel P. W. Ellis. Beat tracking by dynamic program-\nming. Journal of New Music Research , 36(1):51–60,\n2007.\n[7] Daniel G ¨artner. Tempo estimation from urban music\nusing non-negative matrix factorization. In Proceed-\nings of the 42th AES International Conference , pages\n208–215, Ilmenau, 2011.\n[8] Masataka Goto. A real-time beat tracking system for\naudio signals. In Proceedings of the International\nComputer Music Conference , 1995.\n[9] Fabien Gouyon, Anssi Klapuri, Simon Dixon, Miguel\nAlonso, George Tzanetakis, Christian Uhle, and Pedro\nCano. An experimental comparison of audio tempo in-\nduction algorithms. IEEE Transactions on Speech and\nAudio Processing , 14:1832–1844, 2006.\n[10] Tristan Jehan. Creating Music by Listening . PhD the-\nsis, Massachusetts Institute of Technology, 2005.\n[11] Anssi Klapuri, Antti Eronen, and J. T. Astola. Analysis\nof the meter of acoustic musical signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n14(1):342–355, 2006.\n[12] Daniel D. Lee and H. Sebastian Seung. Algorithms for\nnon-negative matrix factorization. Advances in neural\ninformation processing systems , 2001.\n[13] Martin F. McKinney and Dirk Moelants. Ambiguity\nin tempo perception: What draws listeners to differ-\nent metrical levels? Music Perception: An Interdisci-\nplinary Journal , 24(2):155–266, 2006.\n[14] Martin F. McKinney, Dirk Moelants, Matthew E. P.\nDavies, and Anssi Klapuri. Evaluation of audio beat\ntracking and music tempo extraction algorithms. Jour-\nnal of New Music Research , 36(1):1–16, 2007.\n[15] Jouni K. Paulus and Anssi Klapuri. Drum transcription\nwith non-negative spectrogram factorisation. In Pro-\nceedings of the 13th European Signal Processing Con-\nference (EUSIPCO) , 2005.\n[16] Bj ¨orn Schuller, Florian Eyben, and Gerhard Rigoll.\nTango or waltz?: Putting ballroom dance style into\ntempo detection. EURASIP Journal on Audio, Speech,\nand Music Processing , 2008(6):1–12, 2008.\n[17] Christian Uhle. Automatisierte Extraktion rhythmis-\ncher Merkmale zur Anwendung in Music Information\nRetrieval-Systemen . PhD thesis, Technische Univer-\nsit¨at Ilmenau, Ilmenau, 2008.\n[18] Andrew J. Viterbi. Error bounds for convolutional\ncodes and an asymptotically optimum decoding al-\ngorithm. IEEE Transactions on Information Theory ,\n13(2):260–269, 1967.\n[19] Jose R. Zapata and Emilia G ´omez. Comparative eval-\nuation and combination of audio tempo estimation ap-\nproaches. In Proceedings of the 42th AES International\nConference , Ilmenau, 2011."
    },
    {
        "title": "Automatic Alignment of Music Performances with Structural Differences.",
        "author": [
            "Maarten Grachten",
            "Martin Gasser",
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416628",
        "url": "https://doi.org/10.5281/zenodo.1416628",
        "ee": "https://zenodo.org/records/1416628/files/GrachtenGAW13.pdf",
        "abstract": "Both in interactive music listening, and in music performance research, there is a need for automatic alignment of different recordings of the same musical piece. This task is challenging, because musical pieces often contain parts that may or may not be repeated by the performer, possibly leading to structural differences between performances (or between performance and score). The most common alignment method, dynamic time warping (DTW), cannot handle structural differences adequately, and existing approaches to deal with structural differences explicitly rely on the annotation of “break points” in one of the sequences. We propose a simple extension of the Needleman-Wunsch algorithm to deal effectively with structural differences, without relying on annotations. We evaluate several audio features for alignment, and show how an optimal value can be found for the cost-parameter of the alignment algorithm. A single cost value is demonstrated to be valid across different types of music. We demonstrate that our approach yields roughly equal alignment accuracies compared to DTW in the absence of structural differences, and superior accuracies when structural differences occur.",
        "zenodo_id": 1416628,
        "dblp_key": "conf/ismir/GrachtenGAW13",
        "keywords": [
            "alignment",
            "interactive music listening",
            "music performance research",
            "dynamic time warping",
            "structural differences",
            "needleman-wunsch algorithm",
            "audio features",
            "cost-parameter",
            "cost value",
            "structural differences"
        ],
        "content": "AUTOMATIC ALIGNMENT OF MUSIC PERFORMANCES WITH\nSTRUCTURAL DIFFERENCES\nMaarten Grachten1Martin Gasser1\n1Austrian Research Institute for\nArtiﬁcial Intelligence (OFAI), Vienna, Austria\nhttp://www.ofai.at/ ˜maarten.grachtenAndreas Arzt2, Gerhard Widmer1,2\n2Dept. of Computational Perception\nJohannes Kepler Universit ¨at, Linz, Austria\nABSTRACT\nBoth in interactive music listening, and in music perfor-\nmance research, there is a need for automatic alignment of\ndifferent recordings of the same musical piece. This task\nis challenging, because musical pieces often contain parts\nthat may or may not be repeated by the performer, possi-\nbly leading to structural differences between performances\n(or between performance and score). The most common\nalignment method, dynamic time warping (DTW), cannot\nhandle structural differences adequately, and existing ap-\nproaches to deal with structural differences explicitly rely\non the annotation of “break points” in one of the sequences.\nWe propose a simple extension of the Needleman-Wunsch\nalgorithm to deal effectively with structural differences,\nwithout relying on annotations. We evaluate several au-\ndio features for alignment, and show how an optimal value\ncan be found for the cost-parameter of the alignment al-\ngorithm. A single cost value is demonstrated to be valid\nacross different types of music. We demonstrate that our\napproach yields roughly equal alignment accuracies com-\npared to DTW in the absence of structural differences, and\nsuperior accuracies when structural differences occur.\n1. INTRODUCTION AND RELATED WORK\nA variety of music processing scenarios involve alignment\nof music in the form of either symbolic scores, or audio\nrecordings (or both). In some cases, alignment is used to\ncompute a similarity score between instances of a musi-\ncal piece. This is useful for example in plagiarism de-\ntection [7] and cover song identiﬁcation [3, 19]. In other\ncases, it is the alignment itself that is of use. Examples are\nautomatic transcription [20], computer assisted music pro-\nduction [14], real-time score-following for automatic page\nturning [1], and automatic accompaniment [5, 6].\nThere are several factors that make accurate alignment\nof music a challenging task. Firstly, in case of audio align-\nment, the acoustic properties of the recordings may be very\ndifferent, due to differences in instrumentation, recording,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.mixing, and mastering. Secondly, interpretations of mu-\nsical pieces by human performers tend to have expressive\nvariations, causing different interpretations of a piece to di-\nverge in both global and local tempo and dynamics. Thirdly,\nperformance errors may lead to occasional missing, or in-\nserted notes. A fourth complicating factor is the fact that\nmusical pieces are often composed of smaller musical units,\nwhere units may be repeated or not, or even left out com-\npletely, according to the taste of the musician or conductor.\nThis may lead to what we refer to as structural differences\nbetween performances of the piece.\nThe problem of aligning music with structural differ-\nences has been addressed in a number of studies. In most\nof these, the problem setting is score-to-performance align-\nment, in which a symbolic representation of a musical score\nis mapped to a performance of that score. In a symbolic\nscore representation, it is relatively easy to mark points\nwhere performances are likely to diverge. For example,\nFremerey et al. [9] develop a method that relies on explicit\nannotations of possible jump points in the score where dou-\nble bar lines occur. A similar approach is taken by Pardo\nand Birmingham [18].\nIn performance-performance alignment, as opposed to\nscore-performance alignment, it is generally not possible\nto rely on such annotations, since there is no score rep-\nresentation involved. M ¨uller and Appelt [16] propose a\nmethod to deal with structural differences in performance-\nperformance alignment. This approach uses dynamic time\nwarping (DTW) in combination with pre-processing of the\nsimilarity matrix, and post-processing of alignment paths.\nIn this paper we start from the observation that DTW\nhas shortcomings when dealing with structural differences\nin music recordings (Section 2). Our intention is to show\nthat other variants of dynamic programming alignment are\nmore effective. In particular, it is beneﬁcial to include\nskip operations, as well as one-to-many and many-to-one\nmatching, as in the algorithm of Mongeau and Sankoff [13],\nwho use this approach for measuring similarity between\nmelodies as sequences of notes, and Grachten et al. [10],\nwho design alignment operations to capture the semantics\nof expressive musical behavior, like spontaneous ornamen-\ntations of notes in a performance. To our knowledge, such\nextensions of the classical dynamic programming variants\nhave not been used in the context of audio alignment.\nAlong with this alternative alignment method (Section\n3.1), we propose a method to estimate the optimal value forthe gap penalty, a parameter that controls the behavior of\nthe alignment (Section 3.3). In Section 4, we experimen-\ntally determine the utility of various audio features with\nrespect to the effectiveness of the gap penalty across differ-\nent types of music. Based on the most successful feature,\nand the corresponding optimal gap penalty, we perform a\nquantitative evaluation of the alignment accuracy of our\nproposed approach, in comparison to DTW (Section 4.2).\n2. PROBLEM DESCRIPTION\nAligning two sequences requires a distance measure that\nquantiﬁes how different their elements are. In Section 4.1.1,\nwe will discuss different types of features and distance\nmeasures in more detail in the context of audio alignment.\nFor now, let sandtbe the sequences to be compared, of\nlengthsMandNrespectively. We refer to sandtas the\nsource andtarget sequence, respectively. We use d(i;j)to\ndenote the distance between the i-th element of sand the\nj-th element of t, where 1\u0014i\u0014Mand1\u0014j\u0014N.\n2.1 Dynamic time warping (DTW)\nDTW computes the minimal cost of aligning sandt. It can\nbe expressed as dtw (M;N ), where dtw is deﬁned by the\nrecursive equation:\ndtw(i;j) =8\n>>>><\n>>>>:0; ifi= 0,j= 0\n1; ifi= 0,j6= 0orj= 0,i6= 0\nd(i;j) + min8\n<\n:dtw(i\u00001;j\u00001))\ndtw(i\u00001;j)\ndtw(i;j\u00001);otherwise\n(1)\nThe alignment that leads to dtw (M;N )is called the op-\ntimal alignment , and can be easily recovered by keeping\ntrack which argument of the min operator is selected in\n(1).\nAs the name of the algorithm states, dynamic time warp-\ning is a method to align sequences that are time warped\nversions of each other. That means that the sequences rep-\nresent the same order of events, but the duration of events\nmay differ from one sequence to the other. This time warp-\ning assumption explains why in DTW, each element of\none sequence must be matched to an element of the other\nsequence. When the sequences are structurally different\nhowever, this assumption is violated: the sequences con-\ntain elements that are not to be matched to elements in\nthe other sequence. By forcing a match between elements,\nDTW produces undesired alignments in such cases.\n2.2 Needleman-Wunsch alignment (NW)\nA solution to this problem is to allow the alignment al-\ngorithm to skip unmatchable parts of either sequence. The\ncost of skipping should not be proportional to the distances\nbetween the elements of the sequences, since these dis-\ntances are not relevant in the case of unmatchable sequences.\nThis type of alignment is achieved by another member of\nA1 A2 A3 A4 A1 A2 A3 A4 B1 B2\nSequence sA1A2A3A4B1B2Sequence tDTW\nNWFigure 1 . Distance matrix between two structurally dif-\nferent sequences; Dark cells represent low distances, light\ncells high distances; The DTW path jumps over a repeated\nsection ‘uncleanly’, the NW path makes a clean jump\nthe family of dynamic programming algorithms for opti-\nmal sequence alignment – the Needleman-Wunsch algo-\nrithm (NW) [17]. This algorithm computes the minimal\ncost nw (M;N )of aligningsandtusing the equation:\nnw(i;j) =8\n>>>>>><\n>>>>>>:0; ifi= 0,j= 0\n\r+nw(i;j\u00001); ifi= 0,j6= 0\n\r+nw(i\u00001;j); ifj= 0,i6= 0\nmin8\n<\n:d(i;j) +nw(i\u00001;j\u00001)\n\r+nw(i\u00001;j)\n\r+nw(i;j\u00001);otherwise\n(2)\nwhere\ris a constant referred to as gap penalty . (2)\nshows that the distance d(i;j)between two elements iand\njis only relevant to the alignment when it is sufﬁciently\nlow. As soon as d(i;j)> \r, the algorithm will favor an\ninsertion or deletion to a match (the ﬁnal decision for an\ninsertion or a deletion will only be made after the algorithm\nhas processed the sequences entirely).\nThe difference between DTW and NW is illustrated in\nFigure 1, displaying the distances between the elements of\ntwo artiﬁcial sequences sandt, and the optimal DTW and\nNP alignments. The rows and columns are labeled to clar-\nify the structure of sandt. In particular, sconsists of two\nrepetitions of a 4-tuple A, plus a 2-tuple B. Sequencetis\nthe concatenation of one instance of A, andB. The DTW\npath aligns the elements of the second Ainspartly to el-\nementA4int, and partly to element B1, where the exact\nalignment depends on the distances between those (non-\nmatching) elements. The NW path (computed with a suit-\nable value for \r) favors the deletion of the elements of the\nsecond occurrence of Aover a sequence of poor matches.\nNote that this yields a clean and intuitive jump of the NW\npath across the second Ains.\n2.3 Two problems of NW alignment\nThe use of Needleman-Wunsch for aligning music recor-\ndings introduces two problems. The ﬁrst is that although\nNW handles structural differences, it does not handle timewarping . Since elements in the sequences can be either\nmatched to a single other element, or skipped entirely, there\nis no way to deal with the fact that the music of the two re-\ncordings may be played at different tempos. Fortunately,\na simple extension of the Needleman-Wunsch algorithm,\ndescribed in Section 3.1, remedies this shortcoming.\nThe second problem is that – unlike DTW in its ba-\nsic form1– NW involves a parameter \r, and the quality\nof the alignment will depend on the value of \r. Which\nvalue of\rgives good alignments will depend on the audio\ncontent and the features used to represent that content. In\nSection 3.3, we propose a method to estimate the optimal\nvalue for\rbased on empirical data. In Section 4, we use\nthis method to evaluate different features on various types\nof music.\n3. PROPOSED SOLUTION\nIn this section we propose solutions to the two problems\nof NW alignment described above. Firstly, we propose an\nextension of the NW algorithm to deal with the time warp-\ning aspects of aligning music performances. Secondly, we\ndescribe a method to estimate the gap penalty \r.\n3.1 Needleman-Wunsch time warping (NWTW)\nDTW handles time warping by matching multiple elements\nof one sequence to a single element in the other sequence.\nAlthough this is not possible in the original NW algorithm,\nit is easy to add further arguments to the min operator, that\nrepresent many-to-one and one-to-many operations. In the\nfollowing equation, which is a revision of (2), a 1-to-2 and\na 2-to-1 operation have been included:\nnw(i;j) =8\n>>>>>>>>>><\n>>>>>>>>>>:0; ifi= 0,j= 0\n\r+nw(i;j\u00001); ifi= 0,j6= 0\n\r+nw(i\u00001;j); ifj= 0,i6= 0\nmin8\n>>>><\n>>>>:d(i;j) +nw(i\u00001;j\u00001)\nd(i;j)+d(i;j\u00001)+ nw(i\u00001;j\u00002)\nd(i;j)+d(i\u00001;j)+nw(i\u00002;j\u00001)\n\r+nw(i\u00001;j)\n\r+nw(i;j\u00001); otherwise\n(3)\nAppropriate names for these operations are lengthen and\nshorten , respectively, since the ﬁrst is cost-effective when\nthe music in tis up to two times slower than the music in\ns, and the second is cost-effective when it is (up to two\ntimes) faster. In case there is only a slight difference in\ntempo between sandt, shorten and lengthen operations\noccur only occasionally among a majority of match opera-\ntions. Additional operations may be deﬁned to handle even\ngreater tempo differences, but such differences rarely oc-\ncur in practice.\n3.2 Algorithmic complexity\nThe NW algorithm – like DTW – requires the computation\nof a full matrix of intermediate results which are assembled\n1Extensions of DTW that include weights for operations are discussed\nin [15]into the ﬁnal result in a backtracking step. This implies\ntime and space requirements of order O(MN), whereM\nandNare the lengths of the two sequences. Compared\nto NW, our extension NWTW introduces a higher per-cell\ncost during the construction of the dynamic programming\nmatrix, since we add lengthen andshorten operations that\nhave to be taken into consideration when ﬁnding the opti-\nmal operation in (3). However, as this cost is constant and\nnot dependent on MandN, it does not change the overall\ncomplexity of the algorithm.\nIn practice, NWTW based on fully computing the dy-\nnamic programming matrix is feasible on current desktop\ncomputers for audio ﬁles up to about 15 minutes. For\nlonger audio ﬁles, we use multi-step dynamic program-\nming [15], where full dynamic programming is used for\ndownsampled feature vectors. Subsequent alignments for\nhigher resolution feature vectors are computed for a band\nof ﬁxed width around the previously computed (coarse)\nalignment path.\n3.3 Estimation of optimal gap penalty \r\nThe gap penalty \rvalue serves as an upper bound on the\ndistance between pairs of elements that are considered to\nmatch: if the distance is larger than \r, the alignment will\nfavor skipping one of the elements. This means that the\nchoice of\ris essentially a binary classiﬁcation problem,\nin which pairs of elements are to be classiﬁed as match\nornon-match , based on their distance. Let p(xjmatch )\ndenote the distribution of distances between matching el-\nements, and p(xjnon match )the distribution of distances\nbetween non-matching elements, then the optimal value ^\r\ncan be deﬁned as the value of \rthat minimizes the ex-\npected classiﬁcation error:\n^\r= argmin\n\rZ\r\n0p(xjnon match )dx+Z1\n\rp(xjmatch )dx\n= argmin\n\rZ\r\n0p(xjnon match )\u0000p(xjmatch )dx\n(4)\nFigure 2 shows p(xjmatch ), andp(xjnon match )for\nimaginary data, together with the corresponding optimal\nvalue of\r. Since DTW reliably ﬁnds correct alignments\nbetween recordings in the absence of structural differences\n[8], the alignments it produces on such recordings pro-\nvide samples from the population of matching audio fea-\ntures, allowing us to estimate p(xjmatch ). By sampling\nrandomly from the distance matrix (excluding cells on the\nDTW path), we obtain samples from p(xjnon match ).\nWith these distributions (which can often be well approx-\nimated by beta-distributions ), we can obtain ^\rfrom data\nusing a numerical approximation of (4).\nObviously, the actual form of these two distributions\nwill depend on the musical content, the audio features, and\nthe distance function used for alignment. In this context,\nthe best combination of audio features and distance func-\ntion is that which maximizes the divergence between the\ntwo distributions across musical content, since it facilitates\ndistinguishing matching audio from non-matching audio.0 1\ndistancedensityp(xjmatch)\np(xjnon match)\n^°Figure 2 . Schematic diagram of distance distributions\nbetween pairs of matching elements (solid), and non-\nmatching elements (dashed); the optimal value of \ris in-\ndicated with a dotted vertical line\n4. EXPERIMENTAL EVALUATION\nIn this section, we evaluate our proposed method NWTW\nby comparing it to DTW, on recordings both with and with-\nout structural differences. Before we do this, we assess\ndifferent audio features by looking at how well they allow\nfor separating matching from non-matching audio, as de-\nscribed in Section 3.3. Based on the results of that evalu-\nation, we choose an audio feature, and choose the optimal\n\rfor that feature. With this value of \r, we instantiate the\nNWTW algorithm, and perform a quantitative comparison\nof NWTW and DTW.\n4.1 Choice of audio features and \r: Method and data\nThe purpose of this experiment is to ﬁnd audio features for\nwhich the values in the distance matrix are as low as possi-\nble when they lie on the correct alignment path, and as high\nas possible otherwise. More speciﬁcally, we are interested\nin the features that maximize the divergence between the\ndistance distributions of those two classes. The rationale\nfor this is that with increasing divergence, the separation\nof the two classes by the gap penalty parameter will be\nmore successful.\nThe pairs of audio recordings used for the evaluation are\nmanually selected such that no structural differences occur.\nFor each of the pairs, the optimal DTW path is computed\nto align the audio. From this alignment, the two distance\ndistributions are computed.\n4.1.1 Features\nThe features we evaluate are all known from the litera-\nture, including both standard features, such as MFCC and\nCQT coefﬁcients, and more special purpose features such\nas PSD [8] and LNSO/NC [2]. Except for the LNSO/NC\nfeatures (which are deliberately chosen to be used in con-\njunction with an adaptive distance function), all features\nare used in combination with the cosine distance measure,\nnormalized to the interval [0;1].\nMel frequency cepstral coefﬁcients (MFCC) . Mel Fre-\nquency Cepstral Coefﬁcients [12] are an STFT based audio\nrepresentation well known in speech and music processing.MFCC’s are a compressed version of the spectral envelope\nof a short-term section of an audio signal, and they are es-\npecially useful for capturing the timbre and the formant\nstructure of speech/music signals. It is common to ignore\nthe ﬁrst MFCC coefﬁcient, and to take only the ﬁrst nco-\nefﬁcients. Here we evaluate both n= 13 andn= 50 . In\naddition to that, we use two FFT sizes: 46ms, and 372ms.\nConstant Q transform (CQT) . The Constant Q transform\n[4] is a time-frequency transform, but unlike the STFT\n– which implements a constant bin width and therefore\nyields a non-constant bin center frequency to bin width\nratio (theQvalue) — it forces the Qvalue to stay con-\nstant and modiﬁes the bin widths accordingly. The CQT\nis suitable for representing musical audio signals since its\nstructure resembles the diatonic scale: all octaves are an\nequal number of bins apart.\nPositive spectral difference (PSD) . This feature was pro-\nposed in [8], and is designed to capture onset information\nfor performance-to-performance alignment. It is based on\na Short Time Fourier Transform with the frequency bins\nmapped to a musically meaningful scale. The PSD feature\nis computed as the half-wave rectiﬁcation of the energy dif-\nference per frequency bin from one audio frame to the next.\nLocally adaptive features/distance (LNSO/NC) . For the\npurpose of audio alignment, Arzt et al. [2] propose to com-\npute a weighted sum of distances of two features. The ﬁrst,\nLocally Normalized Semitone Onset (LNSO) is an adapta-\ntion of PSD, and responds strongly to onsets. In absence of\nonsets, the distance is dominated by a second feature, Nor-\nmalized Chroma (NC), capturing harmonic information.\n4.1.2 Data\nIn order to ensure generality of the results beyond a single\ntype of acoustic signals, we use three different classes of\nrecordings (all obtained from commercial cd’s):\nSymphony orchestra : 7 Pieces from 5 different Beethoven\nsymphonies, by 10 different conductors, amounting to 148\ncomparisons between 59 recordings (4.4 hours of music).\nSolo guitar : The complete guitar works of Villa-Lobos\n(23 pieces), by 5 performers, amounting to 103 compar-\nisons between 83 recordings (4.6h).\nSolo piano : 14 Movements from 6 different piano sonatas\nby Mozart, by 7 performers, amounting to 720 compar-\nisons between 308 recordings (6.4h).\n4.2 Comparison of NWTW and DTW: Method and\ndata\nIn this part of the experimentation we evaluate alignment\naccuracies quantitatively using manual annotations of the\nbeat in a set of recordings of Mozart piano sonatas. We use\nthe evaluation procedure used in [8], in which for each an-\nnotated beat the alignment error is the Manhattan distance(in frames) to the closest point on the computed alignment.\nWe compare DTW and NWTW, both on pairs of recording\nwith and without structural differences. When structural\ndifferences occur, the alignment error for a given beat is\nthe minimum error among the instances of that beat in the\nrepetitions. Informally, the error criterion does not penal-\nize the alignment for passing through one repetition of a\nsection rather than through another.\n4.2.1 Data\nThe recordings we use for this are from the same solo piano\ndata set as the data described in Section 4.1.2, for which\nmanual beat annotations are available (details on the an-\nnotation process can be found in [21]). We take pairs or\nrecordings from this set such that each pair is a recording\nof the same piece by a different performer. Of these pairs,\n74 are without structural differences. This set involves 6\nperformers, playing 41 movements from 20 sonatas. In\naddition, we take pairs of recordings with structural dif-\nferences. This set consists of 133 pairs, and involves 8\nperformers, and 56 movements from 26 sonatas.\n4.3 Results and discussion\nFigure 3 shows the distance distributions between match-\ning and non-matching audio, computed on the various data\nsets, using the various features. In general, the distribu-\ntions vary more strongly across features than across the dif-\nferent types of audio. The pitch-oriented features with high\nfrequency resolution (most notably MFCC50 / FFT.372s,\nand CQT) tend to be those with highest Jensen-Shannon\ndivergence (JSD, shown in the plots). That said, the solo\nguitar data set in combination with the MFCC features\nshows a substantial reduction in JSD. This could be a con-\nsequence of the sensitivity of the MFCC features to the\nguitar tuning. Note also that this leads to a rightward shift\nof the optimal \rvalue, shown as dotted vertical lines in the\nplots. Although this discrepancy between optimal \rvalues\nacross data sets is principally undesirable, the MFCC50 /\nFFT.372s feature still yields the highest JSD when over the\njoint data set (bottom row in Figure 3). For this reason, we\nuse the MFCC50 / FFT.372s feature, and the correspond-\ning optimal parameter value ^\r= 0:346, for the subsequent\nquantitative evaluation of the DTW and NWTW methods.\nThe success of MFCC features for alignment is at odds\nwith the ﬁndings of [11], even if they only evaluate the\nfeatures indirectly through a retrieval task. An explanation\nfor this may be the number of MFCC’s selected: we obtain\nbest results with 50 MFCC’s, which is substantially more\nthan the ﬁrst 13 MFCC’s typically used.\nTable 1 shows the alignment accuracies for DTW and\nNWTW. When no structural differences occur between re-\ncordings, no jumps are required. In that case, the accuracy\nof NWTW alignment is very similar to that of DTW. When\ndifferences do occur, DTW tends to align parts of non-\nmatching audio segments (as illustrated schematically in\nFigure 1), leading to higher alignment errors. The straight\njumps that NWTW tends to make, ensure that the align-\nment path is always close to a matching position in eitherError\u0014(ms) 0 20 40 60 80 100 200 500 1000\nalignment of performances without structural differences\nDTW 47.1 72.6 84.5 90.6 93.6 95.4 98.3 99.6 99.9\nNWTW 46.3 73.3 85.5 91.5 94.5 96.1 98.6 99.6 100.0\nalignment of performances with structural differences\nDTW 37.0 60.6 73.1 80.2 83.6 85.6 89.2 91.5 92.9\nNWTW 38.1 66.0 79.5 86.7 90.1 91.7 94.5 96.4 97.3\nTable 1 . Alignment accuracies for DTW and NWTW, for\npairs of recordings without (top) and with (bottom) struc-\ntural differences; The values represent the percentages of\nannotated beats with a Manhattan distance less or equal to\nthe corresponding times in the top row; For example, using\nNWTW in structurally different audio, 96,4% of the beats\nare aligned no more than 500ms apart (91.5% for DTW)\none or the other of a section that is repeated in only one of\nthe recordings.\n5. CONCLUSIONS\nIn this paper we propose Needleman-Wunsch time warping\n(NWTW), a pure dynamic programming method to align\nmusic recordings that contain structural differences, and\npropose a way to estimate the optimal value for the gap\npenalty parameter \r. Experiments show that audio features\nwith high frequency resolution allow for the most effective\nuse of the gap penalty parameter. Moreover, a single value\nfor\ris (close to) optimal for different types of music, in-\ncluding both solo instruments, and symphonic orchestra.\nThe advantage of our method over classical dynamic\ntime warping is that it handles structural differences better,\nand the advantage over the original NW algorithm is that it\nhandles tempo discrepancies between different recordings.\nA limitation of the method in its current form is that it\ndoes not prefer jumps at the beginnings or ends of struc-\ntural units over jumps at intermediate positions. Although\nthis is not problematic for application scenarios that only\nrequire a matching position in one recording for each po-\nsition in the other, jumps at intermediate positions in a\nstructural unit are counter-intuitive from a musical point\nof view. We are currently investigating a further extension\nof the method to resolve this.\n6. ACKNOWLEDGMENTS\nThis research is supported by the European Union Sev-\nenth Framework Programme FP7 / 2007-2013, through the\nPHENICX project (grant agreement no. 601166).\n7. REFERENCES\n[1] A. Arzt, G. Widmer, and S. Dixon. Automatic page turning\nfor musicians via real-time machine listening. In ECAI , pages\n241–245, 2008.\n[2] A. Arzt, G. Widmer, and S. Dixon. Adaptive distance nor-\nmalization for real-time music tracking. In Signal Processing\nConference (EUSIPCO), 2012 Proceedings of the 20th Euro-\npean , pages 2689–2693. IEEE, 2012.FFT .046s /\n13 MFCCsBeethoven \nSymphoniesJSD: 0.31FFT .046s /\n50 MFCCs\nJSD: 0.44FFT .372s /\n13 MFCCs\nJSD: 0.34FFT .372s /\n50 MFCCs\nJSD: 0.53CQT\nJSD: 0.41PSD\nJSD: 0.14LNSO / NC\n(Arzt et al., 2012)\nJSD: 0.30Mozart \nSonatasJSD: 0.28 JSD: 0.46 JSD: 0.29 JSD: 0.51 JSD: 0.47 JSD: 0.18 JSD: 0.40Villalobos \nGuitar worksJSD: 0.22 JSD: 0.26 JSD: 0.20 JSD: 0.31 JSD: 0.36 JSD: 0.12 JSD: 0.33\n0.2 0.4 0.6 0.8AllJSD: 0.27\n0.2 0.4 0.6 0.8JSD: 0.40\n0.2 0.4 0.6 0.8JSD: 0.28\n0.2 0.4 0.6 0.8JSD: 0.46\n0.2 0.4 0.6 0.8JSD: 0.42\n0.2 0.4 0.6 0.8JSD: 0.16\n0.2 0.4 0.6 0.8JSD: 0.35Density\nNormalized DistanceFigure 3 . Distance distributions per feature for different types of music; In each subplot, the yellow distribution (left) is the\ndistribution of distances between features of matching audio, the magenta distribution (right) is the distribution of distances\nbetween features of non-matching audio; The dashed vertical lines indicates the optimal value for \r; The value indicated\nwith JSD is the Jensen-Shannon divergence between the two distributions\n[3] J. P. Bello. Audio-based cover song retrieval using approxi-\nmate chord sequences: Testing shifts, gaps, swaps and beats.\nInProc. of the 8th International Conference on Music Infor-\nmation Retrieval , pages 239–244, Vienna, Austria, Septem-\nber 23-27 2007.\n[4] J. C. Brown and M. S. Puckette. An efﬁcient algorithm for\nthe calculation of a constant q transform. The Journal of the\nAcoustical Society of America , 92:2698, 1992.\n[5] R. Dannenberg. An on-line algorithm for real-time accompa-\nniment. In Proceedings of the 1984 International Computer\nMusic Conference . International Computer Music Associa-\ntion, 1984.\n[6] R. B. Dannenberg and C. Raphael. Music score alignment\nand computer accompaniment. Commun. ACM , 49(8):38–43,\n2006.\n[7] C. Dittmar, K. Hildebrand, D. Gaertner, M. Winges,\nF. Muller, and P. Aichroth. Audio forensics meets music in-\nformation retrieval; a toolbox for inspection of music plagia-\nrism. In Proc. of the 20th European Signal Processing Con-\nference (EUSIPCO) , pages 1249–1253, 2012.\n[8] S. Dixon and G. Widmer. Match: A music alignment tool\nchest. In Proceedings of the 6th International Conference on\nMusic Information Retrieval , London, UK, September 11-15\n2005.\n[9] C. Fremerey, M. M ¨uller, and M. Clausen. Handling repeats\nand jumps in score-performance synchronization. In Proc.\nof the 11th International Society for Music Information Re-\ntrieval Conference , pages 243–248, Utrecht, The Nether-\nlands, August 9-13 2010.\n[10] M. Grachten, J. L. Arcos, and R. L ´opez de M ´antaras. A case\nbased approach to expressivity-aware tempo transformation.\nMachine Learning , 65(2–3):411–437, 2006.\n[11] N. Hu, R. B. Dannenberg, and G. Tzanetakis. Polyphonic au-\ndio matching and alignment for music retrieval. In Applica-\ntions of Signal Processing to Audio and Acoustics, 2003 IEEE\nWorkshop on. , pages 185–188. IEEE, 2003.[12] B. Logan. Mel frequency cepstral coefﬁcients for music mod-\neling. In Proceedings of the 1st International Conference\non Music Information Retrieval , Plymouth, Massachusetts,\n2000.\n[13] M. Mongeau and D. Sankoff. Comparison of musical se-\nquences. Computers and the Humanities , 24:161–175, 1990.\n[14] N. Montecchio and A. Cont. Accelerating the mixing phase in\nstudio recording productions by automatic audio alignment.\nInProceedings of the 12th International Society for Music\nInformation Retrieval Conference , pages 627–632, Miami\n(Florida), USA, 2011.\n[15] M. M ¨uller. Information Retrieval for Music and Motion .\nSpringer-Verlag New York, Inc., Secaucus, NJ, USA, 2007.\n[16] M. M ¨uller and D. Appelt. Path-constrained partial music\nsynchronization. In Proceedings of the 34th International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , volume 1, pages 65–68, Las Vegas, Nevada, USA,\nApr. 2008.\n[17] S. B. Needleman and C. D. Wunsch. A general method appli-\ncable to the search for similarities in the amino acid sequence\nof two proteins. Journal of Molecular Biology , 48(3):443 –\n453, 1970.\n[18] B. Pardo and W. Birmingham. Modeling form for on-line\nfollowing of musical performances. In Proceedings of the\n20th national conference on Artiﬁcial intelligence - Volume\n2, AAAI’05, pages 1018–1023. AAAI Press, 2005.\n[19] J. Serr `a, E. G ´omez, P. Herrera, and X. Serra. Chroma binary\nsimilarity and local alignment applied to cover song identi-\nﬁcation. IEEE Transactions on Audio, Speech and Language\nProcessing , 16:1138–1151, 2008.\n[20] R. J. Turetsky and D. P. W. Ellis. Ground-truth transcriptions\nof real music from force-aligned midi syntheses. In Proceed-\nings of the Fourth International Conference on Music Infor-\nmation Retrieval , Baltimore (Maryland), USA, 2003.\n[21] G. Widmer, S. Dixon, W. Goebl, E. Pampalk, and A. Tobudic.\nIn search of the Horowitz factor. AI Magazine , 24(3):111–\n130, 2003."
    },
    {
        "title": "MeUse: Recommending Internet Radio Stations.",
        "author": [
            "Maurice Grant",
            "Adeesha Ekanayake",
            "Douglas Turnbull"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418027",
        "url": "https://doi.org/10.5281/zenodo.1418027",
        "ee": "https://zenodo.org/records/1418027/files/GrantET13.pdf",
        "abstract": "In this paper, we describe a novel Internet radio recommendation system called MeUse. We use the Shoutcast API to collect historical data about the artists that are played on a large set of Internet radio stations. This data is used to populate an artist-station index that is similar to the term-document matrix of a traditional text-based information retrieval system. When a user wants to find stations for a given seed artist, we check the index to determine a set of stations that are either currently playing or have recently played that artist. These stations are grouped into three clusters and one representative station is selected from each cluster. This promotes diversity among the stations that are returned to the user. In addition, we provide additional information such as relevant tags (e.g., genres, emotions) and similar artists to give the user more contextual information about the recommended stations. Finally, we describe a web-based user interface that provides an interactive experience that is more like a personalized Internet radio player (e.g., Pandora) and less like a search engine for Internet radio stations (e.g., Shoutcast). A smallscale user study suggests that the majority of users enjoyed using MeUse but that providing additional contextual information may be needed to help with recommendation transparency.",
        "zenodo_id": 1418027,
        "dblp_key": "conf/ismir/GrantET13",
        "keywords": [
            "Internet radio recommendation system",
            "Shoutcast API",
            "artist-station index",
            "term-document matrix",
            "seed artist",
            "stations",
            "clusters",
            "representative station",
            "diversity",
            "user interface"
        ],
        "content": "MEUSE: RECOMMENDING INTERNET RADIO STATIONS\nMaurice Grant\nIthaca College\nmgrant1@ithaca.eduAdeesha Ekanayake\nIthaca College\naekanay1@ithaca.eduDouglas Turnbull\nIthaca College\ndturnbull@ithaca.edu\nABSTRACT\nIn this paper, we describe a novel Internet radio rec-\nommendation system called MeUse. We use the Shout-\ncast API to collect historical data about the artists that are\nplayed on a large set of Internet radio stations. This data\nis used to populate an artist-station index that is similar\nto the term-document matrix of a traditional text-based in-\nformation retrieval system. When a user wants to ﬁnd sta-\ntions for a given seed artist, we check the index to deter-\nmine a set of stations that are either currently playing or\nhave recently played that artist. These stations are grouped\ninto three clusters and one representative station is selected\nfrom each cluster. This promotes diversity among the sta-\ntions that are returned to the user. In addition, we provide\nadditional information such as relevant tags (e.g., genres,\nemotions) and similar artists to give the user more contex-\ntual information about the recommended stations. Finally,\nwe describe a web-based user interface that provides an\ninteractive experience that is more like a personalized In-\nternet radio player (e.g., Pandora) and less like a search en-\ngine for Internet radio stations (e.g., Shoutcast). A small-\nscale user study suggests that the majority of users enjoyed\nusing MeUse but that providing additional contextual in-\nformation may be needed to help with recommendation\ntransparency.\n1. INTRODUCTION\nPrior to the advent of personal computers and the Internet,\nthere were two primary music recommendation technolo-\ngies: the jukebox and the AM/FM radio. A Jukebox was\na common feature of many social spaces such as bars and\ndiners. Using a jukebox, an individual could chose from\na small set of on-demand songs to play for the rest of the\npeople in the nearby vicinity. AM/FM radios were found\nin more personal spaces such as the home or car. However,\nlisteners were connected to one another through a com-\nmon stream of music that was broadcast over the air waves.\nThis gave popular, trend-setting DJs the opportunity to be\nheard by millions of listeners at the same time.\nToday, we see a number of new music recommenda-\ntion technologies emerging as a result of the availability of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.personal computers, mobile devices and the Internet. We\ncan generally classify them as being similar to a jukebox,\nAM/FM radio or a hybrid of the two (see table 1). Ce-\nlestial jukeboxes [2] such as Apple iTunes1and Spotify2\nprovide users with instant on-demand access to millions\nof songs at the click of a button. Internet radio allows\ncomputer users to listen to broadcasts of songs or pro-\ngrams from traditional radio stations like NPR and BBC\nor through radio aggregators such as Shoutcast3. Accu-\nRadio4, and Live3655.\nThe hybrid of these two digital technologies is person-\nalized Internet radio which allows users to listen to a per-\nsonalized stream of music based seed artists or semantic\ntags (e.g., genres, emotions). Popular examples include\nPandora and Slacker. These systems are like jukeboxes in\nthat a user has some control over which songs are selected,\nbut like radio in that there is some element of serendipity\nin that the user cannot predict exactly what will be played\nahead of time. In this paper, we describe a system called\nMeUse which attempts to harness the strengths of Internet\nradio but functions more like personalized Internet radio.\nThe core of our system relies on data collected using the\nShoutcast API6. Shoutcast is a music service that aggre-\ngates 50,000 Internet radio stations, many of which are cu-\nrated by human DJs. They provide a simple user interface\nthat allows users to search by artist or genre. This search\nreturns a list of stations that are currently playing the artist\nor genre of music that a user is seeking. They also pro-\nvide metadata for each station which includes the station’s\nweb address, the number of current listeners, bit rate and\nstream format. Using the web address, users can launch\na 3rd party media player (e.g., Apple iTunes, Winamp7,\nVLC8) to connect to the radio stream.\nOne problem with Shoutcast is that they may not rec-\nommend any stations for a given seed artist if no station\nis currently playing that artist. They may also return too\nmany stations if the seed artist is currently very popular\n(e.g., a search for Rihanna might return over 100 stations.)\nThis causes the paradox of choice in which increasing con-\nsumer choices can also increase the chance of user dissat-\nisfaction [8]. A user may also not know which station to\n1http://www.apple.com/itunes/\n2https://www.spotify.com\n3http://www.shoutcast.com/\n4http://www.accuradio.com/\n5http://www.live365.com/\n6http://wiki.winamp.com/wiki/SHOUTcast_Radio_\nDirectory_API\n7http://www.winamp.com\n8http://www.videolan.org/index.htmlTechnology Discovery\nModeDelivery\nMediumExamples Advantages\nInternet Radio\n(analogous to\nAM/FM radio)Passive Broadcast NPR, BBC,\nShoutcast,\nAccuRadioOften Curated by Human DJs,\nEase of Use, Serendipity\nPersonalized\nInternet RadioPassive Automatic\nPlaylistPandora,\nSlacker,\nJamendoPersonalization,\nEase of use, Serendipity\nCelestial Jukebox\n(analogous to a\nphysical jukebox)Active Metadata\nSearchiTunes,\nSpotify,\nTomahawkCustomized Playlists,\nImmediate Gratiﬁcation,\nPersonalization\nTable 1 . Comparison of Music Recommendation Technologies\nchoose based on the limited metadata that is provided for\neach radio station. The ability to search by genre is also\nlimited because radio stations are only allowed to have\none genre (even though stations often play music from a\ndiverse set of genres) and the genre label may be vague,\nrarely updated, or simply inaccurate.\nHistorically, another problem with Internet radio aggre-\ngators is they used to forces the user to use a web browser\nfor recommendation and a 3rd party audio player for listen-\ning. This meant that it took a relatively long time before a\nuser could start listening to music and switch between sta-\ntions. More recently, web technologies such as the VLC\nweb plugin and the SoundManager2 JavaScript API9have\nmade it possible to stream radio stations directly from the\nbrowser.\nIn this paper, we are interested in improving Internet\nradio station recommendation both in terms of backend\nrecommendation algorithms as well as developing a better\nfrontend user experience. Speciﬁcally, we use the Shout-\ncast API to collect historical data about the artists that\neach station plays over time. We use this data to pop-\nulate an artist-station index that is similar to the term-\ndocument matrix of a traditional text-based information re-\ntrieval system but where the documents are stations and\nthe terms are artists. When a user wants to ﬁnd stations\nfor a given seed artist, we check the music index to deter-\nmine a set of stations that are either currently playing or\nhave recently played that artist. These stations are grouped\ninto three clusters and one representative station is selected\nfrom each cluster. Clustering is intended to promote diver-\nsity among the stations that are returned to the user. In\naddition, we provide additional information such as rel-\nevant tags and commonly played artists to give the user\nmore contextual information about the recommended sta-\ntions. Finally, we incorporate an embedded audio player\ndirectly into our website so that users do not need to use an\nexternal 3rd party media player.\n2. RELATED WORK\nWhile we have been unable to ﬁnd related work on the\nspeciﬁc task of Internet radio station recommendation, our\n9http://www.schillmania.com/projects/\nsoundmanager2/work is functionally equivalent to a generic text-based\nsearch engine in that the main data structure is an inverted\nindex and we rely on a vector space model to access rele-\nvance and cluster our data [4]. That is, each radio station is\nrepresented as a vector over a vocabulary of artists. When\ngiven a seed artist, we can rank stations by the dimension\ncorresponding to that artist. We are also able to cluster sta-\ntions once they are each represented as a vector.\nThere has been considerably more work on using histor-\nical playlists from Internet radio stations as data for study-\ning music similarity and automatic playlist algorithms\n[1, 3, 5, 6]. Although it is not the focus of this work, our\nartist-station index can also be used to calculate artist sim-\nilarity if we instead think each vector corresponding to an\nartist over a vocabulary of stations.\n3. SYSTEM OVERVIEW\nWhen designing MeUse for Internet radio recommenda-\ntion, we focus on three information retrieval concepts: Rel-\nevance, Diversity and Transparency. In this sections we\ndescribe both the backend recommendation algorithm and\nfrontend user interface in terms of these important design\nconcepts.\n3.1 Backend Recommendation Architecture\nIn this subsection, we discuss how we collect data, use this\ndata to ﬁnd a set of relevant stations, clustering these sta-\ntions, and then select stations that are recommended to the\nuser. An overview of our system architecture is shown in\nﬁgure 1.\n3.1.1 Data Collection\nThe set of artists in the database was initialized by down-\nloading the top 100 tags on Last.fm, and then downloading\nthe top 100 artists for each tag. This provided us with a\ndiverse set of 4460 artists. The set of tags was initialized\nby downloading the set of tags for all of these artists. Once\ndownloaded, the set of tags was pruned by ﬁrst removing\nweak song-tag associations (e.g., a Last.fm tag score <5)\nand then making sure that each tag was associated with a\nminimum of 5 artists.\nWe then grow a set of Internet radio stations by querying\nthe Shoutcast API multiple times for each of our artists.That is, Shoutcast returns a set of stations that are currently\nplaying a given seed artist. For each of these stations, we\nincrement a counter (e.g., a cell in our artist-station index\nmatrix) each time we see that the stations is playing the\nartist. If we have never observed the station in the past, it\nis added to our set of stations.\nIn addition, whenever a user searches for an artist using\nour frontend user interface, we use the given artist as a\nquery to the Shoutcast API. The results are used to further\ngrow the set of artists, the set of stations, and increment the\nvalues in the artist-station index. Finally, we apply a daily\ndecay to the values in the artist-station index so that newer\nobservation have more weight than older observations.\n3.1.2 Relevance\nOnce we had gathered the above data, we used the follow-\ning algorithm to recommend stations for a given seed artist.\nTo begin, we selected a set of between 10 and 30 candi-\ndate stations. These stations have played the seed artist the\nhighest number of times in the past or are currently play-\ning the seed artist according to Shoutcast. If we have too\nfew candidate stations, we ﬁnd the top ranked similar artist\nto the seed artist according to Last.fm and use that artist\nto ﬁnd additional candidate stations. If we have too many\ncandidate stations, we rank order stations by the number of\ntimes they have played the seed artist but also taking into\naccount observation decay as described above.\n3.1.3 Diversity\nNext, we use the information that is stored in our artist-\nstation index to represent each candidate stations as a vec-\ntor over our vocabulary of artists (e.g., columns of the\nartist-station matrix). These vectors are grouped into three\nclusters using the k-mean algorithm [7]. We then selected\na representative station from each cluster by selecting the\nstation with the highest listen count while giving stations\ncurrently playing the seed artist priority. This ensures that\nthe station is not only relevant but also important.\n3.1.4 Transparency\nTo make our station recommendation more transparent, we\nprovide three representative artists and three representative\ntags for each of the recommended station. To select rep-\nresentative artists for a given station, we ranked artists ac-\ncording to the difference between the play count on that\nstation and the sum of the play counts for that artist on\nthe other two stations. By this method, we select repre-\nsentative artists that differentiate the recommended stations\nfrom one another.\nThe three representative tags for a recommended station\nare found by ﬁrst ﬁnding all of the tags for all of the artists\nthat are played on that station and then removing the tags\nthat are also associated with artists who have been played\non the other two recommended stations. The remaining\ntags are then rank ordered by the average Last.fm artist-tag\nscore for all artists associated with the station. Again, this\nalgorithm has been designed to pick tags that differentiate\nthe three recommended stations rather than select the most\nrepresentative tags for the station.\nFigure 1 . Backend System Architecture. A artist-station\nindex is created by counting the number of times an artist\nis played on a station. When a user provides seed artist\n(e.g., Coldplay), the index is used to ﬁnd a set of relevant\nstation. These stations are clustered and a representative\nstation is selected from each cluster.\n3.2 Frontend User Interface\nThe web-based user interface for MeUse is shown in ﬁgure\n2. We wanted the interactive experience to be more like a\npersonalized Internet radio player (e.g., Pandora) and less\nlike a search engine for Internet radio stations (e.g., Shout-\ncast). This is accomplished in a few ways.\nFirst, after a user has entered a seed artist in the search\nbar, only three stations are recommended to the users based\non clustering and station selection as is described in the\npreceding subsection. We provide a clear and concise snip-\npet of information for each station. This includes the name\nof the station and other important metadata that is provided\ndirectly from Shoutcast (e.g., current number of listeners,\nbit rate, audio format). In addition, we provide the lists of\nrepresentative artists and tags that differentiate the recom-\nmend stations.\nWe also provide an embedded VLC audio player that is\nhidden from the user but can be manipulated by the user\nthrough various control mechanisms found on the page\n(e.g., play/pause, volume). In addition, the user can eas-\nily switch between the recommended stations, request new\nrecommendations, or change the seed artists. While the\nVLC plugin is useful for removing the need for an exter-\nnal 3rd party player (e.g., iTunes, Winamp), it does require\nthe user to have the (free) pluggin be installed for their\nbrowser. In the future, we expect that web technologies\n(e.g., HTML5) will allow for more seamless streaming of\nInternet radio directly through the web browser.\n4. EVALUATION\nTo evaluate MeUse, we ﬁrst explore how well our algo-\nrithm is able to recommend a diverse set of Internet radio\nstations. We then describe a small-scale user study that\nwas primarily directed at evaluating our user interfaces but\nalso allows us to ask questions about our backend recom-\nmendation system.Figure 2 . MeUse User Interface\n4.1 Exploring Diversity\nOne of the primary goals of MeUse is to dramatically limit\nthe number of recommended stations while providing a di-\nverse set of relevant stations to match the user’s interests.\nWe also want to provide users with contextual information,\nsuch as representative tags and artists, so that the user can\nmake an informed decision when choosing between the\nrecommended stations. To evaluate this, we designed an\nexperiment that compares how often a representative artist\nfor a station is played on that station verses how often the\nrepresentative artist is played on one of the other recom-\nmended stations.\nFor the experiment, we randomly selected 100 artists\nfrom the set of 500 most popular artists (according to\nLast.fm) in our database. For each artist, we obtained\nthree recommended stations using MeUse. We then lis-\ntened to each of these three stations for the next two hours\nby recording the currently playing artist every 10 minutes.\nFinally, we counted how often the seed artist was played,\nhow often one of the three representative artists for the sta-\ntion was played, and how often one of the six representa-\ntive artists from the other two recommended stations was\nplayed.\nThe results for our experiment are shown in table 2.\nWhile we should have collected on the order of 3600 song-\nplay observations (e.g., 100 artists, 3 stations, 12 observa-\ntions), we found data collection to be a more noisy pro-\ncess than expected. That is, some stations did not appearto update their “recently playing” information and other\nstations’ “recently playing” information contained infor-\nmation about the station and not about the music currently\nbeing played. In both these cases, we ignored the dupli-\ncated “recently playing” information. We also found a few\ncases where a station stopped broadcasting during our two\nhour observation window which prevented us from collect-\ning some additional song-play observation. In the end, we\nwere able to collect 2627 observations.\n# of Artists Play CountPlay Count\n# of Artists\nSeed Artist 1 147 147\nRepresentative Artists\nfor Station3 25 8.3\nRepresentative Artists\nfor Other Recommended\nStations6 30 5\nTable 2 . Results from diversity experiment after recording\n2627 song-play observations.\nThe results show that stations play the seed artists 5.6%\nof the time. We also observe that the stations play each\nof our representative artists 0.32% of the time. While this\nappears to be rather low, our goal in picking representative\nartists is to pick artists that differentiate the station from the\nother two recommended stations rather than simply pick-ing popular artists that have been played on the station in\nthe past. We also note that this is higher than the 0.19%\nof times that each of the representative artists from the\nother two recommended stations are played on the station.\nHowever, this is not a statistically signiﬁcant improvement\n(\u000b= 0:18, one-tailed two-proportion pooled z-test). We\nsuspect that the ability to ﬁnd better representative artists\nwill improve as we are able to collect more data to populate\nthe artist-song index.10\n4.2 User Study\nTo evaluate the usability of MeUse we conducted a small-\nscale user study of our interface. The study involved 20\ncollege-aged individuals who were asked to play around\nwith the interface and then ﬁll out a short survey about their\nexperience. About half of the test subjects were observed\nin our lab while using the system. The other half were\nasked to use MeUse in their own environment. Of the ones\nthat we observed, all users seemed to ﬁnd MeUse easy-\nto-use, were quickly able to listen to music through the\nembedded player, and seem to enjoy switching between\nthe recommended stations.\nNo Not Really Sort of Mostly Deﬁnitely\n0 3 3 9 4\nTable 3 . Relevance: Were the recommend stations that\nwere relevant to you?\nNo Not Really Sort of Mostly Deﬁnitely\n2 3 5 7 2\nTable 4 . Transparency: Did we give you enough infor-\nmation to make a clear choice between the 3 stations we\nrecommended?\nIn terms of our ability to recommend Internet radio sta-\ntions, 70% of the test subjects stated that the recommended\nstations were mostly or deﬁnitely relevant (see table 3) but\nonly 45% of users felt that they were given enough infor-\nmation to make an informed decision on which of the three\nstations to choose (see table 4). This suggest that we need\nto think about additional ways to provide the user with con-\ntextual information about the stations. For example, one\ntest subject suggested adding a point-rating system for sta-\ntions. The test subjects did indicate that the stations that\nwere recommended were diverse in nature (see table 5)\nand that, in general, they enjoyed using MeUse to listen\nto Internet radio (see table 6).\n5. DISCUSSION\nIn this paper we described MeUse as a complete Internet\nradio recommendation system. The results of our small-\nscale user study suggests that the system shows promise\nbut additional user testing is required. In particular we\n10At the time of submission, we have only been able to search the\nShoutcast API approximately 5 times for each of the \u00184,500 artists in\nour database. This is because Shoutcast limits the number of query’s one\ncan make on a daily basis.No Not Really Sort of Mostly Deﬁnitely\n1 1 3 11 3\nTable 5 . Diversity: Were the three stations we recom-\nmended different enough from each other to make select-\ning a station meaningful?\nNo Not Really Sort of Mostly Deﬁnitely\n2 1 2 6 8\nTable 6 . Overall: Did you enjoy using MeUse to listen to\nInternet radio?\nplanned to do extensive A/B testing to isolate speciﬁc as-\npects of our system (e.g. UI design, recommendation algo-\nrithm). This will include both observing a small number of\nusers in our lab as well as large-scale and long-term user\nstudies in natural user enviroments.\nWe also would like to further develop MeUse by explor-\ning additional ways in which we can make MeUse more\nlike a personalized Internet radio player. This will include\nallowing users to be able to rate stations and using col-\nlaborative ﬁltering to improve our station recommendation\nalgorithm. Finally, we plan to explore modifying our artist-\ntag index to beneﬁt from common text retrieval techniques\n(e.g. tf-idf) to further improve recommendations.\nAcknowledgments: Steven Lam help implement\nMeUse. This research was supported by NSF Award IIS-\n1217485.\n6. REFERENCES\n[1] N. Aizenberg, Y . Koren, and O. Somekh. Build your\nown music recommender by modeling internet radio\nstreams. In WWW , 2012.\n[2] P. Lamere and J. Donaldson. Tutorial on using visual-\nization for music discovery. In ISMIR , 2009.\n[3] F. Maillet, D. Eck, G. Desjardins, and P. Lamere. Steer-\nable playlist generation by learning song similarity\nfrom radio station playlists. In ISMIR , 2009.\n[4] C.D. Manning, P. Raghavan, and H. Schtze. Introduc-\ntion to Information Retrieval . Cambridge University\nPress, 2008.\n[5] B. McFee and G. R. G. Lanckriet. The natural language\nof playlists. In ISMIR , 2011.\n[6] J. Moore, S. Chen, T. Joachims, and D. Turnbull.\nLearning to embed songs and tags for playlist predic-\ntion. In ISMIR , 2012.\n[7] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay. Scikit-learn: Machine learning in Python. Journal\nof Machine Learning Research , 12:2825–2830, 2011.\n[8] B. Schwartz. The paradox of choice . HarperCollins e-\nbooks, 2009."
    },
    {
        "title": "Converting Path Structures Into Block Structures Using Eigenvalue Decompositions of Self-Similarity Matrices.",
        "author": [
            "Harald Grohganz",
            "Michael Clausen",
            "Nanzhu Jiang",
            "Meinard Müller"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417813",
        "url": "https://doi.org/10.5281/zenodo.1417813",
        "ee": "https://zenodo.org/records/1417813/files/GrohganzCJM13.pdf",
        "abstract": "In music structure analysis the two principles of repetition and homogeneity are fundamental for partitioning a given audio recording into musically meaningful structural elements. When converting the audio recording into a suitable self-similarity matrix (SSM), repetitions typically lead to path structures, whereas homogeneous regions yield block structures. In previous research, handling both structural elements at the same time has turned out to be a challenging task. In this paper, we introduce a novel procedure for converting path structures into block structures by applying an eigenvalue decomposition of the SSM in combination with suitable clustering techniques. We demonstrate the effectiveness of our conversion approach by showing that algorithms previously designed for homogeneitybased structure analysis can now be applied for repetitionbased structure analysis. Thus, our conversion may open up novel ways for handling both principles within a unified structure analysis framework.",
        "zenodo_id": 1417813,
        "dblp_key": "conf/ismir/GrohganzCJM13",
        "keywords": [
            "music structure analysis",
            "repetition",
            "homogeneity",
            "partitioning",
            "audio recording",
            "musically meaningful",
            "self-similarity matrix",
            "path structures",
            "block structures",
            "eigenvalue decomposition"
        ],
        "content": "CONVERTING PATH STRUCTURES INTO BLOCK STRUCTURES USING\nEIGENV ALUE DECOMPOSITIONS OF SELF-SIMILARITY MATRICES\nHarald Grohganz, Michael Clausen\nBonn University\n{grohganz,clausen }@cs.uni-bonn.deNanzhu Jiang, Meinard M ¨uller\nInternational Audio Laboratories Erlangen\n{nanzhu.jiang,meinard.mueller }@audiolabs-erlangen.de\nABSTRACT\nIn music structure analysis the two principles of repetitio n\nand homogeneity are fundamental for partitioning a given\naudio recording into musically meaningful structural ele-\nments. When converting the audio recording into a suitable\nself-similarity matrix (SSM), repetitions typically lead to\npath structures, whereas homogeneous regions yield block\nstructures. In previous research, handling both structura l\nelements at the same time has turned out to be a challeng-\ning task. In this paper, we introduce a novel procedure for\nconverting path structures into block structures by apply-\ning an eigenvalue decomposition of the SSM in combina-\ntion with suitable clustering techniques. We demonstrate\nthe effectiveness of our conversion approach by show-\ning that algorithms previously designed for homogeneity-\nbased structure analysis can now be applied for repetition-\nbased structure analysis. Thus, our conversion may open\nup novel ways for handling both principles within a uniﬁed\nstructure analysis framework.\n1. INTRODUCTION\nThe task of music structure analysis with the objective\nof partitioning a given audio recording into temporal seg-\nments and of grouping these segments into musically\nmeaningful categories constitutes a central task in the ﬁel d\nof music information retrieval [14]. Because of differ-\nent structure principles including temporal order, repeti -\ntion, contrast, variation, and homogeneity, ﬁnding the mu-\nsical structure is a challenging and often ill-deﬁned prob-\nlem [18]. In particular, the two principles of repetition\nand homogeneity have been in the focus of previous re-\nsearch efforts [14, 15]. On the one hand, repetition-based\nmethods target at identifying recurring patterns and, on th e\nother hand, homogeneity-based methods try to determine\npassages that remain unchanged with respect to some mu-\nsical property. When converting the given audio recording\ninto a suitable feature sequence and then deriving a self-\nsimilarity matrix (SSM), repetitions typically lead to pat h-\nlike structures, whereas homogeneous regions yield block-\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage an d that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieva l.like structures. In previous research, numerous extractio n\nand clustering techniques have been proposed that either\nallow for handling path structures or block structures, see ,\ne g., [1, 5, 8, 12, 13, 14, 15]. However, dealing with both\nstructural elements at the same time has turned out to be a\nchallenging or even irreconcilable task.\nOnly few approaches that try to apply several seg-\nmentation principles at the same time exist. In [13],\na unifying optimization scheme that jointly accounts for\npath and block structures is proposed. In [17], structural\nchanges with regard to path and block elements are cap-\ntured to derive segment boundaries. In [4, 8], approaches\nfor homogeneity-based structure analysis are introduced,\nwhere smoothing and clustering techniques are applied\nfor enhancing the block structure in a pre-processing step.\nA very interesting research direction is sketched in [16],\nwhere an audio recording is locally classiﬁed according to\nthe properties of being repetitive or homogeneous with the\ngoal to locally adapt the segmentation strategy.\nAlong these lines of research, we deal in this pa-\nper with the task of converting a repetition-based into a\nhomogeneity-based structure analysis problem. Opposed\nto [4, 8], who try to enhance already latent block struc-\nture by applying smoothing and image processing tech-\nniques, we generate the block structures from path struc-\ntures. As our main technical contribution, we propose a\nnovel procedure that is based on an eigenvalue decompo-\nsition of the SSM. We show that certain path structures\ninduce some disjoint properties of the supports (non-zero\nentries) of the eigenvectors. This in turn allows us to gener -\nate block-like structures when converting back the suitabl y\nclustered eigenvectors into some SSM. A typical result of\nour procedure is shown in Figure 1, which shows the origi-\nnal path structure in (b) and the resulting block structure i n\n(e). The underlying piece of music has the musical form1\nA1B1A2B2C1C2D1D2A3B3A4B4. Note that in our pro-\ncedure an explicit extraction of the path structure, often a\nfragile step used in repetition-based structure analysis, is\nnot necessary.\nThe general idea of using eigenvalue decompositions\nof SSMs with applications to audio segmentation is not\nnew, see e. g. [2]. However, in [2] this techniques is used\nfor the purpose of dimensionality reduction and clustering ,\nwhereas we exploit speciﬁc properties of the eigenvectors\n1As in [14], musical parts are denoted by the letters A,B,C,... in\nthe order of their ﬁrst occurrence, where indices are used to indicate rep-\netitions.  \n50 100 15020406080100120140160180\n0.40.50.60.70.80.91\n  \n50 100 15020406080100120140160180\n0.10.20.30.40.50.60.70.80.9\n  \n100 200 300 400 500 600 700 800 900100\n200\n300\n400\n500\n600\n700\n800\n900\n00.511.522.53\n  \n100 200 300 400 500 600 700 800 900100\n200\n300\n400\n500\n600\n700\n800\n9001020304050607080\n  \n20 40 60 80 100 120 140 160 18020406080100120140160180\n00.10.20.30.40.50.60.70.80.91\nTime (sec)Time (sec)\nTime (sec)Time (sec)\nTime (frames)Eigenvectors\nTime (frames)EigenvectorsTime (sec)Time (sec)\nA1B1A2B2C1C2D1D2A3B3A4B4(a) (b)\n(c) (d)(e)\n(f)\n(g)\nFigure 1: Illustration of the algorithmic pipeline for converting pa th into block structures. (a)Original SSM. (b)Path-enhanced SSM\nas typically used for repetition-based structure analysis .(c)Eigenvectors (rows) of the SSM weighted and sorted by the cor responding\neigenvalues. (d)Eigenvectors (rows) after clustering and post-processing .(e)SSM obtained from (d). (f)Structure analysis result\nobtained from (e) applying some homogeneity-based cluster ing procedure. (g)Manually labeled structure (ground truth).\nfor converting path into block structures.\nWe demonstrate the effectiveness of our conversion ap-\nproach by discussing a number of explicit examples and\nby presenting some quantitative evaluation based on two\nknown datasets. In particular, we show that upon our con-\nversion standard clustering procedures that are designed\nfor homogeneity-based structure analysis can then be ap-\nplied for repetition-based structure analysis. As a result s,\nour conversion may open up novel ways for combining dif-\nferent music segmentation principles at an early stage of an\naudio structure processing pipeline.\nIn the remainder of this paper, we ﬁrst describe the al-\ngorithmic details for our conversion procedure (Section 2) ,\nthen report on our experiments (Section 3), and conclude\nwith an outlook on future research directions (Section 4).\n2. ALGORITHMIC PIPELINE\nIn this section, we describe our procedure for converting\nSSMs with path-like structures into SSMs with block-like\nstructures, see also Figure 1 for an illustration of the over all\npipeline. We ﬁrst summarize a procedure for computing an\nSSM with path structures as typically used for repetition-\nbased structure analysis (Section 2.1). Such matrices con-\nstitute the input of our conversion procedure. We then de-\nscribe some properties of the eigenvectors obtained from\nsuch matrices (Section 2.2) and show how the eigenvec-\ntors can be used to derive SSMs with block-like structures\n(Section 2.3).\n2.1 Computing SSMs with Path Structures\nRepeating segments in music often share the same melodic\nand harmonic progression while showing differences in in-\nstrumentation and timbre. Therefore, in most approachesfor repetition-based structure analysis, the audio signal\nis converted into twelve-dimensional chroma-based audio\nfeatures, which closely correlate to the aspect of harmony\nand have become a widely used tool in processing and\nanalyzing music data [3]. In the following, we use a\nchroma variant referred to CRP (Chroma DCT-Reduced\nLog Pitch) features2, which show a high degree of invari-\nance to changes in timbre [10]. In our experiments, we\nadapt the feature rate according to the length of the consid-\nered audio recording, which results in feature rates (fea-\ntures per second) between 2Hz and6Hz . Normalizing the\nfeatures, we use the inner product as a similarity measure\nto compute a self-similarity matrix Sby comparing the el-\nements of the feature sequence in a pairwise fashion, see\nFigure 1a.\nTo further enhance the path structure of S, one typical\nprocedure is to apply some kind of smoothing ﬁlter along\nthe direction of the main diagonal, resulting in an emphasis\nof diagonal information in Sand a denoising of other struc-\ntures. In our implementation, we use a smoothing variant\nsimilar to [12], which can deal with local tempo variations.\nFurthermore, we apply image processing and thresholding\ntechniques to eliminate short and weak path fragments, see\nalso [17] for similar strategies.\nThe resulting path-enhanced SSM, as illustrated by Fig-\nure 1b, constitutes the input of our conversion algorithm.\nNote that the implementation details to obtain the path-\nenhanced SSM are not important at this stage. Our con-\nversion procedure is generic and works well as long as\nthe SSM has a relatively sparse structure only showing the\nmost relevant paths.\n2An implementation of these features is available at www.mpi-inf.\nmpg.de/resources/MIR/chromatoolbox/ , see also [11].(a)\n20 40 60 80 100102030405060708090100Time (frames)(b)\n  \n20 40 60 80 10010\n20\n30\n40\n50\n60\n70\n80\n90\n1000.20.40.60.811.2Eigenvectors\n(c)\n20 40 60 80 100102030405060708090100Time (frames)(d)\n  \n20 40 60 80 10010\n20\n30\n40\n50\n60\n70\n80\n90\n100 00.20.40.60.811.21.41.61.8Eigenvectors\n(e)\n20 40 60 80 100 120 140 16020406080100120140160Time (frames)(f)\n  \n20 40 60 80 100 120 140 16020\n40\n60\n80\n100\n120\n140\n160 0.20.40.60.811.21.41.61.822.2Eigenvectors\nTime (frames) Time (frames)\nFigure 2: Various SSMs with path structures ( left) and corre-\nsponding eigenvectors ( right ). The ﬁgures shows the eigenvec-\ntors in some transposed (row-wise) and weighted (multiplie d by\nthe corresponding eigenvalue) form. Furthermore, the eige nvec-\ntors are sorted according to decreasing eigenvalues. (a)/(b) SSM\nIǫ\nK.(c)/(d) SSM reﬂecting the musical form A1A2.(e)/(f) SSM\nreﬂecting the musical form A1B1B2B3A2.\n2.2 Eigenvalue Decomposition\nWhen comparing the elements of a feature sequence in a\npairwise fashion using a symmetric similarity measure re-\nsults in a symmetric SSM S. This property may be lost by\napplying enhancement and image processing techniques as\nused in Section 2.1. However, one can restore the sym-\nmetry by considering1\n2(S+S⊤)instead of S, whereS⊤\ndenotes the transposed matrix of S. Doing so, we may\nassume in the following that Sis a symmetric matrix of\ndimension N×Nfor some N∈N.\nNext, we apply an eigenvalue decomposition of the\nsymmetric matrix Sand investigate the properties of the\nresulting eigenvectors. Principle component analysis tel ls\nus that there exists a real-valued diagonal matrix D=\ndiag(λ1,...,λ N)and an orthogonal matrix Esuch that\nS=EDE⊤, where the nthcolumnenofEis an eigen-\nvector of Swith eigenvalue λn, i. e.,Sen=λnen.\nIn our scenario, we assume that the matrix Sconsists of\npath-like structures, where a prototype of a path of length\nKmay be modeled by the matrix3\nIǫ\nK:=\nǫ1\nǫ1ǫ\n.........\nǫ1ǫ\n1ǫ\n∈ {0,ǫ,1}K×K.\n3Opposed to existing conventions, we enumerate in this paper s the\nrows of the matrix from bottom to top with the aim to better mat ch the\nvisualizations of the SSMs in the ﬁgures.\nTime (frames)\nTime (frames)\nTime (frames)\nTime (frames)Time (frames)\nTime (frames)\nEigenvectors\nTime (frames)(a) (b)\n(c) (d)\nFigure 3: (a) SSMSwith a path structure corresponding to\nthe musical form A1A2B1C1A3A4B2A5.(b)SSM with high-\nlighted block structure. (c)SSM in some normalized form after\napplying some permutation. (d)Illustration of the support of the\nsuitable sorted and transposed eigenvectors.\nIn this matrix the non-speciﬁed entries have the value 0, see\nalso Figure 2a. Intuitively, each path consists of a diagona l\nmatrix with large entries on the main diagonal (represented\nby the value 1inIǫ\nK) and with decreasing entries on the\ndiagonals above and below the main diagonal (represented\nby the value ǫinIǫ\nK). Such paths typically arise when\napplying path enhancement strategies based on smoothing\ntechniques.\nIn [6], an explicit eigenvalue decomposition of the ma-\ntrixIǫ\nKis described. The eigenvalues are\nλk= 1+ǫ·2coskπ\nK+1,1≤k≤K, (1)\nwith corresponding eigenvectors\nek=/parenleftig\nsinkπ\nK+1,sin2kπ\nK+1... ,sinKkπ\nK+1/parenrightig⊤\n.(2)\nIn particular, note that the entries of the eigenvectors are\nnon-zero, see also Figure 2b for an illustration. This prop-\nerty, as we will see, becomes crucial for converting paths\ninto blocks.\nThe matrix Iǫ\nKconstitutes the basic building block for\nSSMs with more general path structures. Instead of a\nmathematically rigorous treatment, which is beyond the\nscope of this paper, we explain the general case by means\nof an illustrative example. Let Sbe the SSM shown in Fig-\nure 3a, which has the path structure corresponding to the\nmusical form A1A2B1C1A3A4B2A5. The desired block\nstructure is indicated in Figure 3b. By applying a suitable\npermutation matrix P, it can be shown that Scan be con-\nverted into a matrix S′:=PSP−1=A⊕B⊕C , which is\nthe direct sum of three matrices A,B, andCcorresponding\nto the musical parts A,B, andC(inclusive repetitions),\nrespectively. This is illustrated by Figure 3c. As for the\neigenvalue decomposition, it can be shown that the eigen-\nvalues of S′are given by the eigenvalues of the matrices A,\nB, andC. Furthermore, the eigenvectors of S′are obtained\nby suitably extending the eigenvectors of the matrices A,B, andCby zero entries. As a result, the supports (non-\nzero entries) of eigenvectors coming from different sum-\nmandsA,B, andCinS′are disjoint. This fact is illustrated\nby Figure 3d. Furthermore, the repetitions induce substruc -\ntures in the summands A,B, andC. Each such substructure\ncan be expressed by a suitable Kronecker product with an\nall-one matrix and a matrix of the form Iǫ\nK. For example,\nthe matrix Bcorresponds to two repeating parts and can be\nexpressed by\n1R×R⊗Iǫ\nK=/parenleftbiggIǫ\nKIǫ\nK\nIǫ\nKIǫ\nK/parenrightbigg\nwithR= 2 being the number of repetitions, 1R×Rbe-\ning the all-one R×Rmatrix, and ⊗being the Kronecker\nproduct. Note that the rank of BisK, so that in the case\nofR= 2 half of the eigenvalues are zero (therefore, the\ncorresponding eigenvectors are not uniquely determined).\nAlso, the permutation Pis not known in practice. In our\npipeline, we multiply the normed eigenvectors with their\ncorresponding eigenvalue and arrange the modiﬁed eigen-\nvectors according to their length in decreasing order. (The\neigenvectors to eigenvalue 0are irrelevant here.)\nTo build up some more intuition, let us consider the ex-\namples shown in Figure 2. The case of Iǫ\nKand its eigen-\nvalue decomposition is illustrated by (a)/(b) of Figure 2,\nwhereas the case of two repeating segments ( 12×2⊗Iǫ\nK)\nis shown in (c)/(d). A third example corresponding to the\nmusical form A1B1B2B3A2is shown in (e)/(f) of Fig-\nure 2. Here, the disjointness property of the supports for\nthe eigenvalues that belong to different parts is visible.\nNote that the permutation matrix Pis not known and that\nthe eigenvectors are not ordered according to the musical\nparts they belong to. Furthermore, in practical applicatio ns\nthe path structures may be noisy and distorted so that the\ndiscussed properties of the eigenvectors are not strictly f ul-\nﬁlled.\n2.3 Deriving SSMs with Block Structures\nLetN∈Ndenote the dimension of the eigenvectors,\nwhich also coincides with the number of frames. As indi-\ncated by Figure 1c, we form a matrix by deﬁning its rows to\nbe the transposed eigenvectors weighted and sorted by the\ncorresponding eigenvalues. We denote this N×Nmatrix\nbyE. As discussed above, the path structure of the SSM\nis reﬂected by the support properties of the rows. In the-\nory (assuming an ideal path structure as discussed before),\ntwo rows either have the same support (when correspond-\ning to the same repeating musical part) or have disjoint\nsupports (when corresponding to repeating, but different\nmusical parts). Furthermore, the support of an eigenvec-\ntor reveals all frames that belong to repeating segments of\nthe same musical part (e. g., the frames of all A-part seg-\nments).\nMotivated by this observation, we consider the columns\nE(n)ofEas features, n∈[1 :N]. This yields a feature\nsequence E(1),...,E(N), which, in turn, can be used to\ndeﬁne a self-similarity matrix S(E). The properties of theeigenvalues imply that two features E(i)andE(j)are sim-\nilar if the frames iandjbelong to repeating segments of\nthe same musical part (or to frames of non-repeating seg-\nments), and dissimilar otherwise. As a consequence, the\nmatrixS(E)has the desired block structure.\nTo make this procedure applicable for real data, we\npost-process the matrix Eprior to forming the self-\nsimilarity matrix. To this end, we ﬁrst replace each entry\neofEbylog(10|e|+ 1) to prevent overrating the most-\nrepeated segment. Then we apply a standard k-means clus-\ntering procedure4to rearrange the eigenvectors (rows of\nE) so that vectors corresponding to similar structures are\nadjacent. Then we smooth the rearranged matrix in both\ndirections, horizontally as well as vertically, see Figure 1d.\nHere, the horizontal smoothing balances out the values of\nthe non-zero entries in the eigenvectors, whereas the verti -\ncal smoothing introduces robustness to local distortions.5\nDenoting the smoothed matrix by E′, we compute the self-\nsimilarity matrix as above to obtain SBlock=S(E′). This\nmatrix constitutes our ﬁnal result, see Figure 1e.\n3. EXPERIMENTS\nTo show how our conversion approach behaves on real\ndata, we now discuss a number of explicit examples (Sec-\ntion 3.2) and report on some quantitative experiments (Sec-\ntion 3.3). Note that optimizing and investigating the spe-\nciﬁc role of the various parameters is not in the scope\nof this paper. Rather than numerically improving a spe-\nciﬁc structure analysis result, our main goal is to high-\nlight the conceptual novelty of our approach. In partic-\nular, we demonstrate that procedures that are designed\nfor homogeneity-based structure analysis (as the one de-\nscribed in Section 3.1) can now be applied for repetition-\nbased structure analysis thanks to our conversion proce-\ndure.\n3.1 Structure Analysis Procedure\nAs a typical example approach, we consider the\nhomogeneity-based structure analysis procedure as de-\nscribed in [5], where a given self-similarity matrix is de-\ncomposed into a prototype matrix and an activation matrix\nusing non-negative matrix factorization (NMF). Looking at\nmaximizing entries in the activation matrix yields a frame-\nwise classiﬁcation of the columns of the SSM, which in\nturn can be used to assign a class label to each frame. A\nsegment is then deﬁned as a maximal run of consecutive\nframes having the same class label, see [5] for more details\nand Figure 1f for an example.\nIn our experiments, we used an NMF-variant with addi-\ntional sparseness constraints [7] setting the sparseness p a-\nrameter to 4·mean(SBlock)and the rank parameter to 6\n(assuming at most six different musical parts). The proce-\ndure was then applied to the matrix SBlock.\n4In our implementation, we used 6 clusters. Our experiments s howed\nthat any number between 5and20led to similar results.\n5In our experiments, we used Gaussian smoothing using an adap tive\nwindow size vertically and 7frames horizontally. Again these values are\nnot crucial here.  \n50 100 150 20020406080100120140160180200\n00.10.20.30.40.50.60.70.80.91\nTime (sec)Time (sec)\nA1A2B1B2C D1:4A3B3B4\n  \n50 100 150 200 250 300 35050100150200250300350\n00.10.20.30.40.50.60.70.80.91\nTime (sec)Time (sec)\nIA1B1A2B2A3C1D1D2A4B3A5C2D3J\n  \n50 100 150 200 25050100150200250\n00.10.20.30.40.50.60.70.80.91\nTime (sec)Time (sec)\nI12A12B1A34B23A56B45A78B6:9(a) (b) (c)\nFigure 4: Results for three different audio recordings. The ﬁgure sho ws the computed block matrix overlaid with the path structur e of\nthe input matrix (top), the computed structure analysis res ults (middle) and the manually generated structure annotat ions (bottom). (a)\nHungarian Dance No. 5 by Johannes Brahms. (b)March No. 1 from Op. 39 (Pomp and Circumstance) by Edward Elga r.(c)The song\n“The winner takes it all” by ABBA.\n3.2 Qualitative Evaluation\nWe now discuss some speciﬁc examples to show the po-\ntential and the limitations of our conversion procedure. We\nstart with our running example shown in Figure 1, which\nis a recording of the Waltz No. 2 from the Suite for Va-\nriety Orchestra by Dmitri Shostakovich. Using the path\nmatrixSas shown in Figure 1b as input, our conversion\nprocedure outputs the block matrix SBlockshown in Fig-\nure 1e. As the ﬁgure illustrates, path structures of repeat-\ning segments have been correctly converted into blocks.\nFor example, the four repetitions of the combined AB-\npart are clearly visible as path structure in Figure 1b and as\nblock structure in Figure 1e. Furthermore, Figure 1f shows\nthe computed structure annotation obtained from SBlock,\nwhereas Figure 1g shows a manually generated structure\nannotation. Indeed, the homogeneity-based clustering ap-\nproach applied to SBlockproduced a reasonable repetition-\nbased structure analysis result. Only subsequent repeatin g\nparts such as the two D-partsD1D2(which are clearly re-\nﬂected by paths in Figure 1b) have not been resolved by our\nframe-based labeling approach. Also, note that, because of\nsigniﬁcant musical variations in harmony and melody, the\ntwo repeating C-parts are neither reﬂected by paths nor by\nblocks.\nNext, we consider the three examples of Figure 4. For\neach example, the computed block matrix SBlockover-\nlaid with the original path structure inputted to our con-\nversion procedure is shown. Also the structure annotations\nobtained from SBlockas well as the manually generated\n“ground truth” annotations (for comparison) are shown.\nThe ﬁrst example shown in Figure 4a is a recording of the\nHungarian Dance No. 5 by Johannes Brahms. The A-part\nas well as the B-part segments are well reﬂected in the\nblock structure despite of some distortions and inconsiste n-\ncies in the path structure. Also, tempo differences between\nB-part segments ( B2andB4are played faster than B1and\nB3) still led to meaningful block structures. As in the pre-vious Shostakovich example, the subsequent repeating A-\npart andD-part segments were not subdivided as a result of\nthe purely frame-based labeling procedure. Also note that\neven in the path representation only the repetitions D1D2\nandD3D4were captured, but not the ﬁner grained subdivi-\nsion (because of the chosen temporal resolution induced by\nthe parameter setting). Finally, the B-part segments were\nfurther subdivided by our structure analysis procedure, il -\nlustrating an over-segmentation as typical for automated\nstructure analysis methods [9].\nThe example shown in Figure 4b is based on a record-\ning of the March No. 1 from Op. 39 by Edward Elgar. As\nbefore, one can say that overall the computed block struc-\nture correspond well to the inputted path structure. The\nerroneously extracted small path fragment indicated by the\nred circle has no major inﬂuence on the computed block\nstructure as well as on the ﬁnal structure. This indicates\nthat our conversion procedure is, at least to some degree,\nrobust to local distortions and noise. In general, our pro-\ncedure tends to yield better results when the inputted path\nstructure is sparse, thus requiring a denoising/smoothing\nand thresholding step to enhance the path structure as is\nalso done in most repetition-based structure analysis ap-\nproaches [1, 14].\nThe ﬁnal example is a recording of the song “The win-\nner takes it all” by ABBA, see Figure 4c. With this ex-\nample, we want to indicate that missing path relations as\nmarked by the red circle may be “recovered” in the block\nstructure. Since the eigenvalue decomposition is a global\nanalysis of the entire matrix S, local deviations and miss-\ning relations are balanced out, thus enforcing some kind of\ntransitivity on the block level.\n3.3 Quantitative Evaluation\nFinally, we quantitatively evaluated and compared our\noverall structure analysis procedure based on two well-\nknown datasets. First, we used the Beatles dataset withDataset Method pairwise boundary ( 3s)\nF [%] P [%] R [%] F [%] P [%] R [%]\nBeatlesTUT proposed 68.0 71.4 68.8 61.4 58.0 69.5\n[5] 60.8 61.5 64.6 N/A N/A N/A\n[13] 59.9 72.9 54.6 N/A N/A N/A\nSMGA (worst) 65.8 70.9 65.9 69.6 68.1 72.9\nSMGA (best) 71.8 65.1 80.0 75.3 73.4 79.1\nMazurka49-Rub proposed 72.3 70.1 78.7 60.6 66.3 60.5\nMazurka49-Coh proposed 70.0 69.3 74.2 62.7 65.3 65.9\nMazurka49-Eza proposed 71.4 69.0 77.4 64.4 70.7 64.1\nMazurka2792 SMGA (worst) 68.1 75.2 65.2 65.9 70.3 65.3\nSMGA (best) 71.9 75.8 71.6 69.2 72.4 69.5\nTable 1: Evaluation results for various procedures, evaluation\nmeasures, and datasets, see text for a detailed explanation .\nthe TUT annotations6described in [13]. Second, we\nused three complete recordings (Rubinstein 1966, Cohen,\nEzaki) taken from the 2792 recordings of the Mazurka\ndataset7with manually generated structure annotations.\nUsing standard precision (P), recall (R) and F-measure\n(F) for labeled pairs of frames as well as for segment\nboundaries (with 3 seconds tolerance), we compared our\napproach to [5, 13] as well as to the best performing\nMIREX2012 method8denoted by SMGA which is based\non an extension of [17]. For SMGA, the results are re-\nported for two different parameter settings corresponding\nto best and the worst performing setting, respectively.\nTable 1 shows the results. Note that we have applied a\nsimilar NMF-based structuring algorithm as in [5], how-\never applied to our converted matrix SBlock. This leads\nto substantial improvements compared to [5] on the Beat-\nles dataset considering pairwise P/R/F-values. Also com-\npared to the SMGA results, we are at least in the same\nrange. As for the segment boundaries, however, we are\nworse. This is by no surprise since our approach is a purely\nframe-based procedure, whereas SMGA is based on a seg-\nment boundary detection step. Similar results hold for the\nMazurka dataset, where SMGA has been evaluated on all\n2792 recordings, which include the three versions used in\nour experiments. Our procedure yields for all three pi-\nanists pairwise P/R/F-values that are in the same range as\nthe ones reported for SMGA. Again we want to empha-\nsize that the quantitative results are not in the focus of thi s\npaper, but should only indicate the overall behavior of our\nconversion procedure.\n4. CONCLUSIONS\nIn this paper, we introduced a novel method for convert-\ning SSMs with path structures into SSMs with block struc-\nture based on eigenvalue decompositions. As main tech-\nnical contribution, we discussed how certain path struc-\ntures translate into characteristic properties of the eige n-\nvectors. Furthermore, as an application of our conver-\nsion, we showed how a homogeneity-based structure anal-\nysis procedure can be applied to the converted path matrix\nto facilitate repetition-based structure analysis. We hop e\nthat our contribution is interesting not only from a concep-\n6http://www.cs.tut.fi/sgn/arg/paulus/structure.html\n7http://www.mazurka.org.uk\n8http://nema.lis.illinois.edu/nema_out/mirex2012/\nresults/struct/mrx09/tual point of view, but may also open up novel ways for\nfusing different segmentation principles at an early stage\nof a structure processing pipeline. In particular, it seems\npromising to directly combine block-like SSMs (reﬂecting\nhomogeneous musical properties) with converted path-like\nSSMs (reﬂecting repetitive musical properties), which can\nthen be handled using the same algorithmic pipeline.\nAcknowledgments: This work has been supported by\nthe German Research Foundation (DFG CL 64/8-1, DFG\nMU 2682/5-1). The International Audio Laboratories Er-\nlangen are a joint institution of the Friedrich-Alexander-\nUniversit¨ at Erlangen-N¨ urnberg (FAU) and Fraunhofer IIS .\n5. REFERENCES\n[1] Roger B. Dannenberg and Masataka Goto. Music structure a naly-\nsis from acoustic signals. In David Havelock, Sonoko Kuwano , and\nMichael V orl¨ ander, editors, Handbook of Signal Processing in Acous-\ntics, volume 1, pages 305–331. Springer, New York, NY , USA, 2008.\n[2] Shlomo Dubnov and Ted Apel. Audio segmentation by singul ar value\nclustering. Proc. ICMC, 2004.\n[3] Emilia G´ omez. Tonal Description of Music Audio Signals . PhD the-\nsis, UPF Barcelona, 2006.\n[4] Florian Kaiser, Marina Georgia Arvanitidou, and Thomas Sikora. Au-\ndio similarity matrices enhancement in an image processing frame-\nwork. Proc. CBMI, Madrid, Spain, 2011.\n[5] Florian Kaiser and Thomas Sikora. Music structure disco very in pop-\nular music using non-negative matrix factorization. Proc. ISMIR,\npages 429–434, Utrecht, The Netherlands, 2010.\n[6] Jerry L. Kazdan. A Tridiagonal Matrix, http://hans.math.\nupenn.edu/ ˜kazdan/AMCS602/tridiag-short.pdf , Retrieved\n11.03.2013.\n[7] Jingu Kim and Haesun Park. Toward faster nonnegative mat rix fac-\ntorization: A new algorithm and comparisons. Proc. ICDM, pa ges\n353–362, Pisa, Italy, 2008.\n[8] Mark Levy and Mark Sandler. Structural segmentation of m usical au-\ndio by constrained clustering. IEEE TASLP, 16(2):318–326, 2008.\n[9] Hanna Lukashevich. Towards quantitative measures of ev aluating\nsong segmentation. Proc. ISMIR, pages 375–380, Philadelph ia, USA,\n2008.\n[10] Meinard M¨ uller and Sebastian Ewert. Towards timbre-i nvariant au-\ndio features for harmony-based music. IEEE TASLP, 18(3):64 9–662,\n2010.\n[11] Meinard M¨ uller and Sebastian Ewert. Chroma Toolbox: M ATLAB\nimplementations for extracting variants of chroma-based a udio fea-\ntures. Proc. ISMIR, pages 215–220, Miami, FL, USA, 2011.\n[12] Meinard M¨ uller and Frank Kurth. Towards structural an alysis of au-\ndio recordings in the presence of musical variations. EURASIP Jour-\nnal on Advances in Signal Processing , 2007(1), 2007.\n[13] Jouni Paulus and Anssi P. Klapuri. Music structure anal ysis using a\nprobabilistic ﬁtness measure and a greedy search algorithm . IEEE\nTASLP, 17(6):1159–1170, 2009.\n[14] Jouni Paulus, Meinard M¨ uller, and Anssi P. Klapuri. Au dio-based\nmusic structure analysis. Proc. ISMIR, pages 625–636, Utre cht, The\nNetherlands, 2010.\n[15] Geoffroy Peeters. Deriving musical structure from sig nal analysis for\nmusic audio summary generation: “sequence” and “state” app roach.\nProc. CMMR, V ol. 2771, LNCS, pages 143–166. Springer, 2004.\n[16] Geoffroy Peeters. Music structure dicovery: Measurig the “state-\nness” of times. ISMIR: Late Breaking Session, 2011.\n[17] Joan Serr` a, Meinard M¨ uller, Peter Grosche, and Josep Lluis Arcos.\nUnsupervised detection of music boundaries by time series s tructure\nfeatures. Proc. AAAI International Conference on Artiﬁcia l Intelli-\ngence, Toronto, Canada, 2012.\n[18] Jordan Bennett Louis Smith, John Ashley Burgoyne, Ichi ro Fujinaga,\nDavid De Roure, and J. Stephen Downie. Design and creation of a\nlarge-scale database of structural annotations. Proc. ISM IR, pages\n555–560, Miami, FL, USA, 2011."
    },
    {
        "title": "Transfer Learning In Mir: Sharing Learned Latent Representations For Music Audio Classification And Similarity.",
        "author": [
            "Philippe Hamel",
            "Matthew E. P. Davies",
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416108",
        "url": "https://doi.org/10.5281/zenodo.1416108",
        "ee": "https://zenodo.org/records/1416108/files/HamelDYG13.pdf",
        "abstract": "This paper discusses the concept of transfer learning and its potential applications to MIR tasks such as music audio classification and similarity. In a traditional supervised machine learning setting, a system can only use labeled data from a single dataset to solve a given task. The labels associated with the dataset define the nature of the task to solve. A key advantage of transfer learning is in leveraging knowledge from related tasks to improve performance on a given target task. One way to transfer knowledge is to learn a shared latent representation across related tasks. This method has shown to be beneficial in many domains of machine learning, but has yet to be explored in MIR. Many MIR datasets for audio classification present a semantic overlap in their labels. Furthermore, these datasets often contain relatively few songs. Thus, there is a strong case for exploring methods to share knowledge between these datasets towards a more general and robust understanding of high level musical concepts such as genre and similarity. Our results show that shared representations can improve classification accuracy. We also show how transfer learning can improve performance for music similarity.",
        "zenodo_id": 1416108,
        "dblp_key": "conf/ismir/HamelDYG13",
        "keywords": [
            "transfer learning",
            "MIR tasks",
            "music audio classification",
            "music similarity",
            "shared latent representation",
            "related tasks",
            "semantic overlap",
            "few songs",
            "high level musical concepts",
            "robust understanding"
        ],
        "content": "TRANSFER LEARNING IN MIR: SHARING LEARNED LATENT\nREPRESENTATIONS FOR MUSIC AUDIO CLASSIFICATION AND\nSIMILARITY\nPhilippe Hamel, Matthew E. P. Davies, Kazuyoshi Yoshii and Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\nhamelphi@google.com, fmatthew.davies, k.yoshii, m.goto g@aist.go.jp\nABSTRACT\nThis paper discusses the concept of transfer learning\nand its potential applications to MIR tasks such as music\naudio classiﬁcation and similarity.\nIn a traditional supervised machine learning setting, a\nsystem can only use labeled data from a single dataset to\nsolve a given task. The labels associated with the dataset\ndeﬁne the nature of the task to solve. A key advantage of\ntransfer learning is in leveraging knowledge from related\ntasks to improve performance on a given target task. One\nway to transfer knowledge is to learn a shared latent rep-\nresentation across related tasks. This method has shown\nto be beneﬁcial in many domains of machine learning, but\nhas yet to be explored in MIR.\nMany MIR datasets for audio classiﬁcation present a se-\nmantic overlap in their labels. Furthermore, these datasets\noften contain relatively few songs. Thus, there is a strong\ncase for exploring methods to share knowledge between\nthese datasets towards a more general and robust under-\nstanding of high level musical concepts such as genre and\nsimilarity.\nOur results show that shared representations can im-\nprove classiﬁcation accuracy. We also show how transfer\nlearning can improve performance for music similarity.\n1. INTRODUCTION\nAs human beings, we are constantly learning to solve new\ntasks every day. The way we learn to perform new tasks is\ninﬂuenced by what we know about similar tasks [17].\nFor instance, let’s think of a pianist that wants to learn to\nplay guitar. The musician already has some knowledge of\nmusic theory, and knows how to use his motor skills to play\nthe piano. When he learns to play guitar, he will not start\nfrom scratch but rather use his prior knowledge on music\nand motor skills and build on top of it. We can see it as if\nthe musician transfers knowledge between tasks by sharing\na common abstract internal representation of music.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.\nClassifierFeatures\nLabelsT arget dataset\nLatent \nRepresentationRepresentation\nTransferLatent \nRepresentationRepresentation\nLearning\nFeatures\nLabelsSource datasetFigure 1 : Schema of our transfer learning approach. In the\nﬁrst step, we learn a latent representation in a supervised\nway using a source dataset. In the second step, we solve\nthe target task by ﬁrst mapping the features to the learned\nlatent space. In this example, the target task is a classiﬁca-\ntion task.\nThe equivalent concept in machine learning is called\ntransfer learning . It has been applied successfully in many\ndomains such as visual object recognition [13] and web-\npage classiﬁcation [8].\nThe performance of a supervised machine learning sys-\ntem is limited by the quantity and the quality of available\nlabeled data. Obtaining such data can be expensive. As a\nconsequence, many datasets in the MIR community have\na relatively small number of labeled examples. Some of\nthese datasets have been built to solve the same task, or\nsimilar tasks. For example, there exist many datasets for\ngenre classiﬁcation, and these datasets exhibit semantic\noverlap in their labels. However, each individual dataset\ncontains a relatively small number of examples. In this\ncontext, it would make sense to try to leverage the infor-\nmation from all these datasets to improve the overall per-\nformance. Transfer learning might allow us to do just that.\nIn this paper, we investigate how transfer learning ap-\nplied to genre classiﬁcation, automatic tag annotation and\nmusic similarity can be beneﬁcial. We hypothesize that\ntransferring latent representations learned on related tasks\ncan improve the performance of a given task when com-\npared with the original features. Our intuition is that the\nlearned representation will retain some knowledge of theoriginal task and that this knowledge should make the\ngiven task easier to solve.\nThe paper is divided as follows. We begin with an\noverview of transfer learning in Section 2. We describe the\ndifferent MIR tasks that are relevant to our experiments in\nSection 3. In Section 4 we give details about how we han-\ndle our features. The representation learning algorithm is\npresented in Section 5. We describe our experimental re-\nsults in Section 6. Finally, we conclude in Section 7.\n2. TRANSFER LEARNING\nTransfer learning is a machine learning problem that fo-\ncuses on reusing knowledge learned on one problem in\norder to help solve another. More formally, we will dis-\ntinguish between the target task , which is the task that we\nultimately want to solve, and the source task which is the\nrelated task that will help us in solving the target task. It is\nworth noting that there could be more than one source or\ntarget task.\nTransfer learning is an active ﬁeld of research, and\nmany approaches have been proposed [2, 8, 13]. Pan et\nal. [6] describe four transfer learning approaches: i) in-\nstance transfer , ii)feature representation transfer , iii) pa-\nrameter transfer and iv) relational knowledge transfer . In\nthis work, we will focus on the feature representation trans-\nfer approach, which consists of learning a common feature\nspace between the source and target tasks. More speciﬁ-\ncally, we will use a supervised approach to construct a fea-\nture space using labeled data from a source task, and then\nuse this feature space to help solve the target task. This\ntransfer learning process is illustrated in Figure 1.\nAlthough transfer learning has been applied success-\nfully in many domains, only a few applications can be\nfound in the MIR domain. In [8], self-taught learning,\nwhich is an extension of semi-supervised learning, is ap-\nplied to many tasks, including a 7-way music genre classi-\nﬁcation. However, very few details are provided on the na-\nture of the music data. In [3], a deep representation learned\non genre classiﬁcation is used for automatic tag annota-\ntion. Although the transferred representation is compared\nto a set of audio features, there is no comparison to the\noriginal spectral features that were used to build the deep\nrepresentation. Thus, it is difﬁcult to assess the impact of\nthe transfer of representation. In [10], a learned automatic\ntag annotation system is used to produce features to help\nsolve a music similarity task. In [15], a method that at-\ntempts to capture the semantic similarities between audio\nfeatures, tags, and artists names is presented. This multi-\ntask approach consists of embedding the different concepts\nin a common low-dimensional space. This learned space\ncan then be used to solve many MIR related tasks. In our\nwork, we use a similar approach to build a shared latent\nrepresentation.\n3. TASKS AND DATASETS\nIn this paper, we investigate transfer learning over three re-\nlated MIR tasks: genre classiﬁcation, music similarity es-Table 1 : Characteristics of the genre classiﬁcation and\nautomatic tag annotation datasets.\nDataset #of excerpts #of classes Audio length\n1517-Artists [11] 3180 19 full\nGTZAN [14] 1000 10 30s\nHomburg [4] 1886 9 10s\nUnique [12] 3115 14 30s\nMagnatagatune [5] 22787 160 30s\nTable 2 : Genre classes for the datasets. In bold are the\nterms which are also tags in the Magnatagatune dataset [5].\n1517-Artists GTZAN Homburg Unique\nAlternative & Punk blues alternative blues\nBlues classical blues country\nChildrens’s country electronic dance\nClassical disco folkcountry electronica\nComedy & Spoken Word hiphop funk soulrnb hip-hop\nCountry jazz jazz jazz\nEasy Listening & Vocals metal pop klassik\nElectronic &Dance pop raphiphop reggae\nFolk reggae rock rock\nHip-Hop rock schlager\nJazz soul rnb\nLatin volksmusik\nNew Age world\nR&B & Soul wort\nReggae\nReligious\nRock &Pop\nSoundtracks & More\nWorld\ntimation and automatic tag annotation. Even though these\ntask all use music audio as input data, they differ in their\ngoal and in the way performance is evaluated.\n3.1 Genre Classiﬁcation\nGenre classiﬁcation consists of choosing the genre that\nbest describes an audio excerpt given a set of genre la-\nbels. We consider 4 different datasets for genre classiﬁ-\ncation: 1517-Artists [11], GTZAN [14] Homburg [4], and\nUnique [12]. These datasets each contain between 1000\nand 3180 audio excerpts, from 10 seconds in length to full\nsongs, classiﬁed in 9 to 19 genres. In the case where full\nsongs are provided, we use only the ﬁrst 30 seconds of each\nsong. Further details about the datasets are in Table 1. The\ngenre labels have strong semantic overlap across datasets\nas can be seen in Table 2. For simplicity, we will some-\ntimes refer to the 1517-Artists dataset as artists .\nTo evaluate the performance of a genre classiﬁcation\nsystem, we use the classiﬁcation accuracy, which is sim-\nply the percentage of correctly classiﬁed excerpts in the\ntest set.\n3.2 Music Similarity\nMusic similarity systems seek to obtain a measure of sim-\nilarity between audio excerpts.\nOne issue with this task is that the meaning of similarity\nis ill-deﬁned. What is considered similar by one listener\nmight not be the same for another. Another issue is thatsimilarity is a pair-wise relative measure. Thus, it is com-\nplicated and costly to obtain enough ground truth informa-\ntion from human listeners to fully evaluate music similarity\nsystems. In order to circumvent these issues, music simi-\nlarity systems often use genre labels as a proxy for simi-\nlarity evaluation [7,9,12]. In this context, we consider that\ntwo excerpts within the same genre must be more similar\nthan two excerpts from different genres. On this basis, we\nwill use the same datasets as for genre classiﬁcation in our\nmusic similarity experiments.\nEven though genre classiﬁcation and music similarity\nuse the same data, the tasks differ on how we use the data\nand on how we evaluate performance. Typically, in the mu-\nsic similarity literature [7,9,12], the labels are not used for\ntraining. Thus, the task must be solved by signal process-\ning, unsupervised learning, or, in our case, by transferring\nsupervised learning from external datasets.\nThe evaluation of music similarity systems typically use\nprecision at kas a performance measure. Precision at k\ngives the ratio of excerpts of the same class in the knearest\nneighbors of a given excerpt. In this work we use k= 10 .\nApproaches to solve this task typically consist of mea-\nsuring distances in a feature space to obtain a distance ma-\ntrix. The type of features and the distance measure used\ncan vary. In [7], distance is computed using the Jensen-\nShannon divergence on a Gaussian representation of the\nfeatures. In [12], an L1-distance is computed over aggre-\ngated block-level features. In [9], an L1-distance is com-\nputed on features extracted in an unsupervised fashion.\nIn this work, we use the L1-distance on our different\nfeature sets in order to obtain a similarity matrix. We also\ntested Euclidian distance and Cosine distance and obtained\nsimilar results.\n3.3 Tag annotation\nThe automatic tag annotation task consists of assigning\nwords to describe an audio excerpt. It is a multi-label\nproblem, meaning that many labels can be applied to a\nsingle example. In this paper, we use the Magnatagatune\ndataset [5] which contains more than 22,000 30-seconds\nexcerpts and 160 tags. Tag labels include musical genre\n(rock, blues, jazz), instrumentation (guitar, piano, vocals),\nmood (sad, mellow), other descriptors (fast, airy, beat), etc.\nThere is high semantic overlap with the genre labels from\nthe four genre datasets. We illustrate this, in Table 2, by\nputting in bold the genres which are also tags in the Mag-\nnatagatune dataset.\n4. AUDIO FEATURES\nIn our experiments, we extract Mel-spectrum features from\naudio. We compute the Discrete Fourier Transform (DFT)\non frames of 46ms (1024 samples at 22kHz sampling rate)\nwith half frame overlap. We then pass the magnitude spec-\ntrum through 200 triangle Mel-scaled ﬁlters and take the\nlog-amplitude to obtain the Mel-spectrum features. These\nare what we will refer to as frame-level features.However, frame level features have been shown to be\nsuboptimal for genre classiﬁcation [1]. To obtain a better\nclassiﬁcation performance, we aggregate features on win-\ndows of 64 frames (about 1.5s), computing the mean, vari-\nance, maximum and minimum of each feature. We can ap-\nply this aggregation process to the Mel-spectrum features\nas well as to the frame-level latent representations. We will\nrefer to aggregated features as window-level features.\n5. LEARNING A LATENT REPRESENTATION\nIn order to transfer knowledge between tasks, we aim\nto learn a latent representation that will be shared across\ntasks. To learn this representation, we use the linear em-\nbedding method described in [16]. This method consists of\nembedding both the features and the labels via linear trans-\nformations in a common space. This algorithm is built to\nhandle a large number of labels in a multi-label problem,\nsuch as in the case of automatic tag annotation. However,\nthe model can trivially be adapted to multi-class problems\nwith a small number of classes such as genre recognition.\nThe model has also been extended to multi-task learning in\nMIR in [15].\nThe algorithm seeks to map both the features and the\nlabels in a common latent space, as illustrated in Figure 2.\nGiven a feature representation x2Rdand a set of labels\ni2 Y =f1; :::; Yg, we seek to jointly learn a feature\nembedding transform that will map the feature space to a\nsemantic space RD\n\bx(x) :Rd!RD\nand a label embedding transform that will map labels to\nthe same semantic space\n\by(i) :f1; :::; Yg!RD:\nThus, in this latent space, it is possible to measure dis-\ntances between different concepts such as between two fea-\nture vectors, a feature vector and a label, or between two\nlabels.\nSince we use linear maps, we have \bx(x) =V xwhere\nVis aD\u0002dmatrix and \by(i) =Wiwhere Wiis the\ni-th column of a D\u0002Ymatrix. We can obtain an afﬁnity\nmeasure between a feature vector and a given label with\nfi(x) = \b y(i)>\bx(x) =W>\niV x:\nEach training example has positive and negative labels\nassociated to it. Given a feature vector, an optimal rep-\nresentation would yield high afﬁnities for positive labels\nand low afﬁnities for negative labels. In other words, if\nwe rank the afﬁnities of the labels to the feature vector, the\npositive labels should be ranked low (i.e. in the ﬁrst few\npositions), and the negative labels should be ranked high.\nComputing the exact ranking of the labels becomes expen-\nsive when there are many labels. Thus, following [16] we\nuse a stochastic method that allows us to compute an ap-\nproximate ranking.\nThe training procedure is as follows. For a given train-\ning example x0, we randomly pick a positive label j. Then,BluesJazzX' Φx Φy\nΦyFeature Embedding\nTransformation\nLabel Embedding\nTransformationLabel Embedding\nTransformationFigure 2 : Illustration of the learning of the latent represen-\ntation. Audio features and labels are mapped to a common\nembedding space via the transformations \bxand\by. In\nthis example, excerpt X0hasjazz as a positive label and\nblues as a negative example. The black arrows illustrate\nhow the learning gradient will push the negative label em-\nbedding and the feature embedding away from each other,\nwhile pulling the positive example embedding and the fea-\nture embedding together.\nwe iterate randomly through the negative labels until we\nﬁnd a label j0for which fj0(x0)> fj(x0)\u00001. If we do\nnot ﬁnd such a negative label, we move to the next training\nexample. If we only need a few iterations to ﬁnd such a\nnegative label, chances are that the rank of the positive la-\nbel is high, we thus try to correct this by boosting the loss.\nOn the contrary, if we need many iterations to ﬁnd such\na negative label, the rank of the positive label is probably\nquite low, so we do not need to change the representation\nas much. We then minimize the loss given by\nL=L(r)j1\u0000fj(x0) +fj0(x0)j\nwhere L(r) =Pr\nk=11=kand r is the approximated rank\nof the label jand is given by\nr=\u0016Y\u00001\nN\u0017\nwhere N is the number of iterations needed to ﬁnd j0,\nandb\u0001cis the ﬂoor function. The loss Lis known as\nthe Weighted Approximate-Rank Pairwise loss, or WARP\nloss [16]. The Lterm increases as the approximate rank r\ngrows. The second term in the loss can be seen as a kind of\nhinge loss, which tries to maximize the margin. For a more\nin depth description of the algorithm, see [16] and [15] .\nIn our experiments we used a batch method, meaning\nthat we average the gradient over a batch before updating\nthe parameters. We use 100 examples per batch. For the\ndimensionality of our latent space, we followed [16] and\n[15] and chose D= 100 as the latent dimensionality for\nall our experiments.\nTo extend the model to a multi-dataset setting, we sim-\nply alternate between datasets after each batch. The feature\nembedding transformation is shared across all datasets, but\nthe label embedding transformations are independent. In\nthis way, we do not assume any semantic similarity be-\ntween similar classes across datasets. In Section 6.1, we\nshow that the model naturally learns these semantic simi-\nlarities.6. EXPERIMENTS\nWe conduct several experiments to assess if transferring\nknowledge across datasets and task can be beneﬁcial. First,\nwe qualitatively evaluate the semantic similarity in a multi-\ndataset genre embedding. Then, we compare genre clas-\nsiﬁcation performance between tag embedding, genre em-\nbedding and the base features. Finally, we use these feature\nspaces for the music similarity task.\n6.1 Semantic similarity\nIn our ﬁrst experiment, we learn an embedding jointly on\nthe four genre datasets. The combination of the four label\nsets gives us a total 52 labels. We then look at the nearest\nneighbours of the class embedding and make a qualitative\nevaluation. If the embedding process learns semantic infor-\nmation about the classes as expected, similar classes across\ndatasets should be close to each other.\nTo do this, we compute a distance matrix using an L1-\ndistance on the embeddings of all the classes. Then, for\neach class, we look at which classes are the closest and per-\nform a qualitative evaluation. Some typical examples are\npresented in Table 3. In general the similar classes across\ndatasets tend to be close to one another. For example, in\nTable 3, we see that the jazz classes all end up near one\nanother in the embedding space. However, there are also\nsome problematic classes. For instance, the blues classes\ndo not appear to all be clustered together. From these re-\nsults, we can say that the embedding space indeed learns\nsome kind of semantic knowledge about the classes.\n6.2 Genre Classiﬁcation\nFor this experiment, we consider three sets of features for\neach genre dataset: base features, genre embedding and\ntag embedding. The base features are the window-level\naggregated Mel-spectrum features described in Section 4.\nFor a given genre dataset, the genre embedding is\nlearned jointly on the 3 other genre datasets. It is learned\non frame-level Mel-spectrum features. The frame-level\nembedded features are then aggregated in a similar fash-\nion as the base features to obtain window-level features.\nThe tag embedding is learned on the Magnatagatune\ndataset. Again, the embedding is learned on frame-level\nfeatures and these are then aggregated to obtain window-\nlevel features. We then train a simple linear regression\nclassiﬁer on the window-level features. Finally to classify\na song, we average the output of the classiﬁer on the whole\nsong and pick the class with the highest output.\nOne of the key strengths of transfer learning compared\nto standard learning is the ability to improve performance\nusing fewer training examples [8]. To test this hypothesis,\nwe measure the accuracy of the classiﬁer across a range\nof training examples per class in the target dataset. Since\nthe number of examples per class is unbalanced in some\ndatasets, there are cases where there are fewer examples for\nthe less frequent classes. We ran a 10-fold cross-validation\nfor each experiment. The results are shown in Figure 3.Table 3 : Nearest neighbouring classes in the genre embedding space for a few examples.\nSeed 5 Nearest neighbours (in order)\nHip-Hop(artists) hip-hop(unique), raphiphop(homburg), schlager(unique), hiphop(gtzan), Electronic & Dance(artists)\nRock & Pop(artists) rock(unique) rock(homburg) Alternative & Punk(artists) metal(gtzan) alternative(homburg)\nElectronic & Dance(artists) raphiphop (homburg) reggae(unique) electronica(unique) pop(gtzan) dance(unique)\ncountry(gtzan) country(unique), folkcountry(homburg), rock(unique), Country(artists), Religious(artists)\njazz(homburg) jazz(unique), jazz(gtzan), Jazz(artists), world(unique), dance(unique)\nblues(unique) alternative(homburg), Alternative & Punk(artists), blues(gtzan), funksoulrnb(homburg), rock(homburg)\nTable 4 : Classiﬁcation accuracy and standard error on the\nfull training set using a 10-fold cross-validation.\nDataset Base Features Genre Embedding Tag Embedding\nArtists 0.323 +/- 0.010 0.310 +/- 0.005 0.338 +/- 0.007\nGTZAN 0.748 +/- 0.010 0.671 +/- 0.014 0.754 +/- 0.015\nHomburg 0.580 +/- 0.012 0.561 +/- 0.009 0.584 +/- 0.008\nUnique 0.651 +/- 0.006 0.634 +/- 0.005 0.666 +/- 0.006\nTable 5 : Precision at 10 for the music similarity task on\ndifferent feature spaces. The genre embedding is learned\nusing the 3 other genre datasets. The tag embedding is\nlearned on the Magnatagatune dataset.\nDataset Base Features Genre Embedding Tag Embedding\nArtists 0.15 0.19 0.19\nGTZAN 0.48 0.52 0.53\nHomburg 0.36 0.41 0.40\nUnique 0.53 0.52 0.54\nThese results show that the tag embedding often signif-\nicantly outperforms the base features. This conﬁrms our\nhypothesis. However, the genre embedding does not per-\nform as well, obtaining better accuracy only for the Hom-\nburg dataset.\nWe then measured the accuracy of the three feature sets\non the full training dataset. The results are in Table 4. We\nsee that the tag embedding tends to give slightly better re-\nsults.\n6.3 Music similarity\nFor this task, we used the same 3 feature sets as in\nSection 6.2. We use precision at 10 as the performance\nmeasure. Results are shown in Table 5. We see that both\nthe genre and tag embedding features perform better than\nthe base features, except for the Unique dataset where the\nthree feature sets perform about as well.\n7. CONCLUSION\nIn this paper, we conducted experiments on sharing a\nlearned latent representation between related MIR tasks.\nWe showed that jointly learning a representation on many\ngenre datasets naturally learns semantic similarity between\ngenre classes. In the context of genre classiﬁcation, we saw\nthat transferring a representation between tasks can signiﬁ-\ncantly improve classiﬁcation accuracy when the number of\ntraining examples is limited. In the context of music sim-\nilarity, we saw that the similarity space obtained by em-\nbedding features using genre and tag labels allows better\nprecision.\nThe fact that the genre embedding performed worsethan the base features for the genre classiﬁcation task goes\nagainst our hypothesis that classiﬁcation accuracy should\nbe improved by such a representation. This might be due\nto the fact that the genre datasets are rather small, and thus\nthere was not enough data to learn a robust representa-\ntion. Another reason might be that some of the seman-\ntic knowledge that was learned ended up in the label em-\nbedding transform rather than the feature embedding trans-\nform. Since we did not use the label embedding transform\nin the classiﬁcation task experiment, some of the learned\nknowledge might have been lost in the transfer. To address\nthis problem in future work, we could try to impose a more\nsevere regularization on the label embedding transforma-\ntion in the learning process. This could help to force the\nsemantic knowledge to go in the feature embedding trans-\nformation.\nIn this work, to focus on the simplest case ﬁrst, we lim-\nited ourselves to basic feature aggregation, a linear embed-\nding method, and a linear classiﬁer. Each of these elements\ncould be improved further. Thus the performance measures\npresented in this paper might not reﬂect the full power of\ntransfer learning. For the features, more complex block-\nlevel features as described in [12] could be constructed\nfrom the learned frame-level representation. For the rep-\nresentation learning, non-linear mappings could be used to\nobtain a more powerful representation. Finally, more com-\nplex classiﬁers, such as support vector machines or neural\nnetworks could be used to improve classiﬁcation accuracy\non the learned features.\nThis work presents a ﬁrst analysis of the potential of\ntransfer learning in MIR. We hope that the results pre-\nsented here will stimulate more research in the ﬁeld and\nmotivate the application of transfer learning in future MIR\napplications.\n8. ACKNOWLEDGMENTS\nThis work was supported by OngaCREST, CREST, JST.\n9. REFERENCES\n[1] J. Bergstra. Algorithms for Classifying Recorded Music by\nGenre. Masters thesis, Universit ´e de Montr ´eal, 2006.\n[2] R. Caruana. Multitask learning. Machine Learning , 28(1):41–\n75, July 1997.\n[3] P. Hamel and D. Eck. Learning features from music audio\nwith deep belief networks. In Proceedings of the 11th In-\nternational Conference on Music Information Retrieval (IS-\nMIR) , pages 339–344, 2010.\n[4] H. Homburg, I. Mierswa, B. Mller, K. Morik, and M. Wurst.\nA benchmark dataset for audio classiﬁcation and clustering.\nInProceedings of the 6th International Society for Music\nInformation Retrieval Conference (ISMIR) , pages 528–531,\n2005.0 10 20 30 40 50 60 70 80\n# of training example per class0.100.150.200.250.300.35Classification accuracy\nBase Features\nGenre Embedding\nTag Embedding(a) Artists\n0 10 20 30 40 50 60 70 80\n# of training example per class0.450.500.550.600.650.700.750.80Classification accuracy\nBase Features\nGenre Embedding\nTag Embedding (b) GTZAN\n0 10 20 30 40 50 60 70 80\n# of training example per class0.200.250.300.350.400.450.500.550.60Classification accuracy\nBase Features\nGenre Embedding\nTag Embedding\n(c) Homburg\n0 10 20 30 40 50 60 70 80\n# of training example per class0.350.400.450.500.550.600.650.70Classification accuracy\nBase Features\nGenre Embedding\nTag Embedding (d) Unique\nFigure 3 : Comparison of base features (baseline) to genre embedding and tag embedding for the genre classiﬁcation task.\nThe genre embedding and tag embedding representations are obtained through our proposed transfer learning method. The\nerror bars correspond to the standard error across the 10 folds.\n[5] E. Law and L. von Ahn. Input-agreement: a new mechanism\nfor collecting data using human computation games. In Pro-\nceedings of the International Conference on Human factors\nin computing systems , pages 1197–1206, 2009.\n[6] S. J. Pan and Q. Yang. A survey on transfer learning.\nIEEE Transactions on Knowledge and Data Engineering ,\n22(10):1345–1359, October 2010.\n[7] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and G. Wid-\nmer. On rhythm and general music similarity. In Proceed-\nings of the 10th International Society for Music Information\nRetrieval Conference (ISMIR) , pages 525–530, Kobe, Japan,\n2009.\n[8] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y . Ng. Self-\ntaught learning: transfer learning from unlabeled data. In Pro-\nceedings of the Twenty-Fourth International Machine Learn-\ning Conference (ICML 2007) , pages 759–766, Corvallis, Ore-\ngon, USA, 2007.\n[9] J. Schl ¨uter and C. Osendorfer. Music Similarity Estimation\nwith the Mean-Covariance Restricted Boltzmann Machine.\nInProceedings of the 10th International Conference on Ma-\nchine Learning and Applications (ICMLA 2011) , pages 118–\n123, Honolulu, USA, 2011.\n[10] K. Seyerlehner, R. Sonnleitner, Schedl, D. M., Hauger, and\nB. Ionescu. From improved auto-taggers to improved music\nsimilarity measures. In Proceedings of the 10th International\nWorkshop on Adaptive Multimedia Retrieval (AMR 2012) ,\nCopenhagen, Denmark, 2012.[11] K. Seyerlehner, G. Widmer, and P. Knees. Frame-level audio\nsimilarity - a codebook approach. In Proceedings of the 11th\nInternational Conference on Digital Audio Effects (DAFx-\n2008) , pages 349–356, Espoo, Finland, 2008.\n[12] K. Seyerlehner, G. Widmer, and T. Pohle. Fusing block-level\nfeatures for music similarity estimation. In Proc. of the 13th\nInt. Conference on Digital Audio Effects (DAFx-2010) , pages\n528–531, Graz, Austria, 2010.\n[13] T. Tommasi, N. Quadrianto, B. Caputo, and C. H. Lampert.\nBeyond dataset bias: Multi-task unaligned shared knowledge\ntransfer. In Proc. of the 11th Asian Conference on Computer\nVision (ACCV ) , pages 1–15, Daejeon, Korea, 2012.\n[14] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of\naudio signals. IEEE Transactions on Speech and Audio Pro-\ncessing , 10(5):293–302, 2002.\n[15] J. Weston, S. Bengio, and P. Hamel. Multi-tasking with\njoint semantic spaces for large-scale music annotation and\nretrieval. Journal of New Music Research , 40(4):337–348,\n2011.\n[16] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to\nlarge vocabulary image annotation. In Proceedings of the In-\nternational Joint Conference on Artiﬁcial Intelligence, IJCAI ,\nvolume 3, pages 2764–2770, 2011.\n[17] R. S. Woodworth and E. L. Thorndike. The inﬂuence of im-\nprovement in one mental function upon the efﬁciency of other\nfunctions. Psychological Review , 8(3):247–261, May 1901."
    },
    {
        "title": "The Million Musical Tweet Dataset What We Can Learn From Microblogs.",
        "author": [
            "David Hauger",
            "Markus Schedl",
            "Andrej Kosir",
            "Marko Tkalcic"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417649",
        "url": "https://doi.org/10.5281/zenodo.1417649",
        "ee": "https://zenodo.org/records/1417649/files/HaugerSKT13.pdf",
        "abstract": "Microblogs and Social Media applications are continuously growing in spread and importance. Users of Twitter, the currently most popular platform for microblogging, create more than a billion posts (called tweets) every week. Among all the different types of information being shared, some people post their music listening behavior, which is why Twitter became interesting for the Music Information Retrieval (MIR) community. Depending on the device and personal settings, some users provide geographic coordinates for their microposts. Having continuously crawled and analyzed tweets for more than 500 days (17 months) we can now present the “Million Musical Tweet Dataset” (MMTD) – the biggest publicly available source of microblog-based music listening histories that includes geographic, temporal, and other contextual information. These extended information makes the MMTD outstanding from other datasets providing music listening histories. We introduce the dataset, give basic statistics about its composition, and show how this dataset allows to detect new contextual music listening patterns by performing a comprehensive statistical investigation with respect to correlation between music taste and day of the week, hour of day, and country.",
        "zenodo_id": 1417649,
        "dblp_key": "conf/ismir/HaugerSKT13",
        "keywords": [
            "Microblogs",
            "Social Media",
            "Music Information Retrieval",
            "Twitter",
            "Music listening behavior",
            "Geographic coordinates",
            "Million Musical Tweet Dataset",
            "Contextual information",
            "Statistical investigation",
            "Music taste"
        ],
        "content": "THE MILLION MUSICAL TWEETS DATASET:\nWHAT CAN WE LEARN FROM MICROBLOGS\nDavid Hauger\nJohannes Kepler\nUniversity Linz\nAustria\ndavid.hauger\n@jku.atMarkus Schedl\nJohannes Kepler\nUniversity Linz\nAustria\nmarkus.schedl\n@jku.atAndrej Ko ˇsir\nUniversity of\nLjubljana\nSlovenia\nandrej.kosir\n@ldos.fe.uni-lj.siMarko Tkal ˇciˇc\nJohannes Kepler\nUniversity Linz\nAustria\nmarko.tkalcic\n@jku.at\nABSTRACT\nMicroblogs and Social Media applications are continuously\ngrowing in spread and importance. Users of Twitter , the\ncurrently most popular platform for microblogging, cre-\nate more than a billion posts (called tweets) every week.\nAmong all the different types of information being shared,\nsome people post their music listening behavior, which is\nwhyTwitter became interesting for the Music Informa-\ntion Retrieval (MIR) community. Depending on the device\nand personal settings, some users provide geographic co-\nordinates for their microposts.\nHaving continuously crawled and analyzed tweets for\nmore than 500days ( 17months) we can now present the\n“Million Musical Tweet Dataset” (MMTD) – the biggest\npublicly available source of microblog-based music listen-\ning histories that includes geographic, temporal, and other\ncontextual information. These extended information makes\nthe MMTD outstanding from other datasets providing mu-\nsic listening histories.\nWe introduce the dataset, give basic statistics about its\ncomposition, and show how this dataset allows to detect\nnew contextual music listening patterns by performing a\ncomprehensive statistical investigation with respect to cor-\nrelation between music taste and day of the week, hour of\nday, and country.\n1. INTRODUCTION\nMicroblogs and social media have continuously been grow-\ning in importance over the past years — for end users, but\nalso for industry and academia. Compared to other sources\nof information, they show high actuality and beneﬁt from\na large number of users. Twitter1, for instance, the\ncurrently largest platform for microblogging, already has\nabout 500million users as of October 2012, according to\n1http://www.twitter.com\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.Twitter CEO Dick Costolo2(the last ofﬁcial numbers\ndate back to 200million users in April 20113).\nMicroblogs have already been proven successful in a\nnumber of different contexts (see Section 2), but up to now\nthey have hardly been exploited within the ﬁeld of Music\nInformation Retrieval (MIR). As there are no strict rules or\nspeciﬁed formats for the up to 140characters a Twitter\npost (tweet) consists of, Twitter is a relatively noisy\nsource of information. However, thanks to hashtags and\nplugins for music players that automatically post music\nlistening events, Twitter is a valuable source of infor-\nmation for MIR, when trying to incorporate or explicitly\nevaluate the user context. Compared to other sources like\nLast.fm4,Twitter also provides information on geo-\ngraphic positions if available (for instance, if posted from\na GPS-enabled device).\nIn this paper, we present a novel dataset of informa-\ntion derived from microblogs (tweets), describing the mu-\nsic listening habits of users. It is composed of preprocessed\ntweets, being consistently mapped to artists and songs from\nMusicBrainz5. As far as we are aware of, this dataset\nis currently the largest source of information on geospatial\nmusic listening events publicly available.\nThe remainder of the paper is structured as follows. First\nwe give an overview of existing datasets and their applica-\ntions in Section 2. In Section 3 we provide information\nabout the acquisition of the dataset and some basic statis-\ntics. In Section 4 we conduct a statistical analysis of the\ncorrelation between music taste (measured via genre distri-\nbution) and temporal as well as geographical properties. In\nSection 5 we brieﬂy present some ideas on how the dataset\ncan be exploited, focusing on music visualization and con-\ntextual clustering. In Section 6 we summarize the work\nand outline possibilities of further exploiting the dataset.\n2. RELATED WORK\nMicroblogging services like Twitter are continuously\ngrowing in importance. They have already been exploited\n2http://www.telegraph.co.uk/technology/\ntwitter/9945505/Twitter-in-numbers.html\n3http://huffingtonpost.com/2011/04/28/\ntwitter-number-of-users\\_n\\_855177.html\n4http://www.last.fm\n5http://musicbrainz.orgfor research in different areas, for instance, detection of\nbreaking news [16], trends [12] [23], earthquakes [17], or\nhealth issues [14], even exploring spatio-temporal dynam-\nics [10].\nNevertheless, within the MIR community, microblogs\nare still a relatively new source of information. As recent\nwork stresses the importance of adding contextual infor-\nmation to music recommendation [1] [22], we believe that\nmicroblogs in general, and the proposed dataset in particu-\nlar, provide valuable information for MIR. Twitter has\nalready been used as a source for music similarity estima-\ntion [20], music recommendation [24], and for identifying\ncultural listening patterns [19].\nUnfortunately, most existing datasets of Twitter posts,\nsuch as “The Edinburgh Twitter Corpus” [15], are not suited\nfor geospatial MIR tasks as only very few tweets are re-\nlated to music and less than 3%of the tweets have ge-\nolocalized information available. Also the frequently used\ndataset of the TREC 2011 and 2012 Microblog tracks6\n[13] is not suited. Although it contains approximately 16\nmillion tweets, this dataset is not tailored to music-related\nactivities, i.e. the amount of music-related posts is marginal.\nMoreover, tweets have no speciﬁc format ( \u0014140char-\nacters of unstructured text) and therefore need preprocess-\ning to be assigned to a speciﬁc artist and/or song. Never-\ntheless, Twitter is one of the few sources with geolocal-\nized listening information being available. Therefore, this\nsource could be valuable for previous work like [2], where\nLast.fm was used as a source for listening histories. The\nmost recent similar, but much smaller dataset for music-\nrelated microblogs is the “MusicMicro” dataset [18]. Be-\nsides being smaller, the “MusicMicro” dataset does not of-\nfer genre information nor other musical metadata (except\nfor artist and song name).\nAs for datasets targeted at the music domain, the “Mil-\nlion Song Dataset” [3] and the “Yahoo! Music Dataset” [5]\nare quite popular. However, they do either not provide lis-\ntening information (“Yahoo! Music Dataset”) or only taste\nproﬁles without geo infomation (“Million Song Dataset”).\nThe “Million Musical Tweet Dataset” (MMTD) presented\nhere, in contrast, provides geolocalized listening informa-\ntion and links plain text tweets to respective artists and\ntracks, which allows for combination with content-based\n[4] and other contextual features [7]. The MMTD cur-\nrently provides the biggest publicly available dataset for\ngeolocalized music listening behavior. It can be down-\nloaded from http://www.cp.jku.at/datasets/\nMMTD/ .\nThe two pieces of information that make the MMTD\nunique are temporal information andgeographic informa-\ntion. The former was shown to be extremely useful in\nthe recommendation systems domain. Koren et al. [9] de-\nveloped a matrix factorization-based method for modeling\ntemporal dynamics in order to improve the rating predic-\ntion in the movies domain. Similarly, Koenigstein et al. [8],\nused this method to improve the rating prediction on the\n“Yahoo! Music Dataset” in the 2011 KDD Cup. Infor-\n6http://trec.nist.gov/data/tweetsrank country\ncodenumber of\nusersrank country\ncodenumber of\ntweets\n1 US 70,204 1 US 227,432\n2 ID 30,605 2 DE 153,163\n3 BR 24,985 3 BR 145,049\n4 MY 14,771 4 GB 130,951\n5 FR 13,890 5 ID 94,245\n6 GB 9,006 6 FR 65,525\n7 RU 5,234 7 MY 50,648\n8 NL 5,223 8 CA 27,370\n9 MX 4,538 9 RU 23,542\n10 TR 2,878 10 MX 18,717\n11 ES 2,847 11 NL 18,320\n12 SG 2,422 12 TR 14,479\n13 PH 2,385 13 ES 11,811\n14 CA 2,344 14 SG 7,637\n15 IT 1,521 15 IT 6,412\n16 JP 1,501 16 AR 6,319\n17 ZA 1,297 17 PH 5,723\n18 DE 1,133 18 JP 5,529\n19 UA 1,062 19 UA 4,940\n20 AR 874 20 ZA 3,733\nTable 1 . Top-20 countries by number of users and tweets.\nmation on countries are valuable for culture-speciﬁc MIR\napproaches [21].\n3. DATA ACQUISITION AND BASIC STATISTICS\nIn this section, we provide the background of the data ac-\nquisition and processing that led to the presented dataset.\nWe further give some basic dataset statistics.\n3.1 Data Acquisition\nBetween September 2011 and April 2013 we crawled the\nTwitter Streaming API7, which provides a random sub-\nset of 1%of all tweets. We retained only tweets with ge-\nographic information attached (less than 3%of all tweets)\nand including potentially music-related hashtags that have\nalready been proven successful [6], e.g. #nowplaying ,\n#np,#itunes ,#musicmonday and#thisismyjam .\nWe employed the pattern-based approach described in [6]\nto map the content of the tweets to artists and tracks. We\nused the MusicBrainz database for indexing, which cov-\ners\u001930% of all tweets including the desired hashtags.\nOf course this method creates some bias in terms of cul-\ntural music listening patterns as the dataset is restricted\ntoTwitter users posting musical information and there\nmight be other conventions for the usage of hashtags in\nnon-western countries.\nTo enable experimenting with the MMTD on a semantic\nlevel, we used a web service provided by Mapquest8to\nmap geographic coordinates to cities, countries, and other\ngeographic entities. To get comparable local time (which is\nnot directly provided by Twitter ) we used GeoNames9\nfor retrieving the time zones for the geographic coordi-\nnates. The top 20 countries in terms of number of users,\nrespectively number of tweets, are listed in Table 1.\nIn addition to this contextual information, we added\ngenre information by querying Last.fm for tags on the\n7https://dev.twitter.com/docs/streaming-apis\n8http://www.mapquest.com\n9http://geonames.orgFigure 1 . Number of tweets per hour of the day.\nFigure 2 . Number of tweets per day of the week.\nartist and song level, ﬁltering them by the 20most popular\ngenres from Allmusic10. This resulted in a multi-genre\nfeature vector for each tweet.\n3.2 Basic Statistics\nThe “Million Musical Tweets Dataset” (MMTD) is based\non1;086;808 tweets referring to 133;968 unique tracks\n(mean = 8:11tweets/track; \u001b= 44:94;median = 1)\nby25;060different artists ( mean = 43:37tweets/artist;\n\u001b= 327:03;median = 3). The tweets were created by\n215;375users from 202different countries. On average we\nhave 1;078users per country ( \u001b= 5;848;median = 29:5)\nand5;381tweets per country ( \u001b= 25;032:45;median =\n74:5), each user creating on average 5:08tweets (\u001b=\n268:19;median = 1).\nAnalyzing the temporal distribution of tweets shows that\ntwitterers are less active during night, as expected (see Fig-\nure 1). However, there is no signiﬁcant difference between\nthe days of the week (see Figure 2).\nUsing the pre-ﬁltered Last.fm tags for multiple genre\nassignment, we obtained 276;697tweets per genre ( \u001b=\n256;662). The distribution of genres is unbalanced as can\nbe seen in Figure 3. For future work, different genre classi-\nﬁcations could be investigated, for instance, using ontolo-\ngies or synonyms for the provided tags.\n10http://www.allmusic.com\nFigure 3 . Number of tweets per genre.\nFigure 4 . Number of tweets per country.\n4. SPATIO-TEMPORAL STATISTICAL ASPECTS\nIn order to show the utility of the presented dataset we per-\nformed two experiments: (i) a geographical analysis of the\nlistening preferences reﬂected in the dataset and (ii) a tem-\nporal analysis. For that purpose, we aggregated the dataset\non a geographical and on a temporal basis. The tweets\nwere ﬁrst grouped by country. Then for each country the\ntweets were grouped by the day of the week. Finally, for\neach country and day of the week, the tweets were grouped\nby the hour of the day (e.g., all tweets from 2:00 to 2:59\nwere grouped together). When grouping, we summed the\ngenre vectors of all the tweets belonging to a group, which\nyielded a summed vector, or histogram, of the genre distri-\nbution in each group. An excerpt of the aggregated data is\nshown in Table 2.\nThe distribution of tweets among countries, on the daily\nbasis, and on the hour-of-the-day basis are shown, respec-\ntively, in Figures 4, 2, and 1.\n4.1 Geographical analysis of the dataset\nIn this experiment we addressed the research question The\nresearch question addressed in this experiment is whether\nthere are any differences between musical tastes among\ndifferent countries that are reﬂected in the dataset. When\nwe use the term musical taste we refer to how often spe-\nciﬁc genres have been played in the observed country (or\nother cluster in the next subsection).\nTo answer this question we ﬁrst conducted the Kruskal-\nWallis ANOV A (Analysis of Variance) (see [11] for de-country day of\nweektime\nof dayrnb rap el ro na cl re bl co wo fo el ja vo ch pu al sw pop hm\nBrazil 4 (Fri) 00 199 343 544 752 10 52 121 167 78 62 290 157 273 44 0 464 707 1 778 262\nBrazil 4 (Fri) 05 29 30 49 70 2 8 13 19 5 6 22 17 28 6 0 39 65 0 69 17\nBrazil 4 (Fri) 12 312 476 700 920 4 74 190 186 103 58 331 250 313 59 0 535 880 1 950 296\nBrazil 4 (Fri) 17 260 424 647 873 6 60 150 164 99 44 299 201 315 45 0 533 847 3 880 310\nBrazil 4 (Fri) 21 332 578 862 1104 6 60 194 220 149 62 385 247 382 63 0 684 1073 01164 426\nFrance 6 (Sun) 00 213 277 194 208 4 10 90 50 17 30 69 56 90 21 0 80 196 6 260 24\nFrance 6 (Sun) 05 22 29 22 18 1 0 8 4 0 1 4 3 9 2 0 6 17 0 24 0\nFrance 6 (Sun) 12 422 529 440 417 4 12 173 105 27 51 112 116 203 40 0 173 403 3 528 41\nFrance 6 (Sun) 17 280 325 276 280 1 13 108 75 26 32 72 89 113 28 0 118 302 5 366 20\nFrance 6 (Sun) 21 265 331 283 283 1 20 113 72 30 22 91 83 129 34 0 106 268 2 348 22\nIndonesia 0 (Mon) 00 128 145 253 352 7 19 60 88 72 18 173 155 154 41 1 187 322 2 395 76\nIndonesia 0 (Mon) 05 27 34 42 59 2 5 13 12 13 1 26 23 27 6 0 32 57 0 67 15\nIndonesia 0 (Mon) 12 206 223 359 509 3 29 58 85 81 17 185 211 175 47 0 226 466 0 580 104\nIndonesia 0 (Mon) 17 245 295 428 619 16 35 84 108 84 17 206 255 201 55 0 296 569 1 706 110\nIndonesia 0 (Mon) 21 273 316 511 722 25 68 81 141 137 31 302 360 287 85 0 360 680 2 843 144\nMalaysia 2 (Wed) 00 133 169 245 306 8 23 40 58 47 15 140 98 122 32 0 163 295 2 358 59\nMalaysia 2 (Wed) 05 9 9 18 21 1 1 2 3 1 2 8 10 6 4 0 13 20 0 29 5\nMalaysia 2 (Wed) 12 75 84 137 173 6 8 26 29 27 5 62 53 63 14 0 92 158 0 209 37\nMalaysia 2 (Wed) 17 167 178 269 327 9 12 53 45 53 20 133 100 111 38 0 157 302 2 392 72\nMalaysia 2 (Wed) 21 173 180 271 355 4 18 52 51 68 22 133 150 123 39 0 157 332 2 432 69\nUnited States 1 (Tue) 00 992 1062 865 923 12 47 392 218 145 59 313 267 466 122 0 502 994 13 1348 143\nUnited States 1 (Tue) 05 248 266 219 251 2 9 100 53 40 14 80 79 125 29 0 132 259 4 346 42\nUnited States 1 (Tue) 12 632 757 667 762 15 30 283 183 124 45 266 198 347 75 0 415 785 18 973 144\nUnited States 1 (Tue) 17 696 833 678 712 7 35 270 145 107 35 239 210 341 69 0 359 792 14 991 104\nUnited States 1 (Tue) 21 1125 1311 1076 1168 15 52 459 297 180 57 405 361 543 126 0 617 1241 17 1649 153\nTable 2 . Some random examples for aggregated data on a per-country, per-day-of-week, and per-hour-of-day basis from\nthe Top-5 countries in terms of numbers of users. The order of genres corresponds to that in Figure 3.\ntails) on the whole dataset grouping the data by genre and\ncomparing each genre separately. The analysis showed that\nallp-values were p < 0:001, meaning that there are sig-\nniﬁcant differences among all variables according to the\ngeographical location of tweets.\nAfter the ANOV A showed signiﬁcant differences, we\nproceeded with a pair-wise comparison between the coun-\ntries. Since there are 202countries, this would mean roughly\n20;000comparisons which makes it very hard, if not im-\npossible, to interpret. As such an analysis would be hard\nto perform, we opted to choose a subset of 20 countries to\nperform the pairwise comparison. We chose the 20 coun-\ntries with the biggest number of tweets (see Table 1).\nIn the pairwise comparison, we compared the histograms\nof all the 20 genres, among 20 selected countries using\nthe Chi-square goodness of ﬁt test (see [11] for details).\nThe test result shows a p-valuep < 0:001for all pairs.\nHence, all countries are signiﬁcantly different from each\nother (compare to previous experiments on cultural listen-\ning patterns in [19]).\n4.2 Temporal analysis of the dataset\nWe compared the distribution of genre preferences among\ndifferent days of the week using the Kruskal-Wallis ANOV A\nto see whether listening habits are different on weekdays\nand on weekends. However, the test did reveal no sig-\nniﬁcant differences among days of the week. Even when\nwe narrowed the selection of countries down to ﬁve cultur-\nally distinct countries (Saudi Arabia, Malaysia, Germany,\nUnited Kingdom and Brazil), the differences were not sig-\nniﬁcant. This means that the listening habits of the users\npresent in this dataset do not differ between days. Two pos-\nsible explanations for this ﬁnding are: (i) although some-\none might listen to the own favorites on week-ends and to\nartists reﬂecting the “common taste” among colleagues on\nworking days, only music that is really liked is also posted\nviaTwitter (if it is not automatically tweeted), or (ii)\nthe aggregation already reﬂects this “common taste” forany day of the week.\nBased on the distribution of the listening habits (tweets\ngrouped by the hour of the day), as depicted in Figure 1, we\nclustered the tweets in the following hour-of-day groups,\nusing the minimum at 5am, and the local maxima at noon\nand10pmas borders:\n\u000ffrom 05:00 to 11:59 (morning)\n\u000ffrom 12:00 to 21:59 (afternoon/evening)\n\u000ffrom 22:00 to 04:59 (night)\nPerforming the cross-group test for all countries with\nthe Kruskal-Wallis ANOV A showed that all p-values are\np < 0:05, which means that there are signiﬁcant differ-\nences among all the hour-of-day groups, e.g. people lis-\nten to different music on Monday mornings vs. Satur-\nday nights. Repeating the test for the 5 culturally differ-\nent countries, however, not all the p-values arep < 0:05,\nwhich we report in Table 3. This means that, for these\ncountries, having p> 0:05, some genres do not show sig-\nniﬁcantly different playing frequency among the various\nhour-of-day groups.\n5. USING THE MILLION MUSICAL TWEETS\nDATASET FOR VISUALIZATION\nHaving discussed statistics of our dataset, the current sec-\ntion brieﬂy points out an example of how the information\nwithin the MMTD may be used for clustering and visual-\nizion. As multi-genre-vectors are not suited to map tweets\nto a certain color, we decided to use the approach presented\nby Hauger and Schedl [6] using non-negative matrix fac-\ntorization of Last.fm genre tags and latent factors to as-\nsign tweets to a color.\nWe aggregated the tweets of our dataset by their ge-\nographic coordinates and displayed them as circles on a\nmap, where the size of the circle represents the number\nof tweets for this area and the color represents the mostrnb rap el ro na cl re bl co wo fo el ja vo ch pu al sw pop hm\nChi-square 15.86 25.74 23.61 23.59 2.09 5.41 11.61 6.82 8.05 4.58 13.22 15.74 10.50 2.53 1.99 21.61 24.44 1.92 23.99 16.00\ndf 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\nAsymp. Sig. 0 0 0 0 0.352 0.067 0.003 0.033 0.018 0.101 0.001 0 0.005 0.282 0.371 0 0 0.384 0 0\nTable 3 . Kruskal-Wallis ANOV A results for the differences among the hour-of-day variable grouped by genre for the 5\nculturally different countries.\nFigure 5 . Visualizing genres / latent factors for Brazil.\npopular genre or latent factor within this set of tweets. Ac-\ncording to our ﬁndings that there are signiﬁcant differences\nin the genre distribution for countries, we hence visually\nshow differences between countries.\nFigure 5 shows that in Brazil the genre cluster for “Rock”\n(yellow) is by far the most popular one. Comparing it\nto France, the European country with the largest number\nofTwitter users, we see that French twitterers show a\nstrong preference for the group representing the “Rap” and\n“Hip-Hop” cluster (violet).\n6. DISCUSSION AND FUTURE WORK\nIn this paper, we presented the “Million Musical Tweets\nDataset” (MMTD). Its unique property of including time\nand geo-location data allows the research community to\nfollow novel research avenues.\nThe results of the statistical tests showed that there are\nsigniﬁcant differences in musical tastes (expressed through\nmulti-genre vectors) among different clusters (both on the\ngeographical basis and on the temporal basis). These dif-\nferences could be exploited for developing adaptive sys-\ntems/services on the geo-temporal basis (e.g., contextual\nﬁltering of music based on country and/or time of day).\nWe also presented one way of visualizing differences in\ngeospatial music listening patterns.\nThe proposed dataset is publicly available and may be\nused, for instance, for contextual music recommendation\nor similarity estimation. As for the latter, the MMTD could\nbe used to build hybrid similarity functions including au-\ndio features, contextual music features (tags, playlist co-\nFigure 6 . Visualizing genres / latent factors for France.\noccurrences), and user context information.\nFuture work will put emphasis on user-related examina-\ntion of the dataset, looking into other user-centric proper-\nties like age, gender, or twitting activity, as well as on the\nsearch for “cultural clusters” of different countries and/or\ncities. In this vein, it would also be interesting to exam-\nine whether there are differences between urban and rural\nareas.\n7. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Funds\n(FWF): P22856, P25655, and by the European Union FP7\nprogramme through the PHENICX project (grant agree-\nment no. 601166).\n8. REFERENCES\n[1] Linas Baltrunas, Bernd Ludwig, Stefan Peer, and\nFrancesco Ricci. Context-Aware Places of Interest\nRecommendations and Explanations. Joint Proceed-\nings of the Workshop on Decision Making and Rec-\nommendation Acceptance Issues in Recommender Sys-\ntems (DEMRA) and the 2nd Workshop on User Models\nfor Motivational Systems: The affective and the ratio-\nnal routes to persuasion (UMMS) , 2011.\n[2] D. Baur, F. Seiffert, M. Sedlmair, and S. Boring. The\nstreams of our lives: Visualizing listening histories incontext. IEEE Transactions on Visualization and Com-\nputer Graphics , 16(6):1119–1128, 2010.\n[3] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInProceedings of the 12th International Conference on\nMusic Information Retrieval (ISMIR) , 2011.\n[4] Michael A. Casey, Remco Veltkamp, Masataka Goto,\nMarc Leman, Christophe Rhodes, and Malcolm\nSlaney. Content-Based Music Information Retrieval:\nCurrent Directions and Future Challenges. Proceed-\nings of the IEEE , 96:668–696, April 2008.\n[5] Gideon Dror and Noam Koenigstein and Yehuda Koren\nand Markus Weimer. The Yahoo! Music Dataset and\nKDD-Cup’11. In Proceedings of KDDCup 2011 , 2011.\n[6] David Hauger and Markus Schedl. Exploring Geospa-\ntial Music Listening Patterns in Microblog Data. In\nProceedings of the 10th International Workshop on\nAdaptive Multimedia Retrieval (AMR) , Copenhagen,\nDenmark, October 2012.\n[7] Peter Knees and Markus Schedl. A Survey of Mu-\nsic Similarity and Recommendation from Music Con-\ntext Data. ACM Transactions on Multimedia Comput-\ning, Communications, and Applications (TOMCCAP) ,\n2013.\n[8] Noam Koenigstein, Gideon Dror, and Yehuda Koren.\nYahoo! music recommendations. Proceedings of the\n5th ACM Conference on Recommender Systems (Rec-\nSys), page 165, 2011.\n[9] Yehuda Koren. Collaborative ﬁltering with tempo-\nral dynamics. Communications of the ACM , 53(4):89,\nApril 2010.\n[10] Chung-Hong Lee, Hsin-Chang Yang, Tzan-Feng\nChien, and Wei-Shiang Wen. A Novel Approach for\nEvent Detection by Mining Spatio-temporal Informa-\ntion on Microblogs. In International Conference on\nAdvances in Social Networks Analysis and Mining\n(ASONAM) , pages 254–259, July 2011.\n[11] E L Lehman, Joseph P. Romano, and Erich L.\nLehmann. Testing Statistical Hypotheses . Springer\nTexts in Statistics. Springer New York, New York, NY ,\n2005.\n[12] Michael Mathioudakis and Nick Koudas. Twittermoni-\ntor: trend detection over the twitter stream. In Proceed-\nings of the 2010 ACM SIGMOD International Confer-\nence on Management of Data , pages 1155–1158, New\nYork, NY , USA, 2010. ACM.\n[13] Richard McCreadie, Ian Soboroff, Jimmy Lin, Craig\nMacdonald, Iadh Ounis, and Dean McCullough. On\nBuilding a Reusable Twitter Corpus. In Proceedings of\nthe 35th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval (SI-\nGIR) , Portland, OR, USA, August 12-–16 2012.[14] Michael J. Paul and Mark Dredze. You Are What You\nTweet : Analyzing Twitter for Public Health. Artiﬁcial\nIntelligence , pages 265–272, 2011.\n[15] Sa ˇsa Petrovi ´c, Miles Osborne, and Victor Lavrenko.\nThe edinburgh twitter corpus. In Proceedings of the\nNAACL HLT 2010 Workshop on Computational Lin-\nguistics in a World of Social Media , pages 25–26, Los\nAngeles, California, USA, June 2010. Association for\nComputational Linguistics.\n[16] S. Phuvipadawat and T. Murata. Breaking news de-\ntection and tracking in twitter. In Web Intelligence\nand Intelligent Agent Technology (WI-IAT), 2010\nIEEE/WIC/ACM International Conference on , vol-\nume 3, pages 120–123, 2010.\n[17] Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.\nEarthquake Shakes Twitter Users: Real-Time Event\nDetection by Social Sensors. In Proceedings of the\n19th International Conference on World Wide Web\n(WWW) , May 2010.\n[18] Markus Schedl. Leveraging Microblogs for Spatiotem-\nporal Music Information Retrieval. Lecture Notes In\nComputer Science , 7814:796–799, 2013.\n[19] Markus Schedl and David Hauger. Mining Microblogs\nto Infer Music Artist Similarity and Cultural Lis-\ntening Patterns. In Proceedings of the 21st Interna-\ntional World Wide Web Conference (WWW): 4th In-\nternational Workshop on Advances in Music Informa-\ntion Research: “The Web of Music” (AdMIRe) , Lyon,\nFrance, 2012.\n[20] Markus Schedl, David Hauger, and Juli ´an Urbano.\nHarvesting microblogs for contextual music similarity\nestimation - a co-occurrence-based framework. Multi-\nmedia Systems , 2013.\n[21] Xavier Serra. Data Gathering for a Culture Speciﬁc\nApproach in MIR. In Proceedings of the 21st Interna-\ntional World Wide Web Conference (WWW): 4th Inter-\nnational Workshop on Advances in Music Information\nResearch (AdMIRe) , Lyon, France, April 17 2012.\n[22] Bo Shao. User-centric music information retrieval .\nPhD thesis, Florida International University, Miami,\nFL, USA, 2011. AAI3472062.\n[23] Beaux Shariﬁ, Mark-Anthony Hutton, and Jugal\nKalita. Summarizing Microblogs Automatically. In\nProceedings of NAACL HLT , June 2010.\n[24] Eva Zangerle, Wolfgang Gassler, and G ¨unther Specht.\nExploiting twitter’s collective knowledge for music\nrecommendations. In Proceedings of the 21st Interna-\ntional World Wide Web Conference (WWW): Making\nSense of Microposts (#MSM2012) , pages 14–17, 2012."
    },
    {
        "title": "Evaluation on Feature Importance for Favorite Song Detection.",
        "author": [
            "Yajie Hu",
            "Dingding Li",
            "Mitsunori Ogihara"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416300",
        "url": "https://doi.org/10.5281/zenodo.1416300",
        "ee": "https://zenodo.org/records/1416300/files/HuLO13.pdf",
        "abstract": "Detecting whether a song is favorite for a user is an important but also challenging task in music recommendation. One of critical steps to do this task is to select important features for the detection. This paper presents two",
        "zenodo_id": 1416300,
        "dblp_key": "conf/ismir/HuLO13",
        "keywords": [
            "detecting",
            "favorite",
            "user",
            "task",
            "music",
            "recommendation",
            "step",
            "select",
            "important",
            "features"
        ],
        "content": "EVALUATION ON FEATURE IMPORTANCE FOR FA VORITE SONG\nDETECTION\nYajie Hu, Dingding Li and Ogihara Mitsunori\nDepartment of Computer Science\nUniversity of Miami\nyajie.hu@umail.miami.edu ,d.wang1@miami.edu\nogihara@cs.miami.edu\nABSTRACT\nDetecting whether a song is favorite for a user is an im-\nportant but also challenging task in music recommenda-\ntion. One of critical steps to do this task is to select im-\nportant features for the detection. This paper presents two\nmethods to evaluate feature importance, in which we com-\npared nine available features based on a large user log in\nthe real world. The set of features includes song metadata,\nacoustic feature, and user preference used by Collaborative\nFiltering techniques. The evaluation methods are designed\nfrom two views: i) the correlation between the estimated\nscores by song similarity in respect of a feature and the\nscores estimated by real play count, ii) feature selection\nmethods over a binary classiﬁcation problem, i.e., “like”\nor “dislike”. The experimental results show the user pref-\nerence is the most important feature and artist similarity is\nof the second importance among these nine features.\n1. INTRODUCTION\nIn the recent digital world, millions of digital songs are\navailable online and it is difﬁcult for users to manually\nsearch favorite songs. A music recommender system is the\nsolution that helps users ﬁnd their possible favorite songs\nin the song ocean, and makes a personalized journey for\nthe user to listen these songs one by one. Essentially, the\nexpected recommender system must be able to answer the\nvery ask, i.e., give me my favorite songs. Automatically\ndetecting favorite songs is therefore an important part in a\nmusic recommender system. Basically, recommender sys-\ntems apply for content-based methods, Collaborative Fil-\ntering (CF) techniques, or both to detect favorite songs.\nAudio content-based methods use favorite songs to pre-\ndict other songs users may like [6], based on their sim-\nilarity to the favorite songs. In order to recommend fa-\nvorite songs to users, a music recommender system using\ncontent-based analysis depends on manual or automatic\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.audio content description to compute the similarity among\nsongs.\nFor instance, Pandora1employs a team of musician an-\nalysts to listen to music and give each recording its descrip-\ntions, which includes melody, harmony, instrumentation,\nrhythm, vocals, lyrics and etc. A weighted Euclidean dis-\ntance is used to ﬁnd similar songs [3].\nZ. Cataltepe et al. music recommender system is based\non audio similarity and users’ listening history [2]. An\nindustrial-strength music recommender system introduces\nthe Euclidean distance of two tracks over a reduced space\nusing Principal Component Analysis [1]. The system uses\nseveral audio features, including Mel-Frequency Cepstral\nCoefﬁcients (MFCC), tempo, key mode and others.\nMusic was one of the ﬁrst forms of content to be ad-\ndressed by collaborative ﬁltering recommender systems such\nas Ringo, Firey and HOMR [9, 10]. This technique ﬁnd-\ns users with similar music preferences and recommends\nitems liked by these similar users to the target user [7]. It\ncould be seen as a method to measure the similarity based\non the user preference. Hence, the user preference is con-\nsidered as one type of feature in our work. Some hybrid\nmethods combined with collaborative ﬁltering techniques\nand content-base methods are proposed to solve the cold-\nstart problem [4, 11]. B. McFee et al. presented a method\nfor deriving item similarity from a sample of collaborative\nﬁlter data, and use the sample similarity to train a distance\nmetric over acoustic features. The trained distance metric\nis used to improve the shortcoming of CF techniques, i.e.,\nincomplete data hamper.\nThis paper focuses on systemic analysis regarding how\nimportant both audio content features and collaborating ﬁl-\ntering techniques are for favorite songs detection in the real\nworld. The two evaluation methods are described in Sec-\ntion 2. In Section 3, we introduce the data and present the\nevaluation results. We make the conclusions and discuss\nthe future work in Section 4.\n2. METHODS\nWe evaluate the importance of a type of feature (In the fol-\nlowing paragraphs, “a feature” means “a type of feature”\ninstead of a value in a feature.) by the correlation between\n1http://www.pandora.com/about/mgpthe predicted scores using this feature and the scores by re-\nal play count. Moreover, the recommendation problem is\nconverted to a binary classiﬁcation problem. As a result,\nthe feature selection results are used to evaluate the feature\nimportance.\n2.1 Correlation Evaluation\nSome content-based recommender systems assume that a\nsong with high similarity to the favorite songs would be\nliked by the user [1, 2]. Thus, the systems measure the\nsimilarity among songs and recommend the song with the\nhighest similarity to the favorite songs. Euclidean distance\nand Dynamic Time Warping (DTW) algorithm are used to\nmeasure the similarity between two songs.\nEuclidean distance sees a vector as a point in Euclidean\nspace, and is given by the Pythagorean formula. This met-\nric works for non-sequential features, like tempo.\nFor sequential feature, like pitch, DTW algorithm is\ngood metric to measure the distance between two sequences,\nwhich may vary in time, since DTW algorithm is able to\nﬁnd the optimal alignment between two time series [8].\nOne time series may be non-linearly “warped” to the other\none by stretching or shrinking it along its time series.\nIf a certain feature is important, the song similarity in\nterms of the feature should be highly effective to detect\nwhether a song is favorite for a user. The highly effective\ndetection is expected to give high relevant rating score, and\nthe predicted rating score should correlate or anti-correlate\nthe true one. Thus, correlation between the predicted s-\ncores and the true scores, which is estimated by play count,\nis therefore used to evaluate the importance of the feature.\nThe correlation result is normalized to [\u00001:0;1:0], which\n-1.0 is in the case of a perfect negative (decreasing) linear\nrelationship, and +1.0 means a perfect positive (increasing)\nlinear relationship. The greater absolute value is, more im-\nportant the feature is.\nIn a user’s log, the predicted rating score is given by the\nfollowing equation.\n^ru\ni=P\nsj2Ru;j6=iru\nj\u0001simf(si;sj)\nP\nsj2Ru;j6=isimf(si;sj); (1)\nwhere ^ru\nidenotes the predicted score for song si, andRu\nis the set of rated songs. Rating score ru\njis estimated by\nthe play count of song sj.simf(si;sj)is the similarity\nbetween song siand songsjin terms of feature f.\nWe apply this approach to predict the rating score for\neach song listened by user u, and get the predicted scores\nf^ru\n1;^ru\n2;:::;^ru\nng. The correlation between fru\n1;ru\n2;:::;ru\nng\nandf^ru\n1;^ru\n2;:::;^ru\nngare used to evaluate the importance of\nfeaturef.\nFurthermore, CF technique predicts the interest of a us-\ner in a song by collecting preferences or taste information\nfrom other users. This technique is based on the assump-\ntion that a user Ais more likely to have a user B’s opinion\non a songxthan to have the opinion on xof a user chosen\nrandomly, if Ahas the same opinion as Bon many songs.\nA user’s preference or taste is represented by the opinion\non the songs the user listened.We consider a user as a document and treat the songs\nwhich the user listened as terms in the document. Then,\nthe user could be represented by a vector of songs using\nTF-IDF values. The cosine distance between two vectors\nare used to measure the similarity between the two corre-\nsponding users.\nThen, we measure the similarity between two songs con-\nsidering user similarity and rating scores by the following\nequation.\nifjUij\u0014jUjj\nsim(si;sj) =\nP\num2Ui\f\f\f\f\u0015max\nun2Ujsim(um;un)2\u0000\u0011\u0002\u0000\nrm\ni\u0000rn\nj\u0001\u000e\nV\u00032\f\f\f\f\njUij;\notherwise\nsim(si;sj) =sim(sj;si);\n(2)\nwhereUiis the set of users who played song si, andum\ndenotes the user m.Vis the rating scale. \u0015and\u0011are two\nregulatory factors.\nThis measurement allows the similarity of two songs\nto be high, if two users have high similar tastes, and rate\nthe two songs at the same score. On the contrary, if two\nusers have high similar tastes, and rate two songs at totally\ndifferent scores, or the two users’ tastes are different but\nthe ratings are similar, the two songs must be different in\nterms of user preference.\nThis type of similarity is also used to predict song rat-\nings by Equation 1, in order to evaluate the importance of\nuser preference.\n2.2 Feature Selection Methods Evaluation\nFurthermore, this problem is about classifying a song to a\nlabel, i.e., “like” or “dislike”, so it is a binary classiﬁcation\nproblem. We compare several feature selection methods,\nand select\u001f2Statistic (Chi), Information Gain (IG), Infor-\nmation Gain Ratio (IG Ratio) and Uncertainty to evaluate\nthe importance of features, respectively.\n\u001f2Statistic is used to investigate how much a feature\ndepends on the category by the following equation.\n\u001f2=(ad\u0000bc)2(a+b+c+d)\n(a+b) (c+d) (b+d) (a+c); (3)\nwherea;b;c anddare the number of instances in different\ncases as shown in Table 1. If the feature is categorical,\n“Data Type” is the category. If the feature is numerical,\n“Data Type” is the data range.\nIG evaluates the importance of a feature from the view\nof information theory. Generally speaking, the expected\nIG is the change in information entropy from a prior state\nto a posterior state that takes some information as given:\nIG (T;f) =H(T)\u0000H(Tjf); (4)Feature Data Type 1 Data Type 2 Total\nCategory 1 a b a+b\nCategory 2 c d c+d\nTotal a+c b+d a+b+c+d=N\nTable 1 . General notation for a 2 x 2 contingency table\nwhere IG (T;f)denotes the IG of the feature fon the\ntraining data T.H(T)is the entropy of the training da-\nta, and H(Tjf)is the average conditional entropy of T.\nIG Ratio introduces Intrinsic Value (IV) to evaluate the\nfeature importance, and it is the ratio between the informa-\ntion gain and the intrinsic value.\nIGR (T;f) = IG (T;f)/IV (T;f); (5)\nwhere IGR (T;f)is the IG Ratio of feature fandIV (T;f)\nis the intrinsic value given by:\nIV (T;f) =\n\u0000X\nvjft2Tjtf=vgj\njTjlog\u0012jft2Tjtf=vgj\njTj\u0013\n(6)\nUncertainty measures the relevance of a feature by cal-\nculating the symmetrical uncertainty with respect to the\nclass.\nUncentainty ( f) = 2\u0001(p(c)\u0000p(cjf))\np(c) +p(f)(7)\nThese four methods evaluate the feature importance from\ndifferent views, and Chi-square statistic and Information\nGain have been proved that they surpass among feature s-\nelection methods in a text classiﬁcation task [5]. We there-\nfore apply each of them to evaluate the feature importance.\n3. ANALYSIS\nAn ideal analysis is expected to run on a large song dataset\nand a big user set. Furthermore, the users should explicitly\nrank every song in the dataset. However, regrading to the\nprivate information protection, the exposed information is\nlimited. This section will compare several dataset, select\none dataset and evaluate several types of features.\n3.1 Dataset\nThere are three possible and available dataset online, i.e.,\nTaste Proﬁle Dataset by the Echo Nest, Last.fm dataset\n360K and Yahoo! Music User Ratings of Songs by Yahoo!\nResearch.\nThe Taste Proﬁle Dataset2is a huge collection of real\nworld anonymous listener data in the form of Echo Nest\nTaste Proﬁles. Considering the protection of individual-\ns private information, the data includes a shufﬂed hash of\npersistent session identiﬁers from a very small random se-\nlection of the musical universe and only play counts associ-\nated with Echo Nest song IDs that overlap with the Million\n2http://labrosa.ee.columbia.edu/millionsong/tasteproﬁle\nFigure 1 . Distribution of users\nSong Dataset (MSD)3. No usernames, listener details, o-\nriginal IDs, dates, IPs, locations or anything but random\nuser string, Echo Nest song ID, and play count are being\nreleased4. The Taste Proﬁle dataset has 1,019,318 unique\nusers, 384,546 unique MSD songs and 48,373,586 <user\nID,song ID ,play count>triplets. The triplets don’t have\nany time stamps and they are not in chronological order.\nHowever, the Echo Nest song ID overlaps with the MSD,\nwhich is a freely-available collection of audio features and\nmetadata for a million contemporary popular music tracks.\nThe MSD therefore enables researchers look into the songs\nby many acoustic features and metadata, it doesn’t provide\nthe audio ﬁle though.\nLast.fm dataset 360K5collects<user,artist ID ,plays>\ntriplets from Last.fm API. Plays has two parts: song ti-\ntleandplay count . The triplets don’t have any time s-\ntamps and not in chronological order. The dataset contains\n359,347 unique users and 294,015 artists and 17,559,530\n<user,artist ID ,plays>triplets. Moreover, more informa-\ntion about users is available, including gender ,age,coun-\ntryanddate (The “date” description is not found). The\nsong metadata and social tags are searchable on Last.fm\nbysong title andartist ID , but the acoustic features are not\navailable on Last.fm.\nYahoo! Music User Ratings of Songs6represents a s-\nnapshot of the Yahoo! Music community’s preferences for\nvarious songs. It contains over 717 million ratings of 136 t-\nhousand songs given by 1.8 million users of Yahoo! Music\nservices. The rating triplet is <user ID ,song ID ,rate>,\nwhich rate is an integer from 1 to 5. All information of\neach song in the dataset is artist, album and genre. The\nusers, songs, artists and albums are represented by random-\nly assigned numeric id’s so there is no identifying informa-\ntion revealed. Consequently, it is impossible to accompany\nmore information to the song.\nThe dataset comparison is summarized in Table 2. Al-\n3http://labrosa.ee.columbia.edu/millionsong/\n4http://blog.echonest.com/post/11992136676/taste-proﬁles-get-\nadded-to-the-million-song-dataset\n5http://mtg.upf.edu/node/1671\n6http://webscope.sandbox.yahoo.com/catalog.php?datatype=rFigure 2 . Distribution of estimated duration\nthough the Taste Proﬁle Dataset doesn’t provide explic-\nit rating values, features and metadata provided by MSD\nsupplies us for the probability of evaluating feature impor-\ntance, and play counts could implicitly represent rating s-\ncores. Hence, we select the Taste Proﬁle Dataset to do the\nexperiment.\nThe distribution of users by the number of songs is shown\nin Figure 1. The user logs, of which the number of played\nsongs are less than three, are removed by the Taste Proﬁle\ndataset. The distribution mainly locates in [10, 1,000], and\nit has a long tail.\nUsers’ preferences would be changed by different con-\ntexts in a long period so the overall listening duration of a\nuser should be taken into account. The Taste Proﬁle dataset\ndoesn’t show the actual listening duration of a song. We\nroughly estimate how long a user listens to the songs in the\nuser log by Equation 8.\nT=iX\ni2Sni\u0001ti\u0001\u0012\n1\u00001\nni+ 1\u0013\n; (8)\nwhereniis the play count of song iby the user. tiis the\nduration of song i, which can be obtained from MSD. The\nestimated duration of playing songs by users is shown in\nFigure 2.\n3.2 Evaluation\nMSD provides many types of features for a song as shown\nin Table 3. We select eight features from the MSD, i.e.,\nloudness ,pitches ,tempo ,duration ,song hotness ,artist sim-\nilarity ,artist hotness andartist familiarity , and these fea-\ntures cover the three categories in Table 3. Furthermore,\nuser preference is counted in. The similarity is able to\nindirectly represent the performance of CF techniques in\nfavorite song detection.\nEuclidean distance is applied to measure the distance\nof songs for non-sequential features, while DTW method\nmeasures the distance of songs for sequential features, like\npitches. DTW has a quadratic time and space complexity\nthat limits its use to only small time series data. Thus, weCategory Features\nAcoustic featuresbars, beats, sections, segment,\nloudness ,pitches , timbre, tatums,\nkey, mode and tempo\nSong metadatasample rate, duration , release,\nsong hotness , title and year\nArtist metadataterms, similar artists ,artist hotness ,\nartist familiarity , artist location,\nartist name and musicbrainz tags\nTable 3 . Features provided by MSD (Bold features are\nselected)\napply FastDTW [8] to accelerate the similarity measure-\nment to O(n) time.\nPlay count is considered as an implicit rating based on\nthe assumption that the rating is positive if the song is\nplayed many times, and vice versa. Considering the bias\nof the ratings, the normalized rating r0\ni;jfrom -1.0 to 1.0 is\ngiven by:\nrj\ni0=rj\ni\u00001\n2\u0000\n\u0016r\u0001\ni+ \u0016rj\n\u0001\u0001\n; (9)\nwhere \u0016r\u0001\niis the average rating of all ratings to song siand\n\u0016rj\n\u0001denotes the average rating by user uj.\nWe evaluate the feature importance by the correlation\nbetween predicted scores and normalized scores as described\nin Section 2.1. As all non-sequential features but user pref-\nerence could represent a song to a vector, they are evaluat-\ned by feature selection methods mentioned in Section 2.2.\nThe numeric value of a feature is assigned into categories\nby predeﬁned window to apply feature selection method-\ns. The normalized rating less than -0.1 is seen as “dislike”\nwhile the song is beloved if the rating greater than 0.1. Be-\ncause the ratings between -0.1 and 0.1 is blur to classify the\nsong to like or dislike, these ratings are ignored. The eval-\nuation result is normalized to [0.0, 1.0] in order to compare\nthe results among different selection methods.\n3.3 Result And Discussion\nThe value of each plot in Figures 3 and 4 is the mean of\nthe results by different users who play the same number\nof songs. Figure 3 shows the correlation between the pre-\ndicted ratings and the normalized ratings by play count in\ntwo views, including nine features. Figure 3(a) shows the\ndistribution of the correlation by the overall played songs.\nIn Figure 3(b), the correlation varies by the distinct songs,\nwhich means that the available information about songs in-\ncreases as the distinct songs increases.\nIn Figure 3, the gray shadow covers the number of played\nsongs by which there are fewer than 30 users so that the\ncorrelation results in the shadow are not statistically reli-\nable. In the remaining part, user preference similarity is\nthe most important feature except at the cold start, name-\nly, CF technique is remarkable for favorite song detection.\nBasically, artist similarity is the secondary important fea-\nture. The other curves ﬂuctuate around 0.0, which meansDataset Taste Proﬁle Dataset Last.fm dataset 360K Yahoo! Music\nNumber of songs 384,546 294,015 artists \u0018136,000\nNumber of users 1,019,318 359,347 \u00181,800,000\nNumber of triplets 48,373,586 17,559,530 \u0018717,000,000\nRating type play count play count rate\nOther information features and metadata provided by MSD metadata provided by Last.fm genre\nTable 2 . Available dataset comparison\n(a)Based on overall songs\n (b)Based on distinct songs\nFigure 3 . Correlation between the predicted ratings and the normalized ratings\n(a)Chi Squared Statistic\n (b)Information Gain\n(c)Information Gain Ratio\n (d)Uncertainty\nFigure 4 . Feature selection results by different feature selection methodsthe corresponding features are not critical for favorite song\ndetection.\nAt the cold start period, it is frustrating that the selected\nfeatures don’t make a great contribution to favorite song\ndetection. As the number of played songs increases, the\nnumber of users reduces, and the ﬂuctuation of curves be-\ncomes vast but the main trend doesn’t change signiﬁcantly.\nThus, in a long playing period, the importance of user pref-\nerence is basically stable.\nWe leverage feature selection methods to measure the\nfeature importance, and the feature selection results are\nnormalized to [0.0, 1.0]. Zero means the feature doesn’t\nplay an important role and one means the feature is cru-\ncial. Figure 4 presents the evaluation results. The gray\nshadow also covers the region that the number of users are\nless than 30.\nWhen the number of played songs is less than 10, most\nfeatures are highly effective to distinguish “like” and “dis-\nlike”. Then, the curves remarkably separate from each oth-\ner. As the play counts increase, artist hotness and artist\nfamiliarity rise and other features descend more or less ex-\ncept Figure 4(c). Referring to Figure 2, the importance of\nfeatures varies by the overall duration. In a long period,\nthe user preferences would be changed by different con-\ntexts. As a result, the importance of tempo, duration, song\nhotness and loudness becomes weak while that of artist fa-\nmiliarity and artist hotness grows. Therefore, artist famil-\niarity and artist hotness are more stable in a long listening\nperiod than other features.\n4. CONCLUSIONS AND FUTURE WORK\nIn this paper, we compare nine features by correlation and\nfeature selection methods. Among these features, user pref-\nerence and artist similarity play important roles in favorite\nsong detection. Artist familiarity and artist hotness are sta-\nble in a long listening period. The evaluation result shows\nthat collaborative ﬁltering technique has high performance\nfor favorite song detection. Song hotness is not as impor-\ntant as people thought.\nIf audio ﬁles of songs are available, we will employ a\nfeature extraction tool to gain more features, and compare\nthem in the future. If more information on the user log is\nexposed, such as time stamps and sequence information,\nthe preference variation model is expected to be analyzed.\n5. REFERENCES\n[1] Pedro Cano, Markus Koppenberger, and Nicolas Wack.\nAn industrial-strength content-based music recommen-\ndation system. In Proceedings of the 28th annual inter-\nnational ACM SIGIR conference on Research and de-\nvelopment in information retrieval , SIGIR ’05, pages\n673–673, 2005.\n[2] Z. Cataltepe and B. Altinel. Music recommendation\nbased on adaptive feature and user grouping. In 22nd\ninternational symposium on Computer and informa-\ntion sciences. , pages 1–6, 2007.[3] Oscar Celma. Music recommendation and discovery,\nthe long tail, long fail, and long play in the digital mu-\nsic space . Springer, Berlin, 2010.\n[4] Justin Donaldson. A hybrid social-acoustic recommen-\ndation system for popular music. In Proceedings of the\n2007 ACM conference on Recommender systems , Rec-\nSys ’07, pages 187–190, 2007.\n[5] George Forman. An extensive empirical study of fea-\nture selection metrics for text classiﬁcation. J. Mach.\nLearn. Res. , 3:1289–1305, March 2003.\n[6] Keiichiro Hoashi, Kazunori Matsumoto, and Naomi I-\nnoue. Personalization of user proﬁles for content-based\nmusic retrieval based on relevance feedback. In Pro-\nceedings of the eleventh ACM international conference\non Multimedia , MULTIMEDIA ’03, pages 110–119,\n2003.\n[7] Ioannis Konstas, Vassilios Stathopoulos, and Joe-\nmon M. Jose. On social networks and collaborative rec-\nommendation. In Proceedings of the 32nd internation-\nal ACM SIGIR conference on Research and develop-\nment in information retrieval , SIGIR ’09, pages 195–\n202, 2009.\n[8] Stan Salvador and Philip Chan. Toward accurate dy-\nnamic time warping in linear time and space. Intelli-\ngent Data Analysis , 11(5):561–580, 2007.\n[9] U. Shardanand. Social information ﬁltering for music\nrecommendation. Master’s thesis, Massachusetts Insti-\ntute of Technology, 1994.\n[10] Upendra Shardanand and Pattie Maes. Social infor-\nmation ﬁltering: algorithms for automating “word of\nmouth”. In Proceedings of the SIGCHI Conference on\nHuman Factors in Computing Systems , CHI ’95, pages\n210–217, 1995.\n[11] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and H. G.\nOkumo. Hybrid collaborative and content-based music\nrecommendation using probabilistic model with laten-\nt user preferences. 12th International Society for Mu-\nsic Information Retrieval Conference , pages 103–108,\n2011."
    },
    {
        "title": "Data Driven and Discriminative Projections for Large-Scale Cover Song Identification.",
        "author": [
            "Eric J. Humphrey",
            "Oriol Nieto",
            "Juan Pablo Bello"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416548",
        "url": "https://doi.org/10.5281/zenodo.1416548",
        "ee": "https://zenodo.org/records/1416548/files/HumphreyNB13.pdf",
        "abstract": "The predominant approach to computing document similarity in web scale applications proceeds by encoding taskspecific invariance in a vectorized representation, such that the relationship between items can be computed efficiently by a simple scoring function, e.g. Euclidean distance. Here, we improve upon previous work in large-scale cover song identification by using data-driven projections at different time-scales to capture local features and embed summary vectors into a semantically organized space. We achieve this by projecting 2D-Fourier Magnitude Coefficients (2DFMCs) of beat-chroma patches into a sparse, high dimensional representation which, due to the shift invariance properties of the Fourier Transform, is similar in principle to convolutional sparse coding. After aggregating these local beat-chroma projections, we apply supervised dimensionality reduction to recover an embedding where distance is useful for cover song retrieval. Evaluating on the Million Song Dataset, we find our method outperforms the current state of the art overall, but significantly so for top-k metrics, which indicate improved usability.",
        "zenodo_id": 1416548,
        "dblp_key": "conf/ismir/HumphreyNB13",
        "keywords": [
            "web scale applications",
            "encoding taskspecific invariance",
            "vectorized representation",
            "efficient scoring function",
            "large-scale cover song identification",
            "data-driven projections",
            "time-scales",
            "local features",
            "semantically organized space",
            "supervised dimensionality reduction"
        ],
        "content": "DATA DRIVEN AND DISCRIMINATIVE PROJECTIONS FOR\nLARGE-SCALE COVER SONG IDENTIFICATION\nEric J. Humphrey, Oriol Nieto, Juan P. Bello\nMusic and Audio Research Laboratory\nNew York University\nfejhumphrey, oriol, jpbello g@nyu.edu\nABSTRACT\nThe predominant approach to computing document sim-\nilarity in web scale applications proceeds by encoding task-\nspeciﬁc invariance in a vectorized representation, such that\nthe relationship between items can be computed efﬁciently\nby a simple scoring function, e.g. Euclidean distance. Here,\nwe improve upon previous work in large-scale cover song\nidentiﬁcation by using data-driven projections at different\ntime-scales to capture local features and embed summary\nvectors into a semantically organized space. We achieve\nthis by projecting 2D-Fourier Magnitude Coefﬁcients (2D-\nFMCs) of beat-chroma patches into a sparse, high dimen-\nsional representation which, due to the shift invariance prop-\nerties of the Fourier Transform, is similar in principle to\nconvolutional sparse coding. After aggregating these local\nbeat-chroma projections, we apply supervised dimension-\nality reduction to recover an embedding where distance is\nuseful for cover song retrieval. Evaluating on the Million\nSong Dataset, we ﬁnd our method outperforms the current\nstate of the art overall, but signiﬁcantly so for top- kmet-\nrics, which indicate improved usability.\n1. INTRODUCTION\nCover song identiﬁcation is a well-established task in the\nMIR community, motivated by both theoretical and practi-\ncal interest. On one hand, a “cover” is an abstract form of\nmusical variation and presents a challenging computer au-\ndition problem. Alternatively, music collections continue\nto expand to unprecedented volumes, particularly in terms\nof amateur and user-generated content. As evidenced by\neven a brief review of websites like YouTube1, Vimeo2,\nor Soundcloud3, a considerable portion of online musical\ncontent now consists of covers.\nIn light of this, previous research in cover song iden-\ntiﬁcation explores a variety of approaches, including the\n1http://youtube.com\n2http://vimeo.com\n3http://soundcloud.com\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.cross-correlation of beat-synchronous chroma features [4],\ndynamic time warping on binary chroma similarities [10],\ncross-recurrence quantiﬁcation [11], etc. For a compre-\nhensive review, the reader is referred to [9]. Over time, it\nhas been shown that these methods can achieve robustness\nto speciﬁc kinds of musical variation, e.g., tempo changes,\ndifferences in structure, or key transpositions. In practice\nhowever, making use of these non-trivial operations yields\ncomplex systems that are computationally prohibitive to\nevaluate, let alone deploy, on large music databases.\nRecognizing this limitation, recent work in cover song\nretrieval explores a slightly different approach to the task\n[1]. Rather than attempting to resolve irrelevant musical\nvariation in the process of comparing two tracks, this par-\nticular system tries to encode this invariance directly with a\nmulti-stage, feed-forward architecture. Local beat-chroma\npatterns are efﬁciently transformed into shift-invariant fea-\ntures via the 2D-Fourier Transform, median-pooled over\ntime into a summary representation, and projected into a\nPCA subspace. Having transformed a collection of tracks\ninto a much lower dimensional space, pairwise compar-\nisons can be efﬁciently computed by Euclidean distance.\nAs a result, this approach scales well to large collections\nlike the Million Song Dataset4(MSD), and offers a promis-\ning research direction for pursuing general web-scale mu-\nsic similarity.\nHere, we seek to advance this initial work by improv-\ning the feed-forward architecture to yield better represen-\ntations for cover song retrieval. After ﬁne-tuning the pre-\nviously developed system, we propose two major modiﬁ-\ncations: sparse, high-dimensional data driven component\nestimation to improve separability, and supervised dimen-\nsionality reduction to recover a cover-similarity space. Our\ninitial analysis on a training subset shows how the com-\nbination of sparse projections and supervised embeddings\ncan lead to better organized spaces and improve cover song\nretrieval. Interestingly, evaluating on the MSD results in\ntwo notable ﬁndings: one, that our approach signiﬁcantly\nimproves performance at top- kmetrics; and two, though\nour supervised embedding can be prone to over-ﬁtting, PCA\nsubspaces help alleviate this issue.\nThe remainder of this paper is organized as follows.\nSection 2 formally motivates and introduces the approach\nin [2] upon which our work is based, while Section 3 de-\n4http://labrosa.ee.columbia.edu/millionsong/tails our proposed modiﬁcations. In Section 4 we present\nresults on the development set and give detailed analy-\nses on the impact of each proposed modiﬁcation. Sec-\ntion 5 discusses results on the MSD and tests strategies\nfor minimizing the generalization error. Finally, Section\n6 draws conclusions and advances a number of ideas for\nfuture work.\n2. SCALABLE COVER SONG RETRIEVAL\n2.1 Problem Formulation\nExpressed symbolically, cover song retrieval proceeds by\ndetermining the relationship Si;jbetween a query Aiand\nreference track Bjvia the composite of feature extraction\nfand a pairwise comparison5functiong:\nSi;j=g(f(Ai);f(Bj)) (1)\nNote then that computing the full comparison matrix Sbe-\ntween a set of Qqueries against a collection of Rreference\ntracks requires a double-for loop, and the total computa-\ntional costCSis expressed as QR\u0016CSi;j, where \u0016CSi;jis the\nexpected cost of computing a single pairwise relationship.\nHowever, when fandgare independent, feature extraction\ncan be performed separately, and the total computational\nload can be re-written as follows:\nCS=QR\u0016Cg+ (Q+R)\u0016Cf (2)\nImportantly, though the average comparison cost \u0016Cgscales\nquadratically, the start-up cost of feature extraction \u0016Cfis\nlinear. The intuition for this trick is a common optimiza-\ntion in software engineering —minimize the amount of\ncomputation inside for-loops— and pinpoints the funda-\nmental deﬁciency of many cover song retrieval systems:\ncomparison functions often rely on expensive operations\nlike cross-correlation or dynamic time warping, which must\nremain inside a nested for-loop. Thus, scalable cover song\nretrieval necessitates choosing an efﬁcient comparison func-\ntiong; the challenge then becomes one of designing the\nfeature extraction stage fso as to maximize the accuracy\nof the rankings according to S.\n2.2 Relating to Previous Work\nTo these ends, the authors of [1] propose an astute solu-\ntion to this challenge. Intuitively, cover song retrieval al-\ngorithms are designed to be invariant to time and key trans-\npositions. One cleverly efﬁcient way of achieving this be-\nhavior is by computing the 2-dimensional Discrete Fourier\nTransform (2D-DFT) of local patches of beat-synchronous\nchroma features and keeping only the magnitude coefﬁ-\ncients. Whereas the phase component of the 2D-DFT en-\ncodes circular rotations in time and pitch class, 2D-Fourier\nMagnitude Coefﬁcients (2D-FMC) capture these patterns\nregardless of absolute position. As shown by [8] in the\ncontext of rhythm analysis, the DFT is sensitive to the or-\nder of events in a sequence, where the addition of different\n5The choice of similarity or distance is only a matter of preference.sinusoids results in patterns of cancellations that affect the\nmagnitude coefﬁcients.\nDescribing holistically, the system presented in [1] de-\nﬁnesfas a feed-forward embedding function that operates\nat multiple time scales. First, 2D-FMC are computed on a\nmoving window of 75 beat-synchronous chroma vectors,\nwith a 1 beat hop size. A track is then pooled over time\nby taking the coefﬁcient-wise median across all 2D-FMC\nvectors and L2-normalized. Having sampled a collection\nof summary 2D-FMC vectors, PCA is performed and used\nto embed tracks in a low dimensional subspace; the au-\nthors experimentally found that preserving anywhere be-\ntween 50 and 200 principal components returns better re-\nsults. Importantly, once tracks are embedded in this fea-\nture space via f, they deﬁne gas Euclidean distance to\nefﬁciently compute pairwise comparisons.\n3. IMPROVING FEATURE EXTRACTION\nStarting from the work presented in [1], we now propose a\nseries of modiﬁcations to make the feature extraction pro-\ncess more robust and improve cover song retrieval, out-\nlined in its entirety in Figure 1. First, we discuss various\ndata pre-processing strategies, including non-linear scal-\ning and normalization. Next, we describe our approach\nto data driven component estimation, addressing its mo-\ntivation and conceptual parallels to recent developments\nin information processing strategies. Lastly, a supervised\nlearning stage is introduced to realize an embedding where\nsummary representations of covers are signiﬁcantly closer.\n3.1 Data Pre-processing\nAs an initial step, here we apply three operations to the\n2D-FMC representation preparing it for further process-\ning: logarithmic compression, vector normalization, and\ndimensionality reduction via PCA. Expressed formally, the\nﬁrst two are achieved by\n^X= log\u0012CX\nkXk2+ 1\u0013\n(3)\nwhereCis a constant hyperparameter, Xis a 2D-FMC\nvector, andk\u0001k 2is theL2-norm. We empirically observed\nthatL2-normalization followed by log-scaling with C= 5\nyields slightly better results than the inverse order with\nC= 100 . Intuitively speaking, while log-compression\nscales all coefﬁcients independently, unit normalization ad-\njusts the dynamic range of each vector relative to the other\ndimensions, and can be viewed as a form of adaptive gain\ncontrol. This contrast adjustment turns out to be quite nec-\nessary, as certain coefﬁcients, e.g., the DC component, are\nprone to dominating the overall representation and unfa-\nvorably biasing distance calculations downstream.\nFinally, PCA is applied for two reasons. First, as we\nwill see, it is important to center the representation such\nthat each coefﬁcient has zero mean. Additionally, we dis-\ncard the redundant components of the Fourier transform,\nreducing the dimensionality from 900 to 450 coefﬁcients.Median Aggregation\nL2-Norm2D-FMCL2-Norm\nDim. ReductionLog-Scale\nSparse CodingShrinkageUnsupervisedSupervisedPCA\nM1275=\nM12 x 75 = 900\nM450\nMk\n1k\n1NFigure 1 . Diagram of the proposed method.\n3.2 Sparse Component Estimation\nPrevious work used 2D-FMCs as a clever way to repre-\nsent meaningful, rotation-invariant beat-chroma patterns.\nWhile this is, strictly speaking, an accurate insight, the\nFourier bases themselves do not necessarily make for good\nfeature extraction. Generally speaking, when the bases of a\nprojection are unlike the data to which it is applied, it is un-\nable to compactly represent this information. As a result,\nthe noise ﬂoor of the resulting representation is higher and\nmost coefﬁcients tend to be active; furthermore, this be-\nhavior becomes especially problematic when pooling fea-\ntures. This observation is quite relevant to this particu-\nlar instance, as the data being projected —non-negative\nchroma— is not sinusoidal.\nAlternatively, data driven transformations learn a set of\nbases6, or a dictionary , from a sampling of data, and are\noften able to encode meaningful behavior with a small num-\nber of active components. This is typically realized by the\ndot-product of an input Xwith a dictionary W, followed\nby an activation function h(\u0001), expressed as follows:\nZ=h(W\u0001X) (4)\nInterestingly, this formulation draws strong parallels to pre-\nvious work both neural networks and sparse coding. Re-\n6These are not strictly bases in the orthogonal, linear algebra sense,\nbut it is a commonly used term in the literature.cent research in these areas has emphasized the importance\nofh(\u0001)being deﬁned as the shrinkage operator, h\u0012(x) =\nsgn(x)\u0003max(kxk\u0000\u0012;0), where\u0012controls the knee or\nthreshold of the function [5]. This non-linearity exhibits\nthe desirable behavior of suppressing low-level activations\nwhile passing sufﬁciently large ones. Such a process in-\nherently leads to sparser outputs, with the rationale being\nthat only the most representative attributes are encoded.\nAdditionally, convolutional variants [7] apply this dic-\ntionary, also referred to as kernels , at all translations over\nan input to achieve shift-invariant feature extraction, given\nby the following:\nZ=h(W~X) (5)\nReusing these kernels at all positions, known as weight\nsharing , results in fewer parameters to learn, reduced over-\nﬁtting, and thus better generalization. Despite these advan-\ntages, convolution is a computationally expensive opera-\ntion. Therefore, to reap the beneﬁts of data driven trans-\nformations while still learning shift-invariant features, we\nleverage the convolution-multiplication duality of the DFT\nand operate on the Fourier magnitude representation:\nZ=h(W~X) =h(W\u0001kF(X)k) (6)\nNote that it is unnecessary to also take the Fourier trans-\nform ofW, as Eq (6) is now equivalent to Eq (4) and the\ndictionary can be learned directly on the pre-processed 2D-\nFMC representation. Additionally, by ﬁrst centering the\ndata, a bias term is unnecessary and both WandZwill\nalso be approximately zero-mean.\n3.3 Semantically Organizing the Space\nHaving designed a transform to project 2D-FMCs into a\nsparse representation, we subsequently pool features over\na track by taking the median over each coefﬁcient. How-\never, while discriminative power can be achieved by pro-\njecting into higher dimensional spaces, it is often necessary\nto recover a lower dimensional embedding where distance\nencodes the desired semantic relationship between vectors,\ni.e. covers are near-neighbors. There are at least two con-\nceptual justiﬁcations motivating an embedding transform.\nFirst, high dimensional representations are known to suffer\nfrom the curse of dimensionality, i.e. distance is not well\nbehaved. Second, and more speciﬁc to this approach, the\ndictionary used in the previous stage is learned as an unsu-\npervised process. As a result, there are no guarantees that\nthe representation it produces provides the latent organiza-\ntion necessary for this task.\nTherefore, using known relationships between songs in\na training set, we can treat covers as distinct classes in\na large, multi-class problem, and apply supervised learn-\ning to recover an embedding that tries to preserve these\nrelationships. The resulting projection can then be used\nto transform unseen data into a cover-similarity space for\ncomputing distances between tracks.4. EXPERIMENTAL DESIGN\n4.1 Methodology\nHaving introduced our main contributions, we now turn\nour attention to a discussion of implementation details and\nexplore various hyperparameters. To quantitatively nav-\nigate this space, we use the training split of the Second\nHand Song (SHS) dataset7for development and save the\ntest split for our ﬁnal evaluation. The SHS is a collection\nof 18,196 tracks from 5,854 “cliques”, or distinct classes,\nwith 12,960 from 4,128, respectively, set aside for training;\nthe remainder constitutes the test set. Importantly, the SHS\nis also a subset of the MSD, which allows for large scale\nevaluation by using the entire MSD as background noise in\na cover song retrieval task.\nIn line with previous work, the primary metrics of in-\nterest here are mean average precision (MAP) and average\nrank (AR). MAP is computed as the mean of the average\nprecision over a set of queries, and reﬂects not only accu-\nracy but also the order of correct documents in a ranked\nlist. As an additional statistic, AR is computed as the av-\nerage position of relevant documents, and measures where\nrelevant documents fall in a ranked list. For evaluating per-\nformance in the training condition, each track in the train-\ning set is treated as a query and ranked relative to the re-\nmaining items in the training set, i.e. 1-vs-12,959; alter-\nnatively, in the test condition, each track in the test set is\ntreated as a query and ranked relative to all other tracks in\nthe MSD, i.e. 1-vs-999,999.\n4.2 Impact of Sparse Projections\nHere, we propose using the k-means algorithm to learn var-\nious dictionaries, inspired by recent work in [3]. While we\nacknowledge that there are alternative methods that could\nbe applied to learn the bases of this transform, k-means\nis particularly attractive being unsupervised and relatively\nsimple, having a single hyperparameter k. Noting that k-\nmeans is a batch, as opposed to on-line, learning algo-\nrithm, we ﬁrst draw 50,000 2D-FMC vectors randomly\nfrom the SHS training set. This subset is used for both\nﬁtting PCA in the pre-processing set as well as learning\ndictionaries for various values of k; at this stage, we con-\nsiderk2[128;512;1024;2048] . It is worth mentioning\nthat due to the nuances of the algorithm —we use the Scipy\nimplementation8— only 2045 elements were returned for\nk= 2048 , as three of the centroids did not change. Addi-\ntionally, after inspecting the data to determine a reasonable\nknee for the shrinkage function, we set \u0012= 0:2for our ex-\nperiments.\nShown in Table 1, we ﬁnd that applying learned k-means\ndictionaries as sparse projections, followed by median pool-\ning andL2-normalization, leads to slightly worse perfor-\nmance than the baseline system. This negative result illus-\ntrates that a sparse, higher dimensional feature space does\nnot necessarily exhibit the organization necessary for dis-\ntance to be meaningful. However, the goal of a sparse pro-\n7http://labrosa.ee.columbia.edu/millionsong/secondhand\n8http://docs.scipy.org/doc/scipy/reference/cluster.vq.htmlk 128 512 1024 2045 Baseline\nMAP 3.44% 4.54% 4.92% 5.51% 8.91%\nAR 3,248 3,154 3,112 3,026 3,097\nTable 1 . Exploring values of kon the Training set.\njection is only to make the information more separable, and\nthis behavior must be explored further to determine its true\nimpact on system performance.\n4.3 Semantically Organizing the Space\nIn light of this, we now seek to better encode semantic re-\nlationships with distance measures. Linear Discriminant\nAnalysis (LDA) is a natural choice for learning a super-\nvised embedding that jointly minimizes intra-class vari-\nance and inter-class discrimination. This approach also has\na single hyperparameter N, the dimensionality of the pro-\njection, and we explore N2[50;100;200].\nAs shown in Table 2, the combination of sparse pro-\njections andsupervised dimensionality reduction leads to\nconsiderably better performance on the training set. While\nthis result says nothing about generalization, it more than\ndemonstrates that the representation produced by project-\ning onto a learned dictionary is indeed signiﬁcantly more\nseparable. It is interesting to note how performance de-\ngrades sharply as a function of decreasing k, and less so\nwith decreasing N. The interpretation of this is two-fold:\none, because the dictionary learning is unsupervised, it re-\nquires an over-complete set of bases to adequately cap-\nture the “right” information for LDA to recover; and two,\nmodel complexity can be constrained by limiting N, and\ntherefore serve as a type of regularization.\nBefore proceeding, it is necessary to ensure that this in-\ncrease in performance is in fact due to the sparse projec-\ntion and not just the supervised embedding. To test this\nhypothesis, we apply LDA to the baseline system, with the\n2D-FMC pre-processing pipeline discussed in Section 3.1.\nTable 3 clearly shows that, though there is some improve-\nment to be had via LDA alone, projecting into a higher di-\nmensional space ﬁrst is indeed signiﬁcant, almost doubling\nMAP as a linear function of N.\nMean Average Precision\nknN 200 100 50\n128 5.34% 4.82% 4.19%\n512 9.30% 7.38% 4.95%\n1024 13.99% 9.63% 5.63%\n2045 28.51% 17.35% 9.05%\nAverage Rank\nknN 200 100 50\n128 2,915 3,116 3,345\n512 2,719 3,153 3,688\n1024 2,420 2,980 3,665\n2045 1,844 2,539 3,249\nTable 2 . Exploring impact of both k-means andLDA on\nthe Training set.Method MAP AR\nBaseline + LDA(50) 5.35% 3,666\nBaseline + LDA(100) 9.85% 3,034\nBaseline + LDA(200) 14.31% 2,434\nk-means(2045) + LDA(50) 9.05% 3,249\nk-means(2045) + LDA(100) 17.35% 2,539\nk-means(2045) + LDA(200) 28.51% 1,844\nTable 3 . Results for the SHS Training set applying LDA to\nthe baseline, versus the best performing sparse projection.\n5. LARGE-SCALE EVALUATION\nSo far, we have focused exclusively on the SHS training\nset, both as a computational simpliﬁcation and an approach\nto system development. We now turn our evaluation to\nthe test split of the SHS dataset to investigate how our ap-\nproach generalizes to unseen data. Based on the results\nof the previous section, we reduce the parameter space by\nﬁxingk= 2045 but continue to observe performance as a\nfunction ofN.\nFirst, evidenced by the results given in Table 4, the com-\nbinedk-means and LDA projections —which we contract\nhere on ask-LDA for brevity— observe radically differ-\nent behavior based on the dimensionality of the embed-\nding. In fact, k-LDA(200), the best performing system on\nthe training set, seemingly fails to generalize at all; MAP\nand AR are over two-times worse than the baseline system,\nand these results clearly indicate extreme over-ﬁtting. Set-\nting this observation aside for a moment though, something\neven more curious occurs with k-LDA(50). While the AR\nis also much worse than baseline, the MAP improves by a\nfactor of 6. This behavior begs an obvious question: what\nis occurring under the surface such that these metrics move\nin drastically different directions?\nOn closer inspection, a rather surprising observation pre-\ncipitates: despite a signiﬁcantly worse AR, the k-LDA(50)\nprojection actually produces a remarkable number of cor-\nrectnearest neighbors, i.e. the top-ranked item in the list\nis an accurate match. This intuitively explains the discrep-\nancy between these metrics, as MAP weights precision as\na function of rank position, e.g. being correct at the top\nmatters more than being correct lower in the list. Further-\nmore, despite pulling relevant tracks to the top of the list,\nthek-LDA(50) system also pushes some to the very bot-\ntom. As a result, the distribution of relevant items in the\nranked list is bimodal, and AR is at a loss to characterize\nthis behavior.\nTo get a better sense of this behavior, we investigate\nprecision-@- k, deﬁned simply as the precision over the\ntop-kitems in a ranked list. Figure 2 clearly illustrates\nhow our proposed method not only yields better perfor-\nmance overall, but offers improved usability as well. For\nthis particular test set, the system gets the top result correct\nnearly 25% of the time, out of a space of one million possi-\nble items. Considering the top 10 results, or approximately\nthe ﬁrst page of a web search, about 5% of the documents\nare correct; in other words, there is a 50% chance that a\ntrue cover will appear on the ﬁrst page of a search.Method MAP AR\nRandom \u00180.001% 500,000\n2DFTM + PCA(50) [1] 1.99% 173,117\n2DFTM + PCA(200) [1] 2.95% 180,304\nk-LDA(50) 13.41% 343,522\nk-LDA(200) 0.83% 398,005\nk-PCA(200) + LDA(200) 12.76% 338,882\nTable 4 . Results for the SHS Test set over the full MSD.\nNote that we contract k-means(2045) here simply as “ k-”.\nTurning back to the k-LDA(200) projection, the ques-\ntion now becomes how to reduce such substantial over-\nﬁtting. Fortunately, projecting into a PCA subspace be-\nfore ﬁtting LDA has been shown to reduce over-ﬁtting in\nthe image processing and pattern recognition communities,\nnotably for face recognition [6]. This is because PCA di-\nmensionality reduction avoids singularities or near singu-\nlarities in any of the scatter matrices used in LDA; this\nproblem is exacerbated for small datasets or high dimen-\nsional feature spaces, of which this application is both.\nFurthermore, the cascade of PCA and LDA has been shown\nto be a general case of other LDA variations like uncorre-\nlated LDA (ULDA), which are also used to avoid the sin-\ngularity issue. Most importantly, how much PCA allevi-\nates LDA over-ﬁtting depends on the dimensionality of the\nintermediate PCA subspace. Therefore, selecting the right\nnumber of principal components is both crucial for good\nresults, and non-trivial.\nIn lieu of a more extensive exploration, we perform an\ninitial inquiry into the potential of PCA to address this par-\nticular problem. Here, we ﬁt a 200 dimensional PCA sub-\nspace by transforming the SHS training set into its mid-\nlevel, 2045-dimensional representation, just before the ap-\nplication of LDA. In an effort to help minimize potential\nsingularities and other such problems, we take two addi-\ntional steps when ﬁtting LDA to encourage better gener-\nalization; however, the true impact of such decisions are\nadmittedly uncertain. First, we subsample the training set,\nonly using cliques with 9 or more tracks each. Then, we\ninclude an arbitrarily large number of tracks that do not\nbelong to any clique. This resulting embedding, dubbed\nk-PCA(200)+LDA(200), is then evaluated on the MSD,\nand we again recover performance roughly on par with k-\nLDA(50), e.g. relatively low AR, but a signiﬁcantly higher\nMAP than baseline.\nFinally, in terms of computation time, our method takes\nthree more times to compute than baseline. In a machine\nwith plenty of RAM, 16 cores, and splitting the process\ninto 10 different threads, the baseline takes 8.7 hours to\ncompute the features of 50, 100 and 200 PCA components.\nHowever, as our method produces features with the same\noutput dimensionality as the baseline, our distance calcu-\nlations —the prohibitive computation— requires the same\namount of time. More speciﬁcally, it takes 0.4, 0.9, and 1.5\nhours using 50, 100, and 200 components respectively.Figure 2 . Comparison of Precision@k on the Test set for\nk-LDA(50), versus the best baseline result.\n6. CONCLUSIONS\nIn this work we have presented an improved system for\nlarge-scale cover song retrieval, demonstrating how sparse,\nhigh-dimensional projections can be combined with low-\ndimensional embeddings to achieve greater performance\nthan either piece alone. This semantically organized space\nis recovered by efﬁciently capturing shift-invariant features\nby effectively performing convolutional sparse coding in\nthe Fourier magnitude domain, and learning a supervised\ncover-similarity space where distance is meaningful. Our\nsystem not only achieves state-of-the-art performance with\nrespect to previously used evaluation metrics (MAP), but\ngreatly improves precision-at- kforkless than 10, indica-\ntive of a more useful system. This encourages the addi-\ntional observation that top- k, as opposed to full-list, met-\nrics may be more informative for characterizing the usabil-\nity of large scale information retrieval systems.\nLooking toward future work, we identify several areas\nwith the potential for improvement. As mentioned, there\nare a variety of ways the sparse dictionary could be learned;\nand, depending on the temporal pooling strategy deﬁned, it\nwould be possible to ﬁne-tune the overall architecture like\na deep network via backpropagation. Additionally, there\nare other pooling strategies that could be employed, lever-\naging structural knowledge to summarize the information\nover a full track in more musically meaningful ways. Lastly,\nthe challenge of realizing a semantically organized space\nfor computing distances between tracks is hardly a solved\nproblem. Over-ﬁtting seems to be a problem in higher di-\nmensions, but the PCA-subspace trick discussed offers en-\ncouraging results, complementing those obtained directly\nfrom low-dimensional LDA.\nFinally, to facilitate reproduction of results and encour-\nage future work, we provide an open source implementa-\ntion of our method in a public repository9.\n7. REFERENCES\n[1] Thierry Bertin-Mahieux and Daniel P. W. Ellis. Large-\nScale Cover Song Recognition Using The 2D Fourier\nTransform Magnitude. In Proc. of the 13th Interna-\n9https://github.com/urinieto/LargeScaleCoverSongIdtional Society for Music Information Retrieval Confer-\nence, pages 241–246, 2012.\n[2] Thierry Bertin-Mahieux, Daniel P. W. Ellis, Brian\nWhitman, and Paul Lamere. The Million Song Dataset.\nInProc of the 12th International Society of Music In-\nformation Retrieval , Miami, FL, USA, 2011.\n[3] Adam Coates and Andrew Y Ng. Learning feature rep-\nresentations with k-means. In Neural Networks: Tricks\nof the Trade , pages 561–580. Springer, 2012.\n[4] Daniel PW Ellis and Graham E Poliner. Identifying\ncover songs’ with chroma features and dynamic pro-\ngramming beat tracking. In Acoustics, Speech and Sig-\nnal Processing, 2007. ICASSP 2007. IEEE Interna-\ntional Conference on , volume 4, pages IV–1429. IEEE,\n2007.\n[5] M. Henaff, K. Jarrett, K. Kavukcuoglu, and Y . LeCun.\nUnsupervised learning of sparse features for scalable\naudio classiﬁcation. In Proc. 12th Int. Conf. on Music\nInformation Retrieval (ISMIR) , 2011.\n[6] Shuiwang Ji and Jieping Ye. Generalized linear dis-\ncriminant analysis: a uniﬁed framework and efﬁcient\nmodel selection. Neural Networks, IEEE Transactions\non, 19(10):1768–1782, 2008.\n[7] Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,\nKarol Gregor, Micha ¨el Mathieu, and Yann LeCun.\nLearning convolutional feature hierachies for visual\nrecognition. In Advances in Neural Information Pro-\ncessing Systems (NIPS) , 2010.\n[8] Geoffroy Peeters. Spectral and temporal periodicity\nrepresentations of rhythm for the automatic classiﬁca-\ntion of music audio signal. Audio, Speech, and Lan-\nguage Processing, IEEE Transactions on , 19(5):1242–\n1252, 2011.\n[9] Joan Serr `a, Emilia G ´omez, and Perfecto Herrera. Au-\ndio cover song identiﬁcation and similarity: back-\nground, approaches, evaluation, and beyond. In Ad-\nvances in Music Information Retrieval , pages 307–332.\nSpringer, 2010.\n[10] Joan Serr `a, Emilia G ´omez, Perfecto Herrera, and\nXavier Serra. Chroma binary similarity and local\nalignment applied to cover song identiﬁcation. Audio,\nSpeech, and Language Processing, IEEE Transactions\non, 16(6):1138–1151, 2008.\n[11] Joan Serr `a, Xavier Serra, and Ralph G Andrzejak.\nCross recurrence quantiﬁcation for cover song identi-\nﬁcation. New Journal of Physics , 11(9):093017, 2009."
    },
    {
        "title": "Motif Spotting in an Alapana in Carnatic Music.",
        "author": [
            "Vignesh Ishwar",
            "Shrey Dutta",
            "Ashwin Bellur",
            "Hema A. Murthy"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416332",
        "url": "https://doi.org/10.5281/zenodo.1416332",
        "ee": "https://zenodo.org/records/1416332/files/IshwarDBM13.pdf",
        "abstract": "This work addresses the problem of melodic motif spotting, given a query, in Carnatic music. Melody in Carnatic music is based on the concept of raga. Melodic motifs are signature phrases which give a raga its identity. They are also the fundamental units that enable extempore elaborations of a raga. In this paper, an attempt is made to spot typical melodic motifs of a raga queried in a musical piece using a two pass dynamic programming approach, with pitch as the basic feature. In the first pass, the rough longest common subsequence (RLCS) matching is performed between the saddle points of the pitch contours of the reference motif and the musical piece. These saddle points corresponding to quasi-stationary points of the motifs, are relevant entities of the raga. Multiple sequences are identified in this step, not all of which correspond to the the motif that is queried. To reduce the false alarms, in the second pass a fine search using RLCS is performed between the continuous pitch contours of the reference motif and the subsequences obtained in the first pass. The proposed methodology is validated by testing on Alapanas of 20 different musicians.",
        "zenodo_id": 1416332,
        "dblp_key": "conf/ismir/IshwarDBM13",
        "keywords": [
            "melodic motif spotting",
            "Carnatic music",
            "raga",
            "signature phrases",
            "extempore elaborations",
            "basic units",
            "query",
            "musical piece",
            "two pass dynamic programming approach",
            "pitch as the basic feature"
        ],
        "content": "MOTIFSPOTTING INAN ALAPANA INCARNATICMUSIC\nVigneshIshwar\nDept. of\nComputer Sci. & Engg.\nIITMadras, India\nishwar@cse.iitm.ac.inShreyDutta\nDept. of\nComputer Sci. & Engg.\nIITMadras, India\nshrey@cse.iitm.ac.inAshwinBellur\nDept. of\nElectricalEngg.\nIITMadras, India\nashwin@ee.iitm.ac.inHema AMurthy\nDept. of\nComputer Sci. & Engg.\nIIT Madras, India\nhema@cse.iitm.ac.in\nABSTRACT\nThis work addresses the problem of melodic motif spot-\nting, given a query, in Carnatic music. Melody in Car-\nnatic music is based on the concept of raga. Melodic mo-\ntifs are signature phrases which give a ragaits identity.\nThey are also the fundamental units that enable extem-\npore elaborations of a raga. In this paper, an attempt is\nmade to spot typicalmelodicmotifsof a ragaqueriedin a\nmusical piece using a two pass dynamicprogrammingap-\nproach,withpitchasthebasicfeature. Intheﬁrstpass,the\nrough longest common subsequence (RLCS) matching is\nperformedbetween the saddle pointsof the pitch contours\nofthereferencemotifandthemusicalpiece. Thesesaddle\npoints correspondingto quasi-stationarypointsof the mo-\ntifs, are relevant entities of the raga. Multiple sequences\nare identiﬁed in this step, not all of which correspond to\nthe themotifthat isqueried. To reducethefalse alarms,in\nthesecondpassaﬁnesearchusingRLCSisperformedbe-\ntweenthecontinuouspitchcontoursofthereferencemotif\nand the subsequences obtained in the ﬁrst pass. The pro-\nposed methodologyis validated by testing on Alapanas of\n20differentmusicians.\n1. INTRODUCTION\nCarnatic music is a sub-genre of Indian classical music\nprominent in south India. It is a heterophonic musical\nform which involves multiple instruments performing at\nthe same time along with the voice. This form of music\nis highly melody centric and thrives on the melodic con-\ncept ofragas. Theragasin Carnatic music consist of a\nset of inﬂected musical notes called svaras.Svarasare\nthe Indian classical equivalent to the solfege and are an-\nnotated as Sa Ri Ga Ma Pa Da Ni Sa . Theoretically fre-\nquencies of svarasfollow more or less the just intonation\nratios unlike the notes in western classical music which\nfollow the equi-temperament scale [16]. In Carnatic mu-\nsic,thesvarasareseldomrenderedasdiscretenotes. They\narerenderedasaseamlessmeanderingacrossthenotesus-\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2013 International Society for MusicInformation Retrieva l.−1500 −1000 −500 0 500 1000 1500 2000 2500050010001500Shankarabharana Pitch Histogram\n−1500 −1000 −500 0 500 1000 1500 2000 250001000200030004000Kalyani Pitch Histogram\nFrequency in CentsSaR2G3\nM1P\nD2N3Sa\nSa\nR2G3\nM2P\nD2N3Sa\nFigure 1 . Pitch Histograms of Ragas Kalyani and\nShankarabharana\ninggamakas1[7,9,17]. Figure1showsa pitchhistogram\nof tworagasnamely,KalyaniandSankarabharana . Ob-\nservethat thereis a signiﬁcantbandoffrequenciesaround\neverypeakthatarealsofrequent. Thisbandoffrequencies\ncorrespondto a single svaraand is referred to as the into-\nnation of that svara. Thischaracteristic is observeddue to\nthe inﬂected nature of the svaras. It has been conjectured\nby musicians that a quantization of the pitch on the basis\noftheabsolutefrequenciesofthe svarasiserroneous[7].\nAragain Carnatic music can be characterised by a\nset of distinctive motifs. Distinctive motifs can be char-\nacterised by the trajectory of inﬂected svarasover time.\nThese motifs are of utmost aesthetic importance to the\nraga. Carnatic music is a genre abundant with compo-\nsitions. These compositions are replete with many dis-\ntinctive motifs. These motifs are used as building blocks\nfor extempore improvisational pieces in Carnatic music.\nThese motifs can also be used for distinguishing between\ntworagas, and also for archival and learning purposes.\nThe objective of this paper is to spot the location of the\n1Gamakais a meandering of a note encompassing other permissible\nfrequencies around it.distinctive motifs in an extempore enunciation of a raga\ncalled the Alapana. For more details on the concepts of\nsvara, gamaka, phraseology pertainingto Carnatic music,\nthereaderisadvisedtoreferto[7].\nIn this paper, pitch is used as the main feature for\nthe task of motif spotting. Substantial research exists on\nanalysingdifferentaspectsofCarnaticmusiccomputation -\nally,usingpitchasafeature. Krishnaswamyetal[8],char-\nacterize and analyse gamakas using pitch contours. Serra\net al [16] study the tuning of Indian classical music us-\ning pitch histograms. M. Subramaniam [17] has exten-\nsivelystudiedthe motifsin the ragaThodiusingpitchhis-\ntograms and pitch contours. All of the above prove the\nrelevance and importance of pitch as a feature for compu-\ntationalanalysisofCarnaticmusic.\nInthepreviouswork,theuniquenessofthecharacteris-\ntic motifs was established using a closed set motif recog-\nnition experimentusingHMMs. As a continuation,in this\nwork an attempt is made to spot motifs given a long Ala-\npanainterspersed with motifs. Time series motif recogni-\ntionhasbeenattemptedforHindustanimusic. J.C.Rosset.\nal.[13]usetheonsetpointoftherhythmiccycle2empha-\nsized by the beat of the tabla (an Indianpercussion instru-\nment)asacueforpotentialmotifregions. Inanotherwork,\nJ.C.Ross et. al.[14]attemptmotifspottingina Bandish(a\ntype of compositionin Hindustani music) using elongated\nnotes(nyaassvara ).\nSpottingmotifsin a raga alapana is equivalentto ﬁnd-\ning a subsequence in trajectory space. Interestingly, the\ndurationofthesemotifsmayvary,buttherelativeduration\nof thesvarasis preservedacrossthe motif. The attempt in\nthis work is to use pitch contoursas a time series and em-\nploytimeseriespatterncapturingtechniquestoidentifyt he\nmotif. The techniquesare customizedusing the properties\nofthemusic. Therehasbeenworkdoneontimeseriesmo-\ntif recognitionin ﬁelds other than music. Pranav et. al. in\ntheirwork[11]deﬁneatimeseriesmotifandattemptmotif\ndiscoveryusing the EMMA algorithm. In [3,18],time se-\nries motifs are discovered adapting the random projection\nalgorithmbyBuhlerandTompatotimeseriesdata. In[2],\nanewwarpingdistancecalledSpatialAssemblingdistance\nisdeﬁnedandusedforpatternmatchinginstreamingdata.\nIn the work of Hwei-Jen Lin et. al. [10], music matching\nis attempted using a variant of the Longest CommonSub-\nsequence(LCS)algorithmcalledRoughLongestCommon\nSubsequence. Thispaperattemptssimilartimeseriesmotif\nmatching for Carnatic Music. Searching for a 2-3 second\nmotif (in terms of a pitch contour) in a 10 min Alapana\n(alsorepresentedasapitchcontour)canbeerroneous,ow-\ningpitch estimationerrors. To addressthisissue, thepitc h\ncontour of the Alapanais ﬁrst quantized to a sequence of\nquasistationarypointswhicharemeaningfulinthecontext\nofaraga. Atwo-passsearchisperformedtodeterminethe\nlocation of the motif. In the ﬁrst pass, a Rough Longest\nCommonSubsequenceapproachis used to ﬁnd the region\ncorrespondingtothelocationofthemotif. Oncetheregion\n2TherhythmiccycleinIndian Musiciscalled Talaandtheonse tpoint\nis called the Sam.islocated,anotherpassismadeusingaﬁne-grainedRLCS\nalgorithmusingtherawpitchcontour.\nThe paper is organised as follows. Section 2 discusses\nthe approach employed to extract the stationary points in\naraga. Section 3 discusses the algorithm to perform the\ntwo-level RLCS approach to spot the motif. In Section\n3.1 the Rough Longest Common Subsequence(RLCS) al-\ngorithmisdiscussed. InSection4,thedatabaseusedinthe\nstudyisdiscussed. Section5discussestheresults. Finall y,\nconclusionsarepresentedinSection6.\n2. SADDLEPOINTS\n2.1 Saddle Points: Reducingthe SearchSpace\n0 1 2 3 4−1000010002000\nTime in MinutesFrequency in CentsPitch Contour of an Alapana\n0 0.5 1600800100012001400\nTime in SecondsFrequency in CentsMotif1\n0 0.5 1600800100012001400\nTime in SecondsFrequency in CentsMotif2\n0 0.5 1600800100012001400\nTime in SecondsFrequency in CentsMotif3\nFigure 2. a) Motifs Interspersed in an Alapana ; b) Mag-\nniﬁedMotif\nThe task of this paper is to attempt automatic spotting\nofamotifthatisqueried. Themotifisqueriedagainstaset\nofAlapanas of a particular ragato obtain locations of the\noccurrences of the motif. The task is non-trivial since in\nAlapanas ,rhythmisnotmaintainedbyapercussioninstru-\nment. Figure 2 (a) shows repetitive occurrences of motifs\ninapieceofmusic. Anenlargedviewofthemotifisgiven\nin Figure 2(b). Since the Alapanais much longerthan the\nmotif,searchingforamotifinan Alapanaislikesearching\nfor a needle in a haystack. After an analysis of the pitch\ncontours and discussions with professional musicians, it\nwas conjecturedthat the pitch contourcan be quantizedat\nsaddle points. Figure 3 shows an example phrase of the\nragaKambojiwiththe saddlepointshighlighted.\nMusically,thesaddlepointsareameasureofthe extent\nto which a particular svarais intoned. In Carnatic music\nsincesvarasare rendered with gamakas, there is a differ-\nence between the notation and the actual rendition of the\nphrase. However, there is a one to one correspondence\nwith thesaddle pointfrequenciesandwhatis actuallyren-\nderedbythemusician(Figure3). Figure4showsthepitch\nhistogramandthesaddlepointhistogramofan Alapanaof\ntheraga Kamboji . The similarity between the two pitch\nhistograms indicates our conjecture that saddle points are\nimportant.0 20 40 60 80 100 120 140 160−2000200400600800100012001400Motif with Saddle Points\nFrame IndexFrequency in Cents\n  \nPitch Contour\nSlope of PitchPoints of Zero Crossing\nof Derivative\nFigure3. A PhrasewithSaddlePoints\n−1500 −1000 −500 0 500 1000 1500 2000 2500020040060080010001200140016001800\nFrequency in CentsPitch Histogram of Raga Kamboji\n−1500 −1000 −500 0 500 1000 1500 2000 2500050100150200250300\nFrequency in CentsSaddle Point Histogram of Raga Kamboji\nFigure 4. The Pitch and Saddle Point Histograms of the\nragaKamboji\n2.2 MethodofobtainingSaddlePoints\n0 20 40 60 80 100 120 1406008001000120014001600\nFrame IndexFrequency in CentsRaw Pitch Contour\n  \n0 20 40 60 80 100 120 140600800100012001400\nFrame IndexFrequency in CentsCubic Hermite Interpolated Pitch Contour\n  Pitch Contour\nSaddle Points\nPitch Contour\nSaddle Points\nFigure5. DistortedandCubicInterpolatedpitchcontours\nCarnatic music is a heterophonic musical form. In a\nCarnatic music concert,a minimumof two accompanying\ninstrumentsplaysimultaneouslyalongwiththeleadartist .\nThese are the violin and the mridangam (a percussion in-\nstrument in Carnatic music). Carnatic music is performed\nat a constant tonic [1] to which all instruments are tuned.This tonic is chosen by the lead artist and is provided by\nan instrumentcalled the Tambura. Thus, the simultaneous\nperformance of many instruments in addition to the voice\nrenders pitch extraction of the predominant voice a tough\ntask. This leads to octave errorsand other erroneouspitch\nvalues. For this task it is necessary that pitch be continu-\nous. After experimenting with various pitch algorithms,\nit was observed that the Melodia-Pitch Extraction algo-\nrithm [15] produced the fewest errors. This was veriﬁed\nafter re-synthesis using the pitch contours. In case of an\noctave error or any other such pitch related anomaly, the\nalgorithm replaces the erroneous pitch values with zeros.\nThesaddlepointsareobtainedbyprocessingthepitchcon-\ntour extracted from the waveform. The pitch extracted is\nconverted to the cent scale using Equation 1 to normalise\nwith respectto thetonicofdifferentmusicians.\ncentFrequency = 1200·log2/parenleftbiggf\ntonic/parenrightbigg\n(1)\nLeast squares ﬁt (LSF) [12] was used to compute the\nslopeofthepitchextracted. Thezerocrossingsoftheslope\ncorrespond to the saddle points (Figure 3). A Cubic Her-\nmite interpolation [4] was then performed with the initial\nestimationof saddlepointsto geta continuouscurve(Fig-\nure 5). The saddle points are then obtained by sampling\nthe interpolated spline using LSF. The interpolated pitch\ncontours were validated by re-synthesizing and listening\ntests3.\n3. ATWO PASS DYNAMIC PROGRAMMING\nSEARCH\nIn Section 2 it is illustrated that the sequence of saddle\npoints are crucial for a motif. Therefore, RLCS is used\ntoqueryforthesaddlepointsofthegivenmotifinthe Ala-\npana.\n0 20 40 60 80 100 120 140 1606008001000120014001600\nFrame IndexFrequency in CentsReference Phrase with Saddle Points\n  \n0 20 40 60 80 100 120 140 160 180 200050010001500\nFrame IndexFrequency in CentsTest Phrase with Saddle Points\n  \nPitch Contour\nSaddle pointsPitch Contour\nSaddle Points\nFigure6. SimilarSaddlePoints,differentcontours\nMusic matching using LCS methods for western mu-\nsic is performedon symbolic music data [5]. The musical\n3Original and re-synthesized waveforms are available at\nhttp://lantana.tenet.res.in/motif_analysis.htmlnotesinthiscontextarethesymbols. However,inthecon-\ntext of Carnatic music, a one to one correspondence be-\ntweenthenotationandsungmelodydoesnotexist. Hence,\nin this paper, saddle points are used instead of a symbolic\nnotation. Onemustkeepinmindthatsaddlepointsarenot\nsymbols but are continuouspitch values (Figure 6). In or-\nder to match such pitch values, a rough match instead of\nan exact match is required. A variant of the LCS known\nas the Rough Longest Common Subsequence [10] allows\nsucha roughmatch.\nIn this paper, a two pass RLCS matching is performed.\nIn the ﬁrst pass, the saddle points of the reference and\nquery are matched to obtain the candidate motif regions.\nNevertheless, given two saddle points, the pitch contour\nbetween two saddle points can be signiﬁcantly different\nfor different phrases (Figure 6). This leads to many false\nalarms. A second pass of RLCS is then performed on the\nregions obtained from the ﬁrst pass to ﬁlter out the false\nalarmsfromthetruemotifs.\n3.1 Algorithmforthe RoughLongestCommon\nSubsequence\nRLCS is a variantofLCS whichperformsanapproximate\nmatching between a query and a reference retaining local\nsimilarity. Here,acubicdistancemeasureisusedtodeter-\nmine the similarity between two saddle points. If the dis-\ntancebetweenthetwosaddlepointsinthequeryandrefer-\nenceislessthanathreshold, Td,theyaresaidtoberoughly\nsimilar. In the RLCS algorithm, unlike LCS, the simi-\nlarity measure (cost) is incremented by a weighted quan-\ntity (between 0 and 1) depending on the proximity of the\npoints. Toaccountforthelocalsimilarity,thewidthacros s\nquery (WAQ) and width across reference (WAR) are in-\ncorporated. WAR and WAQ are the lengthsof the shortest\nsub-strings,in the referenceand queryrespectively,whic h\ncontainthelongestcommonsubsequence. Thesemeasures\nconvey the density of the match given a query and refer-\nence. Thus, lesser the WAQ and WAR, denser is the dis-\ntribution of the RLCS and better is the alignment. The\nRLCS algorithm gives ﬁve matrices, namely, cost matrix,\nWAQ matrix,WARmatrix,scorematrixanddirectionma-\ntrix(fortracingbackthe commonsubsequences).\n3.2 FirstPass: DeterminingCandidateMotifRegions\nusing RLCS\nTheRLCSalgorithmusedinthispaperisillustratedinthis\nsection. The Alapanaisﬁrstwindowedandthenprocessed\nwiththeRLCSalgorithm. Thewindowsizechosenforthis\ntask is 1.5 times the length of the motif queried for. The\nmatrices obtained from the RLCS are then processed as\nfollows:\n•Fromthecellsofthescorematrixwithvaluesgreater\nthan a threshold, seqFilterTd , sequences are ob-\ntainedbytracingthedirectionmatrixbackwards.\n•Theduplicatesequenceswhichmaybe acquiredare\nneglected, preserving unique sequences of lengthgreater than a percentage, ρ, of the length of refer-\nence. Theseare thenaddedto asequencebuffer.\n•Thisprocessisrepeatedforeverywindow. Thewin-\ndowis shiftedbya hopofonesaddlepoint.\n•Thesequencesobtainedthusaregrouped.\n•Each group, taken from the ﬁrst element of the ﬁrst\nmemberto the last element of the last member, rep-\nresentsa potentialmotifregion.\n3.3 SecondPass: DeterminingMotifsfromthe\nGroups\nIn the ﬁrst pass a matching of only the saddle points is\nperformed. As mentioned above, eventhough the saddle\npoints are matched it is not necessary that the trajectory\nbetween them match. This leads to a large number of\nfalse alarms. Now that the search space is reduced, the\nRLCS is performed between the entire pitch contour of\nthe potential motif region obtained in the ﬁrst pass and\nthe motif queried. The entire pitch contour is used in or-\nder to account for the trajectory information contained in\nthe phrases (Figure 7). The threshold Tdused for the ﬁrst\npass is tightenedin this iterationforbetter precisionwhi le\nmatchingtheentirefeaturevector. Inthisiteration,thec ell\nof the score matrix having the maximum value is chosen\nand the sequence is traced back using the directionmatrix\nfromthiscell. Thissequenceishypothesizedtobethemo-\ntif. The database and experimentation are detailed in the\nfollowingsections.\n0 50 100 150 200020040060080010001200Diagonal\n0 50 100 150 2006007008009001000110012001300RLCS Match Enlarged\n020040060080010001200\n500 1000 1500Candidate Motif Region\n0 50 100 150 20050010001500Query\nFigure7. RLCS Matching\n4. DATABASE\nTable 1 givesthe detailsof the database used in this work.\nAs mentioned above, this task will be performed on Ala-\npanas. Thereasonforusingonly Alapanasandnotcompo-\nsitionsforspottingisduetothelimitationsofthepitchex -\ntraction algorithms. The presence of multiple instruments\nandpercussionalongwiththevoicemakespitchextraction\na non trivial task due to which pitch obtained for compo-\nsitions is distorted. Pitch extracted for Alapanas have less\ndistortionascomparedto thatofcompositions.Table 1.Database of Alapanas; N-Al- No. of Alapanas , N-\nArt- No. of Artists, Avg-Dur- Average Duration, Tot-Dur -To tal\nDuration\nR¯ agaNameN-AlN-ArtAvg-Dur(mins) Tot-Dur(mins)\nKamboji 2712 9.73 262.91\nBhairavi 2115 10.30 216.49\nTable 2.Phrases Queried\nRagaName PhraseNotation AverageDuration(seconds)\nKamboji S..N2D2P D2... 1.8837\nBhairavi R2 G2M1P D1P.. 1.3213\nThe details of motifs of the ragas queried are given in\nTable2. Theaveragedurationisobtainedfromtheground\ntruthlabeledin thepreviouswork[6].\n5. EXPERIMENTS AND RESULTS\nRLCS was performed on the database Alapanas querying\nfor the motifs of the ragas mentionedin Table 1. The dis-\ntance function used for RLCS is cubic in nature with the\nequationgivenbelow.\nδ(i,j) =/braceleftBigg\n|i−j|3\n3003;if|i−j| ≤300\n1 ; otherwise(2)\nHere,iandjcorrespond to the ithsaddle point in the\nalapanathatmatchesthe jthsaddlepointinthemotif. Due\ntodifferentstylesofvariousmusicians,anexactmatchbe-\ntweenthesaddlepointsofthemotif,andthe Alapanacan-\nnot be expected. Hence in this paper a leeway of extrTd\n= 300 cents is allowed between two saddle points. Musi-\ncally two points 300 cents (3 semitones) apart cannot be\ncalledsimilar,butinthiscase,duetothedifferentstyles of\nsingingandartefactsintroducedduetopitchextraction,t he\nsaddle points for the same phrase sung by different artists\ndoes not match exactly but is generally within a limited\nrangeoffrequencies. Theupperlimitofthisrangeissetto\n300centsempirically.\nIn this work, the phrases sung across octaves are ig-\nnored. For this experimentthe parametersset were as fol-\nlows:Td=0.45;ρ=0.8;seqFilterTd =0.45. Theparame-\nter0< ρ <1isa userdeﬁnedparameterthat ensuresthat\nρ×length of the query motif is matched with that of the\nAlapana. As a sanity check,the regionsobtainedfromthe\nRLCS were veriﬁed with the ground truth motifs labelled\nfor the experiment in the previous work [6]. The param-\neters were tuned to retrieve as many labelled groundtruth\nmotifs as possible for the raga Kamboji ’s phrase. A high\npercentage of regions coinciding with the ground truth la-\nbelled,wereretrievedbytheRLCS4.\nThe regions obtained from the RLCS were then sub-\njected to a listening test performed by three professional\n4In the previous experiment not all occurrences of the phrase were\nlabelled. The instrumental phrases were ignored and some we re missed\ndue to manual error.musicians. Theregionswhichcontainedthephrasequeried\nweremarkedastrue,andthosewhichdidnotasfalse. Itis\nobserved that the correlation with respect to the carefully\nmarked groundtruth by one musician with veriﬁcation by\ntwo other musiciansis an average of 0.833. The details of\nthe number of ground truth motifs retrieved and the total\nnumberoftruesretrievedafterveriﬁcationbythelistenin g\ntestaregiveninTable3. Thenumberoffalsepositives,re-\ntrievedarehoweversubstantial. Thisisaffordablesincet he\nobjectiveintheﬁrstpassistoobtainthemaximumnumber\nof the regionssimilar to the motif. Theseconditerationof\nRLCS is performed to ﬁlter out the false positives. Now\nTable3.RetrievedregionsFirstPass: TR-TotalRetrieved,LGT-\nLabelled Ground Truth, TrR-True Retrieved, PR-Percentage Re-\ntrieved\nR¯ agaNameNo ofApalanas TRLGTTrRPR\nKamboji 27 719705882.86%\nBhairavi 20 4741039188.35%\nthatthecandidatemotifregionsareknown,thesecondpass\nof RLCS is conducted wherein the same motif from four\ndifferent artists are queried in the regions retrieved by th e\nﬁrst pass. The entire pitch contour of the query and refer-\nenceareusedforthistaskinordertoaccountfortheinfor-\nmation of trajectory of pitches between the saddle points.\nSincethetaskistolocatethemotifinasmallercontinuous\nsearch space, the threshold extrTdwas tightened and the\nallowable leeway for the distance functionwas reduced to\n200 cents (See Eq 2). The parameter seqFilterTd is not\nused in the second pass since the best match with the can-\ndidate region is sought (See Section 3.2). The RLCS is\nrepeated for four examples of the same motif by different\nmusiciansandthescoresareobtained.\n5.1 Evaluationand Discussion\nThis work illustrates the method of spotting a querymotif\ninanimprovisationalformofCarnaticmusiccalledan ala-\npana. Therequirementofaquerymotifforsuchasearchis\nduetothescarcerenditionofcertaincharacteristicphras es\nin analapana. The spotting of such phrases proves to be\nuseful to musiciansandstudentsforanalysispurposes. To\nquantifythisandevaluatetheperformanceofthealgorithm\nin this context, it was decided to compute the recall, pre-\ncision and F2-Measure for the motifs retrieved. The F2-\nMeasure was chosen in order to give a higher weightage\nto the recall. The objective of this work being music ex-\nploration through motif spotting, the recall of the motif\nqueriedisofgreaterimportance.\nThe hits obtained in the second pass are sorted accord-\ning to the RLCS scores. The precision, recall and F2-\nMeasure per Alapana are calculated and the average is\ncomputed across all Alapanas . The motifs are not exact\nsince they correspondto the extempore enunciation by an\nartist. Theresultsarereportedperquery,per alapana. The\nhits of the RLCS in an alapanaare sorted according to\ntheir scores and the precision, recall and F2-Measure are\ncalculated for the top 10 sorted hits. The relevant motifsare all the motifs which were marked as true in that ala-\npana. The results are illustrated in Table 4. The experi-\nments on the phrase of the raga Kamboji were treated as\nthe development set, with parameters optimised to max-\nimise the match. To verify that this works in general, an-\notherraga Bhairavi was taken up for study. The same pa-\nrametersusedforthe ragaKamboji wereusedforthe raga\nBhairavi. The results are described in the Tables 3 and\n4. From the results obtained above, it is clear that even-\nTable 4.Results: Average Precision - Pr%, Recall - Rec%, F2-\nMeasure - F2M% across alapanas, Average of Query Scores -\nAQS\nQueries Kamboji-27Alapanas Bhairavi-20Alapanas\nPrRecF2M PrRecF2M\nQuery1 43.1881.5264.1943.1394.8071.94\nQuery2 33.3363.318 50.5038.1585.1064.33\nQuery3 43.6383.1665.2836.8781.0461.04\nQuery4 25.9058.6242.8538.1283.3363.07\nAQS 40.4576.0060.0041.2591.0468.91\nthoughtheprecisionislow,therecallishighinmostofthe\ncases. Certain partial matches are also obtained where ei-\nther the ﬁrst part of the queryis matched or the end of the\nquery is matched. These are movements similar to those\nof the phrases and are interesting for a listener, learner,\nor researcher. High scores were obtained for certain false\nalarms. Thisisprimarilyduetosomesigniﬁcantsimilarity\nbetweenthefalse alarmandtheoriginalphrase.\n6. CONCLUSION\nInthiswork,RLCSisusedformotifdiscoveryin alapanas\nin Carnaticmusic. It is illustratedthat the saddlepointso f\nthepitchcontourofamusicalpieceholdsigniﬁcantmusic\ninformation. Itisthenshownthatquantizingthepitchcon-\ntour of the alapanaat the saddle points leadsto no loss of\ninformationwhileitresultsinasigniﬁcantreductioninth e\nsearch space. The RLCS method is shown to give a high\nrecall for the motif queried. Given that the objective is to\nexplorethe musical traits of a ragaby spotting interesting\nmelodicmotifsrenderedbyvariousartists,therecallofth e\nmotif queried is of higher importance than the precision.\nThe future work would involve spotting of motifs occur-\nring across multiple octaves. It would also be interesting\nseeifsimilaritymeasurescanbeobtainedbythisapproach\nacrossragas.\n7. ACKNOWLEDGEMENTS\nThisresearchwaspartlyfundedbytheEuropeanResearchCou n-\ncil under the European Union’s Seventh Framework Program, a s\npartoftheCompMusic project (ERCgrantagreement 267583).\n8. REFERENCES\n[1] Ashwin Bellur and Hema A Murthy. A cepstrum based ap-\nproach for identifying tonic pitch in indian classical musi c.\nInNational Conference onCommunications , 2013.\n[2] Yueguo Chen, Mario A. Nascimento, Beng Chin, Ooi An-\nthony, andK.H.Tung.Spade: Onshape-based patterndetec-\ntioninstreamingtimeseries. inICDE,2007 , pages 786–795,\n2007.[3] BillChiu,EamonnKeogh,andStefanoLonardi. Probabilistic\ndiscovery of timeseries motifs . 2003.\n[4] F. N. Fritsch and R. E. Carlson. Monotone Piecewise Cubic\nInterpolation .SIAMJournalonNumericalAnalysis,Vol.17,\nNo. 2., 1980.\n[5] F Scholer I S H Suyoto, A L Uitdenbogerd. Searching mu-\nsical audio using symbolic queries audio, speech, and lan-\nguage processing. IEEE Transactions on In Audio, Speech,\nand Language Processing, IEEE Transactions on, Vol. 16,\nNo. 2.,pages 372–381, 2008.\n[6] Vignesh Ishwar, Ashwin Bellur, and Hema A Murthy. Mo-\ntivic analysis and its relevance to raga identiﬁcation in ca r-\nnatic music. In Workshop on Computer Music , Instanbul,\nTurkey, July2012.\n[7] TMKrishna andVigneshIshwar. Svaras,gamaka, motifand\nraga identity. In Workshop on Computer Music , Instanbul,\nTurkey, July2012.\n[8] A Krishnaswamy. Application of pitch tracking to south i n-\ndianclassicalmusic. InProc.oftheIEEEInt.Conf.onAcous-\ntics, Speech and Signal Processing (ICASSP) , pages 557–\n560, 2003.\n[9] A Krishnaswamy. Inﬂexions and microtonality in south in -\ndian classical music. Frontiers of Research on Speech and\nMusic, 2004.\n[10] HWEI-JEN LIN, HUNG-HSUAN WU, and CHUN-WEI\nWANG. Music matching based on rough longest common\nsubsequence. Journal of Information Science and Engineer-\ning, pages 27, 95–110., 2011.\n[11] Pranav Patel, Eamonn Keogh, Jessica Lin, and Stefano\nLonardi. Mining motifs in massive time series databases. In\nProceedings ofIEEEInternational Conference onDataMin-\ning (ICDM) ,pages 370–377, 2002.\n[12] William H. Press, Saul A. Teukolsky, William T. Vetterl ing,\nand Brian P. Flannery. Numerical Recipes in C: The Art\nof Scientiﬁc Computing. Second Edition . Oxford University\nPress, 1992.\n[13] Joe Cheri Ross and Preeti Rao. Detecting melodic motifs\nfrom audio for hindustani classic music. In ISMIR, Portugal,\nOctober 2012.\n[14] Joe Cheri Ross and Preeti Rao. Detection of raga-\ncharacteristic phrases from hindustani classical music au dio.\nProc. of the 2ndCompMusic Workshop , July12-13, 2012.\n[15] J. Salamon and E. Gomez. Melody extraction from poly-\nphonic music signals using pitch contour characteristics.\nIEEETransactions onAudio,SpeechandLanguage Process-\ning, pages 20(6):1759–1770, Aug. 2012.\n[16] J Serra, G K Koduri, M Miron, and X Serra. Tuning of sung\nindian classical music. In Proc. of ISMIR , pages 157–162,\n2011.\n[17] M Subramanian. Carnatic ragam thodi - pitch analysis of\nnotes and gamakams. in Journal of the Sangeet Natak\nAkademi, pages 3–28, 2007.\n[18] Dragomir Yankov, Eamonn Keogh, Jose Medina, Bill Chiu,\nand Victor Zordan. Detecting time series motifs under uni-\nformscaling. InProceedingsofthe13thACMSIGKDDinter-\nnational conference on Knowledge discovery and data min-\ning, August12-15, 2007 , 2007."
    },
    {
        "title": "Automated Methods for Analyzing Music Recordings in Sonata Form.",
        "author": [
            "Nanzhu Jiang",
            "Meinard Müller"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418283",
        "url": "https://doi.org/10.5281/zenodo.1418283",
        "ee": "https://zenodo.org/records/1418283/files/JiangM13.pdf",
        "abstract": "The sonata form has been one of the most important large-scale musical structures used since the early Classical period. Typically, the first movements of symphonies and sonatas follow the sonata form, which (in its most basic form) starts with an exposition and a repetition thereof, continues with a development, and closes with a recapitulation. The recapitulation can be regarded as an altered repeat of the exposition, where certain substructures (first and second subject groups) appear in musically modified forms. In this paper, we introduce automated methods for analyzing music recordings in sonata form, where we proceed in two steps. In the first step, we derive the coarse structure by exploiting that the recapitulation is a kind of repetition of the exposition. This requires audio structure analysis tools that are invariant under local modulations. In the second step, we identify finer substructures by capturing relative modulations between the subject groups in exposition and recapitulation. We evaluate and discuss our results by means of the Beethoven piano sonatas. In particular, we introduce a novel visualization that not only indicates the benefits and limitations of our methods, but also yields some interesting musical insights into the data.",
        "zenodo_id": 1418283,
        "dblp_key": "conf/ismir/JiangM13",
        "keywords": [
            "sonata form",
            "symphonies",
            "sonatas",
            "exposition",
            "recapitulation",
            "development",
            "substructures",
            "audio structure analysis",
            "relative modulations",
            "Beethoven piano sonatas"
        ],
        "content": "AUTOMATED METHODS FOR ANALYZING MUSIC RECORDINGS IN\nSONATA FORM\nNanzhu Jiang\nInternational Audio Laboratories Erlangen\nnanzhu.jiang@audiolabs-erlangen.deMeinard M ¨uller\nInternational Audio Laboratories Erlangen\nmeinard.mueller@audiolabs-erlangen.de\nABSTRACT\nThe sonata form has been one of the most important\nlarge-scale musical structures used since the early Classi -\ncal period. Typically, the ﬁrst movements of symphonies\nand sonatas follow the sonata form, which (in its most ba-\nsic form) starts with an exposition and a repetition thereof ,\ncontinues with a development, and closes with a recapit-\nulation. The recapitulation can be regarded as an altered\nrepeat of the exposition, where certain substructures (ﬁrs t\nand second subject groups) appear in musically modiﬁed\nforms. In this paper, we introduce automated methods for\nanalyzing music recordings in sonata form, where we pro-\nceed in two steps. In the ﬁrst step, we derive the coarse\nstructure by exploiting that the recapitulation is a kind of\nrepetition of the exposition. This requires audio structur e\nanalysis tools that are invariant under local modulations.\nIn the second step, we identify ﬁner substructures by cap-\nturing relative modulations between the subject groups in\nexposition and recapitulation. We evaluate and discuss our\nresults by means of the Beethoven piano sonatas. In partic-\nular, we introduce a novel visualization that not only indi-\ncates the beneﬁts and limitations of our methods, but also\nyields some interesting musical insights into the data.\n1. INTRODUCTION\nThe musical form refers to the overall structure of a piece\nof music by its repeating and contrasting parts, which stand\nin certain relations to each other [5]. For example, many\nsongs follow a strophic form where the same melody is re-\npeated over and over again, thus yielding the musical form\nA1A2A3A4....1Or for a composition written in rondo\nform, a recurring theme alternates with contrasting sec-\ntions yielding the musical form A1BA2CA3D.... One of\nthe most important musical forms in Western classical mu-\nsic is known as sonata form , which consists of an expo-\nsition (E), adevelopment (D), and a recapitulation (R),\n1To describe a musical from, one often uses the capital letters t o refer\nto musical parts, where repeating parts are denoted by the same letter.\nThe subscripts indicate the order of repeated occurrences.\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieval .where the exposition is typically repeated once. Some-\ntimes, one can ﬁnd an additional introduction ( I) and a\nclosing coda ( C), thus yielding the form IE1E2DRC . In\nparticular, the exposition and the recapitulation stand in\nclose relation to each other both containing two subsequent\ncontrasting subject groups (often simply referred to as ﬁrs t\nand second theme) connected by some transition. How-\never, in the recapitulation, these elements are musically a l-\ntered compared to their occurrence in the exposition. In\nparticular, the second subject group appears in a modulated\nform, see [4] for details. The sonata form gives a compo-\nsition a speciﬁc identity and has been widely used for the\nﬁrst movements in symphonies, sonatas, concertos, string\nquartets, and so on.\nIn this paper, we introduce automated methods for ana-\nlyzing and deriving the structure for a given audio record-\ning of a piece of music in sonata form. This task is\na speciﬁc case of the more general problem known as\naudio structure analysis with the objective to partition\na given audio recording into temporal segments and of\ngrouping these segments into musically meaningful cate-\ngories [2,10]. Because of different structure principles, the\nhierarchical nature of structure, and the presence of musi-\ncal variations, general structure analysis is a difﬁcult an d\nsometimes a rather ill-deﬁned problem [12]. Most of the\nprevious approaches consider the case of popular music,\nwhere the task is to identify the intro, chorus, and verse\nsections of a given song [2,9–11]. Other approaches focus\non subproblems such as audio thumbnailing with the ob-\njective to extract only the most repetitive and characteris tic\nsegment of a given music recording [1, 3, 8].\nIn most previous work, the considered structural parts\nare often assumed to have a duration between 10and60\nseconds, resulting in some kind of medium-grained anal-\nysis. Also, repeating parts are often assumed to be quite\nsimilar in tempo and harmony, where only differences\nin timbre and instrumentation are allowed. Furthermore,\nglobal modulations can be handled well by cyclic shifts\nof chroma-based audio features [3]. When dealing with\nthe sonata form, certain aspects become more complex.\nFirst, the duration of musical parts are much longer of-\nten exceeding two minutes. Even though the recapitula-\ntion can be considered as some kind of repetition of the\nexposition, signiﬁcant local differences that may last for a\ncouple of seconds or even 20seconds may exist between\nthese parts. Furthermore, there may be additional or miss-\ning sub-structures as well as relative tempo differences be -tween the exposition and recapitulation. Finally, these tw o\nparts reveal differences in form of local modulations that\ncannot be handled by a global cyclic chroma shift.\nThe goal of this paper is to show how structure analysis\nmethods can be adapted to deal with such challenges. In\nour approach, we proceed in two steps. In the ﬁrst step, we\ndescribe how a recent audio thumbnailing procedure [8]\ncan be applied to identify the exposition and the recapitu-\nlation (Section 2). To deal with local modulations, we use\nthe concept of transposition-invariant self-similarity m atri-\nces [6]. In the second step, we reveal ﬁner substructures\nin exposition and recapitulation by capturing relative mod -\nulation differences between the ﬁrst and the second sub-\nject groups (Section 3). As for the evaluation of the two\nsteps, we consider the ﬁrst movements in sonata form of\nthe piano sonatas by Ludwig van Beethoven, which con-\nstitutes a challenging and musically outstanding collecti on\nof works [13]. Besides some quantitative evaluation, we\nalso contribute with a novel visualization that not only in-\ndicates the beneﬁts and limitations of our methods, but also\nyields some interesting musical insights into the data.\n2. COARSE STRUCTURE\nIn the ﬁrst step, our goal is to split up a given music record-\ning into segments that correspond to the large-scale mu-\nsical structure of the sonata form. On this coarse level,\nwe assume that the recapitulation is basically a repetition\nof the exposition, where the local deviations are to be ne-\nglected. Thus, the sonata form IE1E2DRC is dominated\nby the three repeating parts E1,E2, andR.\nTo ﬁnd the most repetitive segment of a music record-\ning, we apply and adjust the thumbnailing procedure pro-\nposed in [8]. To this end, the music recording is ﬁrst con-\nverted into a sequence of chroma-based audio features2,\nwhich relate to harmonic and melodic properties [7]. From\nthis sequence, a suitably enhanced self-similarity matrix\n(SSM) is derived [8]. In our case, we apply in the SSM\ncalculation a relatively long smoothing ﬁlter of 12sec-\nonds, which allows us to better bridge local differences in\nrepeating segments. Furthermore, to deal with local mod-\nulations, we use a transposition-invariant version of the\nSSM, see [6]. To compute such a matrix, one compares the\nchroma feature sequence with cyclically shifted versions\nof itself, see [3]. For each of the twelve possible chroma\nshifts, one obtains a similarity matrix. The transposition -\ninvariant matrix is then obtained by taking the entry-wise\nmaximum over the twelve matrices. Furthermore, storing\nthe shift index which yields the maximum similarity for\neach entry results in another matrix referred to as transpo-\nsition index matrix , which will be used in Section 3. Based\non such transposition-invariant SSM, we apply the proce-\ndure of [8] to compute for each audio segment a ﬁtness\nvalue that expresses how well the given segment explains\n2In our scenario, we use a chroma variant referred to as CENS features,\nwhich are part of the Chroma Toolbox http://www.mpi-inf.mpg.\nde/resources/MIR/chromatoolbox/ . Using a long smoothing\nwindow of four seconds and a coarse feature resolution of 1 Hz , we ob-\ntain features that show a high degree of robustness to smaller deviations,\nsee [7] for details.\n  \n0 100 200 300 400 5000100200300400500\n00.050.10.150.20.250.30.350.4\n  \n0 100 200 300 400 5000100200300400500\n00.050.10.150.20.250.30.350.4\n  \n0 100 200 300 400 5000100200300400500\n−200.51\n  \n0 100 200 300 400 5000100200300400500\n−200.51\n0 100 200 300 400 500 0 100 200 300 400 500E1E2D D R C E1E2D R C\nTime (sec) Time (sec)Time (sec) Time (sec)(a) (d)\n(b) (e)\n(c) (f)\nFigure 1: Thumbnailing procedure for Op031No2-01 (“Tem-\npest”). (a)/(d) Scape plot representation using an SSM with-\nout/with transposition invariance. (b)/(e) SSM without/with\ntransposition invariance along with the optimizing path family\n(cyan), the thumbnail segment (indicated on horizontal axis) and\ninduced segments (indicated on vertical axis). (c)/(f) Ground-\ntruth segmentation.\nother related segments (also called induced segments) in\nthe music recording. These relations are expressed by a so-\ncalled path family over the given segment. The thumbnail\nis then deﬁned as the segment that maximizes the ﬁtness.\nFurthermore, a triangular scape plot representation is com -\nputed, which shows the ﬁtness of all segments and yields a\ncompact high-level view on the structural properties of the\nentire audio recording.\nWe expect that the thumbnail segment, at least on the\ncoarse level, should correspond to the exposition ( E1),\nwhile the induced segments should correspond to the re-\npeating exposition ( E2) and the recapitulation ( R). To il-\nlustrate this, we consider as our running example a Baren-\nboim recording of the ﬁrst movement of Beethoven’s piano\nsonata Op. 31, No. 2 (“Tempest”), see Figure 1. In the fol-\nlowing, we also use the identiﬁer Op031No2-01 to refer\nto this movement. Being in the sonata form, the coarse mu-\nsical form of this movement is E1E2DRC . Even though\nRis some kind of repetition of E1, there are signiﬁcant\nmusical differences. For example, the ﬁrst subject group\ninRis modiﬁed and extended by an additional section not\npresent in E1, and the second subject group in Ris trans-\nposed ﬁve semitones upwards (and later transposed seven\nsemitones downwards) relative to the second subject group\ninE1. In Figure 1, the scape plot representation (top) and\nSSM along with the ground truth segmentation (bottom)\nare shown for our example, where on the left an SSM with-\nout and on the right an SSM with transposition invariance\nhas been used. In both cases, the thumbnail segment corre-\nsponds to part E1. However, without using transposition-\ninvariance, the recapitulation is not among the induced seg -\nments, thus not representing the complete sonata form, see\nFigure 1b. In contrast, using transposition-invariance, a lso\ntheR-segment is identiﬁed by the procedure as a repetition1, Op002No1−01\n  \n0 50 100 150 200050100150200\n−200.51\n2, Op002No2−01\n  \n0 100 200 300 4000100200300400\n−200.51\n3, Op002No3−01\n  \n01002003004005006000100200300400500600\n−200.51\n4, Op007−01\n  \n0 100 200 300 4000100200300400\n−200.51\n5, Op010No1−01\n  \n0 100 200 3000100200300\n−200.51\n6, Op010No2−01\n  \n0 100 200 3000100200300\n−200.51\n7, Op010No3−01\n  \n0 100 200 300 4000100200300400\n−200.51\n8, Op013−01\n  \n0100 200 300 400 5000100200300400500\n−200.51\n9, Op014No1−01\n  \n0 100 200 300 4000100200300400\n−200.51\n10, Op014No2−01\n  \n0 100 200 300 4000100200300400\n−200.51\n11, Op022−01\n  \n0 100 200 300 4000100200300400\n−200.51\n15, Op028−01\n  \n01002003004005006000100200300400500600\n−200.51\n16, Op031No1−01\n  \n0 100 200 3000100200300\n−200.51\n17, Op031No2−01\n  \n0100 200 300 400 5000100200300400500\n−200.51\n18, Op031No3−01\n  \n0100 200 300 400 5000100200300400500\n−200.51\n19, Op049No1−01\n  \n050100 150 200 250050100150200250\n−200.51\n20, Op049No2−01\n  \n050100 150 200 250050100150200250\n−200.51\n21, Op053−01\n  \n01002003004005006000100200300400500600\n−200.51\n23, Op057−01\n  \n01002003004005006000100200300400500600\n−200.51\n24, Op078−01\n  \n0 100 200 300 4000100200300400\n−200.51\n25, Op079−01\n  \n050100 150 200 250050100150200250\n−200.51\n26, Op081a−01\n  \n0 100 200 300 4000100200300400\n−200.51\n27, Op090−01\n  \n0 100 200 3000100200300\n−200.51\n28, Op101−01\n  \n0 50 100 150 200 250050100150200250\n−200.51\n29, Op106−01\n  \n0 200 400 6000200400600\n−200.51\n30, Op109−01\n  \n0 50 100 150 200 250050100150200250\n−200.51\n31, Op110−01\n  \n0 100 200 300 4000100200300400\n−200.51\n32, Op111−01\n  \n0100 200 300 400 500 6000100200300400500600\n−200.510 50 100 150 200 0 100 200 300 400 0100 200 300 400 500 600 0 100 200 300 400 0 100 200 300 0 100 200 300 0 100 200 300 400\n0100 200 300 400 500 0 100 200 300 400 0 100 200 300 400 0 100 200 300 400 0100 200 300 400 500 600 0 100 200 300 0 100 200 300 400 500\n0 100 200 300 400 500 0 50 100 150 200 250 0 50 100 150 200 250 0100200300400500600 0100 200 300 400 500 600 0 100 200 300 400 0 50 100 150 200 250\n0 100 200 300 400 0 100 200 300 0 50 100 150 200 250 0 200 400 600 0 50 100 150 200 250 0 100 200 300 400 0100 200 300 400 500 600\nTime (sec) Time (sec) Time (sec) Time (sec) Time (sec) Time (s ec) Time (sec)Time (sec) Time (sec) Time (sec) Time (sec)\nFigure 2: Results of the thumbnailing procedure for the 28 ﬁrst movements in sonata form. The ﬁgure shows for each recording the\nunderlying SSM along with the optimizing path family (cyan), the thumbnail seg ment (indicated on horizontal axis) and the induced\nsegments (indicated on vertical axis). Furthermore, the correspondin g GT segmentation is indicated below each SSM.\nof theE1-segment, see Figure 1e.\nAt this point, we want to emphasize that only the us-\nage of various smoothing and enhancement strategies in\ncombination with a robust thumbnailing procedure makes\nit possible to identify the recapitulation. The procedure\ndescribed in [8] is suitably adjusted by using smoothed\nchroma features having a low resolution as well as apply-\ning a long smoothing length and transposition-invariance\nin the SSM computation. Additionally, when deriving the\nthumbnail, we apply a lower bound constraint for the min-\nimal possible segment length of the thumbnail. This lower\nbound is set to one sixth of the duration of the music\nrecording, where we make the musically informed assump-\ntion that the exposition typically covers at least one sixth\nof the entire movement.\nTo evaluate our procedure, we use the complete Baren-\nboim recordings of the 32piano sonatas by Ludwig van\nBeethoven. Among the ﬁrst movements, we only con-\nsider the 28movements that are actually composed in\nsonata form. For each of these recording, we manually\nannotated the large-scale musical structure also referred\nto as ground-truth (GT) segmentation, see Table 1 for an\noverview. Then, using our thumbnailing approach, we\ncomputed the thumbnail and the induced segmentation (re-\nsulting in two to four segments) for each of the 28record-\nings. Using pairwise P/R/F-values3, we compared the\ncomputed segments with the E- andR-segments speciﬁed\nby the GT annotation, see Table 1. As can be seen, one\nobtains high P/R/F-values for most recordings, thus indi-\n3These values are standard evaluation measures used in audio s truc-\nture analysis, see, e. g. [10].No. Piece ID GT Musical Form P R F\n1Op002No1-01 E1E2DR 0.99 0.90 0.90\n2Op002No2-01 E1E2DR 0.99 0.96 0.96\n3Op002No3-01 E1E2DRC 0.95 0.97 0.97\n4Op007-01 E1E2DRC 1.00 0.99 0.99\n5Op010No1-01 E1E2DR 0.99 0.93 0.93\n6Op010No2-01 E1E2DR 0.95 0.86 0.86\n7Op010No3-01 E1E2DRC 0.93 0.94 0.94\n8Op013-01 IE1E2DRC 0.96 0.95 0.95\n9Op014No1-01 E1E2DRC 0.97 0.97 0.97\n10 Op014No2-01 E1E2DRC 0.94 0.96 0.96\n11 Op022-01 E1E2DR 1.00 0.97 0.97\n12 Op026-01 - - - -\n13 Op027No1-01 - - - -\n14 Op027No2-01 - - - -\n15 Op028-01 E1E2DRC 1.00 0.99 0.99\n16 Op031No1-01 E1E2DRC 0.83 0.74 0.74\n17 Op031No2-01 E1E2DRC 0.90 0.85 0.85\n18 Op031No3-01 E1E2DRC 0.99 0.98 0.98\n19 Op049No1-01 E1E2DRC 0.96 0.91 0.91\n20 Op049No2-01 E1E2DR 1.00 0.96 0.96\n21 Op053-01 E1E2DRC 0.99 0.97 0.97\n22 Op054-01 - - - -\n23 Op057-01 EDRC 0.92 0.78 0.78\n24 Op078-01 IE1E2D1R1D2R20.98 0.84 0.84\n25 Op079-01 E1E2D1R1D2R2C0.50 0.55 0.55\n26 Op081a-01 IE1E2DRC 0.86 0.88 0.88\n27 Op090-01 EDRC 0.76 0.85 0.85\n28 Op101-01 EDRC 0.97 0.89 0.89\n29 Op106-01 E1E2DRC 0.99 0.98 0.98\n30 Op109-01 EDRC 0.92 0.86 0.86\n31 Op110-01 EDRC 0.91 0.81 0.81\n32 Op111-01 IE1E2DRC 0.65 0.64 0.64\nAverage 0.92 0.86 0.89\nTable 1: Ground truth annotation and evaluation results (pair-\nwise P/R/F values) for the thumbnailing procedure using Baren-\nboim recordings for the ﬁrst movements in sonata form of the\nBeethoven piano sonatas.\ncating a good performance of the procedure. This is also\nreﬂected by Figure 2, which shows the SSMs along with\nthe path families and ground truth segmentation for all 28\nrecordings. However, there are also a number of excep-\ntional cases where our procedure seems to fail. For exam-\nple, forOp079-01 (No.25), one obtains an F-measure\nof only0.55. Actually, it turns out that for this recordingtheD-part as well as R-part are also repeated resulting in\nthe form E1E2D1R1D2R2C. As a result, our minimum\nlength assumption that the exposition covers at least one\nsixth of the entire movement is violated. However, by re-\nducing the bound to one eighth, one obtains for this record-\ning the correct thumbnail and an F-measure of 0.85. In\nparticular, for the later Beethoven sonatas, the results te nd\nto become poorer compared to the earlier sonatas. From a\nmusical point of view, this is not surprising since the later\nsonatas are characterized by the release of common rules\nfor musical structures and the increase of compositional\ncomplexity [13]. For example, for some of the sonatas, the\nexposition is no longer repeated, while the coda takes over\nthe role of a part of equal importance.\n3. FINE STRUCTURE\nIn the second step, our goal is to ﬁnd substructures within\nthe exposition and recapitulation by exploiting the relati ve\nharmonic relations that typically exist between these two\nparts. Generally, the exposition presents the main themati c\nmaterial of the movement that is contained in two contrast-\ningsubject groups . Here, in the ﬁrst subject group ( G1)\nthe music is in the tonic (the home key) of the movement,\nwhereas in the second subject group ( G2) it is in the dom-\ninant (for major sonatas) or in the tonic parallel (for mi-\nnor sonatas). Furthermore, the two subject groups are typ-\nically combined by a modulating transition (T) between\nthem, and at the end of the exposition there is often an ad-\nditional closing theme or codetta (C). The recapitulation\ncontains similar sub-parts as the exposition, however it in -\ncludes some important harmonic changes. In the following\ndiscussion, we denote the four sub-parts in the exposition\nbyE-G1,E-T,E-G2, andE-C. Also, in the recapitu-\nlation by R-G1,R-T,R-G2, andR-C. The ﬁrst subject\ngroupsE-G1andR-G1are typically repeated in more or\nless the same way both appearing in the tonic. However, in\ncontrast to E-G2appearing in the dominant or tonic par-\nallel, the second subject group R-G2appears in the tonic.\nFurthermore, compared to E-T, the transition R-Tis often\nextended, sometimes even presenting new material and lo-\ncal modulations, see [4] for details. Note that the describe d\nstructure indicates a tendency rather then being a strict ru le.\nActually, there are many exceptions and modiﬁcations as\nthe following examples demonstrate.\nTo illustrate the harmonic relations between the subject\ngroups, let us assume that the movement is written in C\nmajor. Then, in the exposition, E-G1would also be in C\nmajor, and E-G2would be in Gmajor. In the recapitula-\ntion, however, both R-G1andR-G2would be in Cmajor.\nTherefore, while E-G1andR-G1are in the same key, R-\nG2is a modulated version of E-G2, shifted ﬁve semitones\nupwards (or seven semitones downwards). In terms of the\nmaximizing shift index as introduced in Section 2, one can\nexpect this index to be i= 5in the transposition index ma-\ntrix when comparing E-G2withR-G2.4Similarly, for\n4We assume that the index encodes shifts in upwards direction. Note\nthat the shifts are cyclic, so that shifting ﬁve semitones upw ards is the\nsame as shifting seven semitones downwards.\n  \n0 20 40 60 80 100 120350400450500\n−2−1.5−1−0.500.5\n  \n20 40 60 80 100 120400450500\n 01234567891011\n  \n0 20 40 60 80 100 120350400450500\n−2−1.5−1−0.500.51\nTime (sec) Time (sec)Time (sec) Time (sec) Trans. Index(a) (b)\n(c) (d)\n(e) (f)\nFigure 3: Illustration for deriving the WRTI (weighted relative\ntransposition index) representation using Op031No2-01 as ex-\nample. (a)Enlarged part of the SSM shown in Figure 1e, where\nthe horizontal axis corresponds to the E1-segment and the verti-\ncal axis to the R-segment. (b)Corresponding part of the trans-\nposition index matrix. (c)Path component of the optimizing path\nfamily as shown in Figure 1e. (d)Transposition index restricted\nto the path component. (e)Transposition index plotted over time\naxis ofR-segment. (f)Final WRTI representation.\nminor sonatas, this index is typically i= 9, which cor-\nresponds to shifting three semitones downwards from the\ntonic parallel to the tonic.\nBased on this observation, we now describe a proce-\ndure for detecting and measuring the relative differences\nin harmony between the exposition and the recapitula-\ntion. To illustrate this procedure, we continue our exam-\npleOp031No2-01 from Section 2, where we have al-\nready identiﬁed the coarse sonata form segmentation, see\nFigure 1e. Recall that when computing the transposition-\ninvariant SSM, one also obtains the transposition index\nmatrix , which indicates the maximizing chroma shift in-\ndex [6]. Figure 3a shows an enlarged part of the enhanced\nand thresholded SSM as used in the thumbnailing proce-\ndure, where the horizontal axis corresponds to the exposi-\ntionE1and the vertical axis to the recapitulation R. Fig-\nure 3b shows the corresponding part of the transposition\nindex matrix, where the chroma shift indices are displayed\nin a color-coded form.5As revealed by Figure 3b, the\nshift indices corresponding to E-G1andR-G1are zero\n(gray color), whereas the shift indices corresponding to E-\nG2andR-G2are ﬁve (pink color). To further emphasize\nthese relations, we focus on the path that encodes the sim-\n5For the sake of clarity, only those shift indices are shown tha t cor-\nrespond to the relevant entries (having a value above zero) o f the SSM\nshown in Figure 3a.Time (sec) Time (sec) Time (sec) Time (sec) Time (sec) Time (s ec) Time (sec)Trans. Index Trans. Index Trans. Index Trans. Index\nFigure 4: WRTI representations for all 28recordings. The manual annotations of the segment boundaries betwe enR-G1,R-T,R-G2,\nandR-Care indicated by vertical lines. In particular, the blue line indicates the end of R-G1and the red line as the beginning of R-G2.\nilarity between E1andR, see Figure 3c. This path is a\ncomponent of the optimizing path family computed in the\nthumbnailing procedure, see Figure 1e. We then consider\nonly the shift indices that lie on this path, see Figure 3d.\nNext, we convert the vertical time axis of Figure 3d, which\ncorresponds to the R-segment, into a horizontal time axis.\nOver this horizontal axis, we plot the corresponding shift\nindex, where the index value determines the position on the\nvertical index axis, see Figure 3e. In this way, one obtains\na function that expresses for each position in the recapitu-\nlation the harmonic difference (in terms of chroma shifts)\nrelative to musically corresponding positions in the expo-\nsition. We reﬁne this representation by weighting the shift\nindices according to the SSM values underlying the path\ncomponent. In the visualization of Figure 3f, these weights\nare represented by the thickness of the plotted dots. In the\nfollowing, for short, we refer to this representation as the\nWRTI (weighted relative transposition index) representa-\ntion of the recapitulation.\nFigure 4 shows the WRTI representations for the 28\nrecordings discussed in Section 2. Closely following [13],\nwe manually annotated the segments corresponding to G1,\nT,G2, andCwithin the expositions and recapitulations\nof these recordings6, see Table 2. In Figure 4, the seg-\nment corresponding to R-Tis indicated by a blue vertical\nline (end of R-G1) and a red vertical line (beginning of\nR-G2). Note that for some sonatas (e. g., Op002No3-01\norOp007-01 ) there is no such transition, so that only the\n6As far as this is possible due to many deviations and variation s in the\nactual musical forms.red vertical line is visible. For many of the 28recordings,\nas the theory suggests, the WRTI representation indeed in-\ndicates the location of the transition segment by a switch\nfrom the shift index i= 0 to the shift index i= 5 (for\nsonatas in major) or to i= 9 (for sonatas in minor). For\nexample, for the movement Op002No1-01 (No. 1) in F\nminor, the switch from i= 0toi= 9occurs in the transi-\ntion segment. Or for our running example Op031No2-01\n(No. 17), there is a clearly visible switch from i= 0 to\ni= 5 with some further local modulations in between.\nActually, this sonata already constitutes an interesting e x-\nception, since the shift of the second subject group is from\nthe dominant (exposition) to the tonic (recapitulation) ev en\nthough the sonata is in minor ( Dminor). Another more\ncomplex example is Op013-01 (No. 8, “Path ´etique”) in\nCminor, where E-G1starts with E♭minor, whereas R-\nG1starts with Fminor (shift index i= 2) before it reaches\nthe tonicCminor (shift index i= 9). Actually, our WRTI\nrepresentation reveals these harmonic relations.\nTo obtain a more quantitative evaluation, we located\nthe transition segment R-Tby determining the time po-\nsition (or region) where the shift index i= 0 (typically\ncorresponding to R-G1) changes to the most prominent\nnon-zero shift index within the R-segment (typically cor-\nresponding to R-G2and usually i= 5ori= 9), where we\nneglect all other shift indices. This position (or region) w as\ncomputed by a simple sweep algorithm to ﬁnd the optimal\nposition that separates the weighted zero-indices (which\nshould be on the left side of the optimal sweep line) and\nthe weighted indices of the prominent index (which shouldNo. Piece ID G1T G2C∆(G1) In(T)∆(G2)\n1Op002No1-01 10.6 12.6 20.8 20.4 y\n2Op002No2-01 26.0 24.4 44.2 21.1 y\n3Op002No3-01 37.9 - 82.9 12.3 -0.6 n\n4Op007-01 29.0 - 80.7 5.7 -11.5 n\n5Op010No1-01 23.2 22.4 45.9 22.4 y\n6Op010No2-01 46.2 - 60.3 22.2 n 2.0\n7Op010No3-01 20.1 24.7 46.2 7.5 -5.6 n\n8Op013-01 10.1 12.1 47.2 18.8 y\n9Op014No1-01 22.8 18.6 48.4 13.9 y\n10 Op014No2-01 13.0 31.4 55.7 - y\n11 Op022-01 17.5 23.5 65.7 19.8 y\n12 Op026-01 - - - - - - -\n13 Op027No1-01 - - - - - - -\n14 Op027No2-01 - - - - - - -\n15 Op028-01 45.2 24.7 80.3 25.4 -4.0 n\n16 Op031No1-01 21.6 - 40.2 12.6 -12.5 n\n17 Op031No2-01 85.7 19.6 34.9 13.6 -5.4 n\n18 Op031No3-01 55.4 - 42.9 25.7 -10.3 n\n19 Op049No1-01 30.5 - 33.5 12.5 -6.0 n\n20 Op049No2-01 24.6 8.6 26.2 15.2 n 8.9\n21 Op053-01 47.6 19.3 69.2 29.1 y\n22 Op054-01 - - - - - - -\n23 Op057-01 70.3 22.7 43.7 120.8 -7.3 n\n24 Op078-01 41.7 18.9 11.7 29.5 -15.9 n\n25 Op079-01 8.0 8.9 13.2 2.9 y\n26 Op081a-01 13.9 22.3 8.3 8.8 y\n27 Op090-01 47.1 38.9 14.1 18.2 y\n28 Op101-01 - - - - - - -\n29 Op106-01 60.0 43.4 55.5 24.9 -36.7 n\n30 Op109-01 13.7 - 41.9 36.6 -6.1 n\n31 Op110-01 47.8 32.0 56.0 17.3 -26.0 n\n32 Op111-01 20.3 29.9 61.0 20.4 y\nTable 2: Ground truth annotation and evaluation results for ﬁner-\ngrained structure. The columns indicate the number of the sonata\n(No.), the identiﬁer, as well as the duration (in seconds) of the\nannotated segments corresponding to R-G1,R-T,R-G2, andR-\nC. The last three columns indicate the position of the computed\ntransition center (CTC), see text for explanations.\nbe on the right side of the optimal sweep line). In the case\nthat there is an entire region of optimal sweep line posi-\ntions, we took the center of this region. In the following,\nwe call this time position the computed transition center\n(CTC). In our evaluation, we then investigated whether the\nCTC lies within the annotated transition R-Tor not. In the\ncase that the CTC is not in R-T, it may be located in R-\nG1or inR-G2. In the ﬁrst case, we computed a negative\nnumber indicating the directed distance given in seconds\nbetween the CTC and the end of R-G1, and in the sec-\nond case a positive number indicating the directed distance\nbetween the CTC and the beginning of R-G2. Table 2\nshows the results of this evaluation, which demonstrates\nthat for most recordings the CTC is a good indicator for\nR-T. The poorer values are in most case due to the devia-\ntions in the composition from the music theory. Often, the\nmodulation differences between exposition and recapitula -\ntion already start within the ﬁnal section of the ﬁrst subjec t\ngroup, which explains many of the negative numbers in Ta-\nble 2. As for the late sonatas such as Op106-01 (No. 29)\norOp110-01 (No. 31), Beethoven has already radically\nbroken with conventions, so that our automated approach\n(being naive from a musical point of view) is deemed to\nfail for locating the transition.\n4. CONCLUSIONS\nIn this paper, we have introduced automated methods\nfor analyzing and segmenting music recordings in sonata\nform. We adapted a thumbnailing approach for detecting\nthe coarse structure and introduced a rule-based approach\nmeasuring local harmonic relations for analyzing the ﬁner\nsubstructure. As our experiments showed, we achieved\nmeaningful results for sonatas that roughly follow the mu-sical conventions. However, (not only) automated methods\nreach their limits in the case of complex movements, where\nthe rules are broken up. We hope that even for such com-\nplex cases, automatically computed visualizations such as\nour introduced WRTI (weighted relative transposition in-\ndex) representation may still yield some musically inter-\nesting and intuitive insights into the data, which may be\nhelpful for musicological studies.\nAcknowledgments: This work has been supported by the\nGerman Research Foundation (DFG MU 2682/5-1). The\nInternational Audio Laboratories Erlangen are a joint in-\nstitution of the Friedrich-Alexander-Universit ¨at Erlangen-\nN¨urnberg (FAU) and Fraunhofer IIS.\n5. REFERENCES\n[1] Mark A. Bartsch and Gregory H. Wakeﬁeld. Audio thumbnaili ng of\npopular music using chroma-based representations. IEEE Transac-\ntions on Multimedia , 7(1):96–104, 2005.\n[2] Roger B. Dannenberg and Masataka Goto. Music structure a naly-\nsis from acoustic signals. In David Havelock, Sonoko Kuwano , and\nMichael V orl ¨ander, editors, Handbook of Signal Processing in Acous-\ntics, volume 1, pages 305–331. Springer, New York, NY , USA, 2008.\n[3] Masataka Goto. A chorus section detection method for music al audio\nsignals and its application to a music listening station. IEEE Transac-\ntions on Audio, Speech and Language Processing , 14(5):1783–1794,\n2006.\n[4] Hugo Leichtentritt. Musikalische Formenlehre . Breitkopf und H ¨artel,\n12. Auﬂage, Wiesbaden, Germany, 1987.\n[5] Richard Middleton. Form. In Bruce Horner and Thomas Swiss, edi-\ntors, Key terms in popular music and culture , pages 141–155. Wiley-\nBlackwell, 1999.\n[6] Meinard M ¨uller and Michael Clausen. Transposition-invariant self-\nsimilarity matrices. In Proceedings of the 8th International Confer-\nence on Music Information Retrieval (ISMIR) , pages 47–50, Vienna,\nAustria, 2007.\n[7] Meinard M ¨uller and Sebastian Ewert. Chroma Toolbox: MATLAB\nimplementations for extracting variants of chroma-based audi o fea-\ntures. In Proceedings of the International Society for Music Informa -\ntion Retrieval Conference (ISMIR) , pages 215–220, Miami, FL, USA,\n2011.\n[8] Meinard M ¨uller, Nanzhu Jiang, and Peter Grosche. A robust ﬁtness\nmeasure for capturing repetitions in music recordings with ap plica-\ntions to audio thumbnailing. IEEE Transactions on Audio, Speech &\nLanguage Processing , 21(3):531–543, 2013.\n[9] Meinard M ¨uller and Frank Kurth. Towards structural analysis of au-\ndio recordings in the presence of musical variations. EURASIP Jour-\nnal on Advances in Signal Processing , 2007(1), 2007.\n[10] Jouni Paulus, Meinard M ¨uller, and Anssi P. Klapuri. Audio-based\nmusic structure analysis. In Proceedings of the 11th International\nConference on Music Information Retrieval (ISMIR) , pages 625–636,\nUtrecht, The Netherlands, 2010.\n[11] Geoffroy Peeters. Deriving musical structure from sign al analysis for\nmusic audio summary generation: “sequence” and “state” approa ch.\nInComputer Music Modeling and Retrieval , volume 2771 of Lecture\nNotes in Computer Science , pages 143–166. Springer Berlin / Heidel-\nberg, 2004.\n[12] Jordan Bennett Louis Smith, John Ashley Burgoyne, Ichir o Fujinaga,\nDavid De Roure, and J. Stephen Downie. Design and creation of a\nlarge-scale database of structural annotations. In Proceedings of the\n12th International Conference on Music Information Retrie val (IS-\nMIR) , pages 555–560, Miami, FL, USA, 2011.\n[13] Donald Francis Tovey. A Companion to Beethoven’s Pianoforte\nSonatas . The Associated Board of the Royal Schools of Music, 1998."
    },
    {
        "title": "A Simple Fusion Method of State And Sequence Segmentation for Music Structure Discovery.",
        "author": [
            "Florian Kaiser",
            "Geoffroy Peeters"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416046",
        "url": "https://doi.org/10.5281/zenodo.1416046",
        "ee": "https://zenodo.org/records/1416046/files/KaiserP13.pdf",
        "abstract": "Methods for music structure segmentation are based on strong assumptions on the acoustical properties of structural segments. These assumptions relate to the novelty, homogeneity, repetition and/or regularity of the content. Each of these assumptions provide a different perspective on the music piece. These assumptions are however often considered separately in the methods. In this paper we propose a method for estimating the music structure segmentation based on the fusion of the novelty and repetition assumptions. This combination of different perspectives on the music pieces allows to generate more coherent acoustic segments and strongly improves the final music structure segmentation's performance.",
        "zenodo_id": 1416046,
        "dblp_key": "conf/ismir/KaiserP13",
        "keywords": [
            "methods",
            "music structure segmentation",
            "acoustical properties",
            "novelty",
            "homogeneity",
            "repetition",
            "regularity",
            "perspectives",
            "music pieces",
            "coherent acoustic segments"
        ],
        "content": "A SIMPLE FUSION METHOD OF STATE AND SEQUENCE\nSEGMENTATION FOR MUSIC STRUCTURE DISCOVERY\nFlorian Kaiser\nSTMS IRCAM-CNRS-UPMC\n1 Place Igor Stravinsky\n75004 Paris\nflorian.kaiser@ircam.frGeoffroy Peeters\nSTMS IRCAM-CNRS-UPMC\n1 Place Igor Stravinsky\n75004 Paris\ngeoffroy.peeters@ircam.fr\nABSTRACT\nMethods for music structure segmentation are based on\nstrong assumptions on the acoustical properties of struc-\ntural segments. These assumptions relate to the novelty,\nhomogeneity, repetition and/or regularity of the content.\nEach of these assumptions provide a different perspective\non the music piece. These assumptions are however of-\nten considered separately in the methods. In this paper we\npropose a method for estimating the music structure seg-\nmentation based on the fusion of the novelty and repeti-\ntion assumptions. This combination of different perspec-\ntives on the music pieces allows to generate more coherent\nacoustic segments and strongly improves the ﬁnal music\nstructure segmentation’s performance.\n1. INTRODUCTION\nMusic structure segmentation (MSS) is the task of dividing\na musical audio signal into its main structural parts. Exam-\nples of such main segments for popular music are the verse\nand the chorus. MSS allows for a large set of applications\nof interest in the context of digital music, such as automatic\nsummarization or active listening. Because of this, the task\nemerged as an important challenge for the Music Informa-\ntion Retrieval (MIR) research and industrial communities.\nA musical composition is a layered construction of\nquantiﬁable musical elements of various temporal scales,\ne.g. beats, notes, bars, etc. While these elements are qual-\niﬁed by strict musical deﬁnition, the higher temporal level\nmusic structure is perceptually audible but not qualiﬁed by\nany strict musical deﬁnition. Because of this, the MSS\ntask raised questions about the deﬁnitions of the segments\nto be estimated. In order to cope with this lack of deﬁni-\ntion, MSS researchers have developed assumptions-driven\nmethods. As described by Paulus et al. [13] methods can\nbe categorized according to the used assumptions on the\ncontent: novelty, homogeneity and repetition. We can add\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.to this list the Regularity hypothesis proposed by Sargent\net al. [15].\n1.1 Related Work\nMSS algorithms usually rely on two successive steps: seg-\nmentation and grouping of segments. The temporal seg-\nmentation consists in estimating the borders of potential\nstructural segments, therefore limiting the search space for\nthe structural and ﬁxing its temporal scale. This step is cru-\ncial to ensure the global performance of systems. This pre-\nliminary temporal segmentation is directly inﬂuenced and\nconstrained by the above-mentioned assumptions. This is\nbecause the various assumptions give different perspec-\ntives of the music pieces content. We brieﬂy review these\nassumptions here.\nWith the novelty hypothesis, boundaries between struc-\ntural segments are considered as time points of high\n”acoustical contrast”. This notion of contrast has been\nintroduced by Foote [5] and extends previously proposed\naudio novelty segmentation techniques [3]. It considers\njointly the homogeneity within segments as well as the dis-\nsimilarity between segments. A novelty function is com-\nputed by convoluting a Self-Similarity Matrix (SSM) with\na kernel that reﬂects the novelty hypothesis. Peaks of\nthe novelty function deﬁne potential structural boundaries.\nThis method has been successfully applied to music struc-\nture segmentation and still produces state-of-the-art per-\nformances for the temporal segmentation step [4] [12] [9].\nSlightly different, the homogeneity assumption only re-\nquire strong inner acoustical homogeneity of the structural\nsegments. A popular approach then consists in deﬁning\nthe structural segments as the states of a Hidden Markov\nModel (HMM) [1] [14] [11].\nThe novelty and homogeneity assumptions assume\nstrong inner-homogeneity of segments, a property that\nPeeters formalized in [14] as the ”state” representation of\nstructural segments, and that is very often related to tim-\nbral properties of the music pieces. In contrast, repetition-\nbased temporal segmentation aims at detecting repeated\nsegments, homogeneous or not. In the case of non-\nhomogeneity, repeated segments are visualized in a SSM\nas stripes on the diagonal and off-diagonals. The segmen-\ntation of these stripes is denoted by Peeters as the sequence\napproach and is usually done in a Time-Lag Matrix [6].!\"#\"$%&\"'()*%'&\"+)\n,\"&-.)/\"#012%3)*%'&\"+)\n4)5670$'()80&90$:)!03#09'%;69)\n!'&<-'<&0)=0%'<&0)!03#09'%;69)\n!03#09':)>\":'%9-0)*%'&\"+)\n!03#09':)>\":'%9-0)*%'&\"+)\n?)\n!\"93$0)5670$'()80&90$)!03#09'%;69)\nAUDIO FEATURES \nSEGMENTATION Figure 1 . Fusion strategy between novelty and repetition temporal segmentation\nRecently, an extension of this lag representation was pro-\nposed by Serra et al. [16] and allow for the detection of\nnon-homogeneous as well as homogeneous repeated seg-\nments. This method is further described in the latter of this\npaper.\n1.2 Paper overview\nIn this paper we study the combination of these different\ntemporal segmentation approaches. In particular, we pro-\npose a generic fusion method that we apply to the fusion of\nthe repetition and novelty-based approaches. The choice of\nthese two methods is motivated by the strong antagonism\nthat exists between them. Indeed, the detection of repeti-\ntions detection is global since it requires considering the\nwhole signal (usually represented through a lag matrix). In\ncontrast, the novelty approach is essentially local and has\nno global perspective on the music piece.\nThese two methods thus give different explanations of\nthe music pieces’ structure that are both true. Our claim\nis that these explanations are complementary and that their\nfusion enhances the global music structure segmentation\nperformance. To this end, we propose a late fusion ap-\nproach that maintains the acoustic coherency within the ﬁ-\nnal estimated segments. Segments are ﬁrst estimated under\nboth hypotheses separately. A segments distance matrix is\nthen computed for both segmentations. The sum of these\ntwo matrices then serves as the ﬁnal representation for the\ntemporal segmentation.\n1.3 Paper organization\nIn Section 2 we introduce the algorithms we use for the\ntemporal segmentation under both assumptions. In Section\n3 we present the fusion procedure of the segmentations es-\ntimated with these two methods. The method is then il-\nlustrated in Section 4 on a real signal. In Section 5, we\npropose a comparative evaluation. Finally, conclusions are\ndrawn in Section 6. The general architecture of the fusion\nmethod is illustrated in Figure 1.\n2. TEMPORAL SEGMENTATION ALGORITHMS\n2.1 Repetition-based Segmentation\nThe classical approach for repetition-based segmentation\nrelies on the computation of a time-lag matrix representing\nchroma similarity. Recently, Serra et al. [16] proposed a\nnovel method that extends this approach to a circular time-\nlag matrix representation. The latter incorporates both pastand future samples. The audio signal is therefore repre-\nsented by a multidimensional time series that contains, for\neach time frame instant, the chroma vector of the actual\nframe as well as knowledge of the recent past with the en-\ncapsulation of delay coordinates [10]. A recurrence plot R\nretaining only the nearest similar frames is then computed\non this multidimensional time series. Circular shifting of\nthe rows of Rthen allows to compute the circular time-lag\nmatrix L:\nLi;j=Ri;k+1 (1)\nwith N the size of the feature vector, i = 1, ... , N and\nk = i + j -2 mod(N). An example of such a matrix is shown\nin Figure 4 (a). After smoothing with a bivariate gaus-\nsian kernel, the authors deﬁne the so-called ”structure fea-\nture” that serves for the temporal segmentation as the rows\nofL. Structural changes are indeed detected by strong\nchanges in the structure feature sequence and can be esti-\nmated in the difference between adjacent structure feature\nvectors. Evaluation at the 2012 MIREX1evaluation cam-\npaign for structural segmentation showed very convincing\nperformances of this method.\n2.2 Novelty-based Segmentation\nThe novelty-based segmentation was originally proposed\nby Foote in [5] and allows to detect transitions between ho-\nmogenous segments of a musical signal. This is achieved\nby means of the correlation of 2 \u00022 checkerboard nov-\nelty kernel along with the main diagonal of a SSM. [8]\nextends this method by introducing two new novelty ker-\nnels that allow for the detection of non-homogeneous to\nhomogeneous segments transitions and vice versa. These\nkernels are illustrated in Figure 2. Three novelty curves\nare computed for all three kernels on a SSM computed\non timbre-related features (MFCCs, Spectral Centroid,\nSpread, Skewness and Spectral Flatness). Adaptive peak\ntracking technique described in [7] allows for the estima-\ntion of boundary candidates in the three novelty functions\nand a ﬁnal segmentation is obtained by merging the three\nboundary sets within a tolerance range of 2s. In this paper,\nwe use the novelty method extended by [8].\n2.3 Segmentations Agreement\nIn order to highlight the differences between the two as-\nsumptions, we compare the segmentation results obtained\n1http://www.music-ir.org/mirex/wiki/2012:MIREX2012 Results!\"# $%%&'()*+',-'.*/01*!2#*3%4%5'-'%6)*7)8*9%-:3%4%5'-'%6)*;',-'.*!<#*9%-:3%4%5'-'%6)**7)8*3%4%5'-'%6)*;',-'.*\nFigure 2 . 60\u000260 Novelty detection kernels\nwith the repetition-based and novelty-based method pre-\nsented above. We compare the results obtained on the Iso-\nphonics testset that consists of 297 popular music song.\nIf we denote by TR(TN) one of the segment boundary\nobtained with the repetition-based segmentation (novelty-\nbased segmentation), the boundaries TRandTNare said\nequivalent if within a tolerance window of 2 seconds.\nResults show that that 45.8 %of the TNwere contained\nin the TR. Moreover, 55.9 %of the TRwere contained\nin the TN. There is thus about 50 %of the information\nestimated by the two methods that is very speciﬁc to the\nchosen assumption. While the two methods achieve rather\ncomparative results when evaluated within MIREX, this\nexperiment strengthens the assumption that fusion of both\nassumptions may increase the performance of the temporal\nsegmentation.\n3. SEGMENTATIONS FUSION\nRepetition- and novelty- based segmentations are explic-\nitly designed for different representations of the audio con-\ntent. An early fusion approach of these representations\nwould be therefore irrelevant. Instead, we choose a late fu-\nsion approach: the segmentations using both assumptions\nare ﬁrst estimated, and then merged together. In the re-\nmaining of this section we propose two fusion strategies.\nWe ﬁrst introduce a baseline method (see part 3.1) that is\nthe simplest way to merge the boundaries. We then present\n(see part 3.2) a method that merges the boundaries by ex-\nplicitly considering the acoustical relevancy of the fused\nsegments. This is done by using a segments distance ma-\ntrix representation.\n3.1 Baseline Method\nThe baseline fusion of boundaries simply consists in merg-\ningTRandTNif they are within a given tolerance win-\ndow\u0001. Since identical boundaries may be detected in both\nsets with a slight temporal deviation, the simple union of\nboundary sets is not precise enough for the fusion. We\ntherefore merge boundaries within a tolerance window. As\nillustrated in Figure 3 we retain the earliest boundary of the\ntwo when a matching is found.\nIn our experiment, we set the tolerance window at \u0001= 2\nseconds (1 measure @120bpm) to limit over-segmentation.\n3.2 Segments Distance Based Fusion\nBecause the baseline method is only constrained by the\nheuristic rule of a tolerance range, it does not consider\n!\"#$%&'()$%&*()$\n+,$\n+,$\nFigure 3 . Illustration of the baseline fusion method.\nthe acoustic relevance of the newly formed segment that\nderived from excluding or keeping a boundary. The new\nsegmentation might thus produce irrelevant segments and\ninduce errors at the structure clustering step. We there-\nfore propose here an alternative approach for the fusion of\nboundaries that includes knowledge of the acoustical co-\nherence of the ﬁnal estimated segments.\n3.2.1 Segments distance matrix\nWe introduce the Segments Distance Matrix (SDM) that\nmeasures the acoustical consistency between segments\nformed by TRandTN. In this, the distance between two\nsegments is calculated using the Mahalanobis distance be-\ntween the features distributions within each pair of seg-\nments:\nS(i; j) =d(Xi; Xj) =p\n(Xi\u0000Xj)T\u0006\u00001(Xi\u0000Xj)(2)\nwithXi[m\u0002n] and Xj[p\u0002n] the feature vectors respec-\ntively within segments iandjand\u0006their covariance ma-\ntrix. We use the Mahalanobis distance since it provides a\ngood compromise between the homogeneity and repetition\nassumptions for the segments comparison.\nThe size of the SDM is deﬁned by the number of bound-\naries detected. We then temporally-scale this SDM to the\noriginal SSM size in order to reﬂect the music piece’s\nstructure. Two examples of temporally-scaled-SDMs are\nshown in Figure 4 (c) and (c’) with the song One Vision by\nQueen. This example illustrates that SDMs give a perspec-\ntive on the saliency of each boundary. Indeed, we easily\ndistinguish adjacent segments with very strong acoustical\ndissimilarity as well as strong similarity.\n3.2.2 Fusion and Boundary Selection\nThe fusion process should select the boundaries by tak-\ning into account the acoustic saliency of the newly formed\nsegments. To provide a perspective on the acoustic con-\ntrasts given by the fusion of boundaries, we sum the SDMs\ncomputed corresponding to TRandTN. This summation\nallows displaying in a single representation the acoustic\ncontrast brought by both assumptions. It gives an acous-\ntical perspective on potentially merged boundaries. This is\nillustrated in Figure 4 (d). In this summed SDM, the acous-\ntic contrasts of both segmentations are kept. Note that a\nsimilar fusion of different matrix data representations was\nused by Chen in [2] for structure labelling purposes. The\nsummed SDM can be thought as a simpliﬁed SSM like rep-\nresentation of the music piece that gives a global acoustic\ndescription of the acoustic content.The ﬁnal temporal segmentation is then obtained by ap-\nplying the novelty segmentation on this fused representa-\ntion. Since the matrix describes only segments of strong\ninner-homogeneity we solely employ Foote’s kernel illus-\ntrated in Figure 2 (a).\n4. CASE STUDY\nIn this section we illustrate on a real signal (the song ”One\nVision” by Queen) our method for segment detection based\non late-fusion of repetition and novely-based segmenta-\ntion. Figures 4 display - for the repetition method: the Cir-\ncular Time-Lag Matrix (a), structure feature (b) and SDM\n(c) - for the novelty method: the SSM (a), novelty curve (b)\nand SDM (c). The SDMs calculated for both methods illus-\ntrate the different perspectives given on the song’s tempo-\nral segmentation.\nThe sum of the SDMs and corresponding novelty curve\nwith ﬁnal estimated boundaries are displayed in Figure 4\n(d) and (e). This clearly shows the compromise that is\nmade in our method between repetition-based and novelty-\nbased segments. Indeed, the different acoustic contrasts\nwithin the two segmentations can be corroborated by the\nﬁnal segmentation of the summed SDM but this not nec-\nessarily happening. For example boundaries that were de-\ntected within frames 180 and 420 by the repetition-based\nmethod are not all contained in the ﬁnal segmentation be-\ncause of insufﬁcient acoustic contrast of the newly formed\nsegment. Hence, the fusion method uses consistent acous-\ntic clues to decide of the fusion of segmentations.\n5. EVALUATION\nWe evaluate comparatively the performances of the vari-\nous segmentation methods taken separately (repetition and\nnovelty-based) and the proposed fusion methods (by base-\nline or distance-based fusion) as proposed in this paper. In\norder to investigate the impact of the method on the struc-\nture labelling of segments, an evaluation of the segments\nlabelling is also proposed. We ﬁrst introduce the segment\nlabelling process, evaluation protocol and then present and\ndiscuss the results.\n5.1 Segments Labeling\nFor all segmentation methods studied, the labelling of the\nsegments is achieved using the method proposed by [9],\ni.e. a hierarchical clustering is applied on the basis vec-\ntors of the Non-Negative-Matrix-Factorization (NMF) of\nthe SSM. We improve this method here by estimating au-\ntomatically the optimal number kof clusters (hence of dif-\nferent segment labels) to be formed. For this, we use a\nmethod inspired by [17]. The method consists in varying\nthe number kof clusters to be formed, and for each num-\nber, to compute the dispersion of the obtained partition.\nThe dispersion Dkis deﬁned as the average distance dxx\nbetween all nielements x; xwithin each cluster Ci:\nDk=kX\ni=11\nniX\nx;x02Cidxx0 (3)\n(a) (a’) (b’) (b) (c) (c’) \n(e) (d) \nFigure 4 . Example of the fusion method with the song\nexample ”One Vision” by Queen. (a) Circular Time Lag\nMatrix - (a’) Timbre-related SSM - (b) Structure feature\nwith estimated boundaries - (b’) Novelty curve of the SSM\nwith estimated boundaries - (c) Repetition segments SDM\n- (c’) Novelty segments SDM - (d) Summed SDM - (e)\nFinal novelty score with estimated boundaries\nDkmonotonically decreases with the number of clus-\nters and ﬂattens for some kthat is the ideal number of\nclusters. Differentiation of the Dkallows to estimate for\neach song the optimal number of labels.\n5.2 Evaluation Protocol: Testset and Metrics\nTestset: In order to allow the comparison between the\nresults presented here and the ones obtained at the 2012\nMIREX2evaluation for structural segmentation, we use\nthe Isophonics testset3, also known as the MIREX09 test-\nset. This testset consists of 297 popular music songs (the\n2http://www.music-ir.org/mirex/wiki/2012:MIREX2012 Results\n3http://isophonics.net/Temp. Seg. Eval. @0,5s Temp. Seg. Eval. @3s Seg. Group. Eval.\nMethod F P R F P R pF pP pR\nRepet 22.8 22.4 24.1 64.3 63.6 67.6 59.9 65.7 58.3\nNovel 29.5 26.7 34.7 61.8 55.9 72.5 60.0 62.5 63.2\nBaselineFusion 28.8 24.6 37.7 63.4 52.8 83.4 59.6 64.2 59.6\nSDMFusion 28.9 29.5 29.4 65.2 66.7 65.9 62.1 62.4 66.7\nTable 1 . Temporal segmentation and segment grouping evaluation on the Isophonics testset\nBeatles, Queen, Michael Jackson...)..\nMetrics: The temporal segmentation is, as in MIREX,\nevaluated using the precision P, recall Rand F-Measure\nF. In order to compute the True Positives, False Posi-\ntives and False Negatives, we used two tolerance windows:\n0.5 and 3 seconds. The segment labelling is evaluated, as\nin MIREX, using of the pairwise Precision, Recall and F-\nMeasure proposed by [11].\n5.3 Results and Discussion\nThe results are indicated into Table 1. The repetition and\nnovelty methods are respectively denoted by ” Repet ” and\n”Novel ”. The baseline fusion and segments distance based\nfusion are respectively denoted by ” Baseline Fusion ” and\n”SDM Fusion ”.\nRepet versus Novel: The results obtained for tempo-\nral segmentation shows that both repetition- and novelty-\nbased methods tend to over-segment the signal (recall\n>precision). This is especially true for the novelty-\nbased method. Evaluation with a 0.5s tolerance window\nshows better performances (F-measure) for the novelty-\nbased method. Increasing the tolerance to 3s then turns to\nthe advantage of the repetition-based method. The results\nobtained for segment labelling shows comparable perfor-\nmances (pairwise F-Measure) for both methods. The struc-\ntural segmentations are however of different natures con-\nsidering their differences in the pairwise recall and pre-\ncision balance: - labelling using the Repet method tends\nto over-estimate the number of labels, hence inherently\nproduce over-segmentation (pairwise Precision >pairwise\nRecall). - the inverse phenomenon is observed using the\nNovel method.\nFusion methods: The performance evaluation of\nthe baseline fusion method clearly shows a strong over-\nsegmentation (R >P for both tolerance window). More-\nover, labelling of the segments for the baseline fusion\nmethod shows the worst performance. In contrast, the\nSDM based fusion method shows very convincing perfor-\nmances for both the temporal segmentation and segment\nlabelling. Indeed, its performance for temporal segmenta-\ntion (F-measure) is just behind the novelty- based method’s\nperformance at 0.5s and obtains the best score at 3s. It is\nalso interesting to note that the temporal over-segmentation\nobserved for both Repet and Novel segmentations is not\nobserved in the SDM Fusion segmentation. This illustrates\nhow the acoustic information is considered in the fusion.\nThis is further validated by looking at the segmentations\nagreement. Conducting the same experiment as in Section3.3 indeed shows that 61,8 %of the repeated segments and\n61,9%of the novelty segments are contained in the SDM\nFusion segmentation.\nFinally, the segment labelling evaluation shows a very\npositive impact of the SDM Fusion segmentation. We in-\ncrease of about 2 percentage points the pairwise F-measure\nwith very balanced pairwise precision and recall. Again,\nthe SDM fusion of segments yield an original structural\ninterpretation beneﬁtting from both the repetition and nov-\nelty hypotheses.\n6. SUMMARY AND CONCLUSION\nIn this paper, we proposed a method for the consistent fu-\nsion of repetition- and novelty- based temporal segmenta-\ntions of music. We showed that this fused segmentation\nbeneﬁts from the temporal perspectives given by both hy-\npotheses and is rather inﬂuenced by the acoustical consis-\ntency of the ﬁnal segmentation than from one or the other\noriginal segmentation. Moreover, we showed that the fu-\nsion of the segmentations allows for a strong increase in the\nsegment labelling performance. This paper thus illustrates\nthe potential beneﬁts of developing multiple hypotheses\nbased structural segmentation algorithms. Moreover, we\nbelieve that the method is not restricted to the fusion of\nthe repetition and novelty methods and could be applied to\nother temporal segmentation methods.\n7. REFERENCES\n[1] Jean-Julien Aucouturier, Franc ¸ois Pachet, and M. San-\ndler. The way it sounds: timbre models for analysis and\nretrieval of music signals. IEEE Transactions on Mul-\ntimedia , 7(6):1028–1035, 2005.\n[2] Ruofeng Chen and Ming Li. Music structural segmen-\ntation by combining harmonic and timbral information.\nInProceedings of the 12th International Conference on\nMusic Information Retrieval (ISMIR) , 2011.\n[3] Scott Shaobing Chen and P.S. Gopalakrishnan. Clus-\ntering via the bayesian information criterion with ap-\nplications in speech recognition. In Proceedings of the\nIEEE International Conference on Acoustics, Speech\nand Signal Processing , volume 2, pages 645–648 vol.2,\nMay 1998.\n[4] Matthew L. Cooper and Jonathan Foote. Automatic\nmusic summarization via similarity analysis. In Pro-\nceedings of the International Conference on Music In-\nformation Retrieval (ISMIR) , 2002.[5] Jonathan Foote. Automatic audio segmentation using a\nmeasure of audio novelty. In Proceedings of the IEEE\nInternational Conference on Multimedia and Expo ,\n2000.\n[6] Masataka Goto. Chorus-section detecting method for\nmusical audio signals. In Proceedings of IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP) , 2003.\n[7] A.L. Jacobson. Auto-threshold peak detection in phys-\niological signals. In Engineering in Medicine and Bi-\nology Society, 2001. Proceedings of the 23rd Annual\nInternational Conference of the IEEE , volume 3, pages\n2194–2195 vol.3, 2001.\n[8] Florian Kaiser and Geoffroy Peeters. Multiple hy-\npotheses at multiple scales for audio novelty com-\nputation in music. In 38th IEEE International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , Vancouver, Canada, 2013.\n[9] Florian Kaiser and Thomas Sikora. Music structure\ndiscovery in popular music using non-negative matrix\nfactorization. In Proceedings of the 11th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , Aug 2010.\n[10] Holger Kantz and Thomas Schreiber. Nonlinear Time\nSeries Analysis . Cambridge University Press, 2003.\n[11] M. Levy and M. Sandler. Structural segmentation of\nmusical audio by constrained clustering. IEEE Trans-\nactions on Audio, Speech & Language Processing ,\n16(2):318–326, 2008.\n[12] Jouni Paulus and Anssi Klapuri. Music structure anal-\nysis using a probabilistic ﬁtness measure and a greedy\nsearch algorithm. IEEE Transactions on Audio, Speech\n& Language Processing , 17(6):1159–1170, 2009.\n[13] Jouni Paulus, Meinard M ¨uller, and Anssi Klapuri.\nAudio-based music structure analysis. In Proceedings\nof the 11th International Society for Music Information\nRetrieval Conference (ISMIR) , 2010.\n[14] Geoffroy Peeters. Deriving Musical Structures from\nSignal Analysis for Music Audio Summary Generation:\n”Sequence” and ”State” Approach , volume 2771 of\nLecture notes in Computer Science , pages 143–166.\nSpringer, 2004.\n[15] Gabriel Sargent, Frederic Bimbot, and Emmanuel Vin-\ncent. A regularity-constrained viterbi agorithm and its\napplication to the structural segmentation of songs.\nProceedings of the International Society for Music In-\nformation Retrieval Conference (ISMIR) , 2011.\n[16] Joan Serra, Meinard M ¨uller, Peter Grosche, and\nJosep Ll. Arcos. Unsupervised detection of music\nboundaries by time series structure features. In AAAI\nInternational Conference on Artiﬁcial Intelligence ,\n2012.[17] Robert Tibshirani, Guenther Walther, and Trevor\nHastie. Estimating the number of clusters in a data set\nvia the gap statistics. Journal of the Royal Statistical\nSociety, series B , 63:411–423, 2001."
    },
    {
        "title": "QBT-Extended: An Annotated Dataset of Melodically Contoured Tapped Queries.",
        "author": [
            "Blair Kaneshiro",
            "Hyung-Suk Kim",
            "Jorge Herrera",
            "Jieun Oh",
            "Jonathan Berger",
            "Malcolm Slaney"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415756",
        "url": "https://doi.org/10.5281/zenodo.1415756",
        "ee": "https://zenodo.org/records/1415756/files/KaneshiroKHOBS13.pdf",
        "abstract": "Query by tapping remains an intuitive yet underdeveloped form of content-based querying. Tapping databases suffer from small size and often lack useful annotations about users and query cues. More broadly, tapped representations of music are inherently lossy, as they lack pitch information. To address these issues, we publish QBT-Extended—an annotated dataset of over 3,300 tapped queries of pop song excerpts, along with a system for collecting them. The queries, collected from 60 users for 51 songs, contain both time stamps and pitch positions of tap events and are annotated with information about the user, such as musical training and familiarity with each excerpt. Queries were performed from both short-term and longterm memory, cued by lyrics alone or lyrics and audio. In the present paper, we characterize and evaluate the dataset and perform initial analyses, providing early insights into the added value of the novel information. While the current data were collected under controlled experimental conditions, the system is designed for large-scale, crowdsourced data collection, presenting an opportunity to expand upon this richer form of tapping data.",
        "zenodo_id": 1415756,
        "dblp_key": "conf/ismir/KaneshiroKHOBS13",
        "keywords": [
            "intuitive",
            "underdeveloped",
            "content-based",
            "tapping",
            "databases",
            "small size",
            "useful annotations",
            "lossy",
            "queries",
            "pop song excerpts"
        ],
        "content": "QBT-EXTENDED: AN ANNOTATED DATASET OF MELODICALLY\nCONTOURED TAPPED QUERIES\nBlair Kaneshiro Hyung-Suk Kim Jorge Herrera\nJieun Oh Jonathan Berger\nCCRMA, Stanford University\nfblairbo,hskim08,jorgeh,jieun5,brg g@ccrma.stanford.eduMalcolm Slaney\nMicrosoft Research\nCCRMA\nmalcolm@ieee.org\nABSTRACT\nQuery by tapping remains an intuitive yet underdevel-\noped form of content-based querying. Tapping databases\nsuffer from small size and often lack useful annotations\nabout users and query cues. More broadly, tapped rep-\nresentations of music are inherently lossy, as they lack\npitch information. To address these issues, we publish\nQBT-Extended—an annotated dataset of over 3,300 tapped\nqueries of pop song excerpts, along with a system for col-\nlecting them. The queries, collected from 60 users for 51\nsongs, contain both time stamps and pitch positions of tap\nevents and are annotated with information about the user,\nsuch as musical training and familiarity with each excerpt.\nQueries were performed from both short-term and long-\nterm memory, cued by lyrics alone or lyrics and audio. In\nthe present paper, we characterize and evaluate the dataset\nand perform initial analyses, providing early insights into\nthe added value of the novel information. While the current\ndata were collected under controlled experimental condi-\ntions, the system is designed for large-scale, crowdsourced\ndata collection, presenting an opportunity to expand upon\nthis richer form of tapping data.\n1. INTRODUCTION\nQuery by tapping (QBT) is the process of identifying a mu-\nsical excerpt based upon a tapped representation. QBT is\na canonical Music Information Retrieval (MIR) task and\nan intuitive query to perform [16], yet the literature on this\ntopic remains small relative to other query forms such as\nsinging and humming [9]. This task is also among the\nleast attempted in recent years of MIREX [3]. A num-\nber of retrieval systems and databases using rhythm or tap-\nping have been published to date (Table 1). Some cite the\nneed for larger datasets for testing and validation [4–6].\nSome lack annotations, such as musical ability of the per-\nformer, or the performer’s familiarity with the excerpt be-\ning queried; such annotations could prove useful in devel-\noping improved systems [11, 12, 16]. It is also not always\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.\ncue (audio + lyrics) query (tap data + user info) query (tap data + user info) \nqueries \nusers \nsongs \nDATABASE DATA-COLLECTION SYSTEM (future work) QUERY  SYSTEM \nQBT-Extended DATASET \nresult (song) \nground truth \n+ Figure 1 : QBT-Extended system and dataset overview.\nclear how the performer was cued to perform the query\n(from short-term or long-term memory; or from score,\nlyrics, or audio), nor how the type of cue affected perfor-\nmance.\nStudy Songs Performers Queries\nChen & Chen, 1998 [1] 102 NA NA\nJang et al., 2001 [8] NA 9 269\nEisenberg et al., 2004a [4] 9 3 27\nEisenberg et al., 2004b [5] 9 4 144/288\nPeters et al., 2005 [15] 30 NA NA\nPeters et al., 2006 [16] 30 NA 518\nHanna & Robine, 2009 [6] 103 6 533\nMIREX: MIR-QBT1136 NA 890\nMIREX: QBT symbolic1143 NA 410\nCurrent study 51 60 3,3652\nTable 1 : Size of published QBT datasets to date. NA indicates\nthat information was not available.\nAnother possible hurdle in QBT research is that tapping\ndata are inherently lossy. A performer is likely replaying\nthe pitches of a melody in his head as he taps its rhythm,\nbut this experienced information is not captured in the out-\nput [14]. Confounding the issue further is the fact that mu-\nsical excerpts can share similar or identical rhythms while\nbeing vastly different melodically (consider “Happy Birth-\nday” and “The Star-Spangled Banner” as examples). On\n1http://www.music-ir.org/mirex/wiki/Query_by_Tapping\n2Numbers reported for the current dataset reﬂect those queries for\nwhich at least one tap event was registered. With zero-tap queries (for\nskipped tasks) included, the dataset comprises 3,943 queries.a perceptual level, too, human recognition of musical ex-\ncerpts is generally less successful using rhythm alone than\nmelody alone, regardless of a listener’s level of musical\ntraining [7]. Therefore, melodic information could be a\nuseful addition to the QBT signal.\nWith the goal of facilitating future QBT research, we\npublish a dataset of annotated queries, and a system for col-\nlecting them. The queries were collected from 60 unique\nparticipants performing excerpts from a set of 51 songs. As\nthe queries were performed on the 2D touchscreen of a mo-\nbile device, we were able to collect not only the timestamps\nof tap onset (touch on) and release (touch off) events, but\nalso a rough melodic contour based upon the position of\neach tap on the screen. Participants were cued from ei-\nther long-term memory (lyrics only) or short-term mem-\nory (lyrics and audio) for a given query task. Finally, the\ntapped queries are annotated with information about the\nperformer, including musical training and the level of fa-\nmiliarity with each excerpt.\nThe current dataset was collected under controlled ex-\nperimental conditions. However, the system, being mobile\nand open source, is easily extendable to crowdsourced data\ncollection.\nThe remainder of this paper is structured as follows. We\nﬁrst explain how we devised the system and collected data\n(x2). We then describe the data ( x3) and perform illustra-\ntive analyses for QBT application ( x4). We conclude with\na discussion of implications and next steps ( x5).\n2. DATA COLLECTION\n2.1 Stimulus Set\nWe wished to maximize the number of queries that could\nbe performed from long-term memory (cued by lyrics only).\nTo assemble a set of songs that would be maximally famil-\niar based upon lyrics alone, we conducted a survey to as-\nsess familiarity of lyrics excerpts from 120 top British and\nAmerican pop songs from 1950–2010. Songs were chosen\nbased upon their presence in a variety of Top 10 lists from\neach year, and lyrics were drawn from songs’ main themes\nand choruses. Cued by lyrics, participants rated on a 3-\npoint scale whether they knew the accompanying melody\n(Yes, Maybe/parts of it, No). We targeted the same demo-\ngraphic for the survey as we would for subsequent QBT\ndata collection.\nFifty-ﬁve participants (born between 1929–1994; mean\nbirth year 1985; 27 female) completed the survey. Each\nsong was scored (# Yes responses - # No responses), and\nthe 49 highest-scoring songs were retained for the QBT-\nExtended stimulus set. We additionally included “Happy\nBirthday” and “The Star-Spangled Banner” to illustrate the\nsame-rhythm/different-melody phenomenon. The result-\ning 51 audio excerpts ranged in length from 9 to 28 sec\n(mean length 16.96 sec).\n2.2 Participants\nTapping data were collected from 60 participants; partic-\nipant information is summarized in Table 2. All partic-ipants were ﬂuent in English, at least 18 years old, and\nreported normal hearing and no cognitive or decisional im-\npairments. Informed consent was obtained from each par-\nticipant at the start of his session. Both the survey and tap-\nping study were approved by the local Institutional Review\nBoard (IRB).\nData Value Stored Range Collected\nage integer 18-64 (mean = 30.8)\ngender male/female 33/27\nnative language text English = 53\nmusic listening 0 (never) to 5 (all the time) mean = 3.91\ninstrument training 0 (none) to 5 (professional) mean = 2.53\ntheory training 0 (none) to 5 (professional) mean = 2.08\nhandedness left/right/both 3/55/2\ntone-deafness yes/no/don’t know 1/58/1\narrhythmic yes/no/don’t know 0/59/1\nspeciﬁc training time and instrument varies\nTable 2 : Participant information collected from in-app question-\nnaire. Values for music listening ,instrument training , and theory\ntraining are continuous in the given range. Multiple answers (in-\nstruments) per user were allowed for speciﬁc training .\n2.3 System\nThe system comprises a front end for data collection and\na back end for data storage and processing/analysis. An\niOS application was developed for the front end so that\nparticipants could leverage the 2D touchscreen for queries,\ntapping higher on the screen for higher pitches, and lower\nfor lower pitches. A screenshot of the tap screen is shown\nin Figure 2. Our internal tests show a tap-to-timestamp la-\ntency of 40 ms on average, with standard deviation of 10\nms. Because the start time of each recorded query is set to\nthe onset of the ﬁrst tap event, mean latency is not a factor.\nThe front-end application was also used to obtain informed\nconsent, and for the participant questionnaire ( x2.4). The\nmobile implementation facilitated data collection and pro-\nvided an ecologically valid apparatus that would extend\neasily to real-life use of a QBT system.\nFigure 2 : The data-collection application’s tapping interface.\nUsers can tap anywhere in the shaded gray area of the screen,\nusing the vertical position of the tap to denote pitch height.The back-end application was written in Ruby-on-Rails\nto receive and store the collected data. The data are stored\nin an SQLite3 database. The back end also stores the audio\nand lyrics ﬁles that are fetched by the front end when a new\nexperiment is instantiated.\n2.4 Data Collection Procedure\nAll data were collected using 4th Generation iPod Touch\ndevices and Sony MDR-V6 headphones. Participants\nstarted the session by giving informed consent and ﬁlling\nout the questionnaire (Table 2). Following this, the par-\nticipant completed 3 practice trials in order to learn how\nto use the application and perform queries, with the ex-\nperimenter on hand to provide instruction and clariﬁcation.\nOnce the participant was comfortable with the interface, he\nperformed up to 51 trials in random order for the remainder\nof the 45-minute session.3Participants were given a $10\ngift card at the end of the session. No authors contributed\nto the dataset.\nA single trial is described as follows:\n1. A lyrics excerpt, along with the song title, performer,\nand year, is presented on screen.\n2. Long-term memory task: The participant is asked to\ntap the melody accompanying the lyrics if it is fa-\nmiliar, using the vertical axis to denote approximate\npitch positions. If the user cannot recall the melody,\nhe skips this step.\n3. The participant is asked “How familiar was the song\npresented?” The answer is encoded as a continuous\nvalue from 0 to 5.\n4. The participant listens to the audio accompanying\nthe excerpt. The audio plays only once, and must\nbe heard in its entirety. The lyrics and metadata are\nshown on screen while the audio plays.\n5. Short-term memory task: The participant is taken\nback to the lyrics/metadata screen (described in Step\n1) and taps the melody (regardless of whether he was\nable to do so from long-term memory).\n6. The participant is asked “Did hearing the music help?\nTap on the answer that ﬁts best.” The answer op-\ntions, and distribution of responses, can be found in\nTable 3.\n3. DATASET\n3.1 Ground Truth\nA ground truth was created for each excerpt by a single\nperformer with 14 years of piano training. MIDI keyboard\nrenditions of the sung melodies in the excerpts were con-\nverted to comma-separated value (CSV) ﬁles with MIDI\nnote number, note-on, and note-off times. Both MIDI and\nCSV formats are included in the dataset.\n3Some participants requested to perform queries for all of the songs,\ntaking more than 45 minutes to complete the set.3.2 Statistics of the Collected Data\nA total of 3,365 queries were collected—1,412 tapped from\nlong-term memory (cued by lyrics only) and 1,953 from\nshort-term memory (cued by lyrics and audio). For each\nsong, an average of 27.69 long-term memory queries (min\n= 16, max = 37) and 38.29 short-term memory queries\n(min = 31, max = 47) were collected. Each participant\nperformed on average 23.53 queries from long-term mem-\nory (min = 0, max = 51) and 32.55 queries from short-term\nmemory (min = 20, max = 51).\n3.3 Structure\nThe database contains 3 tables: songs ,users (participants),\nand tasks . The ﬁelds of each table are summarized in\nTable 4. The songs table contains information about the\nsongs in the stimulus set. Due to possible copyright is-\nsues, the dataset does not include lyrics or audio, and only\nspeciﬁes the start and end times within the original song,\nas well as the song part from which the lyrics are derived\n(main theme, chorus, or other). The users table contains\nthe participant information summarized in Table 2. The\ntasks table contains the information for each query includ-\ninguser idandsong title , which can be used to\nidentify the participant and song of a given query.\nsongs (7 ﬁelds)\nﬁlename song title artist\nyear start time end time\nsong section\nusers (11 ﬁelds)\nage gender listening habits\ninstrument training theory training handedness\ntone deaf arrhythmic user id\nnative language speciﬁc training\ntasks (16 ﬁelds)\nversion number song title user id\nsession id experimenter id task order\ndevice type song familiarity with music\naudio helpful tap data tap offdata\ntapxdata tap ydata tap offxdata\ntapoffydata\nTable 4 : Database table ﬁelds. The song title ﬁeld connects\nthesongs andtasks tables, while the user idﬁeld connects the\nusers andtasks tables.\nThe dataset is available as 1) an SQLite3 db-ﬁle;\n2) comma-separated value (CSV) ﬁles; and 3) space-\nseparated .onset ﬁles compatible with previous QBT\ndatasets. In addition to .onset ﬁles for a given task, we\nprovide in separate ﬁles the xandyscreen coordinates and\nrelease times corresponding to each onset.\nThe dataset is distributed using the Creative Commons\nAttribution-NonCommercial-ShareAlike 3.0 license, and\nis available for download at the following URL:\nhttps://ccrma.stanford.edu/groups/qbtextended\n3.4 Observations of the Data\nWe present two example visualizations of the tapped\nqueries. First, a rough melodic contour can be recon-Did hearing the music help? With long-term Without long-term\nYes—it helped me remember more details of the song 70.0 % 30.6 %\nYes—I thought the lyrics were from a different song, but now I know which song it is 0.9 % 4.0%\nYes—I had no idea of the song from just the lyrics, but listening made me recognize it 2.3 % 26.1 %\nNo—I already knew the song really well 24.7 % 1.5%\nNo—this song is totally unfamiliar, so hearing it once didn’t help 0.4 % 10.1 %\nYes—I didn’t know the song at all, but I could tap it out after hearing it 1.6 % 27.7 %\nTable 3 : Distribution of answers to the question asked at the end of the short-term memory task ( x2.4). The second column shows\nthe distribution for short-term memory queries where the participant also did the long-term memory task; the third column presents the\ndistribution for cases where the participant performed the query from short-term, but not long-term, memory.\nstructed by plotting each tap position as a function of its\nonset time. Figure 3 shows the short-term memory queries\nfor “Happy Birthday” from participants in the top and bot-\ntom quartiles of musical instrument training for the song.\nThe ground truth is overlaid in red. Both the query lengths\n(x-axis) and tap positions ( y-axis) have been normalized\nto the length of each query and total vertical range of the\nscreen used, respectively.\n+ ++\n++\n+\n+ ++\n++\n+\n+ ++\n+\n+\n+\n++ +\n+\n++\n+\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nOnset (normalized)Pitch (normalized)\n+ ++\n++\n+\n+ ++\n++\n+\n+ ++\n+\n+\n+\n++ +\n+\n++\n+\n(a) Highest quartile of instrument training [3.5–5.0]\n+ ++\n++\n+\n+ ++\n++\n+\n+ ++\n+\n+\n+\n++ +\n+\n++\n+\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nOnset (normalized)Pitch (normalized)\n+ ++\n++\n+\n+ ++\n++\n+\n+ ++\n+\n+\n+\n++ +\n+\n++\n+\n(b) Lowest quartile of instrument training [0.0–2.0)\nFigure 3 : Tapped pitch contours of “Happy Birthday” queries\nfrom short-term memory (blue) with ground truth (red). Pitch\ncontours and timestamps have been normalized to the vertical\nrange of the screen used and the total length of the query, respec-\ntively. Variance among queries appears to be lower for the highly\ntrained participants, especially in the second half of the query.\nIn contrast, Figure 4 focuses solely on the temporal di-\nmension of the queries. More variability in the temporal\npatterns is evident when absolute tap times are presented\n(Figure 4a), but the phrase structure becomes easier to dis-\ncern across the set when each query is normalized by its\nlength (Figure 4b).\nWe include some anecdotal observations from the ex-\nperiment sessions:\n1.The tapping queries were fun to do. This observa-\ntion is supported by some participants requesting to\nﬁnish the full set of songs; but even participants who\ndid not ﬁnish the full set reported that they had fun.\n2.The pitch positions are not exact representations of\nthe melody being performed. For example, repeatedpitches in a melody were likely not tapped at the\nexact same position on the screen; similarly, recur-\nring taps in a given position do not necessarily re-\nﬂect the same pitch. In addition, the range of the\nmelody relative to the range of the screen may have\nchanged over the course of a query, as participants\ncould encounter notes outside of the range they had\naccounted for initially.\n3.Participants with less musical training reported dif-\nﬁculty with the pitch dimension. Some participants\nreporting this problem preferred to tap in the same\ngeneral area of the screen, while others tapped mono-\ntonically up the screen for each line of lyrics. As\nevidenced in Figure 3, degradation of the pitch con-\ntour is observable among participants with lower re-\nported levels of instrument training.\n4.No participants reported trouble with the rhythm di-\nmension. More analysis is needed to conﬁrm this\nobservation.\n5.Many participants reported often not knowing a\nmelody from the lyrics alone, but recognizing it once\nthe audio started playing. Evidence of this can be\nseen in Table 3, as 26.1% of participants who could\nnot do the long-term memory task actually did know\nthe song once they heard the audio.\n6.Some participants reported that their instrument ex-\nperience (e.g., guitar, drums) distracted them from\nthe vocal line in the audio, and that they wanted to\ntap their instrument’s part instead. More analysis is\nneeded to conﬁrm this observation.\n4. PRELIMINARY ANALYSIS\nAlthough the main focus of this paper is to introduce QBT-\nExtended, in this section we perform some preliminary\nanalyses of the dataset to build a rudimentary QBT system.\nWe acknowledge that these analyses are very basic, and\nthey are intended primarily for illustrative purposes and to\nvalidate the data.\nWe validated the temporal dimension of the dataset us-\ning Rhythmic Contour String encoding [16] for both the\nground truth and the queries.4All queries, as well as\n4This encoding normalizes onset-to-onset durations relative to\nmean inter-onset duration, then converts each inter-onset duration to S\n(“same”), U (“up”), or D (“down”) using a threshold of “sameness” to\ndeﬁne S. We used a threshold of 0.2.0 5000 10000 15000 20000 25000 30000QueriesHappy Birthday\nOnset Time (ms)Long Term\nShort Term\nGround Truth(a) Not normalized\n0.0 0.2 0.4 0.6 0.8 1.0QueriesHappy Birthday\nOnset Time (normalize)Long Term\nShort Term\nGround Truth\n(b) Normalized\nFigure 4 : Temporal dimension of all queries collected for\n“Happy Birthday”.\nthe ground truth, are represented as separate strings. We\nthen used edit distance (or Levenshtein distance) between\nstrings, computed by the approximate matching procedure\n[16], to rank the distance between a query and each of\nthe ground-truth strings. The distances were computed\nover the full set of 51 candidate excerpts, and results were\nranked by increasing order of distance. Table 5 contains\nthe classiﬁcation accuracy for the entire dataset against the\nground-truth data. A query result is considered correct if\nthe actual song was among the top Nsorted matches.\nWe adapted the Rhythmic Contour String method to an-\nalyze the novel pitch dimension. Because the pitch infor-\nmation expressed in the queries is not exact ( x3.4), we en-\ncoded pitch contours as separate strings based upon note-\nto-note variance relative to the overall position range of\nthe query. Differences in position were computed between\nsuccessive tap events, and the set of differences for a given\nquery were normalized between 1 and -1. Following that,\nthe same 0.2 thresholding was used. We then applied the\napproximate matching procedure to compare distances be-\ntween a query and each ground truth, and ranked the re-\nsults.Each query therefore comprises two strings; one for\nrhythm and one for melody. As a preliminary attempt to\nmake use of both representations, we computed the rhyth-\nmic and melodic distances separately and averaged them,\ngiving equal weighting to each.\n4.1 Assessing Performance with Added Pitch\nDimension\nThe accuracy of our simple system using each dimen-\nsion alone, and rhythm and melody combined, is shown\nin Table 5. These results were computed using the entire\nset of queries, and no ﬁltering was applied based on user\nability or familiarity with each song excerpt. Rhythm alone\noutperforms melody alone for all three ranking ranges.\nAccuracy (%)\nCondition Top 1 Top 5 Top 10\nRhythmic contour 51.92 70.82 78.46\nMelodic contour 37.68 54.27 64.67\nBoth contours 53.67 70.70 78.28\nSigniﬁcance analysis\n\u001f2(1;N= 3;365) 4.69 0.017 0.060\np-value 0.030 0.90 0.81\nTable 5 : Accuracy of the simple QBT classiﬁcation system using\nall 3,365 tapped queries (51-class problem). \u001f2andp-values,\ncomparing classiﬁcation using rhythmic contour alone versus\nrhythmic and melodic contours combined, were computed using\nMcNemar’s test.\nTo quantitatively assess the effect of adding the pitch\ndimension, we used McNemar’s test [13], an established\nmethod of comparing two classiﬁcations of a single dataset\n[2]. We compared performance when the classiﬁer used\nonly rhythmic contour, versus rhythmic and melodic con-\ntours together. The tests show that adding melodic contour\nsigniﬁcantly improved accuracy for the Top 1 case, but did\nnot signiﬁcantly affect classiﬁer performance in the Top 5\nand Top 10 cases.\nA speciﬁc case in which melodic information should\nboost accuracy is the 2-class problem of “Happy Birthday”\nversus “The Star-Spangled Banner”, which share similar\nrhythms. As shown in Table 6, using rhythmic and melodic\ncontour together signiﬁcantly improved classiﬁer accuracy\nover using rhythmic contour alone.\nCondition Accuracy (%)\nRhythmic contour 75.54\nMelodic contour 72.66\nBoth contours 89.93\nSigniﬁcance analysis\n\u001f2(1;N= 139) 10.03\np-value 0.0015\nTable 6 : Comparison of accuracies in the 2-class problem\nclassifying “Happy Birthday” and “The Star-Spangled Banner”\nqueries. McNemar’s test measures the signiﬁcance of the change\nin classiﬁer performance when both rhythmic and melodic con-\ntour are used, versus rhythmic contour alone.5. DISCUSSION\nThe QBT-Extended dataset and system presents new pos-\nsibilities for QBT research. By appropriately applying\nthe new pitch position information, and by understanding\nhow user background and memory cue affect performance,\nmore effective QBT systems may be implementable. In\naddition, the touchscreen-based interface for data collec-\ntion may prove useful for users who are not comfortable\nsinging or humming, or who wish to query in situations\nwhere use of the microphone for acoustic input is not ideal\n(for instance, in a library or a noisy bar).\nWe acknowledge limitations of the current dataset. First,\nour choice of device for data collection imposed constraints\nupon the range of vertical space available, and users may\nhave run out of room or needed to tap over the lyrics for\nsome queries. In addition, the act of expressing the pitch\ndimension was confusing for some users, especially those\nwho did not have musical training or could not read mu-\nsic. Therefore, it may be the case that having to focus on\nboth timing and pitch degraded the quality of output along\nboth dimensions. As we noticed that some sliders were not\nmoved, nor instrument training ﬁelds ﬁlled out in the ques-\ntionnaire, some users may not have entered their informa-\ntion completely. Finally, we acknowledge the demographic\nskew of the current participant population for this dataset,\ngiven the community that we targeted for the study [10].\n5.1 Future Work\nMany opportunities for future work are present. First, more\nanalysis can be done to evaluate the usefulness of both the\nadded pitch dimension and the annotations accompanying\neach query. For example, it may be useful to weight the\ntemporal versus pitch dimensions of a query based upon\nusers’ musical expertise, experience, and familiarity with\nthe speciﬁc excerpt. Alternate representations of queries,\nsuch as the normalized signals shown in Figure 3, could\nalso provide feasible feature vectors for classiﬁcation. Be-\nyond the domain of query, the dataset is potentially useful\nfor research on musical memory, expertise, and other as-\npects of music cognition.\nBecause the data-collection system is open source, a\nnatural extension of the current implementation would be\nto port it to other platforms (e.g., Android, web [16]) and\ndevices (e.g., tablets). The system is also well positioned\nfor crowdsourced data collection, and the current dataset\ncould then serve as a control to validate the quality of data\ncollected via crowdsourcing.\n6. REFERENCES\n[1] James CC Chen and Arbee LP Chen. Query by rhythm: An\napproach for song retrieval in music databases. In Research\nIssues In Data Engineering, 1998. Proceedings of Eighth\nInternational Workshop on Continuous-Media Databases\nand Applications , pages 139–146. IEEE, 1998.\n[2] Janez Dem ˇsar. Statistical comparisons of classiﬁers over\nmultiple data sets. J. Mach. Learn. Res. , 7:1–30, December\n2006.[3] J Stephen Downie, Andreas F Ehmann, Mert Bay, and\nM Cameron Jones. The music information retrieval\nevaluation exchange: Some observations and insights. In\nAdvances in music information retrieval , pages 93–115.\nSpringer, 2010.\n[4] Gunnar Eisenberg, Jan-Mark Batke, and Thomas Sikora.\nBeatBank – an MPEG-7 compliant query by tapping system.\nInAudio Engineering Society Convention 116 , 2004.\n[5] Gunnar Eisenberg, Jan-Mark Batke, and Thomas Sikora.\nEfﬁciently computable similarity measures for query by\ntapping systems. In Proceedings of the Seventh International\nConference on Digital Audio Effects (DAFx’04), Naples,\nItaly, October , pages 189–192, 2004.\n[6] Pierre Hanna and Matthias Robine. Query by tapping system\nbased on alignment algorithm. In Acoustics, Speech and\nSignal Processing, 2009. ICASSP 2009. IEEE International\nConference on , pages 1881–1884. IEEE, 2009.\n[7] Sylvie H ´ebert and Isabelle Peretz. Recognition of music in\nlong-term memory: Are melodic and temporal patterns equal\npartners? Memory & cognition , 25(4):518–533, 1997.\n[8] Jyh-Shing Roger Jang, Hong-Ru Lee, and Chia-Hui Yeh.\nQuery by tapping: A new paradigm for content-based music\nretrieval from acoustic input. In Advances in Multimedia\nInformation ProcessingPCM 2001 , pages 590–597.\nSpringer, 2001.\n[9] Alexios Kotsifakos, Panagiotis Papapetrou, Jaakko Hollm ´en,\nDimitrios Gunopulos, and Vassilis Athitsos. A survey of\nquery-by-humming similarity methods. In Proceedings of\nthe 5th International Conference on PErvasive Technologies\nRelated to Assistive Environments , page 5. ACM, 2012.\n[10] Jin Ha Lee and Sally Jo Cunningham. The impact (or\nnon-impact) of user studies in music information retrieval. In\nProceedings of the 13th International Society for Music\nInformation Retrieval Conference , pages 391–396, 2012.\n[11] Micheline Lesaffre, Koen Tanghe, Ga ¨etan Martens, Dirk\nMoelants, Marc Leman, Bernard De Baets, Hans De Meyer,\nand Jean-Pierre Martens. The MAMI query-by-voice\nexperiment: Collecting and annotating vocal queries for\nmusic information retrieval. In Proceedings of the\nInternational Society for Music Information Retrieval\nConference , 2003.\n[12] Mark Levy. Improving perceptual tempo estimation with\ncrowd-sourced annotations. Proceedings of the International\nSociety for Music Information Retrieval Conference , pages\n317–322, 2011.\n[13] Quinn McNemar. Note on the sampling error of the\ndifference between correlated proportions or percentages.\nPsychometrika , 12(2):153–157, June 1947.\n[14] Elizabeth Louise Newton. The rocky road from actions to\nintentions . PhD thesis, Stanford University, 1990.\n[15] Geoffrey Peters, Caroline Anthony, and Michael Schwartz.\nSong search and retrieval by tapping. In Proceedings of the\nNational Conference on Artiﬁcial Intelligence , volume 20,\npage 1696. Menlo Park, CA; Cambridge, MA; London;\nAAAI Press; MIT Press; 1999, 2005.\n[16] Geoffrey Peters, Diana Cukierman, Caroline Anthony, and\nMichael Schwartz. Online music search by tapping. In\nAmbient Intelligence in Everyday Life , pages 178–197.\nSpringer, 2006."
    },
    {
        "title": "Empirical Analysis of Track Selection and Ordering in Electronic Dance Music using Audio Feature Extraction.",
        "author": [
            "Thor Kell",
            "George Tzanetakis"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415654",
        "url": "https://doi.org/10.5281/zenodo.1415654",
        "ee": "https://zenodo.org/records/1415654/files/KellT13.pdf",
        "abstract": "Disc jockeys are in some ways the ultimate experts at selecting and playing recorded music for an audience, especially in the context of dance music. In this work, we empirically investigate factors affecting track selection and ordering using DJ-created mixes of electronic dance music. We use automatic content-based analysis and discuss the implications of our findings to playlist generation and ordering. Timbre appears to be an important factor when selecting tracks and ordering tracks, and track order itself matters, as shown by statistically significant differences in the transitions between the original order and a shuffled version. We also apply this analysis to ordering heuristics and suggest that the standard playlist generation model of returning tracks in order of decreasing similarity to the initial track may not be optimal, at least in the context of track ordering for electronic dance music.",
        "zenodo_id": 1415654,
        "dblp_key": "conf/ismir/KellT13",
        "keywords": [
            "DJ-created mixes",
            "electronic dance music",
            "track selection",
            "automatic content-based analysis",
            "track order",
            "statistically significant differences",
            "playlist generation",
            "heuristics",
            "standard model",
            "decreasing similarity"
        ],
        "content": "EMPIRICAL ANALYSIS OF TRACK SELECTION AND ORDERING IN\nELECTRONIC DANCE MUSIC USING AUDIO FEATURE EXTRACTION\nThor Kell\nIDMIL, CIRMMT, McGill University\nthor.kell@mail.mcgill.caGeorge Tzanetakis\nUniversity of Victoria\ngtzan@cs.uvic.ca\nABSTRACT\nDisc jockeys are in some ways the ultimate experts at\nselecting and playing recorded music for an audience, es-\npecially in the context of dance music. In this work, we\nempirically investigate factors affecting track selection and\nordering using DJ-created mixes of electronic dance mu-\nsic. We use automatic content-based analysis and discuss\nthe implications of our ﬁndings to playlist generation and\nordering. Timbre appears to be an important factor when\nselecting tracks and ordering tracks, and track order itself\nmatters, as shown by statistically signiﬁcant differences in\nthe transitions between the original order and a shufﬂed\nversion. We also apply this analysis to ordering heuristics\nand suggest that the standard playlist generation model of\nreturning tracks in order of decreasing similarity to the ini-\ntial track may not be optimal, at least in the context of track\nordering for electronic dance music.\n1. INTRODUCTION\nThe invention of recording lead to the possibility of select-\ning recorded music to entertain a group of people. The\nidea of listening to records instead of listening to bands\ntook off after the second world war, when sound systems\nand record players began to appear in night clubs and cafes\nin New York, Jamaica, London, Paris, and beyond [5].\nSince then, the disk jockey (DJ) has evolved from a\nsimple selector and orderer of music into a sophisticated\nperformer with considerable skill and training. Although\nthese performance aspects are compelling, the primary fo-\ncus of this paper is the basic selection and ordering of mu-\nsic. DJs generally bring a limited amount of their music\ncollection to any given gig, and play a reasonably large\nsubset of it. Two important questions to consider are ‘What\ntracks go into a playlist?’, and ‘What is the best ordering\nof these tracks?’. Track ordering is not a well understood\nprocess, even by DJs. Many DJs will say only that two\ntracks ‘work’ or ‘do not work’ together, and not be able\nto comment further. [5] We investigate this selection and\nordering process in terms of the automatically computed\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.similarity between tracks, in terms of features represent-\ning timbre, key, tempo and loudness. The source data for\nthis investigation is the British Broadcasting Corporation’s\nEssential Mix radio program1. Broadcast since 1993, the\nEssential Mix showcases exceptional DJs of various genres\nof electronic dance music (EDM), playing for one or two\nhours. It is considered one of the most reputable and in-\nﬂuential radio programs in the world. By investigating the\nrelationships between tracks in DJ sets we hope to better\nunderstand track selection by DJs and inform the design of\nalgorithms and audio features for automatic playlisting.\nThe automatic estimation of music similarity between\ntwo tracks has been a primary focus of music information\nretrieval (MIR) research. [4] Several methods for comput-\ning music similarity have been proposed based on content-\nanalysis, metadata (such as artist similarity, web reviews),\nand usage information (such as ratings and download pat-\nterns in peer to peer networks). [1] Music similarity is the\nbasis of query-by-example which is a fundamental MIR\ntask, and also one of the ﬁrst tasks explored in MIR litera-\nture. In this paradigm the user submits a query consisting\nof one or more ‘seed’ pieces of music, sometimes also in-\ncluding metadata and user preferences. The system then\nresponds by returning a playlist of music pieces ranked\nby their similarity to the query, and set in some order. In\ncontrast, our approach is analytic. Rather than generating\nplaylists, we investigate existing DJ sets through audio fea-\nture extraction and examine the transitions between tracks\nin terms of audio features representing timbre, loudness,\ntempo, and key. We also compare the results of our empir-\nical investigation to common ordering methods, and offer\nsome suggestions for improving current playlisting heuris-\ntics.\n2. RELATED WORK\nEarly MIR work investigating the automatic calculation of\nmusic similarity and how to evaluate different approaches\nformulated a general methodology that is followed by the\nmajority of existing work to this day. In this methodol-\nogy, the primary goal is assessing the relative performance\nof different algorithms for computing music similarity by\nsomehow evaluating the ’quality’ of the generated playlists.\nThe most common approach of generating a playlist is\nto consider the Nclosest neighbors in terms of automat-\nically calculated similarity to a particular query. Several\n1http://www.bbc.co.uk/programmes/b006wkfpautomatic playlists are generated from seed queries repre-\nsenting the desired diversity of the music considered. This\nset of automatically generated playlists is then evaluated,\ntypically using one or both of two approaches: objective\nevaluation using proxy ground truth for relevance, or sub-\njective evaluation through user studies. The basic idea is\nto evaluate a playlist by considering it good if it contains a\nhigh number of ‘relevant items’ to the query [1]. The rele-\nvance ground truth can be provided by users in subjective\nevaluation but this is a time consuming and labor intensive\nprocess that does not scale well.\nObjective evaluation has the advantage that it can scale\nto any number of queries and playlists, as long as the tracks\nhave some associated meta-data that can be used as a proxy\nfor relevance. Common examples of such proxy sources\ninclude artist, genre, and song [1,10]. In addition to music\nsimilarity calculated based on audio content analysis, other\nsources of information such as web reviews, download pat-\nterns, ratings and explicit editorial artist similarity [7] can\nalso be used for estimating music similarity [6]. Playlists\nthemselves have also been used to calculate artist and track\nsimilarity based on co-occurrence. Sources of playlist data\ninclude the Art of the Mix, a website that contains a large\nnumber of hobbyist playlists [4], and listings of radio sta-\ntions [12]. DJ sets remain an untapped resource, however.\nThe most common relevance-based evaluation measures\n(such as precision, recall and F-measure [2]) are borrowed\nfrom text information retrieval and only consider the items\ncontained in the set of returned results, without taking into\naccount their order. The paradigm of a single seed query\nsong creating a list of Nitems ranked by similarity has re-\nmained a common approach to automatic playlisting and\nmusic recommendation. Some notable exceptions in terms\nof ordering include: heuristics about trajectory for the or-\ndering of returned items [10], using song sets instead of\nsingle seeds [11], ordering based on the traveling sales-\nman problem [16], and considering both a start track and\nan end track for the playlist [8]. The assumption of simi-\nlarity has also been challenged by the ﬁnding that in many\ncases users prefer diverse playlists [17] as measured by au-\ntomatic feature analysis. This is the closest work in terms\nof approach to the work described in this paper.\nAnother theme of more recent work has been providing\nmore user control to the process of automatic playlist gen-\neration. If the tracks considered are associated with a rich\nset of attribute/value pairs then techniques from constraint\nsatisfaction programming and inductive learning can be\nused to generate playlists that to some extent optimally sat-\nisfy the user preferences [15]. The ability to control what\nattributes are used for estimating music similarity has also\nbeen investigated [19]. One of the simplest forms of user\ncontrol is skipping behavior, which has been used to it-\neratively improve playlist generation [14]. A more elab-\norate method for steerable playlist generation is based on\ntag clouds and and a music similarity space derived from\nradio station playlists [12].\nPhysiological data such as heart rate has also been in-\nvestigated for playlist generation [13]. The novelty aspectof playlist generation by tracking user listening informa-\ntion has also been explored [9]. A different approach alto-\ngether is to create playlists visually, based on some graph-\nical representation of the music collection [18]. In all of\nthis literature, different approaches to playlist generation\nare also evaluated with a combination of objective mea-\nsures and user studies, comparing different conﬁgurations\nto a random or a simple algorithmic baseline. For example,\na recent study compared two recommender systems (based\non artist similarity and acoustic content) with the Apple\niTunes Genius recommender which is believed to be based\non collaborative ﬁltering [3].\n3. MOTIVATION AND PROBLEM\nFORMULATION\nThe motivation behind our work is to investigate the pro-\ncess of playlist/mix creation by analyzing existing mixes\ncreated by experts - in this case, DJs. Existing work has\nmostly focused on more general playlists created by av-\nerage listeners. Rather than relying on user surveys, we\nfocus on empirical analysis based on audio feature extrac-\ntion. This allows us to investigate what audio attributes DJs\nuse when selecting and order their tracks. We further com-\npare these attributes to collections of random EDM tracks,\nand to artist albums. We speciﬁcally investigate whether\ntrack order matters. We also examine important assump-\ntions that are frequently made by automatic playlist gen-\neration systems. Speciﬁcally, we investigate if ordering\nbased on similarity ranking is a good choice, and if so, in\nwhat manner.\nIn existing literature these assumptions are typically man-\nifested in the design of an automatic playlist algorithm,\nand the results are evaluated through objective or subjec-\ntive approaches. Issues such as the sameness problem in\nplaylists formed from collections of music that do not have\nstylistic diversity, or the playlist drift problem in large di-\nverse collections are also discussed but are not empirically\nsupported [8]. In contrast, our approach is complimentary\nand attempts to test these assumptions directly on existing\nmixes. Our methodology can be also viewed as an em-\npirical musicological approach to understanding how DJs\nselect and order music.\n4. METHODOLOGY\n4.1 Data\nWe obtained 114 Essential Mix DJ sets from the archival\nwebsite themixingbowl.org (DJS). These sets cover\nthree years (2009-2011) of the radio show. In addition, a\ncollection of 189 artist albums ( ALBS ) (from the author’s\nown collection, covering a wide range of music and gen-\nres) was used for comparison purposes. Finally, 100 ran-\ndom EDM ( REDM ) track sets were created by randomly\npicking tracks from the collection of electronic dance mu-\nsic (covering 1,261 tracks) of one of the authors who fre-\nquently performs as a DJ.2In order to investigate track or-\n2It is possible, but unlikely, that there is overlap between the collec-\ntions. If there is overlap, our method of using transitions should make itsdering we created a random shufﬂe of each Essential Mix\nDJ set ( RDJS ).\nThe DJ sets consist of continuous audio, without timing\ninformation about the individual tracks. Each set was split\ninto two-minute exceprts followed by two-minute gaps, giv-\ning 30 excerpts of audio per two hour set. This accounts\nfor any mixing that the DJ might be doing, and factors in\ntrack length. We considered more elaborate manual ap-\nproaches such as extracting each track from each set man-\nually or selecting the most representative part of each track\nfor processing. As this is a very time consuming process\nthis would have severely limited the amount of audio data\nwe would have been able to process.\nThe main issue with the two-minute exceprt approach is\nthat it may miss vital sections, in terms of audio features,\nand include transitions between sections of the same track.\nBy using the exact same approach for albums and for the\nrandomized EDM sets we believe that whatever effects this\narbitrary segmentation has will be approximately the same\nfor all data sets and the relative comparisons we make are\nvalid. It must be noted that the REDM set was not mixed,\nunlike the DJS and RDJS set. However, the audio was con-\ncatenated and the same 2-minute exceprt / 2-minute gaps\nmethodology was applied to it. Thus, some exceprts will\ncontain parts of two tracks, which we hope will approxi-\nmate the impact of mixing.\n4.2 Audio Feature Extraction\nOur goal is to examine different factors affecting selec-\ntion and ordering, based on automatic audio feature extrac-\ntion. More speciﬁcally, we examine the effects of features\nrepresenting timbre, key, loudness, and tempo. The au-\ndio features used for the evaluation were computed using\ntwo sources: the Echo Nest Analyze API3and Marsyas4.\nThe Echo Nest Analyze API returns timbre data as a 12-\ndimensional vector, where each element matches a spectral\ncharacteristic: the ﬁrst dimension is loudness, the second\nindicates a strong attack, and so on. Timbre data is given\nfor each ‘segment’, which roughly corresponds to musical\nevents detected by onsets (on average about half a second\nof audio). The mean of each dimension was taken to sup-\nply a single vector for each 2-minute excerpt of audio. The\nAnalyze API returns loudness as decibels, and tempo as\nbeats per minute. Key is returned as a tuple of pitch class\nand major or minor mode.\nMarsyas returns a 63-dimensional vector for represent-\ning timbre. The features used are based on the Spectral\nCentroid, Roll-Off, Flux and Mel-Frequency Cepstral Co-\nefﬁcients (MFCC). To capture feature dynamics, a running\nmean and standard deviation over the past Mframes of\n23 milliseconds is computed. The features are computed\nat the same rate as the original feature vector but depend\non the past Mframes (e.g. M=40, corresponding approxi-\nmately to a so-called ‘texture window’ of 1 second). This\nresults in a feature vector of 32 dimensions at the same\nimpact minimal.\n3http://echonest.com\n4http://marsyas.inforate as the original 16-dimensional one. The sequence of\nfeature vectors is collapsed into a single feature vector rep-\nresenting the entire audio clip by taking again the mean\nand standard deviation across the track (the sequence of\ndynamics features) resulting in the ﬁnal 64-dimensional\nfeature vector per 2-minute excerpt. For tempo, a method\nbased on autocorrelation of an onset detection function and\nthe creation of a beat histogram is utilized. There is no di-\nrect key estimation implemented in Marsyas.\n4.3 Metrics\nFrom this audio featured data, we characterize the transi-\ntions between successive feature vectors corresponding to\n2-minute excerpts in order to examine track selection dur-\ning the course of a set. Tempo and loudness were simply\nsubtracted, and key was represented by the change in key\nsignature. In order to characterize the transition of the tim-\nbre vectors, we considered both the L1 (Manhattan) dis-\ntance and L2 (Euclidean) distance between successive vec-\ntors after each dimension was max/min normalized across\nthe data set under consideration. The analysis conclusions\nwere similar for the two distance metrics (L1 and L2). Due\nto space limitations, we only report numbers based on the\nL1 metric. Although more elaborate distance metrics be-\ntween the timbre vectors can be devised, we prefer to a use\na simple metric and normalize each feature dimension as\nthis provides a consistent, easily repeatable method.\nAnother objective of our analysis is to investigate the\nimportance of ordering in DJ sets assuming a ﬁxed set of\ntracks (in this case, 2-minute excerpts) to be played. In\norder to characterize different orderings of a set of tracks,\nwe use ideas from combinatorics and permutations. More\nspeciﬁcally, we want to compare excerpt orders created us-\ning different heuristics to the excerpt order of the original\nDJ set. Therefore, we require some measure of similarity\nbetween different permutations.\nWe utilize the concept of inversions, which are a way\nof measuring the differences between an ordered list and a\npermutation of that ordered list. For example, given some\nordered list (e.g. [A, B, C, D, E]) and a permutation of it\n(e.g. [B, C, A, E, D]), an inversion is a pair of positions\nwhere the entries are in the opposite order. The permuta-\ntion in our example has three inversions: (0, 2) for the pair\n(B, A); (1, 2) for the pair (C, A), and (3, 4) for the pair (E,\nD). The number of inversions between two different orderd\nlists of the same items gives a measure of how similar they\nare in terms of ordering. We consider the original DJ set\norder as the original sequence and the sequences created\nby four different ordering heuristics as permutations.\nAssuming a ﬁxed set of excerpts corresponding to a par-\nticular DJ set, we consider the following heuristics: Rank -\nthe excerpts are ranked by increasing distance to the initial\nexcerpt of the original order; NN- each successive excerpt\nis the nearest neighbor of the previous one, without allow-\ning repetitions; Median - the distances of all the clips to the\ninitial excerpt are computed and the one that is closest to\nthe median distance is selected as the next excerpt, without\nallowing repetitions; Furthest Neighbor - each successiveTable 1 . Timbre transition statistics (EN: Echonest, MRS:\nMarsyas)\nMean\u0006Std Q1 Median Q3\nDJS-EN 1.09\u00060.44 0.76 1.02 1.34\nALBS-EN 0.86\u00060.4 0.58 0.80 1.05\nREDM-EN 1.44\u00060.57 1.03 1.39 1.80\nRDJS-EN 1.20\u00060.44 0.87 1.13 1.45\nDJS-MRS 5.75\u00062.02 4.35 5.41 6.82\nALBS-MRS 4.43\u00061.91 3.11 4.11 5.31\nREDM-MRS 6.85\u00062.51 5.18 6.64 8.40\nRDJS-MRS 6.24\u00062.09 4.76 5.9 7.37\nclip is the furthest neighbor of the previous one, without al-\nlowing repetitions. The Rank heuristic corresponds to the\ncommon scenario of ranked list retrieval.\n5. DATA ANALYSIS\n5.1 Transition Analysis\nWe characterize the distribution of transition values (L1\ndistances) for each conﬁguration (DJ sets, artist albums,\nrandom EDM and random shufﬂe DJ sets) by computing\nstatistics (mean, standard deviation, median and quartiles)\nfor each different factor (timbre, tempo, loudness, key).\nIn order to characterize statistical signiﬁcance we use the\nWelch t-test which is appropriate for samples that may have\nunequal variance. All differences reported here are strongly\nstatistically signiﬁcant, with p <0:0001 .\nThe numbers for timbral transitions in Table 15can be\nused to support various assumptions that are commonly\nmade in automatic playlist generation systems. For exam-\nple, the relations between DJS and REDM show that there\nis more to DJ selection than just randomly selecting tracks\nof EDM. The average timbral transition between exceprts\nfor the DJ sets is smaller than the timbral transition be-\ntween exceprts of random EDM tracks. This implies that\nthat DJs try to pick tracks that are similar in timbre, and or-\nder them in ways that further minimize the timbral differ-\nences. The timbral transitions between excerpts of albums\nare the smallest. This is reasonable, as albums tend to be\nsonically coherent, featuring the same instruments and or-\nchestration throughout. Figure 1 shows timbre transition\ndata for single examples of each category. Furthermore, by\ncomparing DJS and RDJS it can be seen that track ordering\nis important: the original ordering results in smaller tran-\nsitions on average than a random shufﬂe of the same clips.\nThese ﬁndings are supported by both the Echo Nest (EN)\nand Marsyas (MRS) feature extraction, increasing our con-\nﬁdence in their validity.\nAnother factor to examine is tempo. As can be observed\nfrom Table 2, DJ sets have the least amount of tempo change.\nTempo is something that can be (and is almost always) con-\ntrolled by the DJ. Therefore it is not surprising that small\n5The numbers reported here are the L1 distance between each succes-\nsive timbre vector, as described in section 4.3Table 2 . Tempo transition statistics (EN: Echonest, MRS:\nMarsyas), Values in BPM.\nMean\u0006Std Q1 Median Q3\nDJS-EN 8.93\u000623 0.03 0.12 1.51\nALBS-EN 17.82\u000620.69 1.87 10 27.92\nREDM-EN 6.53\u000615.25 0.02 0.88 5.05\nRDJS-EN 11.49\u000624.07 0.06 1.04 7.88\nDJS-MRS 3.86\u000613.17 0 0 1\nALB-MRS 20.19\u000622.62 1 11 35.5\nREDM-MRS 7.41\u000616.18 0 0.5 6\nRDJS-MRS 6.6\u000615.01 0 1 4\nTable 3 . Loudness transition statistics. Values in dB.\nLoudness Mean\u0006Std Q1 Median Q3\nDJS 0.76\u00060.71 0.28 0.59 1.06\nALBS 3.39\u00064.13 0.92 2.16 4.32\nREDM 3.18\u00063.07 1.02 2.26 4.37\nRDJS 0.88\u00060.85 0.31 0.67 1.19\ntempo transitions are observed. As expected, due to the\nlarge variety of genres and the unreliability of tempo de-\ntection for some genres, the ALBS dataset shows the high-\nest tempo transitions. The effect of ordering is less pro-\nnounced than in the case of timbre, as can be observed by\ncomparing DJS and RDJS, but is still there. Somewhat\nsurprisingly, the random selection of EDM tracks also ex-\nhibits small tempo transitions, probably due to the consis-\ntent use of a small range of tempi in this style of music\n(House music ranges from 110 BPM to 130 BPM, with a\nlarge peak around 120 BPM, for example). However, note\nthe increase in the median tempo transition from the DJS\ndataset to RDJS and REDM: the tempo transitions for ran-\ndomized DJ sets are similar to those of randomized EDM\ntracks. When a DJ takes control of the tempo, the median\ntransition drops signiﬁcantly.\nLoudness can easily be (and is almost always) controlled\nby the DJ. We expect that DJ sets will be relatively homo-\ngeneous. This is clearly shown by examining the data in\nTable 3, and contrasting DJS with REDM. Ordering also\nhas a small effect, as can be seen by examining the relation\nbetween DJS and RDJS. Thus, DJs appear to vary volume\nslightly over the course of a set. We also examined key\nusing the Echo Nest’s analysis, but did not ﬁnd any stasti-\ncal signiﬁcance in the differences observed. We can thus\nsuggest that DJs do not use key as a primary concern when\nselecting and ordering their tracks - unlike classical musi-\ncians, for whom key and harmony are paramount.\n5.2 Analysis of Ordering Heuristics\nIn addition to transition analysis, we also examined differ-\nent heuristics for ordering playlists and compared them to\nthe ’golden’ order of the original DJ set. Table 4 shows theTable 4 . Average number of inversions per heuristic\nMethod Echo Nest ...Marsyas ...\nHEUR RND HEUR RND\nRanked 186.27 201.10 183.71 192.28\nNN 187.64 202.5 186.77 189.43\nMedian 171.85 203.86 175.89 172.59\nFN 192.71 202.00 191.33 189.35\nEqual-Step 172.75 199.50 170.16 174.71\nresults of this comparison using the number of inversions\nas an estimate of how ’close’ two orderings are. Based on\nthe above transition analysis, timbre is the dominant factor\naffecting playlist selection and ordering. Thus, it is the pa-\nrameter used in Table 4. For each heuristic we report the\naverage number of inversions, across all sets in DJS, com-\nparing the original order of the tracks with the heuristic\norder (HEUR) and as a baseline the number of inversions\ncomparing the original order of the trakcs with the order of\na random shufﬂe (RND). From Table 4 it can be seen that\nall ordering heuristics come closer to the original ordering\nthan to a random shufﬂe, indicating that they are reason-\nable choices.\nOne of the most interesting ﬁndings is that the tradi-\ntional ordering based on ranked similarity is not the best\nheuristic. Both the Median andEqual-step heuristics ap-\npear to be closer to the original set order. This implies that\nconsistent transitions are more important than tracks near\nthe start of the mix being similar to the initial track. Figure\n2 shows the timbral transitions for a speciﬁc DJ set as well\nas two orderings. As can be seen in the middle subﬁgure\ntheRank heuristic results in playlist drift near the end while\ntheMedian heuristic provides more balanced transitions.\n6. DISCUSSION AND FUTURE WORK\nWe have proposed and demonstrated the use of automatic\naudio feature extraction to examine the selection and order\nof tracks in DJ mixes. Our approach is distinct and compli-\nmentary to the traditional approach of generating playlists\nautomatically and evaluating them through proxy ground\ntruth and user studies. It is an analytic approach that uses\nthe playlists as data to be analyzed. We also speciﬁcally\nfocus on DJ selection of EDM tracks rather than music in\ngeneral.\nOur transition analysis has shown timbre to be an im-\nportant attribute used by DJs when selecting and ordering\ntracks. DJ mixes are more timbrally similar than random\nEDM tracks, though not as timbrally similar as artist al-\nbums. Tempo and loudness tend to be controlled by the\nDJ, and this is also reﬂected in our ﬁndings. Our results\nsupport the intuitive idea that DJs tend to play tracks that\nbroadly ‘sound the same’, and ﬁts with the typical state-\nment by DJs that two tracks ’work’ together, although there\nare probably many more subtle factors involved in DJ track\nselection and ordering. The results also support the em-\nphasis on timbral similarity that is common in automaticplaylist generation systems. Our ﬁndings are consistent be-\ntween the two different audio feature front-ends and con-\nﬁrm design choices that have been made in music recom-\nmendation and automatic playlist generation systems.\nThe order of the selected tracks matters, as shown by\nstatistically signiﬁcant differences in the transitions between\nthe original order and a shufﬂed version. These order-\ning differences were found in all factors considered except\nkey. The investigation of ordering heuristics implies that\nthe standard playlist generation model of returning tracks\nin order of decreasing similarity to the initial track may not\nbe optimal (at least in the context of DJ ordering for EDM).\nReturning results ranked by similarity may not be optimal,\nand transitions of roughly equal size are probably a better\nchoice for automatic playlist generation algorithms.\nFuture work includes the analysis of more data, such as\nthe total 900 Essential Mixes rather than 114 mixes consid-\nered here. Commercial mixes are also a possibility, as are\nDJ mixes from the wider internet. We would also like to\nfollow a similar approach to the analysis of playlists across\na variety of genres, as well as playlists created by every-\nday listeners and music recommendation systems. In terms\nof informing automatic playlist generation algorithms, the\nmost promising direction is to investigate the effectiveness\nof ordering heuristics that emphasize smoothness of tran-\nsitions rather than absolute ranking.\nTrack selection and ordering is a tricky process that is\nnot totally understood even by DJs themselves: It is hoped\nthat this paper has shed some light on the role that timbre,\nkey, volume and tempo play in this process. We hope that\nour work informs future work in automatic playlist gen-\neration and music recommendation, and that the proposed\nmethodology inspires more empirical musicological anal-\nysis of how DJs select and order tracks.\n7. REFERENCES\n[1] J.J. Aucouturier and F. Pachet. Music similarity mea-\nsures: Whats the use. In Proc. Int. Conf. on Music In-\nformation Retrieval (ISMIR) , 2002.\n[2] J.J. Aucouturier and F. Pachet. Tools and architecture\nfor the evaluation of similarity measures: case study of\ntimbre similarity. In Proc. Int. Conf. on Music Informa-\ntion Retrieval (ISMIR) , 2004.\n[3] L. Barrington, R. Oda, and G. Lanckriet. Smarter than\ngenius? human evaluation of music recommender sys-\ntems. In Proc. Int. Conf. on Music Information Re-\ntrieval (ISMIR) , 2009.\n[4] A. Berenzweig, B. Logan, D.P.W. Ellis, and B. Whit-\nman. A large-scale evaluation of acoustic and subjec-\ntive music-similarity measures. Computer Music Jour-\nnal, 28(2):63–76, 2004.\n[5] B. Brewster and F. Broughton. Last Night a DJ Saved\nMy Life: The History of the Disc Jockey . Grove Press,\n2000.(a) DJ set\n (b) Random Singles\n (c) Artist Album\nFigure 1 . Change in timbre over time, for examples of each dataset\n(a) Original DJ set\n (b) Rank heuristic ordering\n (c) Median heuristic ordering\nFigure 2 . Speciﬁc example of transitions for different orderings\n[6] D.P.W. Ellis, B. Whitman, A. Berenzweig, and\nS. Lawrence. The quest for ground truth in musical\nartist similarity. In Proc. Int. Conf. on Music Informa-\ntion Retrieval (ISMIR) , 2002.\n[7] B. Fields, C. Rhodes, M. Casey, and K. Jacobson. So-\ncial playlists and bottleneck measurements: Exploiting\nmusician social graphs using content-based dissimilar-\nity and pairwise maximum ﬂow values. In Proc. Int.\nConf. on Music Information Retrieval (ISMIR) , 2008.\n[8] A. Flexer, D. Schnitzer, M. Gasser, and G. Widmer.\nPlaylist generation using start and end songs. In Proc.\nInt. Conf. of Music Information Retrieval (ISMIR) ,\n2008.\n[9] Yajie Hu and Mitsunori Ogihara. Nextone player: A\nmusic recommendation system based on user behavior.\nInProc. Int. Conf. of the Soc. for Music Information\nRetrieval (ISMIR) , 2011.\n[10] B. Logan. Content-based playlist generation: Ex-\nploratory experiments. In Proc. Int. Conf. on Music In-\nformation Retrieval (ISMIR) , 2002.\n[11] B. Logan. Music recommendation from song sets.\nInProc Int. Conf. on Music Information Retrieval\n(ISMRI) , 2004.\n[12] Franc ¸ois Maillet, Douglas Eck, Guillaume Desjardins,\nPaul Lamere, et al. Steerable playlist generation by\nlearning song similarity from radio station playlists. In\nProc. Int. Conf. on Music Information Retrieval , 2009.[13] N. Oliver and L. Kreger-Stickles. Papa: Physiology\nand purpose-aware automatic playlist generation. In\nProc. 7th Int. Conf. Music Inf. Retrieval , pages 250–\n253, 2006.\n[14] E. Pampalk, T. Pohle, and G. Widmer. Dynamic\nplaylist generation based on skipping behavior. In\nProc. Int. Conf. on Music Information Retrieval (IS-\nMIR) , 2005.\n[15] S. Pauws and S. van de Wijdeven. User evaluation of\na new interactive playlist generation concept. In Proc.\nInt.l Conf. on Music Information Retrieval (ISMIR) ,\n2005.\n[16] T. Pohle, E. Pampalk, and G. Widmer. Generat-\ning similarity-based playlists using traveling salesman\nalgo- rithms. In Proc. Int. Conf. on Digital Audio Ef-\nfects (DAFX) , 2005.\n[17] M. Slaney and W. White. Measuring playlist diversity\nfor recommendation systems. In Proc. ACM workshop\non audio and music computing multimedia , 2006.\n[18] R. Van Gulik and F. Vignoli. Visual playlist generation\non the artist map. In Proc. Int. Conf. on Music Infor-\nmation Retrieval(ISMIR) , 2005.\n[19] F. Vignoli and S. Pauws. A music retrieval system\nbased on user-driven similarity and its evaluation. In\nProc. Int. Conf. on Music Information Retrieval (IS-\nMIR) , 2005."
    },
    {
        "title": "Large-Scale Cover Song Identification Using Chord Profiles.",
        "author": [
            "Maksim Khadkevich",
            "Maurizio Omologo"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415844",
        "url": "https://doi.org/10.5281/zenodo.1415844",
        "ee": "https://zenodo.org/records/1415844/files/KhadkevichO13.pdf",
        "abstract": "This paper focuses on cover song identification among datasets potentially containing millions of songs. A compact representation of music contents plays an important role in large-scale analysis and retrieval. The proposed approach is based on high-level summarization of musical songs using chord profiles. Search is performed in two steps. In the first step, the Locality Sensitive Hashing (LHS) method is used to retrieve songs with similar chord profiles. On the resulting list of songs a second processing step is applied to progressively refine the ranking. Experiments conducted on both the Million Song Dataset (MSD) and a subset of the Second Hand Songs (SHS) dataset showed the effectiveness of the proposed solution, which provides state-of-the-art results.",
        "zenodo_id": 1415844,
        "dblp_key": "conf/ismir/KhadkevichO13",
        "keywords": [
            "cover song identification",
            "datasets",
            "millions of songs",
            "compact representation",
            "music contents",
            "Locality Sensitive Hashing (LHS)",
            "search",
            "two steps",
            "progressively refine",
            "state-of-the-art results"
        ],
        "content": "LARGE-SCALE COVER SONG IDENTIFICATION USING CHORD\nPROFILES\nMaksim Khadkevich\nFondazione Bruno Kessler-irst,\nvia Sommarive 18, Povo 38050, Italy\nhadkevich@gmail.comMaurizio Omologo\nFondazione Bruno Kessler-irst,\nvia Sommarive 18, Povo 38050, Italy\nomologo@fbk.eu\nABSTRACT\nThis paper focuses on cover song identiﬁcation among\ndatasets potentially containing millions of songs. A com-\npact representation of music contents plays an important\nrole in large-scale analysis and retrieval. The proposed\napproach is based on high-level summarization of musi-\ncal songs using chord proﬁles. Search is performed in\ntwo steps. In the ﬁrst step, the Locality Sensitive Hash-\ning (LHS) method is used to retrieve songs with similar\nchord proﬁles. On the resulting list of songs a second pro-\ncessing step is applied to progressively reﬁne the ranking.\nExperiments conducted on both the Million Song Dataset\n(MSD) and a subset of the Second Hand Songs (SHS)\ndataset showed the effectiveness of the proposed solution,\nwhich provides state-of-the-art results.\n1. INTRODUCTION\nRecent advances in digital media have allowed for exten-\nsive wide-spread growth of musical collections. We en-\ntered an era of content-based multimedia search engines,\nboosting the demand for advanced audio analysis tools and\napplications. Cover song identiﬁcation based on the anal-\nysis of audio contents is a challenging problem, caused\nby the fact that different renditions of a song can differ\nin tempo, instrumentation, key, or genre. Given this, au-\ndio spectral contents of two covers can vary signiﬁcantly\nfrom one another. During the last decade, the problem of\ncover song identiﬁcation has been of a great interest to sci-\nentists working in the Music Information Retrieval (MIR)\nresearch area [1–3]. Identifying cover songs can help de-\ntect copyright infringements and correctly handle music li -\ncense management.\nIn this paper, we propose an approach to large-scale\ncover song identiﬁcation using chord progressions and\nchord proﬁles. A chord progression is extracted from audio\nor from chroma features provided with the Million Song\nDataset (MSD) [4]. For the most part, the approaches pro-\nposed in the literature are based on the alignment of local\nfeatures, which is typically performed by Dynamic Time\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage an d that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieva l.Warping (DTW) [1], or string alignment [5], and require a\nsigniﬁcant amount of computational resources. To build a\nsystem that can operate on a large scale, we propose the use\nof chord proﬁles for indexing and fast retrieval. A chord\nproﬁle of a song is a compact representation that summa-\nrizes the rate of occurrence of each chord. To solve the\nproblem of cover song identiﬁcation, we propose the use\nof a well-established approach for fast retrieval in multi-\ndimensional spaces, which is Locality-Sensitive Hashing\n(LHS). A more accurate comparison is then performed be-\ntween the top kresults, based on the alignment of chord\nprogressions.\n1.1 Previous work\nMost of the cover song identiﬁcation systems reported in\nthe literature work on small datasets (up to several thou-\nsands of songs) and involve pair-wise comparison, when\na query song is matched against all the songs in the\ndatabase [1, 2]. This makes them impractical for large col-\nlections, containing millions of items. A good overview of\nexisting approaches is given in [3].\nRecent works on large-scale datasets are based on in-\ndexing and fast retrieval. So far, several indexing schemas\nhave been introduced which allow for fast searching over\nlarge databases. Bertin-Mahieux and Ellis [6] proposed us-\ning “chroma jump codes”. They extract beat-synchronized\nchromagram, discover chroma bins with high energy, and\nconstruct “jump codes” between such pairs. For retrieval,\n“jump codes” extracted from a queried song are searched\nin the database and results are ranked according to the\nnumber of matching pairs. In the approach of Bertin-\nMahieux and Ellis [7], 2-D Fast Fourier Transform (FFT)\nof overlapping chromagram segments corresponding to 75\nbeats are averaged with subsequent application of Principa l\nComponent Analysis (PCA). Distances between vectors of\nPCA components are used to generate ranked lists.\nSeveral approaches have been proposed based on chord\nsequence alignment. Lee [8] used chord sequences for\ncover song identiﬁcation. His approach is based on the\nextraction of chords by means of a HMM-based recog-\nnizer trained on data generated from MIDI. The songs\nare ranked according to a DTW-based pairwise similarity\non key-transposed sequences. Bello [5] extended this ap-\nproach, systematically evaluating key shifting, the cost o f\ngap insertions and chord swaps in string alignment. Martin\net al. [9] adapted tools from computational biology to alignchord sequences. However, their system could not perform\nwell on a subset of the MSD dataset, probably due to some\nsimpliﬁcations in the chord representations; instead of ta k-\ning into account chord types, they extract only chord roots.\nThese works show that chord progressions can be success-\nfully used as high-level features for the problem of cover\nsong identiﬁcation.\nThe previous works listed above operate either on\nframe-level features such as chroma [1, 2] or chords [5, 8],\nor by extracting very compact descriptors of the whole\nsong [6, 7]. The former approaches are more accurate\nbut do not scale to large databases, since brute-force se-\nquence matching is computationally expensive. The latter\napproaches allow for scalability, but usually are limited i n\nterms of performance. In this paper, the problem of cover\nsong identiﬁcation is approached trying to ﬁnd a compro-\nmise between fast retrieval and exhaustive matching.\n1.2 Organization of the paper\nThe datasets and the evaluation methodology are presented\nin Section 2. Section 3 brieﬂy describes the proposed sys-\ntem architecture, including front-end processing and in-\ndexing schema. Section 4 is devoted to experimental re-\nsults. Some conclusions and future work are ﬁnally pre-\nsented in Section 5.\n2. DATASET AND EV ALUATION\nMETHODOLOGY\nThe lack of large datasets for cover song identiﬁcation\nis a problem, which has always been an obstacle to de-\nvelop systems suitable for commercial use, i.e., able to run\nqueries on a large database comprising millions of songs.\nThe Million Songs Dataset (MSD) is the ﬁrst attempt to\ncreate a large dataset for different MIR tasks. The MSD\ndataset contains features and meta-data for one million\nsongs. All the features are extracted using the EchoNest1\nAPI. Unfortunately, the distribution of the MSD dataset\ndoes not contain chords. We used chroma segments and\nbeats to estimate chord sequences from MSD features us-\ning the template-matching approach [10].\nThe Second Hand Songs (SHS)2dataset consists of\nmeta-data for around 250000 songs. For about 50000\nsongs a link to YouTube3video is available.\nAn intersection of the SHS and the MSD datasets was\nused to build the SHS-MSD dataset. 18196 songs from the\nSHS dataset are chosen to form the SHS-MSD dataset. It\nwas proposed to divide the SHS-MSD dataset into training\n(12960 tracks) and test (5854 tracks) parts [6]. They will\nbe referred to as SHS-training and SHS-test, respectively.\nDue to the fact that data provided with the MSD dataset\ndoes not contain waveforms, and extracted chroma fea-\ntures are probably not the best solution to produce chord\nrecognition rate comparable to state-of-the-art systems, we\n1http://www.echonest.com\n2http://www.secondhandsongs.com/\n3http://www.youtube.com/collected our own dataset. It will be referred to as SHS-\nWA V dataset. We used the SHS website to download\n24282 videos, from which audio tracks were extracted.\nAll the tracks from the SHS-WA V dataset are divided in\n5650 cover groups. The largest group contains 128 cov-\ners, which is “Summertime” by George Gershwin. To our\nknowledge, this is the largest collection of audio wave-\nforms suitable for the evaluation of large-scale cover song\nidentiﬁcation systems. The list of the songs for both the\ndatasets and the corresponding extracted chords are pub-\nlicly available4.\nWe follow the same evaluation methodology as pro-\nposed in [7]. As with many other information retrieval\nsystems that produce a list of ranked items as a result, we\nadopt Mean Average Precision (MAP) as the main evalu-\nation metric. We also report Average Rank (AR), but this\nmetric is less informative and can be misleading when used\nalone. AR is mostly inﬂuenced by the most difﬁcult covers,\nwhile differences in the top of the rank are neglected [7].\nWe also present distribution statistics of the ranked songs\nfor the whole MSD dataset with a particular emphasis on\nthe top ten results.\n3. PROPOSED SYSTEM ARCHITECTURE\nChords and melody are considered to be essential charac-\nteristics of a song. They are the two attributes that describ e\ntonal and harmonic properties of a musical piece and al-\nlow us to identify a song among many others, regardless\nof tempo, instrumentation, or genre. The proposed cover\nsong identiﬁcation system is based on the use of chord pro-\ngressions and chord proﬁles. Chords are considered to be\na high-level descriptor. Despite possible local changes in\nthe different renditions of a song (e.g., a major chord is re-\nplaced by a minor one) the general characteristics of the\nchord progressions embedded in it are typically preserved\nfrom one cover to another. Therefore, in this paper we ex-\nplore different ways of exploiting chord progressions and\nchord proﬁles to build a robust and large-scale cover song\nidentiﬁcation system. The block diagram of the proposed\nsystem presented in Figure 1. It comprises the following\nstructural components: high-level feature extraction, in -\ndexing and retrieval.\n3.1 Chord progression extraction\nChord progressions and chord proﬁles are the two high-\nlevel features used in the proposed system. The extrac-\ntion of beat-synchronous chord progression is the ﬁrst step .\nTwo different datasets are used to evaluate the proposed\napproach, one containing audio waveforms and the other\ncontaining only features and metadata. Correspondingly,\ntwo different chord extraction techniques are adopted as\ndiscussed below.\nIn the case of SHS-WA V dataset, for the extraction of\nbeats and chords from audio waveform we use Vamp5plu-\ngins Chordino andBarBeatTracker that show state-of-the-\n4https://github.com/FBK-SHINE/CoverSongData\n5http://www.vamp.org\u0001 \u0002 \u0003 \u0004 \u0005\u0007 \b \t \n \u000b \f \r \u000e \f \r\u000f \u0010 \r \f \u000e \b \u000b \n \b \u0005 \u0004 \r \n \u0011\n\u0012 \u0010 \f \r \u0013\n \u0014 \u0005 \r \b \u000f \u0005 \u0015 \f \u0002\u0016 \u0004 \n \r \u0015 \n \u0013 \u0011 \f \u0002 \u0017\n\u0012 \u0010 \f \r \u0013 \u0003 \r \f \u000b \u0015 \u0018 \n\u0005 \r \b \u0002 \u0011 \u0003 \f \u0011 \u0015 \u0005 \u0015 \f \u0002 \u0011\n\u0019 \u001a \u001b\u001c \n \u001d \u0011 \u0010 \u0015 \u000b \u0005 \u001e \u001f\u0019 \u001a \u001b\u001c \n \u001d \u0011 \u0010 \u0015 \u000b \u0005 \u001e  \u0019 \u001a \u001b\u001c \n \u001d \u0011 \u0010 \u0015 \u000b \u0005 \u001e   \n\u0019 \u0015 \u0011 \u0005 \u001f\u0019 \u0015 \u0011 \u0005  \u0019 \u0015 \u0011 \u0005   \n! \n \r \u0017 \n \u0013\u0018 \u0015 \u0011 \u0005\n\" \n # \r \b \u0002 $ \u0015 \u0002 \u0017\n% \u0015 \u0002 \b \u0018\r \b \u0002 $ \n \u0013 \u0018 \u0015 \u0011 \u0005\n& \b \u0005 \b ' \b \u0011 \n\u001b \f \u0002 \u0017\u0012 \u0010 \f \r \u0013 \u0003 \r \f \u0017 \r \n \u0011 \u0011 \u0015 \f \u0002\u0012 ( \u0012 ( \u0012 ( \u0012 ( ) ( ) ( % ( % ( \u0012 ( \u0012 ( * \u000e ( +\u0012 \u0010 \f \r \u0013\u0003 \r \f \u000b \u0015 \u0018 \nFigure 1 : Block diagram of the proposed system\nart results. The beat structure is used to obtain a tempo-\nindependent sequence of chords. Once chords and beat\nstructure are extracted, the chords are split into beat seg-\nments so that each beat segment contains one chord. If\na chord transition occurs inside a beat segment, the chord\nsegment that has the longest intersection with the current\nbeat segment is used to derive the chord label. The chord\ndictionary comprises two chord types, which are major and\nminor .\nTo derive chord progressions from the MSD dataset,\nbeat-synchronous chroma features are ﬁrst extracted. We\nfollow the approach of Bertin-Mahieux and Ellis [6],\nwhere chroma vectors are averaged across beat segments.\nIn the second step, we apply the template matching tech-\nnique proposed in [10]. Template matching for chord\nrecognition is based on the idea of introducing a set of\ntemplates for each chord type. The template conﬁgura-\ntions are derived heuristically. We deﬁne a binary mask as\na 12-dimensional chord template in which the pitch classes\nthat correspond to constituent notes of the given chord are\nset to one, while the other components are set to zero. A\nbinary template Tis deﬁned as\nT= [ZC,ZC♯,ZD,ZD♯,ZE,...,ZA♯,ZB] (1)\nwhereZpdenotes the mask value that corresponds to the\npitch class p. For example, binary masks for C major and\nD minor chords would take the following form:\nT(C:maj)=[1,0,0,0,1,0,0,1,0,0,0,0]\nT(D:min)=[0,0,1,0,0,1,0,0,0,1,0,0]\nThe template that produces the highest cosine similarity\nbetween chroma vectors is used to generate a chord label\nfor the given beat segment. The cosine similarity between\nvectorsaandbis deﬁned as\nSc(a,b) =a·b\n/bardbla/bardbl/bardblb/bardbl(2)where/bardbl·/bardbldenotes Euclidean distance.\nThe resulting sequence of beat-aligned chords is subse-\nquently used as a compact representation of the harmonic\nstructure of the song. However, the comparison of chord\nprogressions is usually done by sequence alignment, which\nis a computationally expensive operation and cannot be\nused on a large scale. Therefore, we propose to further\ncompress the extracted high-level features. This can be\ndone by discarding the temporal information and compact-\ning all the related contents in a chord proﬁle. A chord\nproﬁle is a 24-dimensional vector, in which each dimen-\nsion corresponds to the rate of occurrence of a chord. Let\nnibe number of beat segments containing chord i, where\ni∈1..24. Then, the i-th component of the chord proﬁle\nvectorcis calculated as ci=ni\nN, whereNis the total\nnumber of beat segments.\nChord proﬁles and chord progressions extracted for\neach song of a given dataset are stored in a database. In the\nretrieval stage, high-level features extracted from a quer ied\nsong are used to derive a ranked list of possible covers from\nthat database, as described in the following section.\n3.2 Retrieval\nThe proposed cover song identiﬁcation system relies on a\ntwo-step retrieval schema. Given a large database of chord\nproﬁles and a queried song, we address the problem of\nﬁnding the nearest neighbors. Finding the nearest neigh-\nbors of an element in large databases is a well-known prob-\nlem addressed in many areas of information retrieval. For\nlow-dimensional data, nearest neighbor search can be per-\nformed by partitioning the search space using, for exam-\nple, k-d trees as mentioned in [11]. Data with high number\nof dimensions cause the so-called “curse of dimensional-\nity”, when the distance between neighboring points tend to\nbe large [12]. Locality Sensitive Hashing is a probabilis-\ntic approach to reduce dimensionality by hashing featuresso that items that are close to each other fall in the same\nbucket with high probability. Casey and Slaney [13] used\nLHS for fast shingle retrieval from a comparatively large\ndatabase. Casey et al. [14] extended their work, perform-\ning analysis of optimal parameters and giving examples of\nLHS application for different MIR tasks. Yu et al. [15]\nproposed an adapted two-level LHS scheme to tackle the\nproblem of retrieving multi-variant audio tracks. In their\nwork, a study on trade-off between identiﬁcation accuracy\nand efﬁciency is presented.\nFor small datasets containing several thousands of\nsongs, a straightforward approach suggests computing the\ndistances between a queried song and all the items in the\ndataset. When working with larger databases, containing\nseveral millions of songs, a signiﬁcant increase in speed\ncan be achieved by using LHS. Following the approach of\nCasey and Slaney [13], in the ﬁrst step we use LHS to re-\ntrieve the nearest neighbours. L1distance is adopted as\ndistance metric. Given two chord proﬁles aandb, the dis-\ntance between them is deﬁned as\n||a−b||1=24/summationdisplay\ni=1|ai−bi| (3)\nNote that L2distance was used in our early experiments,\nwith deﬁnitely worse results. However, this topic should\nbe matter of further investigation.\nDue to the fact that a cover of a given song can be per-\nformed in a different key, we should introduce a mech-\nanism to make the distance between chord proﬁles key-\ninvariant. This can be achieved by performing a circular\npermutation of a queried song chord proﬁle 12 times, tak-\ning into account all the possible key transpositions. For\neach transposition, we retrieve a list of candidates. In the\nﬁnal part of the ﬁrst stage, all the lists are merged and all\nthe retrieved songs are ranked according to (3).\nIn the second step, the top kresults are re-ranked by\ncomputing edit (or Levenshtein) distances between chord\nprogressions. The edit distance is the number of insertions ,\ndeletions and substitutions to transform one sequence into\nanother. Thus, a more accurate matching is performed,\nwhich takes temporal information into account. Time com-\nplexity of computing edit distance is O(nm), wheren\nandmare chord progression lengths of the songs under\ncomparison. Choosing kdepends on the balance between\nspeed and precision. In our experimental setup, we set\nk= 2000 , which means that the top 2000 results from the\nmerged rank list obtained in the ﬁrst stage are re-ranked\naccording to edit distance between chord progressions. In\nthis way, the output is reﬁned taking into account temporal\nalignment between a queried song and the top kitems from\nthe merged ranked list.\n4. EXPERIMENTAL RESULTS\n4.1 Evaluating MSD features\nThe errors produced by the chord extraction propagate\nthrough successive processing steps and eventually have\nan impact on cover song identiﬁcation performance. As aresult, it was important to understand if the features pro-\nvided by the EchoNest API could be used for an accu-\nrate chord estimation. As opposed to frame-based [10]\nor beat-synchronous [1] approaches to extracting chroma\nfeatures, the EchoNest API has its own segmentation algo-\nrithm which is independent of beat positions. The chroma\nvectors are averaged across segments that can contain\nseveral beats. On the other hand, some beats can con-\ntain several such segments. Another limitation of these\nchroma features is the absence of bass information. It\nhas been shown that using a lower frequency content, as\nextra-feature, leads to a signiﬁcant improvement in perfor -\nmance [16, 17]. Given this, the chroma vectors delivered\nwith EchoNest API might not be the best features for auto-\nmatic chord extraction.\nIn fact, in our preliminary experiments we applied the\nchroma feature extraction available with the EchoNest API\non the MIREX 2011 corpus to evaluate chord recognition\nperformance. The corpus consists of 220 songs of Beatles,\nQueen, Zweieck and Carol King. The template-based ap-\nproach described in Section 3.1 was used to generate chord\nlabels.\nThe experiment led to a chord recognition rate of\n55.7%, compared to 77.8% obtained using the frame-level\ntemplate matching based on time-frequency reassigned\nchroma features proposed in [18]. In the latter case, a 24-\ndimensional chroma vector was used, where the ﬁrst and\nthe second 12 dimensions corresponded to bass and treble\ncontents, respectively. This suggests that using alternat ive\nchroma feature sets to represent a song can lead to an im-\nprovement in chord recognition rate, and as a consequence\nin cover song identiﬁcation performance.\nIn the following experiments, we compare the perfor-\nmance of the proposed system when applied to the MSD\ndataset and to the collected SHS-WA V dataset.\n4.2 Chroma proﬁles\nThe ﬁrst set of experiments aimed to collect statis-\ntics of chord proﬁle distances between covers and non-\ncovers. The results for the SHS-training and the SHS-WA V\ndatasets are presented in Figure 2.\nTo generate statistics on covers, we calculated distances\nbetween chord proﬁles corresponding to all cover pairs in\nthe given dataset. As for non-covers, for each song we ran-\ndomly choose a non-cover song. In order to take into ac-\ncount possible key shifts, the distance between two chord\nproﬁles is deﬁned as the minimum distance among those\nobtained for all the 12 possible circular permutations.\nAs shown in Figure 2a, for SHS-WA V dataset,\nGaussian-like distributions are obtained with mean values\nand standard deviations of (0.66, 0.28) and (0.97, 0.26), fo r\ncover and non-cover pairs, respectively. A similar behav-\nior is obtained with SHS-training dataset, for which mean\nvalues and standard deviations are (0.76 0.26) and (0.98,\n0.22), respectively.0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 200.010.020.030.040.050.060.070.080.090.1Density\nDistance  \ncovers\nnon−cover\n(a) SHS-WA V dataset\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 200.020.040.060.080.10.12Density\nDistance  \ncovers\nnon−cover\n(b) SHS-training dataset\nFigure 2 : Probability density function of distance between\ncover and non-cover pairs for SHS-WA V and SHS-training\ndatasets\n4.3 Cover song identiﬁcation results\nTable 1 presents the results on the three datasets. We com-\npare our system with “chroma jumps” [6] and 2D-FFT [7].\nOn all the datasets, the proposed approach showed the best\nresults. The results reported for “SHS-training” were ob-\ntained when using the training subset of MSD-SHS dataset\n(12960 tracks). The results reported for “SHS-WA V” were\nobtained when using waveforms extracted from YouTube\nvideos (24282 tracks). The “Full MSD” labeled columns\nrefer to the most important experimental setup, which indi-\ncates the scalability of the approach and shows the perfor-\nmance of the proposed features on the full MSD dataset.\nCovers from SHS-test dataset (5854 tracks) were used for\nquerying.\nIt is interesting to see that the proposed system per-\nformed signiﬁcantly better on SHS-WA V dataset, if com-\npared to SHS-training dataset. This fact conﬁrms that the\nchroma features provided with MSD dataset are not the\nbest solution for chord extraction. It is likely that the ver y\nlow performance at this moment obtained on the full MSD\ndataset can be signiﬁcantly improved by processing the\noriginal waveforms.\nFigures 3 and 4 show the distribution of ranks for the ex-Table 1 : Experimental results on three datasets\nSystem Average Rank MAP\nSHS-training\nproposed approach 958.2 0.10753\n2DFTM (200 PC) [7] 3,005.1 0.09475\n2DFTM (50 PC) [7] 2,939.8 0.07759\nSHS-WA V\nproposed approach 1,378.4 0.2062\nFull MSD\nproposed approach 114,951 0.03709\n2DFTM (200 PC) [7] 180,304 0.02954\n2DFTM (50 PC) [7] 173,117 0.01999\njcodes 2 [6] 308,370 0.00213\nTable 2 : Runtime and memory footprints\nDataset Size (songs) sec/query memory\nSHS-WA V 24282 1.46 346M\nSHS-training 12960 1.08 198M\nfull MSD 1 million 7.56 3690M\nperiments on the full MSD dataset. 10% of the covers were\nranked in the top 1000, and around 25% of them in the top\n10000. For 255 queries, a cover was placed in the ﬁrst\nposition, while 716 covers appeared in the top-ten ranked\nlist.\nAverage querying runtime for each dataset is presented\nin Table 2. All the experiments were conducted on a mod-\nern laptop with 8GB of RAM installed and CPU Intel i7-\n2760QM running at 2.4 GHz. Due to the compactness of\nthe chord progressions and chord proﬁles, it is possible to\nput all the features extracted from full MSD dataset into\na hash table and store it in RAM. Memory footprint was\naround 3.5 GB. In terms of speed, average runtime per\nquery without any parallelization appeared to be 7.56 sec-\nonds when searching in the full MSD dataset.\n0 1 2 3 4 5 6 7\nx 105010002000300040005000\nSong rankNumber of occurences\nFigure 3 : Rank distribution for the results on full MSD\ndataset12345678910050100150200250300\nSong rankNumber of occurences\nFigure 4 : Top ten rank distribution for the results on full\nMSD dataset\n5. CONCLUSION\nIn this paper, we proposed a new system for scalable cover\nsong identiﬁcation. The experimental results showed that\nchord proﬁles can be used as an extremely compact high-\nlevel feature that summarizes harmonic properties of a\nsong. The proposed two-step approach improves the sys-\ntems on which we compared performance and suggests\nroom for an improvement. More sophisticated distances\nthan Levenshtein could be used for sequence alignment,\nsuch as NeedlemanWunsch or Smith-Waterman, which\nwere used in [5] and [2], respectively. More efﬁcient ways\nof introducing key-invariance can be investigated. Instea d\nof using 12 circular permutation to make a query, an alter-\nnative feature vector representation can be utilized. Expe r-\nimental results obtained on different datasets showed that\nswitching from precomputed feature data provided by the\nEchoNest API to state-of-the-art high-level feature extra c-\ntors may further improve the performance.\n6. REFERENCES\n[1] D. P. W. Ellis and G. E. Poliner, “Identifying ‘cover\nsongs’ with chroma features and dynamic program-\nming beat tracking,” in Proc. ICASSP , vol. 4, April\n2007, pp. IV–1429–IV–1432.\n[2] J. Serr` a and E. G´ omez, “Audio cover song identiﬁ-\ncation based on tonal sequence alignment,” in Proc.\nICASSP . Las Vegas, USA: IEEE, 2008, pp. 61–64.\n[3] J. Serr` a, “Identiﬁcation of versions of the same musica l\ncomposition by processing audio descriptions,” Ph.D.\ndissertation, Universitat Pompeu Fabra, Barcelona,\n2011.\n[4] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and\nP. Lamere, “The million song dataset,” in Proc. ISMIR ,\nMiami, USA, 2011, pp. 591 – 596.\n[5] J. P. Bello, “Audio-based cover song retrieval using ap-\nproximate chord sequences: testing shifts, gaps, swapsand beats,” in Proc. ISMIR , Vienna, Austria, 2007, pp.\n239–244.\n[6] T. Bertin-Mahieux and D. P. W. Ellis, “Large-scale\ncover song recognition using hashed chroma land-\nmarks,” in Proc. WASPAA . New York, USA: IEEE,\n2011, pp. 117 – 120.\n[7] T. Bertin-Mahieux and D. P. W. Ellis, “Large-scale\ncover song recognition using the 2d fourier transform\nmagnitude,” in Proc. ISMIR , Porto, Portugal, 2012, pp.\n241 – 246.\n[8] K. Lee, “Identifying cover songs from audio using har-\nmonic representation,” in MIREX task on Audio Cover\nSong Identiﬁcation , 2006.\n[9] B. Martin, D. G. Brown, P. Hanna, and P. Ferraro,\n“Blast for audio sequences alignment: A fast scalable\ncover identiﬁcation tool,” in Proc. ISMIR , Porto, Por-\ntugal, 2012, pp. 529 – 534.\n[10] L. Oudre, Y . Grenier, and C. F´ evotte, “Template-based\nchord recognition : Inﬂuence of the chord types,” in\nProc. ISMIR , Kobe, Japan, 2009, pp. 153–158.\n[11] M. Slaney, Y . Lifshits, and J. He, “Optimal parame-\nters for locality-sensitive hashing,” Proceedings of the\nIEEE , vol. 100, no. 9, pp. 2604–2623, 2012.\n[12] C. M. Bishop, Pattern Recognition and Machine\nLearning (Information Science and Statistics) . Secau-\ncus, NJ, USA: Springer-Verlag New York, Inc., 2006.\n[13] M. Casey and M. Slaney, “Fast recognition of remixed\nmusic audio,” in Proc. ICASSP , vol. 4, Honolulu,\nHawaii, USA, 2007, pp. IV–1425–IV–1428.\n[14] M. Casey, C. Rhodes, and M. Slaney, “Analysis of min-\nimum distances in high-dimensional musical spaces,”\nIEEE Transactions on Audio, Speech and Language\nProcessing , vol. 16, no. 5, pp. 1015–1028, July 2008.\n[15] Y . Yu, M. Crucianu, V . Oria, and L. Chen, “Local\nsummarization and multi-level lsh for retrieving multi-\nvariant audio tracks,” in Proc. ACM international con-\nference on Multimedia , Beijing, China, 2009, pp. 341–\n350.\n[16] M. Mauch and S. Dixon, “Approximate note transcrip-\ntion for the improved identiﬁcation of difﬁcult chords,”\ninProc. ISMIR , Utrecht, Netherlands, 2010, pp. 135–\n140.\n[17] M. Khadkevich and M. Omologo, “Time-frequency re-\nassigned features for automatic chord recognition,” in\nProc. ICASSP , Prague, Czech Republic, 2011, pp. 181\n– 184.\n[18] M. Khadkevich and M. Omologo, “Reassigned\nspectrum-based feature extraction for gmm-based au-\ntomatic chord recognition,” EURASIP Journal on Au-\ndio, Speech, and Music Processing , vol. 2013, no. 1,\npp. 1–12, 2013."
    },
    {
        "title": "Improving the Reliability of Music Genre Classification using Rejection and Verification.",
        "author": [
            "Alessandro L. Koerich"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416570",
        "url": "https://doi.org/10.5281/zenodo.1416570",
        "ee": "https://zenodo.org/records/1416570/files/Koerich13.pdf",
        "abstract": "This paper presents a novel approach for post-processing the music genre hypotheses generated by a baseline classifier. Given a music piece, the baseline classifier produces a ranked list of the N best hypotheses consisting of music genre labels and recognition scores. A rejection strategy is then applied to either reject or accept the output of the baseline classifier. Some of the rejected instances are handled by a verification stage which extracts visual features from the spectrogram of the music signal and employs binary support vector machine classifiers to disambiguate between confusing classes. The rejection and verification approach has improved the reliability in classifying music genres. Our approach is described in detail and the experimental results on a benchmark dataset are presented.",
        "zenodo_id": 1416570,
        "dblp_key": "conf/ismir/Koerich13",
        "keywords": [
            "post-processing",
            "music genre hypotheses",
            "baseline classifier",
            "ranked list",
            "rejection strategy",
            "verification stage",
            "spectrogram",
            "binary support vector machine",
            "disambiguation",
            "benchmark dataset"
        ],
        "content": "IMPROVING THE RELIABILITY OF MUSIC GENRE CLASSIFICATION\nUSING REJECTION AND VERIFICATION\nAlessandro L. Koerich\nPontiﬁcal Catholic University of Paran ´a (PUCPR)\nFederal University of Paran ´a (UFPR)\nalekoe@computer.org\nABSTRACT\nThis paper presents a novel approach for post-processing\nthe music genre hypotheses generated by a baseline classi-\nﬁer. Given a music piece, the baseline classiﬁer produces\na ranked list of the Nbest hypotheses consisting of music\ngenre labels and recognition scores. A rejection strategy\nis then applied to either reject or accept the output of the\nbaseline classiﬁer. Some of the rejected instances are han-\ndled by a veriﬁcation stage which extracts visual features\nfrom the spectrogram of the music signal and employs bi-\nnary support vector machine classiﬁers to disambiguate\nbetween confusing classes. The rejection and veriﬁcation\napproach has improved the reliability in classifying music\ngenres. Our approach is described in detail and the experi-\nmental results on a benchmark dataset are presented.\n1. INTRODUCTION\nFor over ten years the problem of classifying music genres\nhas been the subject of intensive research and is the most\nwidely studied area in Music Information Retrieval [2,13].\nAutomatically classifying music by genre is a challenging\nproblem considering that music is an evolving art and there\nare not clear edges between music genres. A variety of fea-\ntures and classiﬁcation approaches have been proposed in\nthe last years [1,2,6,13]. Relatively high classiﬁcation ac-\ncuracies have been reported in recent papers that carry out\nexperiments on benchmark datasets such as ISMIR 2004,\nGTZAN [15], and LMD [12]. Sturm [13] provides a com-\nprehensive review of the approaches used for evaluating\nmusic genre classiﬁcation. Sturm shows that over 92%\nof the papers approach evaluation of music genre classi-\nﬁcation systems by classifying several music excerpts and\ncomparing the labels to a ground truth. Sturm states that\nthe classiﬁcation accuracy might not be a correct measure\nsince it can not address the problem at all.\nA few survey articles provide an overview of features\nand techniques used for music genre classiﬁcation and re-\nlated tasks [2, 9, 13]. Scaringella et al. [9] reviewed the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.techniques of audio feature extraction and classiﬁcation for\nthe task of genre classiﬁcation only. Fu et al [2] provide\na comprehensive review on audio-based classiﬁcation and\nsystematically summarize the state-of-the-art techniques for\nmusic classiﬁcation, stressing the difference in the features\nand the types of classiﬁers used for different classiﬁcation\ntasks such as music genre classiﬁcation, mood classiﬁca-\ntion, artist identiﬁcation, instrument recognition and mu-\nsic annotation. The review paper of Sturm [13] focused on\nhow music genre recognition systems are evaluated. More-\nover, the ﬁeld of music classiﬁcation research is develop-\ning rapidly in the past few years, with new features and\ntypes of classiﬁers being developed and used. However,\nfew works have evaluated the reliability of the current sys-\ntems or make a deep analysis of the errors [14]. The deﬁ-\nnition of reliability used in this paper is borrowed from [5]\nwhich was referred to the proportion of correct answers\namong the accepted instances as a function of the rejection\nrate.\nIn this paper we propose the use of rejection and veriﬁ-\ncation steps in an attempt to overcome the deﬁciencies of\nconventional classiﬁcation approaches. The rejection fo-\ncuses on not classifying instances that generate high uncer-\ntainty at the output of the classiﬁer, while veriﬁcation fo-\ncuses on the confusions that may happen between particu-\nlar music genres which is revealed by analyzing the confu-\nsion matrices. First, a baseline classiﬁer, which takes into\naccount all the possible music genres is used to classify the\ninput music signal represented by a 68-dimensional feature\nvector. If this baseline classiﬁcation scheme does not pro-\nvide an output with high conﬁdence, which is given by the\na posteriori probability estimated to each possible music\ngenre, the instance is rejected or may be post-processed at\na veriﬁcation stage, which extracts different features from\nthe music signal and employs a set of binary SVM classi-\nﬁers. We show in this paper that this approach is able to in-\ncrease the reliability in music genre classiﬁcation. It is im-\nportant to notice that we tackle the problem from a pattern\nrecognition perspective where music genres are treated as\nlabels. The musicological aspect of the music genre clas-\nsiﬁcation task is not analyzed in this paper.\nThis paper is organized as follows. Section 2 presents a\nbaseline music genre classiﬁcation system. Section 3 intro-\nduces the concept of rejection in music genre classiﬁcation.\nSection 4 introduces the concept of post-processing and\nveriﬁcation of the output of a baseline classiﬁer. Section 5presents the experimental results of the proposed approach\nfor rejection and veriﬁcation on a benchmark dataset. Fi-\nnally in the last section some conclusions are stated.\n2. BASELINE MUSIC GENRE CLASSIFICATION\nSYSTEM\nThe baseline music genre classiﬁcation system is composed\nof two main modules: feature extraction and classiﬁca-\ntion as show in Figure 1. The feature extraction module\nuses the MARSYAS framework to extract seventeen au-\ndio features from 46msframes of the audio signal with\nno overlap. The features are: zero crossing rate, the spec-\ntral centroid, roll-off frequency, spectral ﬂux, and 13 Mel\nfrequency cepstral coefﬁcients, including MFCC0. Frame\nfeatures are collapsed in a two-step process (texture win-\ndowing and computation of global mean and standard de-\nviation) into a 68-dimensional feature vector for the whole\naudio excerpt [8]. For classiﬁcation, two implementations\nwere considered: A ﬁrst version of the baseline system\nemploys a multi-class support vector machine (SVM) al-\ngorithm with one-against all strategy. A second version of\nthe baseline system employs the same features, but uses a\nmultilayer perceptron neural network (MLP) trained with\nthe backpropagation momentum algorithm. Such classiﬁ-\ncation algorithms were chosen because both of them can\nprovide estimations of the a posteriori probability of each\nclass at the output [5, 16].\nLet’s consider a n-dimensional pattern recognition prob-\nlem withcclasses. The baseline classiﬁers perform a task\nof classifying an input music signal, represented by a n-\ndimensional feature vector x= [x1;:::;x n]2<npro-\nvided at the input and produce at the output a posteri-\noriprobability for each of the cpossible classes, denoted\nasP(!1jx);:::;P (!cjx), where!1;:::;! cdenotes thec\npossible classes. Therefore, we can consider that the base-\nline classiﬁers produce at their output a c-dimensional vec-\ntor[P(!1jx);:::;P (!cjx)]TwhereP(!jjx)represents the\nsupport for the hypothesis that vector xsubmitted for clas-\nsiﬁcation comes from class !j. Furthermore, we can as-\nsume that such an output vector is ordered in a decreasing\norder according to the probability, from the most probable\nclass, denoted as TOP 1 to the least probable class. The\nlarger the probability, the more likely the class label !j. In\nreal-life applications, the classiﬁcation system has to come\nup with a single music genre hypothesis at the output or a\nrejection of the input if it is not certain enough about the\nhypothesis. Most of the current works on music genre clas-\nsiﬁcation does not consider the rejection option and force a\ndecision using the MAX operator to select the music genre\nhypothesis which has the highest a posteriori probability\nas shown in Figure 1, i.e. the TOP 1 hypothesis, denoted\nas!0and computed by Equation 1.\n!0= arg max\n!1\u0014!i\u0014!cP(!ijx) (1)\nThe main drawback of using the MAX operator as deci-\nsion rule is that it is very severe and it often overlooks theuncertainty that may be present at the output of the classi-\nﬁers. Such an operator will return a class regardless if the\nhighest probability is close to 1.0 or lower than 0.5. The\nprobabilities estimated by the classiﬁer can be associated\nto how conﬁdent it is in assigning a given class to a input\nvector.\n3. REJECTION OF MUSIC GENRES\nHow can we handle the uncertainty at the output of a mul-\nticlass classiﬁer? Maybe the simplest way is to employ\nrejection [3–5]. The concept of rejection admits the po-\ntential refusal of a music genre hypothesis if the classi-\nﬁer is not certain enough about the genre hypothesis. In\nour case, the probabilities assigned to each output of the\nbaseline classiﬁers give evidence about the certainty. The\nrefusal of a music genre hypothesis may be due to two dif-\nferent reasons: there is not enough evidence to come to a\nunique decision since more than one music genre hypoth-\nesis among the cpossible music genres appear adequate;\nthere is not enough evidence to come to a decision since\nno music genre among the cgenres appears adequate. In\nthe ﬁrst case, it may happen that the conﬁdence scores do\nnot indicate a unique decision in the sense that there is not\njust one conﬁdence score exhibiting a value close to one.\nIn the second case, it may happen that there is no conﬁ-\ndence score exhibiting a value close to one. Therefore, the\nconﬁdence scores assigned to the music genre hypotheses\nin theNbest hypothesis list should be used as a guide to\nestablish a rejection criterion.\nBayes decision rule already embodies a rejection rule,\nnamely, ﬁnd the maximum of P(!ijx)but check whether\nthe maximum found exceeds a certain threshold value or\nnot. Due to decision-theoretic concepts this reject rule is\noptimum for the case of insufﬁcient evidence if the closed-\nworld assumption holds and if the a posteriori probabili-\nties are known [10]. This suggests the rejection of a music\ngenre hypothesis if the conﬁdence score for that hypothe-\nsis is below a threshold \u0015. In the context of our baseline\nclassiﬁers, the task of the rejection mechanism is to, based\non output vector [P(!1jx);:::;P (!cjx)]Tprovided at the\noutput of the classiﬁers, which is ordered in a decreasing\norder according to the probability, decide whether the best\nmusic genre hypothesis, which is so far called the TOP 1,\ncan be accepted or not. Therefore, the conventional clas-\nsiﬁcation approach is modiﬁed. Now, the highest a pos-\nteriori probability provided by the MAX operator is not\nsimply accepted, but it is compared with a threshold ( \u0015). If\nsuch a probability is greater than \u0015then the music genre is\nassigned to the input vector, otherwise, no label is assigned\nto the input vector xand it is rejected. This novel decision\nscheme is show in Figure 2).\nIn summary, the rejection rule is given by:\n1. The TOP 1 music genre hypothesis is accepted when-\neverP(!0jx)\u0015\u0015\n2. The TOP 1 music genre hypothesis is rejected when-\neverP(!0jx)<\u0015Figure 1 . An overview of the baseline music genre classiﬁcation system\nFigure 2 . An overview of the rejection scheme for the baseline music genre classiﬁcation\nwhere\u0015is a rejection threshold and P(!0jx)is the a poste-\nriori probability assigned by the classiﬁer to the best music\ngenre hypothesis !0.\n4. VERIFICATION OF MUSIC GENRES\nThe question that may arise is if we can do better than sim-\nply rejecting the instances that the classiﬁer is not able to\nclassify with conﬁdence. Probably the straightforward way\nto proceed is to re-classify the rejected instances using a\ndifferent kind of classiﬁer or even, represent such rejected\ninstances using a different feature set and submit them to\nfurther classiﬁcation steps [3, 5].\nThe solution that we propose in this paper is to make\nuse of all information provided at the output of the clas-\nsiﬁer, that is, the ranked list of the Nbest music genre\nhypothesis [P(!1jx);:::;P (!cjx)]T, to deal with the re-\njected instances. The main idea is to re-classify the re-\njected instances using specialized classiﬁers. To such an\naim we assume that the baseline classiﬁers are somewhat\ntrustworthy. This means that even if the baseline classiﬁer\nis not able to rank the correct music genre at the TOP 1\nposition, the correct genre hypothesis will show up among\nthe ﬁrst best hypothesis. Therefore, we need to analyze\ncarefully the output of the baseline classiﬁer to understand\nits behavior. In particular, our interest is to ﬁnd out if there\nis any relevant information that can be used to guide us in\nbuilding more specialized classiﬁers.\nTo such an aim, we look at the confusion matrix of the\nbaseline classiﬁers. A confusion matrix shows us how the\nerrors are distributed across the classes. Our prime interest\nis to ﬁnd out where the most signiﬁcant misclassiﬁcation\nhas occurred. In particular, we look for large off-diagonal\nentries of the matrix which might indicate a difﬁcult two-\nclass problem that needs to be tackled separately. Based\non the analysis of the confusion matrix, we can build two-\nclass classiﬁers to handle the most signiﬁcant confusions.\nConsider a setDofLtwo-class classiﬁers. Once the\nTOP 1 music genre hypothesis is rejected we look at the\nclass of the second best music genre hypothesis, denotedas!00which is computed by Equation 2:\n!00= arg max2 !1\u0014!i\u0014!cP(!ijx) (2)\nwhere max2 is an operator that returns the second greatest\nprobability provided by the classiﬁer.\nLet!0and!00the music genre assigned to the TOP 1\nand TOP 2 hypothesis respectively, then we verify if there\nexists a binary classiﬁer, so far called veriﬁer, D(!0;!00)2\nDto handle the confusion between the music genres !0\nand!00. If so, the veriﬁcation stage is invoked. Otherwise\na ﬁnal decision is taken and the input vector xis rejected.\nThe complete classiﬁcation and veriﬁcation approach can\nbe summarized by the pseudo-code as follows:\nCLASSIFY INSTANCE (x;B;D)\n1 // Input: An instance represented by a feature vector x, a\n2 // multiclass baseline classiﬁer B, a set of two-class\n3 // veriﬁersD\n4 // Output: A music genre assigned to xor the\n5 // rejection of x\n6 ifPB(!0jx)\u0015\u0015then\n7 return!0as the music genre\n8 else\n9 readP(!00jx)\n10 ifD!0;!002D then\n11 classifyxwithD!0;!00\n12 return arg max !0;!00fPD(!0jx);PD(!00jx)gas\n13 the music genre\n14 else\n15 reject the instance x\n16 endif\n17 endif\nThe veriﬁcation stage is loosely coupled to the baseline\nclassiﬁer. The only information provided by the baseline\nclassiﬁers is the label of the two best hypothesis, say !0\nand!00. Such information is used to selected the proper\nveriﬁer. The veriﬁer, may or may not use the same feature\nset and classiﬁcation algorithm of the baseline, however,\nin this paper we have chosen a different feature set which\nis based on the texture features extracted from the spec-\ntrogram of the music signal [1]. This feature set was cho-\nsen because it has provided very interesting discriminat-Nbest hypothesisCorrect Classiﬁcation Rate (%)\nBaseline SVM Baseline MLP\nTOP 1 53:00\u00060:67 52:11\u00063:09\nTOP 2 71:11\u00061:39 70:34\u00062:62\nTOP 3 81:33\u00060:00 79:82\u00062:93\nTOP 5 91:00\u00061:45 92:10\u00061:09\nTOP 7 95:89\u00060:84 95:21\u00062:27\nTOP 8 97:78\u00060:96 98:33\u00061:03\nTable 1 . Correct classiﬁcation rate for the baseline classi-\nﬁers\ning results in the previous works [1]. This 59-dimensional\nfeature vector is used to train binary SVM veriﬁers. An\noverview of the complete approach is shown in Figure 3.\n5. EXPERIMENTAL RESULTS\nThe performance of the classiﬁcation-veriﬁcation approach\nwas evaluated on a subset of the LMD [11]. The LMD is\nmade up of 3,227 full-length music pieces uniformly dis-\ntributed along 10 classes: Ax ´e (Ax), Bachata (Ba), Bolero\n(Bo), Forr ´o (Fo), Ga ´ucha (Ga), Merengue (Me), Pagode\n(Pa), Salsa (Sa), Sertaneja (Se), and Tango (Ta). In our ex-\nperiments, we use 900 music pieces from the LMD, which\nare split into 3 folds of equal size (30 music pieces per\nclass). The splitting is done using an artist ﬁlter, which\nplaces the music pieces of an speciﬁc artist exclusively in\none, and only one, fold of the dataset. Furthermore, in our\nparticular implementation of the artist ﬁlter we added the\nconstraint of the same number of artists per fold. There-\nfore, all results reported in this section refer to 3-fold cross\nvalidation, unless otherwise noted.\n5.1 Baseline Classiﬁers\nThe ﬁrst baseline classiﬁer is a 10-class multilayer percep-\ntron neural network with 68 input units, 40 units at the hid-\nden layer and 10 units at the output layer and the following\nlearning parameters: step width = 0.2, momentum = 0.5,\nﬂat spot elimination = 0.1, max non-propagated error = 0.1,\nand 100 learning cycles. After such a number of cycles, the\ngeneralization of the network starts to decrease according\nto the mean squared error measured on a validation dataset.\nThe network provides estimates a posteriori probabilities\nand the value of each output necessary remains between\nzero and one because of the sigmoidal function used. The\nsecond baseline classiﬁer is a 10-class SVM with Gaussian\nkernel. The gamma and cost parameters were found by a\ngrid search on a validation dataset. Pairwise coupling is\nused to handle multi-class classiﬁcation.\nThe performance of the baseline classiﬁers was evalu-\nated using the correct classiﬁcation rate which is deﬁned\nas the ratio between number of samples correctly classi-\nﬁed and the number of samples tested. Table 1 shows the\nperformance of both baseline classiﬁers taken into account\nif the correct music genre is among the TOP Nbest hy-\npothesis. These results support our previous assumption\nthat the baseline classiﬁers provide a somewhat trustwor-\nthy output. For instance, the correct music genre is among\nthe TOP 5 best hypotheses for more than 91% of the cases.Class Ax Ba Bo Fo Ga Me Pa Sa Se Ta\nAx 41 3 0 0 9 2 10 7 18 0\nBa 2 67 4 5 1 3 3 3 2 0\nBo 1 4 45 6 2 1 10 11 5 5\nFo 0 6 3 43 13 3 8 3 11 0\nGa 13 0 5 8 44 8 6 5 1 0\nMe 1 3 1 2 6 66 3 5 3 0\nPa 8 4 11 7 0 1 38 15 5 1\nSa 12 2 9 3 3 12 12 33 4 0\nSe 14 1 7 9 7 2 6 9 35 0\nTa 1 0 7 0 3 0 0 0 0 79\nTable 2 . Confusion matrix for the veriﬁcation set\nFigure 4 . ROC curve for the SVM baseline and MLP base-\nline.\nThe information on Table 1 opens up a plenitude of\nways to improve the performance of the baseline classi-\nﬁers. However, this is not the goal of this paper. Here we\nfocus on the concept of rejection in an attempt to improve\nthe reliability of the baseline classiﬁers, recalling that reli-\nability is the proportion of correct answers among the ac-\ncepted instances as a function of the rejection rate.\n5.2 Rejection Option\nIn this section we evaluate how the rejection can improve\nthe reliability of the baseline classiﬁers. We measure the\nrejection accuracy in terms of its rate of erroneous behavior\nfor each input as false rejection rate FRR =FR=(FR+\nCA)on instances which had been recognized correctly,\nand false acceptance rate FAR =FA=(FA+CR)on\ninstances which had been misrecognized, where CA de-\nnotes correct acceptance, CR denotes correct rejection, FA\ndenotes false acceptance, and FR denotes false rejection.\nThese two types of error naturally trade off; for example,\nraising the rejection threshold reduces FAR but at the cost\nof increased FRR. Therefore, for each measure, we sweep\na rejection threshold ( \u0015) across its entire range of values,\nplotting the two error types as a receiver-operating charac-\nteristic (ROC) curve. A curve reaching closer to the origin\nindicates a superior conﬁdence measure, one enabling low\nrates of both error types simultaneously. Figure 4 shows\nthe ROC curves as a function of the rejection rate which is\ndeﬁned in terms of the \u0015value.\nFigure 4 shows that the curve reaches close to the ori-Figure 3 . An overview of the complete approach including classiﬁcation, rejection and veriﬁcation\nRejection Rate (%)Error Rate (%)\nBaseline SVM Baseline MLP\n0 47.00 47.89\n20 38.59 40.68\n40 30.46 32.18\n50 23.94 30.34\nTable 3 . Reduction on the error rate for different rejection\nrates\ngin for FAR and FRR equal 0.3. This corresponds to re-\njection rates between 20% and 50%. Table 3 shows the\ncorresponding error rates for these rejection levels. There-\nfore, for the remainder of the paper we consider 40% of re-\njection rate. At this rejection rate the reliability for music\ngenre increases from 53.00% to 69.54% and from 52.11%\nto 67.82% for the SVM and MLP classiﬁer respectively.\nThis represents an improvement of about 16% which is\nvery interesting in the context of music genre classiﬁca-\ntion.\n5.3 Veriﬁers\nSince the aim of this paper is not to handle all possible con-\nfusions but to show how the rejection and veriﬁcation can\nimprove the reliability of music genre classiﬁcation sys-\ntems, we have built very few veriﬁers among the 45 possi-\nble ones. Therefore, there are binary veriﬁers only to deal\nwith the most signiﬁcant confusions which were found by\nanalyzing the confusion matrix generated from the output\nof the baseline classiﬁers on a validation dataset. The two-\nclass veriﬁers employ the support vector machines algo-\nrithm with Radial Basis Function kernel and trained with\nthe sequential minimal optimization method. A grid-search\nalgorithm was used to optimize the cost and the gamma pa-\nrameters. The two-class classiﬁers were built based on the\nanalysis of the confusion matrix shown in Table 2. Since\nour goal is not to handle all the confusion of the base clas-\nsiﬁers, in this table are highlighted the two highest confu-\nsions, which are between classes Ax ´e and Sertaneja and\nbetween classes Pagode and Salsa. Therefore, two binary\nveriﬁers were built to deal with the confusing classes D=\nfDAx,Se;DPa,Sag.ClassiﬁerCorrect Classiﬁcation Rate (%)\nAx-Se Class Pa-Sa Class\nBaseline SVM 42:22\u00060:12 39:44\u00060:01\nBaseline MLP 43:33\u00060:11 40:56\u00060:13\nVeriﬁer 82:08\u000613:05 78:33\u00063:33\nTable 4 . Correct classiﬁcation rates for two classes\nApproach Rejection Rate (%) Reliability (%)\nSVM 0 53:00\u00060:67\nMLP 0 52:11\u00063:09\nSVM + Rej 40 69:54\u00060:99\nMLP + Rej 40 67:82\u00063:68\nSVM + Rej + Verif 40 71:35\u00061:00\nMLP + Rej + Verif 40 69:19\u00063:93\nTable 5 . Reliability for the baseline SVM and MLP, base-\nline+rejection (Rej) and baseline+rejection+veriﬁcation\n(Verif)\nThe performance of the two-class veriﬁers is shown in\nTable 4. The results refer to the same 3-fold cross vali-\ndation protocol but for the veriﬁer we have just a subset\nwhere each fold holds only the instances labeled with the\nconfusing classes. We include in this table also the perfor-\nmance of the baseline classiﬁers considering only the joint\ncorrect classiﬁcation rate on the instances of these pair of\nclasses.\nTable 4 shows, that as expected, the veriﬁer achieves a\nmuch higher rate than the baseline classiﬁers. However,\nour main aim is to improve the overall reliability using\nthe veriﬁer. Therefore, we apply the approach proposed\nin Section 4, where we submit an instance to the veriﬁer if\nit is rejected and if it is classiﬁed by the baseline classiﬁer\nat the TOP 1 and TOP 2 positions as one of the two pair\nof classes: Ax-Se or Se-Ax and Pa-Sa or Sa-Pa. Table 5\nsummarizes the results of the complete approach.\nTable 5 shows that both rejection and veriﬁcation are\neffective in improving the correct music genre classiﬁca-\ntion rate. Rejection brings about an impressive increasing\nin the correction classiﬁcation rate, however, 40% of the\ninstances were rejected and should be handled in a differ-\nent way, such as by humans. An alternative way is to havea second stage to reprocess the rejected instances. Even\nif we have not handled all the rejected instances, but only\nthose who felt on the most confusing classes that we have\nchosen, it is possible to observe a further, but moderated,\nimprovement in the classiﬁcation rates.\nThe Friedman test with the post hoc Shaffer’s static pro-\ncedure was employed to evaluate if there are statistically\nsigniﬁcant differences between the results show in Table 5.\nThe multiple comparison statistical test has show that the\np-value is lower than the corrected critical value in most\nof the cases, showing a statistically signiﬁcant difference\nbetween the baseline, baseline+rejection and baseline+ re-\njection+veriﬁcation at 95% conﬁdence level.\n6. CONCLUSIONS\nIn this paper we have presented a rejection and veriﬁcation\napproach to automatic music genre classiﬁcation that post-\nprocess the output of a baseline classiﬁer in an attempt to\nimprove the reliability in classifying music genres. The\noutput of the baseline classiﬁers is evaluated and the prob-\nabilities provided by these classiﬁers serve as a guide to\neither reject or accept the input instance. Furthermore,\nthe rejected instances may be re-classiﬁed at a veriﬁcation\nstage using a different approach if they were previously\nclassiﬁed by the baseline classiﬁer as belonging to speciﬁc\nclasses. The performance resulting from the combination\nof the baseline, rejection and veriﬁcation is signiﬁcantly\nbetter than that achieved by the baseline classiﬁers alone.\nFor instance, the baseline classiﬁers alone achieves a clas-\nsiﬁcation rate of 53%. The rejection stage improves the\nreliability to 69.54% at a rejection level of 40% and the\nveriﬁcation stage further improves it to 71.35%. In spite\nof the current veriﬁcation stage deal with only two pair of\nconfusing classes, it was able to improve the reliability in\nalmost 2%. Given the high number of confusions between\nother classes, as show in Table 2, we expect to achieve fur-\nther improvement by adding more veriﬁers out of the 45\npossible ones.\nCompared with previous works that use the same fea-\ntures, the same dataset, and the same experimental pro-\ntocol [1, 7], the results reported in this paper represent a\nsigniﬁcant improvement in terms of classiﬁcation rate and\nreliability. It is difﬁcult to compare the performance of the\nproposed approach with other results available in the liter-\nature due to the differences in the experimental conditions.\nIn spite of the good results achieved, there are some\nshortcomings related to the use of the second stage. For\ninstance, the second stage depends on the results of the\nﬁrst stage and on the availability of a binary classiﬁer to\nhandle the confusions between speciﬁc classes. As future\nwork we plan to validate the proposed approach on other\ndatasets, such as the Magnatagatune and the Million Song\nDataset.\n7. ACKNOWNLEDGMENTS\nThis research is partially supported under CAPES/Fulbright\ngrant BEX1770/712-9, FA grant 203/12 and CNPq grants306.703/2010-6 and 472.238/2011-6.\n8. REFERENCES\n[1] Y .M.G. Costa, L.E.S. Oliveira, A.L. Koerich, F. Gouyon, and\nJ.G. Martins. Music genre classiﬁcation using LBP textural\nfeatures. Signal Processing , 92(11):2723–2737, 2012.\n[2] Z. Fu, G. Lu, K. M. Ting, and D. Zhang. A survey of audio-\nbased music classiﬁcation and annotation. IEEE Trans. on\nMultimedia , 13(2):303–319, 2011.\n[3] A.L. Koerich. Rejection strategies for handwritten word\nrecognition. In Int’l Workshop on Frontiers in Handwriting\nRecognition , pp.479–484, Tokyo, Japan, 2004.\n[4] A.L. Koerich, L.E.S. Oliveira, and A.S. Britto Jr. Veriﬁca-\ntion of unconstrained handwritten words at character level. In\nInt’l Conf. on Frontiers in Handwriting Recognition , pp.39–\n44, Kolkata, 2010.\n[5] A.L. Koerich, R. Sabourin, and C. Y . Suen. Recogni-\ntion and veriﬁcation of unconstrained handwritten words.\nIEEE Trans. on Pattern Analysis and Machine Intelligence ,\n27(10):1509–1522, 2005.\n[6] T. Lidy, C.N. Silla, O. Cornelis, F. Gouyon, A. Rauber,\nC.A.A. Kaestner, and A.L. Koerich. On the suitability of\nstate-of-the-art music information retrieval methods for an-\nalyzing, categorizing and accessing non-western and eth-\nnic music collections. Signal Processing , 90(4):1032–1048,\n2010.\n[7] M. Lopes, F. Gouyon, A.L. Koerich, and L.E.S. Oliveira. Se-\nlection of training instances for music genre classiﬁcation. In\nInt’l Conf. on Pattern Recognition , pp.4569–4572, Istambul,\nTurkey, 2010.\n[8] S.R. Ness, A. Theocharis, G. Tzanetakis, and L.G. Martins.\nImproving automatic music tag annotation using stacked gen-\neralization of probabilistic SVM outputs. In ACM Multime-\ndia Conf. , pp.705–708, Beijing, China, October 2009.\n[9] N. Scaringella, G. Zoia, and D. Mlynek. Automatic genre\nclassiﬁcation of music content - a survey. IEEE Signal Pro-\ncessing Magazine , 23(2):133–141, 2006.\n[10] J. Schurmann. Pattern Classiﬁcation: A Uniﬁed View of Sta-\ntistical and Neural Approaches . John Wiley and Sons, 1996.\n[11] C.N. Silla, A.L. Koerich, and C.A.A. Kaestner. The Latin\nmusic database. In Int’l Conf. on Music Information Re-\ntrieval , pages 451–456, Philadelphia, USA, 2008.\n[12] C.N. Silla, A.L. Koerich, and C.A.A. Kaestner. A machine\nlearning approach to automatic music classiﬁcation. Journal\nof the Brazilian Computer Society , 14(3):7–18, September\n2008.\n[13] B. L. Sturm. A survey of evaluation in music genre recogni-\ntion. Adaptive Multimedia Retrieval , 2012.\n[14] B. L. Sturm. Two systems for automatic music genre recog-\nnition: What are they really recognizing. In Int’l ACM Work-\nshop on MIR with User-centered and Multimodal Strat. ,\npp.69–74, Nara, Japan, 2012.\n[15] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of\naudio signals. IEEE Trans. on Speech and Audio Processing ,\n10(5):293–302, 2002.\n[16] T.-F. Wu, C.-J. Lin, and R.C. Weng. Probability estimates\nfor multi-class classiﬁcation by pairwise coupling. Journal\nof Machine Learning Research , 5:975–1005, 2004."
    },
    {
        "title": "A Study of Cultural Dependence of Perceived Mood in Greek Music.",
        "author": [
            "Katerina Kosta",
            "Yading Song",
            "György Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415748",
        "url": "https://doi.org/10.5281/zenodo.1415748",
        "ee": "https://zenodo.org/records/1415748/files/KostaSFS13.pdf",
        "abstract": "Several algorithms have been developed in the music information retrieval community for predicting mood in music in order to facilitate organising and accessing large audio collections. Little attention has been paid however to how perceived emotion depends on cultural factors, such as listeners’ acculturation or familiarity with musical background or language. In this study, we examine this dependence in the context of Greek music. A large representative database of Greek songs has been created and sampled observing predefined criteria such as the balance between Eastern and Western influenced musical genres. Listeners were then asked to rate songs according to their perceived mood. We collected continuous ratings of arousal and valence for short song excerpts and also asked participants to select a mood tag from a controlled mood vocabulary that best described the music. We analysed the consistency of ratings between Greek and non-Greek listeners and the relationships between the categorical and dimensional representations of emotions. Our results show that there is a greater agreement in listener’s judgements with Greek background compared to the group with varying background. These findings suggest valuable implications on the future development of mood prediction systems.",
        "zenodo_id": 1415748,
        "dblp_key": "conf/ismir/KostaSFS13",
        "keywords": [
            "music information retrieval community",
            "predicting mood in music",
            "organising and accessing large audio collections",
            "perceived emotion",
            "cultural factors",
            "acculturation",
            "familiarity with musical background",
            "language",
            "Greek music",
            "large representative database"
        ],
        "content": "A STUDY OF CULTURAL DEPENDENCE OF PERCEIVED MOOD IN\nGREEK MUSIC\nKaterina Kosta, Yading Song, Gy ¨orgy Fazekas, Mark B. Sandler\nCentre for Digital Music, Queen Mary University of London\nfirstname.lastname@eecs.qmul.ac.uk\nABSTRACT\nSeveral algorithms have been developed in the music in-\nformation retrieval community for predicting mood in mu-\nsic in order to facilitate organising and accessing large au-\ndio collections. Little attention has been paid however to\nhow perceived emotion depends on cultural factors, such\nas listeners’ acculturation or familiarity with musical back-\nground or language. In this study, we examine this depen-\ndence in the context of Greek music. A large representa-\ntive database of Greek songs has been created and sampled\nobserving predeﬁned criteria such as the balance between\nEastern and Western inﬂuenced musical genres. Listen-\ners were then asked to rate songs according to their per-\nceived mood. We collected continuous ratings of arousal\nand valence for short song excerpts and also asked partici-\npants to select a mood tag from a controlled mood vocab-\nulary that best described the music. We analysed the con-\nsistency of ratings between Greek and non-Greek listen-\ners and the relationships between the categorical and di-\nmensional representations of emotions. Our results show\nthat there is a greater agreement in listener’s judgements\nwith Greek background compared to the group with vary-\ning background. These ﬁndings suggest valuable implica-\ntions on the future development of mood prediction sys-\ntems.\n1. INTRODUCTION\nA large body of research [3, 5, 16] supports that music can\neither evoke emotions in listeners, a phenomenon known\nas felt emotion, or express emotion, known as perceived\nmood. For this reason, understanding emotion or mood1is\nuseful in daily experiences listening to music. Several Mu-\nsic Information Retrieval (MIR) related studies [2, 19, 20]\nhave demonstrated the importance of mood-based organi-\nsation when accessing music catalogues. For instance, Lee\n[20] showed that participants of a survey conducted to as-\nsess the relevance of different modalities in music search-\ning and browsing would use emotional or mood states in\n1Albeit we acknowledge the difference between emotion and mood,\nin this work, the terms will be used interchangeably.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.their queries. More recently, Bischoff [2] reported that\nover 15% of the song queries on the Web-based music ser-\nvice Last.fm were made using mood tags. MIR systems\nthat facilitate the use of mood to interact with music rely\non speciﬁc mood models. For instance, categorical organ-\nisation, or dimensional representation of emotion stated in\na mathematical space associates mood with musical audio,\ntags or other information sources using selected machine\nlearning algorithms [3,9]. In this paper, a dimensional rep-\nresentation of emotion is demonstrated.\nA wealth of research focuses on the above problems in\nthe context of Western musical culture [5, 29], assuming\nthat generic models can be built independently from mu-\nsical style, culture, genre or listeners’ acculturation. This\nstudy investigates these issues in the context of Greek mu-\nsical culture. In particular, we focus on the following re-\nsearch questions:\n1. Do Greek and non-Greek people agree with each\nother on the ratings of perceived arousal and valence?\n2. Are there any differences in the music perception of\nWestern and Eastern inﬂuenced musical genres be-\ntween Greek and non-Greek people?\n3. Are speciﬁc mood labels originated from Western\nmusical studies suitable for Greek music?\nThe rest of the paper is organised as follows: In Sec-\ntion 2, related work on the ﬁeld is presented as well as\ncases that show why Greek music is particularly relevant\nfor study. Due to the unique properties of Greek musical\nculture, we investigate the culture dependence of valence-\nArousal (V A) ratings and tagging in the context of Greek\nmusic. Section 3 outlines the mood classiﬁcation models\nand music perception background considered. In Section\n4, we analyse the database collection that we created and\nused. In Section 5 we present the design of the study, while\nin Section 6 we show our results. In Section 7 we conclude\nand outline some directions for the future work.\n2. BACKGROUND\n2.1 Cross-Cultural Mood classiﬁcation\nIt is well known that music emotion perception is inﬂu-\nenced by cultural background. For instance several stud-\nies have attempted to explore the emotion perception of\npop songs between American and Chinese listeners [33],\nthe music emotion classiﬁcation between Chinese and En-\nglish songs [14], and perceived complexity of Western andAfrican folk melodies by Western and African listeners\n[6]. To our knowledge, no previous studies have been per-\nformed using Greek music which holds particular interest\nas it features compositions inﬂuenced by both Eastern and\nWestern culture.\n2.2 Why do we use Greek Music?\nGreek music databases do exist, and many of them can be\nfound on the Internet. However they are either related to\nspeciﬁc music labels or come from individuals, hence they\nare limited in their scope. A broader database is provided\nby the Greek Music Information Centre of the Institute for\nResearch on Music and Acoustics2. This database how-\never lacks popular and modern Greek music metadata.\nA study for using principles of MIR Systems in Greek\nMusic is described in [17], where a case study for the usage\nof the “Music Control” system by Nielsen - VNU is pre-\nsented. This system provides an airplay monitoring service\nbased on MIR principles, but it struggles with two kinds of\nissues. Firstly, although it could be efﬁciently used in the\ncontext of musical genres such as pop and rock, it could\nnot be utilised for Greek musical genres such as “Entehno”\n(described as “Alternative” in Section 3). Secondly, the\nsystem was misleading due to the “secondary importance”\nof the international musical menu, counting their airplay\ntime. The above indicate the uniqueness of Greek music,\nwhich provides the motivation for our further exploration.\n3. MOOD CLASSIFICATION AND MUSIC\nPERCEPTION\n3.1 Representation of Emotions\nSeveral representations of emotion or mood have been pro-\nposed in psychology and related disciplines. For recent\nthorough reviews in the context of music see for instance\n[5] or [3]. Among these representations, the categorical\nmodel , which assumes that emotion may be represented\nas a set of distinct categories, and the dimensional model ,\nwhich characterises emotions using a small number of di-\nmensions corresponding to the internal human represen-\ntations are paramount. Categorical approaches have been\ncriticised since they constrain emotions to be represented\nas a set of predeﬁned families or landmarks [23]. As a re-\nsult, dimensional models have emerged as dominant, with\nperhaps the most inﬂuential work being Russell’s circum-\nplex model of affect [25]. This consists of a two-dimensional\ncircular structure involving the core affect dimensions of\narousal andvalence . A space corresponding to these di-\nmensions is often referred to as the V A space. The rele-\nvance of these dimensions in music, where emotions may\nbe deﬁned in terms of arousal or energy (i.e. how excit-\ning or calming musical pieces are) and valence or stress,\n(i.e. a dimension of positive or negative emotions), were\nexperimentally validated by Thayer [30].\n2http://www.musicportal.gr/?lang=en3.2 Music Emotion Recognition\nEarly music emotion recognition (MER) studies focused\non categorical models and approached emotion recognition\nfrom audio as a classiﬁcation or auto-tagging problem (see\ne.g. [21, 28]). However, due to the limitations of categor-\nical models, (e.g. ambiguity in the meaning of adjectives\nassociated with emotion categories, and the potential het-\nerogeneity of the taxonomical organisation) recent studies\nhave focussed on continuous valued dimensional models.\nIn the ﬁrst study that addresses these issues [34], MER is\nformulated as a regression problem in order to map high-\ndimensional features extracted from audio to the V A space\ndirectly. However, the dimensional model also presents\nsome barriers, for instance, it is difﬁcult to use in browsing\nand searching, where interfaces based on generic or music\nrelated emotion tags dominate.\n3.3 Music mood tags\nIn order for dimensional representations to be ultimately\nuseful in conventional browsing interfaces, it is necessary\nto establish connections between continuous valued emo-\ntion states and mood tags. As a solution, one may use the\nAffective Norm for English Words (ANEW) database [1]\nwhich contains V A values for a large number of English\nwords derived from psychological experiments. However,\nthese values are not validated in the context of music. More\nrecently, Saari and Eerola [26] proposed a technique called\nAffective Circumplex Transformation (ACT) which uses\nVector Space Modelling, Latent Semantic Analysis (LSA),\nMulti Dimensional Scaling (MDS) and Procrustes analy-\nsis [10] to derive dimensional mood models from music\nrelated social tags (such as those available form Last.fm)\nwhose axes, among other possible dimensions, may be in-\nterpreted in terms of V A. This method has been validated\nagainst human ratings using a listening test, and found to\nbe robust in the context of 600 songs drawn predominantly\nfrom Western music genres. It was also shown to be useful\nin the context of curated editorial tags used by production\nmusic libraries [27].\nThese studies, as well as most recent works in MIR and\nmusic emotion recognition have attempted to use generic\nmood models assuming that the relation between continu-\nous emotion states and mood words is independent from\nmusical style, genre or culture. A notable exception is the\nwork of Eerola [7] demonstrating the genre speciﬁcity of\nmood modelling. In this present work, we investigate the\nlast issue further.\n4. GREEK MUSIC DATABASE AND ANALYSIS\nIn order to perform our study, a music database was created\nwhich includes tracks from the Greek musical scene from\nthe 1970’s until recent years, containing the most critically\nacclaimed compositions. It is a continuously growing re-\nsource, containing 2087 songs at present, all taken from\npersonal collections3.\n3The Greek songs metadata can be found online at\nhttp://greek music data.isoftcloud.grIn order for efﬁcient categorisation, certain descriptions\nsuch as genre titles have been taken from the Western mu-\nsical vocabulary, however, new genre titles have also been\nintroduced to best describe the music. This is crucial be-\ncause of the principles described in [31] and also because\nthe structural categorisation that musical genres provide is\nimportant for MIR systems. The ﬁnal number of the cate-\ngories as shown in Table 1, is 25. Out of these, 16 genres\nidentiﬁed using grey cells in Table 1 were used for our ex-\nperiment.\nWestern Inﬂuenced\nAlternative Pop (AP) Pop (P)\nBallad (B) Progressive Rock\nBlues Rap (Ra)\nDance (D) Reggae (Re)\nElectronic Rock (R)\nHip Hop (HH) Ska\nLatin Tango\nLounge Trip Hop\nNew Age (NA)\nEastern Inﬂuenced\nAlternative (A) Laiko-Pop (LP)\nCretan Laiko-Rock (LR)\nExperimental (E) Tsifteteli (T)\nLaiko (L) Zeibekiko (Z)\nTable 1 . Greek Database Musical Genres. Samples from\nthe genres indicated using grey background have been used\nfor the experiment.\nThe primary demarcation between Western and Eastern\ninﬂuenced musical groups depends on the song’s rhythm\npattern and the origination of the instruments used. The\ngenres presented as Western inﬂuenced are deﬁned in sim-\nilar terms as corresponding music in standard Western cul-\nture.\nThe Eastern inﬂuenced genres consist of the following\ncategories: “Alternative” includes compositions which ei-\nther have speciﬁc tempo variations, or are inﬂuenced by\ndifferent Eastern musical styles within the same piece. “Cre-\ntan” is folk music originated from the Greek island of Crete.\nThe music pieces in “Experimental” comprise experimen-\ntal music with a strong presence of Eastern culture. “Laiko”,\nor otherwise “General folk” includes compositions inspired\nby popular old folk song arrangements, usually following\nthe rhythm known as “Syrtos”. It is a rhythm of the round\ndance, originating from ancient Greece that was claimed\nby ancient Greek theory “for the heroic hexameter, that is,\nthe rhythm of Homer”. It consists of three counts, in which\n“the ﬁrst is longer by one-half than the second and third”\n[11]. “Laiko-Pop” and “Laiko-Rock” are deﬁned as large\nsub-categories of “Laiko”, where the inﬂuence of rhythm\npatterns and instrumentation from “Pop” and “Rock” re-\nspectively are present. The two last genres are “Tsifteteli”\nand “Zeibekiko” named for their respective dance styles.\nTsifteteli is constituted by pieces whose rhythm contains\ntwo counts, while Zeibekiko has similarities with the turk-ish dance “Zeybek” following the rhythm of nine counts\n[24].\n5. EXPERIMENT DESIGN\n5.1 Database Sampling\n108 songs have been selected in total from a sub-category\nof our database with their lyrics in Greek. We used pseudo-\nrandom sampling observing several criteria to ensure bal-\nanced genres and artists. The output of the algorithm se-\nlects songs such that there is a maximum 7 songs per genre,\nwith each artist appearing no more than once. The most\nrepresentative 30 second musical excerpt was selected for\neach song manually such that it has a lyrical part, except\nfor 3 excerpts from the “Experimental” genre.\n5.2 Listening test\nOur experiment includes an online listening test. Its ﬁrst\npage welcomes the user and introduces the subjects with\ncrucial instructions. After selecting either the English or\nGreek-translated version of the test, the system presents a\nform to ﬁll for personal information, including name, age\nrange, gender and nationality. Moreover, it presents the\nuser with a questionnaire about his/her musical culture and\nabout the type of music that he is most familiar with.\nThe second part consists of the questions detailed be-\nlow, following the Goldsmiths Musical Sophistication In-\ndex, about the musical training level [22]. Here, the an-\nswers to the questions 1-3 may be given as discrete rat-\nings on a 7 point Likert scale: (1: Completely disagree, 2:\nStrongly disagree, 3: Disagree, 4: Neither agree nor dis-\nagree, 5: Agree, 6: Strongly agree, 7: Completely agree).\n1.I have never been complimented for my talents as a musical performer\n2.I can’t read a musical score\n3.I would not consider myself a musician\n4.For how many hours I engage in regular, daily practice of a musical\ninstrument?\n5.At the peak of my interest, how many hours per day did I practice on\nmy primary instrument?\n6.For how many years have I played or sung in a group, band, choir, or\norchestra?\n7.For how many years did I have formal training in music theory?\n8.For how many years did I have formal training on a musical instru-\nment?\n9.How many musical instruments can I play?\nThe following listening test procedure is repeated for\neach song; the subject is presented with an audio excerpt\nand rates the mood suggested by the music in terms of va-\nlence and arousal level by selecting a location on a 2D\nplane representation. This user interface was inspired by\nthat of the MoodSwings game developed to collect music\nemotion labels from human listeners [32]. Then, a list of\nthe twenty closest emotion tags to the selected position is\nproposed. The subject may choose the one that best de-\nscribes the suggested mood. The tag positions were de-\nrived from Last.fm data using the ACT transformation [26]\nmentioned in Section 3.3.An optional post-experiment questionnaire was sent to\nall subjects after completing the listening test. The possi-\nble answers were designed to use the same rating scale as\nbefore:\nRate the following 2 sentences:\na. I could ﬁnd a tag in the list that was exactly (or really close to) what I\nwanted to label for the perceived mood of every song.\nb. I listened to a song from my favourite genre, at least once.\n5.3 Participants\nTwenty-two Greek participants (10 male and 12 female)\nand twenty non-Greek participants (15 male and 5 female)\ntook part in the study. Their ages ranged between 18 to 54.\nThey were recruited without consideration of their musical\ntraining. Their musical training level was assessed using\nthe questionnaire shown in Section 5.2. The overall results\nare shown in Table 2.\nMin Max Median SD\nNon-Greek music score 21 43 30 6.07\nGreek music score 21 41 30 5.42\nTable 2 . Musical training level score for Greek and non-\nGreek participants: min. and max. value, median and Stan-\ndard Deviation. Ascending scale from 9 to 63.\n6. RESULTS\nTo answer the research questions as described in Section 1,\nthe following four experiments were carried out. The re-\nsults were aggregated separately in Eastern-inﬂuenced and\nWestern-inﬂuenced musical genres.\n6.1 Effects of Eastern-inﬂuenced and\nWestern-inﬂuenced musical genres across Greek and\nnon-Greek listeners on valence and arousal (V A)\nratings\nTo ﬁnd out the effects of culture-biased musical genres be-\ntween Greek and non-Greek listeners on the V A ratings,\nwe used the two-way analysis of variance (factor 1: musi-\ncal genre, Western-inﬂuenced and Eastern-inﬂuenced, fac-\ntor 2: musical culture, Greek and non-Greek). Signiﬁcant\nresults were found in the ratings of valence for national-\nities ( F(1,0.08) = 72.56, p<0.001) and Eastern-Western\ninﬂuenced genres ( F(1,0.005) = 4.5, p<0.05). Similar re-\nsults were revealed in the ratings of arousal on nationali-\nties ( F(1,0.19) = 167.16, p<0.001), Eastern-Western inﬂu-\nenced musical genres ( F(1,0.006) = 5.7, p<0.05) as well as\nthe interaction of these two factors ( F(1,0.00524) = 4.67,\np<0.05). However, no signiﬁcant result was found for the\ninteraction between culture and nationality on valence rat-\nings.\n6.2 Comparison of Greek and non-Greek responses\non V A\nTo compare the difference in non-Greek and Greek responses\non V A ratings, the Wilcoxon Signed Rank test was carriedout across all the listeners on the mean value of V A rat-\nings. A signiﬁcantly higher standard deviation value was\nfound in the ratings of non-Greek people for both valence\n(p<5.1427e-13) and arousal ( p<6.6826e-17). This sug-\ngests that the ratings of Greek listeners are more consistent\nthan those of non-Greek listeners, since Greek listeners are\nmore familiar with these musical excerpts.\n6.3 Comparison of emotion perception across Western\ninﬂuenced and Eastern inﬂuenced musical genres\nbetween Greek and non-Greek participants\nAs mentioned in section 6.1, (Western-inﬂuenced or Eastern-\ninﬂuenced) musical genres have an effect on V A ratings.\nPrevious studies also showed a link between particular emo-\ntions and speciﬁc musical genres [8]. Therefore, we inves-\ntigated whether in Greek music, a certain musical genre\nwas linked to a category of emotions. The Wilcoxon Signed\nRank test was conducted to compare the mean V A ratings\nbetween non-Greek and Greek listeners.\nGreek Non-GreekP-ValueGenre Mean SD Mean SD\nEastern Inﬂ.\nAV 0.38 0.10 0.33 0.06 0.08\nA 0.55 0.06 0.43 0.06 0.02\nEV 0.36 0.06 0.32 0.04 0.31\nA 0.42 0.10 0.37 0.07 0.31\nLV 0.68 0.08 0.50 0.10 0.02\nA 0.63 0.07 0.55 0.06 0.05\nLPV 0.64 0.10 0.66 0.06 0.47\nA 0.51 0.11 0.61 0.07 0.02\nLRV 0.57 0.09 0.60 0.11 0.30\nA 0.42 0.09 0.44 0.05 0.69\nTV 0.68 0.06 0.61 0.07 0.08\nA 0.54 0.09 0.53 0.07 1.00\nZ V 0.61 0.09 0.47 0.07 0.02\nA 0.40 0.05 0.47 0.07 0.22\nWestern Inﬂ.\nAPV 0.58 0.07 0.58 0.06 0.58\nA 0.59 0.08 0.56 0.06 0.47\nBV 0.51 0.16 0.47 0.14 0.22\nA 0.42 0.06 0.42 0.05 0.81\nDV 0.73 0.07 0.70 0.07 0.16\nA 0.56 0.06 0.62 0.07 0.16\nHHV 0.66 0.08 0.68 0.05 0.47\nA 0.51 0.13 0.48 0.08 0.58\nNAV 0.39 0.12 0.32 0.07 0.05\nA 0.49 0.10 0.46 0.14 0.11\nP V 0.64 0.08 0.63 0.09 0.94\nA 0.59 0.13 0.63 0.08 0.30\nRaV 0.61 0.06 0.69 0.07 0.06\nA 0.41 0.04 0.41 0.10 0.84\nReV 0.58 0.11 0.54 0.10 0.16\nA 0.66 0.11 0.63 0.09 0.44\nRV 0.73 0.09 0.69 0.08 0.08\nA 0.57 0.08 0.54 0.05 0.22\nTable 3 . Valence (V) - Arousal (A) Mean and SD values\nfor each genre (abbreviations from Table 1). Genres Z and\nP are chosen as representatives of low and high P - value\nrespectively.\nSigniﬁcant differences in the ratings of either valence\nand/or arousal were found in the following genres: “Alter-\nnative (A)”, “Laiko (L)”, “Laiko-Pop (LP)” and “Zeibekiko(Z)”. Interestingly, all these genres belong to Eastern-inﬂu-\nenced musical styles. It substantiates the V A rating re-\nsults, since most Western listeners are especially not famil-\niar with these types of compositions. Likewise, a greater\nconsistency in ratings between Greek and non-Greek lis-\nteners are presented in Western-inﬂuenced genres. Mean\nratings and standard deviations (SD) of Greek and non-\nGreek users in Eastern-inﬂuenced and Western-inﬂuenced\ngenres are shown in the Table 3.\nFigure 1 . V A Ratings of Greek (circles) and non-Greek\n(crosses) for the Eastern-inﬂuenced genre “Zeibekiko”; the\nclusters created by both groups are separable.\nFigure 2 . V A Ratings of Greek (circles) non-Greek\n(crosses) for the Western-inﬂuenced genre “Pop”; the clus-\nters created by both groups overlap.\nFigure 1 shows an example of the Eastern-inﬂuenced\ngenre “Zeibekiko”. The ratings of Greek listeners were\ncloser to high valence and low arousal. However, the rat-\nings of non-Greek listeners were closer to the upper-left\npart of the valence-arousal space. The cluster from Greek\nListeners’ ratings indicates that certain emotions are better\nexpressed by certain genres. In addition, the ratings distri-\nbution of the Western-inﬂuenced genre “Pop” is shown in\nFigure 2. Although no signiﬁcant differences were found\nbetween Greek and non-Greek listeners, the cluster of user\nratings in genre “pop” tends to be in the right top quadrant\nin two-dimensional model.6.4 Examination of the Tag Labels\nTen responses from each group (Greek and non-Greek)\nwere collected to the optional post-experiment question-\nnaire. An interesting question to investigate is the confor-\nmity between their perceived emotion and the proposed tag\nlabels in our system for every song.\nWhen answering the question if they could ﬁnd a tag\nin the list that was exactly or really close to what they\nwanted to label for the perceived mood of every song, the\nmean response from Greek participants is close to “Neither\nagree nor disagree”, while non-Greek participants’ mean\nresponse is close to “Disagree” (see Table 4). To some ex-\ntent, it suggests that our proposed tags can represent Greek\nlisteners but doesn’t work at all for non-Greek listeners.\nHowever, the results remain tentative.\nGreek Non-Greek\nMean 4.2 3.3\nSD 1.32 1.41\nTable 4 . Mean and SD for level of tag agreement (scale\n1-7).\n7. CONCLUSIONS AND FUTURE WORK\nThis study investigates the cross-cultural applicability of\nmood categories and classiﬁcation models of perceived emo-\ntion through different musical genres of Greek music. The\nresponses by Greek people on the rating of music percep-\ntion tend to agree with each other, while this does not ap-\npear to be the case as strongly with non-Greek people. A\nlikely reason for this is differences in acculturation. In-\ndeed signiﬁcant difference can be observed in the percep-\ntion of Eastern inﬂuenced musical genres between these\ntwo groups.\nWe have also examined whether speciﬁc metadata that\nis designed for Western music are applicable to Greek mu-\nsic. The negative results we obtained concerning tag posi-\ntions derived from predominantly Western music seem to\nindicate that either the V A positions for the tags were less\nmeaningful in the context of Greek music, or the tag selec-\ntion mechanism was too constrained.\nIn future work we will compare mood classiﬁcation re-\nsults of perceived emotion from the Western inﬂuenced\nmusical genres with those of Western music tracks. Ad-\nditionally, an experiment with participants of speciﬁc non-\nGreek origination could be designed. Another alternative\nis to include the translation of lyrics for non-Greek partic-\nipants.\nIn addition, expansion of the existed database is fore-\nseen. In the new version, the genre ”Rebetiko” (cf. [13]\nfor details) will be included, and a comparison with other\ndatabases of Greek music containing folk songs from dif-\nferent geographical areas will be made.\nFinally, it will be interesting to examine the validity\nof the results from western designed automatic audio fea-\nture extraction and mood estimation techniques on the less-\nwestern Greek database. This work may be enhanced using\nthe system described in [15] for detecting similar phrases\nin music of the Eastern Mediterranean.8. ACKNOWLEDGEMENTS\nWe thank Charalampos Tampakopoulos, Ph.D. student from\nUniversity of Athens, who designed and organized the Greek\ndatabase metadata and made it public accessible.\n9. REFERENCES\n[1] M. Bradley and P. Lang, “Affective norms for english words\n(anew): Instruction manual and affective ratings.” Technical\nReport C-2. University of Florida, Gainesville, FL. , 2010\n[2] K. Bischoff, C.S. Firan, W. Nejdl, R. Paiu, “Can all tags be\nused for search? ” in Proceeding of the ACM Conference on\nInformation and Knowledge Management (CIKM) . pp. 193–\n202, 2008\n[3] M. Barthet, G. Fazekas, M. Sandler, “Music Emotion Recog-\nnition: From Content to Context-Based Models”, Lecture\nNotes in Computer Science, CMMR 2012 Post-proceedings\n(in press)\n[4] T. Eerola, “A comparison of the discrete and dimensional\nmodels of emotion in music” Psychology of Music , 39(1),\n18–49 (2010)\n[5] T. Eerola and J. K. Vuoskoski. “A review of music and emo-\ntion studies: Approaches, emotion models and stimuli” Mu-\nsic Perception , 30(3):307–340, 2012\n[6] T. Eerola, T. Himberg, P. Toiviainen and J. Louhivuori, “Per-\nceived complexity of Western and African folk melodies by\nWestern and African listeners,” Psychology of Music, Vol. 34,\nNo. 3 , pp. 337-371, 2006\n[7] T. Eerola, O. Lartillot, and P. Toiviainen, “Prediction of mul-\ntidimensional emotional ratings in music from audio using\nmultivariate regression models” in Proceeding of Interna-\ntional Society for Music Information Retrieval (ISMIR) , pp.\n621–626, 2009\n[8] T. Eerola, “Are the emotions expressed in music genre-\nspeciﬁc? An audio-based evaluation of datasets spanning\nclassical, ﬁlm, pop and mixed genres” Journal of New Music\nResearch, V ol. 40, No. 4, pp. 349-366, 2011\n[9] Z. Fu, G. Lu, et al., “A survey of audio-based music classi-\nﬁcation and annotation.” IEEE Transactions on Multimedia ,\n13(2):303 –319, april 2011\n[10] J. Gower, “Generalized procrustes analysis”, Psychometrika ,\nvol. 40, pp. 3351, 1975\n[11] T. Georgiades: “Greek music, verse and dance,” Da Capo\nPress, New York , pp. 134-141, 1973\n[12] A.H. Gregory, N. Varney, “Cross-cultural comparisons in the\naffective response to music,” Psychology of Music , V ol. 24,\npp. 47-52, 1996\n[13] G. Holst-Warhaft, “Road to Rembetika: music of a Greek\nsub-culture, songs of love, sorrow and hashish” Athens,\nDenise Harvey , 1989\n[14] X. Hu and J.H. Lee, “A cross-cultural study of music mood\nperception between American and Chinese listeners,” in Pro-\nceedings of International Society for Music Information Re-\ntrieval (ISMIR) , pp. 535-540, 2012\n[15] A. Holzapfel, Y . Stylianou, “Parataxis: morphological sim-\nilarity in traditional music,” in Proceedings of International\nSociety for Music Information Retrieval (ISMIR) , 2010\n[16] P.N. Juslin, S. Liljestr ¨om, D. V ¨astfj¨all, L.O. Lundqvist, “How\ndoes music evoke emotions? Exploring the underlying mech-\nanisms.” In: P.N. Juslin, J. Sloboda (eds.) Handbook of Music\nand Emotion: Theory, Research, Applications, pp. 605642.\nOxford University Press (2011)[17] G. Kapetsis, “Music information retrieval systems: An in-\nvestigation on business relation impacts and user information\nneeds of an airplay monitoring service in the Greek music in-\ndustry,” Ph.D. Thesis, City University Press , 2006\n[18] M. Levy and M. Sandler, “A semantic space for music de-\nrived from social tags.” in Proceedings of International Soci-\nety for Music Information Retrieval (ISMIR) , 2007\n[19] M. Lesaffre, M. Leman, J.P. Martens, “A user oriented ap-\nproach to music information retrieval” in Proceeding of the\nContent-Based Retrieval Conference , Daghstul Seminar Pro-\nceedings, 2006\n[20] J.A. Lee, J.S. Downie, “Survey of music information needs,\nuses, and seeking behaviors: preliminary ﬁndings.” in Pro-\nceedings of International Society for Music Information Re-\ntrieval (ISMIR) , 2004\n[21] T. Li, M. Ogihara, “Detecting emotion in music.” in Proceed-\ning of International Society for Music Information Retrieval ,\n2003\n[22] D. M ¨ullensiefen, B. Gingras, L. Stewart, and J.J. Musil.\nGoldsmiths Musical Sophistication Index (Gold-MSI) Tech-\nnical report, 2012\n[23] M. Mortillaro, B. Meuleman, R. Scherer, “Advocating a com-\nponential appraisal model to guide emotion recognition” In-\nternational Journal of Synthetic Emotions, Special Issue on\nBeneﬁts and Limitations of Continuous Representations of\nEmotions , 2012\n[24] Oxford Music Online Dictionary, Grove Music Online,\nGreece, IV: Traditional music, pp. 3-4\n[25] J.A. Russell, “A circumplex model of affect.” Journal of per-\nsonality and social psychology 39(6), 1161–1178, 1980\n[26] P. Saari, T. Eerola, “Semantic computing of moods based on\ntags in social media of music.”, IEEE Transactions on Knowl-\nedge and Data Engineering , (in press, manuscript submit-\nted), 2013.\n[27] P. Saari, M. Barthet, G. Fazekas, T. Eerola, M. B. Sandler\n“Semantic models of mood expressed by music: Comparison\nbetween crowd-sourced and curated editorial annotations.”\nInIEEE International Conference on Multimedia and Expo\n(ICME 2013): International Workshop on Affective Analysis\nin Multimedia (AAM) .\n[28] Y . Song, S. Dixon, M. Pearce, “Evaluation of musical fea-\ntures for emotion classiﬁcation.” in Proceedings of Inter-\nnational Society for Music Information Retrieval (ISMIR) ,\n2012.\n[29] Y . Song, S. Dixon, M. Pearce, G. Fazekas, “Using tags to\nselect stimuli in the study of music and emotion”. In The 3rd\nInternational Conference on Music & Emotion , 2013.\n[30] J. F. Thayer, Multiple indicators of affective responses to mu-\nsic. (Dissertation Abstracts International 47(12) , 1986\n[31] G. Tzanetakis et al., “Automatic musical genre classiﬁcation\nof audio signals,” in Proceedings of International Society for\nMusic Information Retrieval (ISMIR) , 2001\n[32] Y . Kim, E. M. Schmidt, et al., “Moodswings: A collaborative\ngame for music mood label collection,” in Proc. of the In-\nternational Society for Music Information Retrieval (ISMIR)\nConference , 2008.\n[33] Y .H. Yang, X. Hu, “Cross-cultural music mood classiﬁcation:\na comparison on English and Chinese songs” in Proceedings\nof International Society for Music Information Retrieval (IS-\nMIR) , pp. 15-24, 2012\n[34] Y .H. Yang, Y .C. Lin, et al. “A regression approach to mu-\nsic emotion recognition.” IEEE Trans. on Audio, Speech, and\nLanguage . 16(2), 448–457, 2008"
    },
    {
        "title": "Rhythmic Pattern Modeling for Beat and Downbeat Tracking in Musical Audio.",
        "author": [
            "Florian Krebs",
            "Sebastian Böck",
            "Gerhard Widmer"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416392",
        "url": "https://doi.org/10.5281/zenodo.1416392",
        "ee": "https://zenodo.org/records/1416392/files/KrebsBW13.pdf",
        "abstract": "Rhythmic patterns are an important structural element in music. This paper investigates the use of rhythmic pattern modeling to infer metrical structure in musical audio recordings. We present a Hidden Markov Model (HMM) based system that simultaneously extracts beats, downbeats, tempo, meter, and rhythmic patterns. Our model builds upon the basic structure proposed by Whiteley et. al [20], which we further modified by introducing a new observation model: rhythmic patterns are learned directly from data, which makes the model adaptable to the rhythmical structure of any kind of music. For learning rhythmic patterns and evaluating beat and downbeat tracking, 697 ballroom dance pieces were annotated with beat and measure information. The results showed that explicitly modeling rhythmic patterns of dance styles drastically reduces octave errors (detection of half or double tempo) and substantially improves downbeat tracking.",
        "zenodo_id": 1416392,
        "dblp_key": "conf/ismir/KrebsBW13",
        "keywords": [
            "Rhythmic patterns",
            "music",
            "Hidden Markov Model (HMM)",
            "metrical structure",
            "beats",
            "downbeats",
            "tempo",
            "meter",
            "rhythmic patterns",
            "dance styles"
        ],
        "content": "RHYTHMIC PATTERN MODELING FOR BEAT AND DOWNBEAT\nTRACKING IN MUSICAL AUDIO\nFlorian Krebs, Sebastian B ¨ock, and Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University, Linz, Austria\nflorian.krebs@jku.at\nABSTRACT\nRhythmic patterns are an important structural element\nin music. This paper investigates the use of rhythmic pat-\ntern modeling to infer metrical structure in musical audio\nrecordings. We present a Hidden Markov Model (HMM)\nbased system that simultaneously extracts beats, downbeats,\ntempo, meter, and rhythmic patterns. Our model builds\nupon the basic structure proposed by Whiteley et. al [20],\nwhich we further modiﬁed by introducing a new observa-\ntion model: rhythmic patterns are learned directly from\ndata, which makes the model adaptable to the rhythmical\nstructure of any kind of music. For learning rhythmic pat-\nterns and evaluating beat and downbeat tracking, 697 ball-\nroom dance pieces were annotated with beat and measure\ninformation. The results showed that explicitly modeling\nrhythmic patterns of dance styles drastically reduces oc-\ntave errors (detection of half or double tempo) and sub-\nstantially improves downbeat tracking.\n1. INTRODUCTION\nFrom its very beginnings, music has been built on tempo-\nral structure to which humans can synchronize via musi-\ncal instruments and dance. The most prominent layer of\nthis temporal structure (which most people tap their feet\nto) contains the approximately equally spaced beats . These\nbeats can, in turn, be grouped into measures , segments with\na constant number of beats; the ﬁrst beat in each measure,\nwhich usually carries the strongest accent within the mea-\nsure, is called the downbeat . The automatic analysis of this\ntemporal structure in a music piece has been an active re-\nsearch ﬁeld since the 1970s and is of prime importance for\nmany applications such as music transcription, automatic\naccompaniment, expressive performance analysis, music\nsimilarity estimation, and music segmentation. However,\nmany problems within the automatic analysis of metrical\nstructure remain unsolved. In particular, complex rhythmic\nphenomena such as syncopations, triplets, and swing make\nit difﬁcult to ﬁnd the correct phase and period of downbeats\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.and beats, especially for systems that rely on the assump-\ntion that beats usually occur at onset times. Considering\nall these rhythmic peculiarities, a general model no longer\nsufﬁces.\nOne way to overcome this problem is to incorporate\nhigher-level musical knowledge into the system. For ex-\nample, Hockman et al. [12] proposed a genre-speciﬁc beat\ntracking system designed speciﬁcally for the genres hard-\ncore, jungle, and drum and bass. Another way to make\nthe model more speciﬁc is to model explicitly one or sev-\neralrhythmic patterns . These rhythmic patterns describe\nthe distribution of note onsets within a predeﬁned time in-\nterval, e.g., one bar. For example, Goto [9] extracts bar-\nlength drum patterns from audio signals and matches them\nto eight pre-stored patterns typically used in popular mu-\nsic. Klapuri et al. [14] proposed a HMM representing a\nthree-level metrical grid consisting of tatum, tactus, and\nmeasure. Two rhythmic patterns were employed to ob-\ntain an observation probability for the phase of the measure\npulse. The system of Whiteley et al. [20] jointly models\ntempo, meter, and rhythmic patterns in a Bayesian frame-\nwork. Simple observation models were proposed for sym-\nbolic and audio data, but were not evaluated on polyphonic\naudio signals.\nAlthough rhythmic patterns are used in some systems,\nno systematic study exists that investigates the importance\nof rhythmic patterns for analyzing the metrical structure.\nApart from the approach presented in [17], which learns a\nsingle rhythmic template from data, rhythmic patterns to\nbe used for beat tracking have so far only been designed\nby hand and hence depend heavily on the intuition of the\ndeveloper.\nThis paper investigates the role of rhythmic patterns in\nanalyzing the metrical structure in musical audio signals.\nWe propose a new observation model for the HMM-based\nsystem described in [20], whose parameters are learned\nfrom real audio data and can therefore be adapted easily\nto represent any rhythmic style.\n2. RHYTHMIC PATTERNS\nAlthough rhythmic patterns could be deﬁned at any level of\nthe metrical structure, we restrict the deﬁnition of rhythmic\npatterns to the length of a single measure.2.1 Data\nAs stated in Section 1, strong deviations from a straight\non-beat rhythm constitute potential problems for automatic\nrhythmic description systems. While pop and rock music is\ncommonly concentrated on the beat, Afro-Cuban rhythms\nfrequently contain syncopations, for instance in the clave\npattern – the structural core of many Afro-Cuban rhythms.\nTherefore, Latin music represents a serious challenge to\nbeat and downbeat tracking systems.\nThe ballroom dataset1contains eight different dance\nstyles (Cha cha, Jive, Quickstep, Rumba, Samba, Tango,\nViennese Waltz, and (slow) Waltz) and has been used by\nseveral authors, for example, for genre recognition [6, 18].\nIt consists of 697230 seconds-long audio excerpts (sam-\npled at 11.025 kHz) and has tempo and dance style anno-\ntations. The dataset contains two different meters (3/4 and\n4/4) and all pieces have constant meter. The tempo distri-\nbutions of the dance styles are displayed in Fig. 4.\nWe have annotated both beat and downbeat times man-\nually. In cases of disagreement on the metrical level we re-\nlied on the existing tempo and meter annotations. The an-\nnotations can be downloaded from https://github.com/\nCPJKU/BallroomAnnotations .\n2.2 Representation of rhythmic patterns\nPatterns such as those shown in Fig. 1 are learned in the\nprocess of inducing the likelihood function for the model\n(cf. Section 3.3.3), where we use the dance style labels of\nthe training songs as indicators of different rhythmic pat-\nterns. To model dependencies between instruments in our\npattern representations, we split the audio signal into two\nfrequency bands and compute an onset feature for each of\nthe bands individually as described in Section 3.3. To illus-\ntrate the rhythmic characteristics of different dance styles,\nwe show the eight learned representations of rhythmic pat-\nterns in Fig. 1. Each pattern is represented by a distribution\nof onset feature values along a bar in two frequency bands.\nFor example, the Jive pattern displays strong accents\non the second and fourth beat, a phenomenon usually re-\nferred to as backbeat . In addition, the typical swing style\nis clearly visible in the high-frequency band. The Rumba\npattern contains a strong accent of the bass on the 4th and\n7th eighth note, which is a common bass pattern in Afro-\nCuban music and referred to as anticipated bass [15]. One\nof the characteristics of Samba is the shufﬂed bass line, a\npattern originally played with the Surdo , a large Brazilian\nbass drum. The pattern features bass notes on the 1st, 4th,\n5th, 9th, 12th, and 13th sixteenth note of the bar. Waltz ,\nﬁnally, is a triple meter rhythm. While the bass notes are\nlocated mainly on the downbeat, high-frequency note on-\nsets are also located at the quarter and eighth note level of\nthe measure.\n1The data was extracted from www.ballroomdancers.com .\n2One of the 698 original ﬁles was found duplicated and was removed.\nMean onset feature\n10 20 30 40 50 6000.20.40.60.8Cha cha\n10 20 30 40 50 600.511.522.5\n10 20 30 40 50 6000.20.40.60.8Jive\n10 20 30 40 50 600.511.522.5\n10 20 30 40 50 6000.20.40.6Quickstep\n10 20 30 40 50 60123\n10 20 30 40 50 6000.51Rumba\n10 20 30 40 50 60123\n10 20 30 40 50 6000.20.40.6Samba\n10 20 30 40 50 60123\n10 20 30 40 50 6000.51Tango\n10 20 30 40 50 600.511.522.5\n5 10 15 20 25 30 35 40 4500.51Viennese Waltz\n5 10 15 20 25 30 35 40 451234\n5 10 15 20 25 30 35 40 450.20.40.60.811.21.4Waltz\n5 10 15 20 25 30 35 40 45123\nPosition inside a bar (16th grid)\nFigure 1 . Illustration of learned rhythmic patterns. Two\nfrequency bands are shown (Low/High from bottom to\ntop).\n3. METHOD\nIn this section, we describe the dynamic Bayesian network\n(DBN) [16] we use to analyze the metrical structure. We\nassume that a time series of observed datay1:K=fy1;:::;\nyKgis generated by a set of unknown, hidden variables\nx1:K=fx1;:::;xKg, whereKis the length of an au-\ndio excerpt in frames. In a DBN, the joint distribution\nP(y1:K;x1:K)factorizes as\nP(y1:K;x1:K) =P(x1)KY\nk=2P(xkjxk\u00001)P(ykjxk)(1)\nwhereP(x1)is the initial state distribution ,P(xkjxk\u00001)\nis the transition model , andP(ykjxk)is the observation\nmodel .\nThe proposed model is similar to the model proposed\nby Whiteley et. al [20] with the following modiﬁcations:\n\u000fWe assume conditional dependence between the tempo\nand the rhythmic pattern (cf., Section 3.2), which is a\nvalid assumption for ballroom music as shown in Fig. 4.\n\u000fAs the original observation model was mainly intended\nfor percussive sounds, we replace it by a Gaussian Mix-\nture Model (GMM) as described in Section 3.3.\n3.1 Hidden variables\nThe dynamic bar pointer model [20] deﬁnes the state of\na hypothetical bar pointer at time tk=k\u0001\u0001, withk2\nf1;2;:::;Kgand\u0001the audio frame length, by the follow-\ning discrete hidden variables:\n1. Position inside a bar mk2f1;2;:::;Mg, where\nmk= 1 indicates the beginning and mk=Mthe\nend of a bar;mk mk\u00001nk\u00001 nk\nrk rk\u00001\nyk yk\u00001\nFigure 2 . Dynamic Bayesian network; circles denote con-\ntinuous variables and rectangles discrete variables. The\ngray nodes are observed, and the white nodes represent the\nhidden variables.\n2. Temponk2f1;2;:::;Ng(unitbar positions\naudio frame), where\nNdenotes the number of tempo states;\n3. Rhythmic pattern rk2fr1;r2;:::;r Rg, whereRde-\nnotes the number of rhythmic patterns.\nFor the experiments reported in this paper, we chose \u0001 =\n20ms,M= 1216 ,N= 26 , andR(the number of rhyth-\nmic patterns) was 2 or 8 as described in Section 4.2. Fur-\nthermore, each rhythmic pattern is assigned to a meter \u0012(rk)\n2f3=4;4=4g, which is important to determine the mea-\nsure boundaries in Eq. 4. The conditional independence\nrelations between these variables are shown in Fig. 2.\nAs noted in [16], any discrete state DBN can be con-\nverted into a regular HMM by merging all hidden vari-\nables of one time slice into a ‘meta-variable’ xk, whose\nstate space is the Cartesian product of the single variables:\nxk= [mk;nk;rk]: (2)\n3.2 Transition model\nDue to the conditional independence relations shown in\nFig. 2, the transition model factorizes as\nP(xkjxk\u00001) =P(mkjmk\u00001;nk\u00001;rk\u00001)\u0002\n\u0002P(nkjnk\u00001;rk\u00001)\u0002P(rkjrk\u00001)\n(3)\nwhere the three factors are deﬁned as follows:\n\u000fP(mkjmk\u00001;nk\u00001;rk\u00001)\nAt time frame kthe bar pointer moves from position\nmk\u00001tomkas deﬁned by\nmk= [(mk\u00001+nk\u00001\u00001)mod(Nm\u0001\u0012(rk\u00001))]+1:(4)\nWhenever the bar pointer crosses a bar border it is reset\nto 1 (as modeled by the modulo operator).\n\u000fP(nkjnk\u00001;rk\u00001)\nIf the tempo nk\u00001is inside the allowed tempo rangefnmin(rk\u00001);:::;n max(rk\u00001)g, there are three possible\ntransitions: the bar pointer remains at the same tempo,\naccelerates, or decelerates:\nifnmin(rk\u00001)\u0014nk\u00001\u0015nmax(rk\u00001),\nP(nkjnk\u00001) =8\n<\n:1\u0000pn; n k=nk\u00001;\npn\n2; n k=nk\u00001+ 1;\npn\n2; n k=nk\u00001\u00001:(5)\nTransitions to tempi outside the allowed range are as-\nsigned a zero probability. pnis the probability of a\nchange in tempo per audio frame, and the step-size of\na tempo change per audio frame was set to one bar posi-\ntion per audio frame.\n\u000fP(rkjrk\u00001)\nFor this work, we assume a musical piece to have a char-\nacteristic rhythmic pattern that remains constant through-\nout the song; thus we obtain\nrk+1=rk: (6)\n3.3 Observation model\nFor simplicity, we omit the frame indices kin this section.\nThe observation model P(yjx)reduces toP(yjm;r)due\nto the independence assumptions shown in Fig. 2.\n3.3.1 Observation features\nSince the perception of beats depends heavily on the per-\nception of played musical notes, we believe that a good\nonset feature is also a good beat tracking feature. There-\nfore, we use a variant of the LogFiltSpecFlux onset fea-\nture, which performed well in recent comparisons of on-\nset detection functions [1] and is summarized in the top\npart of Fig. 3. We believe that the bass instruments play\nan important role in deﬁning rhythmic patterns, hence we\ncompute onsets in low-frequencies ( <250Hz) and high-\nfrequencies ( >250Hz) separately. In Section 5.1 we in-\nvestigate the importance of using the two-dimensional on-\nset feature over a one-dimensional one. Finally, we sub-\ntract the moving average computed over a window of one\nsecond and normalize the features of each excerpt to zero\nmean and unity variance.\nz(t) STFTﬁlterbank\n(81 bands)log diff\nsum over fre-\nquency bandssubtract\nmvavgnormalize y[k]\nFigure 3 . Computing the onset feature y[k] from the audio\nsignalz(t)\n3.3.2 State tying\nWe assume the observation probabilities to be constant with-\nin a 16th note grid. All states within this grid are tied and\nthus share the same parameters, which yields 64 (4/4 me-\nter) and 48 (3/4 meter) different observation probabilities\nper bar and rhythmic pattern.60 80 100 120 140 160 180 200 220 2400.010.020.030.040.050.060.070.08\ntempo [bpm]likelihood\n  \nChaCha\nJive\nQuickstep\nRumba\nSamba\nTango\nVienneseWaltz\nWaltzFigure 4 . Tempo distributions of the ballroom dataset\ndance styles. The displayed distributions are obtained by\n(Gaussian) kernel density estimation for each dance style\nseparately.\n3.3.3 Likelihood function\nTo learn a representation of P(yjm;r), we split the train-\ning dataset into pieces of one bar length, starting at the\ndownbeat. For each bar position within the 16th grid and\neach rhythmic pattern, we collect all corresponding feature\nvalues and ﬁt a GMM. We achieved the best results on our\ntest set with a GMM of I= 2 components. Hence, the\nobservation probability is modeled by\nP(yjm;r) =IX\ni=1wm;r;i\u0001N(y;\u0016m;r;i;\u0006m;r;i);(7)\nwhere\u0016m;r;i is the mean vector, \u0006m;r;i is the covariance\nmatrix, and wm;r;i is the mixture weight of component i\nof the GMM. Since, in learning the likelihood function\nP(yjm;r), a GMM is ﬁtted to the audio features for ev-\nery rhythmic pattern (i.e., dance style) label r, the result-\ning GMMs can be interpreted directly as representations\nof rhythmic patterns. Fig. 1 shows the mean values of the\nfeatures per frequency band and bar position for the GMMs\ncorresponding to the eight rhythmic patterns r2fCha cha,\nJive, Quickstep, Rumba, Samba, Tango, Viennese Waltz,\nWaltzg.\n3.4 Initial state distribution\nThe bar position and the rhythmic patterns are assumed to\nbe distributed uniformly, whereas the tempo state proba-\nbilities are modeled by ﬁtting a GMM3to the tempo dis-\ntribution of each ballroom style shown in Fig. 4.\n3.5 Inference\nWe are looking for the state sequence x\u0003\n1:Kwith the highest\nposterior probability p(x1:Kjy1:K):\nx\u0003\n1:K= arg max\nx1:Kp(x1:Kjy1:K): (8)\nWe solve Eq. 8 using the Viterbi algorithm [19]. Once\nx\u0003\n1:Kis computed, the set of beat and downbeat times are\nobtained by interpolating m\u0003\n1:Kat the corresponding bar\npositions.\n3The number of components was set to two (PS2), and four (PS8)4. EXPERIMENTAL SETUP\nWe use different settings and reference methods to evaluate\nthe relevance of rhythmic pattern modeling for the beat and\ndownbeat tracking performance.\n4.1 Evaluation measures\nA variety of measures for evaluating beat tracking perfor-\nmance is available (see [3] for an overview). We chose\nto report continuity-based measures for beat and downbeat\ntracking as in [4, 5, 14]:\n\u000fCMLc (Correct Metrical Level with continuity required)\nassesses the longest segment of correct beats at the cor-\nrect metrical level.\n\u000fCMLt (Correct Metrical Level with no continuity re-\nquired) assesses the total number of correct beats at the\ncorrect metrical level.\n\u000fAMLc (Allowed Metrical Level with continuity required)\nassesses the longest segment of correct beats, consider-\ning several metrical levels and offbeats.\n\u000fAMLt (Allowed Metrical Level with no continuity re-\nquired) assesses the total number of correct beats, con-\nsidering several metrical levels and offbeats.\nDue to lack of space, we present only the mean values per\nmeasure across all ﬁles of the dataset. Please visit http://\nwww.cp.jku.at/people/krebs/ISMIR2013.html for de-\ntailed results and other metrics.\n4.2 Systems compared\nTo evaluate the use of modeling multiple rhythmic pat-\nterns, we report results for the following variants of the\nproposed system (PS): PS2 uses two rhythmic patterns (one\nfor each meter), PS8 uses eight rhythmic patterns (one for\neach genre), PS8.genre has the ground truth genre, and\nPS2.meter has the ground truth meter as additional input\nfeatures.\nIn order to compare the system to the state-of-the-art,\nwe add results of six reference beat tracking algorithms:\nEllis [7], Davies [4], Degara [5], B ¨ock [2], Ircambeat [17],\nand Klapuri [14]. The latter two also compute downbeat\ntimes.\n4.3 Parameter training\nFor all variants of the proposed system PS x, the results\nwere computed by a leave-one-out approach, where we\ntrained the model on all songs except the one to be tested.\nThe B ¨ock system has been trained on the data speciﬁed\nin [2], the SMC [13], and the Hainsworth dataset [10].\nThe beat templates used by Ircambeat in [17] have been\ntrained using their own annotated PopRock dataset. The\nother methods do not require any training.\n4.4 Statistical tests\nIn Section 5.1 we use an analysis of variance test (ANOV A)\nand in Section 5.2 a multiple comparison test [11] to ﬁndSystem CMLc CMLt AMLc AMLt\nPS2.1d 62.2 65.8 87.6 93.1\nPS2.2d 66.7 70.1 88.5 93.2\nPS8.1d 76.6 79.7 87.7 92.1\nPS8.2d 79.5 83.0 87.6 91.6\nPS2 66.7 70.1 88.5 93.2\nPS8 79.5 83.0 87.6 91.6\nEllis [7] 26.7 30.9 65.2 80.2\nDavies [4] 57.9 59.2 87.9 89.8\nDegara [5] 64.6 66.9 85.3 89.5\nIrcambeat [17] 58.1 60.3 86.1 89.6\nB¨ock [2] 65.7 67.7 92.0 94.4\nKlapuri [14] 55.2 57.0 84.9 87.3\nPS2.meter 68.0 71.7 88.7 93.7\nPS8.genre 89.9 93.7 90.9 94.8\nTable 1 . Beat tracking performance on the ballroom\ndataset. Results printed in bold are statistically equivalent\nto the best result.\nstatistically signiﬁcant differences among the mean perfor-\nmances of the different systems. A signiﬁcance level of\n0.05 was used to declare performance differences as statis-\ntically relevant.\n5. RESULTS AND DISCUSSION\n5.1 Dimensionality of the observation feature\nAs described in Section 3.3.1, the onset feature is com-\nputed for one (PS x.1d) or two (PS x.2d) frequency bands\nseparately. The top parts of Table 1 and Table 2 show the\neffect of the dimensionality of the feature vector on the\nbeat and downbeat tracking results respectively.\nFor beat tracking, analyzing the onset function in two\nseparate frequency bands seems to help ﬁnding the correct\nmetrical level, as indicated by higher CML measures in\nTable 1. Even though the improvement is not signiﬁcant,\nthis effect was observed for both PS2 and PS8.\nFor downbeat tracking, we have found a signiﬁcant im-\nprovement for all measures if two bands are used instead\nof a single one, as evident from Table 2. This seems plau-\nsible, as the bass plays a major role in deﬁning a rhythmic\npattern (see Section 2.2) and helps to resolve the ambiguity\nbetween the different beat positions within a bar.\nUsing three or more onset frequency bands did not im-\nprove the performance further in our experiments. In the\nfollowing sections we will only report the results for the\ntwo-dimensional onset feature (PS x.2d) and simply denote\nit as PS x.\n5.2 Relevance of rhythmic pattern modeling\nIn this section, we evaluate the relevance of rhythmic pat-\ntern modeling by comparing the beat and downbeat track-\ning performance of the proposed systems to six reference\nsystems.System CMLc CMLt AMLc AMLt\nPS2.1d 46.9 47.1 70.5 71.1\nPS2.2d 55.5 55.7 76.2 76.5\nPS8.1d 65.4 65.8 80.9 81.8\nPS8.2d 71.1 71.5 85.3 85.9\nPS2 55.5 55.7 76.2 76.5\nPS8 71.1 71.5 85.3 85.9\nIrcambeat [17] 36.5 37.4 57.4 59.4\nKlapuri [14] 39.6 40.1 68.1 68.9\nPS2.meter 62.1 62.4 84.2 84.6\nPS8.genre 82.8 83.1 92.6 92.9\nTable 2 . Downbeat tracking performance on the ballroom\ndataset. Results printed in bold are statistically equivalent\nto the best result.\n5.2.1 Beat tracking\nThe beat tracking results of the reference methods are dis-\nplayed together with PS2 (=PS2.2d) and PS8 (=PS8.2d) in\nthe middle part of Table 1. Although there is no single sys-\ntem that performs best in all of the measures, we can still\ndetermine a best system for the CML measures and one for\nthe AML measures separately.\nFor the CML measures (which require the correct met-\nrical level), PS8 clearly outperforms all other systems. If\nthe correct dance style is supplied as in PS8.genre, the per-\nformance increases even more. Apparently, the dance style\nprovides sufﬁcient rhythmic information to resolve tempo\nambiguities.\nFor the AML measures (which do not require the cor-\nrect metrical level), we found no advantage of using the\nproposed methods over most of the reference methods. The\nsystem proposed by B ¨ock, which has been trained on Pop/\nRock music, outperforms all other systems, even though\nthe difference to PS2 (for AMLc and AMLt) and PS8 (for\nAMLt) is not signiﬁcant.\nHence, if the correct metrical level is unimportant or\neven ambiguous, a general model like B ¨ock or any other\nreference system might be preferable to the more complex\nPS8. On the contrary, in applications where the correct\nmetrical level matters (e.g., a system that detects beats and\ndownbeats for automatic ballroom dance instructions [8]),\nPS8 is the best system to chose.\nKnowing the meter a priori (PS2.meter) was not found\nto increase the performance signiﬁcantly compared to PS2.\nIt appeared that meter was identiﬁed mostly correct by PS2\n(in 89% of the songs) and that for the remaining 11% songs\nboth of the rhythmic patterns ﬁtted equally well.\n5.2.2 Downbeat tracking\nTable 2 lists the results for downbeat tracking. As shown,\nPS8 outperforms all other systems signiﬁcantly in all met-\nrics. In cases where the dance style is known a priori\n(PS8.genre), the downbeat performance increases even more.\nThe same was observed for PS2 if the meter was known\n(PS2.meter). This leads to the assumption that downbeattracking (as well as beat tracking with PS8) would improve\neven more by including meter or genre detection methods.\nFor instance, Pohle et al. [18] report a dance style clas-\nsiﬁcation rate of 89% on the same dataset, whereas PS8\ndetected the correct dance style in only 75% of the cases.\nThe poor performance of Ircambeat and Klapuri’s sys-\ntem is probably caused by the fact that both systems were\ndeveloped for music comprising a completely different met-\nrical structure than present in ballroom data. In addition,\nKlapuri’s system explicitly assumes 4/4 meter (only true\nfor 522 songs) and relies on the high-frequency content of\nthe signal (that is drastically reduced using a sampling rate\nof 11.025 kHz) to determine the measure boundaries.\n6. CONCLUSION AND FUTURE WORK\nIn this study, we investigated the inﬂuence of explicit mod-\neling of rhythmic patterns on the beat and downbeat track-\ning performance in musical audio signals. For this purpose\nwe have proposed a new observation model for the system\nproposed in [20] representing rhythmical patterns in two\nfrequency bands.\nOur experiments indicated that computing an onset fea-\nture for at least two different frequency bands increases the\ndownbeat tracking performance signiﬁcantly compared to\na single feature covering the whole frequency range.\nIn a comparison with six reference systems, explicitly\nmodeling dance styles as rhythmic patterns was shown to\nreduce octave errors (detecting half or double tempo) in\nbeat tracking. Besides, downbeat tracking was improved\nsubstantially compared to a variant that only models meter\nand two reference systems.\nObviously, ballroom music is well structured in terms of\nrhythmic patterns and tempo distribution. If all the ﬁndings\nreported in this paper also apply to music genres other than\nballroom music has yet to be investigated.\nIn this work, the rhythmic patterns were determined by\ndance style labels. In future work, we want to use unsuper-\nvised clustering methods to extract meaningful rhythmic\npatterns from the audio features directly.\n7. ACKNOWLEDGMENTS\nWe are thankful to Simon Dixon for providing access to\nthe ﬁrst bar annotations of the ballroom dataset and to Nor-\nberto Degara and the reviewers for inspiring inputs. This\nwork was supported by the Austrian Science Fund (FWF)\nproject Z159 and the European Union Seventh Framework\nProgramme FP7 / 2007-2013 through the PHENICX pro-\nject (grant agreement no. 601166).\n8. REFERENCES\n[1] S. B ¨ock, F. Krebs, and M. Schedl. Evaluating the online capa-\nbilities of onset detection methods. In Proceedings of the 14th\nInternational Conference on Music Information Retrieval (IS-\nMIR) , Porto, 2012.\n[2] S. B ¨ock and M. Schedl. Enhanced beat tracking with context-\naware neural networks. In Proceedings of the International\nConference on Digital Audio Effects (DAFx) , 2011.[3] M. Davies, N. Degara, and M.D. Plumbley. Evaluation meth-\nods for musical audio beat tracking algorithms. Queen Mary\nUniversity of London, Tech. Rep. C4DM-09-06 , 2009.\n[4] M. Davies and M. Plumbley. Context-dependent beat tracking\nof musical audio. IEEE Transactions on Audio, Speech and\nLanguage Processing , 15(3):1009–1020, 2007.\n[5] N. Degara, E. Argones Rua, A. Pena, S. Torres-Guijarro,\nM. Davies, and M. Plumbley. Reliability-informed beat track-\ning of musical signals. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on , (99):1–1, 2011.\n[6] S. Dixon, F. Gouyon, and G. Widmer. Towards characteri-\nsation of music via rhythmic patterns. In Proceedings of the\n5th International Conference on Music Information Retrieval\n(ISMIR) , Barcelona, 2004.\n[7] D. Ellis. Beat tracking by dynamic programming. Journal of\nNew Music Research , 36(1):51–60, 2007.\n[8] F. Eyben, B. Schuller, S. Reiter, and G. Rigoll. Wearable\nassistance for the ballroom-dance hobbyist-holistic rhythm\nanalysis and dance-style classiﬁcation. In Proceedings of the\n8th IEEE International Conference on Multimedia and Expo\n(ICME) , Beijing, 2007.\n[9] M. Goto. An audio-based real-time beat tracking system for\nmusic with or without drum-sounds. Journal of New Music\nResearch , 30(2):159–171, 2001.\n[10] S. Hainsworth and M. Macleod. Particle ﬁltering applied to\nmusical tempo tracking. EURASIP Journal on Applied Signal\nProcessing , 2004:2385–2395, 2004.\n[11] Y . Hochberg and A. Tamhane. Multiple comparison proce-\ndures . John Wiley & Sons, Inc., 1987.\n[12] J. Hockman, M. Davies, and I. Fujinaga. One in the jungle:\nDownbeat detection in hardcore, jungle, and drum and bass.\nInProceedings of the 13th International Society for Music\nInformation Retrieval (ISMIR) , Porto, 2012.\n[13] A. Holzapfel, M. Davies, J. Zapata, J. Oliveira, and\nF. Gouyon. Selective sampling for beat tracking evaluation.\nIEEE Transactions on Audio, Speech, and Language Process-\ning, 20(9):2539–2548, 2012.\n[14] A. Klapuri, A. Eronen, and J. Astola. Analysis of the me-\nter of acoustic musical signals. IEEE Transactions on Audio,\nSpeech, and Language Processing , 14(1):342–355, 2006.\n[15] P. Manuel. The anticipated bass in cuban popular music. Latin\nAmerican music review , 6(2):249–261, 1985.\n[16] K. Murphy. Dynamic bayesian networks: representation, in-\nference and learning . PhD thesis, University of California,\nBerkeley, 2002.\n[17] G. Peeters and H. Papadopoulos. Simultaneous beat and\ndownbeat-tracking using a probabilistic framework: theory\nand large-scale evaluation. IEEE Transactions on Audio,\nSpeech, and Language Processing , (99):1–1, 2011.\n[18] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and G. Wid-\nmer. On rhythm and general music similarity. In Proceedings\nof the 10th International Society for Music Information Re-\ntrieval (ISMIR) , Kobe, 2009.\n[19] L.R. Rabiner. A tutorial on hidden markov models and se-\nlected applications in speech recognition. Proceedings of the\nIEEE , 77(2):257–286, 1989.\n[20] N. Whiteley, A. Cemgil, and S. Godsill. Bayesian modelling\nof temporal structure in musical audio. In Proceedings of the\n7th International Conference on Music Information Retrieval\n(ISMIR) , Victoria, 2006."
    },
    {
        "title": "Visual Humdrum-Library for PWGL.",
        "author": [
            "Mika Kuuskankare",
            "Craig Sapp"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418057",
        "url": "https://doi.org/10.5281/zenodo.1418057",
        "ee": "https://zenodo.org/records/1418057/files/KuuskankareS13.pdf",
        "abstract": "We introduce a PWGL Humdrum interface that integrates command-line unix tools for music analysis into a visual programming environment. This symbiosis allows users access to the strengths of each system— algorithmic composition and visual programming components of PWGL along with computational analysis and data processing features of Humdrum tools. Our novel interface for Humdrum graphical programming allows nonprogrammers better access to Humdrum analysis tools, particularly with the built-in music notation display capabilities of PWGL. ENP (Expressive Notation Package) data from PWGL can be exported as Humdrum data. Humdrum files in turn can be converted back into ENP data, allowing bi-directional communication between the two software systems.",
        "zenodo_id": 1418057,
        "dblp_key": "conf/ismir/KuuskankareS13",
        "keywords": [
            "PWGL Humdrum interface",
            "command-line unix tools",
            "visual programming environment",
            "algorithmic composition",
            "visual programming components",
            "computational analysis",
            "data processing features",
            "nonprogrammers",
            "built-in music notation display",
            "ENP data"
        ],
        "content": "VISUAL HUMDRUM-LIBRARY FOR PWGL\nMika Kuuskankare\nDocMus\nSibelius Academy\nmkuuskan@siba.fiCraig Stuart Sapp\nCCARH\nStanford University\ncraig@ccrma.stanford.edu\nABSTRACT\nWe introduce a PWGL Humdrum interface that inte-\ngrates command-line unix tools for music analysis into\na visual programming environment. This symbiosis al-\nlows users access to the strengths of each system—\nalgorithmic composition and visual programming compo-\nnents of PWGL along with computational analysis and\ndata processing features of Humdrum tools. Our novel in-\nterface for Humdrum graphical programming allows non-\nprogrammers better access to Humdrum analysis tools,\nparticularly with the built-in music notation display ca-\npabilities of PWGL. ENP (Expressive Notation Package)\ndata from PWGL can be exported as Humdrum data. Hum-\ndrum ﬁles in turn can be converted back into ENP data,\nallowing bi-directional communication between the two\nsoftware systems.\n1. INTRODUCTION\nPWGL [10] is a visual music programming language writ-\nten in Common Lisp, CLOS (Common Lisp Object Sys-\ntem) and OpenGL. Its primary focus is on computer-\nassisted composition in an integrated environment with\nmusic notation, software synthesis and music theory and\nanalysis tools. PWGL comprises several large-scale appli-\ncations, such as ENP [5] for music notation, PWGLCon-\nstraints [7] for rule-based composition, and Kronos [11]\nfor sound synthesis.\nThe Humdrum Toolkit is a widely used open-source\nsoftware package for musicological research conceived of\nby David Huron [1] in the 1990’s and has been devel-\noped and extended by others. It is composed of two main\ncomponents: the Humdrum data format and the Humdrum\nToolkit that processes music in this format. The Humdrum\nﬁle format is a generalized data array. Each column repre-\nsents a stream of data (such as a part in a musical score),\nwhile each row represents simultaneous events across mul-\ntiple data streams. A particular advantage of this format is\nthat it allows inclusion of analytic data streams alongside\nthe original musical score within the same ﬁle. The pri-\nmary music-content subformat in Humdrum is called kern,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.with about 50 other subformats predeﬁned in the standard\ntoolkit for encoding musical features such as lyrics, dy-\nnamics, tuning, harmonic analysis and perceptual data. the\nHumdrum ﬁle structure also allows users to deﬁne their\nown subformats for speciﬁc analytic or markup purposes.\nThe Humdrum Toolkit is a collection of command-line\nutilities operating on data written in the Humdrum syn-\ntax. In addition to the standard Humdrum utilities that\nare written in AWK, a software package called Humdrum\nExtras developed at CCARH at Stanford University ex-\ntends and compliments the original toolset with additional\ncommand-line tools and a C++ code library for manipu-\nlating data in the Humdrum syntax with a focus on efﬁ-\ncient numeric processing of the data.1Humdrum data pro-\ncessing can also done within PERL [2], and kern data can\nbe imported into the music21 system implemented within\nPython.2\nPWGL can be used to analyze scores written in the\nENP format by using a built-in rule-based scripting lan-\nguage called ENP-script [4]. ENP offers both an exten-\nsive and extensible set of visualization devices that can\nbe used to visualize analytic observations directly in the\nscore [6]. It has already been used for music analysis, as\nreported for example in [9] and [8]; however, the current\napproach is not well suited for statistical analyses. Further-\nmore, a scant amount of musical data is readily available\nin the ENP format. The PWGL Humdrum interface now\nenables access to a vast number of pieces encoded in the\nkern data format that is made freely available for download\nfrom http://kern.ccarh.org [12]. In addition to di-\nrect translation of kern data into ENP, the interface allows\naccess to a large number of predeﬁned analytical tools, and\nallows for the batch analysis of a large musical corpus at\nonce.\nOur library attempts to make it easier to work within\nthe Humdrum environment. First, it makes it possible\nfor non-programmers to use and access Humdrum analy-\nsis tools through a visual interface. Many theorists, musi-\ncians, and composers may be intimidated by Humdrum’s\nunix-ﬂavored command-line syntax thus limiting the num-\nber of potential users. Second, for musicians it is impor-\ntant to be able to see the piece of music they are working\non in common music notation. ENP allows an instant, ed-\nitable and extendable representation of thousands of pieces\nof music encoded in kern notation. Finally, the analytic in-\n1http://extras.humdrum.org and http://github.com/craigsapp/humextra\n2http://mit.edu/music21formation obtained via Humdrum tools can be visualized\ndirectly within an ENP score which assists interpretation\nof the results.\nPrevious attempts to create visual interfaces around\nHumdrum tools, such as JRing [3] have been made; how-\never, the approach provided by PWGL is more general and\nuser-extendable since PWGL is a generalized program-\nming environment. The combination of Humdrum and\nPWGL should prove especially beneﬁcial for musicolo-\ngists, but should be of interest to MIR researchers and the\nlike as well.\n2. HUMDRUM LIBRARY\nThe PWGL Humdrum library is implemented in two parts:\nkern import is implemented as a Humdrum command-line\ntool called hum2enp , which is written in C++. The kern\nexport and visual patch interface are implemented on the\nPWGL side and are written in Lisp and CLOS. The con-\nnection between the visual PWGL boxes and the Hum-\ndrum Tools is realized with the help of the PWGL Shell\nlibrary, which provides for a graphical interface between\nPWGL and the unix command-line. Practically every unix\ncommand can be turned into a PWGL box and seamlessly\nused as a part of a patch along with the built-in boxes.\nWhen the Humdrum library is loaded, PWGL scans a spe-\nciﬁc directory deﬁned by the user using the Humdrum li-\nbrary preferences for available Humdrum commands and\ncreates the box interface automatically.\n2.1 Kern import/export\nCommunication between PWGL and Humdrum tools is\nfacilitated by two utilities that convert between ENP and\nHumdrum data. PWGL has a set of built-in Lisp meth-\nods that convert ENP scores into Humdrum data streams or\nﬁles. Humdrum Extras reverses the process with a program\ncalled hum2enp to revert back to ENP data. These two con-\nverters allow for round-trip processing between ENP ob-\njects and Humdrum tools. For example, music generated\nalgorithmically in PWGL can be exported into Humdrum\ndata, then processed with Humdrum tools and converted\nback into ENP with some analytic markup.\nThe Humdrum and ENP representations of music are\ndifferent in several ways. The Humdrum data paradigm is\na spreadsheet. Each column (or more generally, a spine in\nHumdrum terminology) of data represents a stream of in-\nformation such as a part, voice, dynamic, text or analytic\ndata sequence. Each row in the data represents a simul-\ntaneity, meaning that all events in the row occur at the same\ntime, with time progressing downwards in the ﬁle; any row\nabove the current one occurs in its past, and rows below the\ncurrent one occur in its future. In other words, horizontal\nlines of data can be thought of as being the total sound-\ning harmony at any give point in time. This arrangement\ndirectly represents the conﬁguration of notes in a musical\nscore, but rotated 90\u000eclockwise and without system breaks\nas shown in Figure 1. This also means that the lowest part\nin a ﬁle is the left-most spine in the data.\n4dd4d 2g \n4e 8c \n4F 4E 4d \n4A 8B 4B 4G \n/c35(a) (b) (c) (d)\n(e)\n(f)\n(g)\n(h)8Figure 1 . An ENP score displayed as graphical music no-\ntation (top) partitioned into harmonic slices (a–d) which\nare converted into kern records (e–h).\nAn ENP data ﬁle serializes the score by part from\nhighest to lowest in the musical system with a conﬁgu-\nration analogous to Type-1 Standard MIDI Files. While\nHumdrum data is arranged two-dimensionally, ENP data\nis stored in a hierarchical tree structure by part, voice,\nrhythm, chord, then note in a similar manner to most XML\nformats used for encoding musical data. Analytic data is\ntypically stored as expressions on the chord or note level.\nHumdrum ﬁles, on the other hand, typically store ana-\nlytic data in spines (columns) that are parallel to the main\nstreams of musical data.\nIn order to convert from ENP scores into Humdrum, the\nscores are ﬁrst partitioned into harmonic slices which ef-\nfectively represent rows of Humdrum data (see Figure 1).\nA harmonic slice is created whenever there is at least one\nnote object with a unique start time. Notes sharing start\ntimes are collected within the same harmonic slice using\nan identical process for harmonic analysis in PWGL, or to\ngenerate a notated score. Now the harmonic slices, which\nare already ordered by time, can be written as Humdrum\ndata rows by reading each harmonic slice from bottom to\ntop. Each harmonic slice thus becomes a Humdrum record\nwhere the lowest voice is encoded to the left-most column\nand the highest part in the right-most column. In gen-\neral, every ENP voice becomes a kern spine within the\nHumdrum data. The Humdrum export is mainly intended\nfor music analytic applications and therefore the complete\nENP score structure is not necessarily preserved. In ad-\ndition to rhythm and pitch, other notational properties ex-\nported from ENP to kern include instrument names, clefs,Figure 2 . Sample representation of complex rhythms in ENP and kern data formats.\ntempos, time signatures, articulations, and optionally beam\nand tuplet groupings.\nOnce the musical data has been transformed into the\nHumdrum format, it can be processed with Humdrum tools\nand then converted back into ENP using the hum2enp pro-\ngram provided by Humdrum Extras. Humdrum data may\ncontain separate spines for music and analytic data. As\nthehum2enp program converts the musical data into ENP,\nit collates analytic data into ENP expressions attached to\nnotes or chords.\nIn order to correctly rebuild the rhythmic hierarchy in\nENP, layout extensions have been created for kern data to\ndemark tuplet groupings of arbitrary rhythmic complexity.\nFigure 2 shows ENP and Humdrum music encodings that\nrepresent the same rhythmic formations: a nested quintu-\nplet and an irregularly subdivided septuplet. The structure\nof ENP is encoded within the parentheses levels starting\nwith the outer pair for the score, then parts, voices, and\nmeasures. Rhythmic values are stored within measures as\nproportions in a hierarchical manner that allows nested tu-\nplet rhythms as shown in Figure 2, where the triplet encom-\npasses a duration of one quintuplet sixteenth note (1/3rd of\n1/5th of a beat). The top level of the rhythmic structure is\nthe beat, with subdivisions of the beat inserted as a list of\nproportions and notes which occur within the duration of\ntheir parent rhythmic proportion. Humdrum’s linear repre-\nsentation of the music is more direct to the graphical music\nnotation. Each part is a spine (column) of kern data with\ntime progressing downwards. While ENP uses MIDI notenumbers to encode pitch (with optional note attributes to\nresolve enharmonic spellings), kern data uses letter names\nfor pitches. Upper case letters are for the octave below\nmiddle C, and lower case for the middle-C octave. Rhyth-\nmic values are numbers indicating the note count of that\nduration which are needed to create a whole note. For\nexample, a septuplet sixteenth note is represented by the\nnumber 28, since 28 notes with this duration are needed to\ncreate a whole note. Dots following rhythmic values are\naugmentation dots, so “28.” is a dotted septuplet sixteenth\nnote, adding one-half of the dotless duration to the note.\nDots in isolation are sequential alignment place holders\nindicating that the current part has no new activity while\nsomething new is occurring in the other part. The rhythmic\nvalue 60 can be interpreted as 4 \u00025\u00023 (a quarter note di-\nvided into 5, then each ﬁfth divided into 3) or 4 \u00023\u00025\n(a quarter note divided into 3, further divided into 5). The\nHumdrum representation in Figure 2 shows how to disam-\nbiguate between two such possible tuplet interpretations by\nusing optional layout codes for tuplet groupings. The rpa-\nrameter for tuplet layouts indicates the rhythmic duration\nof the tuplet, and the tparameter indicates the tuplet type.\nThus the layout instruction !LO:TUP:r=20:t=3 means\nthat starting on the next note in the data stream ( 60f), a\ntriplet ( t=3) is to be applied with a total duration of a quin-\ntuplet sixteenth note ( r=20 ).$ thrux \nexe pipe menu-box \n\"h://beethoven/sonatas\" \n | \n[L] exe pipe $ gettime \n--total \nexe pipe \ntext-box \n(E) (\"Total time: 7:36:47.7789 hours\") $ thrux \n--variation \nexe pipe value-box \n\"norep\" \ntext-box \n(E) (\"Total time: 9:24:14.0624 hours\") 1\n2Figure 3 . A PWGL Patch containing Humdrum boxes\nsuch as thrux that are distinguished from regular PWGL\nboxes by titles enclosed in rounded rectangles.\n2.2 Interfacing to Humdrum tools\nIn addition to data musical transfer between the Humdrum\nand ENP representational systems, PWGL incorporates ac-\ncess to command-line Humdrum tools as mentioned in the\nintroduction that allows processing of Humdrum data di-\nrectly within PWGL. Figure 3 illustrates a PWGL patch\nthat interfaces with two Humdrum Extras tools, thrux and\ngettime , to measure the total performance duration of a\nHumdrum data source (either a single work or a stream of\nmultiple works or movements). The patch can be used to\ncalculate the performance duration of input Humdrum data\nin two conﬁgurations: either the performance time when\nall repeats are taken, or the total amount of time when only\nsecond endings are taken. In this case the analysis shows\n(1) that the complete Beethoven piano sonatas would take\nabout 9.5 hours to perform when taking all repeats (at the\ntempo speciﬁed within the data, taking no breaks between\nmovements), or (2) about 7.5 hours without repeats. The\nmaster switch at the bottom right of the patch is used to\nchoose between these two analysis methods to calculate\nthe total performance time. Users of the patch can eas-\nily select a different repertory at the top of the patch, and\nthe patch can be displayed in presentation mode to further\nsimplify the interface (see Figures 4 & 5).\nThe menu-box object at the top of the patch stores a\nlist of data sources, with the complete Beethoven piano\nsonatas currently selected. In this example the “h://” pre-\nﬁx indicates that the data will be downloaded dynamically\nfrom the Internet.3When the switch button in the patch\n3speciﬁcally, the h://beethoven/sonatas URI maps onto the URL\nhttp://kern.ccarh.org/data?l=beethoven/sonatas\nFigure 6 . The options dialog for hum2enp command\nshowing all the available ﬂags.\ncauses data to ﬂow through the left-hand thrux box, the\ninput data stream is converted from printed ordering of\nthe notation into performance ordering. The right-hand\nthrux box adds the --variation norep option to the\ncommand, which instructs thrux to take only second end-\nings in the music to avoid repetitions. Output from both\nthrux commands is then piped to the gettime program. The\n--total option tells gettime to calculate the total dura-\ntion of its input data. Without this option the program cal-\nculates the performance time of each harmonic slice. As a\ncomparison, the patch can be expressed in unix shell syn-\ntax as two separate commands:\nthrux h://beethoven/sonatas | gettime -T\nthrux h://beethoven/sonatas -v norep | gettime -T\nwhere -vis an alias for --variation , and -Tfor\n--total .\nA more elaborate visualization of Humdrum analysis\ndata can be seen in Figures 4 & 5 which display processed\nHumdrum data in two very different graphic formats on\nthe patch. A menu-box at the top of the patch in Fig-\nure 4 is used to select a Humdrum ﬁle either from the lo-\ncal hard disk or downloaded via the web. The contents\nof this ﬁle can be pulled through two data paths in the\npatch. Humdrum data can be transformed into ENP data\nusing the import-kern box and then displayed as graphi-\ncal music notation within the Score-Editor box. A second\npath travels through the mkeyscape box which converts the\nHumdrum data into a graphic image. This image shows a\nkeyscape plot [13] that summarizes the harmonic structure\nof the chorale, which is primarily in G major with transient\nkey/chord regions on V , vi, and iii. Future work will im-\nplement a playback cursor in image boxes which link to\nthe playback position when playing music in Score-Editor\nboxes. This will allow interactive visual feedback when\nplaying music in the Score-Editor.\nAll Humdrum Extras command-line tools possess an\noption called --options that lists all options the pro-\ngram recognizes. The PWGL Humdrum library takes ad-\nvantage of this self-documenting feature to extract a list\nof options that the program understands. When double-\nclicking on Humdrum boxes in a PWGL patch, a list of$Figure 4 . A PWGL patch which loads a Humdrum data ﬁle for a J.S. Bach chorale into the ENP Score-Editor as well as a\nvisual analysis of the harmonic structure generated by the Humdrum Extras mkeyscape tool.\n96&#1\n44SS=60\nS S SS#S S S# SS S S S S\n&#44S S S SSS SSS S\nS S SSS S\n&#44S SSSSSSS S S S\nSS SSSSS S\n?#44SS S SS SS S\nSS\nS# SSSSSSSS\"h://chorales/chor009.krn\"\nNumber of Segments:8\nFigure 5 . The patch shown in Figure 4 displayed in the presentation mode. The presentation mode allows us to hide the\nprogramming details from the end users. It also allows us to lay out the patch in a different way to create a more functional\nuser-interface.available command-line options appears, such as the list\nshown in Figure 6. Options can also be deﬁned to accept\nstrings or numbers as arguments, and any default value will\nalso be displayed in the output of the program when using\nthe--options option. In this example the --help op-\ntion includes -has a shorter alias.\nFigure 5 shows the same patch as in Figure 4, but in pre-\nsentation mode . The PWGL presentation mode is used to\nhide programming details in a patch to display it in a sim-\npliﬁed form that focuses on inputs and outputs, suppress-\ning intermediate details. In presentation mode the connec-\ntions between boxes are hidden, and individual boxes can\nthemselves be hidden, resized, displaced, as well as scaled\nor repositioned from their usual programming mode lay-\nout. The -soption for mkeyscape , which controls the\nnumber of analysis segments in the triangular image, is\nvisible in Figure 4, taking a value of 20 from the attached\nnum-box. In Figure 5 the call to the mkeyscape program is\nhidden, with only the number of analysis segments visible\non the patch. In presentation mode the source score can be\nselected at the top of the patch, and the number of segments\ncan be changed by typing a new value in place of 96. To\nrecalculate the patch after changing the settings, the arrow\nat the bottom of the presentation patch is clicked. The pro-\ngram/presentation mode state is persistent for a patch so\nthat it can be saved and then opened in presentation mode.\nThe user can toggle back to the programming mode where\nvisibility and size of all patch boxes are restored as illus-\ntrated in Figure 4.\n3. CONCLUSIONS\nPWGL’s Humdrum library enables users of either PWGL\nor Humdrum to have access to tools available in the other\nsystem. For PWGL users the main beneﬁt is access to\nthousands of classical music scores in the Humdrum ﬁle\nformat, while Humdrum users have access to ENP’s score\neditor that provides a rich palette of analytic markup for\nmusic notation. The PWGL/Humdrum interface allows\nusers to access the full functionality of the Humdrum\ntoolkit within PWGL and also allows for data exchange be-\ntween the two systems via the Humdrum ﬁle format. The\nPWGL Humdrum library also offers enhancements over\nthe standard Humdrum Toolkit by providing Humdrum\nusers with a visual interface to Humdrum command-line\ntools. This is especially beneﬁcial for non-programmers\nwho usually feel more comfortable in a graphical program-\nming environment. Since PWGL is composed of a gener-\nalized programming environment as well as the ENP nota-\ntion editor, data obtained via Humdrum ﬁles/tools can be\nfurther processed using either pre-existing or user-deﬁned\ntools within PWGL.\nThe PWGL Humdrum library can be downloaded at\nhttp://www2.siba.fi/PWGL/downloads . The\nhum2enp command-line tool is distributed as a part of\nHumdrum Extras and can be obtained from http://\ngithub.com/craigsapp/humextra .4. ACKNOWLEDGMENTS\nThe work of Mika Kuuskankare has been supported by the\nAcademy of Finland (SA137619). The authors would also\nlike to thank both CCRMA and CCARH at Stanford Uni-\nversity for hosting this research.\n5. REFERENCES\n[1] David Huron. Music information processing using the\nHumdrum Toolkit: Concepts, examples, and lessons.\nComputer Music Journal , 26(2):15–30, 2002.\n[2] Ian Knopke. The Perlhumdrum and Perllilypond toolk-\nits for symbolic music information retrieval. In Inter-\nnational Symposium on Music Information Retrieval ,\npages 147–152, 2008.\n[3] Andreas Kornst ¨adt. The JRing system for computer-\nassisted musicological analysis. In ISMIR , 2001.\n[4] Mika Kuuskankare and Mikael Laurson. Intelligent\nScripting in ENP using PWConstraints. In Proceedings\nof International Computer Music Conference , pages\n684–687, Miami, USA, 2004.\n[5] Mika Kuuskankare and Mikael Laurson. Expressive\nNotation Package. Computer Music Journal , 30(4):67–\n79, 2006.\n[6] Mika Kuuskankare and Mikael Laurson. Survey of mu-\nsic analysis and visualization tools in PWGL. In Pro-\nceedings of International Computer Music Conference ,\npages 372–375, 2008.\n[7] Mikael Laurson and Mika Kuuskankare. A constraint\nbased approach to musical textures and instrumental\nwriting. In CP01 workshop on Musical Constraints ,\nCyprus, 2001.\n[8] Mikael Laurson, Mika Kuuskankare, and Kimmo\nKuitunen. Introduction to computer-assited music anal-\nysis in PWGL. In Sound and Music Computing ’05 ,\n2005.\n[9] Mikael Laurson, Mika Kuuskankare, and Kimmo\nKuitunen. The Visualisation of Computer-assisted Mu-\nsic Analysis Information in PWGL. Journal of New\nMusic Research , 37(1):61–76, 2008.\n[10] Mikael Laurson, Mika Kuuskankare, and Vesa No-\nrilo. An Overview of PWGL, a Visual Programming\nEnvironment for Music. Computer Music Journal ,\n33(1):19–31, 2009.\n[11] Vesa Norilo. Introducing Kronos - A Novel Approach\nto Signal Processing Languages. In Frank Neumann\nand Victor Lazzarini, editors, Proceedings of the Linux\nAudio Conference , pages 9–16, Maynooth, Ireland,\n2011. NUIM.\n[12] Craig Stuart Sapp. Online database of scores in the\nHumdrum ﬁle format. In Proceedings of ISMIR , pages\n664–665, 2005.\n[13] Craig Stuart Sapp. Computational Methods for the\nAnalysis of Musical Structure . Stanford University,\n2011."
    },
    {
        "title": "On Finding Symbolic Themes Directly From Audio Files Using Dynamic Programming.",
        "author": [
            "Antti Laaksonen",
            "Kjell Lemström"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418349",
        "url": "https://doi.org/10.5281/zenodo.1418349",
        "ee": "https://zenodo.org/records/1418349/files/LaaksonenL13.pdf",
        "abstract": "In this paper our goal is to find occurrences of a theme within a musical work. The theme is given in a symbolic form that is searched for directly in an audio file. We present a dynamic programming algorithm that is related to an existing time-warp invariant algorithm. However, the new algorithm is computationally more efficient than its predecessor, and it can also be used for approximate timescale invariant search. In the latter case the note durations in the query are taken into account, but some time jittering is allowed for. When dealing with audio, these are important properties because the number of possible note events is large and the note positions are not exact. We evaluate the algorithm using a collection of themes from Tchaikovsky’s symphonies. The new approximate timescaled algorithm seems to be a good choice for this setting.",
        "zenodo_id": 1418349,
        "dblp_key": "conf/ismir/LaaksonenL13",
        "keywords": [
            "dynamic programming",
            "audio file",
            "symbolic form",
            "time-warp invariant",
            "note durations",
            "time jittering",
            "approximate timescale invariant",
            "note events",
            "Tchaikovskys symphonies",
            "search algorithm"
        ],
        "content": "ON FINDING SYMBOLIC THEMES DIRECTLY FROM AUDIO FILES\nUSING DYNAMIC PROGRAMMING\nAntti Laaksonen1, Kjell Lemstr ¨om2;1\n1Department of Computer Science, University of Helsinki\n2Laurea University of Applied Sciences\nahslaaks@cs.helsinki.fi, kjell.lemstrom@laurea.fi\nABSTRACT\nIn this paper our goal is to ﬁnd occurrences of a theme\nwithin a musical work. The theme is given in a symbolic\nform that is searched for directly in an audio ﬁle. We\npresent a dynamic programming algorithm that is related\nto an existing time-warp invariant algorithm. However, the\nnew algorithm is computationally more efﬁcient than its\npredecessor, and it can also be used for approximate time-\nscale invariant search. In the latter case the note durations\nin the query are taken into account, but some time jitter-\ning is allowed for. When dealing with audio, these are\nimportant properties because the number of possible note\nevents is large and the note positions are not exact. We\nevaluate the algorithm using a collection of themes from\nTchaikovsky’s symphonies. The new approximate time-\nscaled algorithm seems to be a good choice for this setting.\n1. INTRODUCTION\nIn this paper we present new content-based music retrieval\n(CBMR) algorithms for discovering occurrences of a given\nmusical theme, called the pattern , in a musical work under\nconsideration, called the database . The algorithms have\nboth important scientiﬁc applications and theoretical inter-\nest: they can be used in music analysis to locate occur-\nrences of themes in a single musical work or to search for\na given theme within a music database. Moreover, to the\nbest of our best knowledge, our algorithm for time-warped\nsearch is the most efﬁcient algorithm available for the task.\nWe allow both the pattern and the database to be poly-\nphonic. Traditionally CBMR problems like this have been\ntackled by using a linear string representation combined\nwith a string matching algorithm. However, the represen-\ntation does not effectively capture important intrinsic fea-\ntures of music. The geometric modeling of music [8] is\nmore appropriate for this case and has recently been suc-\ncessfully used by several authors [4, 7, 10, 13]. Reasonable\ngeometric modelling allows the matching process to ignore\nextra intervening notes in the database that do not appear\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.in the query. Such extra notes always occur because of\npolyphony and unexpected noise.\nUntil recently, the geometric framework suffered from\nthe fact that the timing of the notes had to be exact. Recent\nstudies have challenged this problem. First, in [5, 10] the\noccurrences were allowed to be transposed and time-scaled\ncopies of the query. Under this transposition and time-\nscale invariance (theTTSI setting), however, the queries\nstill need to be given exactly in tempo. Under a more re-\nalistic, transposition and time-warp invariance (theTTWI\nsetting), local time jittering is allowed for in every note-\nonset. The ﬁrst solutions for this setting were introduced\nby Lemstr ¨om and Laitinen [4, 6].\nIn this paper we introduce a new dynamic programming\nalgorithm that can be used for both time-scale and time-\nwarp invariant search. As an application of the algorithm,\nwe search for musical themes from audio ﬁles. Only a\nfew other algorithms for symbolic queries from audio have\nbeen proposed in the literature [2, 11, 12]. Our experiment\nshows that our algorithms are efﬁcient in practice, and the\nmusic search results from audio ﬁles are also promising.\nLet us next deﬁne the problems under consideration.\nThe problems are elaborations of Ukkonen, Lemstr ¨om and\nM¨akinens P1 problem [13]. In each case, the music is rep-\nresented in a pitch-against-time representation of note-on\ninformation. The database Tcontainsnnote events, and\nthe query pattern Pcontainsmnote events. In a typical re-\ntrieval case, the pattern Pis monophonic and much shorter\nthan the polyphonic database T. The problems are:\nW1: Find time-warped translations ofPsuch that each\nnote inPmatches with a note in T[4, 6].\nS1: Find time-scaled translations ofPsuch that each\nnote inPmatches with a note in T[5, 10].\nAS1: Find approximate time-scaled translations ofP\nsuch that each note in Pmatches with a note in T.\nThe new problem (AS1) can be seen as an intermediate\nform of W1 and S1. Instead of a constant time-scaling\nfactor\u000b, we allow the scaling to vary in a range [\u000b=\u000f;\u000b\u000f ]\nwhere\u000f\u00151. If\u000f= 1, the problem is the same as S1.\n2. ALGORITHMS\nThe dynamic programming algorithm of Laitinen and Lem-\nstr¨om [4] solves the problem W1 in O(nmw )time where\nwis the window size. In this section we present a modiﬁ-\ncation for the algorithm that reduces the time complexity toO(nm). Furthermore, the new algorithm also solves prob-\nlems S1 and AS1 in O(nms)time wheresis the number\nof possible scaling factors.\nThe input of the algorithm consists of two lists: the\ndatabase notes T[1];T[2];:::;T [n]and the pattern notes\nP[1];P[2];:::;P [m]. Each database note T[k]is a triplet:\nT[k]:time is the onset time, T[k]:pitch is the pitch value\nandT[k]:score is a real value in the range [0;1]. Each\npattern note P[k]is a pair with the ﬁelds P[k]:time and\nP[k]:pitch . We assume that both of the lists are sorted in\nincreasing order according to the onset times and that the\npitches are represented in semitones.\nThe score ﬁeld of the database notes is only used with\naudio material. If the input is symbolic music, each score\nis simply set 1. However, when audio material is used, the\nnote events are approximations. The idea is that the score\nreﬂects the amplitudes of the frequencies that correspond\nto the note in the audio.\nThe output of the algorithm is a list S[1];S[2];:::;S [n]\nconsisting of real values. Each value S[k]is the maximum\nscore for a pattern occurrence starting from the database\nnotek. In an occurrence, each pattern note is matched with\na database note, and the score is calculated as a sum of\nindividual scores of the database notes. In the case of a\nsymbolic database, a score of mdenotes that the pattern is\nfound, and in the case of an audio database a high score\nsuggests that the pattern is found.\nNext, in Section 2.1 we present our modiﬁcations to\nthe time-warped search algorithm. After this, in Section\n2.2, we show how the algorithm can also be used for time-\nscaled search.\n2.1 Time-warped search\nLet us next introduce a reformulation of the dynamic pro-\ngramming algorithm for W1 that was presented in [4]. The\noriginal algorithm uses a three-dimensional array M[t;p;c ]\nwheretandprefer to a database and pattern note and cis\na counter that limits the search window. Instead of this,\nwe use a two-dimensional array M[t;p]and go through all\npossible values of c.\nforp=m!1do\nfort=n!1do\nifp=mthen\nM[t;p] =T[t]:score\nelse\nb=\u00001\nforc= 1!min(w;n\u0000t)do\nd1=T[t+c]:pitch\u0000T[t]:pitch\nd2=P[p+ 1]:pitch\u0000P[p]:pitch\nifd1=d2then\nb= max(b;M[t+c;p+ 1] +T[t]:score )\nend if\nend for\nM[t;p] =b\nend if\nend for\nend forThis algorithm can be used to calculate the list Sbe-\ncauseS[k] =M[k;1]for eachk. The time complexity\nis stillO(nmw ), but next we will show how to reduce it\ntoO(nm). The idea is to remove the innermost loop and\nﬁnd the best value more efﬁciently. To achieve this, we\nmaintain a deque of maximum values for each possible\npitch and use a standard sliding window maximum algo-\nrithm. Note that we assume that the set of possible pitches\nis constant. This is a very natural assumption when we are\nworking with pitch values in semitones.\nConsider the situation where pandthave been ﬁxed and\np < m . We are looking for maximum value M[t0;p+ 1]\nwheret<t0\u0014t+wwith the constraint that T[t0]:pitch =\nT[t]:pitch +P[p+ 1]:pitch\u0000P[p]:pitch . Now, for each\npitchv, we maintain a deque D[v]containing pairs whose\nﬁrst element is the value M[t0;p+ 1] and second element\nis the position t0. The last pair in the deque always corre-\nsponds to the maximum value in the window.\nWe update the deques as follows. When the value thas\nchanged, we always add the pair (M[t+ 1;p+ 1];t+ 1) to\nthe front of deque D[T[t+ 1]:pitch ]and remove the pair\n(M[t+w+ 1;p+ 1];t+w+ 1) from the tail of deque\nD[T[t+w+ 1]:pitch ]. Before adding the new pair, we\nremove all pairs from the front of the deque whose value\nis smaller than the new value. When removing the pair, we\nrequire that the second element equals the current position\n(i.e. the pair has not been removed earlier).\nThe amortized time complexity for ﬁnding all the max-\nimum values for ﬁxed pisO(n)because every value is\nadded and removed to a deque only once. Thus the time\ncomplexity of the whole algorithm is O(nm).\nThe following pseudocode clariﬁes the structure of the\nO(nm)algorithm. The array Dcontains a deque for each\npossible note, and the deque operations work as described\nabove. Note that the operation remove only takes the sec-\nond element of the pair to be removed.\nforp=m!1do\nD:clear ()\nfort=n!1do\nifp=mthen\nM[t;p] =T[t]:score\nelse\nift+ 1\u0014nthen\nD[T[t+1]:pitch ]:add((M[t+1;p+1];t+1))\nend if\nift+w+ 1\u0014nthen\nD[T[t+w+ 1]:pitch ]:remove (t+w+ 1)\nend if\nv=T[t]:pitch +P[p+ 1]:pitch\u0000P[p]:pitch\nifD[v]:empty ()then\nM[t;p] =\u00001\nelse\nM[t;p] =D[v]:maximum () +T[t]:score\nend if\nend if\nend for\nend forNote that if we use a symbolic database and every note\nscore is 1, the deques are not needed. In this case it is\nsufﬁcient to store, for each pitch v, the most recent position\nR[v], for whichM[t0;p+ 1] =n\u0000p. IfR[v]\u0014t+w,\nthenM[t;p]equalsn\u0000p+ 1and otherwise\u00001.\n2.2 Time-scaled search\nThe algorithm described in Section 2.1 can also be used\nfor time-scaled search. The only difference is that the size\nof the window is not constant. Instead of this, the size\nof the window depends on the position in the pattern and\nthe time values of the notes. However, the sliding window\nmaximum algorithm can still be used and the time com-\nplexity of the algorithm remains O(nm)when the range\nof the scaling factor is ﬁxed.\nLet\u000bbe the scaling factor and \u000fbe the jittering toler-\nance. In this case, pattern notes P[p]andP[p+ 1] can be\nmatched to database notes T[t]andT[t0]only if (P[p+\n1]:time\u0000P[p]:time )\u000b=\u000f\u0014T[t0]:time\u0000T[t]:time and\n(P[p+1]:time\u0000P[p]:time )\u000b\u000f\u0015T[t0]:time\u0000T[t]:time .\nIn other words, we are looking for t0values for which\nt+w1\u0014t0\u0014t+w2,T[t+w1]is the ﬁrst note that\nfulﬁls the ﬁrst condition and T[t+w2]is the last note that\nfulﬁls the second condition.\nWe can combine the parameters w1andw2with the al-\ngorithm presented in Section 2.1 as follows. Consider the\nsituation for ﬁxed p. First, we set w1=w2=n. When\ntchanges, we decrease w1as long as (P[p+ 1]:time\u0000\nP[p]:time )\u000b=\u000f\u0014T[w1\u00001]:time\u0000T[t]:time . Then, we\ndecreasew2as long as (P[p+ 1]:time\u0000P[p]:time )\u000b\u000f\u0014\nT[w2\u00001]:time\u0000T[t]:time . When decreasing w1, we\nadd new values to the deque, and when decreasing w2,\nwe remove old values. Even if the deque operations are\nmore irregular than in the time-warped search, each value\nis added and removed only once and the time complexity\nremainsO(nm).\nIn practice, the correct \u000bvalue is unknown and several\nsearches have to be performed. Thus, the time complexity\nbecomesO(nms)wheresis the number of searches. In\npractice, however, the number of scaling factors that needs\nto be tested is small because the duration of a single note\nis typically at most some seconds.\n3. EXPERIMENT\nIn this section, we present the results of our experiment\nwhere we searched for themes within audio material using\nthe algorithms described in Section 2. We consider two\nscenarios: (1) ﬁnding the location of the given theme in a\nsingle audio ﬁle, and (2) ﬁnding the audio ﬁle which con-\ntains the given theme. Moreover, we analyse the perfor-\nmance of the O(nmw )andO(nm)algorithms in practice.\n3.1 Material\nThe material used in the experiment, shown in Table 1,\nconsists of Tchaikovsky’s six symphonies.\nWe created the audio database from Deutsche Gram-\nmophon recordings found under the catalogue id 423 504-Work Movements Total length Themes\nSymphony 1 4 44 min 10\nSymphony 2 4 35 min 11\nSymphony 3 5 46 min 15\nSymphony 4 4 42 min 13\nSymphony 5 4 49 min 15\nSymphony 6 4 45 min 11\nTotal 25 262 min 75\nTable 1 . The material used in the experiment.\n2. We converted the material from CD tracks into mono\nWA V ﬁles with a sample rate 44,100 Hz. Each audio ﬁle\ncontains a single movement of a symphony. The total du-\nration of the audio is 4 hours and 22 minutes.\nThe themes to be searched for were taken from the book\nA Dictionary of Musical Themes [1]. The book contains\n75 themes from Tchaikovsky’s symphonies and we used\nall of them in the experiment. Each theme has an identiﬁer\nin the form of TXXX where X is a digit. The themes are\nalso available online1as MIDI ﬁles which we automati-\ncally converted to simple symbolic notation. In addition,\nfor each theme, we marked the locations where it appears\nin the audio material to allow automatic evaluation.\nThe duration of a theme ranges from about 5 seconds\nto about 25 seconds. We kept all of the themes unchanged\nwith two exceptions. First, theme T231 actually consists\nof two themes from which the second one is very short.\nThey are indexed separately online, and we have removed\nthe second theme from our material. Second, there is an\nerror in the notation of theme T243 both in the book and\nonline: only the ﬁrst two triplets are marked and the other\nnotes are too slow. We corrected this in our material.\n3.2 Audio processing\nOur algorithms require that the database is represented as\na list of note events. However, there is no direct way to\nconvert an audio ﬁle into symbolic notation. Therefore we\nestimated the note events using the discrete Fourier trans-\nform. In general, strong notes in the music can be observed\nas strong frequencies in the audio signal.\nWe used standard techniques for processing the audio\nsignal: we divided each audio ﬁle into frames of 4,096\nsamples. After this, we calculated a frequency spectrum\nfor each frame using the discrete Fourier transform. We\nassumed that the frequency of A4 is 440 Hz and calcu-\nlated the amplitude of its frequency in the spectrum for\neach note from G3 to B5. The selected note range encom-\npasses a large part of the melodies in orchestral music and\navoids very low and very high notes. All the amplitudes\nwere normalized and estimated using linear interpolation.\nWe built three databases of note events, which are sum-\nmarized in Table 2. The value krefers to the amount of\nnote events produced from a single audio ﬁle. The values\nminkandmaxkare the minimum and maximum amounts\n1http://www.multimedialibrary.com/barlow/Database mink maxk n\nD1 107,793 353,249 4,948,386\nD2 33,830 107,914 1,541,098\nD3 37,510 122,703 1,732,997\nTable 2 . The number of note events in the databases.\nof note events in a ﬁle, and the value ndenotes the total\nnumber of note events in the database.\nDatabase D1 contains a note event for each note in each\nframe, and the score of the note event is the amplitude of\nthe note in the spectrum. In this case the database includes\na lot of note events with low scores that are unlikely to be\na part of a theme occurrence.\nDatabases D2 and D3 are subsets of D1, and the aim is\nto locate potential note candidates from the full spectrum.\nDatabase D2 contains the notes whose amplitudes are peak\namplitudes (i.e. the note with pitch pis selected only if\nnotes with pitches p\u00001andp+1have a weaker amplitude).\nDatabase D3 contains the 10 strongest notes in terms of the\namplitude for each frame. A value of 10 is selected so that\nthe sizes of D2 and D3 are nearly equal to each other.\nThis estimation of note events resembles salience value\ncalculation in automatic melody extraction (for a review,\nsee [9]). However, salience values are usually calculated\nas combinations of the harmonics of the note. We exper-\nimented with various ways to use higher harmonics, but\nto our surprise, using only the fundamental frequency pro-\nduced the best results with this material.\n3.3 Algorithms\nWe implemented the audio processing method described\nin Section 3.2 and the time-warped and time-scaled algo-\nrithms described in Sections 2.1 and 2.2 by using C++ and\nFFTW library. For the rest of the paper, we call the time-\nwarped algorithm W and the time-scaled algorithm S.\nWhen using algorithm W, we had to choose the window\nlengthw. In this experiment, whas to be relatively high be-\ncause there can be a large number of note events between\ntwo consecutive theme notes in the database. More pre-\ncisely, the amount of note events per second is about 300\nin D1 and about 100 in D2 and D3, and a note in a theme\noccurrence can have a duration of several seconds.\nIn algorithm S there are several parameters. First, the\nscaling factors \u000bhave to be speciﬁed. In this experiment\nwe assumed that the duration of theme is between 5 and 25\nseconds and tested \u000bvalues for which the time between the\nﬁrst and last note in the theme is 5;6;7;:::; 25seconds.\nThe jittering tolerance \u000fwas more difﬁcult to choose; in\norchestral music the rhythm is usually quite exact, thus in-\ndicating a small value to be the most suitable option.\nMoreover, we had to choose how the occurrences of the\nthemes were reported. For this purpose we used the param-\neter\u000e, a real number in the range [0;1]. All occurrences\nwith a score of at least \u000ebwere reported, where bis the\nbest score. The lower the \u000eis, the more results are pro-\nduced. However, a small \u000ewould result in a large numberDatabase Metric W S\nD1 Themes found 29 34\nTotal results 76 77\nPrecision 0.38 0.44\nD2 Themes found 30 29\nTotal results 76 77\nPrecision 0.39 0.38\nD3 Themes found 31 33\nTotal results 76 77\nPrecision 0.41 0.43\nTable 3 . The best results of the algorithms. For W, pa-\nrameterwis 500 in D1 and 150 in D2 and D3, and for S,\nparameter\u000fis 1.50\nwn\u000e1.00 0.99 0.95 0.90 0.75\n100 12 15 28 46 69\n0.16 0.14 0.09 0.05 0.01\n200 22 25 35 52 70\n0.29 0.24 0.12 0.06 0.01\n500 29 34 51 64 71\n0.38 0.28 0.09 0.03 0.01\n1000 23 32 61 71 74\n0.24 0.15 0.04 0.02 0.01\n2000 22 41 72 73 74\n0.07 0.05 0.02 0.01 0.01\nTable 4 . The results of W on database D1 when wand\u000e\nchange. Themes found and precision levels are reported.\nof false positives. Finally, we required that the interval be-\ntween two reported occurrences is at least 5 seconds. This\nwas done to prevent one occurrence from being reported\nmultiple times.\n3.4 File search\nIn the ﬁrst part of the experiment, we searched for each\ntheme in the ﬁle where it appears. We calculated three\nmeasures: the number of themes located correctly, the to-\ntal number of occurrences reported and the search preci-\nsion as the ratio of previous two values. We considered\nthat the theme was found if the difference between one of\nthe occurrences reported by the algorithm and one of the\noccurrences in the ground truth was less than 5 seconds.\nTable 3 shows the best results of the algorithms. The\nparameter\u000eis 1.00 in each case. For algorithm W, param-\neterwis 500 in D1 and 150 in D2 and D3. For algorithm\nS, parameter \u000fis 1.50. Interestingly, the best results were\nachieved in D1 which contains a note event for each pos-\nsible note in each audio frame. The results of S were in\ngeneral somewhat better than the results of W.\nNote that in some cases there can be several occurrences\neven if\u000eis 1.00. For example, when using W and D1, the\nnumber of occurrences is 76 instead of 75. In those cases\nall normalized note scores in the occurrences are 1, thus\nthe total score is mwheremis the number of notes.\u000fn\u000e1.00 0.99 0.95 0.90 0.75\n1.10 25 29 37 51 70\n0.33 0.27 0.13 0.06 0.02\n1.20 30 34 43 53 71\n0.40 0.33 0.14 0.06 0.02\n1.33 33 38 46 58 71\n0.44 0.36 0.14 0.06 0.02\n1.50 34 37 49 60 72\n0.44 0.36 0.13 0.05 0.01\n2.00 30 37 49 63 73\n0.36 0.25 0.07 0.03 0.01\nTable 5 . The results of S on database D1 when \u000fand\u000e\nchange. Themes found and precision levels are reported.\nTable 4 shows the results of W in D1 with different set-\ntings of parameters wand\u000e. The optimal value for wis\napproximately 500. Finally, Table 5 shows the results of S\nin D1 with different settings of parameters \u000fand\u000echange.\nThe optimal value for \u000fis approximately 1.50. Using larger\n\u000evalues, S would still ﬁnd themes accurately in some situ-\nations. For example, when \u000e= 0:99and\u000f= 1:33, S found\n38 themes with a precision level of 0.36.\nThe database used in the experiment is challenging in\ngeneral; ﬁnding some of the themes requires a moderate\namount of work even for an experienced human listener.\nAn interesting phenomenon is that the results of the algo-\nrithms varied very strongly depending on the symphony.\nFor example, S was able to ﬁnd 9 out of 10 themes from\nSymphony 1, but only 1 out of 11 themes from Symphony\n6. A possible explanation for this is that the themes in\nTchaikovsky’s later works are more subtle.\n3.5 Database search\nIn the second part of the experiment, we searched for each\ntheme in the database D1. This resembles the query-by-\nhumming setting [3]; however, our queries are exact (for\nexample, created by a musician) and the database is atypi-\ncal because it contains a small number of ﬁles, each having\nmore than 100,000 note events.\nThe search was conducted as follows. For each ﬁle in\nthe database, we calculated the maximum score for an oc-\ncurrence of the theme. Having done this, we constructed a\nlist of ﬁles that were sorted in decreasing order according\nto their scores. Following the usual convention, we con-\ncentrated on the rank of the correct ﬁle (the ﬁle where the\ntheme actually appears) in the sorted list. For example, a\nrank of 5 means that the correct ﬁle has the 5th highest\nscore in the search results.\nFigures 1 and 2 show the distribution of ranks using\nW and S in D1 with the same parameters as in Table 3.\nThere were 18 and 22 themes with a rank of 1, and 26\nand 31 themes, respectively, with a rank of 1–3. Some-\nwhat surprisingly, in this material locating the theme in the\nwhole database was not much more difﬁcult than locating\nthe theme in the correct ﬁle.\n 0 5 10 15 20 25 30\n12345678910111213141516171819202122232425Figure 1 . Distribution of correct ﬁle ranks using algorithm\nW and database D1.\n 0 5 10 15 20 25 30\n12345678910111213141516171819202122232425\nFigure 2 . Distribution of correct ﬁle ranks using algorithm\nS and database D1.\n3.6 Performance\nFinally, we analyzed the practical performance of the algo-\nrithms. We ran the tests on an Intel Core 2 Duo Processor\nwith a clock rate of 3.16 GHz.\nFigure 3 shows the running times of three algorithms:\nS is theO(nms)implementation of the time-scaled algo-\nrithm, and W and W’ are O(nm)andO(nmw )imple-\nmentations of the time-warped algorithm. We searched for\ntheme T252 with 27 notes from each audio ﬁle in database\nD1 using parameters \u000e= 1:00,\u000f= 1:50andw= 500 , as\nshown in Table 3. As discussed previously, the variable k\ndenotes the number of note events in the audio ﬁle.\nAs expected, W clearly outperformed the other two al-\n 0 2 4 6 8 10 12 14\n 100000  150000  200000  250000  300000  350000time (s)\nkSW’W\nFigure 3 . Running times of S, W’ and W using the param-\neters shown in Table 3.gorithms: it was more than 10 times faster than both W’\nand S. Furthermore, S was almost as fast as W’ although it\nperformed twenty separate searches for each query.\n4. CONCLUSIONS\nIn this paper we presented a new time-warp invariant poly-\nphonic music search algorithm that works in O(nm)time.\nUnlike the earlier algorithms [4, 6] with time complexities\nO(nlognmw )andO(nmw ), the running time of our al-\ngorithm does not depend on the window size w, and it can\nbe used with arbitrarily large windows.\nIn addition, our algorithm can also be used for time-\nscale invariant search. We introduced a new approximate\ntime-scale invariant schema which allows limited jittering\nwithin the scaled note durations. In this case, the time com-\nplexity of our algorithm is O(nms)wheresis the number\nof possible scalings. In typical queries sis small.\nWe used our algorithms to search for musical themes in\nTchaikovsky’s symphonies. We estimated the note events\nusing amplitudes of frequencies in the audio ﬁles. Consid-\nering the challenging nature of orchestral music input, the\nresults of the algorithms were promising, and the experi-\nment also showed that the modiﬁed dynamic programming\nalgorithm is efﬁcient in practice.\nOur future plan is to further develop the database con-\nstruction. Limiting the number of notes in the database\ncould make the search both more accurate and more efﬁ-\ncient. However, within the context of a frame, it is difﬁcult\nto decide which note pitches should be included; in our ex-\nperiment, the best results were achieved when all pitches\nwere included. Therefore, another approach would be to\nremove entire frames which are probably not needed in the\nsearch.\n5. ACKNOWLEDGEMENTS\nThis work has been supported by the Helsinki Doctoral\nProgramme in Computer Science and the Academy of Fin-\nland (grant number 118653).\n6. REFERENCES\n[1] H. Barlow and S. Morgenstern: A Dictionary of Musi-\ncal Themes , Crown Publishers, New York, 1948.\n[2] A. Duda, A. N ¨urnberger and S. Stober: ”Towards query\nby singing/humming on audio databases,” Proceedings\nof the 8th International Conference on Music Informa-\ntion Retrieval , 331–334, 2007.\n[3] A. Ghias et al: ”Query by humming: musical infor-\nmation retrieval in an audio database,” Proceedings of\nACM Multimedia 95 , 231–236, 1995.\n[4] M. Laitinen and K. Lemstr ¨om: ”Dynamic program-\nming in transposition and time-warp invariant poly-\nphonic content-based music retrieval,” Proceedings of\nthe 12th International Society for Music Information\nRetrieval Conference , 369–374, 2011.[5] K. Lemstr ¨om: ”Towards more robust geometric\ncontent-based music retrieval,” Proceedings of the 11th\nInternational Society for Music Information Retrieval\nConference , 577–582, 2011.\n[6] K. Lemstr ¨om and M. Laitinen: ”Transposition and\ntime-warp invariant geometric music retrieval algo-\nrithms,” Proceedings of the 3rd International Work-\nshop on Advances in Music Information Research , 1–6,\n2011.\n[7] A. Lubiw and L. Tanur: ”Pattern matching in poly-\nphonic music as a weighted geometric translation prob-\nlem,” Proceedings of the 5th International Society\nfor Music Information Retrieval Conference , 289–296,\n2004.\n[8] D. Meredith, G. Wiggins and K. Lemstr ¨om: ”Pat-\ntern induction and matching in polyphonic music and\nother multi-dimensional datasets,” Proceedings of the\n5th World Multi-Conference on Systemics, Cybernetics\nand Informatics , 61–66, 2001.\n[9] G. Poliner et al: ”Melody transcription from music au-\ndio: approaches and evaluation,” IEEE Transactions\non Audio, Speech, and Language Processing , 15(4),\n1247–1256, 2007.\n[10] C.A. Romming and E. Selfridge-Field: ”Algorithms\nfor polyphonic music retrieval: The hausdorff metric\nand geometric hashing,” Proceedings of the 8th Inter-\nnational Society for Music Information Retrieval Con-\nference , 457–462, 2007.\n[11] M. Ryyn ¨anen and A. Klapuri: ”Query by humming\nof midi and audio using locality sensitive hashing,”\nProceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing , 2249–2252,\n2008.\n[12] J. Salamon, J. Serr `a and E. G ´omez: ”Tonal represen-\ntations for music retrieval: from version identiﬁcation\nto query-by-humming,” International Journal of Mul-\ntimedia Information Retrieval , 2(1), 45–58, 2013\n[13] E. Ukkonen, K. Lemstr ¨om and V . M ¨akinen: ”Geomet-\nric algorithms for transposition invariant content-based\nmusic retrieval,” Proceedings of the 4th International\nSociety for Music Information Retrieval Conference ,\n193–199, 2003."
    },
    {
        "title": "K-Pop Genres: A Cross-Cultural Exploration.",
        "author": [
            "Jin Ha Lee 0001",
            "Kahyun Choi",
            "Xiao Hu 0001",
            "J. Stephen Downie"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416222",
        "url": "https://doi.org/10.5281/zenodo.1416222",
        "ee": "https://zenodo.org/records/1416222/files/LeeCHD13.pdf",
        "abstract": "Current music genre research tends to focus heavily on classical and popular music from Western cultures. Few studies discuss the particular challenges and issues related to non-Western music. The objective of this study is to improve our understanding of how genres are used and perceived in different cultures. In particular, this study attempts to fill gaps in our understanding by examining K-pop music genres used in Korea and comparing them with genres used in North America. We provide background information on K-pop genres by analyzing 602 genre-related labels collected from eight major music distribution websites in Korea. In addition, we report upon a user study in which American and Korean users annotated genre information for 1894 K-pop songs in order to understand how their perceptions might differ or agree. The results show higher consistency among Korean users than American users demonstrated by the difference in Fleiss’ Kappa values and proportion of agreed genre labels. Asymmetric disagreements between Americans and Koreans on specific genres reveal some interesting differences in the perception of genres. Our findings provide some insights into challenges developers may face in creating global music services.",
        "zenodo_id": 1416222,
        "dblp_key": "conf/ismir/LeeCHD13",
        "keywords": [
            "current music genre research",
            "focuses heavily on classical and popular music",
            "few studies discuss non-Western music",
            "examines K-pop music genres",
            "examines genres used in North America",
            "fills gaps in our understanding",
            "examines background information on K-pop genres",
            "reports upon a user study",
            "perceptions of genres may differ",
            "developers may face challenges"
        ],
        "content": "K-POP GENRES : A CROSS -CULTURAL EXPLORATION  \nJin Ha Lee  Kahyun Choi  Xiao Hu  J. Stephen Downie  \nUniversity of  \nWashington  \njinhalee@uw.edu  University of Illinois  \nckahyu2@  \nillinois.edu  University of  \nHong Kong  \nxiaoxhu@hku.hk University of Illinois  \njdownie@  \nillinois.edu  \nABSTRACT  \nCurrent music genre research tends to focus heavily on \nclassical and popular music from Western cultures. Few \nstudies discuss the particular challenges and issues related \nto non -Western music. The objective of this study is to \nimprove our understanding of how genres are  used and \nperceived in different cultures. In particular, this study \nattempts to fill gaps in our understanding by examining \nK-pop music genres used in Korea and comparing them \nwith genres used in North America. We provide bac k-\nground information on K -pop g enres by analyzing 602 \ngenre -related  labels collected from eight major music di s-\ntribution websites in Korea. In addition, we report upon a \nuser study in which American and Korean users annota t-\ned genre information for 1894 K -pop songs in order to \nunderstand  how their perceptions might differ or agree. \nThe results show higher consistency among Korean users \nthan American users demonstrated by the difference in \nFleiss ’ Kappa values and proportion of agreed genre l a-\nbels. Asymmetric  disagreements between American s and \nKoreans on specific genres reveal some interesting diffe r-\nences in the perception of genres. Our findings provide \nsome insights into challenges developers may face in cr e-\nating global music services.  \n1. INTRODUCTION  \nThe overemphasis on Western music and context has \nbeen a long-standing  issue  in the Music Information Re-\ntrieval  (MIR) domain . To date, there are only a small \nnumber of studies that deal with the organization  of, and \naccess to , non-Western  music . This is a critical issue  con-\nsidering the trend of increasing global distribution  and \nappreciation of music  [10]. \nThere are a wide variety o f different types of metadata \nwhich  can describe music , but m usic genres  in particular  \nare considered one of the primary method s for organizing \nand retrieving music  ([13], [4]). However, we currently \nhave a limited understanding  of the genres of popular m u-\nsic in non -Western cultures . What kinds of genre s are \nused in these cultures and how similar or different are \nthey to genres used in Western cultures ? What kinds of \nissues or challenges  exist in categorizing non -Western \npopular music by genre ? How are the genres used in m u-\nsic-related resources? How are t hey perceived by average \nmusic users  from Western vs. non -Western culture?  This work attempts to bridge th e gap in our knowledge \nof music genre in non -Western cultures by analyzing  the \ngenres used for K -pop (Korean Pop) , and explore how K -\npop genres are p erceived by users cross -culturally.  Korea \nis a particularly  interesting case in which to study cross -\ncultural issues in music genre, as the country was heavily \ninfluenced  by American pop culture  from the 1950s \nthrough the early 2 1st century , but is now exporting cul-\ntural objects  and music  which  are appreciated by foreig n-\ners and actively sought by people outside of Korea [11]. \nThis suggest s that a lot of cross -cultural music seeking is \nhappening  in this space  from both directions . Genre will \nbe extremely important for those users as it can serve as \nuseful metadata to discover  new music they want to listen \nto. The main objectives of our study are : 1) to improve \nour understanding of  how genre labels are used in the Ko-\nrean context  and, 2)  to explore cross -cultural perception \nof K-pop genres. By doing so, we hope to obtain insights \ninto what kinds of challenges we may encounter when we \nstart building a m usic collection targeted for a global user \nbase where cross -cultural music seeking is unavoidable .  \n2. BACKGROUND AND PRIOR  WORK  \nDespite the increasing number of user studies in the MIR \ndomain, there are still only a handful of cross -cultural \nstudies that inv estigate d issues in music information \nneeds, seeking,  organization,  management, and consum p-\ntion. One such study on non -Western music , in particular \nKorean music,  was [5]. Lee et al. [5] collected music \nqueries from Google Answer s and Naver Know ledge-iN \n(지식 iN), and did a comparative analysis . They ident i-\nfied several challenges  that Korean users experienc ed \nwhen trying to find Western music , including : 1) co m-\nmon failure s in providing traditional bibliographic i n-\nformation such as title or name of the performer(s),  2) \ndifficulty in understanding and using  Western genre l a-\nbels, and 3) difficulty in using lyrics as they often consist \nof common words and lack discriminating power. The \nfindings support the necessity of establishing new access \npoints for accommodating  cross-cultural music searching  \nsuch as associative  metadata (e.g., source of music ).  \n  The issue of genre is also raised  in [6] by McEnnis \nand Cunningham as they discuss ed how music can be “in-\nterpreted  in terms of how it expresses local issues and \nconcerns, often quite removed from the circumstances \nthat inspired  the music ’s creation. ” They conclude that \nattempting to define universal meanings for music across \ncultural boundaries is destined to fail. This strongly su p-\nports the need to investiga te how these genre s are actually \nused in different cultures in order to provide better co n-\ntextual information to potential users.      \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2013  International Society for Music Information Retrieval     \n \nNorowi et al. [8] and Doraisamy et al. [1] investigated \nusing MARSYAS for automatic genre classification giv-\nen a mix of Weste rn and traditional Malay genres and \ndemonstrated that it is possible to classify non -Western \nmusic with such  systems . However, Doraisamy et al. [1] \ndid note that adapting digital music library systems for \nthe retrieval of Malaysian music was challenging  due to \ndifferences in genres and musical structures.  \nOne theme emerging  from a number of these studies is \nthe issue of music genre. Genre is the primary means by \nwhich listeners search and browse music  [4], [13], yet \nstudies on genre classification s show that there is hardly \nany convergence  as demonstrated in this quote from [9]:  \n“Easy listening ” in one classification  is cal led “Variety” in an-\nother and worse, taxonomy structures do not match: “Rock” for \ninstance denotes different songs in different classifications.    \nNavigat ing through these genre classification systems \nis very likely to get more complicated as we move across \ndifferent cultures . This is already happening quite fr e-\nquently with the emergence of new tools and technologies \nfor music distribution targeted for  global music market s \nand users. For instance, K-pop which used to be a rel a-\ntively obscure music categ ory outside of Korea is now \nappreciated in numerous countries around the world  due \nto the fact that users can easily access K -pop songs \nthrough media like YouTube1. As users get exposed to \nmusic from unfamiliar cultures and in unknown la n-\nguages, it will be come increasingly important to provide \nthem with intelligent access to music resources, and de-\nspite its flaws , genre  is still a widely used descriptor  for \norganizing and accessing  music.    \n3. STUDY DESIGN  \n3.1 Analysis of Korean Genre Labels  \nIn order to understand the different types of Korean music \ngenre s, we collected several hundred  genre  labels from \nthe following list of popular Korean commercial websites \n(sorted alphabetically) for music  distribution .  \n 벅스뮤직  (Bugs Music) : http://www.bugs.co.kr/  \n 싸이뮤직  (CyMusic) : http://music.cyworld.com/  \n 다음뮤직  (Daum Music) : http://music.daum.net/  \n 엠넷 (M.net) : http://www.mnet.com/  \n 멜론 (Melon) : http://www.melon.com  \n 네이버뮤직  (Naver Music): http://music.naver.com/  \n 올레뮤직  (Olleh Music) : http://www.ollehmusic.com/  \n 소리바다  (Soribada) : http://www.soribada.com/  \nWe examined the different types, organizational stru c-\ntures, and examples of music that are categorized into \neach genre categor y used in these websites.   \n3.2 User Annotation of K -pop Music  \nThree American and three Korean annotators were  hired \nto provide genre labels for the 1894 songs  found  in a co l-\nlection of K -Pop music  collected as part of a new  K-\n                                                           \n1 YouTube launches exclusive K -Pop Channel . Dec, 2011. ( http://www.  \nsoompi.com/news/youtube -launches -exclusive -kpop -channel ) MIREX ( Music Information Retrieval Evaluation e X-\nchange ) data set . There is always a resource  tradeoff be-\ntween the number of annotators and the number of songs \nto be annotated . Choosing six annotators is consistent \nwith established practice in the inter -indexer consistency \nliterature [6] while allowing us to collect labels on a wide \nvarie ty of songs . The annotators gen erated  a total set of \n11,352 song -label pairs. Annotators chose from eight \ngenre labels : Ballad , Dance/Electronic , Folk, Hip-\nhop/Rap , Rock , R&B/Soul , Trot, and Other . These genres  \nwere selected based on the analysis of  genre labels used \nin Korean music websites (Further discussed in Section \n4.1). A brief training manual providing short descriptions \nof each genre along with exemplar audio samples  was \ncreated to train the annotators . This manual was  prepared  \nin both Korea n and  English .   \n4. DATA AND DISCUSSION  \n4.1 Overview of the Korean Genre Labels  \nFrom eight  major Korean music distribution websites, we \ncollected all the genre labels u sed for organizing their \nmusic. 307 unique labels were distilled from the aggr e-\ngate collection of 602 labels a fter removing duplicate s \n(including different transliterations of the same term) . We \npresent the top -level genre categories  used in Table 1. A \nsolid dot  in Table 1  represents that the specific genre cat-\negory was used on the website. An open dot signifies that \na genre was used in conjunction with an other genre (e.g., \nJazz/Blues) . The genres  are ordered by the frequency of \noccurrences  across  multiple websites and only ones used \nin more than one website are presented in Table 1. The \ngenre labels that only appeared once at the top le vel in-\nclude: 7080 , Music used in commercials , Prenatal educ a-\ntion, Trot (Bugs); Chinese music (Melon); Country/Folk, \nEasy listening, Punk;  Reggae  (M.net);  Funk  (Soribada); \nand Rap (Cy). \nMany of the genres used in Korean Websites were the \nsame or very similar (e.g., Classic for Classical music) to \nthe ones used in North America , most likely due to the \nstrong American influence on K-pop music . For instance, \ngenres such as Hip-hop, Rap, and R&B  were introduced \nto South Korea by Korean American musicians  or st u-\ndents who studied abroad in the US  in the mid -1990s, and \nthey continue to act as leading voices in those genres t o-\nday [10]. In addition to this cultural proximity created by \ninfluences of popular culture or religion, the regional  \nproximity  also plays a role in which genres  are prom i-\nnently represented in a particular culture. This is ev i-\ndenced by J-pop which was used in 7 out of  8 websites as \na top-level  category. There are other examples: for i n-\nstance, on All Music Guide (http://www.allmusic.com) \nwhich is based in United States, Latin is one of the top \ncategor ies although it did not appear anywhere in the top \ncategories of Korea n websites. In some sense, it is natural \nfor users to gravitate toward music that is from places  \nnear them  since there is a better chance for them to be e x-\nposed to such music .   \n \n Bugs  Cy Daum  M.net  Melon  Naver  Olleh  Soribada  \nOST/O.S.T./Soundtrack  ● ● ● ● ●  ● ● \nJ-POP/Japanese music  ● ● ● ● ●  ● ● \nCCM  ● ●   ●  ● ● \nJazz ● ● ○ blues  ○ blues  ●  ● ● \nGayo  (가요 )/K-pop ●  ● ● ●  ● ● \nPop/Pop song  ●  ● ○ soul ●  ● ● \nClassic (클래식 )/Classical  ● ● ○ new age  ● ●   ● \nNew Age  ● ● ○ classic   ●   ● \nGukak  (국악 ) ●   ● ●  ●  \nReligious music    ● ● ●  ●  \nWorld music    ● ● ●   ● \nHip-hop ○ R&B  ○ rap  ●    ● \nDongyo  (동요 )/Children’s  ●   ● ●    \nElectronic/Electronica   ●  ●    ● \nRock   ●  ○ metal     ● \nR&B  ○ hip-hop ○ soul      ● \nIndie  ●       ● \nKorean music ( 국내음악 )  ●    ●   \nForeign music ( 국외음악 )  ●    ●   \nMetal     ○ rock    ● \nSoul  ○ R&B   ○ pop     \nBlues    ○ jazz ○ jazz     \nOther/Etc.    ●   ● ● ● \nTable 1. Comparison of the top-level genres of eight Korean music websites\nAnother  interesting observation of these data is that \nOST  (Original Soundtrack) is the most common ly used \ntop-level genre label . OST  is used to refer to songs that \nwere used in TV shows, dramas, movies, etc. The prev a-\nlence of this label confirms  the findings in [4] that infor-\nmation on “associated  use” (e.g., mov ie, ad) was the most \ncommonly used  feature in  music identification queries.  In \nKorea, it is fairly common for new or relatively unknown \nartists to become extremely popular “overnight ” due to \nthe exposure of their songs in TV commercials  or dramas  \n(e.g. Americano  by 10cm ; Honey Honey Baby  by Yozoh ). \nBroad genres encompassing a wid e range of Korean \nmusic styles were present in all websites. The Gayo (가요) \n(the term referring to all Korean popular music, som e-\ntimes use d interchangeably with K-pop) and  Korean m u-\nsic (국내  음악) genres contained subdivisons in five of the \nwebsites . Table 2 shows the subdivision s of the “Gayo / \nKorean Music” catego ry from the five websites. The most \ncommonly used sub-genres are Ballad, Dance, Hip -hop, \nand R ock. The sub-genres that appeared only once i n-\nclude: Club (Bugs); Jazz/Blues, Idol, 7080  (Daum); U r-\nban, and Rap (M.net).  \n Bugs  Daum  M.net  Melon  Naver  \nBallad  ○ R&B ● ● ● ● \nDance  ○ club ● ● ● ● \nHip-hop ● ● ○ rap ○ rap ● \nRock  ● ● ● ● ○ folk \nTrot  ● ●  ● \nFolk ● ● ●   \nR&B  ○ ballad  ○ soul ○ urban    \nElectronic(a)   ● ● ●  \nIndie   ● ●   \nTable 2. Subdivision of  K-pop in 5 Korean we bsites Based on our analysis, we decided to select the most \ncommonly used eight genre labels for our annotation e x-\nperiment: Ballad, Dance/Electronic, Folk, Hip -hop/Rap, \nRock, R&B/Soul, Trot,  and Other . We grouped two ge n-\nres into one label when the boundaries seem ed fuzzy. \nThis was evidenced by a number of songs being categ o-\nrized into both genres across multiple websites.  \n4.2 Results of the Annotation Experiment  \n4.2.1 Agreements among Users  \nFleiss’  kappa is a standard measure of inter -rater reliabi l-\nity when there are more th an two annotators , and when \nthe rated variable is categorical [2]. As there are three a n-\nnotators in each cultural group and the genre annotation s \nare categorical, we used Fleiss’ kappa to measure the i n-\nter-rater reliability among the annotators. The results  \nshowed that the Koreans reached a higher agreement le v-\nel (κ = 0.664) than the Americans (κ = 0.413). According \nto [2], the se κ values indicate substantial and moderate \nagreement , respectively. The agreement level among all \nsix ann otators was also moderate (κ = 0.477). This ind i-\ncates that while there exist s a common understan ding of \nK-Pop genres across cultural boundaries , it is more co n-\nsistent among Korean listeners  than American listeners. \nWhile this result is not unexpected as th e music stimuli \nwere K -Pop and the genre labels were d eveloped from \nanalyzing Korean w ebsites, it indeed demonstrates  the \nchallenge in design ing cross -cultural MIR system s using \ngenre as access points  to serve users from different cu l-\ntural backgrounds.  \nFigure 1 illustrates the number of songs that received \nthree, two , or zero “agreed ” (i.e., identical) labels  within \nthe Korean and American annotator groups . Among  the \nKoreans, 63%  (1189/1894 ) songs scored three agreed l a-  \n \nbels as opposed to 36% (688/1894) for the Americans. If \nwe loosen the agreement criterion to majority vote (at \nleast two annotators  agree ), Korean consistency was 96% \n(1820/1894) while American consistency was 87% \n(1654/1894) . As there are eight genre choi ces for each \nsong , the probabilities for annotators in each group to \nreach agreement by chance or idiosyncratic ratings are \nextremely low: (1/ 8)3 = 0.19% for unanimous agreement \nand (1/8)2 = 1.56% for agreement by two. Therefore,  \ncompari ng song s with genre  labels agreed by even a \nsmall number of annotators can still show the differences \ninfluenced by the cultural background of the annotators.  \n \nFigure 1. Number of songs that received three, \ntwo, or zero  agreed  labels  \nTo investigate possible cultural differences between \nthe Koreans and Americans , we looked at the distribution \nof songs with unanimous label ing within each annotator \ngroup.  Table 3 presents the distribution of genre labels \nfor the overlap of 505 songs unanimously labeled within \neach group. A chi -square test of independence on th ese \ndistribution s revealed  that the culture factor and genre \nfactor were inde pendent ( χ2 = 2.16, df = 6, p = 0.90). This \nindicates that cultural background did not  affect genre \njudgments for the songs with unanimous agreement .  \n Ballad  Dance  Folk Hip-\nhop R&B Rock  Trot Total  \nKO 236 93 4 25 63 53 31 505 \nAM 227 87 3 24 78 54 32 505 \nTable 3. Distribution of songs with unanimous labels  \n      However, when we consider songs with two or three \nagreed labels, the results are different. With this relaxed \ncriterion,  we can find the overlap of  1,602 songs between  \nthe Koreans and Americans . The distribution across ge n-\nres is shown in  Table 4. A chi -square test of indepen d-\nence on this distribution indicate s that culture and genre \nfactor s were  not independent ( χ2 = 56.70, df = 6, p < \n0.001). In other words, the judgments on genre were r e-\nlated to users’ cultural background.   \nThe discrepancy on these  two test results suggests that \nsome songs are “exemplary” son gs that have strong m u-\nsical characteristics representing a particular genre. These \nsongs will receive unanimous judgments regardless of \ncultural background . However, for songs with genres that \nare less obvious, cross -cultural difference is significant. \nIn other words, cultural background does seem to affect \nhow different judgments are made . This is informative  for \ndesigning cross -cultural MIR systems as in most cases genres are not highly agreed upon even within a single \nculture, due to the issues of variat ions in definition and/or \nhybrid genres ( further discussed in Section 4.3 ).  \n Ballad  Dance  Folk Hip- \nhop R&B Rock  Trot Total  \nKO 759 257 60 59 153 174 140 1602  \nAM 642 208 57 56 281 212 146 1602  \nTable 4. Distribution of songs with two or three  \nagreed labels  \n4.2.2 Disagreement  between Genres  \nWe also examine d which genres showed the highest  \ncross -cultural difference s. As there was a statistically sig-\nnificant relationship between culture and genre when \nconsidering the 1,602 songs with at least two agreed l a-\nbels, we now examine the disagreement  between each \npair of genres on this data set . In Table 5, each cell shows \nthe number of songs labeled as one genre by Korean s \n(column) and another by Americans (row). The cells on \nthe diagonal (highlighted cells) are numbers of songs \nagreed by the two groups, while other cells represent the \ndisagreement  between the two groups. The matrix shows \nasymmetric disagreements between Americans and Kor e-\nans on many  genres . \nThe bolded numbers in Table 5 indicate  major dis a-\ngreements between Korean and American annotators . The \nlargest discrepancy was the 121 songs labeled as  Ballad \nby Kore ans but R& B by Americans. Considering the two \ngroups only agreed on a total of 122 R& B songs , this  dis-\ncrepancy is very substantial. Also note that of the 57 \nsongs labeled as Folk by Americans, 18 songs were l a-\nbeled as Trot by Koreans. This m ay be due to the disa-\ngreement  resulted by same genre labels referring to di f-\nferent types of music in Western vs. Korean cultures. His-\ntorically, Ballad and F olk have evolved somewhat diffe r-\nently in K -pop culture than the Western cul ture and both \ngenres are closely associated with music from a particular \ntime period (i.e., Ballad from the 80s -90s, and Folk from \nthe 60s-80s). Therefore a song that sounds like Ballad but \nis from the 00s may not be categorized as Ballad by K o-\nreans [see Sectio n 4.3.2  for more discussion ]. \nWe also observed that a mong the 212 Rock songs a n-\nnotated by Americans, 42 (18.92%) were labeled as Ba l-\nlad by Koreans. This disagreement might be at tributed to \nthe hybrid genre Rock Ballad . Similarly, the disagre e-\nment  between B allad and Folk may be attributed to the \nhybrid genre Folk Ballad.  \n         KO \n AM Ballad  Dance  Folk Hip- \nhop R&B Rock  Trot Total  \nBallad  561 1 13 0 26 25 16 642 \nDance  7 184 1 6 2 4 4 208 \nFolk 10 0 27 0 0 2 18 57 \nHiphop  0 15 0 39 1 1 0 56 \nR&B 121 20 1 8 122 7 2 281 \nRock  42 25 6 6 0 128 5 212 \nTrot 18 12 12 0 2 7 95 146 \nTotal  759 257 60 59 153 174 140 1602  \nTable 5. Cross -tabulation  between genre annotations \n(majority votes) by Korean and American users  \n  \n \n      In summary, it is easier for Korean annotators  to \nreach an agreement on the genres of K -Pop music than \nAmerican s. Difference on genre definitions  and historical \ncontext  in the two cultures might have caused the di s-\ncrepancies  between Koreans and Americans, especially \nfor genres such as Ballad and Folk. Hybrid genres might \nalso be a reason  for disagreement . The comparison b e-\ntween annotations from two groups  confirmed that it is \nchallenging to identify genre s across cultural boundaries.  \n4.3 Discussions  \n4.3.1 Variations in Genre Criteria  \nOne reason  why genre labels can be difficult to interpret, \nunderstand, or compare  is because they are based on a \nrange of  different criteria. Among the genres we exa m-\nined, the most common defining characteris tic wa s in-\ndeed  musical style . However, this is not always the case . \nWe found that the following other dimensions were fr e-\nquently used to define genres : \n Associated use : e.g., OST  (Original Soundtrack), \nSoundtrack  (사운드트랙 ), Commercial  ads (광고뮤직 ) \n Region : e.g., J -pop (Japanese pop), Chinese music  \n(중국음악 ), World music  (월드뮤직 ) \n Purpose : e.g., Prenatal education  (태교), Ringtones  \n(벨소리 ), Meditation  (명상), MR ( Music Recorded: \nused for “Instrumental ”) \n Era: e.g., 7080 (70s and 80s music), 00 ’ Dance  \nThis issue is not unique to Korean music . Some of \nthese genre s can be observed in Western music gen re \nclassifications  as well (e.g., World music).  However, a \nnumber of examples  appear to be unique to Korean  music \n(e.g., 7080, MR) , which can pose challenges  when ma p-\nping genres across cultural boundaries . For example, t he \ngenre “ MR” could probably be mapped to “instrumental ” \nbut what  about 7080? For someone who is not familiar  \nwith the historical context  of Korean popular music, such \nlabel is essentially meaningless.  \nCateg orization of music based on regions , although \ncommonly used,  seems  especially  problematic and can \npose a problem  for mapping Korean genres to Western \ngenres . K-pop is most likely categorized under World \nmusic in North America  (if it is categorized at all)  as well \nas J-pop, etc., but clearly that will not be the case in Ko-\nrea and Japan . Also many Korean artists now release a l-\nbums in other countries making  it difficult to categorize \ntheir music based on t he region. For instance, when the \nKorean musician Boa releases albums  in Japan  sung  in \nJapanese , should that be categorized as K -pop or J -pop?  \n4.3.2 Discrepancies in Genre Usage  \nAs noted in previous research [5], in Korea, the genre l a-\nbel Pop can be used to denote any Western popular music \nfrom outside of Korea as opposed to popular music orig i-\nnated from Korea ( gayo  or K-pop). The fact that you can \nuse the label P op as a broader term that encompasses all \nWestern music and at the same time use it as a sibling  \ncategory of Rock, Electronic, etc. can lead to confusion \nas to what these labels actually refer to.   Another example of a Korean music genre with fuzzy \nbounda ries is Ballad . The popularity of Ballad peaked  in \nthe 1980s, and it is still one of the most popular genres in \nKorea  as demonstrated in Table 2. Bugs Music’s gen-\nre/style dictionary  defines Ballad as  (translated):  \nBallad is not a genre but a style. In other words, ballad is \nnot a s pecific form  of music  that exists separately , but is used \nin conjunction with other genre s to indicate a particular \nmood of the music. (e.g., Rock ballad, Pop ballad, Jazz ba l-\nlad). In popular music, when the term “Ballad music ” is \nused, it typically indicates a sentimental  song with a slow \ntempo and sad content (usually a bout a  breakup).  \nHere,  “Ballad ” is defined as a style which is dependent \non a genre , yet the term is used as a genre  in a number of \nwebsites , including Bugs  Music  itself . Websites such as \nBugs or Melon provide a list of styles to complement the \ngenre classification, but some of these styles are esse n-\ntially combinations of two genre terms (e.g., Dance Pop, \nPop Rock, Rap-Metal, Club/Dance) which can also be \nconfusing to users .  \n4.3.3 Unique  Genres  \nSeveral  genre s we observed are unique to Korean music . \nKorean Traditional music such as minyo  (민요) or pansori  \n(판소리 ) were categorized under the term gukak  (국악). \nAnother example  of a unique genre from popular music  is \nTrot which refer s to a distinctive  style of popular music \nthat does not exist in Western culture. The term  Trot  \n(트로트 ) is used to describe a “South Korean sentimental \nlove song style performed with an abundance of vocal \ninflections [12]”. This unique style of music has existed \nin Kor ea since the early 20th century and is typically e n-\njoyed by older people . Mapping this music to a Western \ngenre  based on musical characteristics is simply not po s-\nsible because there is not a good counterpart to this style \nof music.  We conjecture  that it is very likely that this  type \nof music will end up being categorized as International or \nWorld music.  However,  in other non -Western culture s, \nwe can in fact find similar styles of music  to Trot . For \ninstance, Trot and Japanese Enka music  do share  some \nmusical similarities, but in the websites we analyzed , \nthese genres  were never grouped to gether  under the same \ncategory . Although Trot is probably the most unfamiliar \ngenre of the eight genres to Americans, there were much \nmore disagreements among Bal lad, R&B, and Rock than \nTrot. When the genre is unique and not similar to any of \nthe genres listeners are accustomed to, it may be easier to \nidentify than a genre that is adopted slightly differently in \nmultiple cultures.  \n4.3.4 Composite and Hybrid Genres  \nExami ning Table 1 and 2 show s that several genres  are \noften used to gether;  for example, Jazz and Blues, Ballad  \nand R&B , Soul and Urban; Club  and Dance , Hip-hop and \nRap, etc. From an outsider perspective, some of these \ncombinations of genres may appear  baffling : what are the  \ndifference s among  R&B/ Ballad, Hip-hop/R&B and \nR&B/ Soul? Composite genres may reflect culturally si g-\nnificant distinctions . For instanc e, on Bugs music, Ba l-\nlad/R&B is a sub -genre of Gayo  (K-pop), but R&B/Soul   \n \nis a sub -genre of Pop song s (Western popular music).  \nThis is because in Korea, Ballad is generally used for any \nslow and sentimental songs , thus fitting well with R&B . \nThis may be why there was such  high disagreement  be-\ntween these genres (see Table 5).  \nSimilarly, we observed many K -pop songs  that are di f-\nficult to categorize in one genre; rather, they are better \ndescribed  as a hybrid , mixing components of  several  dif-\nferent styles of music (e.g., rap + dance + rock). This \ntrend can be explained by two reasons: the influence of \nSeo Taiji and the popularity of idol groups in Korea.   \nThe K -pop artist Seo Taiji had a major influence  on \nKorean hybrid music styles. He is considered a central K -\npop figure,  and was the first to combine elements from \nmultiple genres including Dance, Rock, Rap, Hip -hop, \nBallad, and even Korean Traditional music, gukak . His \ninfluence is still found in many K-pop songs today. In \nfact, hybridity is now regarded as one of the most signif i-\ncant aspects of contemporary K-pop culture [11].  \nAnother  reason for the hybridity of K -pop music is the \npopularity of idol groups . The “idol” culture is quite \nprominent in South Korea . Major  entertainment comp a-\nnies select young teenagers , have them go through years \nof training, and debut them in groups [3]. These groups \nare strategically created and often  consist of m embers \nwho have different strength s (e.g., singer, dancer, rapper) . \nTherefore the music they present is also designed  to inte-\ngrate and  accentuate the multiple roles and styles of the \ngroup’s members . In this way, Idol as a genre, is a hybrid \nfusion of multiple music styles and influences.   \nWebsites  Genre/Style Labels for Album Hayeoga  \nMelon  Genre: Rock ; Style: 90’s Ballad, Alternative \nRock, Rap -Metal, 90’s Dance  \nBugs  Genre: Pop; Style: Dance Pop  \nNaver  Genre: Dance, Rock  \nOlleh  Genre/Style: Gayo (Kpop)/All  \nDaum  Genre: Gayo (Kpop) > Dance  \nM.net  Gnere: Gayo (Kpop) > Dance, Rock  \nTable 6. Genr e/Style Labels assigned for Seo T ai-\nji's Album  Hayeoga  (하여가 )  \nAlthough this creative approach of mixing different \nstyles of music le d to many popular  K-pop songs, it also \nposes challenges  for classifying these songs into a parti c-\nular genre. For instance , Table 6 shows how one of Seo  \nTaiji’s albums was categorized in multiple Korean We b-\nsites.  As you can see, it is categorized in m ultiple gen-\nres/styles including Rock, Ballad, Dance, Rap -Metal, and \nK-pop. Our 6 participants annotated the genre of  the title \ntrack Hayeoga  as: Rock (3), Ballad (2), and Dance (1).  \n5. CONCLUSION AND FUTUR E WORK  \nAs we investigated genre s used in Korean music distrib u-\ntion websites  and analyzed the annotation results , we di s-\ncovered a numbe r of issues , some that are unique to K o-\nrean culture and music , and some that are common is sues \nof musical genres in any context . While the Korean we b-\nsites we examined shared many genre s with the ones used \nin Western culture , certain  labels (i.e., Pop, Ballad) refer \nto different styles of music and there were also a few unique genre s that reflect the context of K -pop culture . \nAnnotation results show more disagreement for these \ngenres as well. We also noted that genres are constructed \nbased on multiple dif ferent criteria in addition to similar i-\nties in musical styles. In the Korea n context , genres based \non associated uses, purposes, and regions seem to be i m-\nportant. Mapping genre labels from multiple cultures will \nbe challenging due to the different structur e that values \ncertain characteristic of division over the others  in add i-\ntion to genre s that are unique to particular cultures.    \nIn our future work, we plan to expand this study by \ncollecting genre s from other cultures for comparison. In \naddition, we want to conduct in -depth interviews of users \nfrom different cultures and ask how they understand, d e-\ntermine, and organize musical genres.    \n6. ACKNOWLEDGEMENTS  \nFunding support: Korean  Ministry of Trade, Industry and \nEnergy ( Grant #100372) & the A.W. Mellon Foundation.  \n7. REFERENCES  \n[1] S. Doraisamy, H. Adnan, and N. M. Norowi: “Towards \na MIR system for Malaysian music, ” Proc . of the \nISMIR , pp. 342 -343, 2006.  \n[2] K. L. Gwet: Handbook of Inter -Rater Reliability , \nGaithersburg : Advanced Analytics, LLC, 2010.  \n[3] E-Y. Jung: “Articulating  youth culture through global \npopular music styles: Seo Taiji ’s use of rap and metal, ” \nKorean Pop Music: Riding the Wave , Folkestone: \nGlobal Oriental , 2006.  \n[4] J. H. Lee and J. S. Downie : “Survey of music \ninformation needs, uses, and seeking behaviours: \npreliminary findings ,” Proc . of the ISMIR , pp. 441-446, \n2004 .  \n[5] J. H. Lee, J. S. Downie, and S. J. Cunningham : \n“Challenges  in cross -cultural/multilingual music \ninformation seeking, ” Proc . of the ISMIR , pp.1 -7, 2005.  \n[6] K. Markey: “Inter -indexer co nsistency tests,” Lib. and \nInfo. Sci. Research , Vol. 6, pp. 155 –177, 1984.  \n[7] D. McEnnis and S. J. Cunningham: “Sociology and \nmusic recommendation s ystems, ” Proc . of the ISMIR , \npp. 185 -186, 2007 . \n[8] N. M. Norowi, S. Doraisamy, and R. Wirza: “Factors \naffecting automatic genre classification: An \ninvestigation incorporating non -Western Musical form, ” \nProc . of the ISMIR , pp. 13 -20, 2005.  \n[9] F. Pachet: Content management for electronic music \ndistribution , Communications of the ACM , Vol. 46, No. \n4, pp. 71 -75, 2003.  \n[10] J-S. Park: “Korean American youth and trans -national \nflows of popular culture across the Pacific, ” Amerasia \nJournal , Vol.30, No.1, pp.147 -169, 2004.  \n[11] D. Shim: “Hybridity and the rise of Korean popular \nculture in Asia, ” Media Culture Society , Vol.28, no.1, \npp. 25 -44, 2006.  \n[12] M-J. Son: “Regulating and negotiating in T ’ûrot’û , a \nKorean popular song style, ” Asian Music , Vol.37, No.1, \npp. 51 -74, 2006.  \n[13] F. Vignoli: “Digital music interaction concepts: A user \nstudy, ” Proc . of the ISMIR , pp. 415 -421, 2004."
    },
    {
        "title": "Towards Light-Weight, Real-Time-Capable Singing Voice Detection.",
        "author": [
            "Bernhard Lehner",
            "Reinhard Sonnleitner",
            "Gerhard Widmer"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415512",
        "url": "https://doi.org/10.5281/zenodo.1415512",
        "ee": "https://zenodo.org/records/1415512/files/LehnerSW13.pdf",
        "abstract": "We present a study that indicates that singing voice detection – the problem of identifying those parts of a polyphonic audio recording where one or several persons sing(s) – can be realised with substantially fewer (and less expensive) features than used in current state-of-the-art methods. Essentially, we show that MFCCs alone, if appropriately optimised and used with a suitable classifier, are sufficient to achieve detection results that seem on par with the state of the art – at least as far as this can be ascertained by direct, fair comparisons to existing systems. To make this comparison, we select three relevant publications from the literature where publicly accessible training/test data were used, and where the experimental setup is described in enough detail for us to perform fair comparison experiments. The result of the experiments is that with our simple, optimised MFCC-based classifier we achieve at least comparable identification results, but with (in some cases much) less computational effort, and without any need for extensive lookahead, thus paving the way to on-line, real-time voice detection applications.",
        "zenodo_id": 1415512,
        "dblp_key": "conf/ismir/LehnerSW13",
        "keywords": [
            "singing voice detection",
            "polyphonic audio recording",
            "MFCCs",
            "classifier",
            "state-of-the-art methods",
            "fair comparisons",
            "publicly accessible training/test data",
            "experiments",
            "on-line",
            "real-time voice detection applications"
        ],
        "content": "TOWARDS LIGHT-WEIGHT, REAL-TIME-CAPABLE\nSINGING VOICE DETECTION\nBernhard Lehner, Reinhard Sonnleitner, Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University of Linz\nk0357205@students.jku.at, freinhard.sonnleitner,gerhard.widmer g@jku.at\nABSTRACT\nWe present a study that indicates that singing voice de-\ntection – the problem of identifying those parts of a poly-\nphonic audio recording where one or several persons sing(s)\n– can be realised with substantially fewer (and less expen-\nsive) features than used in current state-of-the-art methods.\nEssentially, we show that MFCCs alone, if appropri-\nately optimised and used with a suitable classiﬁer, are sufﬁ-\ncient to achieve detection results that seem on par with the\nstate of the art – at least as far as this can be ascertained\nby direct, fair comparisons to existing systems. To make\nthis comparison, we select three relevant publications from\nthe literature where publicly accessible training/test data\nwere used, and where the experimental setup is described\nin enough detail for us to perform fair comparison experi-\nments.\nThe result of the experiments is that with our simple,\noptimised MFCC-based classiﬁer we achieve at least com-\nparable identiﬁcation results, but with (in some cases much)\nless computational effort, and without any need for exten-\nsive lookahead, thus paving the way to on-line, real-time\nvoice detection applications.\n1. INTRODUCTION\nIdentifying the regions in a song where a singing voice is\npresent does not seem to be a difﬁcult task for humans,\nregardless of the singer’s speciﬁc voice characteristics, dy-\nnamics of articulation, instrumental background, or even\nthe language. However, the automatic classiﬁcation of vo-\ncals remains difﬁcult, to a considerable degree due to the\nextreme extent of vocal tone diversity. At the same time,\nautomatic singing voice detection would be extremely use-\nful for many applications such as audio segmentation and\nindexing, language detection, singer recognition, vocal ex-\ntraction, query-by-lyrics, real-time tracking and synchro-\nnisation, etc.\nConsequently, there has been a lot of research recently\ninto this problem. A multitude of diverse audio features\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.have been proposed, along with many (and sometimes com-\nplex) detection methods. In extensive experimental work,\nwhere we tried to reproduce some of this work and deter-\nmine what the most useful features are, we were surprised\nto ﬁnd that, when we invested a lot of effort into search-\ning for optimal parametrisations of the features, in the end\nsimple MFCCs always turned out to be at least as good as\nlarger and more complex sets of features.\nThat is the starting point for this paper, in which we\nwill demonstrate that singing voice detection of state-of-\nthe-art quality can be done in a very light-weight way, us-\ning only appropriately parametrised MFCCs. We will as-\ncertain this by comparing our very simple method to three\nselected methods from the literature where publicly acces-\nsible training/test data were used, and where the experi-\nmental setup is described in enough detail for us to perform\nfair comparison experiments.\nThe result of the experiments is that with our simple,\noptimised MFCC-based classiﬁer we achieve at least com-\nparable identiﬁcation results, but with (in some cases much)\nless computational effort, and without any need for exten-\nsive lookahead, thus paving the way to on-line, real-time\nvoice detection applications.\nThe paper is structured as follows: Section 2 gives an\noverview of previous work on singing voice detection and\non publicly available corpora for this task, and motivates\nthe choice of state-of-the-art methods that we choose to\ncompare our method to. Section 3 presents our own simple\nmethod and describes how we experimentally optimised\nthe MFCC and classiﬁer parameters. Section 4 compares\nour method to the other methods selected above, and Sec-\ntion 5 outlines what we consider to be the most important\ndirection for further improvement.\n2. PREVIOUS WORK AND A V AILABLE DATA\nWe start with a brief overview of selected existing meth-\nods, and of publicly available data corpora. Finally, we\nidentify the methods we chose to compare our results to,\nalong with the reasons why we chose those methods.\n2.1 Problem statement\nThe signal consists only of music and the problem is to\ndetect the presence of singing voice therein. Hence, no\ndiscrimination of normal speech and singing voice is done,in contrast to the speech/music discrimination task done by\nChou and Gu in [4].\n2.2 Previous Work on Singing Voice Detection\nIn [21], Rocamora and Herrera compared MFCCs, Percep-\ntually derived LPC (PLPC), Log Frequency Power Coef-\nﬁcients (LFPC) and Harmonic Coefﬁcient (HC). Spectral\nfeatures often utilised for instruments classiﬁcation were\nalso combined into one vector, namely Centroid, Roll-off,\nFlux, Skewness, Kurtosis, and Flatness. Additionally, pitch\nas the only non-spectral feature, extracted with the mono-\nphonic f0-estimator YIN [5] was used. Several empiri-\ncally motivated post-processing strategies were also imple-\nmented. In their setting, MFCCs are the most suitable fea-\ntures, and an SVM the best performing classiﬁer for sing-\ning voice detection. Although they tried to combine dif-\nferent descriptors, this did not improve the classiﬁcation\nperformance of 78.5% accuracy on a test set of 46 man-\nually annotated songs. Since this material is not publicly\navailable, it is not possible to conduct a fair comparison.\nIn [16], HA-LFPCs (harmonic attenuated LFPC) are\nproposed for singing voice detection. For the construc-\ntion of the attenuation ﬁlter, the key determination tech-\nnique from [23] is used. Compared to MFCCs, the LFPCs\nperformed better, with a Multi-Model HMM (taking song\nstructure into account) as classiﬁer (86.7% vs. 81.3% ac-\ncuracy). The experiments were carried out on 20 unknown\npopular songs from different artists and time spans, with\nsix songs used for training and the remaining 14 songs for\ntesting. Since this material is not publicly available, it is\nnot possible to conduct a fair comparison.\nLi and Wang [14] used a singing voice detection stage\nfor the separation of singing voice from instrumental ac-\ncompaniment. They used MFCCs, LPCs, PLPs, and the\n4-Hz harmonic coefﬁcient as features. A HMM and the\nViterbi algorithm [18] were used to classify clips from ﬁve\nrock and ﬁve country songs via a 10-fold CV . Again, this\nmusical material is not publicly available.\nRegnier and Peeters’ method [20] involves thresholds\nfor vibrato and tremolo to detect singing voice. They\nreached an f-value of 76.8% for singing voice compared\nto a (more sophisticated) machine learning approach us-\ning MFCCs, SFM, and their ﬁrst and second derivatives\nfor both features and a GMM as classiﬁer, which yielded\n77.4% f-measure. The experiments were conducted on the\nJamendo corpus [19], with 63 songs as training set, and 32\nsongs as the test set. Since the results reported are not as\ngood as those reported in [19], which were also obtained\non the Jamendo corpus, we will compare our method to the\nlatter.\nHsu and Jang [11] used GMMs as states in a fully con-\nnected HMM and the Viterbi algorithm [18] to decode mu-\nsic signals into the three classes accompaniment, unvoiced\nandvoiced . They achieved an accuracy of 77.95% with a\n39-dimensional feature vector containing 12 MFCCs, the\nlog energy, and their ﬁrst and second derivatives. They\nused all 1000 clips of the MIR-1K data set [11] for train-\ning and evaluating, divided into two subsets of similar sizes(487 versus 513, recorded by disjoint subjects) for a 2-fold\nCV . Since the precise split of the data set is not publicly\navailable, it is not possible to conduct a fair comparison.\nIn [12], Hsu et al. used basically the same setting as\nin [11], except that they used Harmonic/percussive source\nseparation (HPSS) as a preprocessing step. Additionally,\ntheir HMM decodes into just vocal and nonvocal sections.\nThe usage of the preprocessed signal showed signiﬁcant\nimprovement compared to the raw signal, especially in low-\ner SNR levels. For an -5, 0, and 5 dB SNR they reached\naccuracies of \u001880%,\u001885%, and \u001889% respectively.\nThe three methods of Vembu and Baumann [25], Mauch\net al. [15] and Ramona et al. [19], to which we will com-\npare our own method, will be described in more detail in\n3.4, 4.2, and 4.1 respectively. The reasons why we chose\nthem are explained in Section 2.4.\n2.3 Publicly Available Corpora\nAs outlined above, numerous very different approaches\nhave been taken to the problem of singing voice detection.\nUnfortunately, due to a lack of commonly used corpora, the\nresults are often not directly comparable. To our knowl-\nedge, there are three corpora along with vocal activity an-\nnotations publicly available:\n1.RWC Music Database: Popular Music (RWC-\nMDB-P-2001): 100 songs released by Goto et al.\n[8], with singing voice annotations provided by\nMauch et al. [15]. Along with the annotations a\nnovel method was introduced which will be described\nin more detail in Section 4.2.\n2.Jamendo Corpus: 93 copyright-free songs from the\nJamendo music sharing website [9], collected and\nannotated by Ramona et al. [19]. Also used for sing-\ning voice detection by Regnier and Peeters in [20].\n3.MIR-1K Corpus: 1000 song clips taken from 110\nkaraoke songs, and released by Hsu and Jang in [11].\nThe songs were recorded in their lab, and sung by 8\nfemales and 11 males. Also used for singing voice\ndetection by Hsu et al. in [12].\n2.4 Algorithms Selected for Comparison\nEventually, we selected three different methods as bench-\nmarks for our proposed method.\n2.4.1 Vembu and Baumann [25]\nThis method is relatively simple and achieved remarkable\nresults on an unknown test set. The method is described\nin such detail that it can be re-implemented. It will be\ndescribed in more detail in subsection 3.4. Thus, our re-\nimplementation of it will be used as a “baseline” to com-\npare our simple optimised MFCC-only classiﬁer to (see\nSection 3.4). We will show that MFCCs alone can achieve\nbetter performance, if appropriately parametrised.2.4.2 Ramona et al. [19]\nFor this method, a very large and diverse set of features\nis used. They precisely report (also song-wise) results on\na publicly available corpus (Jamendo) with exact infor-\nmation regarding which pieces were used for training and\nevaluation. This allows for the most fair and precise com-\nparison. The method will be described in more detail in\nsubsection 4.1.\n2.4.3 Mauch et al. [15]\nThis is one of the most recent publications on this topic.\nThe authors report excellent results on (parts of) the pub-\nlicly available RWC corpus, with a rather complex pro-\ncedure. This method will be described in more detail in\nsubsection 4.2.\n3. A SIMPLE MFCC-BASED RECOGNISER\nIn this section, we describe how we designed and opti-\nmised our simple singing voice detector using solely MFCC\nfeatures. The optimisation was done in three phases, and\nwe will show, for each phase, how the improvements com-\npare to our “baseline” algorithm from Vembu and Bau-\nmann [25]. This will give a ﬁrst impression of results\nachievable with MFCCs alone.\n3.1 Classiﬁcation Setting and Basic Features\nOur classiﬁcation setting is as follows. The units of audio\nto be classiﬁed are frames of 200 ms duration. Thus, we\nhave 5 classiﬁcations per second of audio. The actual win-\ndow over which the features for classifying a frame Fare\ncomputed, may be larger than that. We will call this the ob-\nservation window . It will always be placed symmetrically\naround a classiﬁcation frame F. We use the VOICEBOX\ntoolbox [3] to extract a D-dimensional MFCC feature vec-\ntor from the observation window. As the standard param-\neters we used 30 triangle shaped ﬁlters and extracted 13\ncoefﬁcients, without the 0th coefﬁcient.\nFor the parameter optimisation phase, we used a set of\n75 annotated songs by 75 different artists, which come\nfrom a different source than the corpora we will use for\nthe comparison experiments below. All songs were uni-\nﬁed, i.e. downsampled to 22kHz and converted to mono.\nApproximately 52% of the frames are annotated as vocal,\nand the amount of pure singing, i.e. without instrumental\naccompaniment, is negligible. All optimisation decisions\nin the following are based on 15-fold cross validation (CV)\nexperiments, where the data set was randomly split into 15\nsubsets of 5 songs each.\n3.2 Classiﬁer and Post-Processing\nAmong the most popular classiﬁers for singing voice de-\ntection are Gaussian Mixture Models (GMM), Support Vec-\ntor Machines (SVM) and Multi-layer perceptron neural net-\nworks (MLP). Random forests [2] have proven to deliver\ngood results in other contexts, for instance in speech detec-\ntion in [24] or music detection in [22]. Compared to SVMsand MLPs they perform much faster in both the training\nand testing phase. Also, there is no need to determine an\nappropriate kernel function in order to obtain the best per-\nformance. Thus, we chose to use a random forest as clas-\nsiﬁer, and the implementation from WEKA [10].\nFor post-processing (smoothing of the prediction se-\nquence), a simple median ﬁlter with a window length of\nseven frames (1.4s) was found to give the best trade-off\nbetween complexity and accuracy.\n3.3 Optimising the MFCC Features\nIn this subsection we describe the task of the parameter\noptimisation, which was done in three phases:\n3.3.1 Phase I:\nThe number of coefﬁcients as well as the size of the ﬁlter-\nbank was optimised. The best results were achieved with\n30 coefﬁcients (including the 0th) and a ﬁlterbank with 30\ntriangular shaped ﬁlters.\n3.3.2 Phase II:\nThe length of the observation window was optimised. In\nall cases, the observation window, when it was larger than\nthe current classiﬁcation frame, was placed symmetrically\naround the frame. Best results were achieved with a total\nwindow of 800ms around the 200ms center frame. The rel-\natively long observation window has the effect that the con-\ntribution of percussive components (which are only active\nfor a short period of time) to the spectrum is diminished.\nBefore the last phase was conducted, the ﬁrst derivatives\n(deltas) of the MFCCs were also added to the feature vec-\ntor.\n3.3.3 Phase III:\nThe parameters of the classiﬁer were optimised. A good\ntrade-off between computational complexity and the re-\nsults was found to be a random forest with 128 trees, each\nwith ﬁve attributes. Additionally, the threshold for the vo-\ncal class was raised to 55% to reduce some of the false\npositives.\n3.4 Comparison to Vembu and Baumann\nVembu and Baumann [25] extracted 13 MFCCs, 39 PLPs,\nand 12 LFPCs, resulting in a feature vector with 64 el-\nements. They achieved the best results with all features\ncombined and using a SVM as classiﬁer (93.47% accuracy\non an unknown data set). Additionally, they provide the pa-\nrameters of the optimised RBF kernel they used ( C= 28,\n\u001b= 22). They do not mention any post-processing. Since\nthere is enough information available to implement this\nmethod, it will be used as a baseline for further compari-\nson. Although theirs is an online algorithm, it turns out that\ntraining and testing is extremely time consuming (about 75\ntimes slower than with our optimised random forest).\nTable 1 shows the results of the 15-fold CV on our data\nset (see Section 3.1), at different stages of the optimisationMFCC I II PROP VB\nacc [%] 69.14 74.51 78.74 82.36 77.16\nrecall 0.727 0.783 0.834 0.883 0.819\nprecision 0.712 0.757 0.788 0.810 0.774\nf-measure 0.719 0.770 0.810 0.845 0.796\nTable 1 . Results of the parameter optimisation compared\nto the baseline method VB. The columns are as follows:\nMFCC: “standard” (unoptimised) MFCCs. I: after opti-\nmisation stage I (number of MFCCs; ﬁlterbank). II: after\noptimisation stage II (observation window; delta MFCCs).\nPROP: proposed method after optimisation III (optimised\nrandom forest; median ﬁlter). VB: Vembu & Baumann\ntrained and tested in the same way. Recall, precision, and\nf-measure relate to our class of interest, vocals .\nprocedure, along with the results of the VB method (com-\nputed on exactly the same data splits). To illustrate the ef-\nfectiveness of every optimisation stage, we begin with the\nresults achieved with standard MFCCs (see Section 3.1)\nand a standard random forest classiﬁer (column MFCC).\nAfter the ﬁrst optimisation stage (I), where we use 30 co-\nefﬁcients instead of just 13, there is an improvement in\naccuracy of more than 5 percentage points. The second\noptimisation regarding the observation time (II) yields an\nimprovement of almost 10 points compared to the standard\nMFCCs, and already better results than the baseline algo-\nrithm of Vembu and Baumann [25] (VB). The ﬁnal pro-\nposed method (PROP.) with the optimised random forest\nclassiﬁer and median-ﬁlter post-processing, reaches an ac-\ncuracy 13 percentage points higher than standard MFCCs,\nand more than 5 points better than the VB method.\n4. COMPARISON TO STATE-OF-THE-ART\nMETHODS\nIn this section our proposed simple method is compared to\ntwo other methods. In Section 2.4 we already explained\nthe motivation behind the selection of the algorithms; now\nwe explain them in more detail.\nTo recapitulate, our proposed method uses just the opti-\nmised long-term MFCCs (800ms, 30 coefﬁcients incl. the\n0th, and a ﬁlterbank with 30 triangular shaped ﬁlters) along\nwith their ﬁrst derivatives. There is no pre-processing in-\nvolved, and a simple median ﬁlter over seven frames\n(1.4sec) is used to smooth out the predictions of a random\nforest classiﬁer (128 trees with ﬁve features each).\n4.1 Ramona et al.\nRamona et al. [19] use an SVM classiﬁer with a com-\nbination of the most diverse set of features compared to\nthe other methods discussed in this paper. These include\nMFCCs, LPCs, ZCR, sharpness, spread, f0 and aperiod-\nicity measure extracted with the monophonic YIN library\n[5]. Furthermore, short-scale frames contain spectral de-\nscriptors like centroid, width, asymmetry, slope, decreas-\ning, ﬂux, and similar temporal statistical moments. Ad-Ramona et al. PROP VB\nAcc% F% Acc% F% Acc% F%\n03 - Say me Good Bye 80.1 85.8 91.4 83.6 90.6 82.7\n03 - School 84.3 87.3 84.8 86.6 71.5 77.8\n03 - Si Dieu 76.4 80.7 87.2 89.4 76.2 66.7\n03 - Une charogne 85.3 91.7 89.8 93.5 78.8 85.9\n03 - castaway 79.0 87.3 71.3 80.5 73.0 80.0\n04 - Believe 80.0 88.5 94.1 95.6 83.0 87.2\n04 - Healing Luna 85.5 81.6 87.8 84.4 72.7 70.8\n04 - Inside 83.3 68.2 79.4 66.0 75.3 58.0\n04 - You are 87.0 91.9 87.9 90.6 74.4 77.7\n05 - 05 LIrlandaise 57.7 64.2 65.0 68.6 61.7 60.5\n05 - 16 ans 91.5 84.8 87.3 79.8 70.8 60.3\n05 - 2003-Circons[. . . ] 87.6 88.2 75.5 77.7 79.8 79.6\n05 - A Poings Fermes 93.7 92.2 89.7 83.0 86.9 81.2\n05 - Crepuscule 85.2 88.8 80.1 83.6 76.8 80.0\n05 - Dance 77.0 83.2 84.1 88.7 75.7 82.2\n05 - Elles disent 71.8 78.7 84.4 87.4 69.6 77.0\nALL 82.2 84.3 84.8 84.6 77.4 76.9\nTable 2 . Results of the proposed method on the Jamendo\ncorpus, compared to those of Ramona et al. in [19] and\nVembu and Baumann’s method trained and tested in the\nsame way.\nditionally, long-scale frames contain features that do not\nrepresent an instantaneous characteristic. Those include\n(again) the ZCR, tremolo and granularity for the frequen-\ncies 4-8Hz and 10-40Hz, and some temporal statistical mo-\nments. Those features add up to a vector with 116 compo-\nnents.\nAfterwards, the dimensionality is reduced to d=40 with\nthe IRMFSP algorithm [17], leaving only the most dis-\ncriminating features. A silence detection is applied as a\npre-processing step. Finally, a HMM and the Viterbi al-\ngorithm are used for post-processing the SVM output, and\ninstrumental segments shorter than 0.5s are discarded.\nThe authors report 82.2% accuracy on a precisely de-\nscribed split of the Jamendo corpus, with a training set\nconsisting of 63 given songs, and validation and test sets\nof 16 songs each. Thus, a fair comparison of the results is\npossible.\nIn Table 2, the results of Ramona et al. are compared\nto those of the proposed method. All in all, better results\nregarding both accuracy and f-measure are achieved with\nthe proposed method (82.2% vs. 84.8% accuracy). Never-\ntheless, there are some songs that get better classiﬁed with\nthe method of Ramona et al.; the biggest difference is with\nthe song 05 - 2003-Circons[. . . ] (87.6% vs. 75.5% accu-\nracy). It would be interesting to have a feature with which\nwe could determine the better suited method for a speciﬁc\nsong, or even a shorter segment.\n4.2 Mauch et al.\nMauch et al. [15] utilise four features in total, among them\nMFCCs. Additionally, they use Goto’s polyphonic funda-\nmental frequency(f0)-estimator PreFEst [7] to isolate the\npredominant melody. They propose three novel features\nwhich are based on it:\nPitch ﬂuctuation , which is basically the frame-wise stan-\ndard deviation of intra-semitone f0 differences. First, the\nestimated f0 is mapped to pitch space. Afterwards, theseestimations are shifted based on a song-wide inferred tun-\ning. As a last step, the frequency differences are calculated,\nand the frame-wise Hamming-weighted standard deviation\nof those differences yields the pitch ﬂuctuation. Since a\nsong-wide lookahead is necessary to infer its tuning, this\nmethod is not an online algorithm. Pitch ﬂuctuation is\nfound to be the most salient feature for singing voice de-\ntection.\nIn addition to the MFCCs of the signal as it is, the au-\nthors also introduce MFCCs of the re-synthesised predom-\ninant voice to capture its timbre. The re-synthesis employs\nsinusoidal modelling based on the predominant melody as\nwell as the estimated amplitudes of its harmonics as de-\nscribed in [6].\nThenormalised amplitude of harmonic partials is also\nextracted from the predominant voice, and is considered\nto add information on another dimension of timbre which\nis not provided by MFCCs. It is a vector-shaped feature\n(d=12), and calculated by normalising the estimated har-\nmonic amplitudes according to the Euclidean norm.\nA SVM-HMM [1,13] is used as classiﬁer. Additionally,\nsegments shorter than 0.5s are merged with the preceding\nregions.\nThe best result (87.2% accuracy) was achieved with all\nfour features combined, employing a 5-fold CV on a 102\nsong data set that is composed of 90 songs from the RWC\nmusic database [8] (exactly which 90 of the 100 is un-\nknown to us and could, unfortunately, not be found out),\nand 12 additional (also unknown) songs. Since we had\nonly access to the 100 song RWC music database, our re-\nsults are only comparable to a certain extent. Nevertheless,\nto allow for the best comparison possible, we matched their\ndecision frequency of one feature instance every 100ms\nand utilised a 5-fold CV .\nIn Table 3, the results of Mauch et al. are compared\nto those of the proposed method. To illustrate the differ-\nence of the data set used by Mauch et al. to the original\nRWC data set, we also give the results achieved with stan-\ndard MFCCs (see Section 3.1). Additionally, to reveal the\namount of vocals, we give the results of the mode, i.e. the\nproportion of the majority class (which is vocals ). As can\nbe seen, there is a difference of 5 percentage points regard-\ning the vocal content, which indicates a limited compara-\nbility.\nAll in all, our proposed method performs not much worse\nthan the method of Mauch et al. (85.9% vs. 87.2% accu-\nracy). This difference virtually disappears when we apply a\nmore complex post-processing strategy involving a HMM\nand the Viterbi algorithm (row PROP+).\n5. CONCLUSION AND FUTURE WORK\nThis paper has proposed an extremely simple method to\ndetect the presence of singing voice in mixed audio sig-\nnals. By comparing the results to those of three well se-\nlected algorithms, we could show that regarding the fea-\ntures, appropriately parametrised MFCCs along with their\nﬁrst derivatives are sufﬁcient to achieve results as good as\nthose of sometimes much more complicated state-of-the-Mauch accuracy precision recall f-measure\nMODE 0.654 0.654 1.000 0.791\nMFCC 0.738 0.739 0.926 0.822\nFMRH 0.872 0.887 0.921 0.904\nProposed accuracy precision recall f-measure\nMODE 0.604 0.604 1.000 0.753\nMFCC 0.718 0.764 0.771 0.767\nVB 0.813 0.827 0.808 0.818\nPROP 0.859 0.858 0.918 0.887\nPROP+ 0.868 0.879 0.906 0.892\nTable 3 . The results of our proposed method compared\nto the methods of Mauch et al. and Vembu and Baumann.\nAlong with the methods the class distribution in the respec-\ntive test set is given (row MODE – the overall proportion\nof vocals), as well as the results achieved with the standard\nMFCCs, and Vembu and Baumann’s Method (row VB).\nClearly, even though the majority of the data we used is\nthe same as used by Mauch et al., there are differences\nregarding the content of vocals, which makes a fair com-\nparison unfeasible. The results PROP+ are achieved with\na post-processing involving the Viterbi algorithm.\nart systems. Our method is simple, fast, and requires no\nlook-ahead, making it a good candidate for on-line, real-\ntime singing voice detection applications.\nOur main goal for further improvement is precision , that\nis, a reduction of the number of false positives . A detailed\ninspection of the results of our classiﬁer has shown that\ninstruments mistaken for vocals have the biggest negative\nimpact on the results. This is especially true for string in-\nstruments like electric guitars, which can mimic the tem-\nporal as well as the timbral characteristics of vocals. Cer-\ntain effects commonly used to enhance the sound or ex-\ntend the expressiveness of guitars like chorus, ﬂanger, and\nwah-wah are responsible for this. Unfortunately, experi-\nments reported in [21] indicate that features often utilised\nfor speech/music discrimination like harmonic coefﬁcient\n[4] are not able to distinguish between highly harmonic in-\nstruments and vocals. Therefore, it would be beneﬁcial to\ndevelop a method that is less sensitive to differences be-\ntween singers’ speciﬁc voice characteristics, while main-\ntaining good discriminative properties for instruments that\nresemble vocals.\n6. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Fund\nFWF under grants TRP307-N23 and Z159 (Wittgenstein\nAward).\n7. REFERENCES\n[1] Y . Altun, I. Tsochantaridis, T. Hofmann, et al. ”Hidden\nMarkov support vector machines”. In Proceedings of the\n20th International Conference on Machine Learning (ICML\n2003) , volume 20, 2003.[2] L. Breiman. ”Random forests”. Machine learning , 45(1):5–\n32, 2001.\n[3] M. Brookes. ”V oicebox: Speech Processing Tool-\nbox for Matlab”. Website, 1999. Available online at\nhttp://www.ee.ic.ac.uk/hp/staff/dmb/\nvoicebox/voicebox.html ; visited on March 4th\n2013.\n[4] W. Chou and L. Gu. ”Robust singing detection in\nspeech/music discriminator design”. In Proceedings of the\n2001 IEEE International Conference on Acoustics, Speech,\nand Signal Processing, ICASSP 2001 , volume 2, pages 865–\n868. IEEE, 2001.\n[5] A. De Cheveign ´e and H. Kawahara. ”YIN, a fundamental fre-\nquency estimator for speech and music”. The Journal of the\nAcoustical Society of America , 111:1917–1930, 2002.\n[6] H. Fujihara, M. Goto, J. Ogata, K. Komatani, T. Ogata, and\nH. G. Okuno. ”Automatic synchronization between lyrics and\nmusic CD recordings based on Viterbi alignment of segre-\ngated vocal signals”. In Proceedings of the Eighth IEEE Inter-\nnational Symposium on Multimedia, ISM 2006 , pages 257–\n264. IEEE, 2006.\n[7] M. Goto. ”A real-time music-scene-description system:\nPredominant-F0 estimation for detecting melody and bass\nlines in real-world audio signals”. Speech Communication ,\n43(4):311–329, 2004.\n[8] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\n”RWC music database: Popular, classical, and jazz music\ndatabases”. In Proceedings of the 3rd International Con-\nference on Music Information Retrieval (ISMIR 2002) , vol-\nume 2, pages 287–288, 2002.\n[9] P. Grard, L. Kratz, and S. Zimmer. ”Jamendo, open your\nears”. Website, 2005. Available online at http://www.\njamendo.com ; visited on March 18th 2013.\n[10] M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reutemann,\nand I. H. Witten. ”The WEKA data mining software: an up-\ndate”. ACM SIGKDD Explorations Newsletter , 11(1):10–18,\n2009.\n[11] C-L. Hsu and J-S. R. Jang. ”On the improvement of singing\nvoice separation for monaural recordings using the MIR-1K\ndataset”. IEEE Transactions on Audio, Speech, and Language\nProcessing , 18(2):310–319, 2010.\n[12] C-L. Hsu, DL. Wang, J-S. R. Jang, and K. Hu. ”A Tandem\nAlgorithm for Singing Pitch Extraction and V oice Separation\nFrom Music Accompaniment”. IEEE Transactions on Audio,\nSpeech, and Language Processing , 20(5):1482–1491, 2012.\n[13] T. Joachims, T. Finley, and C. J. Yu. ”Cutting-plane training\nof structural SVMs”. Machine Learning , 77(1):27–59, 2009.\n[14] Y . Li and DL. Wang. ”Separation of singing voice from\nmusic accompaniment for monaural recordings”. IEEE\nTransactions on Audio, Speech, and Language Processing ,\n15(4):1475–1487, 2007.\n[15] M. Mauch, H. Fujihara, K. Yoshii, and M. Goto. ”Timbre\nand Melody Features for the Recognition of V ocal Activity\nand Instrumental Solos in Polyphonic Music”. In Proceedings\nof the 12th International Conference on Music Information\nRetrieval (ISMIR 2011) , pages 233–238, 2011.\n[16] T. L. Nwe, A. Shenoy, and Y . Wang. ”Singing voice detection\nin popular music”. In Proceedings of the 12th annual ACM in-\nternational conference on Multimedia , pages 324–327. ACM,\n2004.[17] G. Peeters. ”Automatic Classiﬁcation of Large Musical In-\nstrument Databases Using Hierarchical Classiﬁers with Iner-\ntia Ratio Maximization”. In 115th AES Convention , 2003.\n[18] L. R. Rabiner. ”A tutorial on hidden Markov models and se-\nlected applications in speech recognition”. Proceedings of the\nIEEE , 77(2):257–286, 1989.\n[19] M. Ramona, G. Richard, and B. David. ”V ocal detection in\nmusic with support vector machines”. In Proceedings of the\n2008 IEEE International Conference on Acoustics, Speech,\nand Signal Processing, ICASSP 2008 , pages 1885–1888.\nIEEE, 2008.\n[20] L. Regnier and G. Peeters. ”Singing voice detection in mu-\nsic tracks using direct voice vibrato detection”. In Proceed-\nings of the 2009 IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, ICASSP 2009 , pages 1685–\n1688. IEEE, 2009.\n[21] M. Rocamora and P. Herrera. ”Comparing audio descriptors\nfor singing voice detection in music audio ﬁles”. In Brazil-\nian Symposium on Computer Music, 11th. San Pablo, Brazil ,\nvolume 26, page 27, 2007.\n[22] K. Seyerlehner, T. Pohle, M. Schedl, and G. Widmer. ”Auto-\nmatic music detection in television productions”. In Proceed-\nings of the 10th International Conference on Digital Audio\nEffects (DAFx’07) , 2007.\n[23] A. Shenoy, R. Mohapatra, and Y . Wang. ”Key determination\nof acoustic musical signals”. In Proceedings of the 2004 IEEE\nConference on Multimedia and Expo, ICME 2004 , volume 3,\npages 1771–1774. IEEE, 2004.\n[24] R. Sonnleitner, B. Niedermayer, G. Widmer, and J. Schl ¨uter.\n”A Simple And Effective Spectral Feature For Speech Detec-\ntion In Mixed Audio Signals”. In Proceedings of the 15th In-\nternational Conference on Digital Audio Effects (DAFx’12) ,\n2012.\n[25] S. Vembu and S. Baumann. ”Separation of vocals from poly-\nphonic audio recordings”. In Proceedings of the 6th Inter-\nnational Conference on Music Information Retrieval (ISMIR\n2005) , volume 5, pages 337–344, 2005."
    },
    {
        "title": "Beta Process Sparse Nonnegative Matrix Factorization for Music.",
        "author": [
            "Dawen Liang",
            "Matthew D. Hoffman",
            "Daniel P. W. Ellis"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416206",
        "url": "https://doi.org/10.5281/zenodo.1416206",
        "ee": "https://zenodo.org/records/1416206/files/LiangHE13.pdf",
        "abstract": "Nonnegative matrix factorization (NMF) has been widely used for discovering physically meaningful latent components in audio signals to facilitate source separation. Most of the existing NMF algorithms require that the number of latent components is provided a priori, which is not always possible. In this paper, we leverage developments from the Bayesian nonparametrics and compressive sensing literature to propose a probabilistic Beta Process Sparse NMF (BP-NMF) model, which can automatically infer the proper number of latent components based on the data. Unlike previous models, BP-NMF explicitly assumes that these latent components are often completely silent. We derive a novel mean-field variational inference algorithm for this nonconjugate model and evaluate it on both synthetic data and real recordings on various tasks.",
        "zenodo_id": 1416206,
        "dblp_key": "conf/ismir/LiangHE13",
        "keywords": [
            "Nonnegative matrix factorization",
            "latent components",
            "audio signals",
            "source separation",
            "Bayesian nonparametrics",
            "compressive sensing",
            "probabilistic Beta Process Sparse NMF",
            "automatic inference",
            "proper number of latent components",
            "completely silent"
        ],
        "content": "BETA PROCESS SPARSE NONNEGATIVE MATRIX\nFACTORIZATION FOR MUSIC\nDawen Liang\nLabROSA, EE Dept.\nColumbia University\ndl2771@columbia.eduMatthew D. Hoffman\nAdobe Research\nAdobe Systems Incorporated\nmathoffm@adobe.comDaniel P. W. Ellis\nLabROSA, EE Dept.\nColumbia University\ndpwe@ee.columbia.edu\nABSTRACT\nNonnegative matrix factorization (NMF) has been widely\nused for discovering physically meaningful latent compo-\nnents in audio signals to facilitate source separation. Most\nof the existing NMF algorithms require that the number of\nlatent components is provided a priori , which is not always\npossible. In this paper, we leverage developments from the\nBayesian nonparametrics and compressive sensing litera-\nture to propose a probabilistic Beta Process Sparse NMF\n(BP-NMF) model, which can automatically infer the proper\nnumber of latent components based on the data. Unlike\nprevious models, BP-NMF explicitly assumes that these\nlatent components are often completely silent. We derive\na novel mean-ﬁeld variational inference algorithm for this\nnonconjugate model and evaluate it on both synthetic data\nand real recordings on various tasks.\n1. INTRODUCTION\nNonnegative matrix factorization (NMF) [9] has been ex-\ntensively applied to analyze audio signals, since the ap-\nproximate decomposition of the audio spectrogram into the\nproduct of 2 nonnegative matrices X\u0019WH provides a\nphysically meaningful interpretation. We can view each\ncolumn of X, which represents the power density across\nfrequencies at a particular time, as a nonnegative linear\ncombination of the columns of W, determined by the col-\numn of activation H. Thus Wcan be considered as a dic-\ntionary, where each column acts as a component. This can\nbe particularly useful for audio source separation, where\nthe goal is to ﬁnd out the individual sources from mixed\nsignal.\nAudio source separation poses a meaningful and chal-\nlenging problem, which has been actively studied for the\nlast few decades. One of the obstacles which makes source\nseparation difﬁcult is that the number of sources is gener-\nally not known. For example, when we listen to a piece of\npolyphonic music, it is difﬁcult and tedious to ﬁgure out\nhow many notes or instruments are being played. How-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.ever, most existing NMF algorithms require the number of\ncomponents to be provided as input, based on the assump-\ntion that there exists a certain mapping between the learned\ncomponents and real sources. To address this issue, we\npropose BP-NMF, a nonparametric Bayesian NMF model\nthat uses a beta process prior. The model automatically\ndetermines how many sources it needs to explain the data\nduring posterior inference.\n1.1 Related Work\nNMF has been applied to many music analysis problems\nsuch as music transcription [1,12], music analysis [5], and\nmusic source separation [10, 15].\nOn the other hand, most of the literature on nonpara-\nmetric Bayesian latent factor models focuses on conjugate\nlinear Gaussian models, for example, beta process factor\nanalysis [11] which is the main inspiration for BP-NMF.\nHowever, such models are not appropriate for audio spec-\ntrograms as they do not impose nonnegativity constraints.\nTo address this limitation, [7] proposed a nonparametric\nBayesian NMF model based on the gamma process.\nBP-NMF extends the standard NMF model in two ways:\n\u000fBP-NMF can explicitly and completely silence la-\ntent components when they should not be active. This\ncaptures the intuition that a note which appears fre-\nquently during one phrase may not contribute any-\nthing in another phrase, and most notes are silent\nmost of the time.\n\u000fThe number of latent components, which is difﬁcult\nto set a priori , is inferred by the model.\nBoth of these issues have been addressed in previous work,\nbut to the authors’ knowledge, BP-NMF is the ﬁrst model\nto combine them.\n2. BP-NMF\nWe adopt the notational conventions that upper case bold\nletters (e.g. X;D,SandZ) denote matrices and lower\ncase bold letters (e.g. x,ds, andz) denote vectors. f2\nf1;2;\u0001\u0001\u0001;Fgis used to index frequency. t2f1;2;\u0001\u0001\u0001;Tg\nis used to index time. k2f1;2;\u0001\u0001\u0001;Kgis used to index\ndictionary components.\nBP-NMF is formulated as:\nX=D(S\fZ) +E (1)where Xis aF\u0002Tspectrogram and Dis aF\u0002Kdictio-\nnary withKcomponents. The activation is the Hadamard\nproduct between a nonnegative matrix Sand a binary mask\nZ, both of which have the shape of K\u0002T.Eis an i.i.d.\nGaussian noise matrix which has the same shape as X. We\nuse Gaussian noise model instead of Poisson or exponen-\ntial mainly for mathematical convenience and extending\nBP-NMF to more audio-oriented model is part of the future\nwork. Unlike previous models, BP-NMF can explicitly si-\nlence some components by turning off the corresponding\nelements in Z. For example, when a clarinet is playing A3\nthe model should silence all clarinet notes that are not A3.\nWe place a beta process [6] prior on the binary mask Z\nso that the number of components Kcan potentially go to\ninﬁnity and the inference algorithm will choose the proper\nnumber to describe the data. We adopt the ﬁnite approxi-\nmation to a beta process in [11]:\nZkt\u0018Bernoulli (\u0019k)\n\u0019k\u0018Beta(a0=K;b 0(K\u00001)=K)(2)\nwhereKis set to a large number. As shown in [4], a ﬁnite\napproximation for Indian buffet process1performs compa-\nrably well as the inﬁnite model. In this formulation \u0019k\nexplicitly controls the prevalence of each individual com-\nponent; the closer \u0019kis to zero, the less frequently it will\ncontribute to X. The rest of the model is speciﬁed as:\nlog(dk)\u0018 N (0;IF)\u000ft\u0018 N (0;\r\u00001\n\u000fIF)\nst\u0018Gamma (\u000b;\f)\r\u000f\u0018Gamma (c0;d0)(3)\nThe choice of component dkbeing lognormal distributed\nwill become natural as we describe our inference algorithm\nin Section 3.2. For activation st, being gamma distributed\ninstead of lognormal distributed is easier to extend to a\ntime-dependent gamma chain prior [5]. The full model is\nsummarized in Figure 1.\nExact inference for this model is infeasible, so we in-\nstead derive a mean-ﬁeld [8] variational inference algo-\nrithm. Note that this model is nonconjugate between the\nobservation Xand priors DandS, which makes deriving\nan inference algorithm more difﬁcult.\n3. VARIATIONAL INFERENCE\n3.1 Laplace Approximation Variational Inference\nSince BP-NMF is nonconjugate, we use Laplace approxi-\nmation variational inference [16].\nGiven a probabilistic model P(X;\u0002)whereXdenotes\nthe observation and \u0002denotes hidden variables, mean-ﬁeld\ninference approximates the posterior P(\u0002jX)with a fully\nfactorized variational distribution q(\u0002) =Q\niq(\u0012i)by min-\nimizing the KL-divergence between the variational distri-\nbution and the true posterior. Inference can be carried out\nvia coordinate descent for each hidden variable \u0012iwhich is\nguaranteed to ﬁnd a local optimum.\n1It has been shown [13] that beta process is the de Finetti mixing mea-\nsure for the Indian buffet process.xt st zt\ndk \u0019k\na0; b0\r\u000f\nc0; d0\u000b; \fT\nK\nFigure 1 : Graphical model representation of BP-NMF.\nShaded node represents observed variable (spectrogram).\nUnshaded nodes represent hidden variables. A directed\nedge from node ato nodebdenotes that the variable bde-\npends on the value of variable a. Plates denote replication\nby the value in the lower right of the plate.\nThe general mean-ﬁeld update for a variable \u0012ican be\nshown [3] to be:\nq(\u0012i)/expfhlogP(X;\u0002)i\u0000\u0012ig (4)\nwhereh\u0001i\u0000\u0012iindicates the expectation with respect to q(\u0002n\nf\u0012ig). For simplicity, we will omit the subscript \u0000\u0012iwhen\nthere is no ambiguity.\nFor nonconjugate model, we cannot write Eq. 4 in closed\nform. The Laplace method is used to approximate q(\u0012i):\nf(\u0012i) =hlogP(X;\u0002)i\u0000\u0012i\n\u0019f(^\u0012i) +1\n2(\u0012i\u0000^\u0012i)TH(^\u0012i)(\u0012i\u0000^\u0012i) (5)\nA second-order Taylor expansion is taken in (5) where ^\u0012iis\na local maximum of f(\u0012i)andH(^\u0012i)is the Hessian matrix.\nThis suggests that we use a Gaussian variational distribu-\ntion forq(\u0012i):\nq(\u0012i) =N(^\u0012i;\u0000H(^\u0012i)\u00001) (6)\nH(^\u0012i)is guaranteed to be negative deﬁnite at any local\nmaximum of f(\u0012), providedf(\u0012)is smooth.\u0000H(^\u0012i)\u00001\nis therefore a valid covariance matrix. Conjugate gradient\nor L-BFGS can be used to search for ^\u0012i.\n3.2 Inference for BP-NMF\nLaplace approximation variational inference assumes that\nthe nonconjugate continuous variables are unconstrained,\nthus we reparametrize fD,Sgasf\b;\tg, where \b2\nRF\u0002Kwith\bfk= log(Dfk)and\t2RK\u0002Twith\tkt=\nlog(Skt). The fully-factorized variational distribution is:\nq(\u0002) =q(\r\u000f)KY\nk=1q(\u0019k)\u0012FY\nf=1q(\bfk)\u0013TY\nt=1q(\tkt)q(Zkt)\nwhere the variational distributions are speciﬁed as:\nq(\bfk) =N(\u0016(\b)\nfk;1=\r(\b)\nfk)\nq(\tkt) =N(\u0016(\t)\nkt;1=\r(\t)\nkt)\nq(Zkt) =Bernoulli (p(z)\nkt)\nq(\u0019k) =Beta(\u000b(\u0019)\nk;\f(\u0019)\nk)\nq(\r\u000f) =Gamma (\u000b(\u000f);\f(\u000f))(7)We will brieﬂy describe the mean-ﬁeld update and a Python\nimplementation is available online2.\n3.2.1 Update \band\t\nFollowing Eq. 4, we can write q(\bfk)as:\nq(\bfk)/expfhlogP(X;\u0002)i\u0000\bfkg\n/expfhlogP(xfj\u001ef;\t;Z;\r\u000f)i+ logP(\bfk)g\n= expff(\bfk)g (8)\nand expressP(xfj\u001ef;\t;Z;\r\u000f)in exponential family form:\nhlogP(xfj\u001ef;\t;Z;\r\u000f)i\n=h\u0011(\u001ef;\t;Z;\r\u000f)TiT(xf)\u0000hA(\u0011)i:(9)\nFor BP-NMF, both h\u0011(\u001ef; t;zt;\r\u000f)iandhA(\u0011)ican be\ncomputed in closed form. Thus, we can search for a local\nmaximum ^\bfk. The mean-ﬁeld update following Eq. 6 is:\n\u0016(\b)\nfk ^\bfk;\n\r(\b)\nfk \u0000@2f\n@\b2\nfk(^\bfk):(10)\nThe update for \tktis basically the same as \bfk, except\nthatP(\tkt)is a log-gamma distribution:\nP(\tkt)/expf\u000b\tkt\u0000\fexpf\tktgg: (11)\n3.2.2 Update Z\nSimilarly, we can follow Eq. 4:\nq(Zkt)/expfhlogP(X;\u0002)i\u0000Zktg (12)\n/expfhlogP(xtj\b; t;zt;\r\u000f)i+hlogP(Zktj\u0019k)ig\nSinceZktis Bernoulli distributed, we can explicitly com-\nputeP0/q(Zkt= 0) andP1/q(Zkt= 1) , respectively.\nThen the update for Zktcan be carried out:\np(z)\nkt P1\nP0+P1(13)\n3.2.3 Update \u0019and\r\u000f\nIn BP-NMF,\u0019andZare conjugate, therefore we can di-\nrectly derive the mean-ﬁeld update for \u0019in closed form:\n\u000b(\u0019)\nk a0\nK+TX\nt=1hZkti\n\f(\u0019)\nk b0(K\u00001)\nK+T\u0000TX\nt=1hZkti(14)\nSimilarly,\r\u000fcan also be updated in closed form:\n\u000b(\u000f) c0+1\n2FT\n\f(\u000f) d0+1\n2TX\nt=1kxt\u0000hD(st\fzt)ik2\n2(15)\n2https://github.com/dawenl/bp_nmf3.3 Accelerating inference\nBoth [11] and [7] proposed heuristics to speed up the in-\nference. In general, we want to set number of dictionary\ncomponents Kto be large so that it can better approximate\nthe inﬁnite functional prior. On the other hand, a large\nvalue ofKwill dramatically increase the time for infer-\nence. This can be compensated by setting Kinitially to a\nlarge value and truncating the rarely-used dictionary com-\nponents as the inference proceeds. The heuristic applied\nfor BP-NMF is that, for dictionary component dk, if the\ncorresponding \u0019kdrops below 10\u00003of the maximal \u0019, we\nskip it during the inference. The ﬁrst few iterations may be\nslow, but inference accelerates as more elements of \u0019are\ndriven towards 0.\n4. EXPERIMENTS\nWe conducted a set of experiments to evaluate if BP-NMF\ncan effectively capture the latent components from music\nrecordings. First, we performed a sanity check on a syn-\nthetic example. Then we tested BP-NMF on 2 different\ntasks: bandwidth expansion and blind source separation.\nWe also designed a transcription-based mechanism to eval-\nuate the quality of the learned dictionary.\nAll experiments were done on magnitude spectrum with\nhyperparameters \u000b=\f= 2,a0=b0= 1, andc0=\nd0= 10\u00006. All the variational parameters were randomly\ninitialized. The initial Kwas set to 512. All recordings\nwere sampled at 22.05 kHz.\nWe compared with three other NMF algorithms: GaP-\nNMF [7] which is another nonparametric Bayesian NMF\nbased on the gamma process, IS-NMF [5] which uses the\naudio-oriented Itakura-Saito divergence as loss function,\nand EUC-NMF [9] which minimizes the sum of the squared\nEuclidean distance and can be considered as a ﬁnite ver-\nsion of BP-NMF.\n4.1 Synthetic Data\nWe synthesized a short clip of audio with 5 distinct piano\nnotes and 5 distinct clarinet notes using ChucK3which is\nbased on physical models for the instruments. At any given\ntime, one piano note and one clarinet note are played si-\nmultaneously at different pitches.\nDFTs of 512 samples (23.2 ms) were computed with\n50% overlap. Kquickly converged to 7after a few it-\nerations of variational inference. Ideally, there should be\n10components, but since some of the notes only appear\nwith some others, they were grouped together by BP-NMF.\nThe learned dictionary components (in log scale) and ac-\ntivations are shown in the Figure 2, from top to bottom in\ndescending order of \u0019k. As we can see, there are clear har-\nmonic structures in the learned dictionary and the activa-\ntion does reasonably reﬂect the location where note com-\nbinations are played. Most importantly, the binary mask Z\nsucceeds in explicitly controlling the appearance and dis-\nappearance of the components, which is not reﬂected when\nwe test on GaP-NMF, IS-NMF, and EUC-NMF.\n3http://chuck.stanford.edu/0 2150 4300 6450 8600 10750\nfrequency (Hz)1\n2\n3\n4\n5\n6\n7component(a) Dictionary components Din log scale.\n0.0 0.6 1.2 1.7 2.3\ntime (sec)1\n2\n3\n4\n5\n6\n7component (b) Activations S\fZ.\nFigure 2 : The learned dictionary components and activations from BP-NMF on synthetic data. Both of them are listed in\ndescending order of \u0019k.\n4.2 Bandwidth Expansion\nThe basic idea of bandwidth expansion [2] is to infer the\nhigh-frequency content of a signal given only the low fre-\nquency part of the spectrum.\nWe use 2 pieces of music: Pink Moon by Nick Drake\nandFunky Kingston by Toots and the Maytals, both of\nwhich are also used in [7] for bandwidth expansion evalu-\nation. DFTs of 512 samples are computed with no overlap.\nWe take the middle 4000 frames of each piece and do a\n5-fold cross-validation: 4/5 of the data is used to learn the\ndictionary. For the remaining 1/5, the top 2 octaves (192\nfrequency bins) are removed as a held-out set. We encode\nthe low-frequency content with the corresponding part of\nthe learned dictionary and predict the high-frequency con-\ntent by reconstructing the full frequency band with the whole\ndictionary on the encoded activation.\nHere we use predictive likelihood as a metric. We com-\npare BP-NMF with GaP-NMF and EUC-NMF. The reason\nfor not including IS-NMF is that it has been compared on\nthe exactly same task with GaP-NMF in [7] and GaP-NMF\nhas comparably better performance.\nUnlike BP-NMF and GaP-NMF, EUC-NMF needs to\nspecify the number of components K. Given the rela-\ntionship between BP-NMF and EUC-NMF, we set Kto\nthe average number of dictionary components inferred by\nBP-NMF. Since both BP-NMF and EUC-NMF assume the\nnoise is Gaussian distributed while GaP-NMF assumes the\nnoise is exponential distributed, the predictive likelihood\nshould be computed differently. However, this would give\nthe exponential model an advantage, as the Gaussian dis-\ntribution assigns very low probability to outcomes far from\nits mean, while the exponential distribution can give mod-\nerately high likelihood to values close to 0 even if they are\nfar from the mean. To adjust for this, we evaluate the pre-\ndictive likelihood under an exponential distribution for all\nthree models. This may arguably still favor GaP-NMF as\nit is trained using the exponential model that it is tested on.\nGeometric mean of predictive likelihood with standard\nerror under exponential model is reported in Figure 3. Con-\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8GaP-NMFBP-NMFEUC-NMFPink Moon\n0.0 0.2 0.4 0.6 0.8 1.0\nGeometric mean of predictive likelihoodGaP-NMFBP-NMFEUC-NMFFunky KingstonFigure 3 : The geometric mean of the predictive likelihood\nunder an exponential model for Pink Moon and Funky\nKingston on a 5-fold cross-validation with standard error.\nIn general, there is no signiﬁcant difference among EUC-\nNMF, BP-NMF, and GaP-NMF. However, BP-NMF gives\nmore stable results with smaller standard error.\ntrary to the results in [7], GaP-NMF does not dominate.\nThis is partially due to the adjustment for Gaussian mod-\nels. But the lower sampling rate of 22.05 kHz also helps\nthe Gaussian model, since a ﬁxed-variance Gaussian dis-\ntribution has trouble modeling low-energy signals such as\nthose that tend to appear at very high frequencies.\nThe results in Figure 3 show that there is no signiﬁcant\ndifference among EUC-NMF, BP-NMF, and GaP-NMF.\nHowever, BP-NMF gives more stable results with smaller\nstandard error, while its parametric counterpart EUC-NMF\nhas much larger standard error compared with both BP-\nNMF and GaP-NMF.\n4.3 Blind Source Separation\nAs with GaP-NMF, BP-NMF can also be applied to blind\nsource separation. The model formulation of BP-NMF can\nbe directly adopted for blind source separation, where each\ndictionary component can be considered as all or part of\nthe source.\nWe evaluate BP-NMF for blind source separation and\ncompare with GaP-NMF using MIREX F0estimation data,Table 1 : Instrument-level bsseval results. The last col-\numn lists the number of components Kinferred by the\nmodels.\nSDR SIR SAR K\nBP-NMF 0.65 7.46 4.81 46\nGaP-NMF -1.86 3.89 6.12 31\nwhich is a woodwind quintet recording, consisting of bas-\nsoon, clarinet, ﬂute, horn, and oboe. This piece has rich\ncontent across frequencies and various sound textures. The\ngoal is to separate the signal on the instrument level. We\ncompute DFTs of 1024 samples with 50% overlap.\nTo separate out different instruments, we need to ﬁlter\nout the audio signals belonging to different dictionary com-\nponents. As in [5], given the complex spectrogram Xc, to\nreconstruct the estimated complex spectrogram for the kth\ncomponent ^X(k), we can apply Wiener ﬁltering:\n^X(k)\nft=Xc\nftDfkSktZktPK\nl=1DflSltZlt(16)\nThere is no direct information to determine how the\nsources and instruments correspond. The heuristic in [7]\nis adopted: for each instrument, we pick the single compo-\nnent whose corresponding activation sk\fzkhas the largest\ncorrelation with the power envelope of the single-track in-\nstrument signal. Note that the number of components in-\nferred is larger than the number of instruments, thus the\nselected components only represent part of sources.\nBsseval [14] is used to quantitatively evaluate the\nblind source separation. Table 1 lists the average SDR\n(Source to Distortion Ratio), SIR (Source to Interferences\nRatio), and SAR (Sources to Artifacts Ratio) across instru-\nments for BP-NMF and GaP-NMF (higher ratios are bet-\nter). BP-NMF performs comparably well. BP-NMF de-\ncomposes the piece into 46 components, and GaP-NMF\ndecomposes the piece into 31 components. We attribute\nthis to the sparsity induced by BP-NMF’s binary mask Z;\none needs a richer dictionary to explain the data with a\nsparse activation matrix.\n4.4 Dictionary Quality Evaluation\nThe evaluation of latent component discovery and source\nseparation is always difﬁcult. We propose an evaluation\nmechanism similar to music transcription and provide sta-\ntistically signiﬁcant results.\nTo evaluate the model’s ability to discover latent com-\nponents from mixed signals, we can instead work on mono-\nphonic signals, which is a substantially simpler problem.\nWe can then compare the results with those from mixed\nsignal. If there is signiﬁcant similarity, it indicates that\nthe model can do equally well as it would have even if the\nproblem were made artiﬁcially easier.\nSince we have the single-track recordings for each in-\nstrument in the woodwind quintet recording from Section\n4.3, we can apply BP-NMF to each of them separately andwe will expect the learned dictionaries to be of high qual-\nity. We compute DFTs of 512 samples with no overlap.\nThe number of learned components from each instrument\nis larger than the number of distinct notes V, thus only\nthe topVcomponents are selected according to the cor-\nresponding importance \u0019v. The selected components are\nshown in Figure 4a. The blocks are grouped according to\ninstruments and sorted by approximated fundamental fre-\nquency. In the original piece, the bassoon is mostly play-\ning low-pitch notes, while ﬂute is playing high-pitch notes,\nboth of which are reﬂected in the learned dictionaries.\nNow we would like to see if the results from BP-NMF\non the mixed signal are similar to those from single-track\nrecordings. Again there is no explicit information about\nthe correspondence between the components learned from\nthe mixed signal and the single-track signals. Thus, we\nadopt a greedy search which tries to match the dictionary\ncomponents based on their correlation. BP-NMF discovers\n29 components to describe the data4, which is less than the\nnumber of distinct notes. Thus we only match the top 29\nfrom components in Figure 4a.\nAfter obtaining a one-to-one matching between dictio-\nnaries, we compute the correlations between the correspond-\ning activations sk\fzk. A box-and-whisker plot of correla-\ntions is shown in Figure 4b. As comparison, we also show\nthe correlations from random matchings. Random match-\ning has correlation close to 0, indicating there is no linear\ndependence. The minimum of the correlation from BP-\nNMF matching is close to 0 due to the fact that a few of\nthe activations on the mixed signal are fairly sparse. But\nthe overall quantiles do not overlap.\nTo formally test if the results from BP-NMF matching\nand random matching are signiﬁcantly different, we apply\nhypothesis testing. Since we do not assume the correla-\ntions are normally distributed, a paired Wilcoxon signed-\nrank test [17], instead of a Student’s t-test, is performed\nbetween the correlations from BP-NMF matching and ran-\ndom matching. The null hypothesis is that the correlations\nfrom BP-NMF matching and random matching come from\nthe same population and we get p-value less than 0:01,\nwhich indicates that their difference is statistically signif-\nicant. This gives a solid evidence that BP-NMF is able to\nlearn dictionary components equally well in mixed signal,\nwhen compared with dictionaries learned from single-track\ninstrument recordings.\nWe also apply the same procedures to IS-NMF and GaP-\nNMF. For IS-NMF, as we need to specify the number of\ncomponents K, each single-track recording is decomposed\nwithKequals the number of distinct notes. For the mixed\nsignal, we set K2f5;10;20;\u0001\u0001\u0001;70g. WhenKis be-\ntween 10and30, the Wilcoxon signed-rank test shows that\nthe difference is signiﬁcant at p= 0:05level. For the\nrest of theK’s, we get larger p-values and cannot reject\nthe null hypothesis. GaP-NMF decomposes the data into\n20 components and the test results show signiﬁcant differ-\nence between GaP-NMF matching and random matching.\n4This number is smaller than that in Section 4.3 because a smaller\nDFT size with no overlap is used, which leads to less data.bassoon clarinet flute horn oboe010502100315042005250frequency (Hz)(a) The selected components learned from single-track instru-\nment. For each instrument, the components are sorted by approx-\nimated fundamental frequency. The dictionary is cut off above\n5512.5 Hz for visualization purposes.\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0\ncorrelationBP-NMF\n match\nRandom\n(b) The box-and-whisker plot for the correlations from both BP-\nNMF matching and random matching. A paired Wilcoxon signed-\nrank test shows that they are signiﬁcantly different.\nFigure 4 : The results from the proposed evaluation.\nTherefore, this evaluation mechanism can also be applied\nto determine a range for the “proper” number of compo-\nnents to describe the data.\n5. CONCLUSION\nIn this paper, we propose BP-NMF, a Bayesian nonpara-\nmetric extension of nonnegative matrix factorization, which\ncan automatically infer the number of latent components.\nBP-NMF explicitly assumes that some of the components\nare often completely silent. BP-NMF performs well under\nexisting metrics and under a novel evaluation mechanism.\n6. ACKNOWLEDGMENTS\nThe authors thank the reviewers for comments and the help-\nful discussion with Brian McFee and Colin Raffel. This\nwork was supported by the NSF under grant IIS-1117015.\n7. REFERENCES\n[1] Samer A. Abdallah and Mark D. Plumbley. Polyphonic\nmusic transcription by non-negative sparse coding of\npower spectra. In Proceedings of the 5th International\nSociety for Music Information Retrieval Conference ,\npages 10–14, 2004.\n[2] Dhananjay Bansal, Bhiksha Raj, and Paris Smaragdis.\nBandwidth expansion of narrowband speech using non\nnegative matrix factorization. In 9th European Confer-\nence on Speech Communication (Eurospeech) , 2005.\n[3] Christopher M. Bishop. Pattern Recognition and Ma-\nchine Learning (Information Science and Statistics) .\nSpringer-Verlag New York, Inc., 2006.\n[4] Finale Doshi-Velez, Kurt T. Miller, Jurgen Van Gael,\nand Yee Whye Teh. Variational inference for the Indian\nbuffet process. In International Conference on Artiﬁ-\ncial Intelligence and Statistics , 2009.[5] C ´edric F ´evotte, Nancy Bertin, and Jean-Louis Dur-\nrieu. Nonnegative matrix factorization with the Itakura-\nSaito divergence: with application to music analysis.\nNeural Computation , 21(3):793–830, 2009.\n[6] Nils Lid Hjort. Nonparametric bayes estimators based\non beta processes in models for life history data. The\nAnnals of Statistics , pages 1259–1294, 1990.\n[7] Matthew D. Hoffman, David M. Blei, and Perry R.\nCook. Bayesian nonparametric matrix factorization for\nrecorded music. In Proceedings of the 27th Annual In-\nternational Conference on Machine Learning , pages\n439–446, 2010.\n[8] Michael I. Jordan, Zoubin Ghahramani, Tommi S.\nJaakkola, and Lawrence K. Saul. An introduction to\nvariational methods for graphical models. Machine\nlearning , 37(2):183–233, 1999.\n[9] Daniel D. Lee and H. Sebastian Seung. Algorithms for\nnon-negative matrix factorization. Advances in Neural\nInformation Processing Systems , 13:556–562, 2001.\n[10] Alexey Ozerov and C ´edric F ´evotte. Multichannel non-\nnegative matrix factorization in convolutive mixtures\nfor audio source separation. Audio, Speech, and Lan-\nguage Processing, IEEE Transactions on , 18(3):550–\n563, 2010.\n[11] John Paisley and Lawrence Carin. Nonparametric fac-\ntor analysis with beta process priors. In Proceedings of\nthe 26th Annual International Conference on Machine\nLearning , pages 777–784, 2009.\n[12] Paris Smaragdis and Judith C. Brown. Non-negative\nmatrix factorization for polyphonic music transcrip-\ntion. In Applications of Signal Processing to Audio and\nAcoustics, 2003 IEEE Workshop on. , pages 177–180.\nIEEE, 2003.\n[13] Romain Thibaux and Michael I. Jordan. Hierarchical\nbeta processes and the Indian buffet process. In In-\nternational Conference on Artiﬁcial Intelligence and\nStatistics , 2007.\n[14] Emmanuel Vincent, R ´emi Gribonval, and C ´edric\nF´evotte. Performance measurement in blind audio\nsource separation. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on , 14(4):1462–1469,\n2006.\n[15] Tuomas Virtanen. Monaural sound source separation\nby nonnegative matrix factorization with temporal con-\ntinuity and sparseness criteria. Audio, Speech, and Lan-\nguage Processing, IEEE Transactions on , 15(3):1066–\n1074, 2007.\n[16] Chong Wang and David M. Blei. Variational inference\nin nonconjugate models. Journal of Machine Learning\nResearch , 14:899–925, 2013.\n[17] Frank Wilcoxon. Individual comparisons by ranking\nmethods. Biometrics bulletin , 1(6):80–83, 1945."
    },
    {
        "title": "Exploration of Music Emotion Recognition Based on MIDI.",
        "author": [
            "Yi Lin",
            "Xiaoou Chen",
            "Deshun Yang"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416604",
        "url": "https://doi.org/10.5281/zenodo.1416604",
        "ee": "https://zenodo.org/records/1416604/files/LinCY13.pdf",
        "abstract": "Audio and lyric features are commonly considered in the research of music emotion recognition, whereas MIDI features are rarely used. Some research revealed that among the features employed in music emotion recognition, lyric has the best performance on valence, MIDI takes the second place, and audio is the worst. However, lyric cannot be found in some music types, such as instrumental music. In this case, MIDI features can be considered as a choice for music emotion recognition on valence dimension. In this presented work, we systematically explored the effect and value of using MIDI features for music emotion recognition. Emotion recognition was treated as a regression problem in this paper. We also discussed the emotion regression performance of three aspects of music in terms of edited MIDI: chorus, melody, and accompaniment. We found that the MIDI features performed better than audio features on valence. And under the realistic conditions, converted MIDI performed better than edited MIDI on valence. We found that melody was more important to valence regression than accompaniment, which was in contrary to arousal. We also found that the chorus part of an edited MIDI might contain as sufficient information as the entire edited MIDI for valence regression.",
        "zenodo_id": 1416604,
        "dblp_key": "conf/ismir/LinCY13",
        "keywords": [
            "Audio and lyric features",
            "MIDI features",
            "Music emotion recognition",
            "Valence dimension",
            "MIDI features performance",
            "Instrumental music",
            "MIDI features regression",
            "Melody importance",
            "Chorus part information",
            "Arousal contrast"
        ],
        "content": "EXPLORATION OF MUSIC EMOTION RECOGNITION \nBASED ON MIDI \n \nYi Lin, Xiaoou Chen and Deshun Yang \n \n \nInstitute of Computer Science & Technology, Peking University \nlin.yi.tz@gmail.com\n \n{chenxiaoou, yangdeshun}@pku.edu.cn\n \n \nABSTRACT \nAudio and lyric features are commonly considered in the \nresearch of music emotion recognition, whereas MIDI \nfeatures are rarely used. Some research revealed that \namong the features employed in music emotion recogni-\ntion, lyric has the best performance on valence, MIDI \ntakes the second place, and audio is the worst. However, \nlyric cannot be found in some music types, such as in-\nstrumental music. In this case, MIDI features can be \nconsidered as a choice for music emotion recognition on \nvalence dimension. \nIn this presented work, we systematically explored the \neffect and value of using MIDI features for music emo-\ntion recognition. Emotion recognition was treated as a \nregression problem in this paper. We also discussed the \nemotion regression performance of three aspects of mu-\nsic in terms of edited MIDI: chorus, melody, and accom-\npaniment. We found that the MIDI features performed \nbetter than audio features on valence. And under the rea-\nlistic conditions, converted MIDI performed better than \nedited MIDI on valence. We found that melody was more \nimportant to valence regression than accompaniment, \nwhich was in contrary to arousal. We also found that the \nchorus part of an edited MIDI might contain as sufficient \ninformation as the entire edited MIDI for valence regres-\nsion. \n1. INTRODUCTION AND RELATED WORKS \nMusic is a natural carrier to express and convey emotion. \nSome emotion models have been developed to describe \nemotion state. Russell’s two-dimensional valence-arousal \n(V-A) model [4] consisted of two independent dimension \nof valence and arousal. Valence stands for appraisal of \npolarity and arousal stands for the intensity of emotion. \nMehrabian [10] extended this approach and developed a \nthree-dimensional pleasure-arousal-dominance (PAD) \nmodel, where dimension P distinguishes the positive-\nnegative quality of emotion, dimension A refers to the \nintensity of physical activity and mental alertness, and dimension D refers to the degree of control. In this paper, \nwe focused on Russell's V-A model, especially the va-\nlence (V) dimension, which corresponds to pleasure (P) \ndimension in PAD model. Several types of feature have \nbeen developed to represent a piece of music. Audio and \nlyric features are commonly considered in the research of \nmusic emotion recognition, whereas MIDI features are \nrarely used [1]. Emotion recognition can be viewed as a \nmulticlass-multilabel classification or regression problem \n[1]. In this paper emotion recognition was treated as a \nregression problem. \nThis paper focused on music emotion recognition with \nMIDI features, which can be extracted directly from \nMIDI files. Unlike audio data, MIDI is a kind of the \nelectronic score. Symbolic representations of music (such \nas key, pitch, tempo, etc.), which are high-level musico-\nlogical symbolic features reflecting music concepts, can \nbe easily extracted and calculated from MIDI by toolkits \nsuch as jSymbolic [7]. Therefore, MIDI features may be \nmore effective on music emotion regression. \nOliveira and Cardoso [3] constructed a dataset of 96 \nwestern tonal music (film music) pieces, which lasted \nbetween 20 seconds to 1 minute. These pieces were in \nMIDI format and each piece might express only one type \nof affective content. 80 listeners were asked to label these \nmusical pieces with affective labels on valence and \narousal, respectively. Both musicological symbolic fea-\ntures (e.g., tempo, note duration, note density, etc) and \nacoustical features (such as MFCCs) were extracted. Af-\nter feature selection, they performed SVM regression and \nreceived correlation coefficient of 81.21% on valence \nand 84.14% on arousal from 8-fold cross validation ex-\nperiment. Then Oliveira and Cardoso [3] selected the \nmost important features that were identified separately \nduring feature selection results for valence and arousal \nand performed the 8-fold cross validation of SVM re-\ngression again. The most important features selected \nwere all symbolic features. The performance evaluated in \nterms of correlation coefficient reached 71.5% on va-\nlence and 79.14% on arousal. Oliveira and Cardoso’s \nwork demonstrated that MIDI is effective on music emo-\ntion recognition. \nOliveira and Cardoso’s work [3] only focused on \nMIDI, whilst Guan et al. [2] compared music emotion \nrecognition performance among audio, lyric, and MIDI \nfeatures. They presented an AdaBoost approach on 1687 \nChinese songs. A wave file and a lyric file were collected \nfor each song and a MIDI files was converted from the \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that cop-\nies bear this notice and the full citation on the first page.  \n© 2013 International Society for Music Information Retrieval    \n \nwave file. Audio, lyric, and MIDI features were extracted \nseparately. Guan et al. [2] applied their AdaBoost.RM \napproach on a multi-modal feature set in which audio, \nlyric and MIDI features were combined together. The \nperformance of the experiment was 74.2% on valence in \nterms of correlation coefficient. They also applied regres-\nsion to audio, lyric, and MIDI features, respectively. The \nresults showed that the regressor built using lyric fea-\ntures yielded the best performance on valence; the per-\nformance was 62.3% in terms of correlation coefficient, \nwhereas the regressor built using audio features had the \npoorest performance of 47.3% and the regressor built us-\ning MIDI features was in between, 54.1%. \nBased on findings from the related works, there were \ntwo aspects of views towards MIDI. Oliveira and Cardo-\nso [3] regarded MIDI as an object on which the emotion \nvalue is labeled. Although they received a good result on \nemotion regression, it was very difficult to find a satis-\nfactorily-composed MIDI file for a song. A satisfactorily \ncomposed MIDI file should be a MIDI that sounds exact-\nly the same to the original song when listened to. Instead \nGuan et al. [2] regarded MIDI as an intermediate repre-\nsentation of music audio data. They firstly converted au-\ndio files into MIDI files and then extracted MIDI fea-\ntures from the MIDI files. This was a much easier way \nfor obtaining example MIDI files, as there were large \namount of audio files available on the Internet. \nIn our work, we took Guan et al.’s [2] view and re-\ngarded MIDI as an intermediate representation of music \naudio data. We provided a systematic exploration of the \neffect and value of using MIDI features for music emo-\ntion recognition. Two types of MIDI files were. One type \nof MIDI files was converted from audio files, e.g., mp3, \nwav, etc. MIDI files obtained in this way were referred as \nconverted MIDI. The other type of MIDI files was com-\nposed by musicians and composers via a score editing \ntool such as Guitar pro [5]. MIDI files obtained in this \nway were referred as edited MIDI. Oliveira and Cardoso \n[3] only considered edited MIDI and Guan et al. [2] only \nconsidered converted MIDI, whereas we considered both \nin our work. \nOur work consisted of two parts. In part one we com-\npared the music emotion regression performance of au-\ndio, lyric, and MIDI features. We carried out emotion \nregression with audio, lyric, and MIDI features separate-\nly rather than combined them together. We also com-\npared the music emotion regression performance be-\ntween converted MIDI and edited MIDI; this work was \nnot included in the paper of Guan et al [2]. Moreover, we \nplayed the 653 edited MIDI files back acoustically, rec-\norded that, and converted that back to MIDI automatical-\nly to investigate what helped to predict emotion. In part \ntwo we investigated the edited MIDI from three aspects \nof music: chorus, melody, and accompaniment. \nThe paper is organized as follows. Section 2 presents \nthe datasets and features. Section 3 reports the experi-\nments and results. Section 4 provides the analysis of the \nexperiment results. Section 5 concludes the key findings. 2. DATASETS AND FEATURES \n2.1 Datasets \nWe used Guan et al.’s 2500-Chinese-song list, where \neach song was annotated with a PAD label [2]. Datasets \nwere constructed according to the 2500-Chinese-song list. \nAll these Chinese songs were composed based on West-\nern 12-tone equal temperament. \nIn our work, 7 datasets were constructed as described \nin the following section (including four sets of edited \nMIDI files, one set of audio files, one set of lyric files \nand one set of converted MIDI files). Among the four \ntypes of data (edited MIDI, converted MIDI, audio, and \nlyric), the edited MIDI data was the most difficult to be \ncollected among the four kinds of data: edited MIDI, \nconverted MIDI, audio, and lyric. Therefore we firstly \nconstructed our edited MIDI datasets to determine how \nmany songs could be included in our work. Then we \nconstructed audio dataset and lyric dataset. Finally we \nconstructed converted MIDI dataset in which converted \nMIDIs came from audios. \n2.1.1 Edited MIDI \nWe applied two ways for collecting edited MIDIs. One \nway was to download edited MIDI files from Internet \nthat were composed by musicians and composers. In our \nwork, the notes of Edited MIDI files that collected in \nthis way are entered directly into a sequencer program \nor notation software rather than being recorded directly \nfrom playing on a MIDI instrument. The other way was \nto download scores from Internet that were written by \nmusicians and composers and then translated the scores \ninto MIDI files via a score editing tool called Guitar pro \n[5]. \nThe final edited MIDI dataset contained 653 MIDI \npieces, which lasted between 15 seconds to 7 minutes. \nThis dataset was a subset of the 2500 Chinese songs. \nEach MIDI in the dataset contains melody, accompani-\nment, and at least two different timbres, so that the \nMIDI could sound as similar as possible to the original \nsongs when listened to. If multiple MIDI versions of a \nsong were available, the one that was the closest to the \noriginal song when listened to would be retained. This \ndataset was referred as edit-MIDI. \nWe split each of the 653 MIDI pieces into two parts: \nmelody and accompaniment. This work was carried out \nby manually extracting the tracks corresponding to me-\nlody and accompaniment, respectively, from one MIDI \nto create two new MIDIs. One represented melody and \nthe other represented accompaniment. Thus two more \ndatasets were constructed. The dataset, which contained \n653 MIDI pieces corresponding to the melody part, was \nreferred as edit-MIDI-melody; the other dataset, which   \n \ncontained 653 MIDI pieces corresponding to the accom-\npaniment part, was referred as edit-MIDI-accom. \nShort versions of the 653 MIDIs in edit-MIDI are al-\nso collected. Each short MIDI was the chorus part of the \ncorresponding music. Most of these pieces were used as \ncell phone ringtones, lasting between 15 seconds to 40 \nseconds. This dataset is referred as edit-MIDI-chorus. \nTo conclude, we built four datasets for edited MIDI: \nedit-MIDI, edit-MIDI-melody, edit-MIDI-accom, and \nedit-MIDI-chorus. It worth to mention that it has been \nvery difficult to find for a piece of music a satisfactorily-\ncomposed edited MIDI that sounds exactly the same as \nthe original music. The reasons are two-fold: on one \nhand, not all music had a MIDI; on the other hand, the \ncomposer might not be willing to share the MIDI file. \n2.1.2 Audio and Lyric \nFor the 653 songs corresponding to the edited MIDI, we \ndownloaded their wav audio files and lyric files, which \nconstituted the audio dataset and lyric dataset, respec-\ntively. \n2.1.3 Converted MIDI \nConverted MIDI dataset contained 653 MIDI files con-\nverted from the 653 wav files. WIDI Recognition System \nPro 4.0 [6] was used to help with the conversion. This \ndataset was referred as conv-MIDI. \nThere were several differences between edited MIDI \nand converted MIDI. Firstly, each converted MIDI piec-\nes was of the same length as the original song, whereas \nthe length of edited MIDI pieces varied. Secondly, there \nwas only one timbre existed in converted MIDI; the tim-\nbre was set to be Instrumental Grand in WIDI. Edited \nMIDI, however, contained at least two different timbres. \nFinally, the converted MIDI was very different from the \nedited MIDI of the same song when listened to. The \nconverted MIDI sounded like its pitches were in a mess \nand it was hard or even unable to distinguish how the \nmelody went. Edited MIDI, however, sounded similar to \nor even the same as the original song. \n2.2 Features \nMIDI features were extracted from the MIDI files by \njSymbolic [7]. For each MIDI file 112 types of MIDI fea-\ntures were extracted to compose a feature vector of 1022 \ndimensions. Audio features were extracted from the wave \nfiles by jAudio [8]. For each audio file 27 types of audio \nfeatures were extracted to compose a feature vector of \n112 dimensions for each song. After lyric files were pre-\nprocessed with traditional NLP tools including stop-\nwords filtering and word segmentation, unigram features \nwere extracted from the lyrics file to compose a feature \nvector of 13251 dimensions. 3. EXPERIMENTS AND RESULTS \nSupervised feature selection was carried out on each of \nthe feature sets to reduce the number of features and to \nimprove the regression results. Correlation-based Feature \nSubset Selection with BestFirst was applied as its search \nmethod in our work [9].  \nFollowing results of regression experiments were ob-\ntained using 5-fold cross validation of SMO regression \nwith RBFKernel [9]. SMO regression [11] implements \nsupport vector machine for regression and in our work \nwe used Radial Basis Function (RBF) as its kernel func-\ntion. The performance of regression was measured in \nterms of correlation coefficient (CF).  \nRussell’s two-dimensional valence-arousal (V-A) \nmodel [4] was employed to measure music emotion. \n3.1 Comparison among Lyric, Audio, Edited MIDI, \nand Converted MIDI \nFirstly, regression was applied on edit-MIDI, conv-MIDI, \naudio, and lyric datasets separately to compare the MIDI \nfeatures with commonly used audio and lyric features. \nThe results are showed in Table 1. \nTable 1 shows the regression performance of lyric, \naudio, conv-MIDI, and edit-MIDI on valence and arousal. \nIt worth noting that conv-MIDI was found to perform \nbetter than audio and worse than lyric on valence. We \nalso found that the performance of edit-MIDI was much \nworse than conv-MIDI.  \nDataset V A \nLyric 78.81% 66.52% \nAudio 54.45% 76.5% \nConv-MIDI 57.09% 74.96% \nEdit-MIDI 46.42% 53.37% \nTable 1. Performance of lyric, audio, conv-MIDI, and \nedit-MIDI. \nTo analyze the difference between edited MIDI and \nconverted MIDI, we examined the remaining features on \nvalence after feature selection. For conv-MIDI, there \nwere 10 types of features remaining that consisted of 60 \nfeatures. For edit-MIDI, there were 37 types of features \nremaining that consisted of 315 features. Among these \ntypes of remaining features, there were 7 types of fea-\ntures that were common to both converted MIDI and \nedited MIDI. The remaining feature types on valence are \nshown in Table 2. \nIn Table 2, row 1 listed the remaining feature types \nthat belonged to converted MIDI only; row 3 listed the \nremaining feature types that belonged to edited MIDI on-\nly; row 2 listed the remaining feature types that belonged \nto both converted MIDI and edited MIDI. In row 1, it \ncan be seen that there were only 3 types of features (e.g.   \n \nin row 1: Duration, Combined Strength of Two Strongest \nRhythmic Pulses, and Rhythmic Variability) that were \nincluded in features of converted MIDI, but not in fea-\ntures of edited MIDI. In order to investigate the impor-\ntance of these three types of features, we carried out the \nregression again without these 3 types of features on \nconv-MIDI. The performance of the experiment dropped \n3.01% (from 57.09% to 54.07%) in terms of CF on va-\nlence. \nconv-MIDI \nonly Duration \nCombined Strength of Two Strongest Rhyth-\nmic Pulses \nRhythmic Variability \nconv-MIDI & \nedit-MIDI Chromatic Motion \nStrength of Strongest Rhythmic Pulse \nVariability of Note Duration \nBasic Pitch Histogram \nBeat Histogram \nMelodic Interval Histogram \nTime Prevalence of Pitched Instruments \nedit-MIDI \nonly Amount of Apreggiation \nAverage Melodic Interval \nBrass Fraction \nChanges of Meter \nDominant Spread \nGlissando Prevalence \nMost Common Melodic Interval Prevalence \nMost Common Pitch Class Prevalence \nNote Density \nNumber of Common Pitches \nQuality \nFifths Pitch Histogram \nMelodic Interval Histogram \nNote Prevalence of Pitched Instruments \n……  \nTable 2. The remaining feature groups on valence after \nfeature selection. \n3.2 Conversions of the Edited MIDI \nIn order to investigate whether it was the process of con-\nversion from the original audio to MIDI that helped pre-\ndicting emotion, we went through a two-step experiment. \nFirstly we played the 653 edited MIDI files back acousti-\ncally and recorded the sounds (MIDI-WAV dataset); and \nsecondly, the files were converted back to MIDI automat-\nically (MIDI-WAV-MIDI dataset). We then examined \nthe changes of the regression performance on valence. \nThe results are showed in Table 3. \nDataset V \nEdit-MIDI 46.42% \nMIDI-WAV 26.04% \nMIDI-WAV-MIDI 43.83% \nTable 3. The performance of tow conversions of \nthe edited MIDI.\n \n Table 3 shows how the valence regression performed \non the three datasets obtained from the two conversions. \nThe performance was measured in terms of CF. Results \nin Table 3 revealed that the performance of MIDI-WAV-\nMIDI is lower than that of Edit-MIDI by 2.59% (from \n46.42% to 43.83%). \n3.3 Melody and Accompaniment of Edited MIDIs \nThe edited MIDI sounded similar to, or even the same as \nthe original song. In our work each edited MIDI file was \nsplit into two parts: melody and accompaniment. This \nallowed us to measure the performance of these two parts \non music emotion regression separately. However, we \nwere not able to experiment them on converted MIDI, \nbecause melody or accompaniment could not be ex-\ntracted from converted MIDI. \nThe experiment results are showed in Table 4. \nDataset V A \nEdit-MIDI-melody 46.26% 44.8% \nEdit-MIDI-accom 39.51% 48.94% \nTable 4. Performance of melody and accompaniment of \nedited MIDI.  \nTable 4 shows the performance of melody and accom-\npaniment of edited MIDI in terms of CF. Melody was \nfound to perform better than accompaniment on valence \nregression, which was in contrary to arousal. \n3.4 The Chorus of Edited MIDI \nIn most cases the chorus of a song is the emphatic part \nthat reflects music concept. The chorus may express only \none type of affective content.  \nWe collected edited MIDI files containing only the \nchorus part of the corresponding songs and applied emo-\ntion regression on them. The results are showed in Fig-\nure 1. \nFigure 1 shows the performance of the chorus dataset \nwith different dataset size. Meanwhile we compared the \nperformance of the chorus dataset with the performance \nof the entire MIDIs dataset (i.e. Edit-MIDI dataset). The \nnumber on the vertical axis refers to the number of MIDI \nfiles and the percentage on the horizontal axis refers to \nthe performance of regression on valence measured in \nCF. The black bars refer to the performance of the cho-\nrus dataset and the gray bars refer to the performance of \nthe corresponding entire MIDIs dataset. For each dataset \nsize, we carried out the experiments 3 times by randomly \nchoosing data examples. \nFigure 1 revealed that the performance on the chorus \ndataset was very close to that on the entire MIDIs dataset.   \n \n \nFigure 1. The performance of the chorus. \n4. RESULTS ANALYSIS \nThe result showed in row 1 to 4 of Table 1 aligns with \nthe work of Guan et al [2]. Lyric performed the best, \nconverted MIDI was the second and audio performed the \nworst for valence regression. By converting audio to \nMIDI and then using MIDI features extracted from con-\nverted MIDI files, converted MIDI performed better than \nthe audio by 2.36% on valence.  \nThe result showed in row 3 to 4 of Table 1 indicated \nthat the edited MIDI performs worse than the converted \nMIDI by 10.67% on valence.  \nTo analyze the difference between edited MIDI and \nconverted MIDI, we investigated the remaining features \non valence after feature selection, which was shown in \nTable 2. The performance of the experiment dropped \n3.01% (from 57.09% to 54.07%) in terms of CF on va-\nlence. This result indicated that, three types of features \n(Duration, Combined Strength of Two Strongest Rhyth-\nmic Pulses, and Rhythmic Variability) have stronger \nability to express and distinguish emotion on valence. \nTable 3 shows how the performance varied with the \ntwo conversions processes. The results indicated that the \nMIDI files converted from audio might not perform as \nwell as the original edited MIDIs. From Table 1 we \nfound that the conv-MIDI performed better than the edit-\nMIDI; the reason for this result might be that the source \naudio of conv-MIDI was much better than the source au-\ndio of MIDI-WAV-MIDI rather than that the process of \nconversion from the audio to MIDI that helped predict-\ning emotion. The source audio of conv-MIDI was the \noriginal audio that people listened to, while the source \naudio of MIDI-WAV-MIDI was the audio files synthe-\nsized from the edited MIDI files, which were not exactly \nthe same as the originally performed audio when listened \nto. If a large amount of satisfactorily-composed edited \nMIDIs were available, the regression performance of \nedited MIDIs might well be better than that of converted \nMIDIs. However, it was not always practical and feasible \nto obtain such perfect dataset. Considering the difficulty in collecting satisfactorily-composed edited MIDIs, con-\nverted MIDIs can be considered as a good choice for mu-\nsic emotion regression. \nTable 4 shows that for edited MIDI the melody MIDI \nperformed better than the accompaniment MIDI by \n6.75% on valence and accompaniment performed better \nthan the melody by 4.14% on arousal; this result indi-\ncates that melody is more effective and important to dis-\ntinguish the positive-negative quality of affective content, \nand accompaniment is more effective to distinguish in-\ntensity of physical activity and mental alertness.  \nFigure 1 shows the regression performance on the \nchorus dataset and entire MIDIs dataset with different \ndataset size. The results showed that the performance on \nchorus dataset was very close to that on entire MIDIs da-\ntaset. Most of the entire MIDIs were sufficiently long to \ncontain more than chorus part of music. On one hand, \nthe result revealed that the use of chorus instead of the \nentire song did not improve the valence regression in \nterms of edited MIDI. On the other hand, the result \nshowed that the chorus part of an edited MIDI might \nhave contained as sufficient information as the whole \nedited MIDI for valence regression. \n5. CONCLUSION \nIn this presented work, much valuable findings were ob-\ntained. Firstly, we found that the MIDI features extracted \nfrom converted MIDI files performed better than audio \nfeatures that were extracted from audio files Secondly, \nwe found the edited MIDI performed worse than con-\nverted MIDI under the realistic conditions. Therefore, \nthe converted MIDI could be considered as a good choice \nfor music emotion regression rather than the edited \nMIDI. We also compared and illustrated the differences \nbetween them based on features. The results indicated \nthat three types of features (Duration, Combined \nStrength of Two Strongest Rhythmic Pulses, and Rhyth-\nmic Variability) have stronger ability to express and dis-\ntinguish emotion on valence. Finally, we decomposed the \nedited MIDI and explored three aspects that were be-\nlieved to be important to music emotion recognition: me-\nlody, accompaniment, and chorus. Two conclusions were \ndrawn from the experimental results. One was that me-\nlody was more effective to valence regression and ac-\ncompaniment to arousal; the other one was that the cho-\nrus of an edited MIDI may have contained as sufficient \ninformation as the whole edited MIDI for valence regres-\nsion.   \n \n6. ACKNOWLEDGMENT \nThis work is supported by the Natural Science Founda-\ntion of China (No.61170167) and Beijing Natural \nScience Foundation (4112028). \n7. REFERENCES \n[1] Y. E. Kim, E. M. Schmidt, R. Migneco, B. G. \nMorton, P. Richardson, J. Scott, J. A. Speck, and D. \nTurnbull: “Music emotion recognition: A state of \nthe art review, ” 11th International Society for \nMusic Information Retrieval Conference , 2010  \n[2] Di Guan, Xiaoou Chen, and Deshun Yang: “Music \nEmotion Regression based on Multi-modal \nFeatures,” 9th International Symposium on \nComputer Music Modeling and Recognition , 2012. \n[3] A. P. Oliveira, and Amılcar Cardoso: “Modeling \naffective content of music: a knowledge base \napproach,” Sound and Music Computing \nConference , 2008. \n[4] J. A. Russell, “A circumspect model of affect,” \nJournal of Psychology and Social Psychology , vol. \n39, no. 6, p. 1161, 1980 \n[5] http://www.guitar-pro.com\n \n[6] http://www.widisoft.com\n \n[7] C. McKay, and I. Fujinaga: “jSymbolic: A feature \nextractor for MIDI files,” Proceedings of the \nInternational Computer Music Conference , 2005 \n[8] D. McEnnis, C. McKay, and I. Fujinaga: “jAudio: \nA Feature Extraction Library,” Proceedings of the \nInternational Conference on Music Information \nRecognition , 2005 \n[9] Weka: Data Mining Software in Java, \nhttp://www.cs.waikato.ac.nz/ml/weka\n \n[10]  Mehrabian, A.: “Framework for A Comprehensive \nDescription and Measurement of Motional States,” \nGenetic, Social, and General Psychology \nMonographs , vol. 121, pp. 33—361, 1995 \n[11] S.K. Shevade, S.S. Keerthi, C. Bhattacharyya, and \nK.R.K. Murthy: “Improvements to the SMO \nAlgorithm for SVM Regression,” IEEE \nTransactions on Neural Networks , 1999"
    },
    {
        "title": "Music Cut and Paste: A Personalized Musical Medley Generating System.",
        "author": [
            "I-Ting Liu",
            "Yin-Tzu Lin",
            "Ja-Ling Wu"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415764",
        "url": "https://doi.org/10.5281/zenodo.1415764",
        "ee": "https://zenodo.org/records/1415764/files/LiuLW13.pdf",
        "abstract": "A musical medley is a piece of music that is composed of parts of existing pieces. Manually creating medley is time consuming because it is not easy to find out proper clips to put in succession and seamlessly connect them. In this work, we propose a framework for creating personalized music medleys from users’ music collection. Unlike existing similar works in which only low-level features are used to select candidate clips and locate possible transition points among clips, we take song structures and song phrasing into account during medley creation. Inspired by the musical dice game, we treat the medley generation process as an audio version of musical dice game. That is, once the analysis on the songs of user collection has been done, the system is able to generate various medleys with different probabilities. This flexibility brings us the ability to create medleys according to the user-specified conditions, such as the medley structure or some must-use clips. The preliminary subjective evaluations showed that the proposed system is effective in selecting connectable clips that preserved chord progression structure. Besides, connecting the clips at phrase boundaries acquired more user preference than previous works did.",
        "zenodo_id": 1415764,
        "dblp_key": "conf/ismir/LiuLW13",
        "keywords": [
            "musical medley",
            "composition of existing pieces",
            "manual creation",
            "time-consuming",
            "difficulty in finding clips",
            "personalized music creation",
            "user music collection",
            "song structures",
            "song phrasing",
            "audio version of musical dice game"
        ],
        "content": "MUSIC CUT AND PASTE: A PERSONALIZED MUSICAL MEDLEY\nGENERATING SYSTEM\nI-Ting Liu Yin-Tzu Lin Ja-Ling Wu\nGraduate Institute of Networking and Multimedia, National Taiwan University\nftinaliu,known,wjl g@cmlab.csie.ntu.edu.tw\nABSTRACT\nA musical medley is a piece of music that is composed of\nparts of existing pieces. Manually creating medley is time\nconsuming because it is not easy to ﬁnd out proper clips\nto put in succession and seamlessly connect them. In this\nwork, we propose a framework for creating personalized\nmusic medleys from users’ music collection. Unlike ex-\nisting similar works in which only low-level features are\nused to select candidate clips and locate possible transition\npoints among clips, we take song structures and song phras-\ning into account during medley creation. Inspired by the\nmusical dice game, we treat the medley generation process\nas an audio version of musical dice game. That is, once the\nanalysis on the songs of user collection has been done, the\nsystem is able to generate various medleys with different\nprobabilities. This ﬂexibility brings us the ability to create\nmedleys according to the user-speciﬁed conditions, such\nas the medley structure or some must-use clips. The pre-\nliminary subjective evaluations showed that the proposed\nsystem is effective in selecting connectable clips that pre-\nserved chord progression structure. Besides, connecting the\nclips at phrase boundaries acquired more user preference\nthan previous works did.\n1. INTRODUCTION\nA musical medley is a music piece that is composed from\nparts of existing pieces [22]. It often composed from famous\ntracks of a speciﬁc artist, year or genre. The song excerpts\ncan be played successively with or without cross-fading. In\nthe past, medleys are usually made by professional audio\nengineers and published by music production companies.\nNowadays, more and more music hobbyists create their\nown medleys from their favourite songs with the help of\nnewly developed audio technologies and publish the results\non websites like Youtube. The resulting medley can be used\nas the background music of personal ﬁlms and slideshows\nor non-stopping dance suites. Since each song track just\nappeared as short clips (usually less than 30 seconds) in a\nmedley, the users can avoid copyright infringement while\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.\nVocal Vocal Instrumental Instrumental Instrumental I will  I will  \nVocal Vocal Instrumental Instrumental Instrumental I will  I will  Ben Mr. Tambourine \nMan  Piano Man  \nFigure 1 . The scenario of the proposed system. Upper part:\nthe user-speciﬁed structure and must-used clips. Bottom\npart: the generated medley by our system.\nkeeping their personal video with background music of\ntheir tastes. Existing editing tools like Goldwave1and Au-\ndition2enable users to cut and connect audio clips at the\nposition they specify directly. However, these tools are not\ngood enough to achieve the pre-described goal. In other\nwords, users still face the following challenging issues: (i)\nhow to choose suitable song excerpts to put together and (ii)\nhow to euphoniously connect them. With the vast amount\nof digital music, it is not easy for users to ﬁnd song excerpts\nthat are suitable for playing together. Besides, once the user\ndecides which song excerpts to adjoin, they still need to\nlisten to all of them to decide the position for cutting audio\nﬁles into clips and connecting them. Furthermore, users\nmay need to manually adjust the tempi and volume levels\nof the clips if needed to make them smoothly connected.\nSome previous works provided automatic schemes to create\nmedleys or medley-like mixes. However, they either only\nfocused on the rhythm and beat synchronization issues of\nadjacent clips or on directly connecting full songs [12, 15].\nUser preferences of the picked songs, the phrase boundary,\nand the structure of the medley were rarely considered [14].\nWe believe that whether music sounds pleasant highly de-\npends on each person’s taste. Thus, in this work, a frame-\nwork for creating personalized musical medleys from users’\nmusic collection is proposed, in which the completeness of\nphrases and chord progression of a song can be preserved.\nAs shown in Figure 1, users specify the structure of the\ntarget medley, and optionally select a few song excerpts that\nshould appear at certain positions in the medley, i.e. the\ndarker parts in the ﬁgure. Then the system will complete\nthe medley with song excerpts automatically selected from\nthe song collection user provided. The excerpts, automat-\nically segmented by our system, are about four-bars long\n(\u001810 seconds on average.) We treat the medley generation\nprocess as an audio version of musical dice game. Various\n1http://www.goldwave.com/\n2http://www.adobe.com/products/audition.htmlVocal \ncluster 1 Instrum. \ncluster 1 \nVocal  \ncluster 1 Instrum. \ncluster 2 Vocal  \ncluster 3 \nInstrum.  \ncluster 1 Vocal. \ncluster 2 Instrum. \ncluster 3 \nInstrum. \ncluster 1 Vocal \ncluster 1 \nBeginning  \nClusters \nMiddle \nClusters \nEnding  \nClusters Figure 2 . An example of a musical dice graph.\nmedleys can be generated once we’ve ﬁnished analyzing\nuser’s song collection. The ﬂexibility allows us to create\nmedleys based on user needs. We also provide an interactive\nscheme for users to change selected clips, adjust connecting\npositions and the overlap ratios between clips if users are\nnot satisﬁed with the result.\n2. RELATED WORK\nThe problem of medley creation can be divided into two\nparts: (i) ﬁnding proper song excerpts and their orders\n(including the order of the clips), and (ii) concatenating\nthe selected clips (including ﬁnding connecting positions).\nThere are several previous works addressed these issues\nseparately, but few considered them both.\nThe clip selection issue is similar to the song selection\nproblem in the study of playlist generation. The general\napproach to these tasks is to select similar songs based on\nsome speciﬁc criteria. Commonly used criteria usually fall\ninto one of the following types: (i) meta-data based (e.g.\nsame artist, album, or genre), (ii) content-based (e.g. audio\nfeature similarity [7, 17], key, tempo [5]), and (iii) collabo-\nrative ﬁltering based (e.g. occurrence of the songs the the\nuser’s friends’ past playlists [1]). In our case, we dealt with\nsong excerpts rather than the whole song, i.e, songs are\nusually interrupted instead of being played to their end. As\na result, the required properties used to ﬁnd adjacent clips\nare stricter than that of ﬁnding adjacent songs. In addition\nto the global similarities between songs, local audio simi-\nlarities between song excerpts should also be considered to\nmeet the listeners’ expectations for the musical ﬂow.\nThe works focused on clip concatenation are often found\nin the studies of DJ tools [3, 9]. In those works, the clips\nare designated by users, and the authors deal with the con-\ncatenation issue only. Some studies of DJ tools also con-\nsidered both the issues of clip selection and clip concate-\nnation [12, 15]. However, in the aforementioned works,\n“rhythm similarity” and “beat alignment” are emphasized\nmost because the results are often used for dance. Since a\nmedley is still a kind of music composition, we focus more\non the chord euphoniousness of the adjacent clips in this\nwork.\nThere are also some works similar to us except that they\ntargeted at single song only [16, 25]. In these works, self-\nsimilarity matrix of audio feature sequences are used to\nsegment songs [16] or to ﬁnd cut points [25], and thus the\nmethods cannot be directly applied to our case. The cor-\nresponding computational complexity grows dramatically\nMusic al Dice Graph \nCons truction Song  Segment ation \nClip Clustering \nCluster Conn ecting Input: User’s Collection \nOutput:   \nPersonalized  \nMedle y \nMedle y Generation Clip Pasting Input: \nUser specified clips  \n& song  structure \nPath Finding  \nFigure 3 . The proposed system framework.\nas the number of songs increases. Besides, one can only\nﬁnd near-identical clips with self-similarity matrix. In our\ncase, clips are selected from different songs. If we group\nthe clips based on the same criteria as these works, the\nresulting medley would be composed only with excerpts\ntaken from the same song, violating the deﬁnition of a med-\nley. Therefore, a higher level feature – the chord sequence\nsimilarity is adopted in this work. Another similar work\nwas Bernardes et al.’s EarGram [4], but the unit they used\nto combine music is much shorter than us, i.e. as short as\na musical note (usually less than 1 second). Consequently,\nthe music piece produced by EarGram will not keep the\nphrases of the original songs. It becomes a totally new song\nrather than a combination of existing songs, and the main\ninteresting feature of medley songs – it is a combination of\ndifferent songs – is lost.\nThe work most related to ours is Lin et al.’s “Music\nPaste” [14]. They dealt with both the clip selection and\nthe clip concatenation issues in their work. However, they\ndid not consider phrase structures while ﬁnding connecting\npositions among clips. Consequently, clips might be cut in\nthe middle of a phrase, resulting in unsmooth transitions. In\naddition, in Lin et al.’ s work, the order of the clips is deter-\nmined only by matching similar short-time chroma features.\nIt is, therefore, difﬁcult to incorporate user’s preferences\ninto the process of clip ordering. In our work, it is easy\nto put users’ preference into consideration, and the chord\nprogression can be preserved better.\n3. PROPOSED FRAMEWORK\nThe proposed framework is inspired by the musical dice\ngame in the symbolic domain [18]. In a musical dice game,\nplayers throw dices to randomly “generate” new music from\nthe pre-composed interchangeable musical ﬁgures3at each\nbar. Similarly, in our system, we would like to generate\nmedleys by choosing the clip at a given position from a\nset of interchangeable clips. In order to achieve that, we\nﬁrst analyze the songs in the user-provided collection and\ncut the songs into self-coherent clips. Then, we group sim-\nilar clips into clusters. The clips in the same cluster are\nmusically similar to each other, and thus are assumed to\nbe interchangeable. We then connect clusters according\nto the transition probability calculated from clips’ connec-\ntivities in the songs they are extracted from. We term it\n3A short musical phrase [22].the “musical dice graph” in the rest of this paper. A path\non the graph is a version of a medley. Figure 2 shows an\nexample of a musical dice graph. With this graph, we can\ngenerate numerous medleys based on user’s preferences\nand the transition probability. Then, the aforementioned\ntwo issues in medley creation, “clip selection” and “clip\nconcatenation”, can now be turned into “musical dice graph\nconstruction” and “medley generation from the walk on\nthe graph”, respectively. Figure 3 illustrates the proposed\nframework. We will discuss the details in the following\nsections.\n4. MUSICAL DICE GRAPH CONSTRUCTION\nWe divide the construction of the musical dice graph into\nthree steps: song segmentation, clip clustering, and transi-\ntion probability calculation.\n4.1 Song Segmentation\nThe goal of song segmentation is to divide songs into self-\ncoherent segments. According to [24], phrasing is one of\nthe most important factors to be considered when concate-\nnating different tracks. Interruption happened in the middle\nof a musical phrase is just as unpleasant and unexpected as\nthat of an oral sentence in a conversation. Therefore, the\ntransition between clips should occur at the boundary of\nmusical phrases. The length of a musical phrase is ambigu-\nous. Here, we deﬁne a musical phrase as a self-coherent\nclip that sounds like ending, usually four or eight-bars long\nbecause pop songs usually take a thirty-two-bar form4. In\norder to identify the appropriate timing of transition, i.e.,\nthe boundary of musical phrases, we perform singing voice\ndetection on each song track. The reason is that most med-\nleys are composed of pop songs and that the boundary of\neach singing voice section often corresponds to that of a\nmusical phrase. As a result, we cut the songs into clips\nbased on the boundary of detected vocal segments, and use\nthem as the basic unit for creating medleys.\n4.1.1 Singing voice detection\nThe goal of singing voice detection is to recognize a given\naudio segment as vocal or instrumental. An instrumen-\ntal segment is deﬁned as a segment consisting of purely\ninstrumental sounds, e.g. the bridges and the intro. A vo-\ncal segment, on the other hand, is deﬁned as a mixture of\nsinging voice with or without background music, as was\ndeﬁned in [21]. Many studies have been done in the past\nfocusing on the task of singing voice detection. The typi-\ncal procedure performed to solve this problem is to extract\nshort-time audio features, and train a two-class classiﬁer\nto classify each frame of the audio into instrumental or vo-\ncal class. Common classiﬁers used are Gaussian Mixture\nModels (GMM), Hidden Markov Models (HMM) and their\nvariants, and Support Vector Machines (SVM). Frequently\nused features include Mel Frequency Cepstral Coefﬁcients\n(MFCC), Linear Prediction Coefﬁcients (LPC), and Zero-\nCrossing Rate (ZCR) [19]. Often, temporal smoothing tech-\n4also known as AABA form [22]niques are also performed to constrain the length of each\nvocal and instrumental segment afterwards to prevent over-\nsegmentation [23]. Here, we employ beat-synchronized\nMFCC as audio features and HMM as the classiﬁer. The\nidea of using beat-synchronized features comes from the\nphenomenon that voices are more likely to join the accom-\npaniment at beat times [13]. MFCCs are ﬁrst extracted\nduring the training phase. The MFCCs within a musical\nbeat act as the observed sequence for an HMM classiﬁer.\nThe beats are detected by using BeatRoot [6], a state-of-the-\nart beat-detection tool. In the testing phase, each beat in\na test song can then be classiﬁed as vocal or instrumental.\nConsecutive vocal/instrumental beats can then be group into\nvocal/instrument clips, i.e. the unit we used to create med-\nleys. In addition, in order to avoid over-segmentation, we\napply a median ﬁlter of 8 beats long (i.e. about 4 seconds)\nto the singing voice detection result.\n4.2 Clip Clustering\nAfter dividing songs into clips, we group these clips accord-\ning to the similarities among them. There are numerous\nmetrics for measuring similarity among clips, e.g. volume,\nrhythm, timbre, tonality, genre, etc.. Here we choose the\nchord sequence similarity because in the musical dice game,\nthe candidates of musical ﬁgures are often chord-similar or\ndominant-pitch-similar. The clip clustering process can be\ndivided into three steps: chord detection, chord sequence\nsimilarity computation and clustering.\n4.2.1 Chord detection\nFirst, we detect the chords of all songs with Harmony\nProgression Analyzer (HPA) [20], a state-of-the-art chord\nestimation system. The number of possible chords esti-\nmated is limited to 25, which are 12 major chords, 12 minor\nchords, and no-chord for the periods of silence. Then, beat-\nsynchronized chord sequences are extracted for each clip\nby aligning the chord estimation results with the detected\nbeats of the clips.\n4.2.2 Chord Sequence Similarity Computation\nWe then measure the chord sequence similarity between\neach clip pairs. A dynamic-time-warping (DTW)-like ap-\nproach proposed by Hanna et al. [11] is used in our system\nto measure the edit distance between two given chord se-\nquences. In order to better capture the harmonic relationship\nbetween the sequences, the substitution score used to cal-\nculate the edit distance varies with the consonance of the\ninterval between the two given chord roots, as Hanna et\nal. [10] proposed. Consonant intervals are the intervals that\nsound stable [22], and chords whose roots form a conso-\nnance are given higher substitution scores.\n4.2.3 Clustering\nGiven the similarity score between each pair of the chord\nsequences, we could then cluster the clips by hierarchical\nclustering. The vocal clips and the instrumental clips are\nclustered separately. Also, the clusters are categorized into\nthree types according to the positions of the clips in thesong. In other words, we would have six types of clusters in\ntotal: beginning, ending and middle clusters, each of which\ncan be either vocal or instrumental. Note that each cluster\ntype also contains several clusters.\n4.3 Cluster Connecting\nThen, we connect the clusters according to the transition\nprobability deﬁned as in Equation (1). For two arbitrary\nclusters AandB, the transition probability P(BjA)is de-\nﬁned as the proportion of clips in cluster Athat originally\nconcatenate with clips in cluster B, that is:\nP(BjA) =jSj\njAj; S=f(a; b)ja2A; b2[B\\N(a)]g\n(1)\nwhere aandbstands for an arbitrary clip, and N(a)is the\nset of clips that appeared next to clip ain the original song.\n5. MEDLEY GENERATION\nAfter the musical dice graph is constructed, we can then\ncompose medleys by ﬁnding a path on the graph and con-\ncatenating them according to the path.\n5.1 Path Finding\nNow we ﬁnd a path on the musical dice graph with the max-\nimum transition probability along the path. First, we picked\ncandidate clusters according to user-speciﬁed medley struc-\nture. For example, the user may designate the structure as\nI!V!I!V!I, where “I” and “V” stand for instrumental\nand vocal clips, respectively. Then, we choose clusters con-\nforming to the types the user speciﬁed in the structure as\ncandidate clusters. That is, instrumental-beginning clusters\nfor the ﬁrst clip slot, and vocal-middle clusters for the sec-\nond, and so forth, for the previous example. If the user has\nspeciﬁed the third slot to be clip a, then the cluster that clip\nabelongs to is chosen. Then, we use Viterbi algorithm [8]\nto ﬁnd a path of candidate clusters with maximal transition\nprobability, where the candidate clusters at each slot are\nregarded as the states in the algorithm. After determining\nthe clusters, we randomly select one clip per cluster since\nclips in the same cluster are interchangeable. The selected\nclips then compose the ﬁnal medley.\n5.2 Clip Pasting\nOnce the clips are selected along the chosen path on the\ngraph, we then concatenate them smoothly. For each se-\nlected clip, we adjust the tempi with Lin et al.’s method [14].\nThe algorithm takes just noticeable difference (JND) be-\ntween the tempi of two clips into account, adjusting the\ntempi gradually to lessen listeners’ discomfort. After ad-\njusting the tempi, the volumes of the clips are then nor-\nmalized, and the clips are ﬁnally concatenated by using a\nshort-length logarithmic cross-fade in between.\n6. EXPERIMENT\nIn this section, we conduct three experiments, two subjec-\ntive and one objective, to evaluate the three componentsof the system separately: the performance of the song seg-\nmentation method and the clip selection mechanism, and\nthe quality of the transition. The experimental data set con-\ntains 100 best-selling English songs from 1950s to 1990s\ncollected from Youtube5, and the genres include folk, pop,\njazz, musical and movie soundtrack, the lengths of which\nrange from one and a half minutes to ﬁve and a half min-\nutes. Two sets of annotation are built manually for each\ntrack: the singing voice annotation and the musical phrase\nannotation. The former was used to perform singing voice\ndetection, while the latter is used as the ground truth of\nboundary detection task. All songs in this dataset contain\nsinging and instrumental parts, i.e. none of them is a pure\ninstrumental nor A cappella music.\nMethod G-to-T T-to-G Pre. Rec. F-meas.\nGT 0.00 0.00 0.73 0.78 0.75\nGMM 2.85 10.29 0.20 0.17 0.18\nHMM 2.55 2.66 0.25 0.30 0.27\nDTM 2.38 5.13 0.23 0.18 0.20\nTable 1 . The song segmentation result.\n6.1 Effectiveness of Song Segmentation\nThis experiment measures the effectiveness of using singing\nvoice detection techniques to detect phase boundaries. We\ncompare three kinds of segmentation scheme: singing voice\ndetection with GMM and HMM classiﬁer (which will be\nreferred to as GMM and HMM later in this paper), and\nDynamic Texture Model (DTM), a state-of-the-art song\nsegmentation method proposed by Barrington et al. [2].\nWe evaluate the boundary detection performance by pre-\ncision, recall and F-measure, where a detected boundary\nis regarded as a “hit” if its time interval between the near-\nest true boundary lies within a certain threshold. We also\ncalculate two median time values: guess-to-true (G-to-T)\nand true-to-guess (T-to-G), which are the median time in-\nterval between each detected boundary and the closest true\nboundary and vice versa, as deﬁned in [2]. For singing\nvoice detection, all audio ﬁles are 22050 Hz-sampled, and\n26 MFCCs are extracted every 256 samples, overlapping\nby half. Singing voice detection by GMM and HMM were\nperformed respectively with 5-fold cross validation, and the\nnumber of mixtures of GMM and the number of hidden\nstates of HMM are both empirically set to 5. The hit time\ninterval was set to 0.5 second as in [2].\nThe results are presented in Table 1. The song seg-\nmentation performances using ground-truth singing voice\nannotation are also listed as a reference, denoted as GT.\nFrom Table 1, HMM and DTM outperform GMM obvi-\nously, while HMM performs approximately as good as\nDTM. In this work, we chose HMM over DTM for its\nsimplicity and speed during the singing voice detection test-\ning phase. Different parameters, such as the length of the\nmedian ﬁlter used to prevent over-segmentation, may inﬂu-\nence the performance, and therefore introduce a trade-off\n5Song names and URLs are listed at: http://goo.gl/khjtB .1234567\nA B C D E FMean Score  \nTest Pairs  proposed\nmusic paste [14]Figure 4 . The user evaluation results of clip concatenation.\nbetween precision and recall. In this work, the goal of seg-\nmentation is to avoid interruption in the middle of a phrase\nwhen pasting songs. Thus, when choosing between differ-\nent parameters, we would favour those results in higher\nprecision.\n6.2 Effectiveness of Clip Concatenation\nTo see whether segmentation really helped on song concate-\nnation, we compare 6 pairs of medleys. Each pair contains\ntwo medleys, and each medley is composed of two song\nclips. The two song clips used in the medleys of one test\npair are the same. For one medley in each pair, the transi-\ntion of clips happens immediately and directly at the end\nof a phrase of the ﬁrst clip (the proposed method). For the\nother medley, the transition point and cross-fade length are\ndecided with Lin et al. [14]’s method, which is the location\nwhere the short-term chroma features of two clips matched\nthe best and are in the middle of a phrase rather than the\nboundary. In this and the following experiments, we used\nhuman-labeled annotation for musical phrase information\nbecause we wish the results wouldn’t be biased by inac-\ncurate segmentations. The generated medleys used in the\nexperiments can be found in http://goo.gl/khjtB .\nWe ask 20 users to listen to and compare these test pairs.\nEach pair of medleys are played twice. Users are asked to\nreport how these two medleys sound according to transition\nsmoothness between two adjacent clips. All questions are\ndesigned on a 7-point Likert scale. Figure 4 shows the score\nof each test pair. In 5 out of 6 pairs, the mean score of the\nmedleys generated by the proposed method are higher than\nthe one created by Lin et al.’s method.\n6.3 Effectiveness of Clip Selection\nThis experiment is designed to measure the effectiveness\nof selecting clips based on their chord sequence transition\nprobability. The test set for this experiment contains 5 pairs\nof medley. The number of transitions and the structure of\nthe two medleys in a pair are the same, and the beginning\nand the ending clips of the medleys are speciﬁed to be the\nsame. For one medley in each pair, the chosen clips and\ntheir order are decided by the Viterbi algorithm along with\nthe calculated transition probability. For the other medley,\nclips are selected randomly.\nWe asked 17 participants to listen to and evaluate gener-\nated medleys. Each pair of medleys are played twice. Fig-\nure 5 illustrates the score of each test pair. We performed\n1234567\nA B C D EMean Score  \nTest Pairs  Viterbi\nRandomFigure 5 . The results of user evaluation on clip selection.\nFigure 6 . A screenshot of the proposed system.\npairwise t-test to analyze the results. Overall speaking, the\nmean score of medleys composed with Viterbi algorithm\nis signiﬁcantly higher than the one composed with random\nclip selection ( p <0:05). If the 5 test sets are evaluated sep-\narately, the mean score of medleys composed with Viterbi\nis signiﬁcantly higher than randomly generated one in 4\nout of 5 test pairs (Pair A, B, D, and E, p <0:05). The\nresult shows that the proposed clip selecting scheme is ef-\nfective in selecting clips that sound pleasant when they are\nconcatenated and played in sequence.\n7. USER INTERFACE\nThe quality of the generated medley is highly subjective and\ndepends on users’ tastes. For example, the actual boundary\nof a vocal phrase that fades out gradually is hard to deter-\nmine and there is no correct answer. The cross-fade length\nbetween two clips is also subjective given that some may\nprefer longer overlaps because of its smoothness, while\nothers may prefer shorter or no cross-fade to avoid blur\nof sounds. In order to better satisfy different user needs,\nwe developed a simple GUI, as shown in Figure 6. With\nthe interface, users could specify parameters, modify the\nsegmentation result and change the selected clips.\n8. CONCLUSION AND FUTURE WORK\nIn this work, we propose a framework for creating personal-\nized music medleys from users’ music collection. Borrow-\ning the idea of musical dice, we construct a music dice graph\nwith self-coherent clips that are segmented by singing voice\ndetection. After graph construction, the system is able to\ngenerate numerous medleys, automatically. The complete-\nness of phrases in a song is retained, and harmonic contentsare considered to improve the quality of the generated med-\nley. Since the quality of a medley is highly dependent on\nlisteners’ tastes, we also provide a simple feedback mech-\nanism that enables users to specify song clips, modify the\ntransition positions and the length of cross-fades to better ﬁt\ntheir preference. The objective experimental results demon-\nstrated that song segmentation performed with singing voice\ndetection is comparable with state-of-the-art methods. In\naddition, the preliminary subjective evaluations show that\nconnecting clips at phrase boundary acquired more user\npreference than previous works did. We also demonstrated\nthe proposed Viterbi-based algorithm is effective in select-\ning harmonious clips.\nMany aspects of our system can be improved. First,\nmore clip similarity measures can be introduced during clip\nclustering, e.g. rhythm, volume, genre, mood, etc.. Second,\nthe number of cluster types can be extend into , for example,\n“ﬁrst half verse”, “second half chorus”, “bridge”, etc.. Third,\nsong segmentation with singing voice detection restricted\nthis work to vocal songs. In the future, we will delve into\nother music segmentation methods that are able to recognize\nphrases in instrumental music. Finally, automatic separation\nof background music from foreground singing voice may\nenable us to create and add intermediate bridges, allowing\nmore ﬂexibility in medley generation.\n9. ACKNOWLEDGMENTS\nSpecial thank to Professor Jyh-Shing Roger Jang for con-\nstructive discussion with us, and the course members of\nMSAR for helping annotate the songs.\n10. REFERENCES\n[1]C. Baccigalupo and E. Plaza: “Case-based Sequen-\ntial Ordering of Songs for Playlist Recommendation,”\nLNCS , vol. 4106, pp. 286–300, 2006.\n[2]L. Barrington, et al. : “Modeling Music as a Dynamic\nTexture,” IEEE Trans. ASLP , vol. 18, no. 3, pp. 602–612,\n2010.\n[3] S. Basu: “Mixing with Mozart,” Proc. ICMC , 2004.\n[4]G. Bernardes, et al. : “EarGram : an Application for\nInteractive Exploration of Large Databases of Audio\nSnippets for Creative Purposes,” Proc. CMMR , June,\npp. 19–22, 2012.\n[5]L. Chiarandini, et al. : “A System for Dynamic Playlist\nGeneration Driven by Multimodal Control Signals and\nDescriptors,” Proc. MMSP , 2011.\n[6]S. Dixon: “Evaluation of the Audio Beat Tracking Sys-\ntem BeatRoot,” J. New Music Res. , vol. 36, no. 1, pp.\n39–50, 2007.\n[7]A. Flexer, et al. : “Playlist Generation Using Start and\nEnd Songs,” Proc. ISMIR , pp. 2–7, 2008.\n[8]J. G. D. Forney: “The Viterbi Algorithm,” Proc. of the\nIEEE , vol. 61, no. 3, pp. 302–309, 1973.[9]G. Grifﬁn, et al. : “Beat-Sync-Mash-Coder: a Web Ap-\nplication for Real-Time Creation of Beat-Synchronous\nMusic Mashups,” Proc. ICASSP , pp. 2–5, 2010.\n[10] P. Hanna, et al. : “On Optimizing the Editing Algorithms\nfor Evaluating Similarity Between Monophonic Musical\nSequences,” J. New Music Res. , vol. 36, no. 4, pp. 267–\n279, 2007.\n[11] P. Hanna, et al. : “An Alignment Based System for\nChord Sequence Retrieval,” Proc. JCDL , p. 101, 2009.\n[12] H. Ishizaki, et al. : “Full-Automatic DJ Mixing System\nwith Optimal Tempo Adjustment based on Measure-\nment Function of User Discomfort,” Proc. of ISMIR , pp.\n135–140, 2009.\n[13] Y . Li and D. Wang: “Separation of Singing V oice\nFrom Music Accompaniment for Monaural Recordings,”\nIEEE Trans. ASLP , vol. 15, no. 4, pp. 1475–1487, 2007.\n[14] H.-Y . Lin, et al. : “Music Paste: Concatenating Music\nClips Based on Chroma and Rhythm Features,” Proc.\nISMIR , Kobe, 2009.\n[15] Q. Lin, et al. : “Music Rhythm Characterization with Ap-\nplication to Workout-Mix Generation,” Proc. ICASSP ,\npp. 69–72, 2010.\n[16] Z. Liu, et al. : “Adaptive Music Resizing with Stretching,\nCropping and Insertion,” Multimedia Syst. , 2012.\n[17] B. Logan: “Content-based Playlist Generation: Ex-\nploratory Experiments,” Proc. ISMIR , pp. 2–3, 2002.\n[18] G. Loy: Musimathics , vol. 1, pp. 295–296,347–350,\nThe MIT Press, USA, 2006.\n[19] N. C. Maddage, et al. : “Content-based Music Struc-\nture Analysis with Applications to Music Semantics\nUnderstanding,” Proc. ACM MM , p. 112, 2004.\n[20] Y . Ni, et al. : “An End-to-End Machine Learning System\nfor Harmonic Analysis of Music,” IEEE Trans. ASLP ,\nvol. 20, no. 6, pp. 1771–1783, 2012.\n[21] T. L. Nwe, et al. : “Singing V oice Detection in Popular\nMusic,” Proc. ACM MM , p. 324, 2004.\n[22] D. M. Randel: The Harvard Dictionary of Music , Belk-\nnap Press, 2003.\n[23] L. Regnier and G. Peeters: “Singing V oice Detection\nin Music Tracks Using Direct V oice Vibrato Detection,”\nProc. ICASSP , pp. 1685–1688, 2009.\n[24] S. Webber: DJ Skills: The Essential Guide to Mixing\nand Scratching , Focal Press, 2007.\n[25] S. Wenger and M. Magnor: “Constrained Example-\nbased Audio Synthesis,” Proc. ICME , 2011."
    },
    {
        "title": "The Audio Degradation Toolbox and Its Application to Robustness Evaluation.",
        "author": [
            "Matthias Mauch",
            "Sebastian Ewert"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415862",
        "url": "https://doi.org/10.5281/zenodo.1415862",
        "ee": "https://zenodo.org/records/1415862/files/MauchE13.pdf",
        "abstract": "We introduce the Audio Degradation Toolbox (ADT) for the controlled degradation of audio signals, and propose its usage as a means of evaluating and comparing the robustness of audio processing algorithms. Music recordings encountered in practical applications are subject to varied, sometimes unpredictable degradation. For example, audio is degraded by low-quality microphones, noisy recording environments, MP3 compression, dynamic compression in broadcasting or vinyl decay. In spite of this, no standard software for the degradation of audio exists, and music processing methods are usually evaluated against clean data. The ADT fills this gap by providing Matlab scripts that emulate a wide range of degradation types. We describe 14 degradation units, and how they can be chained to create more complex, ‘real-world’ degradations. The ADT also provides functionality to adjust existing ground-truth, correcting for temporal distortions introduced by degradation. Using four different music informatics tasks, we show that performance strongly depends on the combination of method and degradation applied. We demonstrate that specific degradations can reduce or even reverse the performance difference between two competing methods. ADT source code, sounds, impulse responses and definitions are freely available for download.",
        "zenodo_id": 1415862,
        "dblp_key": "conf/ismir/MauchE13",
        "keywords": [
            "Audio Degradation Toolbox",
            "controlled degradation",
            "audio signals",
            "evaluation",
            "comparison",
            "robustness",
            "audio processing algorithms",
            "Music recordings",
            "degradation types",
            "real-world degradations"
        ],
        "content": "THE AUDIO DEGRADATION TOOLBOXAND ITS APPLICATION TO ROBUSTNESS EVALUATIONMatthias Mauch Sebastian EwertQueen Mary University of London, Centre for Digital Music{matthias.mauch, sebastian.ewert}@eecs.qmul.ac.ukABSTRACTWe introduce the Audio Degradation Toolbox (ADT) forthe controlled degradation of audio signals, and proposeits usage as a means of evaluating and comparing the ro-bustness of audio processing algorithms. Music recordingsencountered in practical applications are subject to varied,sometimes unpredictable degradation. For example, audiois degraded by low-quality microphones, noisy recordingenvironments, MP3 compression, dynamic compression inbroadcasting or vinyl decay. In spite of this, no standardsoftware for the degradation of audio exists, and music pro-cessing methods are usually evaluated against clean data.The ADT ﬁlls this gap by providing Matlab scripts thatemulate a wide range of degradation types. We describe14 degradation units, and how they can be chained to cre-ate more complex, ‘real-world’ degradations. The ADTalso provides functionality to adjust existing ground-truth,correcting for temporal distortions introduced by degra-dation. Using four different music informatics tasks, weshow that performance strongly depends on the combinationof method and degradation applied. We demonstrate thatspeciﬁc degradations can reduce or even reverse the perfor-mance difference between two competing methods. ADTsource code, sounds, impulse responses and deﬁnitions arefreely available for download.1. INTRODUCTIONAdvances in audio and music processing technology dependon the ability to evaluate the success of different methodsagainst each other, and to show in which ways they behavedifferently. Consequently, better evaluation methods andthe creation of new, bigger ground-truth data sets are top-ics of lively research and debate [1, 7]. Considerably lessattention is usually devoted to the question of whether theprocessing methods are robust against degradations in audioquality. For example, the robustness of automatic speechrecognition systems is often only tested by adding noiseor a limited set of additive environmental sounds [6], orPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page.c\u00002013 International Society for Music Information Retrieval.by ﬁltering the audio [11], e.g. using the FaNT toolkit1.Similarly, in music informatics only a few studies considerinsufﬁcient or variable audio quality, exceptions includingtwo studies on the inﬂuence of MP3 compression on genreclassiﬁcation [2] and onset detection [12], and a study ofonset detection methods under reverberation [21].Real-world applications often require robustness againsta broad range of signal degradations, as the quality of au-dio recordings is often entirely unknown. For example,mobile music-processing apps suffer from low quality mi-crophones in smartphones [3], in addition to noisy recordingenvironments and unpredictable non-linear processing inthe phones’ DSP chips. Similarly, guaranteeing a consistentaudio quality in a series of recordings is almost impossiblein exploratory ethnomusicology [13, 18]. Audio effects ap-plied at radio stations can lead to signiﬁcant challenges foraudio identiﬁcation methods. Many such applications arelikely to receive input audio of mixed levels of degradation.Therefore, studying the inﬂuence of various signal degrada-tions on a given method can aid the detection of problemsand the development of appropriate countermeasures.In this paper, we describe the Audio Degradation Tool-box, which offers a well-deﬁned way of applying a widerange of audio degradations, from basic sanity checks oversimple parameter sweeps to highly-speciﬁc degradationsimitating audio sourced from radio, smartphones or vinylrecords. The breadth of degradations goes beyond the ap-plication of noise and MP3 degradations currently used inthe literature. Using four typical music informatics tasksas an example, we show how the toolbox can be used toanalyse the behaviour of methods under degradation, com-pare the performance of competing approaches in variousscenarios, locate conceptual weaknesses in methods anddevelop suitable countermeasures. To facilitate integrationinto existing evaluation workﬂows, the toolbox providesmeans to adjust ground-truth annotations to compensatefor temporal distortions resulting from degradations. Thecombination of audio and ground-truth transformation func-tionality makes the toolbox an easy-to-use instrument toboost the informative value of systematic evaluations.The rest of the paper is organised as follows: Section 2describes the components of the toolbox itself and how theywork together. In Section 3 we examine the behaviour ofseveral music informatics methods under audio degradation.Section 4 summarises and concludes the paper.1http://dnt.kr.hs-niederrhein.de/download.htmlaudio_out = degradationUnit_addNoise(audio, samplingFreq)a) only audio processingparameter.noiseColor = ’brown’;[audio_out, timestamps_out] =degradationUnit_addNoise(audio, samplingFreq,timestamps, parameter)b) with timestamps and a parameter variableFigure 1: Example calls to a degradation unit function.2. THE AUDIO DEGRADATION TOOLBOXThe Audio Degradation Toolbox (ADT) consists of Mat-lab code for the controlled degradation of audio signals,and for the adaptation of ground-truth to the degraded au-dio. Additionally, some scripts are provided to facilitatebatch processing. The source code is hosted on SoundSoft-ware.org2and is freely available under a GPL license. TheADT is based on two concepts:(a) degradation unit:aMatlab function with optional parameters that performs asimple audio (and ground-truth) transformation,(b) degra-dation: parametrisation of several degradation units into achain. The remainder of this section details the rationale andimplementation of the degradation units and degradations.2.1 Degradation UnitsThe ADT contains 14 basic degradation units implementedas Matlab functions. All degradation units use the samecalling conventions.Figure 1a illustrates the simplest case,in which a degradation unit is called with two arguments:an audio data matrix (audio) and a scalar providing theaudio sampling frequency (samplingFreq); the functionreturns a matrix of degraded audio data (audioout) withthe same sampling frequency as the input audio.Some degradations change the speed of the audio orwarp time in other ways (see detailed descriptions, be-low) with the result that timestamps annotated with respectto the original audio will no longer be valid. In particu-lar, ground-truth annotated on the original audio would bemeaningless on such degraded audio. Therefore, all degra-dation units implement a timestamp transformation thatmaps the input timestamps to the timeline of the outputaudio. The transformed ground-truth can then be used forevaluation on degraded audio data. Figure 1b illustrates howthe user can provide additional timestamps (timestamps)and receive an output variable containing adjusted time-stamps (timestampsout). For convenience, all degrada-tion units have a default setting, but they will typically becalled with customised parameter settings, as in Figure 1b.More on parameters in Section 2.2.What follows are short descriptions of the individualdegradation units contained in the ADT. Degradation unitsthat introduce a time warping are marked with the letter W.More documentation can be found as comment headers inthe respective source code ﬁles.2http://code.soundsoftware.ac.uk/projects/audio-degradation-toolboxRoom Impulse Responses [20]:Great Hall, Classroom, Octagon.Smartphone Impulse Responses(recorded for the ADT: exponentialsine sweep, inverse ﬁltering):Google Nexus One microphone, Google Nexus One speaker.Vinyl(recorded for the ADT: inverse ﬁltered sine sweep output fromiZotope Vinyl 1.73b plugin): Vinyl Player 1960 (raw and smoothed)(a) Impulse ResponsesPub Sound Environment[9].Vinyl:recorded for the ADT using the iZotope Vinyl plugin.(b) SoundsTable 1: Data included in the ADT.Add Noise.Adds artiﬁcial random noise of different‘colours’ (brown, pink, white, blue, violet) to the signalat a speciﬁed signal-to noise ratio (SNR). Implemented aswhite noise plus ﬁlter.Add Sound.Adds an arbitrary signal to a given signal at aspeciﬁed SNR and takes care of sampling rate conversions(the ADT comes with two sounds, see Table 1b).Attenuation.Makes the signal quieter by a speciﬁed num-ber of decibels or a factor speciﬁed.Aliasing.Purposeful violation of the sampling theorem:the signal is down-sampled to a speciﬁed sampling ratewithout lowpass ﬁltering. The original sampling rate isrestored using a regular resampling method (with ﬁltering).Clipping.Normalises the audio signal such that a speciﬁednumber or percentage of samples is outside the interval[\u00001,1], and each resulting samplexis clipped tosign(x).Delay (W).Pads the beginning of the input audio with aspeciﬁed number of zero samples. The timestamps aredelayed accordingly.Dynamic Range Compression (W).Applies a signal-dependent normalisation to the audio signal, reducing theenergy difference between soft and loud parts of the signal.The implementation is adapted from [22], allowing speciﬁ-cation of several parameters including attack, release, delayand slope. As the delay parameter introduces a constanttime delay, the output timestamps are adjusted accordingly.Apply Impulse Response (W).Filters the signal usingone of six natural impulse responses (IR) provided withthe toolbox (seeTable 1a). As this process introduces anIR-dependent delay, timestamps are adjusted using the IR’smean group delay.High-pass Filter.Applies a linear-phase high-pass ﬁlterconstructed using the Hamming window method. Parame-ters are stop and pass band edge frequencies. The output iscropped to achieve zero phase, hence no time-warp.Low-pass Filter.Analogous to High-pass Filter.MP3 Compression.Compresses the audio data to an MP3ﬁle with a speciﬁed bit rate using the Lame encoder3. Theencoder and decoder delays are compensated for by alsousing Lame as the decoder.Saturation.Applies the transformationx sin(⇡x)mul-tiple times (as speciﬁed) to each samplex2[\u00001,1], whichdrives the sample towards the margins of -1 or 1, resultingin a simple imitation of a saturation effect.3http://lame.sourceforge.netfunction degr_cfg = demoDegradation()degr_cfg(1).methodname = ’degradationUnit_1’;degr_cfg(1).parameter.someParam1 = 3;degr_cfg(2).methodname = ’degradationUnit_2’;degr_cfg(2).parameter.someParam2 = 4;a) Specifying a degradation.audio_out = applyDegradation(’demoDegradation’,audio,samplingFreq);b) Applying a degradation to audio data.Figure 2: Demo degradation: speciﬁcation and application.Speed-up (W).The signal is resampled at a speciﬁed sam-pling rate but returned using the original sampling rate,which results in a speed-up (or slow-down). Timestampsare adjusted accordingly.Wow Resampling (W).Similar to Speed-up, but the re-sampling frequency is time-dependent: it oscillates aroundthe original sampling rate at a speciﬁed frequency and am-plitude, imitating non-constant speed in record players ortape machines. Timestamps are non-linearly warped tocorrespond to the output audio.These degradation units implement audio and ground-truth transformation and thus form the building blocks forhigher-level degradations, the subject of the next subsection.2.2 DegradationsA degradation is a chain of degradation units with ﬁxedparameters. The purpose of daisy-chaining degradationunits is to allow the creation of more complex degrada-tions than would be possible by using the degradation unitsalone. A degradation is deﬁned in a Matlab function thatacts as a conﬁguration ﬁle describing the order and pa-rameters of all degradation units used. An example isgiven inFigure 2a: the demo degradation speciﬁes a ﬁrstdegradation unitdegradationUnit1with the parame-tersomeParam1set to3, and a second degradation unitdegradationUnit2with the parametersomeParam2setto4. In this way, the audio processing chain can be preciselyspeciﬁed. Any degradation thus deﬁned can be applied toaudio using the Matlab functionapplyDegradationpro-vided by the ADT, seeFigure 2b. Based on the degradationname, given as the ﬁrst argument, the function retrievesthe degradation deﬁnition in the struct arraydegrcfgandcascades the degradation units in the speciﬁed order. Op-tionally, timestamp data can be supplied, which is alsosequentially transformed to match the output audio, whichis useful to make ground-truth usable on the degraded au-dio (seeSection 2.1). The ADT comes with a range ofpre-deﬁned degradations. For the purpose of this paper wefocus on the subset of six‘Real World Degradations’thatcover a variety of scenarios (more precise deﬁnitions comewith the ADT source code).Live Recording.Based on two degradation units: 1. ApplyImpulse Response, using an IR of a large room (‘Great Hall’,taken from [20] and included in the ADT), 2. Add Noise:adding light pink noise.Radio Broadcast.Based on two degradation units: 1. Dy-namic Range Compression at a medium level to emulatecorrect incorrect not identiﬁedOriginal10000Live 0 0 100Radio 3 3 94PhonePlay 0 1 99PhoneRec 5 7 88MP310000Vinyl 4 0 96Table 2: EchoNest audio ID results for 100 test songs.020406080100\ndB SNRcorrect●●●●●●●orig4030201050Figure 3: EchoNest audio ID results with pink noise.the high loudness characteristic of many radio stations,2. Speed-up, by 2%, which is commonly applied to mu-sic in commercial radio stations to shorten the music tocreate more advertisement time.Smartphone Playback.Based on two degradation unitssimulating a user playing back audio on a smartphone:1. Apply Impulse Response, using the IR of a smartphonespeaker (‘Google Nexus One’,Table 1a), which has a high-pass characteristic and a cutoff at⇡500Hz. 2. Add Noise,adding light pink noise.Smartphone Recording.Based on four degradation units,simulating a user holding a phone in front of a speaker:1. Apply Impulse Response, using the IR of a smartphonemicrophone (‘Google Nexus One’,Table 1a), 2. DynamicRange Compression, to simulate the phone’s auto-gain,3. Clipping, 3% of samples, 4. Add Noise, adding mediumpink noise.Strong MP3 Compression.Based on one degradationunit: MP3 Compression at a constant bit rate of 64 kbps.Vinyl.Based on four degradation units: 1. Apply ImpulseResponse, using a typical record player impulse response(Table 1a), 2. Add Sound, adding record player crackle(Table 1b), 3. Wow Resample, imitating wow-and-ﬂutter,with the wow-frequency set to 33 rpm (speed of Long Playrecords), 4. Add Noise, adding light pink noise.3. APPLICATIONSIn order to illustrate the insights that can be gained by usingthe ADT we evaluated several methods for standard musicinformatics tasks on suitable audio data. The results andbrief discussions are given below.3.1 Audio Identiﬁcation ServiceAs a proof of concept we used the free web-based audioidentiﬁcation (audio ID) service from the Song API4pro-4http://developer.echonest.com/docs/v4/song.htmlpercentage in 50ms window30405060708090100\n  Original  LiveRadioPhonePlayPhoneRecMP3VinylFigure 4: Score-to-Audio alignment accuracy underRealWorld Degradations. The boxes indicate the 1st, 2nd (me-dian) and 3rd quartiles, the whiskers extend to ‘the mostextreme data point which is no more than 1.5 times theinterquartile range’ (Rsoftware [19]).vided by the company EchoNest. A user can compute an au-dio ﬁngerprint using a dedicated program (ENMFP method)and query the EchoNest database for a corresponding identi-ﬁcation number, artist and track name. If the system cannotidentify a recording, that information is also returned.We queried the database using 100 original rock and popsongs5taken from commercial CDs as well as degradedversions using theReal World Degradationsdeﬁned in Sec-tion 2.2. The returned metadata was manually validated.The design goals for the EchoNest Audio ID service areclearly reﬂected in the results in Table 2: all 100 originaltracks are correctly identiﬁed, as were all 100 ﬁles withstrong MP3 compression. In contrast, all other degradationsled to a clear failure with at most ﬁve recordings identiﬁedcorrectly, and low precision with up to seven recordingsidentiﬁed incorrectly.6However, as illustrated in Figure 3,an additional test showed that the system is reasonably ro-bust against added pink noise up to a signal-to-noise ratio(SNR) of 10dB, for which 80 pieces were still correctlyrecognised (Figure 3). The poor real-world results are notsurprising, since the service is supposed todiscriminatebetween versions of the same song, not to detect similarsongs. By contrast, the music informatics tasks below areconcerned with the extraction of musical attributes thatshould persist even in degraded audio.3.2 Score-to-Audio AlignmentGiven a score and an audio recording for a piece of music,the aim of score-to-audio alignment is to ﬁnd, for every posi-tion in the score, the corresponding position in the recording.In contrast to the audio ID task, which assumes a particularrecording, the task is meant to work on any rendition orrecording of the same musical work, and hence we expect ahigher robustness against our real-world degradations. Weuse all 50 pieces from the Saarland Music Data [16], whichcontains audio recordings and corresponding MIDI ﬁles,both recorded using a Yamaha Disklavier. Our experimental5A list of ﬁles is available on the project’s website.6In all cases, the songs are still easily recognisable to human listeners.\nF measure0.00.20.40.60.81.0\n  Original  LiveRadioPhonePlayPhoneRecMP3VinylBeatRootDaviesFigure 5: Comparison of beat-tracking performance underReal World Degradations.setup is similar to the one described in [8]: the MIDI ﬁlesare temporally distorted by randomly changing their tempoin 10-second intervals by up to50%, faster or slower. Wecompute the alignment between the distorted MIDI ﬁles andthe original audio recordings using the method describedin [8], which combines chroma features with onset features.To measure the alignment accuracy, we computed for eachrecording the percentage of notes with an alignment errorof less than50ms for the onset position. The distribution ofthese values over all ﬁles is shown in the form of box-and-whisker plots in Figure 4.This score-to-audio method is more robust than theEchoNest audio ID retrieval. The median over all ﬁlesremains greater than90% for all degradations, with twoexceptions:LiveandSmartphone Playback. Here, it is in-teresting to investigate the underlying reasons. The methodemploys a relatively simple onset detector to reﬁne thealignment. The room IR used in theLivesetting containsseveral early reﬂections, which generates several closelylocated ‘copies’ of onsets. These can easily be confusedwith the original onset. In thePhone Playbackscenario,the signiﬁcantly lower performance might be a result of ap-plying the impulse response for the phone’s speaker, whichstrongly attenuates all frequencies below 500 Hz includingall fundamental frequencies up to B4. This leads to sub-stantial differences between the observed audio and audioexpected based on the score.3.3 Beat-trackingThe aim of beat-tracking is to automatically ﬁnd the time-stamps of all beats in a piece of music. We compare twobeat-trackers:BeatRoot7[5] andDavies[4] (QM VampPlugins implementation).BeatRootﬁrst estimates note on-set times, and forms a large number of tempo and beathypotheses based on these onsets. A multiple-agent archi-tecture is then used to determine the ﬁnal beat estimatesfrom the hypotheses. TheDaviesbeat-tracker does notdirectly work on onsets but uses a continuous mid-levelrepresentation of onset salience, on which a comb ﬁlter isused to calculate the salience of different beat periods andbeat alignments. Dynamic programming is used to retrieve7http://www.eecs.qmul.ac.uk/˜simond/beatroot, vers. 0.5.8relative correct overlap0.00.20.40.60.81.0\n  Original  LiveRadioPhonePlayPhoneRecMP3VinylChordinoHPAFigure 6: Chord detection performance:Real World Degra-dationsthe ﬁnal beat estimate. Due to its dependence on onsets wewould expectBeatRootto be particularly susceptible to theLivesetting (see Section 3.2).We prepared 180 songs by the Beatles by degradingthem using theReal World Degradations, resulting in 1260wav ﬁles. Beats were extracted with both beat-trackers. Weused a±70ms tolerance window to calculate theFmeasurefor every song against human annotated ground-truth [14].Figure 5 shows box-and-whisker plots of theFmeasuredistributions, by degradation and beat-tracking method. Forthe original audio and most of theReal World Degradations,both beat-tracking methods show good performance, withmedianFmeasures always exceeding 0.85. With similarmedians and inter-quartile ranges neither method has a clearadvantage. The obvious exception is theLive Recordingdegradation, where the medianFmeasure of both methodsis substantially lower.BeatRoot: 0.65 (original: 0.92);Davies’s: 0.77 (original: 0.94). As explained in the caseof score-to-audio alignment (Section 3.2), the likely causeare spurious onsets introduced by the impulse response; theDaviesbeat-tracker, which does not work on discrete onsets,is less affected.3.4 Chord DetectionChord detection is concerned with the transcription of thechord sequence in a piece of music. We test two differentchord detection tools:Chordino8[15] andHPA9[17].ChordinousesNNLS Chromaas a low-level feature, thenmatches manually deﬁned chord templates to the chroma.Chords are modelled as hidden states in a hidden Markovmodel, and smoothing is achieved using Viterbi-decoding.HPAuses the same basic architecture, with some distinctdifferences: a beat-quantised, perceptually-inspired chromarepresentation (HPA chroma); a more complex probabilisticmodel that involves key and bass context; machine-learnedchord proﬁles and transition parameters.We continue to use the 180 songs by the Beatles fromour beat-tracking experiment, as chord annotations are alsoavailable for them [10]. The chord detection outputs are8http://isophonics.net/nnls-chroma, Version 0.2.19https://patterns.enm.bris.ac.uk/hpa-software-package, Version 1.0\nrelative correct overlap0.00.20.40.60.81.0\n  Original  HP 50HP 100HP 200HP 400HP 800ChordinoHPAFigure 7: Chord detection performance:High-pass ﬁlterdegradations.evaluated by calculating the relative correct overlap forevery song using aMIREX-stylemajor/minor scheme [15].Figure 6 shows box-and-whisker plots of the song-wiseresults by degradation and method. For the original au-dio and most degradations,HPAconsistently outperformsChordino, possibly due to its advanced exploitation of musi-cal context and machine learning. Unlike the beat-trackers,both chord detection methods are relatively robust to theLive Recordingdegradation, with medians dropping lessthan 10 percentage points:Chordino0.74 (original: 0.80),HPA: 0.75 (original: 0.84). Instead, they falter on theSmartphone Playbackdegradation:Chordino0.67 (original:0.80),HPA: 0.36 (original: 0.84). In order to understandwhether this drop was caused by the degradation’s high-pass characteristic (compare Section 3.2), we calculatedﬁve further degradations using theHigh-pass Filterdegra-dation unit with the stop band edge parameter set to 50,100, 200, 400 and 800 Hz, respectively. Figure 7 shows thatthe methods react very differently. TheChordinomethodremains relatively robust with the lowest median, 0.73, fora 400Hz stop band edge. TheHPAmethod’s advantage overChordinois maintained for the 50Hz ﬁlter, but increasinglyfails for higher cutoff frequencies with median values of0.60 (200Hz), 0.29 (400Hz) and 0.05 (800Hz). In orderto locate the reason for the strong drop-off, we studied theHPAchroma feature.Figure 8shows an example of how thehigh-pass ﬁlter strongly affects the character of the feature,obfuscating the clear C major and A minor patterns.\nCDEFGABoriginal400Hz High-pass\ntimetimeFigure 8:HPA chromafor the original and a high-passﬁltered version of a snippet from ‘Misery’ by the Beatles.3.5 Summary of ExperimentsAs demonstrated by these examples, the ADT can be avaluable tool for investigating the robustness of music pro-cessing methods against a range of audio degradations. TheReal World Degradationset in particular helps to providean overview of the main problem areas for a method. Forexample, it has revealed that the otherwise excellent chorddetection methodHPAis not very robust to high-pass ﬁl-tering (see Section 3.4). This gives an interesting examplefor how evaluating audio processing methods under mul-tiple degradations can reveal their weaknesses and how itcould drive the development of improved algorithms. Whilethe ADT cannot replace ‘looking at the data’, experimentslike the ones conducted in Section 3 can lead to interestingdirections for further investigation.4. CONCLUSIONSIn this paper, we have introduced and described the AudioDegradation Toolbox. The toolbox is written in Matlab andconsists of 14 degradation units which can be chained intomore complex degradations. Several predeﬁned degrada-tions can be used to simulate real-world degradations. Withall required sounds, impulse responses and code modulesincluded, the toolbox is entirely self-contained. Existingground-truth can easily be transformed to account for timedistortions introduced by the degradations.We used the toolbox to analyse several methods in fourmusic informatics tasks: audio ID, score-to-audio align-ment, beat-tracking and chord detection. Our analysis offersinsights into the robustness of different methods in variousscenarios. In particular, we showed that a performance ad-vantage of one method over another can be substantiallyreduced or even reversed by quasi-real-world degradations.We demonstrate how to track down particular weaknessesand argue that using the toolbox in this way makes it apowerful tool for developing more robust audio technology.We are currently working on extending the toolbox withnew sounds and degradation deﬁnitions. Furthermore, weplan to conduct user studies to investigate whether the degra-dations we propose have an inﬂuence on the way a humanannotates an audio recording.Acknowledgements:Matthias Mauch is funded by a Royal Academy ofEngineering Research Fellowship, Sebastian Ewert is funded by EPSRCgrant EP/J010375/1.5. REFERENCES[1]J. Burgoyne, J. Wild, and I. Fujinaga. An expert ground-truthset for audio chord recognition and music analysis. InProc.12th Intl. Society for Music Information Retrieval Conf. (IS-MIR), 2011.[2]M. Casey, B. Fields, K. Jacobson, and M. Sandler. The effectsof lossy audio encoding on genre classiﬁcation tasks. InProc.124th Convention of the Audio Engineering Society, 2008.[3]V. Chandrasekhar, M. Shariﬁ, and D. A. Ross. Survey andevaluation of audio ﬁngerprinting schemes for mobile query-by-example applications. InProc. 12th Intl. Society for MusicInformation Retrieval Conf. (ISMIR), pages 801–806, 2011.[4]M. Davies and M. Plumbley. Context-dependent beat trackingof musical audio.IEEE Transactions On Audio Speech AndLanguage Processing, 15(3):1009–1020, 2007.[5]S. Dixon. Evaluation of the audio beat tracking system Beat-Root.Journal of New Music Research, 36(1):39–50, 2007.[6]C.-T. Do, D. Pastor, and A. Goalic. A novel framework fornoise robust ASR using cochlear implant-like spectrally re-duced speech.Speech Communication, 54(1):119–133, 2012.[7]J. Downie, A. Ehmann, M. Bay, and M. Jones. The Music In-formation Retrieval Evaluation eXchange: Some observationsand insights. InAdvances in Music IR. 2010.[8]S. Ewert, M. M¨uller, and P. Grosche. High resolution au-dio synchronization using chroma onset features. InProc.IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing(ICASSP), pages 1869–1872, 2009.[9]D. Giannoulis, E. Benetos, D. Stowell, and M. D. Plumb-ley. IEEE AASP CASA challenge: Public dataset for sceneclassiﬁcation task, 2012.[10]C. Harte, M. Sandler, S. A. Abdallah, and E. G´omez. Symbolicrepresentation of musical chords: A proposed syntax for textannotations. InProc. 6th Intl. Conf. on Music InformationRetrieval (ISMIR), pages 66–71, 2005.[11] H.-G. Hirsch and D. Pearce. The Aurora experimental frame-work for the performance evaluation of speech recognitionsystems under noisy conditions. InISCA Tutorial and Re-search Workshop (ITRW), 2000.[12]K. Jacobson, M. Davies, and M. Sandler. The effects of lossyaudio encoding on onset detection tasks. InProc. 125th Con-vention of the Audio Engineering Society, 2008.[13]P. v. Kranenburg, J. Garbers, A. Volk, F. Wiering, L. Grijp,and R. Veltkamp. Towards integration of music informationretrieval and folk song research. Technical Report UU-CS-2007-016, Utrecht University, 2007.[14]M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte,S. Kolozali, D. Tidhar, and M. Sandler. OMRAS2 metadataproject 2009. InLate-breaking session, 10th Intl. Conf. onMusic Information Retrieval (ISMIR), 2009.[15]M. Mauch and S. Dixon. Approximate note transcription forthe improved identiﬁcation of difﬁcult chords. InProc. 11thIntl. Society for Music Information Retrieval Conf. (ISMIR),pages 135–140, 2010.[16]M. M¨uller, V . Konz, W. Bogler, and V . Ariﬁ-M¨uller. Saarlandmusic data (SMD). InLate-breaking session, Intl. Society forMusic Information Retrieval Conf. (ISMIR), 2011.[17]Y. Ni, M. McVicar, R. Santos-Rodriguez, and T. De Bie. Anend-to-end machine learning system for harmonic analysis ofmusic.IEEE Transactions on Audio, Speech, and LanguageProcessing, 20(5):1771–1783, 2012.[18]P. Proutskova and M. Casey. You call that singing? Ensembleclassiﬁcation for multicultural collections of music recordings.Proc. 10th Intl. Conf. on Music Information Retrieval (ISMIR),pages 759–764, 2009.[19]R Core Team.R: A Language and Environment for StatisticalComputing. R Foundation for Statistical Computing, 2013.[20]R. Stewart and M. Sandler. Database of omnidirectional andb-format room impulse responses. InProc. IEEE Intl. Conf.on Acoustics Speech and Signal Processing (ICASSP), pages165–168, 2010.[21]T. Wilmering, G. Fazekas, and M. Sandler. The effects of rever-beration on onset detection tasks. InProc. 128th Conventionof the Audio Engineering Society, 2010.[22]U. Z¨olzer, editor.DAFx—Digital Audio Effects. John Wiley &Sons, 2002."
    },
    {
        "title": "JProductionCritic: An Educational Tool for Detecting Technical Errors in Audio Mixes.",
        "author": [
            "Cory McKay"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416180",
        "url": "https://doi.org/10.5281/zenodo.1416180",
        "ee": "https://zenodo.org/records/1416180/files/McKay13.pdf",
        "abstract": "jProductionCritic is an open-source educational framework for automatically detecting technical recording, editing and mixing problems in audio files. It is intended to be used as a learning and proofreading tool by students and amateur producers, and can also assist teachers as a timesaving tool when grading recordings. A number of novel error detection algorithms are implemented by jProductionCritic. Problems detected include edit errors, clipping, noise infiltration, poor use of dynamics, poor track balancing, and many others. The error detection algorithms are highly configurable, in order to meet the varying aesthetics of different musical genres (e.g. Baroque vs. noise music). Effective general-purpose default settings were developed based on experiments with a variety of student pieces, and these settings were then validated using a reserved set of student pieces. jProductionCritic is also designed to serve as an extensible framework to which new detection modules can be easily plugged in. It is hoped that this will help to galvanize MIR research relating to audio production, an area that is currently underrepresented in the MIR literature, and that this work will also help to address the current general lack of educational production software.",
        "zenodo_id": 1416180,
        "dblp_key": "conf/ismir/McKay13",
        "keywords": [
            "open-source educational framework",
            "automatically detecting technical recording",
            "audio files",
            "learning and proofreading tool",
            "timesaving tool",
            "Baroque vs. noise music",
            "extensible framework",
            "MIR research",
            "audio production",
            "educational production software"
        ],
        "content": "JPRODUCTIONCRITIC : AN EDUCATIONAL TOOL FOR \nDETECTING TECHNICAL ERRORS IN AUDIO MIXES  \n  Cory McKay    \n  \n  Marianopolis College and CIRMMT  \ncory.mckay@mail.mcgill.ca    \n  \nABSTRACT  \njProductionCritic is  an open -source  educational \nframework for automatically detect ing technical \nrecording , editing  and mixing  problems  in audio file s. It \nis intended to be used as  a learning  and proofreading tool \nby students and amateur producers, and can also assist \nteachers as a timesaving tool when  grading  recordings.  \nA number of novel error detection algorithms are \nimplemented  by jProductionCritic . Problems  detected \ninclude edit errors , clipping, noise infiltration, poor use of \ndynamics , poor track balancing, and many others . \nThe error detection algorithms are highly configurable , \nin order to meet the varying  aesthetics of different \nmusical genres  (e.g. Baroque  vs. noise music). Effective \ngeneral -purpose default settings were developed based on \nexperiments with a variety of student pieces , and these \nsettings were then validated using a reserved set of \nstudent pieces . \njProductionCritic  is also designed to serve as a n \nextensible  framework to which new detection modules \ncan be easily plugged in . It is hoped that this will help to \ngalvanize MIR research relating to audio production, a n \narea that is  currently underr epresented in the MIR \nliterature , and that this work  will also help to address the \ncurrent general lack of educational  production  software . \n1. INTRODUCTION  \nAudio production is a broad field that essentially involves  \nrecording and  creating music. Important  aspects include:  \n Recording : configuring an acoustic environment, \nmicrophone selection, microphone placement, etc.  \n Editing:  shifting segments of audio in time within a \ntrack, or moving them between tracks.  \n Mixing:  combining multiple tracks with appropriate \ngains, panning and EQ settings, applying effects, etc.  \n Synthesis:  artificially generating audio.  \n Sampling:  incorporating  pre-existing  audio.  \n Mastering : preparing a mix for final distribution via \nspecific  audio formats . DAW (Digital Audio Workstation) software  has come \nto play a central role in production . Such software ranges \nfrom  recording -oriented  tools like Avid Pro Tools , to live \nperformance -oriented softwar e such as Ableton Live, to \nfree too ls like Audacity.  \nImprove d functionality, better user interfaces and \ndecreasing costs have made audio production more and \nmore  accessible  in recent years . This has helped  to cause  \nan explo sion of content created in home s tudios, ranging \nfrom amateur  mashups to recordings by professional \nmusicians . While this has certainly resulted in a great \ndeal of interesting music, it has also led to error -prone  \ntechnical work on the part of overconfident amateur \nproducers  who lack  the professional training that was \npreviously necessary to be involved in production  at all.  \nThis problem is part of the  motivation behind  the \njProductionCritic  software, which automatically  detect s \ntechnical production errors, especially  those relating to \nediting and mixing . It can help students and amateur \nproducers  check their work for unnoticed errors, much as \none might use a grammar checker  when writing prose . \nThis is beneficial from an educational perspective, as it \nteaches  users to notice problems that  they might  not \notherwise have known to look for . This in effect trains \nthem to improve their listening skills, which are arguably \na producer’s greatest asset, and pushes them to learn how \nto avoid or correct the detected problems , thus  improving  \nnot only their current work, but also the skills that they \nwill be able to apply in the future . \nSuch e rror checking software is also useful to those \nteaching audio production, as it can greatly facilitate \ngrading . While it would  certainly  be ill -advised to rely \nexclusively on automated marking, as expert humans are \nneeded to fully evaluate the  difficult -to-quantify \naesthetics involved in  the art of production, simply \nautomating the painstaking task of enumerating  and time -\nstamping  basic technical errors can be a gre at time saver.   \nFinally, error checking software could even be of  some  \nuse to professional audio engineers as a final verification \ntool, just as professional writers make use of \nspellcheckers. It is not unheard of to find  technical errors \nin professional w ork, often due to rushed production  \nschedules and the high costs of studio time . \nAside from such practical benefits, developing \nalgorithms for detecting production errors can also ha ve \nimportant research value. As discussed  in Section 2, not  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2013  International Society for Music Information Retrieval    \n \nonly is there currently a surprising dearth of production -\noriented research in the MIR community  specifically, but \nthose tools that do exist in the gener al audio world tend to \nbe closed -source black boxes or based  on disappointingly \nnaïve algorithms. This presents an ex citing research \nopportunity, particularly considering the importance of \nproduction from both commercial and artistic \nperspectives . jProductionCritic has  therefore  been \ndesigned not only as a ready -to-use application, but also \nas a modular framework for dev eloping and deploying \nnew error detection and analysis algorithms in the future.  \n2. RELATED RESEARCH  \nCommercial  DAW software like Pro Tools  tends to  offer  \nsome basic error detection functionality, and extensive \nadditional functionality can be added via plug-ins. \nUnfortunately, with certain important exceptions, such \nfunctionalit y tends to be relatively simplistic in \nimplementation and based on proprietary closed -source \ncode , making it expensive to use and difficult to extend . \nAlso, such software tends to emphasize  correcting \nproblems rather than detecting where they occur (the \nlatter is not always necessary for the former), something \nthat is of limited  educational value. Finally , DAW  \napplications  and plug-ins tend to address  specific \nproblems independentl y, with limited  functionality for \npresent ing errors to users via integrated interface s. \nMoving outside the domain of DAW software, there \nare a few commercial integrated error detection systems , \nsuch as  Fraunhofer IDMT’s A/V Analyzing Toolbox  [4] \nand Quadriga Audiofile -Inspector [12]. Unfortunately, as \nwith commercial DAWs , such  software is closed -source \nand thus difficult  for independent researchers to extend . \nThis software is also limited in the range of errors \ndetected and in the sophistication of its processing.  \nIn terms of open -source  integrated  systems, the very \nbasic Digital Audio Error Detection  [13] is the only one \navailable . A few open -source error detection DAW plug-\nins can also be found , but they are isolated algorithms \nthat each only look f or individual errors, and have no \nintegration with one another.  Furthermore, they are \nlargely intended for professional use, and typically \nrequire a significant amount of knowledge  to use , limiting \ntheir usefulness in educational contexts.  \nWith respect to research in the MIR community, \nsurprising ly little work has been done relating directly to  \naudio production, even though many  of the audio features \nand metrics used in MIR research  are highly  relevant  to \nthis domain . Scott and Kim [9] and Montecchio and Cont \n[6] provide  good  examples of the kind of  production -\noriented MIR research has been done , but even this  high-\nquality work focuses on automating production tasks \nrather than finding errors. Such automation is certainly \nvery useful in practice, but it do es not address the \neducational needs emphasized by jPoductionCritic . There has been a substantial  amount of research  done \noutside the MIR  community on detecting errors in audio \nsignals. However, this focuses  mainly  on techniques \nassociated with specific  problems rather than general \nintegrated systems. Furt hermore, much of th is research \nrelates  to domains such as broadcasting and audio \ncompression, with less focus on production -oriented \nproblems, and with almost no attention paid to addressing \nthe issue from  an educational perspective. Having noted \nthis, there are many technical papers than can each be \nvery useful i n detecting specific production  problems, \nparticularly in  AES (Audio Engineering Society) and \nIEEE (Institute of Electrical and Electronics Engine ers) \npublications . There are also a number of important \ngeneral references on audio production, including books \nby Barlett and Barlett [1], Vaseghi [10], Owsinki [7] and \nHuber and Runstein [3], the first of which include s a \nparticularly  useful chapter  on the kinds of defects that one \ncan encounter  in an improperly prepared mix.  \n3. DESIGN AND FUNCTIONA LITY  \nThe first main  design  objective of jProductionCritic is \nthat it be useful and accessible to music students, amateur \nproducers and teachers, all of whom may have little or no  \nexperi ence with software development. To this end, \njProductionCritic is distributed with a detailed manual in \norder to make it as easy to learn as possible. Its basic \ninterf ace is also designed to be minimalistic and direct so \nthat users can avoid  being distracted by anything \nsuperfluous. Users simply need to specify an audio file or \nbatch of files to check  and where reports are to be saved , \nand the software automatically tak es care of the rest . \nOf course, it is also important that the software be  \nhighly configurable for those who desire flexibility . \nThere is therefore a separate  extensive configuration file  \nthat advanced users can modify in order to control  which \nerrors are c hecked for, what error thresholds are used for \neach error, and so on.  It is thus  possible to customize \njProductionCritic for certain styles of music (e.g. metal  \nvs. jazz ), or to simply use the provided general -purpose \ndefault settings without worrying abou t the details.  \njProductionCritic  processes final mixes in the form of \nsingle mono or stereo  files rather than DAW projects with \ntracks still separated out. Although this does make certain \nerrors much harder to detect, it also ensures  that no new \nunchecked  errors are incorporated during final mixing \nand mastering. This also makes it possible to process any \nstandard audio r ecording with jProductionCritic, and \navoids tying it to any particular DAW framework . \nThree  types of error reports can be  generated  by \njProductionCritic  for each audio file . The first is simply \nan enumeration  of the errors that were detected , annotated  \nwith time stamps indicating either an instantaneous time \nor time range , as appropriate. E rrors are also marked as \nbeing mild, moderate or severe .    \n \nThe second type of report consists of a series of \nAudacity Label Tracks, one for each type of time -specific \nerror. These are metadata tracks that Audacity displays \nalongside audio and MIDI tracks. This can be very useful \nfor visually demonstrating  to users where errors occur in \na waveform  or spectrogram . Audacity was chosen in \nparticular because it is fr ee and thus accessible to  all \nusers , whether or not they used it to prepare the audio  \nbeing checked for errors . \nThe third type of error report cons ists of error \nannotations in Weka ARFF  [11] or ACE XML  [5], two \nfile formats related to  machine learning that are  used by \nthe MIR community. Although not directly applicable to  \nthe educational context  targeted  by jProductionCritic, \nthese reports  could be h elpful  to MIR researchers who \nmight want to use the output of jProductionCritic in \nresearch involving machine learning. These formats can \nalso be useful when performing experimental validations.  \njProductionCritic is implemented in  Java, in order to \nhelp make it as cross -platform and accessible  as possible. \nThis avoids forcing users to buy proprietary \nenvironments such as Matlab, and avoids  the installation \nand linking problems that one can encounter with  \nlanguages like C++.  \nThe second main design objective  of jProductionCritic \nis that it serve as a framework under which new error \ndetection algorithms can be developed and deployed , and \nnot only as a ready -to-use application . This is an \nimportant priority in encouraging future MIR research \nand development foc using on audio production. A strong \nemphasis was therefore put on designing \njProductionCritic using a modular and highly extensible \narchitecture to which new error checking algorithms c an \nbe easily added as plug -ins, with virtually no changes \nneeded to the  overall jProductionCritic  processing  \ninfrastructure.  Special attention was  also paid to \nextensively documenting the code . \njProductionCritic  is distributed as an integrated part of \nthe jMIR  [5] suite of MIR research software. This allows \nresearchers to easily combine jProductionCritic’s \nfunctionality with other jMIR components, such as the \njAudio feature extractor or the ACE meta -learning \nsystem.  \nAs with all jMIR components, jProductionCriti c is free \nand open -source .  \n4. TECHNICAL ERRORS ASSESSED  \nDue to limited space, only broad overviews of \njProductionCritic’s main error detection algorithms are \nprovided in the sub -sections below. Those wishing to \nread more details on any particular algorithm a re \nencouraged to view the jProductionCritic manual or the \nJava class associated with the error type, both of which \nare available at http://jmir.sourceforge.net . \nIt is important to emphasize here that there are many \nimportant subtle subjective and artistic qualities that must be considered if one is to truly evaluate the production \nquality of a mix. Performing such an evaluation is well \nbeyond the current technological capabilities of any \nautomated system, and is best left to human experts, such \nas professio nal producers  and instructors . \njProductionCritic  therefore only attempts to detect  \nclear objective technical errors , which many students and \namateurs can still unfortunately produce many  of. \njProductionCritic is intended as a supplement and aid to \nhuman ex perts, not as a replacement for them.  \nOf course, even with this policy there can still be \nambiguity with respect to certain error types. What might \nbe considered unwanted noise in a classical flute \nrecording, for example, might be part of a desirable \nprodu ction aesthetic in a flute sample used in an \nelectronic dance track. Fortunately, jProductionCritic’s \ndiverse range of configuration settings make s it possible \nto easily modify the detection thresholds of given  error \ntypes, or to disable them entirely, in order to match the \nvarious  production aesthetics of different musical styles.  \n4.1 Digital Clipping  \nDigital clipping occurs when a signal exceeds the \nrepresentational limits of its bit depth. Clipped signals are \ncharacterized by flat peaks and troughs, as samples  are \nrounded  to maximum and minimum values . Digitally \nclipped signals sound rough and distorted, and are almost \nnever aesthetically desirable . Analog clipping, in contrast, \ncan be desirable in certain styles of music, and is \ncharacterized by more cu rved peaks and troughs.  \nDigital clipping tends to occur in two main ways in \nstudent work : either the  gain is set too high during \nrecording or synthesis  of an individual  track, or the gains \non individual tracks mixed onto the same channel are too \nhigh, such  that the combined  signals clip, even if none of \nthe source signals are themselves clipped  individually . \nAlthough clipping detection  is a common software \nfeature , the popular  implementation  of simply flagging \nany samples at the representational limits  is surprisingly \nnaïve . This approach has two major problems. Firstly, a \nsample that actually should have  a value at the \nrepresentational limit is not in fact clipped, and such \nsamples are to be expected in normalized signals. \nSecondly, students may attempt to hide clipping by \nreducing the master gain in the final mix, such that \nsample values fall below representational limits (and are \nthus not flagged) but the signal distortion caused by the \nclipping remains . \nThe approach used by jProductionCritic can overcome \nthese two problems: if a number of  adjacent samples \nbeyond a threshold have an identical signal value  \n(whether or not it is at the representational limit) , then \nreport  clipping. The number of such consecutive samples \ngives an indication of clipping severit y.  \nDespite its simplicity  and effectiveness , other uses of \nthis technique were  not found in the literature , although   \n \nrelated counting techniques at the representational limit \nhave been used . It should be noted that the literature also \ninclude s spectral approaches for detecting clipping, but \nthese can be too sensitive for styles of music where \nanalog clipping (or its digital simulation) is desirable.  \n4.2 Edit Clicks  \nAn edit click occurs when an improperly treated edit is \nmade , and  can result in a discontinuity in the waveform \nthat typically sounds like a click. This can happen when \ntwo signals are spliced together, or at the beginning s and \nends of tracks (due to a sudden jump in the signal from or \nto silence). Although there are a number of techniques \nthat can be used to avoid edit clicks, students and \namateurs often neglect to use them.  \nAlthough the literature include s a substantial number \nof techniques for detecting instantaneous noise like clicks \nin general, it largely neglects e dit clicks in particular. \nThis is problematic from an educational perspective, as it \nis useful for students to know where imperfections in \ntheir work come from.  \njProductionCritic uses a simple technique  to detect edit \nclicks based on windows of only four s amples : report  an \nedit click if a signal jumps in value beyond a  threshold \nfrom samples 2 to 3 , but does not change in value beyond \nanother threshold when progressing from samples 1 to 2 \nor 3 to 4 . This approach is sensitive to improperly \nexecuted edits, i s relatively impervious to false positives , \nand can also provide a severity measurement . This \ntechnique is also surprisingly absent from the literature, \nalthough related techniques considering much broader \nspreads than four samples are  used for detecting \ninstantaneous  noise in general.  Clicks at the beginning s \nand ends of track s are simply found by looking for first \nand last samples far from zero, respectively.  \nIt should be noted that this algorithm  focuses only on a \nparticular kind of edit error . It does not detect edit errors \nin general, of which there are many  other  types  (e.g. a \nsplice  involving two segments of audio recorded under  \nvery different reverberant conditions ). \n4.3 Other Clicks, Pops and Instantaneous Noise  \nThere are also many other types of undes irable \ninstantaneous noise . Plosive pops due to the improper \nmicing of a singer or noise when a needle jumps on a \nrecord are just two examples amongst many.   \nAlthough , there are a number of established techniques \nfor detecting such problems , many of them t end to \nproduce false positives . jProductionCritic’s approach , \nwhich also produces some false positives but  was still \nfound to be  the most effective during comparative \nexperiments,  is to high -pass filter the signal (due to the \ncommon assumption that unwante d noise will stand out \nmost clearly against the musical signal in the high \nfrequency range) and look for sudden and unusual peaks \nin the filtered signal’s spectral flux.  4.4 Hums and Other Background Noise  \nTracks  can also be infiltrated by  various types of  \nsustained noise (as opposed to the more sudden and \nshort -lived types of noise discussed above). Ventilation \nsystems in recording environments and faulty  cable \nshielding are two of the many possible sources. Detecting \nsuch noise in general can be particularly difficult, as it \ncan be hard to distinguish from the musical signal . \nAlthough the literature does include certain sophisticated \ntechniques, including approaches based on Hidden \nMarkov models  [8], these tend to be too limited in the \nstyles of music to which  they can be applied, so it was \ndecided to use  a simpler and more general technique11 . \njProductionCritic’s basic  approach is to calculate the \npower spectrum of the audio and look for sustained peaks \nin particular frequency regions that are present in all o r \nmost of the audio. Extra weighting is applied  if these \npeaks are still present in otherwise quiet parts of the \nsignal. This approach tends to work reasonably well for \ndetecting loud noise, but can miss quieter  noise, and can \nresult in false positives for those styles of music that \nfeature sustained drones.  \njProductionCritic also has  speci alized  detector s that \nlook for electrical noise (e.g. ground loops), a common \nproblem in imperfectly configured or used studi os. Such \nnoise  consists of  a hum at the AC frequen cy of the power \nsupply  (and its  integer  multiples ), which is generally \neither 50 Hz or 60 Hz, depending on where one is. \n4.5 Phasing  \nPhasing is a problem that occurs when a signal is mixed \nwith another signal t hat includes a phase delayed version \nof itself . This can occur, for example, when two \nomnidirectional microphones mapped to the same \nchannel are too close to each other, or a single \nmicrophone is too close to an acoustically reflective \nsurface. This result s in cancellation or reinforcement of \nvarious  frequencies, depending on the phase offset, which \ncan result in  a muddy tone. \nAlthough the literature specifies several  effective ways \nto detect phasing before mixing is carried out , it is much \nmore difficult t o automatically detect afterwards , and is \neasily confused with sometimes desirable comb filter \neffects like flanging . jProductionCritic’ s (admittedly \nlimited)  approach is  to look for consistent troughs in the \npower spe ctrum of a track.  \n4.6 Dynamic Range  \nA com mon mistake  made by  students is to keep gains \nexcessively  low due to fear of  clipping. Students  then \nsometimes exacerbate this by forgetting  to normalize \ntheir work  during mastering  (which can be desirable in \norder to achieve relatively consistent volumes ). Another  \npotential  problem is that some tracks are insufficiently \ndynamically compressed (a desirable “hot” aesthetic in   \n \npop styles) or, conversely, do  not have enough dynamic \nrange  (a problem for styles such as classical music).  \nTo address the first issu e, jProductionCritic reports an \nerror if the maximum absolute sample value is too far \nbelow the representational maximum. To address the \nother two problems, optional style -specific configuration \nsettings can be specified to generate errors if the standard \ndeviation of the windowed RMS across a track is too high \nor too low, respectively.  \n4.7 Stereo Balance and Channel Similarity  \nSome students do not include  enough channel separation \nin their recordings to create a sufficient sense of stereo \nspace , or even forget  to specify panning settings at all . \nAdditionally, students  sometimes fail to  properly  balance \nthe stereo channels , with the result  that one stereo \nchannel is consistently louder than the other . \njProductionCritic compares the left and right stereo \nchannels  and generates an error if  the signal correlation is \ntoo h igh. It was found that this works  better in general \nthan spectral approaches. An error is also generated if the \nRMS of one channel as a whole is too high relative to the \nRMS of the other channel as a whole.  \n4.8 Other Errors Assessed  \nThere are several  additional  errors that can be reported  by \njProductionCritic  if desired . These include , among others : \n Too much silence  (either absolute or at the noise floor)  \nat beginnings and ends of tracks . \n Audio dropout.  \n DC signal offset.  \n Poor encoding parameters (e.g. low sampling rate or bit \ndepth , lossy compression, etc. ) in cases where high-\nquality  masters should be used.  \njProductionCritic also report s basic summary meta data \n(e.g. track length , aud io encoding parameter s, etc.) . \n5. VALIDATION EXPERIMEN TS \nMuch  of the error detection processing described above  is \nbased on thresholds , which the user has the option of \nspecifying  via configuration settings . However, it is \nimportant that it also be possible to apply \njProductionCr itic easily and effectively  to arbitrary types \nof music without any user tweaking. To this end, \nexperiments were performed to first arrive at good default \nconfiguration settings , and to then  validate the se settings’  \neffectiveness.  \nIn order to do this, musi c technology assignments were \ncollected  from  multiple sections of  three different courses \nover four semesters at Marianopolis College . Most but \nnot all of the students involved were enrolled in the music \nprogram.  Some of these  assignments required students to \nmake classical or jazz recordings using Pro Tools  (in \nstudio and live) , and others  required students to make mashups in any musical style using Audacity. The \ninstructor’s original (and later re-verified) corrections to \nthe assignments served as the ground truth. In all, 110 \nassignments were collected.  \nForty -four of these assignments were randomly \nselected and used to  experimentally  choose  the error \ndetection algorithms  and tune their configuration settings . \nOnce this was done, the remaining 66 assignments were \nthen processed by jProductionCritic in order to verify that \nthe configuration settings had not been overfitted to the \ntuning set. The results of this validation experiment are \nshown in Table 1 :  \n \n True \nPositives  False \nPositives  False \nNegatives  \nHuman  499 0 8 \njPC 452 38 55 \nTable 1. Results of the validation experiment comparing  \njProductionCritic ’s performance  with expert human \ncorrect ion. Values indicate  the total number of errors \ndetected combined across all 66 validation assignments.  \nIt is interesting to note that 8 true technical errors were \ndetected by jProductionCritic  that were wrongly missed \nduring original human correction (they were found to be \ntrue errors upon manual secondary verification).  It was \nalso found upon secondary verification that , unlike \njProductionCritic,  the o riginal corrector did not wrongly  \nindicate any  false errors.  \nOverall, it can be seen th at jProductionCritic \nperformed quite well. It found 8 9% of the true errors, \ncompared with 98% found by the  expert course \ninstructor . Furthermore, 92 % of the errors detected by \njProductionCritic  were in fact true errors. This is \nimpressive when one recall s that the assignments were in \na variety of musical styles, and were all processed using \nthe same default configuration settings.  It should be \nnoted , however, that these results would be  even  more \nmeaningful if student s at different  institutions with \ndifferent instructors had been involved in the study.  \nWith respect to the relative  performance of the \ndifferent error types, the algorithms for detecting phasing, \nbackground noise and, to a lesser degree, instantaneous \nnoise (other than edit clicks) were by far the worst \nperformers. It was difficult during the tuning stage to find \nconfiguration settings for them that would minimize false \npositives while also maximizing true positives , and this \nwas reflected in the validation stage, where these three \ntypes of error detectors were res ponsible for 73 % of all \njProductionCritic’s false positive s and false negatives . \nThe other algorithms performed relatively similarly  (and \nsuccessfully) . \nWhile jProductionCritic is still not as good as a human \nexpert, it did perform well enough to be at least \ncomparable , and it certainly caught  many errors missed \nby the students . It is sufficiently good to serve as a time -  \n \nsaving and verification tool for teachers, and can \neffectively  provide students and amateur  producers with \nvaluable feedback for improv ing their work.  \n6. CONCLUSIONS AND FUTU RE RESEARCH  \nIt is hoped that jProductionCritic will help to address \nseveral underserved needs: the absence of integrated \nopen -source production error checking software  in \ngener al; the absence of software intended  to meet the \neducational needs  of audio production students  in \nparticular ; and the relatively limited  attention given to \nboth production and education software in the MIR \ncommunity  to date . \nFrom a research perspective, j ProductionCritic has the \nadvantages of including a number of original error \ndetection algorithms and of being fully open -sourc e. \nUnlike almost all other integrated production error \ndetection software, its algorithms are not proprietary  \nblack boxes. Further more,  jProductionCritic has a \nmodular and easily extensible design that is intended to \nencourage its use as a framework  for future MIR  research \non develop ing additional error checking algorithms . \nFrom an applied perspective, jProductionCritic is the \nonly k nown production -oriented software that is intended \nto meet the specific needs of education , and looks for \nmany more errors  than any other known general \nintegrated system . Moreover , the validation experiments \nfound that the software performs more than well enough \nto be used successfully  in practice . \nThe first priority for future work  is to port \njProductionCritic  to a standard DAW plug -in format, \nsuch as VST or Nyquist . This will greatly increase its \naccessibility to students. Another priority is to implement \nit as a Vamp plug -in so that it can be used with Sonic \nVisualiser [ 2], which would increase the scope and clarity \nof information that could be shown to users  by \nsupplementing the Audacity Label Tracks currently used . \nIt would also be useful to implement  an interface with \nwhich teachers could specify a grading scheme, so that \nassignments could be marked more easily, and students \ncould have an idea what grade s they wi ll receive before \nsubmitting. Of course, it is important to reserve room for \nthe instructor’s subjective judgment when doing this. \nThere are also many other useful error detection \nalgorithms that remain to be implemented, including \ndetection of poor EQ, to o much or too little  reverb eration , \nexcessive performance artifacts , etc . There is also still \nplenty of potential  to refine and improve the existing \nalgorithms, especially those that performed poorly in the \nvalidation tests, perhaps with the ultimate goal of making \njProductionCritic more useful in professional contexts.  \njProductionCritic , its code  and documentation  can all \nbe downloaded for free from:  http://jmir.sourceforge.net . 7. REFERENCES  \n[1] Barlett, B. and J. Barlett. 2009. Practical recording \ntechniques: T he step -by-step approach to \nprofessional audio recording.  Burlington, MA: \nFocal Press.  \n[2] Cannam, C., C. Landone, M. Sandler, and J. P. Bello. \n2006. The Sonic Visualiser: A visualisation platform \nfor semantic descriptors from musical signals. \nProceedings of the International Conference on \nMusic Information Retrieval . 324 –7. \n[3] Huber, D. M., and R. E. Runstein. 2009. Modern \nrecording techniques. Burlington, MA: Focal Press.  \n[4] Kühhirt, U. 2008. A/V Analyzing Toolbox.  Retrieved \n8 May 2013, from \nhttp://www.idmt.fraunh ofer.de/en/Service_Offerings\n/technologies/a_d/av_analyzing_toolbox.html . \n[5] McKay, C. 2010. Automatic music classification \nwith jMIR. Ph.D. Dissertation . McGill University, \nCanada.  \n[6] Montecchio, N., and A Cont. 2011. Accelerating the \nmixing phase in studio recording productions by \nautomatic audio alignment . Proceedings of the \nInternational Society for Music Information \nRetrieval Conference.  627–32. \n[7] Owsinski, B. 2013. The mixing engineer’s handbook.  \nIndependence, KY: Course Technology PTR.  \n[8] Sabri, M., J. Alire zaie, and S. Krishnan. 2003. Audio \nnoise detection using hidden Markov model. \nProceedings of the IEEE Workshop on Statistical \nSignal Processing . 637 –40. \n[9] Scott, J., and Y. E. Kim. 2011. Analysis of acoustic \nfeatures for automated multi -track mixing. \nProceed ings of the International Society for Music \nInformation Retrieval Conference.  621–6. \n[10] Vaseghi, S. V. 2009. Advanced digital signal \nprocessing and noise reduction. Chichester, West \nSussex: Wiley.  \n[11] Witten, I. H., E. Frank, and M. A. Hall. 2011. Data \nmining: Practical machine learning tools and \ntechniques.  New York: Morgan Kaufman.  \n[12] Audiofile -Inspector & Digital Error Checker . \nRetrieved 8 May 2013, from http://www.cube -\ntec.com/products/quadriga/quadriga -features -\nsystem -integration/quadriga -features -audiofile -\ninspector -and-digital -error -checker.  \n[13] Digital Audio Error Detection.  Retrieved 8 May \n2013, from http://digauderrodetec.sourceforge.net."
    },
    {
        "title": "Automatically Identifying Vocal Expressions for Music Transcription.",
        "author": [
            "Sai Sumanth Miryala",
            "Kalika Bali",
            "Ranjita Bhagwan",
            "Monojit Choudhury"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416624",
        "url": "https://doi.org/10.5281/zenodo.1416624",
        "ee": "https://zenodo.org/records/1416624/files/MiryalaBBC13.pdf",
        "abstract": "Music transcription has many uses ranging from music information retrieval to better education tools. An important component of automated transcription is the identification and labeling of different kinds of vocal expressions such as vibrato, glides, and riffs. In Indian Classical Music such expressions are particularly important since a raga is often established and identified by the correct use of these expressions. It is not only important to classify what the expression is, but also when it starts and ends in a vocal rendition. Some examples of such expressions that are key to Indian music are Meend (vocal glides) and Andolan (very slow vibrato). In this paper, we present an algorithm for the automatic transcription and expression identification of vocal renditions with specific application to North Indian Classical Music. Using expert human annotation as the ground truth, we evaluate this algorithm and compare it with two machinelearning approaches. Our results show that we correctly identify the expressions and transcribe vocal music with 85% accuracy. As a part of this effort, we have created a corpus of 35 voice recordings, of which 12 recordings are annotated by experts. The corpus is available for download 1 .",
        "zenodo_id": 1416624,
        "dblp_key": "conf/ismir/MiryalaBBC13",
        "keywords": [
            "Music transcription",
            "uses",
            "music information retrieval",
            "education tools",
            "vocal expressions",
            "identification",
            "vibrato",
            "glides",
            "riffs",
            "Indian Classical Music"
        ],
        "content": "AUTOMATICALLY IDENTIFYING VOCAL EXPRESSIONS FOR MUSIC\nTRANSCRIPTION\nSai Sumanth Miryala\nMicrosoft Research India\nmssumanth99@gmail.comKalika Bali\nMicrosoft Research India\nkalikab@microsoft.comRanjita Bhagwan\nMicrosoft Research India\nbhagwan@microsoft.comMonojit Choudhury\nMicrosoft Research India\nmonojitc@microsoft.com\nABSTRACT\nMusic transcription has many uses ranging from music in-\nformation retrieval to better education tools. An important\ncomponent of automated transcription is the identiﬁcation\nand labeling of different kinds of vocal expressions such\nas vibrato, glides, and riffs. In Indian Classical Music such\nexpressions are particularly important since a raga is often\nestablished and identiﬁed by the correct use of these ex-\npressions. It is not only important to classify what the ex-\npression is, but also when it starts and ends in a vocal ren-\ndition. Some examples of such expressions that are key to\nIndian music are Meend (vocal glides) and Andolan (very\nslow vibrato).\nIn this paper, we present an algorithm for the automatic\ntranscription and expression identiﬁcation of vocal rendi-\ntions with speciﬁc application to North Indian Classical\nMusic. Using expert human annotation as the ground truth,\nwe evaluate this algorithm and compare it with two machine-\nlearning approaches. Our results show that we correctly\nidentify the expressions and transcribe vocal music with\n85% accuracy. As a part of this effort, we have created a\ncorpus of 35 voice recordings, of which 12 recordings are\nannotated by experts. The corpus is available for down-\nload1.\n1. INTRODUCTION\nV ocal expressions, such as glides, licks, and vibrato are an\nintrinsic part of vocal music of any genre. The use of suit-\nable vocal expressions establishes the characteristic mood\nof a song and enhances its emotional appeal. In western\nclassical music, the appropriate use of vibrato and tremolo\nwhile singing can drastically change the appeal of a given\npiece.\nSimilarly, in North Indian Classical Music (NICM), not\nonly do vocal expressions enhance or characterize a song’s\nmood, they also establish the correctness of a raga’s2ren-\n1http://research.microsoft.com/apps/pubs/?id=198396\n2Araga is based on an ascending and descending scale, but is charac-\nterized using many other features and evades a formal deﬁnition. See [2]\nfor a detailed exposition.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.\nFigure 1 . The glide from the third to the ﬁfth note for raga\nBhupali is quick, but in Shuddha Kalyan , it is slower and\nperceivably touches the fourth’s microtones.\nFigure 2 . The third note is steady in raga Bhairavi , but\noscillates across the note’s microtones in raga Darbari\ndition. For instance, the nature of the glide between two\nnotes is a key difference between two ragas ,Bhupali and\nShuddha Kalyan , that are based on the same melodic scale\n(see Figure 1). Whether the glide from the third to the ﬁfth\nnote is quick or slow differentiates them (see Table 1 for\nnote deﬁnitions). Also, whether a note is steady or oscil-\nlating can also depend on the raga (see Figure 2).\nHence, automatically identifying and labeling such vo-\ncal expressions is very important for accurate music tran-\nscription. In addition, identifying vocal expressions is a\nbasic requirement towards building tools for music educa-\ntion. Such an educational tool can process a student’s ren-\ndition, identify the vocal expression and provide feedback,\nvisual or auditory, on the correctness.\nIn this paper, we propose an algorithm for automati-\ncally identifying vocal expressions and can therefore be a\nbuilding-block for various music transcription and educa-\ntion tools. The proposed algorithm (a) estimates the pitch\ncurve, (b) identiﬁes the singing voice frames, (c) processes\nthe pitch envelope to obtain a canonical representation and(d) uses templates to identify each expression and ﬁnally\ncreate a transcription of the audio signal.\nWe concentrate on two main expressions predominantly\nused in NICM: Meend or a slow glide between notes, and\nAndolan or slow microtonal oscillations within the same\nnote. We limit our discussion in this paper to these because\nthey are two of the most important vocal expressions that\ndetermine the correctness of a raga. While we do not claim\nthat our speciﬁc algorithm can capture every kind of vocal\nexpression across all genres, we believe that the overall ap-\nproach we have proposed can be used to identify different\nstyles of vocal expressions across different genres.\nThe ground-truth is created by manual annotation of the\nrecordings by experts. We compare the results of our work\nwith two different machine learning techniques: decision\ntrees and conditional random ﬁelds. Our ﬁndings show\nthat we can achieve up to 85% accuracy across all vocal\nexpressions identiﬁed, an improvement of 7% over a ma-\nchine learning-based competitive baseline.\n2. RELATED WORK\nThe automatic transcription of polyphonic music needs to\ndeal with source separation and note detection in the pri-\nmary source. Rao et al. [12] proposed a system for ex-\ntracting vocal melody from a polyphonic music signal in\nNICM using a spectral harmonic-matching pitch detection\nalgorithm. The voice signal extracted using these methods\ncan then be input to our algorithm for identifying expres-\nsions. However this method does not apply to identifying\nﬁne vocal expressions.\nTypically, audio alignment for note detection, specif-\nically onset, steady period and offset of a note, employ\nsignal processing methods like Dynamic Time Warping\n(DTW) in conjunction with graphical models like Hidden\nMarkov Models (HMM). Devaney et al. [4] used an HMM\nmodel with acoustic features like power and aperiodicity\nalong with DTW priors to align as well as identify the\ntransient as well as steady portions of a note. Our tech-\nnique does not use onset detection for reasons outlined in\nSection 3.2.\nClassiﬁcation of ragas based on the concept of Pitch\nClass Distribution Dyads (PCDD) [3] uses spectrum based\npitch extraction and onset-detection to create Pitch Class\nDistribution (PCD) and PCDD. A classiﬁer is then used\non the PCD and PCDDs to identify raga labels. Ross\net al. [13] detects melodic motifs to identify repetition of\nphrases in a raga rendition. While all these methods are\nsuccessful to a certain extent, they do not take into account\nthe vocal expressions that may be speciﬁc to the composi-\ntion or introduced by the singer for a more rich rendition,\nor in the music education scenarios, mistakes by a learner.\nThe biggest challenge for an automatic transcription of\nNICM is the absence of a written score that makes any top-\ndown processing using the score as a knowledge source\npractically impossible. A number of approaches for the\ntranscription of Western music [9] have made use of the\navailability of a score as one of the knowledge sources in\ntheir models. Klapuri [6], has an extensive discussion onSa Re Ga Ma Pa Dha Ni\nDo Re Mi Fa So La Ti\n1st2nd3rd4rd5rd6rd7rd\nTable 1 . Relative note names in Indian (top) and Western\n(bottom) traditions .\nFigure 3 . Pitch Envelope of a meend from SatoDha in\nraga Darbari ,Sais 220Hz.\nthe use of a “musicological model” as a part of a transcrip-\ntion system. While the system developed in [6] primarily\nuses the sinusoidal properties of the polyphonic music sig-\nnal, the concluding discussion clearly points to the use of\nan existing score for improvement. In a later version of\nthe polyphonic music transcription [7], they make use of\na reference database for note event modeling, as well as\na musicological model for the transcription system. The\nwork reported in this paper makes no assumptions about\nthe availability of a musical score or a knowledge model\nand aims to identify and label the different vocal expres-\nsions from the signal using signal processing and machine\nlearning techniques.\n3. BACKGROUND AND DEFINITIONS\nNICM follows a relative note system, where all the notes\nare sung with respect to the tonic which is called Sa(same\nas the Do). The frequency of the tonic depends on the\nsinger. Table 1 shows the Indian names for the seven notes\ncorresponding to the western Do-Re-Mi . In this paper, we\nfocus on the Alap , which is a meterless form of singing\nthat typically starts any NICM rendition. The alap cap-\ntures the essence of the raga being rendered. Alap is usu-\nally sung with no accompaniment except for a background\ndrone called the Tanpura , which provides the singer a ref-\nerence to the tonic.\n3.1 Vocal Expressions\nAs mentioned earlier, the NICM genre uses various char-\nacteristic vocal expressions for enhancing as well as estab-\nlishing a raga rendition In our work, we concentrate specif-\nically on the meend and the andolan .\nMeend is a smooth glide from one note to another, as\nshown in Figure 3, where the singer moves from SatoDha.\nThis is clearly distinct from a steady note, as shown in Fig-\nure 4 (top). A very short glide is often termed as a sparsh .\nFor the purpose of this work, we will use the term glide to\nrefer to both of these.Figure 4 . Pitch Envelope of a steady Re(top) and andolan\naround komal Dha (bottom) in raga Darbari .\nAndolan is a gentle swing that oscillates between the\nlower and the higher microtones of a certain note. Fig. 4\n(bottom) shows the pitch curve of an andolan sung around\nkomal Dha or the minor sixth.\n3.2 Problem Deﬁnition\nGiven an audio recording of vocal music in NICM, we\nwant to label it with a set of annotations that clearly mark\nthe steady notes as well as the vocal expressions, viz. meend ,\nsparsh andandolan . In the example in Figure 5. From 0\nto 0.6s, a steady Sa(or the ﬁrst note) is sung, followed\nby a meend to the Reor the major second, until time 1.2s.\nThis is followed by a steady rendition of Reuntil time 1.6s.\nAfter a meend tokomal Ga or the minor third, the singer\nsings an andolan around komal Ga . This extends from 2.2s\nto 3.9s. Given a vocal rendition in NICM, our objective is\nto output the time-series of such annotations.\n3.3 Challenges\nThe task of identifying these vocal expressions and tran-\nscribing NICM faces two primary challenges. First, there\nis no written score available. Indian classical music is an\nimprovisational art-form, and textual representation of mu-\nsical pieces, if they exist are very rough and used only as a\ntentative guide. There are no equivalent notations for vocal\nexpressions like trills, vibrato, or tremolo, that exist quite\nextensively in western classical music.\nSecond, in Indian classical music notes generally do not\ncorrespond to clear onsets. Hence, conventional transcrip-\ntion methods that rely on onset detection cannot be used.\nOnset detection algorithms depend on detecting transient\nregions in the signal, including sudden bursts in energy or\nchanges in the spectrum of the signal etc. These methods\nfail whenever there is a smooth glide between two notes. In\nthis work, we present a transcription scheme, which relies\nsolely on the pitch envelope.\nFigure 5 . Annotation of an audio recording, using a mock\npitch envelope.\n4. TRANSCRIPTION AND EXPRESSION\nIDENTIFICATION\nThe ﬁrst step towards transcription is the estimation of fun-\ndamental frequency. We chose a difference function based\nmethod for this purpose. This method was preferred over\nthe frequency-domain methods, because real-time perfor-\nmances are important for music education tools. No sig-\nniﬁcant improvement was observed by using the spectral\nmethods. Any accurate pitch detection method maybe used\nfor this step.\nThe second step is to detect audio segments with vo-\ncal singing as against segments with only the background\ndrone. From the background drone, we detect the tonic,\nor the absolute frequency of the base-note for the singer.\nThe third step is to obtain a canonical representation of the\npitch curve, for which we use a line ﬁtting algorithm to the\ncurve. The ﬁnal step is to perform vocal expression identi-\nﬁcation using template representations for each expression.\nThese steps are explained in the following sections.\n4.1 Pitch Estimation\nIn this work the terms pitch and fundamental frequency\nare used interchangeably. For each frame, the difference\nfunction is evaluated by subtracting the original signal with\ndelayed versions of itself. The fundamental frequency cor-\nresponds to the absolute minimum of the difference func-\ntion. The other local minima correspond to the harmonics.\nWe ﬁxed the frame size as 50 ms with 50% overlap for this\nwork. A low-pass ﬁlter with a cutoff frequency of 700Hz is\napplied before pitch estimation to remove high frequency\ncontent. Variance of the pitch track is chosen as a mea-\nsure to eliminate octave errors using the Viterbi algorithm.\nIn this work, source separation or multi-band pitch estima-\ntion is not necessary as the singing voice masks the tanpura\nsound well.\n4.2 Drone and Tonic Detection\nThe background drone or the tanpura is a stringed instru-\nment that provides a continuous pitch reference to a vo-\ncal performer. The drone usually consists of four strings,\nthree of them at the tonic of the scale, and one string tuned\nto the ﬁfth note. To identify the singing voice frames in\nthe recording, we use resonance properties of the drone.\nDue to the special form of the bridge ﬁxed to a resonant\nbody, tanpura shows remarkably different acoustic proper-\nties compared to other stringed instruments [11]. The wideFigure 6 . Pitch envelope & Lines ﬁt using the proposed\nmethod. Critical points are marked with ‘X’\nbody of the bridge induces a large number of overtones that\nmanifest in the output of the pitch estimation algorithm. In\nframes that contain voice, these overtones are masked by\nthe voice.\nConsequently, the frames with only the drone have higher\nentropy than the frames that contain voice. We therefore\nuse an entropy based method [5] to differentiate the singing\nvoice frames from the drone. For each audio recording, we\ndynamically estimate an entropy threshold from the his-\ntogram of entropy values. Any frame with lower entropy\nthan the threshold is labeled as a singing voice frame, while\nthe frames with higher entropy are labeled as the tanpura\nframes. Tonic is calculated as the mode of the pitch values\nin the frames where tanpura is prominently audible.\n4.3 Line Fitting\nThe third step in the process, is to obtain a canonical rep-\nresentation of the pitch-curve in terms of straight lines and\ncritical points of inﬂection. We use an augmented version\nof a previously proposed line-ﬁtting algorithm [1] for this\npurpose. We outline our algorithm as follows:\nStep 1: Identify the local minima and maxima in the pitch\ncurve. This gives us a set of points representing the curve.\nTo achieve better ﬁt along transient regions, the set of points\nwhere the singer is changing notes are added to the list.\nThese points are identiﬁed by scaling down the pitch val-\nues to one octave and mapping the frequencies to notes.\nStart a sweep from the left of the curve.\nStep 2: Start from the ﬁrst point on the curve, and connect\nit to the third point using a straight line. From this line, if\nthe second point lies within the distance speciﬁed by Just\nNoticeable Difference (JND) threshold (equation 1), the\nsecond point is removed. Then, connect the ﬁrst point to\nthe fourth point and repeat the JND threshold-based check\nfor the third point. Repeat this process until you ﬁnd a\npoint that lies outside the JND threshold. This point is the\nstarting point for the next iteration.\nStep 3: Starting from the new critical point, repeat Step\n2 to ﬁnd the next critical point. Continue this process un-\ntil the whole pitch curve is represented by a set of critical\npoints and ﬁt lines between these points, by minimizing\nthe squared error between the pitch curve and the lines ﬁt.\nJND 1(F) = 3:13\u00000:13\u0003F\n100(1)\nFigure 6 shows a sample pitch envelope and the ﬁnal\ncritical points identiﬁed. For each pitch curve, we calcu-\nFigure 7 . Pitch envelope of the andolan note & Lines ﬁt\nusing threshold JND1 (top) & JND1/2 (bottom).\nlate canonical representations by varying the value of the\nJND threshold. Small variations in pitch due to singing im-\nperfections are eliminated in the canonical representation.\nThese representations are useful in the identiﬁcation of vo-\ncal expressions of certain types, as we shall describe in the\nnext subsection.\n4.4 Identifying Vocal Expressions\nGiven the canonical representation of the pitch curve, we\nusetemplates for each kind of vocal expression to recog-\nnize and classify them. A template for each expression is a\nloose representation based on some subset of duration, line\nlengths, slopes, and number of points. In the next subsec-\ntions, we describe templates for andolan andmeend , and\nhow we detect these expressions.\n4.4.1 Andolan\nUsing the expert manual annotations and by studying the\npitch curves ourselves, we have found that an andolan has\nthe following template: six to ten straight lines, with con-\nsecutive lines having alternating slope signs. All pitch val-\nues that the lines touch should be within the same or adja-\ncent notes . This template captures the slow oscillations of\nanandolan , that touch the microtones within a single note.\nThis is as opposed to a vibrato, which manifests itself as a\nmuch faster oscillation. However, we could use a similar\ntemplate for vibrato detection as well.\nTo match this template, we look for such a pattern across\nthe different canonical representations of the pitch curve\nthat are obtained by decreasing the JND threshold value\niteratively a maximum of 3 times. In our work, we have\nused the thresholds JND 1,JND 1=2and 0:4\u0003JND 1.\nThe threshold needs to be decreased, because the ampli-\ntude of the oscillation can vary from very small to quite\nlarge. With large JND thresholds, the canonical represen-\ntation may not capture all the oscillations, as shown in Fig-\nure 7.\nHowever, if the threshold is too low, the oscillatory pat-\ntern may be found in steady notes too. So, the threshold\nshould not be decreased too much. If we ﬁnd such a deﬁ-\nnite pattern in at least one of the canonical representations,\nwe classify the corresponding segment of the pitch curve\nas an andolan . This matching algorithm is similar to using\nDTW iteratively to do the template matching.4.4.2 Glides\nThe template for a glide is the following: a line between\ntwo critical points where the line starts and ends at different\nnotes. Any line which satisﬁes this property is either a\nmeend orsparsh . If the glide segment is longer than 300\nms, it is labeled a meend , else a sparsh .\n4.4.3 Steady notes\nSegments with no expressions are typically represented as\nhorizontal lines, with very low slope values. Hence, we use\nthis simple template to classify segments as steady notes.\nSteady notes are transcribed using the median of the pitch\nvalues between its two end points.\n5. EV ALUATION & RESULTS\nIn this Section, we describe our evaluation of the proposed\nalgorithm. First, we describe and characterize the data\nwe collect for evaluation. Next, we compare the accu-\nracy of our technique with that of two machine-learning\ntechniques – the C5.0 Decision Tree Classiﬁer [10], and a\nConditional Random Field classiﬁer (CRF) [8] and present\nthe results.\n5.1 Data Collection\nWe have created a corpus of 35 recordings in 8 ragas sung\nby 6 singers of varying expertise which are publicly avail-\nable for the purposes of Music Information Retrieval re-\nsearch. In this paper, we use 12 recordings of 3 singers\nsinging alap in 4 ragas for evaluation of our algorithms.\nWe ensured that all the three singers sang identical pieces,\nwith the same set of notes and same vocal expressions in\neach raga. This is to ensure that we have a balanced data-\nset across different ragas and different singers.\nWe asked two experts, one who has been a professional\nmusic teacher for 25 years, and the other a serious music\nlearner for 11 years, to manually annotate the vocal ex-\npressions on 12 recordings sung by 3 singers in 4 differ-\nentragas . We had one expert annotate each ﬁle ﬁrst, and\nthe second expert revised and veriﬁed these annotations to\nensure no expressions were missed. In case of an ambi-\nguity among the two annotators, the more experienced an-\nnotator’s labels are used. Each ﬁle is approximately 2.5\nminutes long, and the sum total of the length of all twelve\nrecordings is approximately 30 minutes. The audio was\ncollected in a recording studio and is therefore compara-\ntively noiseless.\nThe experts used Praat [14] to annotate these record-\nings. Praat allows a user to listen to and annotate audio\nﬁles, while also displaying the pitch envelope. Using Praat\ntextgrids, the experts annotated each ﬁle with note and ex-\npression boundaries, and they labeled each segment as ei-\nthertanpura , Steady, andolan ,meend , orsparsh . Annotat-\ning a 3 minute recording took the two experts 120 minutes\non average. Therefore, a total of about 24 hours were re-\nquired for the manual annotation process.Feature No. of Features\nPitch 2n+1\nFirst derivative of Pitch 2n\nPitch (warped to one octave) 2n+1\nEntropy 2n+1\nAmplitude (Normalized) 2n+1\nTable 2 . Features for the classiﬁers.\nProposed DT CRF Improvement(%)\nDrone 0.964 0.966 0.956 -0.18\nSteady 0.828 0.825 0.828 5.4\nAndolan 0.647 0.449 0.448 44.31\nMeend 0.72 0.38 0.314 89.49\nSparsh 0.651 0.295 0.344 89.31\nTable 3 . F1-scores for each class. The last column shows\nthe percentage improvement that our approach shows over\nthe better classiﬁer\n5.2 Evaluation Methodology\nThe annotations of the algorithms are compared with the\nground-truth, frame to frame and the overall accuracy is\ndeﬁned as the percentage of frames labeled correctly. We\ncompare the classiﬁcation of these frames by our algorithm\nwith that of two stock classiﬁers: the C5.0 decision tree,\nand CRF. The reason for trying the CRF is to evaluate a\nclassiﬁer which uses a notion of time or sequences, which\nseems inherent to expressions such as the andolan . The\ndecision tree, on the other hand, does not incorporate time.\nThe weights vector for CRF is initialized randomly and\neach note is considered as a sequence. These methods are\nevaluated using the leave one out cross-validation method.\nThe features used for classiﬁcation are shown in table\n2. To provide the note change information, pitch is warped\ndown to one octave and fed to the classiﬁer. We collect\nthese features for the current frame, and a ﬁxed number\n(n) of frames before and after the current frame. The per-\nformance of the classiﬁers is similar across several values\nofn. In this section, we report the results for n= 10 .\n5.3 Results\nThe overall accuracy of our technique is 84.7%, whereas\nwith CRF, it is 77.6% and with C5.0, it is 77%. Hence,\nour proposed method improves the error produced by the\nbetter classiﬁer by 31.7%.\nTable 3 shows the F1-scores for each class, for each of\nthe evaluated techniques. All the three approaches iden-\ntify the Drone segments with about 96% accuracy. Our\napproach shows a 5.4% improvement over the better of the\ntwo classiﬁers for steady notes. For the more complex vo-\ncal expressions, our approach shows much higher improve-\nAlgorithm Singer 1 Singer 2 Singer 3\nProposed 15.27 13.63 16.8\nDT 21.25 21.96 25.44\nCRF 18.69 20.72 21.05\nTable 4 . singer-wise classiﬁcation errors (in % of frames)Algorithm Bhairav Darbari Janpuri Lalit\nProposed 14.49 17.02 20.97 10.65\nDT 24.77 29.98 20.69 17.45\nCRF 20.43 24.82 22.61 12.75\nTable 5 .raga-wise classiﬁcation errors (in % of frames)\nment: 44.31% for andolan , and about 89% for the glides.\nNote that this is in spite of the machine learning meth-\nods using approximately 90% of the available frames as\ntraining data. Our algorithm, on the other hand, does not\nuse any training data. Moreover, we have no tunable thresh-\nolds in the core algorithm. We do use ﬁxed thresholds\nfor certain templates, for instance, to differentiate meends\nfrom short glides ( sparsh ). However, given the nature of\nthese expressions, we feel this is unavoidable.\nHowever, for all three evaluated techniques, the F1-scores\nfor identifying the vocal expressions are much lower than\nthose for identifying steady notes. For instance, the F1-\nscore for andolan using our approach is 0.647, for meend\nit is 0.72, whereas for Steady is 0.828. One source of error\nis that the boundaries between the glides and steady notes,\nas annotated by the experts, do not align exactly with the\nalgorithm’s labels. Therefore, some frames in these bound-\nary regions, which the expert has annotated as glides are\nvery often mis-labeled by our approach as steady. Another\nsource of error is the mis-labeling of vocal expressions by\nthe annotators. Some of the short vocal glides are hard\nto perceive and are labeled ‘steady’ by the annotators. In\ncase of andolan , if the range of oscillation is less, the algo-\nrithms would identify it as a steady note and sometimes the\npitch estimation algorithm does not pick up the microtonal\nvariation accurately enough. Also, the way in which these\nexpressions are achieved sometimes depends on the raga\nand the singer’s expertise.\nTable 4 shows the classiﬁcation error by singer. Singer\n1 is a professional artiste with 30 years of training, Singer\n2 is a music educationist with 15 years of training, and\nSinger 3 is a music educationist with 40 years of training.\nSinger 3 uses much smaller microtonal variations in the\nrendition of andolans , some of which are labeled as steady.\nHence, the errors are slightly higher for Singer 3 across all\nthree approaches as compared to Singers 1 and 2.\nTable 5 shows the classiﬁcation error by raga. Of the\nfour ragas , only Lalit does not use andolans .Janpuri and\nDarbari , on the other hand, use signiﬁcant amounts of this\nvocal expression. Hence, the error associated with Lalit is\na lot less(10.65% using our approach), and that associated\nwith Jaunpuri (20.97%) and Darbari (17.02%) are higher.\n6. CONCLUSIONS\nWe proposed an algorithm for automatic expression identi-\nﬁcation in vocal music. The idea is to use templates for\neach expression and match these templates in the pitch\ncurve. We compared the performance of our algorithm\nwith two machine learning methods. Our algorithm is more\naccurate than the better classiﬁer by about 7%.\nIn future work, we intend to apply this technique tomore vocal expressions across different genres of music.\nWe also intend to use this algorithm as a building-block in\nvarious music education and music transcription applica-\ntions.\n7. REFERENCES\n[1] Bret Battey: “B ´ezier spline modeling of pitch-\ncontinuous melodic expression and ornamentation,”\nComputer Music Journal , V ol. 28(4), pp. 25-39, 2004.\n[2] Bhatkhande V . N, Garg P.K.: Hindustani Sangit Pad-\ndhati , Sakhi Prakashan, 1990.\n[3] Chordia P.: “Automatic raag classiﬁcation of pitch\ntracked performances using pitch-class and pitch-class\ndyad distributions,” Proc. of Intl. Computer Music\nConf. , 2006.\n[4] Devaney J. et al.: “Improving MIDI-audio alignment\nwith acoustic features,” In Proc. IEEE WASPAA , 2009.\n[5] Jia C., Bo Xu: “An improved entropy-based endpoint\ndetection algorithm,” International Symposium on Chi-\nnese Spoken Language Processing , 2002.\n[6] Klapuri A., Ryyn ¨anen, M.P.: “Modeling of note events\nfor singing transcription.”, ISCA Tutorial and Research\nWorkshop on Statistical and Perceptual Audio Process-\ning, 2004.\n[7] Klapuri A., Ryyn ¨anen, M.P.: “Transcription of the\nsinging melody in polyphonic music.”, Proc. 7th Intl.\nConf. on Music Information Retrieval , V ol. 15, 2006.\n[8] Lafferty J. et al.: “Conditional random ﬁelds: Proba-\nbilistic models for segmenting and labeling sequence\ndata,” Intl. Conf. on Machine Learning , 2001.\n[9] Martin K.D.: “Automatic transcription of simple poly-\nphonic music: robust front end processing,” in the\nThird Joint Meeting of the Acoustical Societies of\nAmerica and Japan , 1996.\n[10] J.R. Quinlan: “Induction of decision trees”, Machine\nLearning , V ol. 1.1, pp. 81-106.\n[11] Raman C.V .: “On some Indian stringed instruments,”\nProceedings of the Indian Association for the Cultiva-\ntion of Science , V ol. 7, pp. 29–33, 1921.\n[12] Rao V ., Rao P.: “V ocal melody extraction in the pres-\nence of pitched accompaniment in polyphonic music.”,\nIEEE Trans. on Audio, Speech, and Language Process-\ning, V ol 18(8), pp. 2145-2154, 2010.\n[13] Ross J.C., Vinutha T.P., Rao P.: “Detecting melodic\nmotifs from audio for Hindustani classical music.”,\nProceedings of the 13th Intl. Society for Music Info.\nRetrieval Conf. , 2012.\n[14] The PRAAT Speech Analysis and Annotation Toolkit:\nhttp://www.fon.hum.uva.nl/praat/."
    },
    {
        "title": "Taste Over Time: The Temporal Dynamics of User Preferences.",
        "author": [
            "Joshua L. Moore",
            "Shuo Chen 0008",
            "Douglas R. Turnbull",
            "Thorsten Joachims"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416148",
        "url": "https://doi.org/10.5281/zenodo.1416148",
        "ee": "https://zenodo.org/records/1416148/files/MooreCTJ13.pdf",
        "abstract": "We develop temporal embedding models for exploring how listening preferences of a population develop over time. In particular, we propose time-dynamic probabilistic embedding models that incorporate users and songs in a joint Euclidian space in which they gradually change position over time. Using large-scale Scrobbler data from Last.fm spanning a period of 8 years, our models generate trajectories of how user tastes changed over time, how artists developed, and how songs move in the embedded space. This ability to visualize and quantify listening preferences of a large population of people over a multi-year time period provides exciting opportunities for data-driven exploration of musicological trends and patterns.",
        "zenodo_id": 1416148,
        "dblp_key": "conf/ismir/MooreCTJ13",
        "keywords": [
            "temporal embedding models",
            "exploring listening preferences",
            "population development",
            "time-dynamic probabilistic embedding",
            "users and songs",
            "joint Euclidian space",
            "trajectory of taste change",
            "artists development",
            "songs movement",
            "large-scale Scrobbler data"
        ],
        "content": "TASTE OVER TIME: THE TEMPORAL DYNAMICS OF USER\nPREFERENCES\nJoshua L. Moore, Shuo Chen, Thorsten Joachims\nCornell University, Dept. of Computer Science\nfjlmo|shuochen|tj g@cs.cornell.eduDouglas Turnbull\nIthaca College, Dept. of Computer Science\ndturnbull@ithaca.edu\nABSTRACT\nWe develop temporal embedding models for exploring how\nlistening preferences of a population develop over time. In\nparticular, we propose time-dynamic probabilistic embed-\nding models that incorporate users and songs in a joint Eu-\nclidian space in which they gradually change position over\ntime. Using large-scale Scrobbler data from Last.fm span-\nning a period of 8 years, our models generate trajectories\nof how user tastes changed over time, how artists devel-\noped, and how songs move in the embedded space. This\nability to visualize and quantify listening preferences of a\nlarge population of people over a multi-year time period\nprovides exciting opportunities for data-driven exploration\nof musicological trends and patterns.\n1. INTRODUCTION\nEmbedding methods are a class of models that learn posi-\ntions for discrete objects in a metric space. Such models\nare widely employed in a variety of ﬁelds, including nat-\nural language processing and music information retrieval\n(MIR). In MIR, these methods ﬁnd use in recommendation\nand playlist prediction, among other problems. Embed-\nding methods offer advantages in two main aspects. First,\nthey are often very easy to interpret: the resulting space\ncan be easily visualized and inspected in order to explain\nthe behavior of the model. Second, they can be applied\nto discrete objects without features, learning feature repre-\nsentations of the objects as part of model training.\nIn this work, we explore the use of embedding meth-\nods as a tool for identifying trends and patterns in multi-\nyear listening data of Last.fm users. In particular, we pro-\npose novel time-dynamic embedding models that gener-\nate trajectories of musical preferences by jointly embed-\nding listeners and the songs they play in a single metric\nspace. In order to do this, we extend existing probabilis-\ntic playlists models [1, 2] by adding time dynamics, al-\nlowing users and songs to change position on a multi-year\nscale. By examining these models, we can draw conclu-\nsions about the behavior of listeners and musical artists\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.over time. Based on these ﬁndings, we conjecture that\nthese time-dynamic embedding methods provide exciting\nopportunities for data-driven exploration of musicological\ntrends and patterns. To facilitate such research, scalable\nsoftware implementations of our embedding methods are\navailable at http://lme.joachims.org .\n2. RELATED WORK\nEmbedding music into a low-dimensional space is useful\nfor visualization and automatic playlist generation. There\nare numerous existing algorithms, such as Multi-Dimensional\nScaling [7] and Local Linear Embedding [4], which have\nbeen applied to large corpora of songs and artists.\nOur work is motivated by recent work of Moore et al.\n[2, 5] and Aizenberg et al. [1] on using historical playlist\ndata, but we focus on long-term temporal dynamics. This\nis different from the short-term dynamics considered by\nAizenberg et al., namely, the time of day of a track play\nby a station. This time dependency is employed to factor\nout the inﬂuence of different format blocks at a radio sta-\ntion based on the time of day (i.e. a college station may\nplay classical music from 6 AM to 9 AM and jazz from\n9 AM to noon). In our work, we allow the positions of\nusers or songs to vary smoothly over the long-term, learn-\ning a representation for each three-month time step from\nthe beginning of 2005 to the end of 2012. Second, both of\nthese related works focus on automatic playlist prediction.\nIn this paper, we instead use our model as a data analysis\ntool to explore long-range trends in the behavior of users,\nsongs, and artists.\nWeston et al. [9] use music embedding for a variety of\nMIR tasks including tag prediction and determining song\nsimilarity. Their embedding algorithm works by optimiz-\ning a rank-based loss function (e.g., AUC, precision at k)\nover training data for a given task. Our work differs from\nthis in that our embedding results from a probabilistic se-\nquence model that is learned from the track histories of\nusers. In addition, the work by Weston et al. does not at-\ntempt to model the temporal dynamics.\nDror et al. [3] explore the use of temporal dynamics in\ncollaborative ﬁltering for music. However, the use of time\ndynamics in their work is mainly restricted to modeling\nbiases for songs and users, which does not permit the visu-\nalization and analysis applications enabled by our work.\nFinally, Shalit et al. [8] applies a dynamic topic model\nto audio features of songs for the purpose of modeling and(a)\n (b)\n (c)\nFigure 1 : Illustrations of the embedding models. Blue dots and red crosses represent songs and users respectively. (a)\nStatic playlist model. A playlist is represented by songs that are linked by arrows. The next song sneis decided by both\ncurrent song scuand the user u(The popularity term also has its effect, which is not shown here). (b) The drifting of a user\nuover timesteps in the user-dynamic model. At each timestep, a random walk governed by a Gaussian distribution is taken.\n(c) Similar drifting of a song sover timesteps in the song-dynamic model.\ndiscovering musical inﬂuence. While this work does not\nexplicitly involve embedding songs, users or artists, it is a\ngood example of the use of temporal dynamics in analysis\nof music data. In addition, their topic model requires audio\nfeatures to represent each song. This is in contrast to our\nmodel where features are not required.\n3. MODEL\nIn this section we detail the probabilistic embedding mod-\nels we propose for temporal embedding of users and songs.\nStarting from a static playlist model (Section 3.1) similar\nto [2, 5], we incorporate a macroscopic temporal model\nunder which the embedding can change over time, pro-\nviding trajectories for users and songs through embedding\nspace. In particular, we propose a user-dynamic embed-\nding model (Section 3.2) in which users move against over\na map of songs, as well as a song-dynamic embedding\nmodel (Section 3.3) in which songs move against a map\nof users. For both models, we brieﬂy outline how they can\nbe trained using maximum likelihood (Section 3.4).\n3.1 Embedding Songs and Users for Playlist Modeling\nGiven a collection of songs S=fs1;:::;sjSjgand a col-\nlection of usersU=fu1;:::;ujUjg, each user’s listen-\ning history can be described as a sequence of songs p=\n(p[1];:::;p[kp])of lengthkp, where each p[i]2S. We refer\nto this (multi-year) sequence as the “playlist” of that user.\nThe collection Dof all user playlists is the training data\nfor our embedding method.\nFollowing the approach proposed in [2], we model a\nuser’s playlist using a ﬁrst-order Markov model, but also\naugment it with a user model similar to [1]. As a result,\nthe probability Pr(snejscu;u)of the next song in a playlist\ndepends only on the current song and the user. The overallprobability of a playlist is therefore\nPr(pju) =kpY\ni=1Pr(p[i]jp[i\u00001];u): (1)\nNote that transition triples (snejscu;u)(i.e., reads as “user\nulistened to the current song scu, then listened to the next\nsongsne”) are a sufﬁcient statistic for this model.\nOur goal is to embed each song and user into a d-\ndimensional latent Euclidean space such that song-song\nand song-user distances model the transition probabilities\nPr(snejscu;u). This provides such distances with a clear\nsemantic meaning. More speciﬁcally, we want to learn a\nmappingX(\u0001)that maps every song sor useruinto that\nspace, namely X(s);X(u)2Rd. The dimensionality d\nis manually speciﬁed. Alternatively, Xcan be considered\nas a(jSj+jUj)\u0002dembedding matrix, the rows of which\ncorrespond to the position of songs and users in the latent\nspace. We will not distinguish the two interpretations of X\nin the rest of the paper if it is clear from the context.\nThe speciﬁc model we propose for relating distances to\ntransition probabilities is\nPr(snejscu;u) =e\u0000\u0001(sne;scu)2\u0000\u0001(sne;u)2+bidx(sne)\nZ(scu;u);(2)\nwhere \u0001(x;y) =jjX(x)\u0000X(y)jjis the Euclidean dis-\ntance between two embedded items (either song or user) in\nthe latent space. bidx(s)is a scalar bias term that is added\nto model the popularity of each song, where idx (s)returns\nthe index for song s. For example, idx (si) =i.Z(scu;u)\nis the partition function that normalizes the distribution. It\nis deﬁned as\nZ(scu;u) =jSjX\ni=1e\u0000\u0001(si;scu)2\u0000\u0001(si;u)2+bi: (3)\nPanel (a) of Figure 1 illustrates how song and user posi-\ntions in the embedding space relate to the transition prob-\nability Pr(snejscu;u). The red cross is the position of theuser, and the blue dot labeled scuis the current song in the\nplaylist. The probability of playing some song snenext de-\npends on its sum of the squared distances to the current\nsong and the user, plus its inherent popularity bsne. This\nmeans that the transition to the next song sneis more likely\nif (1) the next song is close to the current song in the latent\nspace, (2) the next song is close to the user(’s taste) in the\nlatent space, or (3) the next song is popular. We focus our\nexperiments on two-dimensional embeddings, since this\nprovides us with an Xthat can easily be visualized. How-\never, higher-dimensional embeddings are possible as well.\n3.2 User-dynamic Embedding Model\nCombining Equations (1) and (2) models a playlist as\na stochastic process on a microscopic level (i.e., on the\ntimescale of minutes). In addition, we also model changes\nin user preferences as a stochastic process on a macro-\nscopic level. In the following experiments, each macro-\nscopic timestep t2T (Tis the set of all timesteps) de-\nnotes a quarter of a year, and notation like 20083 denotes\n“third quarter of year 2008”.\nLet us ﬁrst consider a macroscopic stochastic process\nwhere positions of users are changing over time, while the\nposition of the songs are ﬁxed in the latent space. Denot-\ning withu(t)the position of user uin embedding space\nat timestep t, the overall trajectory of a user is u(\u0003)=\n(u(1);u(2);:::). At each timestep t, the microscopic transi-\ntion probability Pr(snejscu;u(t))now depends on the users\ncurrent position, and the conditional probability of the next\nsong is\nPr(snejscu;u(t)) =e\u0000\u0001(sne;scu)2\u0000\u0001(sne;u(t))2+b(t)\nidx(sne)\nZ(scu;u(t)):(4)\nNote that even though the positions of songs are ﬁxed, we\nstill give each song a time-varying popularity term b(t)\ni.\nTo restrict users from drifting too much from one\ntimestep to the other, we model a users trajectory as a\nGaussian random walk. Panel (b) of Figure 1 illustrates\nsuch a random walk. Concretely, this means that the user’s\nnext position u(t)is a Gaussian step N(u(t\u00001)\ni;1\n2\u0017userId)\nfrom the current position u(t\u00001). Here,Idis thed-\ndimensional identity matrix, and \u0017useris the variance\n(which can be viewed as a regularization coefﬁcient that\ninﬂuences step sizes). This Gaussian distribution makes\nit more likely that the user’s positions at two consecutive\ntimesteps are close to each other.\nConsidering both the stochastic process over transition\ntriples and the stochastic process describing the users’ tra-\njectories, the overall user-dynamic embedding model can\nbe trained via maximum likelihood. The resulting opti-\nmization problem is\nmax\nX2R(jSj+jTjjUj )\u0002d\nb2RjTj\u0002jSjY\n(snejscu;u(t))2DPr(snejscu;u(t))\n\u0001jUjY\ni=1Y\nt2f\u001cj(\u001c2T)\n^(\u001c\u000012T)ge\u0000\u0017user\u0001(u(t\u00001)\ni;u(t)\ni)2\n;(5)where the song and time-dependent user positions are opti-\nmized to maximize the likelihood of the observed playlists.\n3.3 Song-dynamic Embedding Model\nSimilar to the user-dynamic embedding model, we also\nconsider a song-dynamic embedding model which ﬁxes the\nposition of users and allows songs to drift over time. In this\nmodel, the probability of each transition triple is\nPr(s(t)\nnejs(t)\ncu;u) =e\u0000\u0001(s(t)\nne;s(t)\ncu)2\u0000\u0001(s(t)\nne;u)2+b(t)\nidx(sne)\nZ(s(t)\ncu;u):(6)\nAfter introducing an analogous Gaussian random walk for\nsongs over different timesteps (as illustrated in Panel (c) of\nFigure 1), we get the training problem\nmax\nX2R(jTjjSj +jUj)\u0002d\nb2RjTj\u0002jSjY\n(s(t)\nnejs(t)\ncu;u)2DPr(s(t)\nnejs(t)\ncu;u)\n\u0001jSjY\ni=1Y\nt2f\u001cj(\u001c2T)\n^(\u001c\u000012T)ge\u0000\u0017song\u0001(s(t\u00001)\ni;s(t)\ni)2\n;(7)\nwhere users and time-dependent song positions are opti-\nmized.\nFrom a technical perspective, it is conceivable to train\nan embedding model with both users and songs varying\ntheir position over time, which will output an embedding\nmatrixXof(jTj(jSj+jUj))rows. We brieﬂy explored\nthis model, but found it difﬁcult to interpret the resulting\ntrajectories. We therefore focus on the restricted models in\nour empirical evaluation.\n3.4 Training of Probabilistic Embedding Models\nThe maximum likelihood optimization problems in Equa-\ntions (7) and (5) are of substantial scale. Previous sequence\nmodels were trained using stochastic gradient methods\n[1, 2, 5]. However, those training algorithm does not scale\nwell, since the complexity of each training iteration is\nquadratic in the number of terms in the partition function\n(in our casejSj). In related work on (non-temporal) se-\nquence modeling for natural language [6], we developed\nan approximate, sampling-based training algorithm which\nestimates the partition function on the ﬂy. This training\nprocedure has complexity which is only linear in the num-\nber of terms in the partition function, and we adopt this\nalgorithm for training. A software package implement-\ning the training algorithm is available online at http:\n//lme.joachims.org .\n4. EXPERIMENTS\nOur experiments revolve around a Last.fm data set which\nwe crawled using the site’s API1. The crawl was con-\nducted over the course of several weeks in the fourth quar-\nter of 2012. Although it is unused in this work, we were\ninitially also interested in the social network data, so we\n1http://www.last.fm/apiFigure 2 : The song-dynamic model’s song space plotted (from left to right) at 20051, 20091, and 20124\ncrawled through the social network using the top listener\nfor each of the 10 top artists on the site at the time as seeds.\nFor each user, we crawled the user’s complete timestamped\ntrack history and friends list. We later augmented this data\nwith the age, gender, and country of each user (for those\nfor which it was available). We also crawled the tags for\nsome of the songs, although we do not take advantage of\nthis data in this work.\nThe result contains over 300,000,000 track plays by\nroughly 4700 users, with over 550,000 unique tracks. This\ndata contains many noisy track names, so we pruned the\ndata further by only considering tracks with at least 1000\nplays and discarding users with no remaining track his-\ntory after infrequent songs are discarded. This yields\nthe set of track histories used in the experiments, which\ncontains 4,551 users, 32,401 unique tracks, and roughly\n200,000,000 track plays. We used this to create our “per-\nuser playlist” data by splitting the track histories into\nplaylists of consecutive songs that were played within 15\nminutes of each other. Finally, we quantized the times-\ntamps to divide each user’s track history into year quarters,\nranging from ﬁrst quarter, 2005 until fourth quarter, 2012,\nfor a total of 32 timesteps. From this point on, we will refer\nto thenth quarter of year yyyy asyyyyn , such as 20051\nfor 2005 ﬁrst quarter.\nWe considered models with 2 dimensions in this work\nfor the sake of simplicity and ease of visualization. In or-\nder to ﬁnd good values for \u0017songand\u0017user, we further di-\nvided the data by placing each ﬁfth song transition into a\nvalidation set and the rest into the training set. We then\nused these to validate for the optimal values of these pa-\nrameters. The user-dynamic model performed best with a\nlow value of \u0017user, with its optimal value at 0.01. In con-\ntrast, the song-dynamic model performed best with strong\nregularization, and the optimal \u0017songwas found to be 2.0.\n4.1 Demographics of users\nThe demographics of the data set reﬂect characteristics of\nthe average Last.fm user. For each demographic category,\nwe report percentages based on the number of users report-\ning in that category. 83% reported an age, 89% reported\na country, and 91% reported gender. In our data, about\nFigure 3 : Artist trajectories over time. The legend gives\nthe ﬁrst quarter in which each artist was observed\n78% of the users are male, and about 88% are between the\nages of 15 and 25 (roughly evenly split between the two\ngroups) as of the crawl in 20124. The median user age is\n20, and the average is about 20.8. Due to the social net-\nwork crawl and a coincidence of the seed users, roughly\n57% of our users are from Brazil. The country distribu-\ntion has a fairly long tail, with only 84% coming from\nthe 10 most popular countries, and 91% coming from the\n20 most popular countries. The ten most well-represented\ncountries in the data set are Brazil (57%), US (8%), UK\n(4%), Poland (3%), Russia (2.6%), Germany (2.3%), Spain\n(1.6%), Mexico (1.6%), Chile (1.3%), and Turkey (1.1%).\n4.2 Song-dynamic Model\nIn the song-dynamic model, songs can move over time\nthrough a map of users. Among other things, the result-\ning trajectories give insight into how the appeal of songs\nand artists changed over time.\nIn Figure 2, we show the embedding of the songs at the\nstart, middle, and end of the time sequence (i.e., timestepsFigure 4 : The 10 artists with the smallest variance in position over time (left) and the 10 with the largest variance in position\nover time (top 5 in center, next ﬁve at right). The ﬁrst timestep at which each artist was observed is listed in parentheses.\n20051, 20091, and 20124). A song is plotted once it has\nbeen played at least once, which explains why the space\nbecomes more dense over time. The locations of users\nare not plotted to reduce clutter. Generally speaking, the\ndensity of users is greatest around the origin and then de-\ncreases outwards. In this sense more popular music lies in\nthe center, but note that we also capture popularity through\nthe speciﬁc song bias parameter.\nAre similar songs embedded at similar locations? To\nillustrate the semantic layout of the embedding space, we\nhighlight the songs of some reference artists. Note that\nthe songs of the reference artist cluster even though our\nembedding method has no direct information about artists.\nThis veriﬁes that the model can indeed learn about songs\nsimilarity merely from the listening patterns. We also\nnote that our intuitive notion of artist similarity generally\nmatches the distance at which our model positions them in\nembedding space.\nHow do songs and artists move? Figure 2 also shows\nthat the songs of some artists move in the embedding space,\nwhile others remain more stationary. The artists’ changes\nare aggregated into trajectories and displayed in Figure 3.\nEach dot in Figure 3 indicates the mean location of the\nsongs of one artist at a speciﬁc time step. This plot enables\nus to see more clearly some events and trends in the music\nworld that inﬂuence the model.\nFirst, note that Michael Jackson’s trajectory starts off\nclumped together in the same space, moving very little.\nThen, after some number of timesteps, it starts moving\nquickly towards the center. Upon closer inspection, the\nturning point in this trajectory turns out to line up exactly\nwith the death of Michael Jackson in June, 2009.\nSimilarly, the Beatles start to drift slightly away from\nthe center as many other artists enter the model. Then, they\nmake an abrupt turn back towards the center. This aligns\nwith the release of the Beatles’ full catalog on iTunes in\nthe 20104 after being totally unavailable via digital distri-\nbution before then.Daft Punk also starts to drift away from the center until\nthe release in December, 2010 of the motion picture Tron:\nLegacy , which featured a popular soundtrack by the duo.\nWe can also see Girls Aloud and Cheryl Cole (of Girls\nAloud) drift from the edges rapidly towards the center in\ncorrelated paths, and the emergence of David Archuleta, an\nAmerican Idol runner-up in May, 2008. All follow a sim-\nilar trajectory in user space, indicating that the users that\npreviously listened to Girls Aloud are listening to David\nArchuleta a few years later.\nWe can also see artists like Katy Perry and Lady Gaga\ndrift away from the center after the peak of their popular-\nity, and we see Drake drift towards the center in what can\npartly be explained by a shift in his style from something\nmore hip hop oriented to a somewhat more poppy style.\nWhat does the variance of a trajectory indicate? The\ntrajectories are useful not only for visualization, but also as\nthe basis for further aggregating and quantifying the behav-\nior of an artist. Figure 4 shows the artists with the smallest\nand largest variance in position over time. The speciﬁc cri-\nterion used here for a given artist is the average distance\nover timesteps from the artist’s embedding at that timestep\nto the mean vector of that artist’s representation over all\ntime steps. To avoid obscure artists that would be difﬁ-\ncult to interpret without further background knowledge, we\nonly consider artists who appeared in the track histories of\nat least 10% of the users.\nThe left-hand panel in Figure 4 shows the 10 artists with\nsmall variance. Many of these are well-established artists\nthat probably undergo little change in style or fan base.\nThe panel in the middle and on the right-hand side of\nFigure 4 show the 10 artists with the largest variance. Many\nof these are popular artists that have a large change in ap-\npeal – i.e., those that go from being relatively obscure to\nquite popular.\nThe variance of a trajectory in only one possible statistic\nthat summarizes a path. We conjecture that other summary\nstatistics will highlight other aspects of an artist’s devel-\nopment, providing additional criteria for exploratory dataFigure 5 : Trajectories of users with age, grouped by age\nin 2005. Each point is labeled with the average age of the\ngroup at that time. The legend also gives the average age\nin 2005 of the users in that group (in parentheses).\nanalysis.\n4.3 User-dynamic model\nThe user-dynamic model is dual to the song-dynamic model,\nin that it models trajectories of users on a map of songs.\nWhile the trajectories of indiviual users provide an inter-\nsting tool for reﬂection, they are difﬁcult to interpret for\noutsiders. We therefore only show aggregate user paths.\nOne such aggregation is shown in Figure 5. Here, we\ncan see the behavior of users when aggregated by age.\nSpeciﬁcally, the users are grouped by age in 2005 in order\nto separate the effect of a person’s absolute age from the\neffect of the change in the average listener’s taste proﬁle.\nDistinctive differences in trajectory can be seen, with\nthe youngest group moving to north, away from Katy Perry\nand many other more “sugary” pop artists, and towards\nmore dance and R&B oriented pop artists as well as the\nhip hop cluster which is further north, outside the ﬁgure.\nThe other age groups see more lateral moves and tend\nto be further north, even when age is ﬁxed. The oldest age\ngroups (where 22 to 30 and 31 to 62 were aggregated with a\nlarger interval due to a smaller number of users in these age\nranges) start very far north, and the 31 to 62 group mostly\nhovers around the eastern part of the ﬁgure. Outside of the\nﬁgure and to the right are where many older rock bands\nsuch as the Rolling Stones and the Beatles lie, and this\noldest age group is also closer to them.\n5. CONCLUSIONS\nWe presented novel probabilistic embedding methods for\nmodeling long-term temporal dynamics of sequence data.\nThese models jointly embed users and songs into a met-\nric space, even when no features are available for either\none. Users and/or songs are allowed to change positionover time, which enables the analysis of long-term dynam-\nics of user tastes and artist appeal and style. The ability to\nvisualize the learned embeddings is a key feature for easy\ninterpretability and open-ended exploratory data analysis.\nWe conjecture that such embedding models will provide\ninteresting tools for analyzing the growing body of listen-\ning data. Furthermore, the embedding models described\nin the paper can easily be adapted and extended to include\nfurther information (e.g., social network data), providing\nmany directions for future work.\n5.1 Acknowledgements\nThis work was supported by NSF grants IIS-1217485, IIS-\n1217686, and IIS-1247696. The ﬁrst author is supported\nby an NSF Graduate Research Fellowship. We would also\nlike to thank the anonymous reviewers for their feedback,\nand Brian McFee for helpful discussions and technical ad-\nvice.\n6. REFERENCES\n[1] N. Aizenberg, Y . Koren, and O. Somekh. Build your\nown music recommender by modeling internet radio\nstreams. In Proceedings of the 21st international con-\nference on World Wide Web , pages 1–10. ACM, 2012.\n[2] S. Chen, J. L. Moore, D. Turnbull, and T. Joachims.\nPlaylist prediction via metric embedding. In Proceed-\nings of the 18th ACM SIGKDD international confer-\nence on Knowledge discovery and data mining , pages\n714–722. ACM, 2012.\n[3] G. Dror, N. Koenigstein, and Y . Koren. Yahoo! music\nrecommendations: modeling music ratings with tem-\nporal dynamics and item taxonomy. In Proceedings of\nthe ﬁfth ACM conference on Recommender systems ,\npages 165–172. ACM, 2011.\n[4] V . Jain and L. Saul. Exploratory analysis and visualiza-\ntion of speech and music by locally linear embedding.\nICASSP , 2004.\n[5] J. L. Moore, S. Chen, T. Joachims, and D. Turnbull.\nLearning to embed songs and tags for playlist predic-\ntion, 2012.\n[6] J. L. Moore and T. Joachims. Fast training of proba-\nbilistic sequence embedding models with long-range\ndependencies. Arxiv pre-print , 2013.\n[7] J. Platt. Fast embedding of sparse music similarity\ngraphs. NIPS , 2004.\n[8] U. Shalit, D. Weinshall, and G. Chechik. Modeling mu-\nsical inﬂuence with topic models. In ICML , 2013.\n[9] J. Weston, S. Bengio, and P. Hamel. Multi-tasking\nwith joint semantic spaces for large-scale music anno-\ntation and retrieval. Journal of New Music Research ,\n40(4):337–348, 2011."
    },
    {
        "title": "Virtualband: Interacting with Stylistically Consistent Agents.",
        "author": [
            "Julian Moreira",
            "Pierre Roy",
            "François Pachet"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1414798",
        "url": "https://doi.org/10.5281/zenodo.1414798",
        "ee": "https://zenodo.org/records/1414798/files/MoreiraRP13.pdf",
        "abstract": "VirtualBand is a multi-agent system dedicated to live computer-enhanced music performances. VirtualBand enables one or several musicians to interact in real-time with stylistically plausible virtual agents. The problem addressed is the generation of virtual agents, each representing the style of a given musician, while reacting to human players. We propose a generation framework that relies on feature-based interaction. Virtual agents exploit a style database, which consists of audio signals from which a set of MIR features are extracted. Musical interactions are represented by directed connections between agents through these features. The connections are themselves specified as mappings and database filters. We claim that such a connection framework allows to implement meaningful musical interactions and to produce stylistically consistent musical output. We illustrate this concept through several examples in jazz improvisation, beatboxing and interactive mash-ups.",
        "zenodo_id": 1414798,
        "dblp_key": "conf/ismir/MoreiraRP13",
        "keywords": [
            "VirtualBand",
            "multi-agent system",
            "live computer-enhanced music performances",
            "stylistically plausible virtual agents",
            "generation framework",
            "feature-based interaction",
            "style database",
            "audio signals",
            "MIR features",
            "musical interactions"
        ],
        "content": "VIRTUALBAND: INTERACTING WITH STYLISTICALLY CONSISTENT\nAGENTS\nJulian Moreira\nSony CSL\njulian.moreira.fr@gmail.comPierre Roy\nSony CSL\nroy@csl.sony.frFranc ¸ois Pachet\nSony CSL\npachetcsl@gmail.com\nABSTRACT\nVirtualBand is a multi-agent system dedicated to live\ncomputer-enhanced music performances. VirtualBand en-\nables one or several musicians to interact in real-time with\nstylistically plausible virtual agents. The problem add-\nressed is the generation of virtual agents, each represent-\ning the style of a given musician, while reacting to hu-\nman players. We propose a generation framework that re-\nlies on feature-based interaction . Virtual agents exploit a\nstyle database, which consists of audio signals from which\na set of MIR features are extracted. Musical interactions\nare represented by directed connections between agents\nthrough these features. The connections are themselves\nspeciﬁed as mappings and database ﬁlters. We claim that\nsuch a connection framework allows to implement mean-\ningful musical interactions and to produce stylistically con-\nsistent musical output. We illustrate this concept through\nseveral examples in jazz improvisation, beatboxing and in-\nteractive mash-ups.\n1. INTRODUCTION\nCollective improvisation is a group practice in which sev-\neral musicians contribute their part to produce a coherent\nmusical whole. Each musician typically brings in musi-\ncal knowledge, taste, and technical skills, more generally\nastyle, which makes him or her unique and recognizable.\nHowever, good improvisations are not only about putting\ntogether the competence of several individuals. Listening\nand interacting to each other is crucial, as it enables each\nmusician to adapt to the global musical output in terms of\nrhythm, intensity, harmony, etc. The combination of indi-\nvidual styles with the deﬁnition of their interaction deﬁnes\nthe quality of a music band. In short, group improvisation\ncan be seen as principled interactions between stylistically\nconsistent agents.\nMany works have attempted to model and simulate the\nbehavior of a real musician. A MIDI-based model of an\nimproviser’s personality is proposed in [6], to build a vir-\ntual trio system, but no explicit interactions between vir-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.tual agents and real musicians are proposed. Improtek [11]\nis a system for live improvisation that generates musical\nsequences built in real-time from a live source using con-\ncatenative synthesis. Improtek plays along with a musi-\ncian improvising on harmonic and beat-based music, in\nthe style of this musician, but interactions with him are\nbased on feature similarity measures, thereby limiting the\nscope of man-machine interactions. The Jambot [4] infers\nin real-time various features from an audio signal and then\nproduces percussive responses, following alternatively pre-\ndeﬁned behaviors, but the style of the Jambot itself is not\nclearly deﬁned. Beatback [7] generates a MIDI signal based\non rhythmic patterns and using Markov models. Besides\nthe limitations of MIDI, interactions are limited to rhyth-\nmic elements only. This limitation also applies to the Chi-\nmera Architecture [3], a system that infers rhythmical in-\nformation from an audio input, and generates percussive\nsounds depending on scenarios that ﬁt with the current mu-\nsical context. [9] presents a system that learns rhythmic\npatterns from drum audio recording and synthesizes musi-\ncal variations from the learnt sequence. In addition to be-\ning dedicated to percussive sounds only, this system has no\nreal-time live application. [16] presents an interactive mu-\nsic system driven by syncopation measurements, but also\naddresses rhythmical features only. [10] describes a system\nthat interacts in real-time with a human musician, by adapt-\ning its behavior both on prior knowledge (database of mu-\nsical situations) and parameters extracted during the cur-\nrent session, but this system forces the musician to manip-\nulate a graphical interface during the improvisation, which\nmakes the system hardly usable in a live performance for a\nsolo musician.\nIn this paper we revisit the issue of musical interac-\ntion with stylistically consistent agents by taking a feature-\nbased approach to the problem. We introduce VirtualBand\n(VB), a reactive music multi-agent system in which human\nmusicians can control and interact with virtual musicians\nthat retain their own style.\nIn VB, both human and virtual musicians are repre-\nsented by agents that interact together through feature-based\ninteractions. Human agents extract their features at ev-\nery beat from the audio input of their corresponding musi-\ncian. Virtual agents use features of audio chunks stored in a\ndatabase. Interactions are modeled by connections , which\nare the core contribution of our approach. A connection\nis a directed link from a master (human or virtual) agent\nto aslave virtual agent. In reaction to a feature value pro-vided by the master agent, the connection speciﬁes to the\nslave agent which audio chunk to play from its database.\nVB can be seen as a reactive rather than deliberative multi-\nagent system [8]. Its potential lies in the seemingly inﬁnite\npossibilities provided by feature interaction, as illustrated\nin this paper.\nIn Section 2, we describe the main components of VB.\nIn Section 3, we illustrate feature interaction with several\nconﬁgurations of the system for jazz improvisation. Sec-\ntion 4 describes applications of VB in two other musical\ncontexts: beatboxing and automatic mash-up.\n2. SYSTEM DESCRIPTION\nThe core of VB consists of 1) a clock that establishes the\ntempo and sends notiﬁcations at each beat to every agent -\nwe consider ﬁxed tempos in the current version, 2) a set of\nagents that receives these notiﬁcations and generate audio\nand 3) an audio playback engine.\n2.1 Agents\nThere are two types of agents: human and virtual agents,\nrepresenting respectively actual and virtual musicians. Hu-\nman agents are responsible for extracting acoustic features\nfrom the audio signal of real musicians in real-time. These\nfeatures are the main controlling device for virtual agents\nvia connections, as explained below.\nVirtual agents are designed to play in the style of the\nmusicians they represent. To build a virtual agent, we record\nthe musician in a musical situation that ﬁts with the tar-\ngeted performance context. The recorded audio is stored\nin astyle database , organized in musically relevant chunks\n(usually beats and bars). The musician is asked to play\nso as to fully express his musical style, i.e., cover a large\nrange of musical situations (e.g., playing with different in-\ntensities, in different moods, staccato or legato, using var-\nious patterns). Of course, it is difﬁcult to express one’s\nstyle exhaustively, so style databases are limited to speciﬁc\nmusical situations , e.g., deﬁned by musical genre such as\nBossa Nova, Swing, etc. and a tempo. Note that the subject\nofindividual style capture is still in its infancy and further\nstudies should reﬁne this concept.\nIn the database each chunk is associated to a set a fea-\nture values. A set of MIR features are automatically ex-\ntracted from the audio signal of the chunk, e.g., RMS,\nnumbers of onsets, spectral centroid, harmonic to noise\nratio, or chroma. Contextual features are also associated\nto each chunk such as the harmony. Note that the har-\nmony can either be extracted automatically or be imposed\nby a chord sequence. During performance, each virtual\nagent uses concatenative synthesis [15] to generate music\nstreams as seamless concatenations of database chunks.\n2.2 Connections as a Combination of Mappings\nA connection between two agents models the intention un-\nderlying a musical interaction between two musicians. Con-\nnections are directed from a master agent AMto a slaveagent AS, and relate a master feature FMofAMto a slave\nfeature FSofAS.\nAt every beat, the connection selects the audio chunk\nthatASis going to play, among the available chunks of\nits associated database DS. To perform this task, the con-\nnection applies two successive mappings: 1) a feature-to-\nfeature mapping from the domain XMofFMto the do-\nmain XSofFS, and 2) a feature-to-chunk mapping from\nXStoDS.\n2.2.1 The feature-to-feature (f2f) mapping\nFigure 1 . Illustration of a percentile mapping (in red) be-\ntween two distributions. 1) RMS values extracted from a\nguitar track (top) and 2) hit-counts extracted from a drum\ntrack (bottom). Both tracks come from a recording of the\njazz standard Body and Soul .\nFMandFSmay take their values in different domains\n(for instance integers or ﬂoating points) and with differ-\nent value distributions. Normalizing the values of FMand\nFSallows to deﬁne a simple feature-to-feature (f2f) map-\nping, that maps any value xMofFMonto the same value\nxMofFS. However, with such a mapping, the distribution\nof the virtual agent’s actual output doesn’t match the dis-\ntribution of the database, i.e., the virtual agent doesn’t play\nconsistently in the style of the musician it represents.\nInstead we use a percentile-based f2f mapping, which\npreserves this distribution. If Dis the domain of a variable\nx, the percentile function is deﬁned by:\npercentile (x; D ) =jfd2Djd < xgj\njDj(1)\nThe f2f mapping is then deﬁned as:\nf2f(xM) =percentile\u00001(percentile (xM; DM); DS)\n(2)\nSuppose a guitarist wants a virtual drummer to adapt its\ndensity to the guitar’s energy. As a proxy of energy, we use\nmaster feature FM= RMS for the guitar. Drums’ density\nis represented by slave feature FS= hit-count (i.e., number\nof onsets in the chunk). Fig. 1 shows the distribution of\nthe RMS values of a typical guitar recording, and of thehit-count values of a typical drum recording. The distribu-\ntions strongly differ, by their total number of values (257\nvalues for the RMS, 157 for the hit-count), ranges (RMS\nvalues range from 0.03 to 0.16 whereas hit-count values\nrange from 0 to 19) and number of bins (for the RMS, there\nare 50 different bins, and only 19 for the hit-count). The\nf2f mapping selects the hit-count value xSwith the same\npercentile as the RMS value xM. A value xM= 0:076,\ncorresponds to a percentile of 0.067. The hit-count value\nwith the same percentile is 2, as shown in Fig. 1.\n2.2.2 The feature-to-chunk (f2c) mapping\nEventually, the connection selects which chunk to play,\ngiven xS. This is implemented by the feature-to-chunk\n(f2c) mapping, which represents the musical intention be-\nhind the connection. We specify this mapping operationally ,\nby composing various ﬁlters . Given xSand a subset C\u0012\nDSof chunks, each ﬁlter selects a subset of Cthat satisﬁes\na speciﬁc rule. Filters are deﬁned once for all, as illustrated\nin the following examples:\n\u000fClosestChunks (C; xS)def=argmin\nc2CjFS(c)\u0000xSj\n\u000fFarthestChunks (C; xS)def=argmax\nc2CjFS(c)\u0000xSj\n\u000fMatchingChunks (C; xS)def=fc2CjFS(c) =xSg\n\u000fUnmatchingChunks (C; xS)def=fc2CjFS(c)6=xSg\n\u000fChordMatchingChunks (C; xS)def=fc2Cjchord (c)\nis substitutable to xSg\n\u000fAt a given beat b,\nAdaptiveClosestChunks (C; xS)\ndef=8\n>>>><\n>>>>:ClosestChunks (C; xS)ifbis the ﬁrst beat\nof a bar\nfcgwhere cis the chunk that follows in DS\nthe chunk currently playing otherwise\n\u000fRandomChunk (C)def=random (C)\nThe f2c mapping of a connection is a composition of\nﬁlters, such as:\nRandomChunk (fn(: : : f 2(f1(DS; xS); xS): : : ; x S))\nwhere f1; : : : ; f nare ﬁlters.\nIn our example of the guitar controlling the drums, we\nuse a ClosestChunks ﬁlter. A value of xM= 0:076for\nthe guitar RMS will trigger a drum chunk with a hit-count\nvalue closest to 2.\nWhen the composition of ﬁlters yields an empty sub-\nset, speciﬁc procedures are applied, such as using a default\nstyle database for the instrument.\n2.2.3 Multiple masters for one slave\nTo better approximate the complexity of interaction occur-\nring in a real band, a slave agent may be connected to sev-\neral master agents. In this case, the f2f mappings are ﬁrst\ncomputed independently and then intersected.2.3 Representing Harmony as a Connection\nIn tonal music, such as jazz, musicians improvise on a\nchord sequence that is known to all beforehand. VB im-\nplements such shared harmonic information by a speciﬁc\nagent that represents the chord sequence.\nThe chord sequence agent ACprovides a unique feature\nwhose value xCis the name of the next chord in the se-\nquence. ACis typically connected to harmony-dependent\nagents, such as pitched instruments (e.g., piano, guitar).\nFor such a virtual agent A, the connection is deﬁned as\nfollows: the master agent is AC, the master feature value\nisxC, and the slave agent is A.\nDepending on the expected behavior of the virtual agent,\nwith respect to harmony, various slave features and map-\npings may be used.\nThe most typical example is that of a piano comping\nagent APthat is expected to play chords in the same har-\nmony as xC. Each audio chunk of the database DPis as-\nsociated to a feature value xPwhich represents the corre-\nsponding chord name. In this case, the slave feature value\nisxP, the f2f mapping is the identity: xP=xC, and the\nﬁlter is ChordMatchingChunks . Note that the matching is\nnot necessarily strict, but can use substitution rules, as ex-\nplained in the next section.\n2.4 Reﬂexive Style Databases\nVB also features a memoryless mode, i.e., which doesn’t\nrequire pre-recorded style databases. One motivation is to\navoid “canned music” effects caused by the audience being\nunaware of the content of the style databases. In this mode,\ndatabases are built on-the-ﬂy, typically like interactive sys-\ntems such as Continuator [12] or Omax [11]. Reﬂexive\nstyle databases are used to generate various species of self-\naccompaniments such as duos or trios with oneself [13].\nConceptually, reﬂexive style databases do not differ from\npre-recorded ones. Technically, they raise various real-\ntime issues (segmentation and feature extraction must be\nperformed in real-time without interfering with the main\nVB loop) that are not discussed in this paper.\nMore interestingly, reﬂexive style databases raise spar-\nsityissues. Because a database contains only what the mu-\nsician has played so far, its size will typically be much\nsmaller than that of pre-recorded databases. As a conse-\nquence, the system has far fewer possibilities for genera-\ntion. This is particularly annoying in contexts using pre-\ndetermined chord grids. In principle, the system requires\na long feeding phase , to accumulate at least one chunk for\nevery chord of the sequence before it can start playing back\nrelevant audio material. Such a feeding phase is usually\nboring for the musician as well as for the audience.\nIn order to reduce feeding, we propose to automati-\ncally expand style databases by using audio transforma-\ntions, such as transpositions , implemented with pitch shift-\ning algorithms [14]. In VB, the audio chunks of a database\ncan be pitch-shifted so that the transposed bars can be used\nin new harmonic situations.\nWe also use so-called substitution rules to further ex-\npand the style database. Substitution rules consist in re-Figure 2 . The feeding phase reduction using (a) transposi-\ntions, (b) substitutions, and (c) a combination of both.\nplacing a chord by another one with an equivalent har-\nmonic function. This operation is widely used in jazz to\nbring variety in performance [1]. VB uses chord substi-\ntutions to play in a certain harmonic context audio that\nwas recorded in another harmonic context, provided the\ntwo contexts may be substituted.\nBy combining transpositions and substitutions, the feed-\ning phase is drastically reduced. The three examples of\nFig. 2 show how to harmonize the song Solar from a lim-\nited number of input chords, using transpositions (a), sub-\nstitutions (b), and a combination of both (c). The input\nchords (highlighted) generate the remaining chords of So-\nlar (marked in the same color). Without substitutions or\ntranspositions, 12 input chords would be required in the\nfeeding phase. In contrast, (a) requires 5 input chords, (b)\nrequires 8, and (c) only 2. For instance in (c), G min 7 can\nbe substituted for C 7using substitution “G min 7 : C 7”,\nbut also F min 7 using a 1 tone downward transposition,\nandBb 7 with a combination of these two operations.\n3. APPLICATION TO JAZZ IMPROVISATION\nThe following examples illustrate various conﬁgurations of\nVB with one human guitarist and one or two reﬂexive vir-\ntual agents. In the ﬁrst and second examples, we present\nduos in which the virtual agent is controlled respectively\nby a spectral centroid feature and by a rhythm pattern fea-\nture extracted from the human guitarist. Then, we extend\nthe example to a guitar trio conﬁguration. An accompany-\ning web site illustrates all these conﬁgurations with videos\nof the corresponding performances1.\n3.1 Spectral Centroid-Based Duo\nTwo jazz guitarists improvising together commonly use a\nquestion-answer interaction scheme: one of the guitarists\nplays a melody; as soon as he ﬁnishes, the other guitarist\nplays another melody that borrows elements from the ﬁrst\none. Typically the answering melody is in the same pitch\nrange or shares similar rhythm patterns with the original\nmelody. While a guitarist proposes a melody, the other one\neither stops playing or accompanies the ﬁrst one with, e.g.,\nchord comping.\n1http://francoispachet.fr/virtualband/virtualband.html\nFigure 3 . Score of a spectral-centroid based duo: a human\nagent ( AH) plays the melody of Solar. A virtual agent AV\nmatches the spectral centroid of AH, with a one-bar delay.\nAt bar 11 ( Db maj7 )AVuses a transposition of bar 5 ( F\nmaj7 ) ofAHto match the spectral centroid of bar 10 of\nAH. At bar 8 ( Bb 7 ), substitution “F min 7 : Bb 7” allows\nAVto play bar 7 of AH(F min 7 ).\nWe simulate these scenarios with various conﬁgurations\nof VB based on pitch features of audio content. A reﬂexive\nstyle database records the incoming audio of a human gui-\ntaristAHand extracts the spectral centroid of each bar. We\nchose the spectral centroid as an approximation of pitch,\nbut more reﬁned descriptors could be used interchange-\nably. A virtual agent AV, associated to the database, is\nconnected to AH. At each bar, this connection is speciﬁed\nby the following elements:\n\u000fmaster feature FH= spectral centroid of AH; slave\nfeature FV= spectral centroid of AV;\n\u000ff2f = percentile from a value FHto a value of FV\n(details in Section 2.2.1);\n\u000ff2c = ClosestChunks (details in Section 2.2.2).\nNote that in the meantime, another connection ensures that\nAValso plays according to the harmonic constraints (see\nSection 2.3 for details).\nIn this conﬁguration, the system behaves as a self har-\nmonizer: AVfollows the spectral centroid of AHwith\na one-bar delay (see Fig. 3), using music material that\nsounds like what the guitarist just played.\nReplacing the ﬁlter in the conﬁguration above by:\n\u000ff2c = FarthestChunks\nimplements another musical intention: the agent plays au-\ndio that is far away pitch-wise to the human’s input. In this\nconﬁguration, the two outputs (human and virtual guitar)\nare clearly distinct.\n3.2 Rhythm-Based Duo\nQuestion-answer musical dialogues can also be based on\nrhythmical similarities. A guitarist plays a rhythm pattern\nfor a few bars, and the other one responds by playing a\nsimilar pattern. We introduce a feature that represents the\nrhythm pattern: the RMS proﬁle . This proﬁle is obtainedby computing the RMS 12 times per beat over one bar.\nThe agent plays back chunks with a similar proﬁle, with\na systematic one-bar delay. Technically, the connection is\nspeciﬁed by:\n\u000fmaster feature FH= RMS proﬁle of AH; slave fea-\ntureFV= RMS proﬁle of AV;\n\u000ff2f = identity, i.e., for a value xHofFHand a value\nxVofFV:xV=xH;\n\u000ff2c = ClosestChunks . The distance between two RMS\nproﬁles is the scalar product of the two vectors.\nThis mode is fun and lively as the interaction is easily per-\nceived by the musician and the audience. However, it re-\nquires larger databases, so it should be used when the sys-\ntem has accumulated enough rhythm samples to play back\ninteresting variations.\nAs before, changing the ﬁlter to:\n\u000ff2c = FarthestChunks .\nimplements a very different musical interaction: the virtual\nagent plays chunks that are dissimilar to the input of the\nmusician, which is more difﬁcult to anticipate for a human\nthan similar patterns.\n3.3 Trio\nIn a typical jazz guitar trio, one of the guitarists impro-\nvises melodies on a harmonic grid, while the other two pro-\nvide respectively chordal and bass accompaniments. These\nroles (melody, chords, and bass) represent different guitar\nplaying modes . During an improvisation, guitarists typi-\ncally shift roles in turn. When a musician takes the lead\n(solo), the other guitarists adapt their behavior so that each\nmode is always played by someone. With VB, we rep-\nresent this conﬁguration so that a guitarist can be self-\naccompanied by two reﬂexive virtual agents, sharing the\nsame reﬂexive database.\nIn addition to the RMS proﬁle, we extract the play-\ning mode from each recorded beat, among four possible\nmodes: melody, chord, bass and silence [2]. Each virtual\nagent is associated to a unique playing mode (one to the\nbass, one to the chords), and agents follow a mutually ex-\nclusive rule, i.e., they play only if the human guitarist is\nnot playing in the same mode, following [13]. Given a vir-\ntual agent AV(virtual bass or virtual chords), such a rule\nis easily modeled by a connection:\n\u000fmaster feature FH= playing mode of AH; slave fea-\ntureFV= playing mode of AV;\n\u000ff2f = identity;\n\u000ff2c = UnmatchingChunks .\nFurthermore, like in the previous example, each agent fol-\nlows the rhythmical patterns of the guitarist. For instance,\na walking bass, or chord comping (a lot of notes per bar,\nregularly spaced) can be triggered by playing a fast and\nregular melody. This conﬁguration provides the feeling of\na standard jazz guitar trio to a solo musician.4. OTHER APPLICATIONS\nIn this section, we describe applications of VB in two other\nmusical contexts: beatboxing and mash-ups; also illus-\ntrated by videos on our web site.\n4.1 Reﬂexive Beatboxing\nBeatboxing is a music style where musicians use their mouth\nto simulate percussion. Beatboxing also involves hum-\nming, speech or singing (see [17]). Common beatboxers\ntypically alternate between modes, but some great beat-\nboxers are able to play two modes at the same time (e.g.,\npercussion and humming).\nBeatboxing with VB aims at augmenting the perfor-\nmance of a moderately good beatboxer by allowing him\nor her to play, via reﬂexive virtual agents, several modes\nat the same time. Using the same connection settings as\nin Section 3.3, the system records and stores in a reﬂexive\ndatabase the incoming audio of the real beatboxer. From\nthis audio, the database distinguishes between two play-\ning modes: the percussion mode and the humming mode.\nThen two virtual agents are connected to this database, one\nrepresenting the percussion mode and the other the hum-\nming mode. The classiﬁcation is performed following a\nsimilar scheme to [2], except for the set of features se-\nlected by the classiﬁer (here harmonic-to-noise ratio, spec-\ntral centroid and Yin). Following a mutually exclusive\nprinciple, virtual agents play alternatively, depending on\nthe mode of the human beatboxer, and following his or her\nrhythmic patterns. Table 4 illustrates a typical session with\nsuch an augmented beatboxer.\nFigure 4 . A 16-bar performance of the beatboxing system:\ntandkare percussive sounds, OandAare hummed vow-\nels.APis the percussive agents, and AHis the humming\nagent. Agents follow a mutually exclusive principle and\ntry to match the human’s rhythm with a one-bar delay.\n4.2 Mash-up\nA mash-up is created by blending two or more songs, usu-\nally by overlaying a track of one song over the tracks of an-\nother [5]. Mash-ups exploit multi-track songs whose tracks\nare available as separated audio ﬁles. A straightforward\nway to implement mash-up with VB is to represent eachtrack of each song as a virtual agent. The database of each\nagent consists of all the audio chunks obtained by segment-\ning the corresponding track. The mash-up is obtained by\nmuting and replacing one agent by a track agent represent-\ning the same instrument of another song. The virtual agent\nrepresenting the replacing track is connected to the muted\nagent of the original song.\nFor instance, one can replace the drums in song Rox-\nanne (The Police ) by another drummer. The muted agent\nthat represents the original drum track controls the substi-\ntute drummer through rhythm patterns. Technically, the\nsettings of the connection are the same as presented in the\nﬁrst example of Section 3.2. On the accompanying web\nsite we provide mash-ups of Roxanne with the drummer of\n1)Smells Like Teen Spirit (Nirvana ), 2) Hey (Pixies ), 3)\nBossa Nova and 4) Funk drums played by Jeff Boudreaux.\nWe can hear in the provided example each drummer\nplaying in his style while seemingly following the song’s\nstructure by, e.g., playing breaks at the right time, or play-\ning more intensively on choruses and bridges.\n5. CONCLUSION\nWe revisit the problem of interacting with stylistically con-\nsistent agents from a MIR viewpoint. In VirtualBand, in-\nteractions are speciﬁed using features pairs , taken from\nthe vast library of features developed in MIR. VB agents\nare reactive but not deliberative, i.e., they do not attempt\nto exhibit autonomy, make goals, or plan ahead. But the\nexamples show that even with only reactive agents, rich\nand complex interactions can take place, by exploiting the\ncomplex correlations that typically occur between pairs of\nfeatures computed on human audio signals.\nHowever, VB only scratches the surface of the new ﬁeld\nofindividual style modeling . Current work focuses on is-\nsues like how to “saturate” a style database, or how to pre-\ndict the emergence of long-term structure from low-level\nfeature interaction.\n6. ACKNOWLEDGEMENT\nThis research is conducted within the Flow Machines project\nwhich received funding from the European Research Coun-\ncil under the European Union’s Seventh Framework Pro-\ngramme (FP/2007-2013) / ERC Grant Agreement n. 291156.\n7. REFERENCES\n[1] J. Coker. Elements of the Jazz Language for the Devel-\noping Improvisor . Alfred Music Publishing, 1997.\n[2] R. Foulon, F. Pachet, and P. Roy. Automatic classiﬁca-\ntion of guitar playing modes. Proc. of the CMMR Sym-\nposium , 2013.\n[3] T. Gifford and A.R. Brown. Do androids dream of elec-\ntric chimera? In Proc. of the ACMC , pages 56–63,\n2009.[4] T. Gifford and A.R. Brown. Beyond reﬂexivity: Me-\ndiating between imitative and intelligent action in an\ninteractive music system. In Proc. of the HCI Confer-\nence, 2011.\n[5] J. Grobelny. Mashups, sampling, and authorship:\nA mashupsampliography. Music Reference Services\nQuarterly , 11(3-4):229–239, 2008.\n[6] M. Hamanaka, M. Goto, H. Asoh, and N. Otsu.\nA learning-based jam session system that imitates a\nplayer’s personality model. In Proc. of the IJCAI , vol-\nume 18, pages 51–58, 2003.\n[7] A. Hawryshkewich, P. Pasquier, and A. Eigenfeldt.\nBeatback: A real-time interactive percussion system\nfor rhythmic practise and exploration. In Proc. of the\nNIME Conference , pages 100–105, 2011.\n[8] L. Iocchi, D. Nardi, and M. Salerno. Reactivity and\ndeliberation: A survey on multi-robot systems. In\nM. Hannebauer, J. Wendler, and E. Pagello, editors,\nBalancing Reactivity and Social Deliberation in Multi-\nAgent Systems , volume 2103 of Lecture Notes in Com-\nputer Science , pages 9–34. Springer, 2000.\n[9] M. Marchini and H. Purwins. Unsupervised generation\nof percussion sound sequences from a sound example.\nInProc. of the SMC Conference , 2010.\n[10] A. Martin, A. McEwan, C.T. Jin, and W. L. Martens. A\nsimilarity algorithm for interactive style imitation. In\nProc. of the ICMC , pages 571–574, 2011.\n[11] J. Nika and M. Chemillier. Improtek: integrating har-\nmonic controls into improvisation in the ﬁliation of\nOMax. In Proc. of the ICMC , pages 180–187, 2012.\n[12] F. Pachet. The continuator: Musical interaction with\nstyle. Journal of New Music Research , 32(3):333–341,\n2003.\n[13] F. Pachet, P. Roy, J. Moreira, and M. d’Inverno. Reﬂex-\nive loopers for solo musical improvisation. In Proc. of\nthe SIGCHI Conference , CHI ’13, pages 2205–2208.\nACM, 2013. Best paper honorable mention award.\n[14] C. Sch ¨orkhuber and A. Klapuri. Pitch shifting of audio\nsignals using the constant-q transform. In Proc. of the\nDAFx Conference , 2012.\n[15] D. Schwarz. Current research in concatenative sound\nsynthesis. In Proc. of the ICMC , pages 9–12, 2005.\n[16] G. Sioros, A. Holzapfel, and C. Guedes. On measur-\ning syncopation to drive an interactive music system. In\nProc. of the ISMIR Conference , pages 283–288, 2012.\n[17] D. Stowell and M. D. Plumbley. Characteristics of the\nbeatboxing vocal style. Dept. of Electronic Engineer-\ning, Queen Mary, University of London, Technical Re-\nport, Centre for Digital Music C4DMTR-08-01 , 2008."
    },
    {
        "title": "Combining Timbric and Rhythmic Features for Semantic Music Tagging.",
        "author": [
            "Nicola Orio",
            "Roberto Piva"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415834",
        "url": "https://doi.org/10.5281/zenodo.1415834",
        "ee": "https://zenodo.org/records/1415834/files/OrioP13.pdf",
        "abstract": "In this paper we propose a novel approach to music tagging. The approach uses a statistical framework to model two acoustic features: timbre and rhythm. A collection of tagged music is thus represented as a graph where the states correspond to the songs and the models probabilities are related to the timbric and rhythmic similarity. Under the assumption that acoustically similar songs have similar tags, we infer the tags of a new song by adding it to the graph structure and observing the tags visited in acoustically meaningful random walks. The approach has been tested using the CAL500 dataset, with encouraging results in terms of precision.",
        "zenodo_id": 1415834,
        "dblp_key": "conf/ismir/OrioP13",
        "keywords": [
            "novel",
            "approach",
            "statistical",
            "framework",
            "timbre",
            "rhythm",
            "acoustic",
            "features",
            "graph",
            "random walks"
        ],
        "content": "COMBINING TIMBRIC AND RHYTHMIC FEATURES\nFOR SEMANTIC MUSIC TAGGING\nNicola Orio\nDepartment of Cultural Heritage\nUniversity of Padua, Italy\norio@dei.unipd.itRoberto Piva\nDepartment of Information Engineering\nUniversity of Padua, Italy\npiva.roberto.88@gmail.com\nABSTRACT\nIn this paper we propose a novel approach to music tag-\nging. The approach uses a statistical framework to model\ntwo acoustic features: timbre and rhythm. A collection\nof tagged music is thus represented as a graph where the\nstates correspond to the songs and the models probabilities\nare related to the timbric and rhythmic similarity. Under\nthe assumption that acoustically similar songs have similar\ntags, we infer the tags of a new song by adding it to the\ngraph structure and observing the tags visited in acousti-\ncally meaningful random walks. The approach has been\ntested using the CAL500 dataset, with encouraging results\nin terms of precision.\n1. INTRODUCTION\nThe ability of humans to associate tags or generic meta-\ndata with multimedia content is a difﬁcult task to simulate,\nbecause it relies on subjective judgments and on the iden-\ntiﬁcation of connections between abstract concepts. In the\ncase of music content, tagging has always been a feature\nof online streaming services like LastFM, Apple Genius,\nPandora or Grooveshark, since these services rely on music\ndescriptors to deliver the right songs to the right user. Their\ntags have different origins though: while Pandora pays mu-\nsic experts to annotate music with reliable and expressive\nterms, the other services rely on user generated tags and\nplaylists and exploit statistics tools like collaborative ﬁl-\ntering [8] for annotating and recommending music. A typ-\nical problem of manual tagging regards the annotation of\nnew items. While songs by renowned artists may easily get\nproper tagging by their advertisers, there are thousands of\ntracks – for instance produced by small independent labels\n– that are likely to be unreachable because of the lack of\ngood descriptors.\nAnother interesting problem is the long tail distribution:\nthe analysis of listening charts highlights that the distribu-\ntion of play counts over artists follows a power law. This\nmeans that a restricted number of artists gets the major-\nity of play counts. However, the total play counts of the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.long tail largely outnumbers the one of the top artists. This\nsituation is also common for the global charts of a music\nstreaming service and has a reﬂection on how these ser-\nvices make money [6]. The problem is related to tagging\nbecause in most of the times the long tail consists of poorly\ntagged music pieces.\nEarly works on automatic music tagging addressed the\nrecognition of the music genre [11,12,25]. The subjective-\nness of “genre” classiﬁcation led researchers to propose a\nnovel genre taxonomy [18] and to identify music tagging\nas a more broad concept that includes other types of infor-\nmation – like track’s pace and dance-ability. To this end,\ntagging has also been deﬁned as “music semantic annota-\ntion” [10, 24]. Other approaches have explored the associ-\nation between related tags [16], or the inclusion of mined\nsocial tags to infer some relation between tags and audio\nfeatures [2].\nA variety of features were considered in search of the\nright combination to correctly annotate novel songs. First\nattempts started with Mel Frequency Cepstral Coefﬁcients\n(MFCC), which were successfully used for music classi-\nﬁcation while improvements were obtained with the aid\nof other sources such as social tags [5], or temporal fea-\ntures [15]. More recent work investigates the improvement\nof MFCC by using Principal Component Analysis [13]\nwhile learning frameworks have been applied as well, such\nas Support Vector Machines [3] and Artiﬁcial Neural Net-\nworks [9].\nThis paper describes a method for semantically tagging\nmusic clips by exploiting timbre and rhythm features rep-\nresented in a single statistical framework, where audio fea-\ntures are related to different model parameters that have\nbeen developed on top of a previously deﬁned retrieval\nmodel based on content and context descriptors [17].\n2. THE TAGGING MODEL\nOur basic assumption is that the relation between tags and\nmusic features can be better exploited by using multiple\nfeatures in a single tagging model. Our framework, in-\nspired by Hidden Markov Models (HMMs), accounts for\nmusic similarity in terms of two different audio features:\ntimbre represented by MFCC and rhythm represented by\nRhythm Histograms (RH).Figure 1 . A graphical representation of the tagging model\n2.1 Model Elements\nGiven the close relationship with HMMs, we introduce the\ndifferent elements of the model using, when possibile, the\nsame notation introduced in [21].\nStates: A song is represented as a state Sifrom a setS=\nfS1;:::;SNg, and state at time tis denoted as s(t).\nPrior Probabilities: The query song to be tagged is added\nto the model as state Sq, requiring the computation of the\ntransition-wise similarity with each song in the collection\nto obtainaqifor alli. Each path in the model is forced to\nstart from the query song.\nTransition Probabilities: They are related to the acous-\ntic similarity between the songs using one of the two fea-\ntures, thus they can be related either to timbre or to rhythm\nsimilarity. Referring to the usual naming convention for\nHMMs,aijstands for the acoustic similarity of song Si\nwith songSj.\nObservation Probabilities: They are related to the acous-\ntic similarity between the songs and the query song, com-\nputed from a different music dimension than the on used\nfor transitions. We introduce symbol \u001ei(q)as the proba-\nbility for song Siof emitting the audio features of query\nsongSq.\nTags: The information regarding the tags of the known\nsongs is stored in a separate variable, named bi(w), that\nassociate tag wto songSiin the collection. We assume\nthat the vocabulary of tags consists in jVjdifferent symbols\nbelonging toV=fw1;w2;:::;wjVjg. The vector biis in\nbinary form, it contains a 1in positionjif tagjis present,\n0otherwise.\nThe model is then exploited to generate random walks\nacross the collection, starting from the song to be tagged.\nTransition probabilities guarantee acoustic consistency of\nthe songs in the paths, while observation probabilities guar-\nantee acoustic similarity with the query song. A graphical\nrepresentation of the model can be found in Figure 1.\nAlthough in principle aij>0for each (i;j)pair, we\nartiﬁcially keep, for each song Si, only a subsetR(Si)of\nnon-zero transitions. That is, we keep the top Phighesttransition probabilities for each state, and we set to 0the\nremaining transitions, with an increase in model scalabil-\nity. The value of Phas been set to 10% of the global num-\nber of the songs. Transitions are normalized to maintain\nthe stochastic properties.\nAs a consequence of the analogy with HMMs, the ob-\nservation’s feature plays a more important role than the\ntransition’s feature in discriminating the songs. This hap-\npens because observation’s feature is compared to the query\nat each step in a random walk across the model, while the\ninﬂuence of transition similarity decreases as the path gets\nlonger. Having the tags in a separate structure let us grow\nthe model easily if there is a novel tag.\n2.2 Acoustic Features\nWe use MFCC to capture the timbre signature of each song.\nMFCC are computed using 23ms, half-overlapping win-\ndows over 6 seconds of music after the ﬁrst 30 seconds.\nWe thought 6 seconds may be a good amount of data to\nbe used for automatic tagging for online services and for\nfast tagging of large collections. The output is a 13-sized\nvector for each window, but since we include the ﬁrst and\nsecond derivatives, we end up with a matrix of 39 elements\nmultiplied by the number of windows.\nWe use Rhythm Histograms (RH) and Rhythm Patterns\n(RP) to hold information regarding the rhythm of a given\npiece of music, as described in [19, 22] as psychoacousti-\ncally motivated Rhythm Patterns . Rhythm Patterns them-\nselves are a variation of Fluctuation Pattern [7]. RH are\ncomputed using the same 6 seconds of audio as MFCC.\nSimplifying some psychoacoustic adjustments steps in the\ndeﬁnition of RP and slightly varying the calculation param-\neters, we ended up with a matrix descriptor of 120 modu-\nlation frequencies by 24 frequency bins.\n2.3 Similarity Measures\nWe adopted two approaches to compute the similarity be-\ntween two songs. The ﬁrst one considers each frame as a\nﬁxed-width word in a dictionary [14]. We calculate meanand covariance of the (letters of the) words over the length\nof the song and the result is a representative multivariate\nGaussian distribution of the song. To compare these two\ndistribution we use the Kullback-Leibler divergence (KL\ndivergence):\nKL(pjjq)\u0011Z\np(x) logp(x)\nq(x)dx; (1)\nwherep(x)andq(x)are the two distributions. The KL di-\nvergence is then transformed to a [0;1]value (with 1rep-\nresenting two identical songs) by exponentiating the diver-\ngence:\nsim(p;q) =e\u0000\rKL (pjjq); (2)\nwhere\ris a parameter to be tuned (for this work it has\nbeen set to 0:009, see [20]). In this paper this similarity is\nreferred to as “single gaussian”.\nWe investigated also a number of alternative similarity\nmeasures [4]: the Cosine similarity and its varied version\nModiﬁed Chord, a similarity based on Euclidean distance,\ntwo biology-based similarities (Bray-Curtis and Ruzicka),\nthe Similarity Ratio, the Kulczynski similarity and the ex-\nponentiated version of the discrete KL-divergence (very\nsimilar to the one from Equation 2). This second group\nof measures is based on a vectorized version of the matrix\nrepresentations. For MFCC we keep the absolute value of\nthe time-wise sum, obtaining a 39-sized vector represent-\ning the average timbre across the piece of music. For the\nRH we sum along the frequency bins (barks) to obtain a\n118 sized vector representing the inﬂuence of each modu-\nlation frequency in the [0;10]Hz range.\n2.4 Querying the model\nWe developed a modiﬁed version of the Viterbi algorithm,\nwhich application to HMMs can be found in [21] where \u000e\nrepresents the probability of the optimal path and  is used\nto keep track of its actual state sequence. We refer to the\nquery song using the subscript q(e.g.Sqfor the song, bq()\nfor the tags, and so on).\nInitialization: As introduced in Section 2.1 the song to\nbe taggedSqis inserted in the model and the initialization\nstep is, for all i= 1;:::;N :\n\u000e1(i) =(\n1i=q\n0i6=q(3)\n 1(i) =0: (4)\nRecursion: Here we present a variation from the original\nViterbi. For t= 2;:::;T , andi= 1;:::;N :\n\u000et(i) = max\n1\u0014i\u0014N[\u000et\u00001(i)aji]\u001eq(i) (5)\n t(i) = arg max\n1\u0014i\u0014N[\u000et\u00001(i)aji] (6)\nari=ari\n\u0011forr= t(i); \u0011= 10: (7)\nWhere\u001eq(i)is the similarity between SqandSiand can\nbe computed using one of the possible similarity measures\nintroduced in Section 2.3. Equation 7 aims at preventinglooping by lowering the probability of performing a tran-\nsition twice [20].\nDecoding: The most probable path is computed as in the\noriginal Viterbi algorithm:\ns(t)\u0003=8\n<\n:arg max\n1\u0014i\u0014N[\u000et(i) ]ift=T\n t+1(s(t+ 1)\u0003)ift=T\u00001;:::; 1;(8)\np(t)\u0003=8\n<\n:max\n1\u0014i\u0014N[\u000et(i) ]ift=T\n\u000et(s(t)\u0003) ift=T\u00001;:::; 1:(9)\nWherep(t)\u0003represents the probability of the optimal path\nat each step. The parameter Tis the maximum length of\nthe subpath, which is discussed in the following section.\nTagging: The tags of states belonging to the optimal path\nare used to tag the query song, because they are likely to\nbe acoustically similar to the query and to share some of\ntheir textual descriptors. We take the Tsongs extracted by\nEquation 8 and we calculate a vector of tag weights bq(w)\nfor all tags in j= 1;:::;M as in:\nbq(w) =TX\nt=1bt(w)!(t): (10)\nWhere!(t)is a decreasing monotonic function of the path\npositioni. That is, tags from songs acoustically similar\n– reached early in the optimal path – to the query should\nhave more importance than songs that are not that similar\n– observed at the latter steps of a random walk.\nIteration: To keep a high transition-wise similarity, the\nprocedure is split in substeps. After having computed a\npath of length T, withT= 4giving the best experimental\nresults, we restart Viterbi decoding from the initialization\nstep without resetting the modiﬁed transitions (i.e. keep-\ning the effects of Equation 7). The procedure is iterated a\nnumber of times until enough songs are visited in order to\ncorrectly infer the tags for the query song. The ﬁnal weight\nof a tag is computed as the sum of the weights computed\nat each iteration.\nIt has to be noted that there is no real control of the\npaths extracted at each iteration, in a sense that there can\nbe songs that appear multiple times on different iterations\nor maybe multiple times on the same iteration. This as a\ndesired behavior: if a song is chosen multiple times we\nassume that it is particularly relevant to the query, and so\nits tags may be better related to the it. In fact, we sum\nthe tags contribution of these songs every time they are\nchosen, regardless of the number of times and the position\nin the path where they appear.\nThe proposed approach allows us to exploit the inﬂu-\nence of two features at once. It can be noted that we could\ncalculate query’s tag by simply using all the neighbor songs\ncomputed from a forward exploration of the model starting\nfrom the query. The advantage of using Viterbi decoding\nrelies on the fact that it ﬁnds an acoustically meaningful\n“music path” from the query, which helps us avoiding non-\nrelated songs. Another important advantage is that we canalso decide which weight we want to assign to each song\nin the path, which could lead to better results.\n2.5 Weighting the Tags\nAs of the weighting function in Equation 10 we explored\ndifferent options, that have been tested experimentally. For\neach time step t= 1;:::;T the function !(t)can be com-\nputed according to:\nPath probability: As a ﬁrst option, the path probabil-\nity at steptcan be used directly a the tag weight, using\n!(t) =p(t)\u0003:\nLinear decay: The relevance of tags can decrease linearly\nwith the length of the path required to obtain them, accord-\ning to!(t) = 1\u0000m(t\u00001), with 0<m< 1.\nExponential decay: Since the probability of a path across\nHMMs decreases exponentially with the length of the path,\ntag weight can be computed also according to !(t) =a(t\u00001),\nwith0<a< 1.\nHyperbolic decay: In order to obtain, for small values of\nT, intermediate weights between linear and exponential,\ntag weight can be also computed as !(t) = 1=t.\n3. RESULTS\nAn automatic tagging system is expected to put meaning-\nful tags on novel songs in a reliable way. We have already\nseen the importance of automatic tagging in the introduc-\ntion and we wanted to test how our model performs in this\ndifﬁcult task.\n3.1 Data Source\nFor the experimental evaluation we focus on the quality of\nthe source data, as we need to rely on it to put the right\ntags to songs. For these reasons we have chosen to use\nthe CAL500 dataset [23], which consists of 502 popular\nsongs of Western music by different artists. Songs from\nthis dataset have been tagged, through a controlled survey,\nby at least three human annotators each. The semantic vo-\ncabulary consists of 149 tags spacing from genre classiﬁ-\ncation (e.g. “rock”, “pop”) to vocal and acoustic charac-\nteristics (e.g. “female lead vocals”), as well as emotions\n(e.g. “aggressive”) and song usages (e.g. “studying”). The\nsurvey results is a binary annotation of each song.\nAcoustic features (i.e. MFCC and RH) have been com-\nputed from a degraded version of the clips in the dataset,\nwith an encoding quality which was still high enough for\nour purposes. The availability of the audio motivates the\nchoice of CAL500. Moreover, we are more interested in\nthe reliability of the tagging procedure, so we prefer to\nevaluate our approach with a small yet controlled collec-\ntion. Experimental evaluation with larger collections, such\nas CAL10k or Magnatagatune, will be part of our future\nwork.\n3.2 Evaluation Measures\nWhat the users expect from an automatic tagging system\nis that proposed tags are relevant, in a sense that they truly\ndescribe the content of the song, and the tags are complete,so there is no lack of information. Any extra tag is counted\nas an error, or at least as noise. We also expect that the sys-\ntem has to be concise, that is, if the output of the system is\na ranked list of tags, where the rank is a relevance measure,\nwe may want to keep only the top most tags as the query\nresult. In other words, we need to measure whether the\nsystem is able to propose the most relevant tags at the top\nof the ranked list, while other tags (not-so-relevant ones\nand wrong ones) should have lower ranks. With this aim\nin mind we choose to use the precision metric: we mea-\nsured the precision at 10 (P@10), which reports the frac-\ntion of relevant tags of the top 10 results from the ranked\nlist, and the mean average precision (MAP), which aver-\nages the precision at each point of the ranked list of tags.\n3.3 Testing Procedure\nSince the role of this model is to tag a new song we can test\nit using the ground truth (the CAL500 dataset) in a leave-\none-out fashion on all the songs minus one. What we have\ndone is calculating the acoustic similarity for each song\nin the dataset, and, in turns, we simulated the querying of\neach song against the rest of the dataset.\nTo this end, the tagging procedure ignores the tag con-\ntribution from the query song as it assumes that it does not\nhave tags on it.\n3.4 Parameters Tuning\nThe tagging model as proposed in this thesis has some pa-\nrameters which have to be tuned.\nWe tested some combinations of the values of T(length\nof the optimal path) in relation to the number of effectively\nretrieved songs per path and the total number of iterations\n(see section 2.4). What we have seen is that short paths\ngive better results, and the number of effectively retrieved\nsongs should be as close as possible to the length of the\npath: this could mean that the conservation of acoustic sim-\nilarity is preferred over the number of retrieved results. We\nended up choosing a path length of 4 with 3 retrieved song\nper iteration. This approach can produce iterations where\nas little as 1 song are retrieved, that is, there can be a loop\nof length 3 with the query and one song. Other combi-\nnations of path length and retrieved songs led us to worse\nresults MAP-wise.\nRegarding the total number of iterations of the Viterbi\nalgorithm we have seen that, for our data, the best results\nwere obtained with 9 iteration. Of course the inﬂuence of\nthese parameters should be further discussed and optimal\nvalues may change for different datasets or as the model\ngrows integrating the tagged songs.\n3.5 Experimental Results\nThe approach was tested using MFFC for transitions and\nRH for observations and viceversa. Moreover, we wanted\nto evaluate the effect of individual features and compare\nthem with a baseline approach. To measure this we tried\nthree strategies: ﬁrst, we measured the inﬂuence of the ob-\nservation similarity by imposing uniform transitions. Thenwe measured the inﬂuence of the transition similarity by\nuniforming the observation probabilities. The last test sim-\nulated a completely random walk by uniforming both prob-\nabilities, in order to obtain a true baseline.\nFigure 2 . P@10 from MFCC on transitions with KL (sin-\ngle gaussian) similarity, RH on observations with Modiﬁed\nChord similarity, comparing the effects of different weight-\ning and the single features. Note: The span of this and the\nfollowing ﬁgures is set in the range [0; 0:7]in order to bet-\nter appreciate the differences between the values\nIn Figure 2 are shown the results of P@10 compar-\ning different combinations of uniform probabilities and tag\nweighting schemes. We can see that the exploitation of\nboth features gives consistently better results than uniform\nprobabilities. In turn, using only observations gives bet-\nter results than using only transitions while the baseline\ngives always poorer results. It can also be observed that\nlinear weighting performs slightly better than all weight-\ning strategies and that the simple use of path probability –\nalthough a natural choice – gives the lowest results.\nFigure 3 . P@10 from MFCC on transitions with KL (sin-\ngle gaussian) similarity, RH on observations with multiple\nsimilarities, with linear tag weighting\nIn Figure 3 and Figure 4 are shown the results of P@10\ncomparing different similarity measures, using linear tag\nweighting. Our best results lead to over 0.6 precision,\nwhich means that if we pick a random song and we keep\nonly the top 10 proposed tags, we can expect 6 of them\nto be correct on average, which is a good result for the\nﬁrst implementation of a novel approach. As it can beseen, not all similarities combinations have the same ra-\ntio between “normal” approach and uniform probabilities\napproach. For some combinations “normal” approach has\nworse results than “uniform transition probabilities” which\nunderlines the importance of the choice of the distance\nmeasure.\nFigure 4 . P@10 from RH on transitions with KL (single\ngaussian) similarity, MFCC on observations with multiple\nsimilarities, with linear tag weighting\nOur MAP performance is in line with the one of other\nworks at initial stage described in the literature. Table 1\nshows a MAP results summary for the same conﬁguration\nas Figure 3 and Figure 4 respectively.\nMFCC on transitions (KL - single gaussian),\nRH on observations (Modiﬁed Chord)\nNormal Un. Tr. Un. Obs. All Un.\nPath P. 0.481 0.474 0.442 0.442\nLinear 0.536 0.507 0.450 0.442\nExp. 0.533 0.504 0.451 0.453\nHyperb. 0.533 0.504 0.452 0.450\nRH on transitions (KL - single gaussian),\nMFCC on observations (Euclidean)\nNormal Un. Tr. Un. Obs. All Un.\nPath P. 0.478 0.489 0.447 0.446\nLinear 0.528 0.523 0.465 0.442\nExp. 0.526 0.520 0.468 0.453\nHyperb. 0.526 0.520 0.467 0.450\nTable 1 . Mean average precision summary table\n4. CONCLUSIONS\nWe describe a novel approach for semantic music tagging\nbased on a statistical framework that combines two mu-\nsic features: timbre and rhythm. Tagging is based on a\nmodiﬁed Viterbi algorithm to carry out iterated random\nwalks in the graph that represents a collection of tagged\nsongs. The approach was evaluated using the CAL500\ndataset. Experiments have shown encouraging results in\nterms of precision at 10 and Mean Average Precision of\nthe ranked lists of tags. Performance contribution of each\nfeature has been also measured separately and compared toa random baseline. The effects of different similarity mea-\nsures for observations have been tested as well, together\nwith four approaches to weight tags according to the num-\nber of steps required to obtain them. To the best of our\nknowledge, we think that our performances are in line with\nother early stage approaches. As pointed out in [1], purely\naudio-based approaches are instrinsically limited because\nthey cannot capture all the music dimensions perceived by\nlisteners. We think that additional parameter tuning, possi-\nbly using other collections, will give further improvements\nbefore the “glass ceiling” is reached.\nOne issue that will be addressed in future work is the\neffect of loops in the optimal path, which at the moment\nis minimized with the modiﬁed Viterbi algorithm (Equa-\ntion 7) but can be improved by alternative strategies to\nmodify the transition probabilities. The current tagging\nprocedure suggests a way to grow the collection by adding\nthe newly tagged song to the graph, thus we aim also at\nmeasuring how performances degrade with the increase of\nthe collection size.\n5. REFERENCES\n[1] J.-J. Aucouturier and Pachet F. Improving timbre sim-\nilarity: How high is the sky? Journal of Negative Re-\nsults in Speech and Audio Sciences , 1(1), 2004.\n[2] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere.\nAutotagger: A model for predicting social tags from\nacoustic features on large music databases. Journal of\nNew Music Research , 37(2):115–135, 2008.\n[3] S. Bourguigne and P.D. Ag ¨uero. Audio tag classiﬁca-\ntion using feature trimming and grid search for SVM.\nInProc. of MIREX , 2011.\n[4] S.-H. Cha. Comprehensive survey on dis-\ntance/similarity measures between probability density\nfunctions. City, 1(2):1, 2007.\n[5] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green.\nAutomatic generation of social tags for music recom-\nmendation. Advances in neural information processing\nsystems , 20(20):1–8, 2007.\n[6] A. Elberse. Should you invest in the long tail? Harvard\nbusiness review , 86(7/8):88, 2008.\n[7] H. Fastl. Fluctuation strength and temporal mask-\ning patterns of amplitude-modulated broadband noise.\nHearing Research , 8(1):59–69, 1982.\n[8] D. Goldberg, D. Nichols, B.M. Oki, and D. Terry.\nUsing collaborative ﬁltering to weave an information\ntapestry. Communications of the ACM , 35(12):61–70,\n1992.\n[9] P. Hamel. Multi-timescale pmscs for music audio clas-\nsiﬁcation. In Proc. of MIREX , 2012.\n[10] M. Hoffman, D. Blei, and P. Cook. Easy as cba: A\nsimple probabilistic model for tagging music. In Proc.\nof ISMIR , pages 369–374, 2009.[11] T. Li, M. Ogihara, and Q. Li. A comparative study on\ncontent-based music genre classiﬁcation. In Proc. 26th\nACM SIGIR conference , pages 282–289. ACM, 2003.\n[12] T. Lidy and A. Rauber. Evaluation of feature extractors\nand psycho-acoustic transformations for music genre\nclassiﬁcation. In Proc. of ISMIR , pages 34–41, 2005.\n[13] S.-C. Lim, K. Byun, J.-S. Lee, S.-J. Jang, and\nMoo Young Kim. Music genre/mood classiﬁcation. In\nProc. of MIREX , 2012.\n[14] M.I. Mandel and D.P.W. Ellis. Song-level features and\nsupport vector machines for music classiﬁcation. In\nProc. of ISMIR , pages 594–599, 2005.\n[15] M.I. Mandel and D.P.W. Ellis. Multiple-instance learn-\ning for music information retrieval. In Proc. of ISMIR ,\npages 577–582, 2008.\n[16] R. Miotto and G. Lanckriet. A generative context\nmodel for semantic music annotation and retrieval. Au-\ndio, Speech, and Language Processing, IEEE Transac-\ntions on , 20(4):1096–1108, 2012.\n[17] R. Miotto and N. Orio. A probabilistic model to com-\nbine tags and acoustic similarity for music retrieval.\nACM Transactions on Information Systems (TOIS) ,\n30(2):8, 2012.\n[18] F. Pachet, D. Cazaly, et al. A taxonomy of musical gen-\nres. In Proc. Content-Based Multimedia Information\nAccess (RIAO) , pages 1238–1245, 2000.\n[19] E. Pampalk. Islands of music: Analysis, organization,\nand visualization of music archives. Master’s thesis,\nVienna University of Technology , 2001.\n[20] R. Piva. Combining timbric and rhythmic features for\nsemantic music tagging. Master’s thesis, University of\nPadua, Padova, Italy , 2013.\n[21] L.R. Rabiner. A tutorial on hidden markov models and\nselected applications in speech recognition. Proceed-\nings of the IEEE , 77(2):257–286, 1989.\n[22] A. Rauber, E. Pampalk, and D. Merkl. The som-\nenhanced jukebox: Organization and visualization of\nmusic collections based on perceptual models. Journal\nof New Music Research , 32(2):193–210, 2003.\n[23] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Towards musical query-by-semantic-description\nusing the CAL500 data set. In Proc. of ACM-SIGIR ,\npages 439–446. ACM, 2007.\n[24] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Semantic annotation and retrieval of music and\nsound effects. Audio, Speech, and Language Process-\ning, IEEE Transactions on , 16(2):467–476, 2008.\n[25] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. Speech and Audio Processing,\nIEEE transactions on , 10(5):293–302, 2002."
    },
    {
        "title": "A Comprehensive Online Database of Machine-Readable Lead-Sheets for Jazz Standards.",
        "author": [
            "François Pachet",
            "Jeff Suzda",
            "Dani Martínez"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417473",
        "url": "https://doi.org/10.5281/zenodo.1417473",
        "ee": "https://zenodo.org/records/1417473/files/PachetSM13.pdf",
        "abstract": "Jazz standards are songs representative of a body of musical knowledge shared by most professional jazz musicians. As such, the corpus of jazz standards constitutes a unique opportunity to study a musical genre with a “closed-world” approach, since most jazz composers are no longer in activity today. Although many scores for jazz standards can be found on the Internet, no effort, to our knowledge, has been dedicated so far to building a comprehensive database of machine-readable scores for jazz standards. This paper reports on the rationale, design and population of such a database, containing harmonic (chord progressions) as well as melodic and structural information. The database can be used to feed both analysis and generation systems. We report on preliminary results in this vein. We get around the tricky and often unclear copyright issues imposed by the publishing industry, by providing only statistical information about songs. The completeness of such a database should benefit many research experiments in MIR and opens up novel and exciting applications in music generation exploiting symbolic information, notably in style modeling.",
        "zenodo_id": 1417473,
        "dblp_key": "conf/ismir/PachetSM13",
        "keywords": [
            "Jazz standards",
            "musical knowledge",
            "closed-world approach",
            "Internet scores",
            "comprehensive database",
            "harmonic progressions",
            "melodic and structural information",
            "analysis and generation systems",
            "copyright issues",
            "MIR experiments"
        ],
        "content": "A COMPREHENSIVE ONLINE  DATABASE OF MACHINE -\nREADABLE LEAD SHEETS FOR JAZZ STANDARDS  \nFrançois Pachet  Jeff Suzda  Daniel Martín    \nSony CSL  \npachetcsl @gmail.com  Sony CSL  \njeff@jeffsuzda.com  Sony CSL  \ndaniel.martin @csl.sony.fr    \nABSTRACT  \nJazz standards are songs representative of a body of \nmusical knowledge shared by most professional jazz m u-\nsicians. As such , the corpus of jazz standards  constitutes a \nunique opportunity to study  a musical genre with a \n“closed -world” approach , since most jazz composers are \nno longer in activity today. Although many scores for \njazz stand ards can be found on the I nternet, no effort, to \nour knowledge, has been dedicated so far to building a \ncomprehensive database of machine -readable scores for \njazz standards . This paper reports on the rationale, design \nand population of such a  database, containing  harmonic \n(chord progressions ) as well as  melodic and structural \ninformation. The database can be used to feed  both analy-\nsis and generation systems. We report on preliminary r e-\nsults in this vein. We get around the tricky and ofte n un-\nclear copyright issues imposed by the publishing indu s-\ntry, by providing only statistical information about songs.  \nThe completeness of such a database should  benefit many \nresearch experiments in MIR and opens up novel and ex-\nciting applications in music generation  exploiting sy m-\nbolic information , notably in style modeling.  \n1. MOTIVATION  \nBuilding  a reference  database for music information \nretrieval is a complex  issue . Many database s of audio \ncontent have been made available with some success to \nthe research c ommunity , raising essential  annotation i s-\nsues [2 5]. For scores  and symbolic information in ge n-\neral, the situation is more problematic. There is a large \namount of this information on the net, and many illegal \nscans of scores ( e.g. in pdf format) but, to our  knowledge, \nthere is no machine -readable  online reference  database  \nfor well-defined cor pora, such as jazz standards.  \nA difficulty when defining a reference database is to \ndefine its boundary. In the case of jazz, most composers \nare no longer active,  so it is relatively easy  to define such \na boundary. For instance, Pepper Adams composed exac t-\nly 43 songs; most of Charlie Parker’s compositions are \nknown and available in various formats, and the same \nholds for almost all compos ers of jazz standards . Such a  closed -world approach to jazz standards is key to schola r-\nly and academic work , in particular for evaluating  opera-\ntional music systems.  Ideally, research experiments i n-\nvolving  analyzing and generating  jazz compositions \nshould exploit, or apply to, all jazz tunes ever composed, \nbut the absence of such information makes it impossible \nin practice.   As a consequence, many research papers  \ndealing with jazz  compositions are based on ad hoc  data-\nbases which are not publicly available  ([2], [11-12], [20-\n21], [23]).  \nAn obvious option to build such a reference database \nwould be to use automatic chord recognition and melodic \nextraction software on existing audio repositories. There \nare two problems with this approach. Most importantly, \nunlike many other musical genres, scores in j azz, called \nleadsheets , play a central role as they represent the “e s-\nsence” of a tune, harmony - and melody -wise. As a cons e-\nquence, jazz musicians rarely play the chords as they are \nwritten, and part of the game of jazz is precisely to take \nliberty and inte rpret the score: unlike classical music, the \nleadsheet, in general, cannot be deduced from actual pe r-\nformances. Second, the accuracy of chord recognition \nsoftware is not sufficient to enable fully automatic pr o-\ncesses. State of the art methods such as [3], [7] report a c-\ncuracies in the order of 70%, which is insufficient for our \ntask. \nThere are numerous attempts at building database s of \nscores in various genres . For instance, the International \nMusic Score Library Project (IMSLP ) assembles scores \nfor classical  music composers , but only those in public \ndomain . UCLA ’s score library propose s many popular \nmusic  scores , including jazz but it is by no means co m-\nplete.  \n2. A REFERENCE  CORPUS OF STANDARDS  \nThe notion of jazz standards is ubiquitous in jazz, al t-\nhough not com pletely well -defined:  Jazz standards and \npieces that are routinely performed by jazz musicians and \nwidely known to listeners. Most of these songs were \ncomposed from the 20s up to the 80s. In practice, jazz \nstandards are often thought of as the songs which appear \nin the so -called “Fake Books ”. The most well -known  of \nthese is probably the “ Real Book”, published by Berklee \nstudents in the 70s  as a reaction to previous  Fake Books, \nwhich were considered as over simplified to be used by \njazz musicians  [13]. This book, still widely used today, \ncontains 460 hand -written songs with the melody, the \nchord sequence, and basic editorial information (compo s-\ner, style, tempo,  and a reference recording of the song).  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or di stributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2013  International Society for Music Information Retrieval    \n \nSince the 70s however, Real Books  have evolved signi fi-\ncantly. The original Real Book being illegal , several pu b-\nlishers subsequently released other song books containing \nsets of songs  for which they obtained or cleared cop y-\nrights. The most important publishers are Sher  (New Real \nBook s, Volume I  to III  [26] and Hal Leonard (the Real \nBook Sixth edition, and the Real Book Volume II, III, IV \nand V  [15]). However, other sources of jazz standards are \ncommonly available through va rious channels (printed, \nonline as well as  illegal).  Other notable sources are co m-\nposer -specific song books, which often contain yet diffe r-\nent versions of songs , such  as the Charlie Parker Omn i-\nbook  [24] or Michel Legrand song book [14] . As a result, \nsongs appear usually in several song books, with som e-\ntimes significant differences. For instanc e, Figure 2 and \nFigure 3 show several versions of the song Solar by Miles \nDavis ( or rather, Chuck Wayne , see  [1]). Subtle diffe r-\nences are visible concerning chord s. In some cases more \nsignificant differences appear, including mistakes or di f-\nferent harmonizations.  \nFinally it is important to observe that , in our exper i-\nence at least,  some songs (such as Body and Soul) are \nplayed in almost every jam session , but many others are \nhardly  played at all:  all songs are not equa lly “standard”.  \n \nFigure 1. The original Real Book version of Solar.  \nTo summarize, we can point out two important facts \nabout jazz standards that provide us with guidelines:  \n1) There is no official  version  of any given score  unless \ndirectly from the author's personal collection, and \neven then, composers often \"update\" their compos i-\ntions afterw ards. There are indeed significant diffe r-\nences between scores, depending on the publisher. \nDifferences affect the chord notation used as well as \nthe chords themselves (e.g. their various enric h-\nments) as well as the song structure.  \n2) The very notion of a standard relies on the existence \nof songbooks. These books are the medium by which \nmusicians learn and play songs, maintain and evolve \nthe repertoire . The publication of new volumes or \nnew editions of existing volumes impacts the evol u-\ntion of standards, though on a slow time pace.  \n  \nFigure 2. The New Real Book version (Sher) of S o-\nlar. Note the different chords (e.g. first chord is C min \nmaj7 instead of C minor), the different chord , and the \ndifferent structure (ending).  \n \n \nFigure 3. Two other versions of Solar found in po p-\nular fake books. Note that none of the m can be consi d-\nered as the of ficial version.  \n3. AN ONLINE DATABASE  \nThere is a wealth of information about jazz standards \non the Internet, but no online database of machine -\nreadable jazz standards exists, to our knowledge. Ha r-\n  \n \nmonic information (chord progressi ons) is known to be \ncopyright -free so several collections can be found on the \nweb, notably the smartphone application iRealB [10]. But \nthis database does not contain melodies, because of cop y-\nright issues, and their content is determined in part by u s-\ners th rough a social, collaborative process, with no gua r-\nantee on coverage and quality.  \n3.1 Design: Sources and Songsets  \nOur database is a web service based  on two concepts: \nsources  and songsets . We define the scope of jazz stan d-\nards by referring to the already subs tantial body of work \none by reference publishers (such as Sher or Hal Leo n-\nard). The primary concept in the database is therefore the \n“source”, which contains the list of song s of a given, pu b-\nlished corpus. Figure 4 shows a list of currently entered \nsources.  Sources already contain implicit editorial info r-\nmation concerning the choice of songs  (publishers want \nto publish songs that people will actually pl ay), as well as \ntheir notation (they try to propose an accurate and co n-\nsistent notation for musicians). Of course, there are many \nredundancies in sources, as a popular song will typically \nappear in various published collections. This redundancy \nin itself i s informative, and can be used, to some extent, \nto derive automatically information about the popularity \nof a title , from the viewpoint of publishers . A preliminary \nanalysis of occurrence of songs within 10 sou rces shows \nthat only one song, Body and Soul  appears in 8 sources \n(out of 10), a fact that is confirmed, e.g. by the site jaz z-\nstandards.com in which Body and Soul appears as the \nmost popular song to record among jazz musicians . Only \n3 compositions  occur 7 times ( Here’s that rainy day , In a \nSentimental  Mood, Bye Bye Blackbird ), and , like Body \nand Soul , they are all famous and routinely performed . \nMore precise information will be enabled as the reposit o-\nry grows, and many analysis can be performed, e.g. on \nthe distribution of popularity in relation with c omposers, \neras, styles, etc.  \n \nFigure 4. A snapshot of the interface showing t he list of \ncurrently entered sources (number of completed  songs \nbetween parenthesis) . \nSongsets  are defined by users, and contain meaningful \ncollections of songs, taken from various sources.  Typical  \nsongsets are: all (the list of all songs in all versions), be-\nbop (the complete collection of all compositions by b e-\nbop composers such as Charlie Parker or Dizzy Gille s-\npie), Charlie Parker blues , the li st of all Charlie Parker \ncomposi tions which are 12 -bar blues (see Section 4), ter-\nnary, the list of all standar ds in  3/4, etc.  Users define songsets by selecting sources, authors or \nindividual songs, and by filtering them using the info r-\nmation in the database. Information about the redundancy \ncan also be used for specifying songsets (e.g. all songs \nthat appear only once  in a given source, or at least 3 \ntimes, etc.). Songsets are stored in the database cloud, and \ncan be shared and reused by other users.  \n \nFigure 5. A search tool, here all songs with the word \n“blues” in the title.  \n3.2 Song entering  \nSongs are entered by professional musicians (inclu d-\ning the second author), source by source. For each song, a \nspecific online song editor is used, that enables the mus i-\ncian to enter the structure, chords and then melody, as \nwell as basic editorial informat ion (composer, tempo, \nstyle, metrics).  Avera ge time to enter a song is 3 minutes, \nbut this varies greatly from about 2 to 15  minutes , for \ncomplex songs.  Note that only basic information about \nthe melody is entered (pitch, quantized position and dur a-\ntion). For instance, the melody of the song Solar , from \nthe Real Book (original) source  is illustrated in Figure 6. \nIt can be noted that no typographic information is saved, \nonly the basic MIDI data.  This melody is then synchr o-\nnized to the structure (organization in sections) and chord \nsequence s of the song.  \nSong enterers do not “copy” the source, but reinter-\npret it to be stored in the database. Interpretation concerns \nchord n otation  (see next section) and structure . Indeed, \none of the problems with extrapolating musical info r-\nmation from a leadsheet  is the  “folding”  problem : Many \nleadsheets are pub lished in a condensed, f olded format - \nusually a one page lea dsheet - of musical information, \nwhich is very practical for use in performance situations. \nHowever, this is not always the best solution for a m a-\nchine -readable format.  For this reason, some of the co m-\npositions are \"unfolded\" in terms of their form so that \nthere is no ambigu ity with regards to repeats, codas, or \nmelodic variations.  Of course, s uch transform ation s pre-\nserve  the semantics as both version s describe the same \nsequence of events (chords and notes).  \n  \n \n \nFigure 6. The melody  and chord sequence  of Solar  \n[Real Book, 5th edition]  entered with our online editor . \n \nFigure 7. The song entering process: interpreting a \npublished leadsheet  to enter  it in a machine -readable \nformat.  \nFinally a few  songs are ignored, either because they \ncontain no melody ( Domino Biscuit  by Steve Swallow) \nhave no time signature ( And now, the Queen  or Batterie  \nby Carla Bley), or because the melody is too polyphonic \n(Ay Arriba  by Stu Balcomb ), and therefore outside the \nscope of our target  (all examples from the original Real \nBook) . \nError checking is performed using two means. First, \nautomatic checks are performed to ensure that the dur a-\ntions of melodies in each bars and section are the same as \nthe corresponding durations of chord sequences. Second, \nsong enterers periodically manually check about 5% ra n-\ndom songs entirely (melodies and chords) entered by ot h-\ner song enterers.  Manual checking has revealed so far that \nvery little errors are encountered (less than 1% of songs \ncontain  errors).  \n3.3 API and Implementation  \nThe API is a delicate matter . Because we do not own \ncopyrights to the compositions , melodies in particular , we \nprovide an API that only delivers statistical information. \nThe API provides, for a given songset , the following i n-\nformation:  \n- The chords  prior probabilities for songset  with id s: \nhttp://.../api/getChords.php &songset_id= s returns the \nlist of chords in s with their probability:  \n{{\"prob\": 0.217634 , \"chord\": \"Am7\"},  \n {\"prob\": 0.119352 , \"chord\": \"CM7\"},  \n {\"prob\": 0.112842 , \"chord\": \"G7\" }... \n-          The prior probabilities for pitches occurring in \na songset. For instance,  query  \nhttp://../api/getPitches.php &songset_id=s   would return:  \n{{\"prob\": 0.251634 , \"pitch\": \"G\"},  {\"prob\": 0.250932 , \"pitch\": \"C\"},  \n{\"prob\": 0.247842, \"pitch\": \"D\" }... \n- For any prefix of chords, the probabilities of all po s-\nsible continuation chords , at the order equal  to the prefix  \nlength . For instance, to get the continuations of Gm7, the \nquery \nhttp://.../api/chords.php?method=getTransitions&chord=\nGm7 &songset_id=s  would return:  \n{\"+5/7\": { \"prob\": 0.537634 , \"chord\": \"C7\"}, \n\"+5/m7\": { \"prob\": 0.071774 ,\"chord\": \"Cm7\"},  \n\"+5/7b9\": { \"prob\": 0.028494 ... \nwhere for each continuation, we have the distance in \nsemitones between G and the continuation's root (+5 b e-\ntween G and C), type (7, minor7 and 7b9), probability \nand actual chord name.  \n- For any prefix of pitches, the list of probabilities of \nall possible continuations, at the order corresponding to \nthe length of the prefix. For instance, \nhttp://.../api/chords.php?method=getTra nsitions&pitch=\nA&songset_id=s   would return:  \n{\"-2\": {\"prob\": 0.064516 , \"pitch\": \"G\"},  \n \"+5\": {\"prob\": 0.043709 , \"pitch\": \"D\" }... \nAdditionally, the API provides, f or each song in a \nsongset, the histogram of chords and pitches , as well as \nthe joint probabili ties of chord and pitches.  \nTo our knowledge, such an API does not violate co p-\nyright, as it is, in general , impossible to completely re-\nconstruct a melody or even a chord sequence from this \nstatistical information. This API will, however, evolve, to \nadapt to  the needs of applications and the evolution of \ncopyright policies of the music publishing industry.  \nSongs for which copyright has ceased will be made pr o-\ngressively available to users in their entirety. Chord s e-\nquences, in principle not copyrighted, are provided entir e-\nly in text format . \nCurrent implementation uses standard web technology \nHTML/CSS and Javascript in the client side, PHP in the \nserver side with a noSQL database in JSON format. Me l-\nodies are stored in musicXML format [ 19].  \n3.4 Chord notation and substitution rules  \nAs can be seen by the example, there is no common, \nreference notation for jazz chords, and sources use diffe r-\nent notations  [6]. Some works in MIR have addressed the \nproblems of chord  notation  ([8], [16-17], [28]) but these \nnotations are mostly used for automatic audio chord e x-\ntraction tasks.   \nAdditionally, within a given notation, there are diffe r-\nences in precision. For instance, a dominant seventh \nchord can be written simply as “7”, or, in other sources , \nwith additional notes (e.g. “9” , or “dim9” ). In order t o \npreserve as much as possible the data accuracy we have \nchosen to enter sources with chord names that are as \nclose as possible to the chord written in the source, and \nadding them when the score enterer considered it is not in \nthe cur rent list (we have reached currently a total of 86 \nchord names, see Figure 8): no effort at consistency or \nuniformity has been conducted at this step.   \nSuch an approa ch is obviously not sufficient when \nseveral sources are mixed together to form a coherent \nsonglist. In order to cope with this problem (seen here as \na sparsity  problem), we use sets of substitution rules, that \n  \n \ntransform chords from their original formulati on (e.g. C \n7#4#5 ) into a sparser formulation  that is significant for \nthe task at hand. For instance, some applications may \nneed to distinguish only between, say, 4 chord types (m a-\njor, minor, dominant 7th, diminished), while other may \nneed more.  \nTo address this issue, we introduce transformers : sets \nof substitution rules that transform a chord in a source \ninto the most relevant chord name in a given vocabulary. \nFor instance, C 7#4#5 => C7 , or DM7#11 => D M . \nSuch a use of chord substitution rules can be extended \nto cope not only with lexical redundancy, but also with \nsome form of semantic equivalence. This problem has \nbeen well studied in computer music ([20], [27]) and a c-\ncepted sets of rules can be easily identified. For instance, \nmany forms of “ii -V7-I” can be considered as more or \nless equivalent: a dominant chord such as C 7 can be r e-\nwritten as G min7 / C7 , or even as G min7 / F# 7 , depen d-\ning on the degree of precision requested and the task. \nSuch application -dependent considerations can all be \nhandled  through sets of substitution rules, defined once \nand for all by users and shared, like songsets.  \n \n(empty)  2 5 6 m \n+ 7 9 11 13 \n+7 m6 69 M7 m9 \nm7 M9 7b9 7#11  aug \nAlt m13 m#5 m69 m11 \nDim 7#5 7#9 9b5 7b5 \nmb6 9#5 7#4 M13  7b6 \n#11 Sus 7b13  add9  11b9  \n7alt 6#11  m7#5  M7b9  +7#9  \n+7b9  m9M7  (b5) 7sus 13b9  \n9#11  mM7  dim7  9sus 4sus \nM7#5  M7#4  m9b5  M9#5  13b5  \nsus2 sus4 M9b5  M7#9  7#9b5  \n7#5b5  7#4#5  13#11  M9#11  13sus  \n7b9#9  7#5#9  pedal  +add9  7b5#9  \n(#11)  m(M9)  dimM7  7#9#5  7b9b5  \nM7#11  7b9#5  aug#4  +(b9)  6sus4  \nm11b5  madd9  5add9  7#5#11  7b9#11  \nLydian  7#9#11  7b9b13  Dorian  M7#9b9  \nm7add4  m7b5#5  (add9)  m7sus4  7b9sus  \ndim7M7  add9b5  mM7#11  mM7b13  13b9b5  \nadd#11  M13#11  7omit5  Aeolian  m(add9)  \n13b9sus  +(add9)  m7b5b13  (no3rd)  m(m7M7)  \n(b9b13)  7b13#11  7b13sus  13b9#11  M7add13  \nm9add13  m7addM7  Phrygian  M7(?4)  m7(b5b2)  \n(9, #11)  halfdim7  7susadd3  13(b9b5)  m(omit5)  \nsus4add9  7b9b13sus  13(b9#11)  m7(omit5)  7susomit5  \n13(add11)  6#9 M7b5  13#9  m9#11  \nm7#11  7#5b9  69#11  mb5b13  m13#11  \nM7#5#11  M7#9#11  add9addb13  madd9add11  halfdim7b9  \nm7add11add13  halfdim7add11     \nFigure 8. The current chord names used in about 1 2 \nreference sources . \n4. APPLICATIONS  \nOur database is developed in the context of a large -\nscale project about the representation of musical style, in \nparticular for popular music. In this context, son gsets are \nconsidered as concrete representations of a user -defined \nstyle. Various style analysis and generation mechanisms , \ne.g. using the technology of Markov constraints  [22] can \nbe implemented  to generate seq uences “in the style of”, \nthat also satisfy arbitrary user constraints. A n example \nwas exhibited in  [22] with the so-called Boulez Blues : a 12-bar Blues chord sequence in the style of Charlie Pa r-\nker blues (the Parker Blues  songset) that satisfies an “ All \ndifferent” constraint  (hence the Boulez label) , and is o p-\ntimally Parkerian , i.e. maximizes its probability w/r the \nParker Blues corpus .  \nOther applications can be developed to exploit this \ndatabase. Generation algorithms based on statistical i n-\nformation , in particular using random walk  algorithms \ncan be trivially implemented with our API. Indeed, ra n-\ndom walk consists in selecting at random the “next” event \n(chord or note) using the transition probabilities, given a \nprefix (the sequence already created), whi ch is exactly \nwhat our API provides.  \nThe database is also used for analysis studies. To our \nknowledge, few studies attempt  to assess to what  extent \ncomposers are recognizable through their chord sequen c-\nes only , or through their melodies, or both. Attempts to \naddress these issues (e.g. [ 18-19]) are not comprehensive, \nnor easily reproducible. Such studies are under way [9], \nand its results will be made credible only the comprehe n-\nsive nature of this database . \n5. CONCLUSION  \nWe described the motivation and rationale for a co m-\nprehensive online database of machine -readable lead-\nsheets  of jazz standards1. The specification of the dat a-\nbase is simple because its goals are very clear: provide a \nmachine -readable representation of melodies and chord \nprogressions as found in reference, published fake books,  \nand following  a “closed -world” approach. The database is \nalready being used by several projects dealing with ana l-\nysis and genera tion of jazz compositions.  \nThe closed -world approach does not mean that this \ndatabase e ffort is to be stopped soon. First, new compos i-\ntions are regularly been published, such as the European \nReal Book [5], though not at a pace  comparable to that of \nthe Fake Books of the 1970s and after . The contents of \nsuch books will be added progressively to the database, \nwhich will enable interesting experiments, for instance, \nregarding the evolution of composi tional styles.  \nWe do not infringe on copyright s, because 1° our d a-\ntabase does not contain typographical information  speci f-\nic to publishers  and 2° we  provide an API that prevents \nreverse  engineering to the original sources.  \nOther sources of editorial information will be progre s-\nsively added, such as the list of official recordings for \neach standard, with the audio content when possible , or \nthe exact dat e of composition, when available . \nOur effort can be generalized to other music genres, \nnotably for which leadsheets  play such a central role. \nThis concerns for instance large chunks  of the Brazilian \npopular music repertoire  such as Bossa Nova or Choros:  \nlike jazz, th ese repertoire are somewhat closed but rich \nenough musically to deserve such a treatment. Several \nworks have already addressed analysis tasks on partial \ndatabases [4]. Most importantly our approach applies to \nsongs  that can be reduced to their leadsheet  represent a-\ntion without losing their essence .  \n                                                           \n1 www.flow -machines.com/lsdb    \n \nOur jazz database target s a total of  15 sources (see \nFigure 4) and 8000 songs  (4000 of them unique)  by the \ndate of presentation of this paper , obtained through a \nsteady song entering process . With such a consistent \nmass of information, the first compreh ensive style -based \njazz composition and analysis systems will , at last , see \nthe li ght of day . The corresponding research will be eas i-\nly reproducible. Hopefully, more genres will follow.  \n6. ACKNOWLEDGEMENTS  \nThis research is conducted within the Flow Machines \nproject which received funding from the European R e-\nsearch Council under the European Union’s Seventh \nFramework Programme (FP/2007 -2013) / ERC Grant \nAgreement n. 291156.  \n7. REFERENCES  \n[1] L. Appelbaum : “Performing Art Blog ”, \nhttp://blogs.loc.gov/music/2012/07/chuck -wayne -\nsonny -solar , 2012.  \n[2] J. Biles: “GenJam: A Genetic Algorithm for \nGenerating Jazz Solos”,  International Computer \nMusic Conference , pp. 1 31-137, 1994.  \n[3] J. A. Burgoyne, J. Wild, and I. Fujinaga: An Expert \nGround Truth Set for Audio Chord Recognition and \nMusic Analysis , ISMIR , pp. 633-638, 2011.  \n[4] G. Cabral  and R. Willey : \"Analyzing Harmonic \nProgressions with HarmIn : the Music of Antonio \nCarlos Jobim \", 11th Brazilian Symposium on \nComputer Music , São Paulo , 2007.  \n[5] Europe: European Real Book, Sher music, 2012.  \n[6] M. Granroth -Wilding and M . Steedman : “Statistical \nParsing for Harmonic Analysis of Jazz Chord \nSequences ”, International Computer Music \nConference , pp. 478–485, 2012.  \n[7] B. de Haas, J. P. Magalhães, F. Wiering : Improving \nAudio Chord Transcription by Exploiting Harmonic \nand Metric Knowledge, ISMIR , pp.  295-300, 2012.  \n [8] C. Harte  et al: “Symbolic Representation of Musical \nChords: A Propo sed Syntax for Text Annotations”,  \nISMIR , pp. 66-71, 2005 . \n[9] T. Hedges, P.  Roy and F. Pachet : Predicting the \nComposer and Style of Jazz Chord Progressions , \nsubmitted , 2013 . \n[10] iRealB, smartphone application, \nhttp://www.irealb.com , 2013 . \n[11] R. Keller and D. Morrison : “A Grammatical \nApproach to Automatic Improvisation ”, Fourth \nSound and Music Com puting Conference , Greece, \n2007 . [12] R. Keller  et al. : “Jazz Improvisation Advi sor”, \nhttp://www.improvisor.com , 2009 . \n[13] B. Kernfeld : The Story of Fake Books: Bootlegging \nSongs to Musicians , Scarecrow Press , 2006 . \n[14] M. Legrand, The Michel Legrand Songbook,  Warner \nBros. Publications, 1997.  \n[15] H. Leonard: The Real Book, Volume I, II, III, IV and \nV, Hal Leonard, 2012.  \n[16] M. Mauch, S. Dixon, C. Harte, M. Casey, and B. \nFields : “Discovering Chord Idioms through Beatles \nand Real Book Songs ”, International Symp osium on \nMusic Information Retrieval , 2007 . \n[17] M. Mauch  et al. : “Can Statistical Language Models \nbe used for the Analysis of Harmonic Progressions? ” \nInternational Computer Music Conference , Japan , \n2008 . \n[18] L. Mearns, D. Tidhar, and S. Dixon:  \nCharacterisation of composer style using high -level \nmusical features , In 3rd ACM Workshop on Machine \nLearning and Music , 2010.  \n[19] MusicXML 3.0 Specification, MusicXML.com. \nMakeMusic, Inc. Retrieved 26 February 2013.  \n[20] M. Ogihara  and T. Li: N-Gram Chord Profiles for \nComposer Style Representation , ISMIR , pp. 671 -\n676, 2008.  \n[21] F. Pachet : “Surprising Harmonies ”, International \nJournal of Computing Anticipatory Systems , Vol. 4, \n1999 . \n[22] F. Pachet  and P. Roy: “Markov constraints: steerable \ngeneration of Markov sequences ”, Constraints , \n16(2):148 -172, 2011 . \n[23] G. Papadopoulos, G. Wiggins: “A genetic algorithm \nfor the generation of jazz melodies”, STeP, 8th \nFinish Conference on Artificial Intelligence , \nJyväskylä , 1998.  \n[24] C. Parker: Charlie Parker Omn ibook , Atlantic \nMusic Corp, 1978.  \n[25] G. Peeters, K. Fort : “Towards A (Better) Definition \nOf The Description Of Annotated M.I.R. Corpora ”, \nISMIR , pp. 25-30, Porto, 2012 . \n[26] Sher Music , The New Real Book, Volume I, II and \nIII. Sher Music Co, Petaluma, USA , 2012 . \n[27] M. J. Steedman : “A Generative Grammar for Jazz \nChord Sequences ”, Music Perception  2(1):52 –77, \n1984 . \n[28] C. Sutton, Y. Raimond, M. Mauch, and C. Harte : \n“The Chord Ontology ”, \nhttp://purl.org/ontology/chord , 2007 ."
    },
    {
        "title": "A Computational Comparison of Theory And Practice of Scale Intonation in Byzantine Chant.",
        "author": [
            "Maria Panteli",
            "Hendrik Purwins"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417040",
        "url": "https://doi.org/10.5281/zenodo.1417040",
        "ee": "https://zenodo.org/records/1417040/files/PanteliP13.pdf",
        "abstract": "Byzantine Chant performance practice is quantitatively compared to the Chrysanthine theory. The intonation of scale degrees is quantified, based on pitch class profiles. An analysis procedure is introduced that consists of the following steps: 1) Pitch class histograms are calculated via non-parametric kernel smoothing. 2) Histogram peaks are detected. 3) Phrase ending analysis aids the finding of the tonic to align histogram peaks. 4) The theoretical scale degrees are mapped to the practical ones. 5) A schema of statistical tests detects significant deviations of theoretical scale tuning from the estimated ones in performance practice. The analysis of 94 echoi shows a tendency of the singer to level theoretic particularities of the echos that stand out of the general norm in the octoechos: theoretically extremely large scale steps are diminished in performance.",
        "zenodo_id": 1417040,
        "dblp_key": "conf/ismir/PanteliP13",
        "keywords": [
            "Byzantine Chant",
            "quantitative comparison",
            "Chrysanthine theory",
            "intonation",
            "scale degrees",
            "pitch class profiles",
            "analysis procedure",
            "pitch class histograms",
            "histogram peaks",
            "phrase ending analysis"
        ],
        "content": "A COMPUTATIONAL COMPARISON OF THEORY AND PRACTICE OF\nSCALE INTONATION IN BYZANTINE CHANT\nMaria Panteli1& Hendrik Purwins2,3\n1Department of Computer Science, University of Cyprus, Cyprus\n2Neurotechnology Group, EE & CS, Berlin Institute of Technology, Germany\n3Sound and Music Computing Group, Aalborg University Copenhagen, Denmark\n{m.x.panteli, hpurwins }@gmail.com\nABSTRACT\nByzantine Chant performance practice is quantitatively\ncompared to the Chrysanthine theory. The intonation of\nscale degrees is quantiﬁed, based on pitch class proﬁles.\nAn analysis procedure is introduced that consists of the\nfollowing steps: 1) Pitch class histograms are calculated\nvia non-parametric kernel smoothing. 2) Histogram peaks\nare detected. 3) Phrase ending analysis aids the ﬁnding of\nthe tonic to align histogram peaks. 4) The theoretical scale\ndegrees are mapped to the practical ones. 5) A schema\nof statistical tests detects signiﬁcant deviations of theoret-\nical scale tuning from the estimated ones in performance\npractice. The analysis of 94echoi shows a tendency of\nthe singer to level theoretic particularities of the echos that\nstand out of the general norm in the octoechos: theoreti-\ncally extremely large scale steps are diminished in perfor-\nmance.\n1. THE OCTOECHOS AND THE CHRYSANTHINE\nTHEORY\nByzantine Chant is the Christian liturgical song of the East-\nern Roman Empire ( Byzantium ) that gradually emerged\nfrom the Roman Empire from the 4th century on. Byzan-\ntine Chant has been the dominant liturgy of the Eastern\northodox Christianity. Referring to various theoretic ac-\ncounts on Byzantine Chant, Zannos in [18] argues that\n‘none of them can be said to correspond with contempo-\nrary empirical study ’.\nThe main analysis tool used was a pitch class proﬁle\n[17] with high bin resolution, extracted from audio record-\nings with the aid of speciﬁcally designed algorithms. These\nwere applied on a music collection of 94Byzantine Chants\nlabeled after the scale they are performed in. The overall\nbehavior and consistency of empirical scale degree tuning\nwas computed and contrasted to theory through a series of\ntests and experiments.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc￿2013 International Society for Music Information Retrieval.According to Mavroeidis [12] and Thoukididi [16], a\nmode (singular: echos , plural: echoi ) in Byzantine Chant\nis deﬁned by the following ﬁve characteristics: 1) the scale\ndegree steps (SD) between consecutive scale degrees, 2)\nthe most prominent scale degrees (two or three scale de-\ngrees out of which I - III and I - IV scale degrees are\nthe most reoccurring pairs), 3) a short introductory phrase\nthat marks the reference tone, 4) the cadences in the mid-\ndle and the end of a phrase, and 5) the modulations (al-\nterations) applied to particular scale notes depending on\nwhether they are reached by an ascending or a descending\nmelody. Subject to a reform in the 1880s in particular con-\ncerning the sizes of the intervals, the Chrysanthine notation\nmethod (from the 1820s) is used in the ofﬁcial chant books\nof the Greek Orthodox Church up to now [11]. In Chrysan-\nthine theory, the octave is divided into 72equal partitions,\neach of 16.67cents (singular: morio , plural: moria ). The\nscale degree steps are measured in multiples of a morio\n(cf. Table 1). According to this theory there are in total\neight basic echoi, a system also referred to as octoechos\n(‘eight’ + ‘mode’). These eight modes occur in pairs of\nauthentic and corresponding plagal modes: First Authen-\ntic, Second Authentic, Third Authentic, Fourth Authentic,\nFirst Plagal, Second Plagal, Grave, Fourth Plagal . The\nplagal mode has a different reference tone (tonic) than its\nauthentic counterpart, usually a perfect ﬁfth lower than the\none of the authentic mode1but it may share the same scale\nstep sequence. Furthermore, both differ in melodic charac-\nteristics. The scale degree steps may vary according to the\nchant genre ( Heirmoi, Stichera, Papadika ) [12, 16]. Our\nstudy is limited to the basic and simplest echos scales2.\nWe will not consider the fact that scale degree steps of\nan echos can be modulated (altered) based on the melodic\ncharacteristics of a chant or other criteria [12,16] (cf. Sec-\ntion 4).\n2. MUSIC CORPUS\nThe music corpus analysed in this study consists of recor-\nded monophonic songs from the album series of Protop-\n1TheGrave shares the same scale degree steps with the Third Authen-\ntic as well as the same reference tone.\n2Chants of Heirmoi genre of Second, Heirmoi of Second Plagal, Pa-\npadika of Grave, Stichera and Papadika of Fourth, and Papadika of Fourth\nPlagal use different scale steps than the eight basic echoi thus omitted in\nthis study (cf. Table 1).Echos Chant Type Step (in Moria) between Scale Degrees\nI II III IV V VI VII I\nFirst All chants 10 8 12 |12|10 8 12\nFirst Plagal All chants 10 8 12 |12|10 8 12\nSecond Stichera/Papadika 8 14 8 |12|8 14 8\nSecond Plagal Stichera/Papadika 6 20 4 |12|6 20 4\nThird All chants 12 12 6 |12|12 12 6\nGrave Heirmoi/Stichera 12 12 6 |12|12 12 6\nFourth Heirmoi 8 12 12 |10|8 12 10\nFourth Plagal Heirmoi/Stichera 12 10 8 |12|12 10 8\nTable 1 . The scale structure of the eight modes (octoechos) measured in multiples of a morio. The two tetrachords are\nindicated.\nsaltes Georgios Kakoulides [8], Protopsaltes Ioannis Damar-\nlakis [3], Protopsaltes Panteleimon Kartsonas [9], and Pro-\ntopsaltes Dimitrios Ioannidis [7]. From the total of 94\nrecordings, 13are in the First Authentic echos, 15in First\nPlagal, 6in Second Authentic, 6in Second Plagal, 18in\nThird Authentic, 9in Grave, 10in Fourth Authentic and\n17in Fourth Plagal.\n3. COMPUTATIONAL ANALYSIS PROCESS\nTo study scale degree pitch empirically, pitch modulo oc-\ntave histograms are investigated. Built on pitch histograms,\npitch class proﬁles have been applied to detect key and\ntone centres in classical Western music [6, 14]. The au-\nthors in [2] adapted the latter approach to Raag recogni-\ntion. Bozkurt [1] proposed a method to extract the tuning\nof a scale, applied to Turkish maqam. Moelants et al. [13]\nintroduced a peak picking heuristics to extract the scale\ntuning from a modulo octave pitch histogram of African\nscales. Serr `a et al. [15] used pitch class histograms to study\nscale tuning in Hindustani and Carnatic music, investigat-\ning whether this music follows equal temperament rather\nthan just intonation.\nThe procedure followed in this article is summarized\nin Figure 1. First, the pitch ( f0) trajectory is extracted\nfrom each audio recording. A pitch histogram is com-\nputed, compressed into one octave and smoothed. Peaks\nare extracted from the histogram and are employed in tonic\ndetection. The pitch trajectory is then aligned to the es-\ntimated tonic. For recordings of a particular echos, the\naligned pitch trajectories are used to compute the echos\nhistogram . Peaks are then detected in the echos histogram,\nand peak locations are mapped to theoretical scale degree\npitches. Pitches around the selected peak locations are\nused to determine a neighbourhoud of pitches around the\nempirical scale degrees. Finally, a sequence of statistical\ntests is used to compare the estimated practical scale tun-\ning with the theoretical ones.\n3.1 Pitch Trajectory via F0 Detection\nIn the current study we use the f0estimation Yin algo-\nrithm [4]. Considering the melodic characteristics of the\nanalysed music as well as the particularities of the singing\nvoice, the following post processing ﬁlters were designed:noise, silent gaps, octave/ﬁfth error. As a result, a trajec-\ntory of Nmestimated pitches p=(p1,...,p Nm)is gen-\nerated for each recording m. Three experts evaluated the\nestimated pitch trajectory of 20excerpts, presented as a\nsythesized version of the original melody, with an average\n4.2of5(max).\n3.2 Pitch Histogram via Kernel Smoothing\nFor the vector p, we deﬁne the pitch histogram\ncp,b\nk=ΣN\nn=1qr(pn−bk\nh) (1)\nwith the rectangular kernel function\nqr(u)=￿1if|u|≤1\n2\n0otherwise(2)\nforKcenter bins b=(b1,...,b K), being multiples\nbk=(k−1).h+p∗of bin widths hand an offset p∗. The\nChrysanthine theory divides the octave into 72equal par-\ntitions and we multiply this division by 3arriving at K=\n216center bins in the range of one octave, thereby yield-\ning sufﬁcient bin resolution and robustness. The choice of\nhis critical for the subsequent stage of peak picking, since\na too high hcan eliminate relevant peaks whereas a too\nsmall hcan create spurious peaks in the pitch histogram.\nAlthough a large hincreases the smoothness of the pitch\nhistogram, discontinuities in the histogram remain. These\ndiscontinuities are artefacts due to the partitioning of the\npitches in a discrete set of predeﬁned bins. The sharp-\nedged rectangular kernel function in (1) is replaced by a\nsmooth Gaussian kernel function yielding the equation\ncp,b\nk=ΣN\nn=11√\n2πh2e−￿pn−bk￿2\n2h2. (3)\nThe selection of the appropriate smoothing parameter h\nis guided by the task the histogram is used for. For the es-\ntimation of the SD tuning a relatively high smoothing fac-\ntor is employed to avoid spurious peaks in a too detailed\nhistogram. To determine an adequate h, the following as-\nsumption is made: Byzantine theory deﬁnes 4moria ( 67\ncents) as the smallest SD interval. Choosing the quarter-\ntone ( δmin= 50 cents) as the smallest acceptable distance\nbetween two histogram peak locations allows for a marginF0 DetectionHistogramAudio Recording Peak ExtractionTonic DetectionPitch AlignmentFor recordings of the same echos\nEchos HistogramPeak ExtractionPractice-Theory Aligned SDAnalysis/Statistical TestingPractice - Theory ComparisonFigure 1 . Flow diagram of analysis process of a recording\nyielding a theory-practice comparison of the scale tuning\n(SD=Scale Degree).\nfor investigating the deviations between theory and prac-\ntice. Experimenting with h, this assumption is satisﬁed\nwhen his set to 18cents.\n3.3 Peak Extraction\nUsing a peak extraction algorithm, from the smoothed pitch\nclass histogram cp,b,Upeaks (λ, π)can be detected con-\nsisting of peak locations λ=(λ1,...,λ U)andpeak am-\nplitudes π=(π1,...,π U). In [13] the authors propose a\nnumber of heuristics to be used in peak picking, such as\nthe size and height of peaks and intervals between peaks.\nOur proposed algorithm iteratively chooses the peak po-\nsition λu=bkuwith maximum peak height πu=cku,\nthen removing the potential location candidates in a ±δmin\nneighborhood around the selected peak position, to choose\nthe next peak until Upeaks are picked. Uis deﬁned as\nfollows: According to Byzantine theory each echos has at\nleast 7scale notes. In addition, Byzantine theory knows of\nnote alterations. To account for further intonation variants\nin practice, we set U= 12 .δmin= 50 cents is deﬁned as\nthe minimum neighborhood.\n3.4 Tonic Detection\nTo make the ﬁrst bin b1corresponding to the tonic, the\nhistogram has to be circularly (modulo K) shifted by p0,\nthe pitch of the tonic. The authors in [5] calculated the\ncross-correlation between all scale degree prototypes and\nall circularly shifted versions of a smoothed histogram.\nThe pitch shift p0that gives the maximum cross-correlation\nis the estimated tonic and is used to circularly shift the\nsecond histogram. According to theory, the tonic of the\nByzantine echos considered in this study is stated at the\nend of the phrase. Our tonic detection algorithm computes\nthe pitch of the last phrase note from the onset and fre-\nquency information assuming that 1) the last note in the\nrecording is the last note of the melodic phrase and that 2)\nthe ﬁnal phrase note lasts for at least half a second.To compensate with inaccuracies [5], the tonic detection\nalgorithm integrates pitch information from a set of maxi-\nmum three onsets detected at the end of the phrase. Condi-\ntions apply to decide which of the three onsets correspond\nto the last phrase note considering vibrato and ornamen-\ntation (the main incaccuracies in this case), and the pitch\nis then estimated as an average of these. As a ﬁnal step,\nthe estimated pitch of the last phrase note is reﬁned to the\nclosest histogram peak that represents the closest empirical\nscale degree. Three experts found a wrongly automatically\nestimated tonic in 2 out of 20 excerpts.\n3.5 Practice - Theory Comparison\nFor recordings m,1≤m≤Mi, of echos i, the pitch tra-\njectories pm=(p1,...,p Nm)are aligned to the estimated\ntonic and the echos histogram Ci=cp1≤m≤Mi,b\nkis com-\nputed. The peak extraction algorithm applied on Ciyields\nthe peak locations λ=(λ1,...,λ U)and peak amplitudes\nπ=(π1,...,π U)of echos i.\n3.5.1 Practice - Theory Aligned Scale Degrees\nThe normalized theoretical scale degrees are deﬁned as a\nset of scale degree pitches (locations) νθ=(νθ\n1,...,νθ\nL),\nwith normalization νθ\n0=0(in cents) and L=73. From\nthe preselected peaks (λ, π)of echos i, with pitches λ=\n(λ1,...,λ U)and amplitudes π=(π1,...,π U)we esti-\nmate the practical scale degree pitches νπ=(νπ\n1,...,νπ\nL).\nFirst, only those peaks (λ, π)are selected that correspond\nto theoretic scale notes νθ, i.e. those pitches λuthat lie\ncloser to and within a ddistance, from a νθ\nl, yielding the\npitch vector ν. The distance dis set to 150cents (3\n4of\na whole tone) to allow enough margin of deviation be-\ntween empirical peaks and theoretical scale notes. If two\nor more pitches λufulﬁll this condition, the peak with\nhighest amplitude πlis selected, since it corresponds to\na more prominent note such as a scale note. The esti-\nmated scale degree pitch ˆν=(λt1,...,λ tL￿)is deﬁned by\ntl=arg|λt−νl|≤dmax(πt). If no pitches λulie within a d\nrange around the theoretical pitch νθ\nl,tlis not deﬁned and\nthe estimated scale ˆνhas less degrees than the theoretical\nscale νθ,(1≤L￿≤L).\nFrom the estimated scale ˆνand for each recording m,\n1≤m≤Mi, of echos i, pitches ˆplmare selected from the\ntrajectory p=(p1,...,p Nm)that lie within 2-moria dis-\ntance of the estimated scale degree pitches ˆνl,1≤l≤L￿.\nThe 2-moria distance deﬁnes half the size of the small-\nest theoretical scale interval (cf. Table 1). Statistical test-\ning is applied to check whether pitches ˆpl1≤m≤Miare de-\nrived from distributions with mean equal to the theoretical\nscale degree pitches νθ\nlforl=1,...,L￿. For the pitches\nˆplmand theoretical scale degree νθ\nl, the Mean Deviation ,\nmean (ˆplm−νθ\nl)is also computed.\n3.5.2 Statistical Testing\nTo assess the deviation between theoretic and practical scale\ndegree pitches and steps, we apply a chain of tests as an\n3Exceptions with L=8theoretical scale degree pitches exist in some\nvariations of echoi not included here.analytical instrument. As a ﬁrst step, the Shapiro-Wilk test\nis applied, to determine whether the scale degree pitches\nˆpl1≤m≤Miare normally distributed across all Miinstances\nof the same echos i. If the pvalue is above signiﬁcance\nlevel αn=0.05, we assume normal distribution and ap-\nply the t-test to ˆpl1≤i≤I. The t-test hypothesis is formu-\nlated that for echos i, the l-th estimated scale degree pitches\nˆpl1≤m≤Miare derived from a distribution with mean equal\nto the l-th theoretical scale interval νθ\nl. In case the Shapiro-\nWilk rejects the normality hypothesis the Wilcoxon Signed-\nRank test is applied instead. The signiﬁcance level αi\nfor an individual test is set based on the Bonferroni cor-\nrection ; since n= 48 individual tests are applied and\nα=0.05deﬁnes the conﬁdence interval of the whole fam-\nily of tests, each hypothesis is tested at the signiﬁcance\nlevel αi=α\nn=0.05\n48≈0.001.\nIfp<α i, for the probability pof observing ti,lun-\nder the null hypothesis, we reject the null hypothesis and\nconclude that theoretical scale degree pitch deviates signif-\nicantly from the practical scale degree pitch. In addition,\nthe histogram around an empirical scale degree pitch could\nbe characterised by parameters such as variance, skewness,\nkurtosis, following [10].\n4. RESULTS\nThe pitch histograms of all recordings of all echoi can be\nfound in Figure 2. The tuning of the Byzantine scales has\nbeen investigated by comparing the pitches of empirical\nand theoretical scale notes. A series of statistical test was\nemployed to determine for which scale notes practice and\ntheory of tuning deviate. For all recordings of a given\nechos, the pitches ˆparound the estimated scale degrees ˆν\n(at the peaks of the histogram) are gathered. The Shapiro\nWilk test on normality performed for pitches ˆpshowed that\nthe normality hypothesis is rejected for all estimated scale\ndegree pitches. The Wilcoxon Signed-Rank Test ( W-test)\nwith signiﬁcance level αi=0.001% (based on the Bonfer-\nroni correction) is therefore applied to test the null hypoth-\nesis that the median of the empirical pitches is the same as\nthe theoretical pitch of a particular scale degree in a partic-\nular echos.\nTable 2 reveals that the majority of scale degrees of all\nechoi differ signiﬁcantly from theory. The null hypothe-\nsis was not rejected for the scale degrees V of First, VII\nof First Plagal, VI of Second Plagal, and II of Third and\nGrave. The Second, Fourth and Fourth Plagal echoi have\nall their empirical scale degrees signiﬁcantly deviating from\ntheory. The VI. scale degree of the First and First Pla-\ngal has a relatively large mean deviation value with neg-\native sign, i.e., the empirical scale degree pitch is smaller\nthan the theoretical one. According to theory, alterations of\nthese echoi apply that diminish particularly the VI. scale\ndegree when the melody is descending. Other large mean\ndeviations appear for the VII. scale degree of Second Au-\nthentic as well as the III. and VII. scale degrees of Second\nPlagal. These scale degrees are reached by relatively large\ntheoretical scale intervals; the VII scale degree of Second\nAuthentic is reached by the VI-VII scale step of 14 moriawhereas the III. and VII. scale degrees of Second Plagal are\nboth reached by a scale step of 20 moria (cf. Table 1). The\nempirical scale degrees appear with negative deviation, i.e.\nlarge scale intervals are diminished in performance. Fourth\nis the only echos, in which the ﬁrst tetrachord is extended\nby two moria comparised to the other echoi. In practice,\nthe V . scale degree of Fourth Authentic is signiﬁcantly di-\nminished with respect to the theoretical scale degree pitch.\nAn interpretation could be that the singer tends to diminish\nthe abnormally high tetrachord pitch of this echos. In the\nsame echos, also scale degree step VII-I (theoretically 10\nmoria) tends to be diminished towards the more common\nstep VII-I of 6 moria.\nOne may argue that the assignment of empirical peaks\nto the nearest theoretical peak may introduce a dependency\nbetween the theoretical and practical peak positions and\ntherefore bias the test. However this bias is limited due\nto the following reasons: An informal inspection reveals\nthat the empirical peaks are relatively close to the theoret-\nical peaks, and, in all but few exceptions, there are exactly\nseven empirical peaks that correspond to the theoretical\nscale degrees I-VII.\n5. CONCLUSION\nIn this paper, a new method has been introduced to empir-\nically study the tuning of scale degrees. The method has\nbeen used to investigate to what degree performance prac-\ntice of Byzantine Chant follows the widely known Chrysan-\nthine theory. The theoretic hypotheses have been tested on\na corpus of recordings of chants of the octoechos. A com-\nbined method of pitch estimation with appropriate post ﬁl-\ntering has been applied to the recordings. Among the novel\nmethods proposed here are the histogram computation al-\ngorithm that comprises the use of Gaussian kernel and suit-\nable tuning of the smoothing factor. The analysis gives\nsupport to the conjecture that the singer levels the extreme\nstep and scale degrees particularities within the octoechos\nin performance practice of Byzantine Chant. The method-\nology introduced here has applications to a wide range of\noral music traditions.\n6. ACKNOWLEDGEMENTS\nH. P. was supported in part by the German “Bundesminis-\nterium f ¨ur Bildung und Forschung” (BMBF), Grant BFNT,\nNo. 01GQ0850. We are grateful to Daniel Bartz from\nBerlin Institute of Technology for helpful discussion and\nproof reading the manuscript. Thanks to the Music Tech-\nnology Group at Universitat Pompeu Fabra, Barcelona.\n7. REFERENCES\n[1]B. Bozkurt. An Automatic Pitch Analysis Method for\nTurkish Maqam Music. Journal of New Music Re-\nsearch , 37(1):1–13, March 2008.\n[2]P. Chordia and A. Rae. Raag recognition using pitch-\nclass and pitch-class dyad distributions. In Proceedings\nof ISMIR , pages 431–436, 2007.Echos Feature Scale Degrees\nII III IV V VI VII\nFirst (13) p(W-test) 0 0 0 0.0032 0 0\nMean Deviation −16.56 11.25 10.45 0.84 −43.89 5.79\nFirst Plagal (15) p(W-test) 0 0 0.0001 0 0 0.8767\nMean Deviation 28.06 −5.65 −0.91 4.98 −50.06 −0.06\nSecond (7) p(W-test) 0 0 0 0 0 0\nMean Deviation 5.79 −5.56 −6.55 10.36 20.95 −49.64\nSecond Plagal (7) p(W-test) 0 0 0 0 0.1592 0\nMean Deviation 11.20 −44.32 5.06 10.79 0.47 −138.47\nThird (18) p(W-test) 0.0048 0 0 0 0 –\nMean Deviation 0.45 −21.70 −11.19 −5.48 −11.16 –\nGrave (8) p(W-test) 0.0233 0 0 0 0 –\nMean Deviation −0.44 −27.81 −11.05 −17.42 −10.53 –\nFourth (11) p(W-test) 0 0 0 0 0 0\nMean Deviation 5.27 5.58 −5.21 −39.14 11.13 38.85\nFourth Plagal (16) p(W-test) 0 0 0 0 0 0\nMean Deviation 22.06 11.17 16.63 17.86 17.00 22.25\nTable 2 . Signiﬁcance of scale degree pitch deviation between practice and theory for all echoi (number of instances in\nbrakets). Since all echoi are aligned to pitch 0 for scale degree I, only II-VII are shown. For scale degrees II-VII the p-value\nof the test statistic and the mean pitch deviation (in cent) between practice and theory are indicated. The p-value of the\nWilcoxon Signed-Rank test (W-test) is denoted. Zero p-values correspond to values smaller than 10−4. Practice-theory\ndeviations (cf. Table 1) greater than two moria are colored and discussed in the text.\n[3]I. Damarlaki. Pws tha mathw na psallw praktika\n[Learn how to perform the Byzantine chants] [Audio\nCD]. Polychronakis, Crete, 1999.\n[4]A. de Cheveigne and H. Kawahara. YIN, a fundamen-\ntal frequency estimator for speech and music. The Jour-\nnal of the Acoustical Society of America , 111(4):1917,\n2002.\n[5]A. C. Gedik and B. Bozkurt. Evaluation of the makam\nscale theory of arel for music information retrieval on\ntraditional turkish art music. Journal of New Music Re-\nsearch , 38(2):103–116, 2009.\n[6]E. G ´omez. Tonal Description of Music Audio Signals .\nPhD thesis, Universitat Pompeu Fabra, 2006.\n[7]D. Ioannidis. H theoria tis Byzantinis mousikis stin\npraxi [Theory of Byzantine music in practice] . Ioan-\nnidis Dimitrios, Athens, 2005.\n[8]G. I. Kakoulidis. Pws na vriskoume praktika tous okto\nihous tis Byzantinis mousikis [How to estimate empir-\nically the eight modes of byzantine music] . Athens,\n1999.\n[9]P. Kartsonas. Praktiki Methodos Ekmathisis tis\nPsaltikis Tehnis [Practice Methods for Learning\nByzantine Chant] [Audio CD] . The Hunt of St. George\nMount Athos.\n[10] G. K. Koduri, J. Serr `a, and X. Serra. Characterization\nof intonation in carnatic music by parametrizing pitch\nhistograms. In Proceedings of ISMIR , pages 199–204,\n2012.[11] K. Levy and C. Troelsg ˚a rd. Byzantine chant . Ox-\nford University Press, accessed August 24, 2011,\nhttp://www.oxfordmusiconline.com/subscriber/article/\ngrove/music/04494, 2011.\n[12] M. Mavroeidis. Oi mousikoi tropoi stin Anatoliki\nMesogeio [The musical modes in East Mediterranean] .\nFagotto, Athens, 1999.\n[13] D. Moelants, O. Cornelis, and M. Leman. Exploring\nAfrican Tone Scales. In Proceedings of ISMIR , pages\n489–494, 2009.\n[14] H. Purwins, B. Blankertz, and K. Obermayer. A new\nmethod for tracking modulations in tonal music in au-\ndio data format. In Proceedings of IJCNN , volume 6,\npages 270–275, 2000.\n[15] J. Serr `a, G. K. Koduri, M. Miron, and X. Serra. As-\nsessing the tuning of sung indian classical music. In\nProceedings of ISMIR , pages 157–162, 2011.\n[16] A. Thoukididi. Kripida Byzantinis mousikis [Founda-\ntions of Byzantine music] . Kykkos Church, Cyprus,\n2003.\n[17] G. Tzanetakis, A. Ermolinskyi, and P. Cook. Pitch His-\ntograms in Audio and Symbolic Music Information Re-\ntrieval. Journal of New Music Research , 32(2):143–\n152, 2002.\n[18] I. Zannos. Intonation in Theory and Practice of Greek\nand Turkish Music. Yearbook For Traditional Music ,\n22(1990):42, 1990.Figure 2. For all recordings of all echoi, the pitch histograms are displayed. The vertical lines indicate pitches of scaledegrees according to Chrysanthine theory [16]. The y axis represents the normalized histogram count. The bold red linesrepresent the echos histogram computed from pitch trajectories across all recordings of the same echos."
    },
    {
        "title": "Combining Harmony-Based and Novelty-Based Approaches for Structural Segmentation.",
        "author": [
            "Johan Pauwels",
            "Florian Kaiser",
            "Geoffroy Peeters"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416104",
        "url": "https://doi.org/10.5281/zenodo.1416104",
        "ee": "https://zenodo.org/records/1416104/files/PauwelsKP13.pdf",
        "abstract": "This paper describes a novel way to combine a well-proven method of structural segmentation through novelty detection with a recently introduced method based on harmonic analysis. The former system works by looking for peaks in novelty curves derived from self-similarity matrices. The latter relies on the detection of key changes and on the differences in prior probability of chord transitions according to their position in a structural segment. Both approaches are integrated into a probabilistic system that jointly estimates keys, chords and structural boundaries. The novelty curves are herein used as observations. In addition, chroma profiles are used as features for the harmony analysis. These observations are then subjected to a constrained transition model that is musically motivated. An information theoretic justification of this model is also given. Finally, an evaluation of the resulting system is performed. It is shown that the combined system improves the results of both constituting components in isolation.",
        "zenodo_id": 1416104,
        "dblp_key": "conf/ismir/PauwelsKP13",
        "keywords": [
            "structural segmentation",
            "novelty detection",
            "harmonic analysis",
            "probabilistic system",
            "key detection",
            "chord detection",
            "structural boundaries",
            "novelty curves",
            "chroma profiles",
            "constrained transition model"
        ],
        "content": "COMBINING HARMONY-BASED AND NOVELTY-BASED APPROACHES\nFOR STRUCTURAL SEGMENTATION\nJohan Pauwels, Florian Kaiser and Geoffroy Peeters\nSTMS IRCAM-CNRS-UPMC\njohan.pauwels@ircam.fr, ﬂorian.kaiser@ircam.fr, geoff roy.peeters@ircam.fr\nABSTRACT\nThis paper describes a novel way to combine a well-proven\nmethod of structural segmentation through novelty detec-\ntion with a recently introduced method based on harmonic\nanalysis. The former system works by looking for peaks in\nnovelty curves derived from self-similarity matrices. The\nlatter relies on the detection of key changes and on the dif-\nferences in prior probability of chord transitions accordi ng\nto their position in a structural segment. Both approaches\nare integrated into a probabilistic system that jointly est i-\nmates keys, chords and structural boundaries. The nov-\nelty curves are herein used as observations. In addition,\nchroma proﬁles are used as features for the harmony analy-\nsis. These observations are then subjected to a constrained\ntransition model that is musically motivated. An informa-\ntion theoretic justiﬁcation of this model is also given. Fi-\nnally, an evaluation of the resulting system is performed. I t\nis shown that the combined system improves the results of\nboth constituting components in isolation.\n1. INTRODUCTION\nStructural segmentation of music is the process in\nwhich an audio recording is divided into a number of\nnon-overlapping sections that correspond to the macro-\ntemporal organisation of a piece. These entities usually\ntake the form of verses and choruses in popular music, or\nof movements in classical music. The obtained sections\ncan then be used for interactive listening, audio summa-\nrization, synchronization or as an intermediate step in fur -\nther content-based indexing.\nTraditional approaches to structural segmentation have\nbeen categorized into three categories [14]: repetition-\nbased, novelty-based and homogeneity-based methods. A\nmid-level representation, called self-similarity matrix , is\noften used for these task. It is obtained from the feature se-\nquence by comparing each instance with all time-delayed\ncopies of itself according to some similarity measure. The\nresult is a visualisation of the musical structure. It was\noriginally introduced into the music domain by Foote [2].\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage an d that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieva l.Repetition-based approaches rely on the hypothesis that re -\ncurring patterns in the feature sequence cause a perception\nof a higher structure. In a self-similarity matrix this be-\ncomes visible as stripes on the off-diagonals [4]. Novelty-\nbased systems try to identify transitions between two con-\ntrasting parts, which are also perceived as structural boun d-\naries by humans [1]. Initially, these methods were the\ndual approach of homogeneity-based methods [10], as one\nwas looking exclusively for transitions between two dis-\ntinct sections that are similar according to some musical\nproperty [3]. Recently however, this approach has been\nextended to also include contrast between a homogeneous\nand a non-homogeneous section [6].\nThe method we propose builds upon the novelty-based\nmethod of [3], but integrates this with a novel approach\nthat is based on the estimated harmony of the piece [16].\nPrevious efforts of deriving a structural segmentation fro m\nharmony have mostly been concerned with using chroma\nfeatures for the construction of a self-similarity matrix, in-\nstead of or in addition to timbre-related features [5]. We\nhowever work with a higher-level harmony description,\nin the form of key and chord estimates. In this sense,\nour approach is somewhat similar to previous systems by\nMaddage [11] or Lee [8], but in contrast to their systems,\nours works simultaneously, not sequentially. They ﬁrst ex-\ntract key and chord estimates, which they subsequently use\nas inputs for a structure estimation. A sequential system\ncan also be constructed the other way around, using an es-\ntimate of the structure as input to aid with chord estima-\ntion. An example of this kind is the method of Mauch [12].\nWe, on the other hand, construct a probabilistic system that\njointly estimates keys, chords and structural boundaries. It\nis based on the assumption that some chord combinations\nare more common around structural boundaries. This is es-\npecially clear when they are expressed as relative chords in\na key, as this gives a musicologically richer representatio n.\nThese relative chord combinations will then be used as evi-\ndence for structural boundaries, together with the positio ns\nof key changes and peaks in the novelty measure.\nIn the remainder of this paper, we’ll ﬁrst give an out-\nline of our probabilistic system in Section 2.1. Then we’ll\ngo deeper into the details of how to integrate the harmony-\nbased and the novelty-based approach into this framework\nin Section 2.2, respectively Section 2.3. Afterwards, we\ndescribe the experiments we performed and analyse the re-\nsults in Section 3. We conclude with some closing remarks\nand ideas for future work in Section 4.2. A PROBABILISTIC SYSTEM FOR THE JOINT\nESTIMATION OF KEYS, CHORDS AND\nSTRUCTURAL BOUNDARIES\n2.1 Overview\nIn this section we will describe the probabilistic system\nthat we propose for the determination of a structural seg-\nmentation of a track, along with an estimation of the local\nkeys and chords. As a starting point we use the system of\nPauwels et al. [15] for the simultaneous estimation of keys\nand chords. It consists of an HMM in which each state\nrepresents a combination of a key and a chord. We extend\nit by letting each state qrepresent a structural position in\naddition to a key and a chord. A key kcan take one of Nk\nvalues, a chord one of Ncvalues and the structural posi-\ntionsscan take one of two values: Lwhich means that qis\nthe last state of a structural segment or Owhich means that\nit is not. Finally, we add a single state to handle the case\nwhen no chord is being played, notably at the beginning\nand end of a recording. In this state, the key will accord-\ningly take a “no-key” value and the structural position will\ntake a value of s=R. In summary, q= (s,k,c)with\ns∈ {L,O},k∈ {K1,...,K Nk},c∈ {C1,...,C Nc}or\nq= (R,no-key,no-chord).\nFinding the most likely sequence of key, chord and\nstructure labels given a sequence of observations X=\n{x1,x2,...,x T}then amounts to ﬁnding the state se-\nquenceˆQ={ˆq1,ˆq2,...,ˆqT}that optimally explains these\nobservations. Because the state variable qconsists per\ndeﬁnition of the combination of a chord, key and struc-\nture variable, these three optimal sequences will always be\njointly estimated. Afterwards, the structural boundaries ˆS\ncan be derived from the optimal state sequence by inserting\na boundary for every transition from a state where s=L\nto one where s=O, or from or to a state with s=R.\nThe derivation of the optimal key ˆKand chord sequence\nˆCfrom the latter is even more trivial.\nBy applying Bayes’ theorem, and further assuming the\nﬁrst order Markov property and independence of the ob-\nservations, we can rewrite the probability to be maximized\nto\nˆS,ˆK,ˆC= argmaxT/productdisplay\nt=1P(xt|st,kt,ct)\nP(st,kt,ct|st−1,kt−1,ct−1)\nThe time-interval there indicates the index of the interbeat\nsegments, where the beats are estimated by ircambeat [17].\nThe probabilities of this HMM will now be determined by\nthe combination of 2 different components that are each\nusing their separate observations. The ﬁrst component is\na method to estimate a structural segmentation simultane-\nously with the harmony of a piece. It uses chroma features.\nThe second component determines structural boundaries\nbased on a novelty measure that is derived from timbral\nfeatures. For clarity reasons, we introduce separate nota-\ntions for the chroma observations ytand for the novelty\nmeasurezt(soxt= [ytzt]). We will consider the nov-\nelty observations independent of the chroma observations:Figure 1 . An example annotated sequence with the three\nstructure dependent positions indicated\nP(xt|st,kt,ct) =P(yt|st,kt,ct)P(zt|st,kt,ct). The\nﬁnal probability to be maximized will therefore be\nˆS,ˆK,ˆC= argmaxT/productdisplay\nt=1P(yt|st,kt,ct)P(zt|st,kt,ct)\nP(st,kt,ct|st−1,kt−1,ct−1)\nOf these three terms, the chroma observation prob-\nabilityP(yt|st,kt,ct)and the transition probability\nP(st,kt,ct|st−1,kt−1,ct−1)will be set by the harmony-\nbased component of our system, while the novelty observa-\ntion probability P(zt|st,kt,ct)will be set by the novelty-\nbased component.\nWe use different observations for both components to\ncapture different types of information, in hopes of them\nbeing complementary. For instance, a change of instru-\nmentation won’t be detected as a structure boundary by the\nharmony-based component, while the novelty-based com-\nponent won’t recognize a chord sequence that is typical for\nan ending. The choice of their respective observations re-\nﬂects this.\n2.2 The harmony-based component\n2.2.1 Motivation and information theoretical justiﬁcatio n\nThe basic premise upon which our approach is built, is that\nchord sequences, and more speciﬁcally chord pairs, exhibit\na different prior probability depending on their position\nwith respect to structural boundaries. In addition to the\nspeciﬁcity of these chord combinations, we also argue that\nthe number of distinct chord pairs at the end of a segment\nis lower compared to all possible chord sequences in the\nmiddle of a structural segment. Structural boundaries seem\nan implausible place to experiment with some less com-\nmon chord combinations, established by well-known, dia-\ntonic chord combinations. Real world examples that sup-\nport these statements are the observation that movements\nin classical music typically end with one of a select num-\nber of chord combinations, called cadences, or that musical\nsection changes in jazz and blues are often preceded by so-\ncalled “turn-arounds”.\nIn order to verify these statements in a methodological\nway, we ﬁrst identify three categories of chord pairs based\non their position with respect to structural boundaries. An\nexample sequence for which these three categories are in-\ndicated can be found in Figure 1. The ﬁrst class is named\nﬁnal and this contains the chord pairs that form the two last\nchords of a structural segment. The second category we’llIsophonics Quaero\nmajor minor major minor\nintra 6.17 6.25 4.29 4.98\ninter 3.91 4.52 2.99 2.37\nﬁnal 2.91 3.51 2.36 3.18\nTable 1 . Perplexity of relative chord transition models per\nmode according to structural position\ncallinter and this consists of all chord pairs that straddle a\nstructural boundary. The last class of chord pairs is called\nintra and this includes all the remaining chord pairs, the\nones that occur in the beginning and middle of a structural\nsegment.\nTo reason about chord pairs in a musicologically more\ninformative way, we will interpret them as relative chords\ninterpreted in a key. This representation reﬂects more\nclosely the way scholars analyse harmonic movement.\nAlso in accordance to musicological analysis, both chords\nwill be interpreted in the same key. Because of the for-\nward motion in music, this will be the key annotated at the\ntime of the ﬁrst chord. In mathematical notation, we will\ndeﬁne a key kas the combination of a tonic tand a mode\nm. A chord cis deﬁned as the combination of a root r\nand a type p. Bothtandrbelong to one of the 12 different\npitch classes. We restrict ourselves in this paper to 2 modes\nand 4 chord types ( m∈ {major,(natural)minor },p∈\n{maj,min,dim,aug}). We then deﬁne a relative chord\nc′with respect to a key kby expressing the root ras the\ninterval between the tonic and the root: i=d(t,r). There-\nfore we can equivalently express a key-chord pair as a key-\nrelative chord pair (k,c) = (t,m,r,p) = (t,m,i,p) =\n(k,c′). In order to take the inherent shift-invariance in\nharmony analysis into account, we just ignore the tonic\nof the key and only keep the mode. For sequences of 2\nkey-chord pairs, we end up with just a mode and a pair of\nrelative chords as a representation for the local harmony:/parenleftbig\nmn,c′\nn,c′\nn+1/parenrightbig\nwherenis the chord index.\nWe then construct relative chord transition models for\neach of the three structural categories by counting occur-\nrences of all successions of 2 consecutive relative chords\nin a corpus that is annotated with keys, chords and struc-\ntural segments. Sequences that do not appear in the data\nset are assigned a probability using Kneser-Ney smooth-\ning [7]. The corpus should be annotated such that all the\npositions are indicated where at least one of key, chord or\nstructural segment changes. We have two such data sets at\nour disposal. The ﬁrst one is the publicly available “Iso-\nphonics” set and more speciﬁcally the subset that has been\nused for the MIREX 2010 chord estimation competition.\nIt consists of 217 full songs, mostly by the Beatles (180\nsongs), the remainder by Queen (20) and Zweieck (17).\nThe second one is a private data set called “Quaero” and it\ncontains 53 songs from a number of diverse artists in the\npopular genre. We can now quantify the difference in rel-\native chord distribution between the various structure de-\npendent transition models by calculating the bigram model\nperplexity PP(C′\n1,C′\n2|m)per mode mfor each of them.C′\n1andC′\n2represent the collection of all relative chords\nthat appear as ﬁrst, respectively second, element in the bi-\ngrams. The model perplexity is deﬁned as the exponential\nof the entropy H(C′\n1,C′\n2|m)expressed in nats:\nPP(C′\n1,C′\n2|m) = exp( H(C′\n1,C′\n2|m))\n= exp\n−/summationdisplay\nc′\n1,c′\n2P(c′\n1,c′\n2|m)logP(c′\n2|c′\n1,m)\n\n= exp\n−/summationdisplay\nc′\n1P(c′\n1|m)\n/summationdisplay\nc′\n2P(c′\n2|c′\n1,m)logP(c′\n2|c′\n1,m)\n\nThis expresses the mean prior uncertainty of a bigram as\na function of its mode. A lower value means that the\ntransition probability is concentrated into fewer combina -\ntions of two chords. The perplexities for both data sets\ncan be found in Table 1 and as can be seen, they conﬁrm\nour hypothesis: the values for the “intra”-model are indeed\nsigniﬁcantly higher than those for the “inter” and “ﬁnal”-\nmodel and this for both corpora.\nFor the calculation of the transition probabilities in the\nfollowing paragraphs, we will make use of the information\ncaptured in these structure dependent relative chord trans i-\ntion models. The reasoning will therefore be reversed: in-\nstead of showing that a structural boundary often suggests\na speciﬁc set of relative chord pairs, we’ll use the occur-\nrence of such relative chord combinations as evidence to\nestimate structural boundaries.\n2.2.2 Transition probabilities\nThe transition probabilities P(st,kt,ct|st−1,kt−1,ct−1)\nare calculated by a prior musicological model that con-\nsists of a number of submodels. By introducing some mu-\nsicologically motivated constraints to the transition pro b-\nabilities, we want to enforce a number of relationships\nbetween the concepts of keys, chords and structural seg-\nments. These will ensure that our estimation always pro-\nduces sensible results and have as an added beneﬁt that this\nalso speeds up the calculation. The ﬁrst three constraints\nwe impose are 1) a key change kt/ne}ationslash=kt−1is only allowed\nto occur together with a chord change ct/ne}ationslash=ct−1, 2) a struc-\ntural segment must contain at least two different chords (or\na single no-chord), 3) there must be a change in chord or\nin key between segments. These three limitations can be\neasily enforced by ensuring that every state change implies\na chord change. This makes the state duration model effec-\ntively a chord duration model that we control by a single\nparameter Ps:\nP(st,kt,ct|st−1,kt−1,ct−1) =\n/braceleftBigg\nPsst=st−1∧kt=kt−1\n0st/ne}ationslash=st−1∨kt/ne}ationslash=kt−1,∀ct=ct−1(1)\nWe use the same value for Psas found in the original sys-\ntem [15].The remaining probabilities P(st,kt,ct|st−1,kt−1,\nct−1),∀ct/ne}ationslash=ct−1of the chord changing transitions are\ncalculated by the combination of three submodels. We fur-\nther apply Bayes’ theorem repeatedly to arrive at a decom-\nposition into three terms\nP(st,kt,ct|st−1,kt−1,ct−1)\n=P(st|st−1,kt−1,ct−1)P(ct|st,st−1,kt−1,ct−1)\nP(kt|st,st−1,kt−1,ct,ct−1)\nThe ﬁrst term P(st|st−1,kt−1,ct−1)will be used to\ncontrol the ease of changing the structure variable sand\nthus to control the insertion rate of segment boundaries.\nWe use a simple model that ignores the key and chord in-\nﬂuence and consists of a single parameter ωthat balances\nthe probability of going to s=Oors=Lafter leaving\ns=O.\nP(st|st−1) =\n\nω s t−1=O,st=O\n1−ω st−1=O,st=L\n1st−1=L,st=O\n0st−1=L,st=L\nWe can already recognize the structure-dependent rela-\ntive chord transition model of the previous section in the\nsecond term P(ct|st,st−1,kt−1,ct−1). Our three cate-\ngories of chord transitions – inter ,intra andﬁnal – each\ncorrespond to a certain combination of the state variables.\nTheintra model will be used when st−1=Oandst=O,\ninter whenst−1=Landst=Oandﬁnal whenst−1=O\nandst=L. Finally, from our deﬁnition of Lit follows\nthat when st−1=Landst=L, only the self probability\nPsshould be allowed, to account for the fact that the last\nchord of a structural segment can – and most likely will –\nlast more than one time step. The other probabilities are\nset to zero.\nSince we already established that a key change implies\na chord change, we can neglect the inﬂuence of the chords\nct−1,ctin the third term P(kt|st,st−1,kt−1,ct,ct−1).\nWe thus end up with P(kt|st,st−1,kt−1). Furthermore,\nwe impose the supplemental constraint that a key change\ncan only occur between segments. Mathematically, this\ncan be expressed as st−1=O⇒P(kt|st,st−1,kt−1) =\nδkt,kt−1withδthe Kronecker-delta. For the inter key tran-\nsitionsP(kt|st=O,st−1=L,kt−1), we reuse the the-\noretical model from [15], based on Lerdahl’s distance [9]\nbetween keys.\nIn Figure 2 one can ﬁnd a simpliﬁed state diagram of\nour system, in which states are regrouped by the structure\nvariable. Only the transitions from the point of view of the\nstructure variable are drawn in order not to overload the\npicture, but the constraints on key and chord transitions ar e\nindicated next to the arrows. The names of the structure de-\npendent relative chord transition models are also indicate d.\n2.2.3 System complexity\nThe result of adding the additional constraints is that the\ncomplete transition matrix will have a well-deﬁned, sparse\ns=O s=L \nﬁnal \ninter intra NcNkPs\nNcNkPs\nkt=k t-1 kt=k t-1 \nct≠c t-1 \nct≠c t-1 s=R Ps\nk=no-k, c=no-c \nFigure 2 . State diagram of the system\nstructure. The two upper quadrants consist of block diag-\nonal matrices with Nkblocks of side Nc, the lower left\nquadrant is dense and the lower right quadrant is a diago-\nnal matrix. In comparison to a system that only estimates\nkeys and chords concurrently, the number of states gets\ndoubled by repeating every key-chord state for s=Oand\ns=L. On the other hand, because of the sparsity of the\ntransition matrix, the increase in the number of transition s\nremains limited. More speciﬁcally, the number of transi-\ntions is(NkNc)2+ 2NkN2\nc+NkNc+ 1, which corre-\nsponds in our conﬁguration to an increase of 8%instead of\nthe theoretically maximum of 400% that would be reached\nfor a dense transition matrix. This sparsity is exploited in\nthe implementation of the Viterbi algorithm to limit the in-\ncrease in computation time.\n2.2.4 Chroma emission probabilities\nAs our chroma features, we use the implementation by\nNi et al. [13] known as Loudness Based Chromagrams.\nThese are 24-dimensional vectors that represent the loud-\nness of each of the 12 pitch classes in both the tre-\nble and the bass spectrum. They are calculated with a\nhop size of 23 ms and are afterwards averaged over the\ninterbeat interval. We make the assumption that keys\nand chords can be independently tested for compliance\nwith an observation and that the structure position is\nconditionally independent of the observations, such that\nP(yt|st,kt,ct) =P(yt|ct)P(yt|kt). The chord acous-\ntic probability P(yt|ct)is modelled as a multi-variate\nGaussian with full covariance matrix. Its parameters are\ntrained on the aforementioned “Isophonics” data set. The\nkey acoustic probability P(yt|kt)is calculated by taking\nthe cosine similarity between the observation vector ytand\nTemperley’s key templates [18]. These represent the stabil -\nity of each of the 12 pitch classes relative to a given key.\n2.3 The novelty-based component\nThe other major component of our system is a novelty-\nbased structural segmentation algorithm. We’ll use a\nsimple implementation that is conceptually very close to\nFoote’s original proposal [3]. First, a self-similarity ma -0 50 100 15000.51Original novelty curve\n0 50 100 15000.51Peak salienced novelty curve\n0 50 100 15000.51Log compressed peak salienced novelty curve\nFigure 3 . An example of the transformations of the novelty\ncurve\ntrix is calculated from a sequence of MFCC’s and the ﬁrst\nfour spectral moments (spectral centroid, spread, skew-\nness and kurtosis). The similarity measure that is used\nis the cosine similarity. From this matrix, a time vary-\ning novelty curve is derived by convolving the matrix with\na two-dimensional novelty kernel along its diagonal. We\nuse a kernel size of 22.5 s and a step size of 250 ms. In\na stand-alone system, the peaks of the resulting novelty\ncurve are detected and structural boundaries are inserted a t\nthose positions. In our case however, we’ll use the com-\nplete curve to calculate the novelty observation probabili ty\nP(zt|st,kt,ct).\nWe assume that the novelty observations are condition-\nally independent of chord and key state, such that we end\nup withP(zt|st,kt,ct) =P(zt|st). We thus need to\nmodel the novelty observations for each of the possible\nvalues of st, i.e.O,LorR. By deﬁnition, there is a clear\nrelation between the peaks of the novelty curve and a high\nprobability of being in a state that will insert a structural\nboundary upon leaving it ( s=Oors=R). Therefore\nwe model the structure acoustic probability for the Land\nRstates as a (half) Gaussian centered on 1. Likewise, the\nprobability of the s=Ostates will be modelled by a half\nGaussian centered on 0. However, we won’t use the values\nof the novelty curve directly as the observations ztfor the\nnovelty observation probability P(zt|st). A ﬁrst remark is\nthat the novelty curves are designed to be used in conjunc-\ntion with peak picking. Therefore their relative value with\nrespect to surrounding valleys is what matters, and not thei r\nabsolute value as is desired for our probabilistic system. I n\norder to adapt the novelty values to our use, we therefore\nﬁrst transform the values of the curve. If we represent thevalues of the novelty curve by v, then the transformation\nis the following:\nv′\nj= (vj−min(vj−w:vj))(v−min(vj:vj+w))\nwherejis the index of the novelty curve, which has a ﬁxed\nsample rate of 4 Hz, and wthe size of a window around the\nj-th value. An example of this transformation can be seen\nin Figure 3, where the original signal is represented in the\nﬁrst row and the processed one in the second. The effect\nis that valleys now reach all the way down to 0 and that\nhighs indicate peaks with a high salience. As evident in\nour example, there can be quite a large difference between\nthe saliences of the peaks corresponding to annotated seg-\nments. In order to diminish the differences, we apply a log\ncompression, which is subsequently rescaled to the interva l\n[0,1]for convenience:\nv′′\nj=log10/parenleftbig\n1+αv′\nj/parenrightbig\nThe result on our example curve is shown in the third row\nof Figure 3. Finally, we want to account for small de-\nviations of the peak positions with respect to the actual\nstructural boundaries. Therefore we look for extrema of\nthe log-compressed novelty curve in the current beat seg-\nment and the segments that are adjacent on each side. For\nP(zt|st=L)andP(zt|st=R)this will be the maxi-\nmum and for P(zt|st=O)the minimum, so that the ﬁ-\nnal dimension of ztwill actually be 2-dimensional: one\ndimension with the local minima of v′′and one with its lo-\ncal maxima. This step also changes the time scale from a\nﬁxed step size of 250 ms to beat-synchronous observations\n(indexjtot).\n3. EXPERIMENTAL RESULTS\nIn this section, we will evaluate our system for various\nconﬁgurations. We evaluate the structural segmentation\nby calculating the precision P(tol)and recall R(tol)be-\ntween the generated and the annotated structure. They are\na function of a tolerance interval tolwhose purpose is to\nallow for small deviations from the desired result to be stil l\nconsidered correct. The precision is deﬁned as the number\nof estimated boundaries for which an annotated boundary\nlies within the tolerance interval centered around its posi -\ntion divided by the total number of estimated boundaries.\nThe recall on the other hand, is the relative number of an-\nnotated boundaries that have an estimated boundary within\nits tolerance interval. Both measures are combined in an F-\nmeasureF(tol). These measures are calculated for every\nsong of the data set and are afterwards averaged to give one\nglobal result.\nWe calculate the results for two tolerance intervals, 0.5 s\nand 3 s, in accordance with the MIREX structure seg-\nmentation competition. Both the harmony-based and the\nnovelty-based system were tested separately, as well as the\ncombination of both approaches. For the harmony-based\nand the combined system, we performed our experiments\ntwice: once with the structure dependent relative chord\nmodels derived from the Isophonics data set and once withF(3s)F(0.5s)\nharmony-based (Isophonics) 54.72 34.90\nharmony-based (Quaero) 52.44 29.47\nnovelty-based 61.84 33.53\ncombined (Isophonics) 64.38 35.41\ncombined (Quaero) 64.13 34.08\nTable 2 . Results on the Isophonics data set\nthose from the Quaero set. The results on the Isophonics\ndata can be found in Table 2. We can see that the com-\nbination of both approaches has a synergetic effect. Sep-\narately, they each have different strengths. The harmony-\nbased approach is better in precisely locating the structur e\nboundaries, as apparent from the F(0.5s)results, while\nthe novelty-based approach performs better when a larger\ndeviation is allowed. As could be expected, the harmony-\nbased and combined systems work better with the Isophon-\nics relative chord models, since they are perfectly matched\nwith the test set. However, most of the synergy remains\nwhen using the models learned on the Quaero set, showing\nthe generality of these models. Additionally, the combined\nsystem is also less sensitive to the choice of relative chord\nmodels than the harmony-based method is.\n4. CONCLUSION\nIn this paper, we proposed a method for structure estima-\ntion by combining 2 different approaches. The ﬁrst is a\ntraditional way of segmenting structure based on a timbral\nnovelty measure. The second is based on a harmonic anal-\nysis that is performed concurrently with the structure es-\ntimation. It makes use of chroma features. Together they\nform a probabilistic system for the simultaneous estimatio n\nof keys, chords and structure boundaries. We’ve shown\nthat the combination of both approaches works better than\neach of the two systems on its own.\nIn the future, we will experiment with a post-processing\nstep to extend our resulting structural segmentation into a\nfull structure estimation that includes the identiﬁcation and\nlabelling of repeated segments. After all, for each of our\nestimated segments we have a harmonic analysis available\nthat could be used as a feature for the clustering of similar\nsegments, in addition to the more low-level features that\nare currently used for this task.\n5. ACKNOWLEDGEMENTS\nThis work was partly supported by the Quaero Program\nfunded by Oseo French agency.\n6. REFERENCES\n[1] M. J. Bruderer, M. McKinney, and A. Kohlrausch.\nStructural boundary perception in popular music. In\nProc. ISMIR , 2006.\n[2] J. Foote. Visualizing music and audio using self-\nsimilarity. In Proc. ACM Multimedia , 1999.[3] J. Foote. Automatic audio segmentation using a mea-\nsure of audio novelty. In Proc. ICME , 2000.\n[4] M. Goto. A chorus section detection method for mu-\nsical audio signals and its application to a music lis-\ntening station. IEEE Trans. Audio, Speech, Language\nProcess. , 15(5), 2006.\n[5] K. Jensen. Multiple scale music segmentation using\nrhythm, timbre, and harmony. EURASIP Journal Ad-\nvances in Signal Processing , 073205, 2007.\n[6] F. Kaiser and G. Peeters. Multiple hypotheses at multi-\nple scales for audio novelty computation within music.\nInProc. ICASSP , 2013.\n[7] R. Kneser and H. Ney. Improved backing-off for m-\ngram language modeling. In Proc. ICASSP , 1995.\n[8] K. Lee. A system for acoustic chord transcription and\nkey extraction from audio using hidden Markov mod-\nels trained on synthesized audio . PhD thesis, Stanford\nUniversity, 2008.\n[9] F. Lerdahl. Tonal pitch space . Oxford University Press,\nNew York, 2001.\n[10] M. Levy and M. Sandler. Structural segmentation of\nmusical audio by constrained clustering. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n16(2), 2008.\n[11] N. C. Maddage. Automatic structure detection for pop-\nular music. IEEE MultiMedia , 13(1), 2006.\n[12] M. Mauch and S. Dixon. Using musical structure to\nenhance automatic chord transcription. In Proc. ISMIR ,\n2009.\n[13] Y . Ni, M. McVicar, R. Santos-Rodr´ ıguez, and T.\nDe Bie. An end-to-end machine learning system for\nharmonic analysis of music. IEEE Transactions on Au-\ndio, Speech and Language Processing , 20(6), 2012.\n[14] J. Paulus, M. M¨ uller, and A. Klapuri. Audio-based mu-\nsic structure analysis. In Proc. ISMIR , 2010.\n[15] J. Pauwels, J.-P. Martens, and M. Leman. Improving\nthe key extraction accuracy of a simultaneous key and\nchord estimation system. In Proc. ICME , 2011.\n[16] J. Pauwels and G. Peeters. Segmenting music through\nthe joint estimation of keys, chords and structural\nboundaries. In Proc. ACM Multimedia , 2013.\n[17] G. Peeters and H. Papadopoulos. Simultaneous beat\nand downbeat-tracking using a probabilistic frame-\nwork: theory and large-scale evaluation. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n19(6), 2011.\n[18] D. Temperley. The cognition of basic musical struc-\ntures . MIT Press, 1999."
    },
    {
        "title": "Comparing Onset Detection &amp; Perceptual Attack Time.",
        "author": [
            "Richard Polfreman"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415232",
        "url": "https://doi.org/10.5281/zenodo.1415232",
        "ee": "https://zenodo.org/records/1415232/files/Polfreman13.pdf",
        "abstract": "Accurate performance timing is associated with the perceptual attack time (PAT) of notes, rather than their physical or perceptual onsets (PhOT, POT). Since manual annotation of PAT for analysis is both time-consuming and impractical for real-time applications, automatic transcription is desirable. However, computational methods for onset detection in audio signals are conventionally measured against PhOT or POT data. This paper describes a comparison between PAT and onset detection data to assess whether in some circumstances they are similar enough to be equivalent, or whether additional models for PAT-PhOT difference are always necessary. Eight published onset algorithms, and one commercial system, were tested with five onset types in short monophonic sequences. Ground truth was established by multiple human transcription of the audio for PATs using rhythm adjustment with synchronous presentation, and parameters for each detection algorithm manually adjusted to produce the maximum agreement with the ground truth. Results indicate that for percussive attacks, a number of algorithms produce data close to or within the limits of human agreement and therefore may be substituted for PATs, while for non-percussive sounds corrective measures are necessary to match detector outputs to human estimates.",
        "zenodo_id": 1415232,
        "dblp_key": "conf/ismir/Polfreman13",
        "keywords": [
            "accurate performance timing",
            "perceptual attack time",
            "notes",
            "manual annotation",
            "real-time applications",
            "automatic transcription",
            "onset detection",
            "audio signals",
            "onset detection data",
            "onset types"
        ],
        "content": "COMPARING ONSET  DETECTION  & PERCEPTUAL  \nATTACK  TIME  \n Dr Richard Polfreman  \n \n University of Southampton \nr.polfreman@soton.ac.uk  \nABSTRACT  \nAccurate p erformance timing is associated with the pe r-\nceptual attack time (PAT) of notes, rather than their phy s-\nical or perceptual onsets (PhOT, POT). Since manual a n-\nnotation of PAT for analysis is both time -consuming and \nimpractical for real -time applications, automatic tra n-\nscription is desirable. However, computational methods \nfor onset detection  in audio signals are  conventionally \nmeasured against  PhOT or POT data. This paper d e-\nscribes a comparison between PAT and onset detection \ndata to assess whether in some circumstances they are \nsimilar enough to be equivalent , or whether  additional \nmodel s for PAT -PhOT difference  are always necessary. \nEight published  onset algorithms, and one commercial \nsystem,  were tested with five on set types in short mon o-\nphonic sequences.  Ground truth was established by mul-\ntiple human transcription of the audio for PATs  using \nrhythm adjustment with synchronous presentation , and \nparameters for each detection algorithm manually adjust-\ned to produce the maximum agreement with the ground truth. Results indicate that for percussive attacks, a nu m-\nber of algorithms produce data  close to or within the li m-\nits of human agreement and therefore may be substituted \nfor PATs, while for non -percussive sounds corrective \nmeasures are necessary to match detector outputs to h u-\nman estimates.  \n1. INTRODUCTION AND MOT IVATION  \nThis research forms part of a larger project invo lving \nevaluation of controller hardware and parameter ma p-\npings in the context of real -time physical modeling sy n-\nthesis  [10]. Thus a specific device (e.g. Microsoft K i-\nnect) will have its control outputs (e.g. performer’s 2D \nhand position) mapped onto synthe sis model parameters \n(e.g. plectrum position in relation to a string). A number \nof techniques for controller evaluation have been pr o-\nposed, e.g.  [9], including qualitative and quantitative \nmethods. One method of evaluation to be  used will ask \nthe performer  to match as accurately as possible a given \naudio target phrase using a given combination of contro l-\nler, mapping and synthesis configuration. The ta rget and \nthe attempt will then be compared to assess how well the task was completed, in addition to other q ualitative a s-\nsessments. Given that a number of participants, contro l-\nlers and targets may be used, it would be helpful to co m-\nplete the performance analysis comput ationally rather \nthan rely on expert markup of the a udio. While in some \nsituations it would be possible to use the timing of co n-\ntrol data such as MIDI NoteOn events directly, with pe r-\nhaps a fixed latency, here the timing of a note or onset \nmay vary significantly for a given control value depen d-\nant on other parameters. For example, the position of a \nplectrum along a string, pluck release threshold, current string displacement and velocity and tension (pitch) will \nall impact upon the dis tance from the string the plectrum \nwill need to reach before releasing the string and genera t-\ning the onset. This indi rect control over event timing \nmeans that measuring the audio output is necessary. Pr e-\nvious work on onset detection generally does not consi d-\ner timing accuracy in detail, justifiably prioritising dete c-\ntion rates (type 1 and type 2 errors) and using  a tempo ral \ntolerance between ground truth and detection s beyond \nwhich  an onset i s said to have been missed [3 ]. Here \nhowever, the detailed timing of the onsets is critical.  \nThe measure of two performances being “in time” is a \ncomplex issue with a large nu mber of conte xtual fa ctors, \nbut in this case the target and performance are short mo n-\nophonic solo instrument phrases with a fixed tempo and it \nwas felt that this case would be simple enough to be stu d-\nied. More expressive ti ming feature are  ignored and PAT \nsynchronous events are considered  the ideal. \n2.  ONSET TIME  \n2.1 When is a Note?  \nThree potential onset times are described in pu blished \nwork. Physical onset time  (PhOT) is usually consi dered \nto be the audio signal first rising from zero, perceptual \nonset time  (POT) the time at which a human listener can \nfirst detect this change and finally percep tual attack time  \n(PAT) is the “perceived m oment of rhythmic placement” \n[15], or rhythmic centre, and is similar to the p -centre \nconcept in speech analysis [1 3]. A “correct” performance \ntherefore places the events’ PATs appropriately, rather \nthan PhOTs or POTs.  \nWhile most studi es have considered PAT to be a sp e-\ncific time, Wright proposes that PAT is distribu ted over a \nfinite time period and should be considered  as a probabi l-\nity density function describing the likel ihood of a listener \nhearing the PAT at  each time point (PAT -pdf) [15 ]. This  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full cit ation on the first page.   \n© 2013 International Society for Music Information Retrieval    \n \ncould account for variation between listeners and by ind i-\nviduals in repeated trials and i mplies that there wi ll be \nspan of ti me over which an event can remain in musical \ntime with another . This spread of time values is of inte r-\nest here , since this governs how well -localized the PAT \nfor a particular sound is and how accurately  a detection \nalgorithm must match the ground truth.  \n2.2 PAT Measurement  \n2.2.1 Measurement Methods  \nPAT can be measured in a number of ways as ident ified \nin [4, 7,  13, 14, 15]. The intrinsic PAT of a sound is typ i-\ncally not measured directly, but rather the delta -PAT \n(!PAT) [7], calculated via compar ison against a refe r-\nence tone. If the re ference sound  is very short in time, its \nPAT will be very close to its PhOT and so the target \nsound PAT can be estimated from the !PAT. PAT must \nbe expressed relative to a zero point, usual ly either the \nsound’s PhOT or the offset from th e beginning of the au-\ndio file where a se quence is being considered [15 ]. \nThe most common measurement is  rhythm adjus t-\nment , where two sounds are aligned by the li stener until \nthey either appear synchronous (sound together) or isoc h-\nronous (sound evenly space d rhythmically, alternately \npresented)  [14]. Both synchronous and isochronous \nmethods have problems (such as event fusion in synchr o-\nnous presentation) while isochronous cannot be used \nwhere the PATs for a musical sequence are to be mea s-\nured, rather than is olated events. Likewise Villing’s phase \ncorrection r esponse (PCR) method [14 ] is unsuitable for \nsequences and so the synchronous method was  used here.   \nA tool was created for participants to align a reference \nsound against a series of test sounds containin g a number \nof onsets (F igure 1). While the reference sound  should  be \nshort, Wright found that if it is too short there are pro b-\nlems for accurate alignment. He also found that a refe r-\nence click based on matching the spectrum of the test sound aided PAT alig nment [15 ]. In our experiment, the \nreference was a simple sine tone, which is the same as the \ntarget we will use for the perfor mer to fo llow, which will \ninclude pitch changes at a later stage.  Wright gave u sers \ncontrol over amplitudes to help avoid fusion  of the two \nevents and this was included here. Our tool also a llowed \nthe user to change the pitch of the refe rence, again to help \nlimit fusion  ([4] suggests frequency ind ependence of \nPAT) . Gordon [7] indicated that subjects had difficulty \nmatc hing sounds w ith very different attack time s, and so \na user variable attack time was included to amelio rate \nthis, although clearly this has the potential  to add u ncer-\ntainty to the gr ound truth and so was limited to <127ms . \nThe participant can choos e a sound, select any part of \nit to be looped and place a marker on the sound that tri g-\ngers the reference tone. The marker can be dragged with the mouse and fine -tuned by changing the value in a \nnumber box, in samples at 44.1kHz sa mple rate. Thus the \nlocat ion of the refe rence can be adjust ed by ~0.02ms. \nParticipants were instructed to adjust this value until the \ntest event and ref erence soun ded musically sy nchronous. The vi sual di splay is to aid u sers in finding physical o n-\nsets quic kly before searching thos e regions for percept ual \nalignment. For each ev ent the tool re corded the PAT  and \nthe other user settings  so that these could also be an alysed \nif nece ssary. Pa rticipants were each gi ven a training se s-\nsion (in a ddition to a wri tten manual) and asked to com-\nplete the task u sing hea dphones . \n \nFigure 1. Software tool for ground truth collection.  \n2.2.2 Test Sounds  \nFive test sounds were used, four were synthesized with \nthe IRCAM’s Modalys software [6] and the performan ces \nmade deliberately i mperfect, so that each event in the s e-\nquence would not be identical and the timing of events not strictly metrical.  The sequences provide a set of vari a-\ntions in timbre  and attacks as one might expect in an i n-\nstrumental performance. The dynamics were genera lly \nstable but with occ asional deviations. Th e models were:  \nplucked string (un -damped), legato bowed string, struck \nplate (un -damped) and a single reed-tube. Only in the \nreed sound was complete silence reached  between onsets \nand not for all of those . The final sound was a sine tone, \nwhich is used as the target for performance matching, in \nthis case precisely metrical. These beeps were 95ms long \n(5ms attack, 90ms de cay) with a 500ms inter-PhOT inte r-\nval. Each sound was normalized,  had a fundamental fr e-\nquency of 130.81Hz (an octave be low mid dle C) and con-\ntained 16 onsets , providing  80 events in t otal. \n2.2.3 Ground Truth Results \nNine  participants completed the task for all 80 events  and \nso each sound file had 144 marked- up onsets and there \nwere 720 data points  in tota l. All participants had some \nmusical experience, typically in ensem bles or bands \nand/or formal performance training.  Where data seemed \nparticularly erroneous, such as a missing or d uplicated \nevent, or in isolation extremely different to others  , parti c-\nipants were asked to review and double -check their d ata \nto ensure they were content with the values ori ginally \nsupplied , and , only if not, amend them .  As with other \nstudies, participants reported that the task was challen g-\ning, particularly  with the non -percussive sounds,  while \none r eported that ( in the reed case) there were a range of \ntime values over which the refe rence and test sound were   \n \nequally “in time”, and that they had simply tried to be \nconsistent in where they plac ed the refer ence sound.  \n!\"#$%&'()*+,-).(%&)/-#050.00 25.00 .00 -25.00 -50.00 -75.001,2&3beep\nstrike\nreed\npluck\nbow\n  \nFigure 2. Scatterplot of ! PAT ground truth data . \n \nTo group the results of ! PAT values across different \nevents within a particular sequence, the mean ! PAT val-\nue was taken for each event and then each ! PAT value \nrepla ced by its distance from that mean. Figure 2 shows \nscatter plots of these mean -shifted !PAT values (with \nvertical jitter to improve visibility). As expected , shorter  \nattacks gave rise to more tightly clustered ! PAT times, \nalthou gh outliers remain , while th e longer  attacks pr o-\nduce more widely spread results,  as the location of the \nnote is more ambiguous. We also expect smaller  varia tion \nin the beep sounds since each event is almost identical, \ndiffering only in the phase of the sine in each. The pluck s \nshow g reater spread than the other percussive attacks, \nagain expected due to the more complex articulation: a \ndouble attack of the ini tial plectrum impact on the string \nfollowed rapidly by the release of  the string creating the \nnote (F igure 3). The time between impact and release was \ntypically be tween 20ms and 40ms, averaging 23ms . The \naudio files were also annotated for PhOT for comparison with !PAT and onset detector times. For the percussive \nattacks this was straightforward as in each case there \nwere discont inuities in the signal at the point where each \nnew event began  and which could be found through vis u-\nal inspe ction. In the case of the pluck sounds, both the \nimpact and string release times were noted. For the reed sound, o nsets starting from silence were similarly clear, \nwhile ot hers were estimated from the inflection point  in \namplitude between the decay  of one note to the beginning \nof the next. The bow sound was particularly difficult and \nrequired inspection of the sonogram in addi tion to the \ntime domain sig nal and PhOT was estimated from disru p-\ntion to the harmonic structure as one event ends and the \nother begins.  Table 1 shows the mean and standard devi a-\ntion offsets from !PAT to PhOT for each sound, where \nthe pluck sound is using the string release time. All apart \nfrom pluck are positive values as ex pected, where \"PAT \nis later than PhOT. As can be seen from the table, \"PAT \nappears very close to PhOT  for the short  attacks, although \nwith some variation as reflected in Figures 2 and 4. Inte r-estingly pluck is very close to the string release point , in \nfact slightly earlier , suggesting  an effect of the preceding \nimpact bringing the PAT forward. Given the close \nagreement between mean \"PAT and PhOT for the short  \nattacks, this is indicates that onset detectors which m eas-\nure PhOT should provide timing data close to \"PAT. For \nthe non -percussive at tacks \"PAT is significantly later \nthan PhOT, and so the utility of onset detectors will d e-\npend on whether they remain close to PhOT or are sim i-\nlarly d elayed.  \n \nimpact \n release \n23ms \n \nFigure 3. Section of pluck waveform showing decay of \nprevious event followed by initial plectrum impact and \nrelease of string . \n \n \nTabl e 1. Mean  and standard deviation for \"PAT- PhOT  \ndistance . \n \n!\"#$%beep strike reed pluck bow!&'$%'(%)%*+,'&,\"$)-./025.00\n20.00\n15.00\n10.00\n5.00\n.00\n  \nFigure 4. Mean !PAT standard deviations . \nFigure 4 shows the # across even ts for ! PAT, indica ting \nhow consistently human listeners can determine ! PAT \nfor each sound (against the  reference  tone). Th us for \nbowed sounds, ± # gives a spread of ~42ms  and for the \nsine beep ~11.0 ms. While only bow and pluck passed \nShapiro Wilks normality tests, over 70% of data for each \nsound were within ± # of the mean. The limit of discrim i-\nnation of temporal events is typically considered to be \n~10ms [4 ]. Wright logically proposed a system for auto-\nmatic mark -up of a udio using onset dete ction followed by Sound Bow Pluck  Reed  Strike  Beep  \nMean  (ms) 23.52 -0.46 51.21 3.48 2.08 \n# (ms)  22.83 10.64 18.46 7.42 5.91   \n \na PAT model to correct for the difference between PhOT \nand PAT  [15]. However, i f the time differences b etween \nthe ground t ruth and onset times reported by onset dete c-\ntors are within similar limits  to human listeners  it indi-\ncates that these may be used directly to provide PAT data  \nwithout adding a specif ic PAT -PhOT  model . \n3. ONSET DETECTION  \n3.1 Onset Detection Algorithms  \nOnset detect ion algorithms are typically based on PhOT  \nor POT , with  a time tolerance to decide  successful dete c-\ntions . The task  usually comprises  three main steps: (o p-\ntional) pre -processing;  generation of an onset detection \nfunction (ODF) that i ndicates the probability  of an onset \nat each moment in time;  and peak selection across the \nODF. While some methods are psychoacoustically moti-\nvated, differences between PhOT, POT and PAT are us u-\nally ignored. Here those  differences are important  if an \nonset detector is to provide  \"PAT estimate s. \nSeveral  comparative studies of the performance of on-\nset detection algorithms have been published,  while the \nMIREX event compares a number of new algorithms a n-\nnually. Studies,  including  [1, 3, 5], co mpare the rates of \nfalse positives and fal se negatives against a sele ction of \ntest sounds. Col lins [3] compared 16 onset detection a l-\ngorithms with NPP (non -pitched percussive) and PNP \n(pitched non -percussive) mono phonic sounds, fin ding \nthat for the NPP case, a spectral difference fun ction based \non work by Klapuri [8] was most effective, while for the \nPNP case all algorithms performed less well, with a phase \ndeviation method being the most successful  [1]. While \ncomparing algorithms against PhOT rather than PAT, \nCollins used detection tolerances of 5 0ms for PNP sounds \nand 25ms for NPP, which compare  well with the figures \nshown in F igure 4  [3]. \n3.2 Onset Measurement  \n3.2.1 Implementation  \nA Max patch was developed to run a number of onset d e-\ntection algorithms against the test audio. This di splays the \nODF for each detector as well as the detection hits. In i-\ntially 8 algorithms were tested, including two widely \nknown Max objects bonk ~ and sigmund ~, both later r e-\njected as unable to provide su fficiently accurate results. \nTo compare results with a commercial onset detect ion \nsystem, the audio software Melodyne 3.21 was also i n-\ncluded in the experiment . For each sound Melodyne’s \npercussive mode detection was used, as this outperformed the other o ptions, even on non -percussive sounds, and it \nshould be noted that no detection param eters were user \nadjusted in this case.  \nA modified\n2 version of the aubioonset ~ MSP object \nby Andrew Robertson [11], itself a port of algorithms i m-\n                                                             \n1 http://www.celemony.com/  \n2 Modified to include audio rate output of the onset dete c-\ntions as 1 -sample delta functions, rather than  Max bangs , \nto improve timing accuracy.  plemented by Paul Brossier [2] was used for high fr e-\nquency con tent (HFC), energy based, modified Kullback -\nLeibler (MKL), complex, spectral difference (SD) and \nphase d eviation (PD) functions, the equations for which \ncan be found in the literature  [2]. In each case the FFT \nsize was 2048 with a hop of 128 samples. While there are \nmore recent algorithms, these wer e chosen as being wid e-\nly available and frequently r eferred to in the literature as \nthe basis for other algorithms or tests. Due to difficulties \nwith non -percussive attacks, two adaptations were i m-\nplemented as Max patches – weighted phase d eviation \n(WPD) an d spectral flux (SF) following Dixon [5], the \nlatter rectifying the difference between frames in SD, i m-\nportant in distinguishing between onsets and off sets. \nPeak -picking for WPD and SF involved  taking the diffe r-\nence between the outputs of two moving ave rage filters \n(using average~ ) and passing the result to a Schmitt tri g-\nger (thresh ~).  One filter was coarse  providing  an adap-\ntive threshold (avera ging over ~130ms), the other fine to \nsmooth the ODF  (typically ~20ms ). \n3.2.2 Comparison with Ground Truth  \nDetection fun ction parameters were adjusted to achieve \nas close as possible to 100% success rate, i.e. 0  false posi-\ntives (FP) or false negatives (FN). This was achieved for \nall the percussive attacks with all dete ctors, but the reed \nand bow sounds proved more problemat ic. Figure 5  \nshows the mean distance of each a lgorithm from the \nground truth for the percussive attacks. The error bars i n-\ndicate one standard deviation above and below the mean , \nthe first set being those of the ground truth .  \n  \n \nFigure 5. Mean distance fr om ground truth for each a l-\ngorithm, percussive attacks (positive values are la ter). \nAs can be seen in the figure, HFC and Energy are typica l-\nly late detectors, and fall outside the standard deviation \n(#) ranges for the ground truth, while Melodyne pe r-\nformed  very well across all three percussive attacks,  pre-\nempt ing the !PAT values ( PhOT earlier than PAT ) as \nexpected.  In fact compa rison with the PhOT data shows \nthat Melodyne  very accurately tracked PhOT (e.g. - 0.1ms \naverage distance  for the beep), performing slightly worse   \n \nwith pluck as it marked the impact time rather than string \nrelease  for two events.  Strike and beep across all the d e-\ntectors show relatively consistent offsets from the ground truth, albeit varying by sound and detector, the # values \nare all < 3ms.  For pluck , HFC, Energy, WPD and Melo-\ndyne achieved a #  less than the ground truth.  \nTo decide whether an onset detector can be used as a \n!PAT measurement tool  we must define how close ly the \noutputs of the detector must correspond to th e ground \ntruth.  Table 2  shows a summary of each detector against \neach percu ssive attack  for three simple  tests. The first test \n(a) is simply whether  the standard deviation of the detec-\ntor output is less than that of the ground truth – not in i t-\nself suffici ent, but indica tive of relative stability. The s e-\ncond (b) and third (c) state whether combinations of the \ndetector mean and standard deviation lie within limits of \nthe 1 or 2 standard deviatio ns of the ground truth:  \n(µD+!D)<!GT                                   (1) \n(µD+(2!!D))<(2!!GT)                                   (2) \nwhere µD is the de tector mean deviation from ground \ntruth, #D and #GT the detector and ground truth standard \ndeviations respectively. Equation 1 implies  (for normal \ndistributions) that we expect ~64% of detector values to \nlie within one standard dev iation of the ground truth \nmean,  changing to ~95% of detector  outputs within two \nstandard de viations  of ground truth mean  in equation 2.  \n \n  \nTable  2. Onset detector  summary  for percussive attacks.   \n \n As can be seen in Table 2 , Melodyne passed each test for \neach perc ussive attack , indicating that it is  likely to pro-\nvide a useful equivalent to !PAT data, while WPD is e f-\nfective for beep and pluck, and MKL for strike.  \nWith  the reed sounds  several algorithms conflated o n-\nset and offsets, with Complex, SD, MKL and PD often \nshowing stronger peaks, sometimes double peaks, on of f-\nsets in the detection function (see Figure 6), making their \nFN rate  unacceptably large1. Melodyne also suffered \nfrom of fset conflation  with the reed sound , although the \ndetection function  could not be examined. As e xpected, \n                                                             \n1 Testing with a single clarinet sample indicated that th ese \nalgorithms suffer offset conflation there also, rather than this being a product of the physical model synthesis.  the half- wave rectification introduced in the SF algorithm \neliminated this problem, with offset peaks significantly \nlowered, as did  WPD by shaping the response by ampl i-\ntude. The remaining detectors were able to provide zero \nFN and FP rates, and for HFC, SF and WPD with # less \nthan the ground truth. All means were delayed with r e-\nspect to the !PAT ground truth and none were co ntained \nwithin ±# of the groun d truth (see Figure 7).  \n \nTime (ms)!DF!\n0! 100! 200! 300! 400! 500! 600! 700!\n \n \nFigure 6 . Complex  onset detection function over the d u-\nration of a single reed event, showing large offset peaks.  \n \n \n \nFigure 7 . Mean distance from ground truth for selected \nalgorithms, reed sound.  \n \n \n \nFigure 8 . Mean distance from  ground truth for selected \nalgorithms, bow sound.  \n The bowed sound was most problematic  (Figure 8 ), with \nonly Melodyne achieving  0 FN and FP rates,  while others \n(e.g. HFC)  resulted in an FP rate of ~33%  if adjusted to \nzero FN rates. SF had one FP with rect ification off -  i.e. \nas SD but with the dual -filter peak picker (labeled MSF). \nMKL only ident ified a single on set, while WPD achieved \nzero FN and FP, when  rather than weighting each phase \ncontribution by the magnitude from the FFT fr equency Sound  Beep  Strike  Pluck  \nDetector  a b c a b c a b c \nHFC  Y - - Y - - Y - - \nEnergy Y - - Y - - Y - - \nSF Y - Y Y - Y - - - \nWPD  Y Y Y Y - - Y Y Y \nMKL  Y - Y Y Y Y - - - \nComplex Y - Y Y - Y - - - \nSD Y - Y Y - - - - - \nPhase  Y - Y Y - Y - - - \nMelodyne  Y Y Y Y Y Y Y Y Y   \n \nbins, a thresho ld was used (TPD) . None of the three best \ndetectors were within the ground truth range or had # \nlower than the ground truth.  \n4. DISCUSSION AND FUTURE WORK  \nThe aim of this work was to assess whether automatic \nonset detection methods might be used to provide metrics \nfor measuring performance acc uracy, where the phrases \nto be assessed would be monophonic  but the sounds p o-\ntentially complex . This required testing  since p erfor-\nmance timing is considered  to be PAT based, while onset \ndetection  PhOT or POT based. Further, the reported  per-\nformance  of onset dete ctors is often reduced to type I and \nII errors , rathe r than distances from ground truth.  \nGround truth data cap tured via rhythm adjustment \nwith synchronous presentation indicated levels of agre e-\nment of approximately 12-20ms for percussive attacks \nand 42ms for non -percussive (within a single standard \ndeviation ). Given the likelihood that there is indeed a \nspan of time offset  over which t wo sounds may be said to \nremain in time rhythmically, it would seem useful to d e-\nvelop a new method of ! PAT measurement that does not \nforce the participant to select a single tim e value, but r a-\nther supports identification of a range. Such a method \ncould make the task easier for participants, speeding up \nthe a nnotation process and increasing accuracy.  \nAll of the onset detectors managed zero type I and \ntype II errors with the percussive attacks, but only some \nproduced results close enough to the ground truth to be regarded as PAT equivalent data. For the non- percussive \nsounds, achieving 100% detection even in these short s e-\nquences proved challenging, and the timing did not match the ground truth closely enough, requiring  some form of \nPAT model to cor rect for this. Future work should inve s-\ntigate existing models, such as those tested  in [4]. \nThe algorithms used are well known and therefore r e-\nsults may usefully be compared with other stu dies, but it \nwould be helpful  to test more recent algorithms  for per-\nformance improvements.  Recent work has explored the \ninfluence of peak -picking algorithms on the perfo rmance \nof onset detection and it would be useful to test altern a-\ntive methods in this co ntext, particularly as the temporal \nlocation of the peak is so critical here  [12]. Similarly, pre -\nprocessing could be explored.  \nIt would be useful  if the MIREX onset detection test \ndata were additionally annotated fo r PAT so that alg o-\nrithms  could be asses sed against PAT as well as \nPhoT/ POT data and against a large data set.  \n5. REFERENCES  \n[1] J. Bello , L. Daudet , S. Abdallah , C. Duxbury , M. \nDavies, and M. Sandler:  “A Tutorial on Onset \nDetection in Music Signals ”, IEEE Trans.  On \nSpeech And Audio Proc.,  Vol 13, No 5 , pp.1035-\n1047, 2005.  \n[2] P. Brossier:  Automatic Annotation of Musical A u-\ndio for Interactive Applications , Ph.D. Thesis, \nQueen Mary University of London, UK, 2006.  [3] N. Collins:  “A Comparison of Sound Onset \nDetection Algorithms with Emphasis on \nPsychoacoustica lly Motivated Detection Functions ”, \nProceedings of AES118 Conve ntion, 2005.  \n[4] N. Collins:  “Investigating computational models of \nperceptual attack time ”, Proceedings of the 9th \nInternational Conference on Music Perception & \nCognition (ICMPC9 ), pp. 923- 929, 2006.  \n[5] S. Dixon:  “Onset Detection Revisited ”, Procee dings \nof the 9th Int. Conference on Digital Audio Effects (DAFx’06) , pp. 133-137, 2006.  \n[6] N. Ellis , J. Bensoam  and R. Caussé: “ Modalys \nDemonstration ”, Proceedings  of the International \nComputer Music Conference ( ICMC) , 2005.  \n[7] J. W. Gordon: “ The perceptual attack time of \nmusical tones ” J. Acoust. Soc. Am., Vol. 82, No. 1, \npp. 88–105, 1987.  \n[8] A. Klapuri: “ Sound onset detection by applying \npsychoacoustic knowledge ”, Proc.  of the IEEE \nInternational Conference on Acoustic s, Speech, and \nSignal Proc. (ICASSP), pp. 3089– 92, 1999.  \n[9] S. O’Modhrain: “A Framework for the Evaluation of \nDigital Musical Instruments”,  Computer Music \nJournal , Vol. 35, No. 1, pp. 28– 42, 2011.  \n[10] R. Polfreman:  “Multi -Modal Instrument: Towards a \nPlatform for Comparative Controller Evaluation ”, \nProceedings of the International Computer Music \nConference (ICMC),  pp. 147- 150, 2011.  \n[11] A. Robinson : “Queen Mary University of London: \nAndrew Robertson –  Software”, http://www.eecs.  \nqmul.ac.uk/~andrewr/software.htm , access ed July \n2013.  \n[12] C. Rosão , R. Ribeiro , and D. Martin de Matos : \n“Influence Of Peak Selection Methods On Onset \nDetection ”, Proceedings of the 13th  International \nSociety for Music Information Retrieval Conference  \n(ISMIR 2012) , pp. 517- 12, 2012.  \n[13] S. Scott: “The po int of p -centres”,  Psychological \nResearch , Vol. 61, pp. 4–11, 1998.  \n[14] R. Villing:  Hearing the Moment: Measures and \nModels of the Perceptual Centre , Ph.D. Thesis, \nNational University of Ireland Maynooth , 2010.  \n[15] M. Wright:  The Shape Of An Instant: Measuring \nAnd Modeling Perceptual Attack Time With \nProbability Density Functions,  Ph.D. Thesis, \nStanford University, Stanford, CA, 2008 ."
    },
    {
        "title": "Dunya: A System to Browse Audio Music Collections Exploiting Cultural Context.",
        "author": [
            "Alastair Porter",
            "Mohamed Sordo",
            "Xavier Serra"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417355",
        "url": "https://doi.org/10.5281/zenodo.1417355",
        "ee": "https://zenodo.org/records/1417355/files/PorterSS13.pdf",
        "abstract": "Music recommendation and discovery is an important MIR application with a strong impact in the music industry, but most music recommendation systems are still quite generic and without much musical knowledge. In this paper we present a web-based software application that lets users interact with an audio music collection through the use of musical concepts that are derived from a specific musical culture, in this case Carnatic music. The application includes a database containing information relevant to that music collection, such as audio recordings, editorial information, and metadata obtained from various sources. An analysis module extracts features from the audio recordings that are related to Carnatic music, which are then used to create musically meaningful relationships between all of the items in the database. The application displays the content of these items, allowing users to navigate through the collection by identifying and showing other information that is related to the currently viewed item, either by showing the relationships between them or by using culturally relevant similarity measures. The basic architecture and the design principles developed are reusable for other music collections with different characteristics.",
        "zenodo_id": 1417355,
        "dblp_key": "conf/ismir/PorterSS13",
        "keywords": [
            "Music recommendation",
            "Music industry impact",
            "Generic music recommendation systems",
            "Carnatic music",
            "Web-based software application",
            "Audio music collection",
            "Musical concepts",
            "Database",
            "Audio recordings",
            "Editorial information"
        ],
        "content": "DUNYA: A SYSTEM FOR BROWSING AUDIO MUSIC COLLECTIONS\nEXPLOITING CULTURAL CONTEXT\nAlastair Porter, Mohamed Sordo, Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\nfalastair.porter,mohamed.sordo,xavier.serra g@upf.edu\nABSTRACT\nMusic recommendation and discovery is an important MIR\napplication with a strong impact in the music industry, but\nmost music recommendation systems are still quite generic\nand without much musical knowledge. In this paper we\npresent a web-based software application that lets users in-\nteract with an audio music collection through the use of\nmusical concepts that are derived from a speciﬁc musical\nculture, in this case Carnatic music. The application in-\ncludes a database containing information relevant to that\nmusic collection, such as audio recordings, editorial infor-\nmation, and metadata obtained from various sources. An\nanalysis module extracts features from the audio record-\nings that are related to Carnatic music, which are then used\nto create musically meaningful relationships between all\nof the items in the database. The application displays the\ncontent of these items, allowing users to navigate through\nthe collection by identifying and showing other informa-\ntion that is related to the currently viewed item, either by\nshowing the relationships between them or by using cul-\nturally relevant similarity measures. The basic architecture\nand the design principles developed are reusable for other\nmusic collections with different characteristics.\n1. INTRODUCTION\nThe research being performed within the CompMusic [14]\nproject aims to provide new technologies, interfaces and\nnavigation methods for browsing music collections from\nspeciﬁc music cultures. In particular we are working with\nseveral non-western music traditions and are taking ad-\nvantage of their speciﬁc characteristics to promote an ap-\nproach in MIR that emphasises the use of domain knowl-\nedge in every step of the research project.\nOur research goal is to extract musically meaningful de-\nscriptions from audio recordings by using as much contex-\ntual information as possible. From these descriptions we\ncan develop similarity measures between all the entities\nthat have been identiﬁed in a particular music collection.\nSeveral music navigation and discovery systems have\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.been proposed in the last few years [8, 9, 15]. Although\ndiffering in visualisation aspects, they all share the notion\nof computing audio similarity for music discovery. Other\nsystems, like the Sonic Visualiser [2], are designed for vi-\nsualising low-level and mid-level audio features. One of\nthe main advantages of Sonic Visualiser is that it allows\nthe integration of third party algorithms through the use of\nplug-ins. More recently, Goto et al. proposed Songle [3],\na system that, among other things, allows users to actively\nlisten to audio while visualising musical scene descriptors\nby engaging them in correcting estimation errors of the ex-\ntracted audio features.\nSome of these systems make use of related semantic\ninformation from the Internet and other sources, however\nnone use cultural-speciﬁc information to build a compre-\nhensive music listening experience.\nIn this paper we introduce Dunya, a system that allows\nusers to explore a given music collection and discover mu-\nsically relevant relationships between all the items that have\na musical relevance. The name Dunya means world1in\nseveral languages of the cultures that we are studying. In\nDunya we are emphasising the concepts of exploration and\nsimilarity based on cultural speciﬁcity and the design of\nthe interface and of the system architecture emphasises this\nidea.\nFor this ﬁrst version of Dunya we are focusing on Car-\nnatic music, the classical music tradition of the south of\nIndia. In the next sections we will describe the architecture\nof the Dunya system, including the types and the sources\nof the information that we are using. We then describe the\ndevelopment of our interface for a music collection of the\nCarnatic music tradition.\n2. ARCHITECTURE\nThis section presents the overall architecture of the Dunya\nsystem. Dunya consists of three main aspects (Figure 1).\nFirstly, raw data is gathered from many different sources.\nThis data includes audio and other information about the\nmusic and musicians who performed it. The second as-\npect is the development of a data schema (a structured data\nrepresentation) that speciﬁcally reﬂects the concepts that\nare found in a music tradition. The data schema allows us\nto store relationships and similarities between items in the\ndatabase. The ﬁnal part of Dunya is a graphical interface\n1Originally coming from the Arabic language, Dunya means more\nprecisely world in a metaphysical sense.Figure 1 . An architecture overview of Dunya. We gather raw data from a number of sources and combine them in a\nstructured database.\nfor exploring this structured data. The following sections\ndescribe these three aspects in more detail.\n3. RAW DATA\n3.1 Audio recordings\nThe audio recordings are the core of Dunya. In order to\nbuild our audio collection, we bought 300 audio CDs, con-\ntaining approximately 380 hours of audio and 1900 distinct\nrecordings. We will be extending this collection and hope\nto reach 500 hours of audio in the next two years. We\nripped these audio CDs, and stored the recordings in our\nﬁle server.\n3.2 Editorial metadata\nEvery audio recording of the collection is accompanied\nby editorial metadata. Since most audio recordings come\nfrom commercial CDs, the editorial metadata comes from\nthe cover or the booklet accompanying the CD. We use\nMusicBrainz2to store and organise this metadata, which\nincludes names of recordings, releases, compositions, com-\nposers, artists, and other culture-speciﬁc musical concepts.\nThough Musicbrainz contains metadata for a large number\nof audio and albums, most of the metadata of the audio\nrecordings that we obtained was not yet in MusicBrainz\nand so we added it ourselves.\nMusicBrainz is a very useful information source for us\nfor a number of reasons. MusicBrainz provides stable iden-\ntiﬁers for musical entities, in the form of MusicBrainz Iden-\ntiﬁers (MBIDs). These IDs are guaranteed to be unique\n2http://musicbrainz.orgover all items in the MusicBrainz database. This makes\nit possible to unambiguously refer to a musician, compo-\nsition, recording, or released album. For this project we\nuse MBIDs to refer to all entities in our corpus. We see\nMusicBrainz as a useful tool in many ﬁelds of MIR re-\nsearch to explicitly identify datasets used in research. It is\nable to represent additional relationships in addition to the\ntypically used artist-release-recording model. An example\nrelationship that we preserve in our dataset is the informa-\ntion indicating the people who performed on albums and\nthe instruments that they played. All factual data entered\ninto MusicBrainz becomes freely available under the Mu-\nsicBrainz project’s licenses to be used for any desired use.\nFor a research project such as ours the free availability of\nthe data makes it a useful source for factual information.\n3.3 Biographical metadata\nWhere they exist we use Wikipedia for biographies of artists\nin our database. When MusicBrainz lists a link to Wikipedia\nas the artists biography, we use that page. Where such a\nlink doesnt exist we create a link between MusicBrainz and\nWikipedia as part of our import process.\n3.4 Images\nWe gather representative images for each item. We get\ncover art images for commercial CDs from the Cover art\narchive,3a free repository of CD cover art images. We\nadd cover images to the Cover art archive whenever we add\nnew CDs to MusicBrainz. Images of artists and composers\n3http://coverartarchive.orgare currently obtained from Wikipedia and other commu-\nnity websites related to the culture whose music we are\nworking with. Images of instruments used in the musi-\ncal culture are taken from Wikipedia and from repositories\ncontaining images with a freely available license such as\nFlickr.\n3.5 Audio features\nWe compute low to mid level audio features from the audio\nrecordings in our music collection and store them with the\naudio recordings. We are currently focusing on melodic\nand rhythmic dimensions of the music and so have ex-\ntracted low level audio features such as perceptual ampli-\ntude, onsets, and predominant pitch [12]. For the Indian\nmusic collections, we also extract the tonic pitch of the\nlead performing artist [13]. We use the Essentia library [1],\nan audio analysis library developed by our research group\nthat includes most of the common low and mid level fea-\nture analysis algorithms, to compute these features. We are\ncurrently performing research on various culture speciﬁc\ndescriptors that will be integrated into Essentia and used\nin Dunya as they become available. For example we are\ncurrently working on intonation analysis [6], motivic anal-\nysis [11], and r¯agacharacterisation [5]. The audio features\nare stored in the system using the YAML format.4\n4. CULTURALLY STRUCTURED DATA\nAfter we have gathered all of the raw data that is part of\na given corpus we organise it and store it in a database.\nThe information is stored using a schema that is designed\nto reﬂect the culture that we are working with. We are cur-\nrently working to develop cultural-speciﬁc schemas for all\nmusic cultures that are part of the CompMusic project. For\nthis ﬁrst version of the system, we have developed a basic\nschema for Carnatic music which we discuss in Section 5.\nIn this section we discuss the main types of structured data\nthat we create (on top of the raw data) and display in our\ncorpus browsing application.\n4.1 High level audio features\nCarnatic music is based on well-established melodic and\nrhythmic frameworks, r¯agaandt¯alarespectively. It is he-\nterophonic with the most dominant characteristics of the\nmusic arising from melody. Each melody is set in a speciﬁc\nr¯aga. The main characteristics of r¯agaare the constituent\nsvaras (musical notes), their ascending and descending pro-\ngressions, their role, and gamakas (ornamentations) sung/-\nplayed with them, and characteristic phrases [7]. Several\ndescriptors are extracted from the audio for a comprehen-\nsive description of melodies. These include tonic, F0 con-\ntours, loudness and timbre of the predominant melodic source.\nGiven a r¯aga, the intonation of each svara is a function of\nits role and the gamakas sung with it. A compact descrip-\ntion of the overall intonation of svaras is extracted using\npitch histogram of a given recording [6]. Melodic motives\n4http://www.yaml.orgare extracted to retrieve the characteristic phrases of the\nr¯aga[4, 11]. Intonation description and motives are then\nused to establish similarity between performances. These,\ncoupled with the pitch contours [1] and the tonic provide a\ngood account of r¯aga.\nWhen characterising the rhythmic structure of Carnatic\nmusic the t¯alacycle is the basis for repetitions of phrases\nand motives. Tracking the progression of these cycles is es-\nsential for a rhythmic description of a piece. Each t¯alacy-\ncle is divided into metrical positions, sometimes unequally\nspaced. We extract these metrical positions of the t¯ala,\nwhich are often called the beats of the t¯ala. Of all the po-\nsitions, the ﬁrst metrical position of the cycle, called the\nsama is the most important. The work towards the com-\nplete description of the t¯alais in progress. Though tempo\nis not clearly deﬁned in Carnatic music, we presently ex-\ntract the rhythmic density based on the onsets, which is an\nindicator of the tempo of the piece.\nThe characterisation of rhythmic and melodic phrases\nin Carnatic music and the extraction of these phrases is\nresearch in progress. In addition we are also working to-\nwards the structural segmentation of the music piece at dif-\nferent time scales such as motives of a phrase, phrases of\na piece, and pieces of a concert. These structural segmen-\ntation features provide a perspective about the structure of\nthe performance and all these melodic description compo-\nnents are the building blocks for developing melodic simi-\nlarity measures.\n4.1.1 Analysing audio\nWe have developed an audio processing framework that is\nused to compute the high level audio features from the au-\ndio recordings in our corpus. Because the project is still\nunder active development it is important to be able to re-\nanalyse the audio whenever our analysis algorithm is mod-\niﬁed, and to run the analysis on newly added audio. Our\naudio processing is performed on a cluster of servers. Each\nserver in the cluster has access to a shared NFS disk that\ncontains the entire audio corpus. A central PostgreSQL\ndatabase keeps track of which audio ﬁles have been anal-\nysed by the algorithms. We use the Celery task queue sys-\ntem5to distribute the analysis tasks to the machines in the\ncluster. When audio is added to the corpus the cluster mas-\nter directs a worker machine to analyse an album using a\nspeciﬁc algorithm. Each worker performs analysis on its\nworkload and stores the results with the audio ﬁles, where\nthey become available to our browser application. By per-\nforming the analysis using a cluster of servers we are able\nto add more servers as our corpus grows. The result of\nthe analysis is stored in a log, allowing an administrator to\nquickly check if there there was a problem with the analy-\nsis of an album.\n4.2 Structured editorial metadata\nOur system stores structured editorial metadata for all items\nin the corpus in a PostgreSQL database. This includes in-\nformation from MusicBrainz as well as information that\n5http://www.celeryproject.orgFigure 2 . The main entity types in Carnatic music and\ntheir relationships.\nwe have obtained from other online sources such as Wiki-\npedia or websites that contain information and discussions\nrelated to the style of music that we are working with.\nSome information can be represented as ontologies, ex-\nplicitly expressing the relationships between concepts used\nin that culture. We store the RDF ontologies in a graph\ndatabase allowing us to navigate through related concepts\nin our database. We initially attempted to access data on\ndemand via APIs provided by each service that provides\ndata for our corpus, however we abandoned this approach\nfor two reasons. Firstly, the time taken to perform queries\non all dependent sites in order to display the relevant in-\nformation on a page would take too long. Secondly, by\ncontrolling all of the metadata in our database we are able\nto create relationships between items in the database that\nmay not exist in the other data sources we are using. We\nkeep references to all of our external sources in order to di-\nrect users to them if they wish to ﬁnd more information. By\nstoring the date at which data was taken from an external\nsource we can periodically refresh the data by checking if\nthe source information has changed since we last accessed\nit. Because we store all of the relevant information in a\nlocal database it is also possible to take the entire collec-\ntion and access it locally in situations where an Internet\nconnection is not available.\n5. BROWSER\nThe Dunya front-end is a web-based application written\nin Python using the Django web framework,6with an in-\nteractive interface written in Javascript. The interface is\ndesigned to work on a range of computing devices.\nWe also plan to make the same data that we show in\nthe browser available in a machine-readable format. To\ndo this we will use the Music Ontology initiative [10] to\nprovide an endpoint that lets people write systems to query\nthe items in our database using publicly known identiﬁers\nsuch as MBIDs.\nFor the Carnatic music collection we focus on seven\nmain concepts that are important to the culture surround-\ning the music. The concepts are: artist, composer, work,\n6https://www.djangoproject.com\nFigure 3 . The search navigation interface showing a query\nfor items related to a speciﬁc r¯agaand two artists.\nrecording, concert, r¯aga, and t¯ala. We refer to each in-\nstance of one of these concepts as entities . For every entity\nin our database we are able to show the information that\nwe have gathered from linked sites as well as suggestions\nfor other entities that are related to the currently viewed\none. r¯aga andt¯alaare concepts that are represented as\nattributes of a work. These works are performed by musi-\ncians at concerts. Concerts are a fundamental musical unit\nin the Carnatic culture and so we use these as a type of\nentity. Almost all albums of this kind of music are record-\nings of live concerts. We have developed ontologies for the\nr¯agaand instrument entity types for Carnatic music.\n5.1 Search and discovery\nIn Dunya we provide two different ways to ﬁnd music and\nother items in the collection. The search interface allows\na user to type in the name of any entity or concept that\nwe store in the database. As the user types in the search\nbox we suggest recommendations that they might wish to\nuse. We show results of all entities that contain the text\nthat the user has entered. When searching we also con-\nsider alternative spellings of entities due to different ways\nof romanising Indian text to Latin characters. We make use\nof alternative spellings entered in MusicBrainz, as well as\na fuzzy Levenshtein distance-based measurement.\nThe second search interface is an interactive browser\n(Figure 3). For each of the core concepts (artist, composer,\nwork, concert, r¯aga, and t¯ala) we show ﬁlters that allow\nusers to ﬁnd entities that match certain criteria. For exam-\nple, an artist can be ﬁltered by a time period, the musical\nschool that they trained in, or the region that they are from.\nThe data for these ﬁlters comes from online sources such\nas biographies. A user can query with as many concepts as\nthey like. As more queries are added the number of rele-\nvant results returned is reduced.\n5.2 Entity pages\nOnce the user selects an entity in the search results we\nshow a page describing it. At the top of the page we show\neditorial and relationship data. This information comes\nfrom the information that is present in Musicbrainz, as well\nas from other linked information that we have discovered.\nFor example, the page for an artist shows basic biograph-\nical information such as their name and birth and death\ndates. The page includes biographical information if weFigure 4 . Visual display of a recording in Dunya. Suggestions for similar recordings are shown underneath the audio\nplayer.\nhave been able to source it with a link to the original en-\ntry. Where available we also show an image of the entity.\nArtist pages show an image of the person, while for con-\ncerts we show the cover art of a CD if available. We also\nshow links to other related entities. For example, the page\nfor an artist will show concerts that they have played in and\nthe instrument that they play. The page for a r¯agashows\ncomposers that often wrote compositions using that r¯aga\nand popular compositions written using it.\nOne core concept in Dunya is to be able to listen to\na relevant sound sample for each entity. In the display\nof each entity there is a button that when pressed plays\na sound fragment that has been automatically chosen for\nits relevance. From the chosen fragment a user would be\nable to access the entity of the full recording that the seg-\nment comes from. Audio recordings are added to a global\nplaylist that continues playing as the user navigates around\nthe corpus. At any point in time the user can view the\nplaylist, edit it, or go to the recording page of the currently\nplaying track.5.3 Recording entity\nOne of the primary entity interfaces that we are developing\nin Dunya is the view that represents a single audio record-\ning. Most of the technologies developed in this project re-\nsult in new methods to analyse and understand audio and\nthe recording page is where we visually show the results\nof the analysis while playing the recording (Figure 4). The\nrecording interface overlays the extracted features on top\nof a visual representation of the recording. In this version\nof the system we show two types of low-level audio fea-\ntures and three types of higher level features on the record-\ning page.\nWe show two different representations of the audio. The\nupper view shows a time-domain waveform, and the lower\none shows a spectral view. These views can be zoomed in\nto see data in more detail. For performance reasons we\npre-render the low-level visualisations at different zoom\nlevels and draw the features on top of them. Between the\ntwo views there is a full-length representation of the time-\ndomain signal. This gives an indication of the progress\nthrough the track even if the other views are zoomed in.\nOn the time-domain waveform we show extracted rhyth-mic information. This information indicates the metrical\npositions of the t¯alaof the recording. On the lower spec-\ntrogram image we show the pitch contour of the extracted\nmelody of the recording. To the left of the spectrogram is a\nhistogram of the predominant pitches in the entire record-\ning. We indicate the tonic pitch of the recording with a\nhorizontal line across this image. A playback cursor is\nshown on both of these visualisations as the recording is\nbeing played back. As the recording plays we indicate the\ncurrent predominant pitch on the histogram to the side of\nthe melody pane.\nBelow the audio player we show recordings that are\nsimilar according to the distance metrics that we have de-\nveloped. Currently we show recordings that have a similar\nt¯ala(metrical framework) and r¯aga(melodic framework).\nThe list of recordings is ordered by the similarity of the\nrecordings to the current recording. The ﬁrst recordings in\nthe list may have the same r¯agaas the current recording,\nhowever following recordings may use r¯agas that are are\nrelated based on the ontologies that we have developed.\n6. CONCLUSIONS\nWe have presented Dunya, a music discovery system be-\ning developed as part of the CompMusic project designed\nto support the exploration of a music corpus using concepts\nrelevant to the particular musical culture of the corpus.\nThe application displays the typical information shown in\nmusic playback applications, but also includes additional\ninformation collected from a variety of sources and in-\nformation that has been automatically computed from the\nrecorded audio. It stores a complex set of relationships be-\ntween the items in the database. With this information a\nuser can explore given music collection and discover mu-\nsically relevant relationships. The architecture of the sys-\ntem and the interface have been designed to support cul-\ntural speciﬁcity, starting in this paper with Carnatic mu-\nsic. In the near future we will extend the system to the\nrest of the music collections we are working with and we\nwill continue integrating research results resulting from the\nCompMusic project to enhance the discovery capabilities\nof Dunya.\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank Ajay Srinivasamurthy, Go-\npala K. Koduri, and Sankalp Gulati for contributing to the\ndescriptions of their respective research that is being used\nin the CompMusic project. Pere Esteve assisted with the\ndesign and development of the graphical interface of the\nDunya Browser. The CompMusic project is funded by the\nEuropean Research Council under the European Union’s\nSeventh Framework Program (FP7/2007-2013) / ERC grant\nagreement 267583\n8. REFERENCES\n[1] Dmitry Bogdanov, Nicolas Wack, Emilia G ´omez, Sankalp\nGulati, Perfecto Herrera, Oscar Mayor, Gerard Roma, Justin\nSalamon, Jos ´e Zapata, and Xavier Serra. Essentia: An AudioAnalysis Library For Music Information Retrieval. In Pro-\nceedings of the International Society for Music Information\nRetrieval Conference , 2013.\n[2] Chris Cannam, Christian Landone, Mark Sandler, and\nJuan Pablo Bello. The sonic visualiser: A visualisation plat-\nform for semantic descriptors from musical signals. In Pro-\nceedings of the 7th International Conference on Music Infor-\nmation Retrieval , 2006.\n[3] Masataka Goto, Kazuyoshi Yoshii, Hiromasa Fujihara,\nMatthias Mauch, and Tomoyasu Nakano. Songle: A web\nservice for active music listening improved by user con-\ntributions. In Proceedings of the 12th International Society\nfor Music Information Retrieval Conference , pages 311–316,\n2011.\n[4] Vignesh Ishwar, Ashwin Bellur, and Hema A Murthy. Mo-\ntivic analysis and its relevance to raga identiﬁcation in car-\nnatic music. In Proceedings of the 2nd CompMusic Work-\nshop , 2012.\n[5] Gopala K. Koduri, Sankalp Gulati, Preeti Rao, and Xavier\nSerra. Raga Recognition based on Pitch Distribution Meth-\nods.Journal of New Music Research , 41(4):337–350, 2012.\n[6] Gopala K. Koduri, Joan Serr `a, and Xavier Serra. Charac-\nterization of intonation in carnatic music by parametrizing\npitch histograms. In Proceedings of the International Society\nfor Music Information Retrieval Conference , pages 199–204,\n2012.\n[7] TM Krishna and Vignesh Ishwar. Carnatic music: Svara,\ngamaka, motif and raga identity. In Proceedings of the 2nd\nCompMusic Workshop , 2012.\n[8] Franc ¸ois Pachet, Jean-Julien Aucouturier, Amaury La Bur-\nthe, Aymeric Zils, and Anthony Beurive. The cuidado music\nbrowser: an end-to-end electronic music distribution system.\nMultimedia Tools and Applications , 30(3):331–349, 2006.\n[9] Elias Pampalk and Masataka Goto. Musicsun: A new ap-\nproach to artist recommendation. In Proceedings of the 8th\nInternational Conference on Music Information Retrieval ,\npages 101–4, 2007.\n[10] Yves Raimond, Samer Abdallah, Mark Sandler, and Freder-\nick Giasson. The music ontology. In Proceedings of the 8th\nInternational Conference on Music Information Retrieval ,\npages 417–422, 2007.\n[11] Joe Cheri Ross, TP Vinutha, and Preeti Rao. Detecting\nmelodic motifs from audio for hindustani classical music. In\nProceedings of the International Society for Music Informa-\ntion Retrieval Conference , 2012.\n[12] Justin Salamon and Emilia G ´omez. Melody extraction from\npolyphonic music signals using pitch contour characteristics.\nIEEE Transactions on Audio, Speech and Language Process-\ning, 20:1759–1770, 2012.\n[13] Justin Salamon, Sankalp Gulati, and Xavier Serra. A multip-\nitch approach to tonic identiﬁcation in indian classical music.\nInProceedings of the International Society for Music Infor-\nmation Retrieval Conference , 2012.\n[14] Xavier Serra. A Multicultural Approach to Music Informa-\ntion Research. In Proceedings of the 12th International Soci-\nety for Music Information Retrieval Conference , 2011.\n[15] George Tzanetakis. Musescape: An interactive content-aware\nmusic browser. In Proceedings of the 6th Conference on Dig-\nital Audio Effects (DAFX) , 2003."
    },
    {
        "title": "Freischütz Digital: A Case Study for Reference-Based Audio Segmentation for Operas.",
        "author": [
            "Thomas Prätzlich",
            "Meinard Müller"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416814",
        "url": "https://doi.org/10.5281/zenodo.1416814",
        "ee": "https://zenodo.org/records/1416814/files/PratzlichM13.pdf",
        "abstract": "Music information retrieval has started to become more and more important in the humanities by providing tools for computer-assisted processing and analysis of music data. However, when applied to real-world scenarios, even established techniques, which are often developed and tested under lab conditions, reach their limits. In this paper, we illustrate some of these challenges by presenting a study on automated audio segmentation in the context of the interdisciplinary project “Freisch¨utz Digital”. One basic task arising in this project is to automatically segment different recordings of the opera “Der Freisch¨utz” according to a reference segmentation specified by a domain expert (musicologist). As it turns out, the task is more complex as one may think at first glance due to significant acoustic and structural variations across the various recordings. As our main contribution, we reveal and discuss these variations by systematically adapting segmentation procedures based on synchronization and matching techniques.",
        "zenodo_id": 1416814,
        "dblp_key": "conf/ismir/PratzlichM13",
        "keywords": [
            "Music information retrieval",
            "Computer-assisted processing",
            "Analysis of music data",
            "Real-world scenarios",
            "Established techniques",
            "Limits of techniques",
            "Audio segmentation",
            "Interdisciplinary project",
            "Freischütz Digital",
            "Automated audio segmentation"
        ],
        "content": "FREISCH ¨UTZ DIGITAL: A CASE STUDY FOR REFERENCE-BASED\nAUDIO SEGMENTATION OF OPERAS\nThomas Pr ¨atzlich\nInternational Audio Laboratories Erlangen\nthomas.praetzlich@audiolabs-erlangen.deMeinard M ¨uller\nInternational Audio Laboratories Erlangen\nmeinard.mueller@audiolabs-erlangen.de\nABSTRACT\nMusic information retrieval has started to become more\nand more important in the humanities by providing tools\nfor computer-assisted processing and analysis of music\ndata. However, when applied to real-world scenarios,\neven established techniques, which are often developed\nand tested under lab conditions, reach their limits. In this\npaper, we illustrate some of these challenges by presenting\na study on automated audio segmentation in the context\nof the interdisciplinary project “Freisch ¨utz Digital”. One\nbasic task arising in this project is to automatically seg-\nment different recordings of the opera “Der Freisch ¨utz”\naccording to a reference segmentation speciﬁed by a do-\nmain expert (musicologist). As it turns out, the task is more\ncomplex as one may think at ﬁrst glance due to signiﬁcant\nacoustic and structural variations across the various record-\nings. As our main contribution, we reveal and discuss these\nvariations by systematically adapting segmentation proce-\ndures based on synchronization and matching techniques.\n1. INTRODUCTION\nIn recent years, the availability of digital music material\nhas increased drastically including data of various formats\nand modalities such as textual, symbolic, acoustic and vi-\nsual representations. In the case of an opera there typically\nexist digitized versions of the libretto, different editions\nof the musical score, as well as a large number of perfor-\nmances given as audio and video recordings, which in its\ntotality constitute the body of sources of a musical work.\nThe goal of the ongoing project “Freisch ¨utz Digital”1is to\ndevelop and apply automated methods to support musicol-\nogists in editing, analyzing and comparing the various mu-\nsical sources. The opera “Der Freisch ¨utz” by Carl Maria\nvon Weber is a work of central musical importance offer-\ning a rich body of sources. Working out and understanding\nthe variations and inconsistencies within and across the dif-\nferent sources constitutes a major challenge tackled in this\nproject. Another more general objective is to apply and\n1http://freischuetz-digital.de/\nPermission to mak\ne digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieval.to adjust computer-based methods to real-world scenarios\nand to the needs of domain experts.\nOne particular problem arising in this case study con-\ncerns the automated segmentation of all available audio\nrecordings of the opera. The opera “Der Freisch ¨utz” is\nanumber opera in the style of a Singspiel, starting with\nan overture followed by 16numbers (arias, duets, trios,\ninstrumental pieces, etc.) which are interspersed by spo-\nken text (dialogues). In our scenario, the musicologists are\ninterested in a speciﬁc segmentation of the opera, which\nwe refer to as the reference segmentation. The audio seg-\nmentation task is aimed at automatically transferring this\nreference segmentation onto all available recordings of the\nopera, see Figure 1 for illustration.\nA related scenario is described in [6], where the goal is\nto identify unknown audio recordings. By applying auto-\nmated matching procedures, the unknown recordings are\ncompared to well-annotated audio material in a database.\nUpon identiﬁcation, the matching result also allows for\nsegmenting the unknown recording. However, this seg-\nmentation is more a byproduct, which is not evaluated in\ndetail. In our scenario, the focus lies on the segmentation\nand, in a certain sense, we follow a reversed approach as\nwe start from known material that we match to a database\nwhich we assume to contain representatives of the same\nmusical work.\nThe contributions of this paper are twofold. First, we\napply and adjust existing synchronization and matching\nprocedures to realize an automated reference-based seg-\nmentation procedure. The second and even more important\ngoal of this paper is to highlight the various challenges aris-\ning in the context of this seemingly easy segmentation sce-\nnario. In fact, the various audio recordings reveal signif-\nicant acoustic and structural deviations. Considering dig-\nitized material from old sound carriers (shellac, LP, tape\nrecordings etc.), one often has to deal with artifacts. Struc-\nturally, there are omissions or changes of numbers, rep-\netitions, verses and dialogues. By systematically adjust-\ning the segmentation procedure to reveal these variations,\nwe not only successively improve the segmentation qual-\nity, but also gain insights into and a better understanding\nof the audio material.\nThe remainder of this paper is organized as follows. In\nSection 2, we describe the various types of sources that nat-\nurally exist in the opera scenario and describe the dataset\nin more detail. In Section 3, we review some basic music\nsynchronization and audio matching procedures. Then, in1M4M1D 6D18M 27M 16D 38MWei2001Saw1972Pen1998Orl1946Mue1950Mat1967Leo1972Kub1979Kle1955Kle1973Kei1958Joc1960Jan1994Heg1969Hau1985Har1995Gui1957Fur1954Elm1944Dav1990Bru1957Boe1972Ack1951\nFigure 1. Segmentation result for 23dif ferent audio recordings of “Der\nFreisch ¨utz” according to a reference segmentation speciﬁed by musicol-\nogists. The reference segmentation includes 38musical sections (Over-\nture: yellow, Act I: green, Act II: red, Act III: blue) as well as 16spoken\ndialogue sections (gray).\nSection 4, we introduce various segmentation procedures\nand present a musically informed evaluation of the various\nresults. In Section 5, we conclude the paper and give an\noutlook to future work. Related work is discussed in the\nrespective sections.\n2. MUSICAL BACKGROUND\nMusic in itself is complex and manifested in many differ-\nent formats and modalities [5, 9]. For example, for “Der\nFreisch ¨utz” by Carl-Maria von Weber, there are textual\nrepresentations in form of the libretto (text of the opera),\nsymbolic representations (musical score), acoustic repre-\nsentations (audio recordings) and visual representations\n(video recordings). In the following, “Der Freisch ¨utz” –\nan important representative of the German romantic opera\n[11] – serves as a challenging case study. The opera is\nstructured in three acts which are further subdivided into\nan overture and 16 following numbers interspersed by spo-\nken text passages (dialogues). The numbers cover a wide\nrange of musical material (arias, duets, trios, instrumental\npieces, etc.). Some of the melodic and harmonic mate-\nrial of the numbers is already introduced in the overture.\nAlso, some of the numbers contain repetitions of musical\nparts or verses of songs. In the acoustic domain, these are\nnot always part of the performance, as a the conductor or\nproducer may take the artistic freedom to deviate substan-\ntially from what is speciﬁed in the musical score. Besides\ndifferences in the number of played repetitions, further de-\nviations include omissions of other parts or entire numbers\nas well as variations in the spoken text and the length of\nthe dialogues. Apart from such structural deviations, au-\ndio recordings of the opera usually differ in overall length,\nsound quality, language and many other aspects. For exam-\nple, our dataset includes historic recordings that are often\nprone to noise, artifacts, or tuning problems resulting from\nthe digitization process. Furthermore, the recordings show\na high variability in their duration, which can be explained\nby signiﬁcant tempo differences and also by omissions of\nmaterial, see Table 1 and Table 2 for details. Also, there\nare versions which were adapted into French, Italian and\nRussian language.\nOur raw audio data mostly originates from CD record-\nQueriesUnannotated Versions\nSegmentation ResultAnnotated Reference Version\nMatchingD1 M1 M2\nFigure 2. Illustration of the reference-based segmentation procedure.\nings, which were initially segmented in CD tracks, see\nTable 1. These track segmentations are not consistent,\nvarying between 17 and 41 tracks per recording. In some\nrecordings, each number of the opera was put into a sep-\narate track, whereas in others the numbers were divided\ninto music and dialogue tracks, and sometimes the remain-\ning music tracks were even subdivided. In order to com-\npare semantically corresponding parts in different versions\nof the opera, a consistent segmentation is needed. In the\ncontext of the “Freisch ¨utz Digital” project, such a segmen-\ntation is a fundamental requirement for further analysis and\nprocessing steps such as the computation of linking struc-\ntures across different musical sources, including sheet mu-\nsic and audio material.\nIn our scenario, a reference segmentation of the musical\nscore into musically meaningful sections was speciﬁed by\na domain expert (musicologist), who divided the opera into\n38musical segments and 16dialogue segments. Accord-\ning to this reference segmentation, we manually created\nan annotation for each of the 23audio recordings in our\ndatabase, resulting in over 1000 audio segment, see Fig-\nure 1 for an overview. The objective of this paper is to re-\ncover this annotation using automated methods and to get\na better understanding of the variations and inconsistencies\nin the audio material.\n3. SYNCHRONIZATION AND MATCHING\nTECHNIQUES\nAs discussed before, the basic task is to segment an un-\nknown audio recording (assuming no pre-segmentation)\naccording to a given reference segmentation. In the follow-\ning, we assume that this reference segmentation is spec-\niﬁed on the basis of a reference audio recording. Then\nthe objective of the segmentation task is to transfer the\nsegmentation from the reference version to the unknown\nrecording. In this section, we introduce some mathemat-\nical notions to model our segmentation problem and then\nreview some standard audio synchronization and matching\ntechniques that are applied in the subsequent section.\nLetX:= (x1,x2,...,x N)be a suitable feature rep-\nresentation of a given audio recording (the feature type is\nspeciﬁed later). Then, a segmentαis a subset α= [s:t]⊆[1:N] :={1,2 ....,N}withs≤t. Let|α|:=t−s+1\ndenote the length of α. Furthermore, we deﬁne a (partial)\nsegmentation ofXto be a sequence (α1,...,α I)of pair-\nwise disjoint segments, i. e. αi∩αj=∅fori,j∈[1 :I],\ni/ne}ationslash=j. Note that in this deﬁnition we do not assume that\n[1 :N]is completely covered by the segmentation.\nIn our scenario we assume that we have a refer-\nence sequence Xwith a reference segmentation A=\n(α1,...,α I). Furthermore, let Y:= (y1,y2,...,y M)\nbe a feature representation of an unknown audio record-\ning. In the case that XandYare structurally similar on\na global scale, the transfer of the reference segmentation\nofXontoYcan be done by using standard synchroniza-\ntion or alignment techniques [1,3,7]. Here, music synchro-\nnization denotes a procedure which, for a given position in\none representation of a piece of music, determines the cor-\nresponding position within another representation. When\nsynchronizing two audio recordings, the ﬁrst step consists\nin transforming the recordings into feature representations,\ntypically chroma-based audio features.2Based on these\nfeature representations and a suitable cost measure, one ap-\nplies dynamic time warping (DTW) to compute a cost min-\nimizing warping path which realizes the linking between\nXandY, see [7, Chapter 4].\nThis synchronization-based transfer works as long as\nXandYglobally coincide. However, problems arise\nin the presence of signiﬁcant structural differences. Fur-\nthermore, in case XandYare long (as is the case for\ncomplete recordings of entire operas), running time and\nmemory issues arise when performing DTW. Even though\n(multiscale, forward estimation) acceleration techniques\nexist [1,10], such techniques are not suited when structural\ndifferences occur. As an alternative, one may apply more\nlocally oriented audio matching techniques, where the in-\ndividual segments αiof the reference segmentation (used\nas “queries”) are matched to subsegments of the unknown\nsequence Y(resulting in “matches” or “hits”), see [4]. In\nother words, the cost-intensive global DTW alignment is\nreplaced by several smaller local alignments (realized by a\nsubsequence variant of DTW), see also Figure 2 for illus-\ntration. Another positive effect is that using local matches\nallows for a better handling of missing segments and struc-\ntural differences. On the downside, by querying the ref-\nerence segments individually, one may loose temporal co-\nherence, while the chance of obtaining local mismatches is\nincreased (in particular for short segments).\nIn the subsequent section, we systematically apply,\nmodify and combine both techniques – global synchroniza-\ntion and local matching – for performing our segmentation\ntask. Here, besides the actual segmentation, our main goal\nis to obtain a better understanding of various kinds of vari-\nations and inconsistencies in the audio material.\n4. AUDIO SEGMENTATION\nIn this section, after introducing our evaluation measure to\nassess the accuracy of segmentation results (Section 4.1),\n2In our e xperiments,we use chroma-based CENS features of 2 Hz\nresolution as supplied by the chroma toolbox [8].we discuss various strategies to tackle the segmentation\ntask based on global synchronization (Section 4.2) and lo-\ncal matching procedures (Section 4.3 – 4.6). Furthermore,\nwe discuss the beneﬁts and limitations of the respective\nprocedures while revealing the musical and acoustic varia-\ntions and inconsistencies in the audio material.\n4.1 Evaluation Measure\nFirst of all, we need a measure that allows us to compare\ntwo given segments αandβ. To this end, we deﬁne the\nrelative overlap measure betweenαandβto be the value\nµ(α,β) :=|α∩β|\n|α∪β|∈[0,1],\nwhich indicates\nthe ratio of the absolute overlap and the\nlength of the union segment. Note that µ(α,β) = 1 if and\nonly ifα=β, andµ(α,β) = 0 ifα∩β=∅.\nAs before, let us assume that the reference version is\nrepresented by the sequence X:= (x1,x2,...,x N)and\nthereference segmentation byA:= (α1,...,α I). Fur-\nthermore, let Y:= (y1,y2,...,y M)be the unknown ver-\nsion to be segmented. For the purpose of evaluation,\nwe assume that there is also a ground truth segmentation\nB:= (β1,...,β I)forY, where each βimusically cor-\nresponds to the αi. The goal is to automatically derive\nthe segmentation of Y. LetPdenote such a segmenta-\ntion procedure, which automatically transfers each refer-\nence segment αito a computed segment P(αi)⊆[1 :M].\nThen, the relative overlap measure µ(βi,P(αi))indicates\nthe segmentation quality of the procedure P.\nBecause of the mentioned structural variations, the ver-\nsionYdoes not necessarily contain a segment that musi-\ncally corresponds to a reference segment αi. In this case,\nthe ground truth segment is set to βi=∅. Furthermore,\nthe procedure Pdoes not have to output a computed seg-\nment, which is modeled by setting P(αi) =∅. In the case\nthat both the segment P(αi)andβiare empty, we deﬁne\nµ(βi,P(αi)) = 1 (a non-existing segment has been iden-\ntiﬁed as such). Note that if only one of the segments is\nempty,µ(βi,P(αi)) = 0.\n4.2 Global Approach (S1, S2)\nIn the following matching procedures and evaluation,\nwe only consider the musical sections (indicated by the\nnon-gray segments in Figure 1) while leaving the dia-\nlogue sections (the gray segments in Figure 1) uncon-\nsidered. Exemplarily, we use a reference segmentation\nA= (α1,α2,...,α 38)based on the recording conducted\nby Carlos Kleiber in 1973 (Kle1973), which is a perfor-\nmance that closely follows the musical score. Quantitative\nresults for all procedures to be discussed are presented in\nTable 1 (relative overlap averaged over versions) and Ta-\nble 2 (relative overlap averaged over segments).\nIn the two procedures S1andS2, we apply a global\nsynchronization approach. For S1, we employ DTW us-\ning the step size condition Σ1={(1,1),(1,2),(2,1)},\nsee [7, Chapter 4]. This strategy is usually very robust\nas long as there are no signiﬁcant deviations in structureand tempo between the two versions compared. How-\never,\nthe procedure S1is not able to compensate well for\nstructural variations leading to an average relative overlap\nof0.852, see Table 1 When using the step size condition\nΣ2={(1,1),(1,0),(0,1)}(calling this procedure S2),\nperformance improves signiﬁcantly, yielding the average\nrelative overlap of 0.930, see Table 1. For example, in the\nversionSaw1972, the dialogues are comparatively short,\nsee also the gray rectangles in Figure 1. Such a situation\ncausesS1to fail, resulting in an overlap of 0.615 com-\npared to0.896forS2, see Table 1. For both procedures,\nthe alignment accuracy for α38is very low with 0.714(S1)\nand0.724(S2), see Table 2. This is due to audio material\nnot belonging to the actual opera that is appended at the\nend (CD bonus tracks) in some versions. In this case, the\nglobal synchronization procedures do not allow to skip the\nﬁnal tracks. Despite the promising results of S2, this ap-\nproach has several limitations. First, it is inefﬁcient consid-\nering runtime and memory requirements, especially when\nincreasing the feature resolution, see also Section 3. Sec-\nondly, it is not well suited to accommodate for structural\nchanges in a controlled manner. And thirdly, the procedure\ndoes not give deeper insights into the musical and acoustic\nproperties of the underlying audio material.\nOur goal in the following sections is to develop a more\nﬂexible segmentation strategy that achieves a quality com-\nparable to S2while yielding better insight into the ver-\nsions’ properties.\n4.3 Local Approach (M1)\nThe remaining approaches discussed below rely on a lo-\ncal matching procedure based on a subsequence variant\nof DTW using the step size condition Σ1. Here, for each\nαi∈ A (used as a query) applied to a given unknown ver-\nsion, we compute a ranked list of matching candidates. For\nthe segmentation procedure M1, we only consider the top\nmatch in the list, see also Figure 2 for illustration of the\ngeneral matching strategy.\nIn Figure 3a, the relative overlap values for M1com-\nputed on all recordings in our dataset are presented in a\ngray-scale matrix visualization, where the rows indicate\nthe audio versions and the columns indicate the segments.\nBlack corresponds to µ= 0 (no overlap) and white to\nµ= 1 (perfect overlap). Row-wise, the segmentation\naccuracy of a speciﬁc version becomes obvious, whereas\ncolumn-wise, segments which are problematic across ver-\nsions can easily be spotted. An example for a problematic\nversion isElm1944, which generally seems to perform\npoorly, showing many black entries in Figure 3a and hav-\ning a low average relative overlap of 0.705, see Table 1.\nA closer look at the audio material revealed that there are\nsome issues concerning the tuning of this version, probably\nresulting from the digitization process. Furthermore, there\nare segments which show a poor segmentation accuracy\nacross versions, see for example the black entries for α14to\nα16in Figure 3a. It turns out that these three segments cor-\nrespond to the three verses of a song (No. 4) in the opera.\nThe reason why this song has been divided into individualsegments is that there are dialogues between the verses (re-\ncall that a requirement of the reference segmentation was\nto separate music and dialogue sections). The verses all\nshare the same melodic and harmonic material and are thus\neasily confused with each other in the matching procedure.\nAnother interesting problem appears for α32, whereM1\nnearly fails for every version, resulting in an overall seg-\nmentation accuracy of 0.157, see Table 2 and Figure 3a.\nActually, α32(having a duration of only 12.4seconds) is\na short snippet of a chorus section for which many repeti-\ntions exist in the surrounding segments α31(song with sev-\neral verses and chorus sections) and α33(chorus) which are\ninterspersed by dialogues. Thus it is very likely that α32is\nmatched into the harmonically similar parts within α31or\nα33. For the version Kle1955, segment α38seems to be\nproblematic, see Figure 3a. Actually, α38contains musical\nmaterial which is already used in the overture of the opera\n(covered by α3). A closer look into the matching results\nforKle1955 revealed that α38matched indeed into the\nmusically very similar section in the overture.\nIn conclusion, procedure M1is more efﬁcient3, see\nalso Section 3, while its main drawback is the loss of ro-\nbustness due to confusion of local matches.\n4.4 Tuning Issues (M2)\nIn real world scenarios, the tuning of a music performance\noften slightly deviates from the standard tuning, where a\nchamber tone of 440 Hz serves as reference frequency.\nThis usually inﬂuences pitch related audio features such as\nchroma features. To compensate for different tunings, one\ntypically integrates a tuning estimation procedure in the\nfeature extraction process [2]. In the previous approaches,\nwe already used tuned chroma features. But since an un-\nkown version of the opera also contains a lot of non-music\nmaterial (dialogues, applause, etc.), which is also consid-\nered in the tuning estimation, the resulting estimate may be\nincorrect.\nWith procedure M2, we evaluate the inﬂuence of the\ntuning estimation on the matching procedure. This prob-\nlem can either be addressed on the side of the unknown\nversion or on the query side. In our approach, we use\nthe same chroma sequence for the unknown version as in\nM1, and simulate the tuning deviations on the query side\nby computing the chroma sequence for the query with re-\nspect to six different reference frequencies (in the range of\na semitone). Doing this for each query αi, we then use the\nchroma sequence yielding the minimum cost in the match-\ning.\nForElm1944, the local tuning adjustment indeed leads\nto a substantial improvement from 0.705 (M1) to0.777\n(M2), see Table 1. Also, there are improvements for cer-\ntain segments, e.g., α38with0.921 (M1) compared to\n0.968(M2), see Table 2. In this example, the improvement\n3On a 64bit machine,the average memory requirement for a global\nDTW run on one piece of our dataset is 1.7 GB (2 Hz feature resolu-\ntion) and 42.6 GB (10 Hz ), computed from the length of the reference\nversion and the average version length. Upper bounds for the local match-\ning approaches (derived from the maximum query length and the average\nversion length) are 114 MB (2 Hz) and 2.9 GB (10 Hz).Version #O dur. S1 S2 M1 M2 M3 M4\nAck1951 19 6904.81 0.596 0.811 0.808 0.851 0.850 0.853\nBoe1972 30 7771.77 0.784 0.931 0.889 0.865 0.962 0.962\nBru1957 24 7439.33 0.906 0.933 0.927 0.905 0.923 0.966\nDav1990 30 8197.88 0.972 0.984 0.926 0.926 0.950 0.961\nElm1944 19 7081.52 0.698 0.827 0.705 0.777 0.806 0.865\nFur1954 34 9121.69 0.923 0.936 0.866 0.861 0.938 0.949\nGui1957 18 6911.30 0.908 0.937 0.801 0.851 0.860 0.886\nHar1995 17 8044.99 0.974 0.981 0.944 0.943 0.965 0.973\nHau1985 17 8245.23 0.955 0.957 0.935 0.933 0.932 0.943\nHeg1969 25 7436.75 0.896 0.958 0.913 0.895 0.943 0.946\nJan1994 30 7843.21 0.881 0.987 0.916 0.917 0.964 0.976\nJoc1960 26 7178.21 0.922 0.948 0.887 0.911 0.968 0.967\nKei1958 32 8043.00 0.886 0.965 0.904 0.902 0.976 0.975\nKle1973 29 7763.00 1.000 0.996 0.989 0.990 0.990 0.990\nKle1955 41 7459.35 0.776 0.873 0.849 0.876 0.980 0.980\nKub1979 23 8044.65 0.959 0.985 0.927 0.929 0.953 0.974\nLeo1972 19 7726.17 0.861 0.926 0.905 0.900 0.875 0.896\nMat1967 17 8309.35 0.984 0.983 0.874 0.876 0.948 0.965\nMue1950 35 7559.97 0.814 0.881 0.825 0.824 0.885 0.895\nOrl1946 32 7368.58 0.559 0.807 0.853 0.854 0.852 0.883\nPen1998 26 7768.00 0.866 0.904 0.890 0.891 0.968 0.977\nSaw1972 29 6871.02 0.615 0.896 0.893 0.894 0.968 0.974\nWei2001 38 7220.13 0.859 0.974 0.915 0.916 0.965 0.975\n∅ 26 7665.65 0.852 0.930 0.884 0.891 0.931 0.945\nTable 1. Relative o verlap values averaged over segments for different\nversions and different procedures. The ﬁrst column indicates the version,\nthe second (#O) the number of segments on the original sound carrier,\nand the third column (dur. ) the overall duration in seconds of the record-\ning.S1,S2,M1,M2 M3 , andM4denote the respective segmentation\nprocedures.\n  \n5 10 15 20 25 30 3500.10.20.30.40.50.60.70.80.91\nWei2001Saw1972Pen1998Orl1946Mue1950Mat1967Leo1972Kub1979Kle1955Kle1973Kei1958Joc1960Jan1994Heg1969Hau1985Har1995Gui1957Fur1954Elm1944Dav1990Bru1957Boe1972Ack1951\n  \n5 10 15 20 25 30 3500.10.20.30.40.50.60.70.80.91\nWei2001Saw1972Pen1998Orl1946Mue1950Mat1967Leo1972Kub1979Kle1955Kle1973Kei1958Joc1960Jan1994Heg1969Hau1985Har1995Gui1957Fur1954Elm1944Dav1990Bru1957Boe1972Ack1951(a)\n(b)\nFigure 3. Matrix visualization of relati ve overlap values, where the ver-\nsions correspond to rows and the segments to columns. (a):P = M1.\n(b):P = M4.\nmainly comes from the version Kle1955, where α38is\nnow matched onto the correct position.\n4.5 Global Constraints (M3)\nAs mentioned in Section 4.3, the local matching procedure\ncan easily confuse musically similar parts. Also, the com-\nputed segments obtained by individual matches may not be\ndisjoint. In the procedure M3, we impose additional global\nconstraints on the overall segmentation to cope with these\ntwo problems.αiNo.occ. dur. S1 S2 M1 M2 M3 M4\n1 0 23 216.5 0.995 0.994 0.968 0.975 0.975 0.975\n2 0 23 283.3 0.996 0.995 0.977 0.976 0.976 0.976\n3 0 23 081.4 0.962 0.972 0.881 0.918 0.918 0.918\n4 1 23 069.9 0.888 0.927 0.900 0.937 0.937 0.937\n5 1 22 070.9 0.808 0.938 0.753 0.747 0.747 0.930\n6 1 23 138.4 0.826 0.986 0.969 0.970 0.952 0.952\n7 2 23 122.9 0.854 0.983 0.930 0.932 0.932 0.932\n8 2 23 152.4 0.959 0.992 0.977 0.977 0.977 0.977\n9 2 23 139.8 0.987 0.987 0.986 0.988 0.970 0.977\n10 3 22 073.1 0.930 0.945 0.775 0.772 0.772 0.842\n11 3 23 230.3 0.989 0.992 0.985 0.985 0.985 0.985\n12 3 23 074.6 0.990 0.993 0.964 0.967 0.967 0.967\n13 3 23 092.1 0.939 0.982 0.979 0.976 0.976 0.976\n14 4 23 034.6 0.749 0.876 0.617 0.735 0.904 0.904\n15 4 23 029.3 0.635 0.798 0.496 0.524 0.838 0.838\n16 4 20 026.4 0.550 0.692 0.519 0.479 0.789 0.789\n17 5 23 186.0 0.979 0.985 0.930 0.930 0.930 0.930\n18 6 23 287.8 0.984 0.994 0.987 0.989 0.989 0.989\n19 7 23 223.9 0.963 0.972 0.992 0.992 0.992 0.992\n20 8 23 499.4 0.989 0.997 0.995 0.994 0.994 0.994\n21 9 23 258.6 0.979 0.992 0.945 0.988 0.988 0.988\n22 9 23 137.6 0.971 0.978 0.985 0.980 0.980 0.980\n23 10 22 337.3 0.944 0.951 0.944 0.943 0.987 0.987\n24 10 23 301.9 0.977 0.986 0.989 0.988 0.981 0.981\n25 10 23 243.8 0.910 0.986 0.933 0.932 0.924 0.924\n26 10 23 059.7 0.740 0.889 0.908 0.847 0.883 0.883\n27 11 19 104.5 0.631 0.725 0.807 0.807 0.938 0.938\n28 12 23 356.9 0.882 0.988 0.982 0.982 0.982 0.982\n29 13 22 161.5 0.794 0.940 0.943 0.943 0.986 0.986\n30 13 22 208.8 0.814 0.951 0.945 0.944 0.984 0.987\n31 14 23 168.4 0.729 0.923 0.796 0.790 0.796 0.917\n32 14 19 012.4 0.439 0.643 0.157 0.198 0.698 0.735\n33 14 22 057.7 0.714 0.846 0.864 0.869 0.827 0.913\n34 15 23 147.2 0.745 0.946 0.938 0.937 0.980 0.980\n35 16 23 303.6 0.827 0.996 0.990 0.989 0.989 0.989\n36 16 23 503.2 0.812 0.965 0.994 0.994 0.994 0.994\n37 16 23 241.2 0.781 0.894 0.987 0.987 0.987 0.987\n38 16 23 068.2 0.714 0.724 0.921 0.968 0.968 0.968\n∅ 22.5 176.47 0.852 0.930 0.884 0.891 0.931 0.945\nTable 2. Relative o verlap values averaged over versions for different\nsegments and different procedures. The ﬁrst column (α i) indicates the\nreference segment, the second column (No.) the musical number within\nthe opera, the third column (occ.) the number of occurrences of αiin\nthe 23 versions of the dataset, and the fourth column (dur.) refers to the\nduration in seconds of αi.S1,S2,M1,M2 M3, and M4denote the\nrespective segmentation procedures.\nWhen using αias query, we now consider the entire\nranked list of matches (instead of only using the top match\nas inM1andM2). From each list we choose the best can-\ndidate so that the following global constraints are satisﬁed:\ni)Disjointness condition: P(αi)∩P(αj) =∅\nii)Temporal monotonicity: αi≺αj⇒P(αi)≺P(αj).\nHere, we deﬁne the partial order ≺on the set of segments\nbyα1= [s1:t1]≺α2= [s2:t2] :⇔t1< s2. An op-\ntimal selection of matches from the ranked lists satisfying\nthese global constraints can be computed using dynamic\nprogramming (similar to DTW). Howevever, note that in\nthis case the dynamic programming is performed on the\ncoarse segment level and not on the much ﬁner frame level\nas in the case of global synchronization.\nApplying this strategy does indeed improve the overall\nmatching accuracy, on a version level as well as for indi-\nvidual segments, see Table 1 and Table 2. For example,\nfor the segments α14/α15/α16, the results improve from\n0.735/0.524/0.479 forM2to0.904/0.838/0.789 forM3.\nAlso, the results for α32improve from 0.198(M2) to0.698\n(M3).\nAnother interesting example is the relative overlap of\n0.938forα27. This segment is actually missing in fourrecordings of the opera. Using global constraints, the\nnonexistence of these\nsegments was correctly identiﬁed by\nprocedure P = M3 resulting in P(α27) =∅. However, the\ncorresponding segment in Leo1972 was misclassiﬁed as\nnonexistent by M3. A closer inspection revealed that the\nassumption modeled in the constraint that segments always\nappear in the same order as in the reference version was\nviolated in this audio version. Here, the musical section\ncovered by α27was placed after α30and used as an intro-\nduction before α31. Thus, although strategy M3stabilizes\nthe overall matching, ﬂexibility concerning the temporal\norder of segments is lost.\n4.6 Structural Issues (M4)\nAnother problem occurs for the segments α5,α10and\nα31, having the relative overlap values of 0.747,0.772,\nand0.796forM3, respectively. According to the musical\nscore, all these sections include repetitions of some music\nmaterial. The segment α5for example should, according\nto the musical score, follow the structure IA1A2B1B2O,\nwhereIis an introductory and Oan “outro” part. How-\never, not all the repetitions are always performed. For ex-\nample, the alternative structures IA1B1O,IA1A2B1O, or\nIA1B1B2Oforα5all appear in recordings of our dataset\n(similar variations occur for α10andα31). Such structural\ndeviations can generally not be compensated well in the\nlocal matching procedure. Also, for further processing and\nanalysis steps, such as the synchronization between corre-\nsponding segments in different recordings, it is important\nto know the exact structure of a given segment.\nForM4, we investigate how structural correspondence\nof the query with an unknown version inﬂuences the seg-\nmentation quality. We manually annotated the musical\nstructures occurring for α5,α10andα31in the different\naudio versions of the opera. This information is then used\nin the matching to generate a query which structurally cor-\nresponds to the unknown version. The actual matching al-\ngorithm is the same as in M3. From the quantitative results\nin Table 2, we can conclude that the structural variations\nwere indeed the cause of the poor performance for these\nsegments: α5improves from 0.747(M3) to0.930(M4),\nα10from0.772(M3) to0.842(M4) and α31from0.796\n(M3) to0.917(M4), see also Figure 3b.\n5. CONCLUSIONS\nIn this paper, we presented a case study on segmenting\ngiven audio versions of an opera into musically mean-\ningful sections that have been speciﬁed by a domain ex-\npert. Adapting existing synchronization and matching\ntechniques, we discussed various challenges that occur\nwhen dealing with real-world scenarios due to the variabil-\nity of acoustic and musical aspects. Rather than presenting\ntechnical details, our main motivation was to show how au-\ntomated methods may be useful for systematically reveal-\ning and understanding the inconsistencies and variations\nhidden in the audio material. Furthermore, we showed\nhow a procedure based on a combination of local match-ing and global constraints yields a more ﬂexible and efﬁ-\ncient alternative to a global black-box synchronization ap-\nproach. Besides yielding slightly better results, this alter-\nnative procedure also provides a more explicit control to\nhandle the various musical aspects and yields deeper in-\nsights into the properties of the audio material. For the\nfuture, we plan to expand our segmentation approach by\nexplicitly including the dialogue sections into the analysis.\nFurthermore, the segmentation results will serve as basis\nfor a ﬁner grained analysis and multimodal processing in-\ncluding informed source separation.\nAcknowledgments: This work has been supported by\nthe BMBF project Freisch ¨utz Digital (Funding Code\n01UG1239A to C). The International Audio Laboratories\nErlangen are a joint institution of the Friedrich-Alexander-\nUniversit ¨at Erlangen-N ¨urnberg (FAU) and Fraunhofer IIS.\n6. REFERENCES\n[1] Simon Dixon and Gerhard Widmer. MATCH: A music align-\nment tool chest. In Proceedings of the International Confer-\nence on Music Information Retrieval (ISMIR), London, GB,\n2005.\n[2] Emilia G ´omez. Tonal Description of Music Audio Signals.\nPhD thesis, UPF Barcelona, 2006.\n[3] Ning Hu, Roger B. Dannenberg, and George Tzanetakis.\nPolyphonic audio matching and alignment for music retrieval.\nInProceedings of the IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics (WASPAA), New Paltz,\nNY , USA, 2003.\n[4] Frank Kurth and Meinard M ¨uller. Efﬁcient index-based au-\ndio matching. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, 16(2):382–395, 2008.\n[5] Cynthia C. S. Liem, Meinard M ¨uller, Douglas Eck, George\nTzanetakis, and Alan Hanjalic. The need for music informa-\ntion retrieval with user-centered and multimodal strategies.\nInProceedings of the International ACM Workshop on Mu-\nsic Information Retrieval with User-centered and Multimodal\nStrategies (MIRUM), pages 1–6, 2011.\n[6] Nicola Montecchio, Emanuele Di Buccio, and Nicola Orio.\nAn efﬁcient identiﬁcation methodology for improved ac-\ncess to music heritage collections. Journal of Multimedia,\n7(2):145–158, 2012.\n[7] Meinard M ¨uller. Information Retrieval for Music and Motion.\nSpringer Verlag, 2007.\n[8] Meinard M ¨uller and Sebastian Ewert. Chroma Tool-\nbox: MATLAB implementations for extracting variants of\nchroma-based audio features. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Conference\n(ISMIR), pages 215–220, Miami, FL, USA, 2011.\n[9] Meinard M ¨uller, Masataka Goto, and Markus Schedl, edi-\ntors. Multimodal Music Processing, volume 3 of Dagstuhl\nFollow-Ups. Schloss Dagstuhl - Leibniz-Zentrum f ¨ur Infor-\nmatik, Germany, 2012.\n[10] Meinard M ¨uller, Henning Mattes, and Frank Kurth. An efﬁ-\ncient multiscale approach to audio synchronization. In Pro-\nceedings of the International Conference on Music Infor-\nmation Retrieval (ISMIR), pages 192–197, Victoria, Canada,\n2006.\n[11] John Warrack. Carl Maria von Weber. Cambridge University\nPress, 1976."
    },
    {
        "title": "Toward Understanding Expressive Percussion Through Content Based Analysis.",
        "author": [
            "Matthew Prockup",
            "Erik M. Schmidt",
            "Jeffrey J. Scott",
            "Youngmoo E. Kim"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1414726",
        "url": "https://doi.org/10.5281/zenodo.1414726",
        "ee": "https://zenodo.org/records/1414726/files/ProckupSSK13.pdf",
        "abstract": "Musical expression is the creative nuance through which a musician conveys emotion and connects with a listener. In un-pitched percussion instruments, these nuances are a very important component of performance. In this work, we present a system that seeks to classify different expressive articulation techniques independent of percussion instrument. One use of this system is to enhance the organization of large percussion sample libraries, which can be cumbersome and daunting to navigate. This work is also a necessary first step towards understanding musical expression as it relates to percussion performance. The ability to classify expressive techniques can lead to the development of models that learn the the functionality of articulations in patterns, as well as how certain performers use them to communicate their ideas and define their musical style. Additionally, in working towards understanding expressive percussion, we introduce a publicly available dataset of articulations recorded from a standard four piece drum kit that captures the instrument’s expressive range.",
        "zenodo_id": 1414726,
        "dblp_key": "conf/ismir/ProckupSSK13",
        "keywords": [
            "Musical expression",
            "Percussion instruments",
            "Articulation techniques",
            "Percussion sample libraries",
            "Emotion and connection",
            "Classification system",
            "Enhancing organization",
            "Understanding musical expression",
            "Development of models",
            "Communication of ideas"
        ],
        "content": "TOWARD UNDERSTANDING EXPRESSIVE PERCUSSION THROUGH\nCONTENT BASED ANALYSIS\nMatthew Prockup, Erik M. Schmidt, Jeffrey Scott, and Youngmoo E. Kim\nMusic and Entertainment Technology Laboratory (MET-lab)\nElectrical and Computer Engineering, Drexel University\nfmprockup,eschmidt,jjscott,ykim g@drexel.edu\nABSTRACT\nMusical expression is the creative nuance through which\na musician conveys emotion and connects with a listener.\nIn un-pitched percussion instruments, these nuances are a\nvery important component of performance. In this work,\nwe present a system that seeks to classify different expres-\nsive articulation techniques independent of percussion in-\nstrument. One use of this system is to enhance the orga-\nnization of large percussion sample libraries, which can be\ncumbersome and daunting to navigate. This work is also a\nnecessary ﬁrst step towards understanding musical expres-\nsion as it relates to percussion performance. The ability to\nclassify expressive techniques can lead to the development\nof models that learn the the functionality of articulations\nin patterns, as well as how certain performers use them\nto communicate their ideas and deﬁne their musical style.\nAdditionally, in working towards understanding expressive\npercussion, we introduce a publicly available dataset of ar-\nticulations recorded from a standard four piece drum kit\nthat captures the instrument’s expressive range.\n1. INTRODUCTION\nIn music, it is the human component of expression that im-\nparts emotion and feeling within a listener. Expression re-\nlates to the nuances in technique that a human performer\nimparts on a piece of music. Musicians creatively vary tim-\ning, dynamics, and timbre of the musical performance, in-\ndependent from the score, in order to communicate some-\nthing of deeper meaning to the listener [1]. For example,\na musician can alter tempo or change dynamics slightly\nto impart tension or comfort. Similarly, they can alter the\ntimbre of their instrument to create different tonal colors.\nAll of these parameters add an additional level of intrigue\nto the written pitches, rhythms, and dynamics being per-\nformed.\nIn studying percussion, one of the fundamental ways of\ncommunicating a musical idea is through expressive artic-\nulation . Differences in articulation are created by the cre-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.\nTimingDynamicsTimbreArticulationExpressionFigure 1 . Expression: Creative alterations in timing, dy-\nnamics, and instrument timbre can deﬁne a musician’s ex-\npressive style.\native combination of dynamics and excitation timbre. This\nsimple relationship is outlined in Figure 1. There are an al-\nmost inﬁnite number of ways that a percussionist can strike\na drum. While the strike itself is restricted to being a sin-\ngle discrete event, there exists a vast range of articulations\nthat make each of those seemingly discrete actions sit in a\ncontinuous and highly dimensional space.\nIn percussion, there are four main techniques of excita-\ntion: strikes, rim shots, cross sticks, and buzz strokes. An\nexplanation of these techniques is outlined in Table 1. This\nsimple set of excitation techniques become the building\nblocks of the standard rudiments that deﬁne most aspects\nof percussion music [2]. Each expressive articulation has\nmeaning in the context of a rudiment, and many individual\nperformers have unique ways of expressing and combining\nthem. This deﬁnes their style and identity as a musician.\nIn this initial work, we seek to quantify and understand\ndifferences in excitation techniques. It is important in the\ncontext of percussion that a bottom up approach be taken to\nexpressive performance analysis. Percussion performance\nis built on the rudimentary combination of unique artic-\nulations, so this is a logical place to start. In the music\ninformation retrieval community, it has been a large aspect\nof percussion performance and expression that has been\nignored.\nIn working towards this understanding of expressive per-\ncussion, we have compiled a comprehensive new public\ndataset of expressive samples recorded from a standard\nfour piece drum kit. The dataset includes samples varied\nby intensity of stroke (staccato vs legato), height of stroke,\nand strike position over a variety of excitation techniques\nfor each instrument of the drum kit. Using this dataset we\ntrain a simple four class support vector machine (SVM)\nto distinguish these expressive articulations both depen-Articulation Description\nStrike The drumhead is struck with the tip of the\nstick.\nRim Shot Both the drumhead and rim are struck with\nthe tip and shaft of the stick simultaneously.\nCross Stick The butt of the stick strikes the rim while\nthe tip rests on the head.\nBuzz Stroke The stick is pressed into the drum to create\nmultiple, rapid strokes.\nTable 1 . Excitation Techniques: There are four basic drum\nexcitation techniques.\ndent on and independent of percussion instrument type.\nIn the context of this paper, we will investigate the three\ndrums commonly struck with sticks (snare drum, rack tom,\nand ﬂoor tom) and the four excitations that become the\nbuilding blocks of rudiments. Excitation classiﬁcation is\nonly a small aspect of percussion expression, but the abil-\nity to recognize these differences in articulation is a neces-\nsary ﬁrst step in understanding percussion performance as\na whole.\n2. BACKGROUND\nThere are a few areas of research tangentially related to ex-\npressive percussion performance. The ﬁrst and most widely\nstudied is the task of instrument identiﬁcation. Earlier stud-\nies in instrument recognition have focused mainly on the\nability to classify a wide range of traditional instrument\ntones, but more recently, a greater effort has been made\nto classify instruments speciﬁc to the realm of percussion.\nIn [3], a set of systems using a wide range of feature se-\nlection and classiﬁcation techniques performed well at dis-\ncriminating percussion instruments. However, this study\nonly took into account a standard drum strike and pur-\nposely did not include alternative articulations, such as rim\nshots or buzz strokes.\nSome studies take the instrument identiﬁcation approach\na step further and attempt to transcribe drum patterns. One\nsuch transcription study presented in [4] used non-negative\nspectrogram factorization and onset detection techniques\nin order to separate drum sounds and classify them as ei-\nther a snare drum, bass drum, or hi-hat. This shows promise\nin the ability to retrieve drum sounds directly from pat-\nterns. In [5], Battenberg and Wessel used deep learning\napproaches in order to learn beat sequence timings of the\nsnare drum, bass drum, and hi-hat in different drum pat-\nterns. Understanding a drum’s context within a perfor-\nmance can lead to models that can inform musical style.\nThis was a step in the right direction for the analysis of\npercussion expression.\nThere has also been an evolving volume of work study-\ning musical performance analysis and expression speciﬁ-\ncally. Mion and Poli in [1] stated that musical expression\nis best represented with score independent descriptors that\nmodel intricacies in timing, dynamics, and timbre. They\nshowed that a simple set of features can be used to cap-ture and classify the expressive intent of a performer in\nboth affective and sensorial domains. Other work in mu-\nsic expression focuses on the intricacies of speciﬁc instru-\nments. In [6], an analysis-by-synthesis experiment was\nperformed to model, synthesize, and evaluate the expres-\nsive characteristics of a clarinet performance. The authors\nidentiﬁed feature dynamics that relate to expressive perfor-\nmance. They then forced the dynamic features to be static,\ncreating a less expressive re-synthesis. A listening test was\nthen performed which asked if subjects preferred the origi-\nnal or altered recordings. Results from the test showed that\nlisteners preferred the original musically expressive per-\nformance. It also showed that expression is captured in\nthe evolution of features over time, and removing this as-\npect effectively removes musical expression. This demon-\nstrated that the dynamic nature of instrument timbre is an\nimportant aspect of music expression. In order to capture\nfeature dynamics, simple polynomial expressions can be\nﬁt to the time varying process. This provides a compact\nrepresentation of sequential data in both the time and fre-\nquency domains [7].\nA vast majority of prior work in musical expression\nanalysis has revolved around understanding the timbral char-\nacteristics of pitched instruments. A detailed analysis of\nexpressive percussion is also necessary, yet it is largely ig-\nnored. However, some sparse examples of these studies do\nexist. The work in [8] focuses on snare drum expression\nand attempts to distinguish playing position on the head\nas well as excitation techniques, such as using brushes or\nplaying a rim shot. These experiments, however, were very\nlimited in scope, with models being only applicable to one\ndrum. Additionally, all training and testing examples were\nperformed at a single volume and intensity level.\nIn this paper, we perform the task of percussion artic-\nulation classiﬁcation similar to the work found in [8]. In\nour study however, it is important for the models to gener-\nalize over multiple pieces of the drum kit. Secondly, our\nmodels incorporate additional excitation techniques (buzz\nstrokes and cross stick strokes) as well as a dataset con-\ntaining many different ways of performing these articula-\ntions. Using compact representations of timbral character-\nistics over time, we train classiﬁers to distinguish excita-\ntion techniques independent of drum, stick height, inten-\nsity of stroke, and head strike position.\n3. DATASET OF EXPRESSIVE PERCUSSION\nIn domains outside of percussion, there exist large datasets\nthat can be used for expressive performance analysis. A\ncomprehensive, well-labeled set of expressive percussion\nsamples is less common. The presented work makes use\nof a newly recorded dataset that encompasses a vast array\nof percussion performance expressions on a standard four\npiece drum kit. In the context of this paper, only the snare\ndrum, rack tom, and ﬂoor tom samples are used. Each\ndrum used has samples that span the following range:\n\u000fstick heights: 8cm, 16cm, 24cm, and 32cm\n\u000fstroke intensities: light, medium, heavyFeature Feature Feature\nNames Abbreviation Description Source\nRMS energy RMS root-mean-squared energy n/a\nroughness R energy of beating frequencies [9]\nbrightness B description of spectral brightness [9]\n2 bin ratio (bottom half) SRA ratio of spectral energy below 1000Hz to the full spectrum [1]\n3 bin ratio (low) SRL ratio of spectral energy below 534Hz to the full spectrum [1]\n3 bin ratio (med) SRM ratio of spectral energy between 534Hz and 1805Hz to the full spectrum [1]\n3 bin ratio (high) SRH ratio of spectral energy above 1805Hz to the full spectrum [1]\nTable 2 . Basic Features: Single dimensional time and frequency domain features are used as the basis for the evolution\nfeatures.\n\u000fstrike positions: center, halfway, edge\n\u000farticulations: strike, rim shot, buzz stroke, cross stick\nThis subset includes 1804 individual examples across\nthe four articulations over the three drums. Additionally,\nthere are at least 4 examples of each expressive combi-\nnation. Recordings include samples with the snare wires\nboth touching (snares on) and not touching (snares off) the\nbottom head of the snare drum. The division of sample\nvariety is not completely uniform across the entire set, but\nit was designed to allow for the most complete coverage\nof each instrument’s expressive range. That being said,\nno one combination of expressive parameters vastly out-\nweighs another and all are adequately represented.\nThe full dataset also includes a complete array of ex-\npressive bass drum, hi-hat, and cymbal samples as well.\nEach articulation example has monophonic and stereo ver-\nsions with multiple mixes using direct (attached) and in-\ndirect (room) microphone positioning techniques. This is\nthe ﬁrst publication where this dataset appears and it can\nbe made freely available to others upon request.\n4. PREDICTING EXPRESSIVE ARTICULATION\nIn expressive performance, the evolution of timbre over\ntime is an important component on both a micro and macro\nlevel. This work investigates expression at the micro level\nby attempting to model the evolution of percussion articu-\nlations. Using the sequential evolution of features derived\nfrom time domain and frequency domain components of\nthe signal, a set of classiﬁers is trained to predict percus-\nsion articulations within subsets containing only individual\ndrums (only snare, only rack tom, etc.) as well as within\nthe superset of all drum samples.\n4.1 Feature Design\nThe aural differences in percussion articulations are de-\nﬁned by the short time evolution of their spectral compo-\nnents. For example, a buzz stroke evolves very differently\nthan a rim shot. These differences are apparent in both\ntheir time domain and frequency domain characteristics.\nIn order to capture this evolution, a set of compact fea-\ntures was implemented that model the envelope of single\ndimensional features over time. This compact representa-\ntion is derived from the coefﬁcients of a polynomial ﬁt to\nthe time varying feature data similar to [7]. This compact\npolynomial representation was calculated for the featuresoutlined in Table 2. Descriptions of the new polynomial\ncoefﬁcient features are described in Table 3.\nFeature Feature\nNames Description\nRMS 3RMS 63rdand6thorder coefﬁcients of RMS\nR3R6 3rdand6thcoefﬁcients of R\nB3B6 3rdand6thcoefﬁcients of B\nSR3SR6 3rdand6thaggregated coefﬁcients of\nSRA, SRL, SRM, and SRH\nTable 3 . Evolution Features: New features are derived\nfrom the coefﬁcients of polynomials ﬁt to the the single\ndimensional features in Table 2 over time.\nFigure 2 shows the time evolution of selected features\nand their polynomial representations for a snare drum across\neach of the articulation examples. It is easy to qualitatively\ndiscriminate the differences in shape for each of the ar-\nticulations. Polynomials ﬁt to the feature data are able to\ncapture this shape in a compact manner. It was found in\nearly experimentation that the third and sixth degree poly-\nnomial ﬁts were optimal for representation. In order to\nevaluate the salience of these newly implemented features,\nMel-Frequency Cepstral Coefﬁcients (MFCCs) and their\nﬁrst and second derivatives were also used in the classiﬁ-\ncation tasks for comparison.\n4.2 Experiments\nThe main focus of the work presented is to classify the\nexcitation techniques of expressive drum strike articula-\ntions. The articulations observed and their descriptions\nare shown in Table 1. Using the polynomial coefﬁcient\nfeatures from Table 3, a four class support vector machine\n(SVM) using a radial basis function (RBF) kernel was trained\nto discriminate excitation. In all experiments, ﬁve-fold\ncross validation was performed for both parameter tuning\nand training/testing. The classiﬁcation task was run for\neach drum individually as well as for all drums in combi-\nnation. This tested the effectiveness of the system to under-\nstand expression on individual drums as well as throughout\nthe entire drum kit. For example, in a robust system a rim\nshot should be classiﬁed as such regardless of the instru-\nment on which it was performed. In order to compare the\neffectiveness of each of the new features, the classiﬁcation\ntask was also performed using the means of the MFCCs\nand their ﬁrst and second derivatives over the duration of\nthe sample.0 100 200 30000.51Strike SRA60 100 200 30000.51Strike SRL60 100 200 30000.51Strike SRM60 100 200 30000.51Strike SRH60 50 100 15000.51Strike B60 20 40 60−1000010002000Strike R60 50 100 15000.20.4Strike RMS6\n0 100 200 30000.51Rim Shot SRA60 100 200 30000.51Rim Shot SRL60 100 200 30000.51Rim Shot SRM60 100 200 30000.51Rim Shot SRH60 50 100 15000.51Rim Shot B60 20 40 60−5000500Rim Shot R60 50 100 15000.20.4Rim Shot RMS6\n0 100 200 30000.51Cross Stick SRA60 100 200 30000.51Cross Stick SRL60 100 200 30000.51Cross Stick SRM60 100 200 30000.51Cross Stick SRH60 50 100 15000.51Cross Stick B60 10 20 30−2002040Cross Stick R60 50 100 15000.20.4Cross Stick RMS6\n0 100 200 30000.51Press Buzz SRA60 100 200 30000.51Press Buzz SRL60 100 200 30000.51Press Buzz SRM60 100 200 30000.51Press Buzz SRH60 50 100 15000.51Press Buzz B60 20 40 60−50005001000Press Buzz R60 50 100 15000.20.4Press Buzz RMS6Figure 2 . Feature Evolution Example: Sixth order polynomials are ﬁt to the temporal feature data of four snare drum\narticulations.\nThe ﬁrst experiment involved classifying excitation on\nthe each drum individually. Features were used both alone\nand in aggregation. In order to aggregate the features, each\ndimension was normalized to have zero mean and unit vari-\nance. The testing data was transformed using the mean and\nvariance derived from the training data. This allowed each\nfeature to be simply concatenated for training and testing.\nThe raw features and projections via a principal compo-\nnents analysis (PCA) were also explored, but in practice,\nthe simple normalization transformation yielded the best\nresults. The second experiment classiﬁed excitation over\nthe set of all drum samples. Again, the features were used\nboth individually and in aggregation with the simple nor-\nmalization. In both experiments, the new features and their\ncombinations were also used in conjunction with MFCCs.\nThis MFCC aggregation shows their ability to add time do-\nmain information to an already salient, yet static, feature\nand improve its performance.\n4.3 Results\nThe ﬁrst experiment classiﬁes excitation for each drum in-\ndependently using the features individually as well as in\nselected aggregations. Table 4 shows the accuracies for\nthe features individually. MFCCs averaged over the ex-\nample are the best single performing feature for both the\nsnare drum and rack tom. The ﬂoor tom, however, shows\nbetter performance with the 3rd and 6th order polynomial\ncoefﬁcients of the spectral ratios (SR 3SR6) than it does\nwith the MFCCs. While standard MFCCs do not take into\naccount any information about time evolution, each artic-\nulation does have an inherently different average timbre.\nBecause MFCCs are designed to provide an estimate of\nthe spectral envelope and capture this timbre, they per-\nform reasonably well. However, when the samples have\na greater length and therefore a longer timbre evolution,\nsuch as that of a ﬂoor tom, MFCCs start to degrade in per-\nformance while some of the evolution features start to im-\nprove.Individual Feature Snare Rack Tom Floor Tom\nMFCC 0.956\u00060.012 0.914\u00060.037 0.872\u00060.027\n\u0001MFCC 0.771 \u00060.015 0.661\u00060.029 0.834\u00060.015\n\u00012MFCC 0.646 \u00060.031 0.544\u00060.061 0.637\u00060.020\nSR3 0.897\u00060.016 0.835\u00060.017 0.907\u00060.045\nSR6 0.776\u00060.023 0.838\u00060.033 0.896\u00060.025\nB3 0.736\u00060.032 0.846\u00060.031 0.859\u00060.036\nB6 0.713\u00060.013 0.755\u00060.032 0.845\u00060.009\nR3 0.407\u00060.017 0.670\u00060.017 0.523\u00060.022\nR6 0.514\u00060.021 0.822\u00060.017 0.578\u00060.050\nRMS 3 0.637\u00060.013 0.696\u00060.039 0.795\u00060.039\nRMS 6 0.773\u00060.014 0.893\u00060.030 0.845\u00060.018\nTable 4 . Classiﬁcation Accuracies: Excitation techniques\nwere classiﬁed using each feature on each drum individu-\nally.\nTable 5 shows the performance of features in combina-\ntion on the individual drums. The feature combinations\nwith the highest classiﬁcation accuracies for each drum\nare displayed along with the best performing individual\nfeatures for comparison. In all cases, the aggregated fea-\nture combinations had a higher classiﬁcation accuracy than\neach of the best performing individual features. This shows\nthat combining an estimation of general timbre with certain\nfeatures that capture that timbre’s evolution can improve\nclassiﬁcation accuracy. In Table 5 only the top ﬁve per-\nforming feature combination accuracies for each drum are\nshown. Those that appear in multiple lists show they are\nbetter at generalizing over the different drum types. The\n6thorder brightness feature in combination with MFCCs\n(B6MFCC) was the only aggregation to appear within the\ntop ﬁve best performing combinations over all three drum\ntypes.\nIn the second experiment, a single classiﬁer was trained\non articulation samples from all three drums. The classi-\nﬁers were again trained on each feature individually and in\ncombination. The accuracies for the classiﬁcation of per-\ncussion articulations, independent of drum, are shown in\nTable 6. In the classiﬁcation of excitation over the supersetFeature Aggregation Snare Drum Rack Tom Floor Tom\nSR3R3B3MFCC 0.987\u00060.001 - 0.982 \u00060.010\nSR3B3MFCC 0.982 \u00060.005 - 0.972 \u00060.007\nB3MFCC 0.982 \u00060.007 0.956\u00060.013 -\nB6MFCC 0.978\u00060.005 0.963\u00060.015 0.974\u00060.019\nSR3R3MFCC 0.977 \u00060.012 - -\nSR6R6B6MFCC - 0.9712\u00060.009 0.982\u00060.014\nSR6MFCC - 0.955 \u00060.020 -\nR6MFCC - 0.955 \u00060.012 -\nSR6B6MFCC - - 0.984\u00060.005\nBest Individual 0.956 \u00060.012 0.914\u00060.037 0.907\u00060.045\n(MFCC) (MFCC) (SR 3)\nTable 5 . Classiﬁcation Accuracies: Excitation techniques\nwere classiﬁed using selected feature aggregations on each\ndrum individually. Results are shown for the top ﬁve per-\nforming features on each drum. Feature combinations that\nare outside the top ﬁve best performing aggregations for a\nsingle drum type are marked with ‘-’.\nof all drums, MFCCs were shown to be the best perform-\ning feature. However, when the polynomial envelope fea-\ntures were used in combination with MFCCs, accuracy was\nagain improved. The 6thorder brightness feature in combi-\nnation with MFCCs (B 6MFCC) was the best performing\nfeature for over the superset of all drums. This is likely\ndue to the fact that this combination was also the only one\ncontained within the top performing combinations of all\nindividual experiments from Table 5.\nFeature All Drums\nMFCC 0.930\u00060.011\n\u0001MFCC 0.745 \u00060.021\n\u00012MFCC 0.534 \u00060.016\nSR3 0.847\u00060.010\nSR6 0.744\u00060.020\nB3 0.734\u00060.024\nB6 0.719\u00060.018\nR3 0.498\u00060.020\nR6 0.514\u00060.006\nRMS 3 0.731\u00060.017\nRMS 6 0.590\u00060.008\nB6MFCC 0.972\u00060.004\nSR3R3B3MFCC 0.969 \u00060.011\nSR6B6MFCC 0.967 \u00060.008\nSR6R6B6MFCC 0.965 \u00060.006\nSR3B3MFCC 0.963 \u00060.004\nTable 6 . Classiﬁcation Accuracies: Excitation techniques\nwere classiﬁed using features individually and in aggrega-\ntion over the superset of all drum types.\nIn all cases, for each individual drum and the superset\nof all drums, MFCCs performed rather well on their own.\nHowever, they do not take into account any information\nregarding the temporal evolution of the signal. The deriva-\ntives of MFCCs were also used, but they provide only a\nstatic picture of the amount of change present when aver-\naged over the example. They still lack information as to\nhow those changes evolve. Additionally, in the presented\nexperiments, MFCCs were shown to be better at model-\ning articulations than were their derivatives. However, by\nusing the polynomial coefﬁcients of simple time varying\nfeatures along with standard MFCCs, the system was able\nto gain temporal context, leading to better performance.5. CONCLUSIONS AND FUTURE DIRECTIONS\nIn this paper, it was shown that the coefﬁcients of polyno-\nmials ﬁt to model feature evolution can provide a compact\nrepresentation with the ability to quantify percussive ar-\nticulation. These features in conjunction with popular fea-\ntures, such as MFCCs, can improve performance by adding\ntemporal context. In this paper, we also introduced a new\ncomprehensive dataset of expressive percussion articula-\ntions. This presented work only scratches the surface of\nthis dataset’s applicability to problems involving expres-\nsion in musical performance. Classifying articulation is a\nsmall, yet very necessary step in the understanding of per-\ncussion performance and expression in general. Moving\nforward, more work must be done towards understanding\nthe micro and macro evolution of expression.\nOn the micro level, this work can be expanded upon by\nusing more sophisticated systems to improve the modeling\nof feature evolution. It was shown in [10] that linear dy-\nnamical systems (LDS) are a compact way of representing\nand synthesizing pitched percussive instrument tones. This\nintroduces the possibility of training an LDS for each ar-\nticulation example and training a classiﬁer that uses system\nparameters as features. Secondly, an LDS is a generative\nmodel, so it may also be possible generate or alter learned\nsets of percussive articulation. Understanding this micro\nevolution can greatly assist in the navigation and organi-\nzation of large humanly expressive sample libraries, which\nare usually cumbersome for percussion instruments.\nIn future work, we look to model not only the micro\nevolution, but the macro evolution of expression as well.\nIf we are able to classify percussion articulations, we can\nlook further into its meaning by developing models that\nlearn the functionality of articulation in patterns and per-\nformance. The articulation classiﬁcation along with statis-\ntics of their usage, dynamics, and time onsets can lead\nto models that contain information about human playing\nstyle. This performance style can be used to model indi-\nvidual percussionists or larger populations of similar per-\ncussionists. With these performance models in conjunc-\ntion with the ability to classify articulation, we can inves-\ntigate the possibility of expressive performance generation\nusing unlabeled sets of any custom sample library that a\nproducer or composer wishes to use. This may seem like\na lofty goal in relation to this work’s present state, and in\nmost respects, it is. However, expressive articulation is one\nof the most important parameters of a percussionist’s per-\nformance. The ability to classify expressive excitation, in-\ndependent of percussion instrument, is the necessary ﬁrst\nstep towards understanding the unique intricacies and nu-\nances of percussion performance and its relation to human\nexpression in general.\n6. ACKNOWLEDGMENTS\nThe authors would like to thank the Music Industry De-\npartment of Drexel University’s College of Media Arts and\nDesign for their support and assistance in the recording,\nmixing and organization of the expressive percussion sam-ple library. It was with their help that we were able to\ncreate a comprehensive, high quality, labeled audio dataset\nof expressive percussion.\n7. REFERENCES\n[1] L. Mion and G. D. Poli, “Score-independent audio\nfeatures for description of music expression,” Audio,\nSpeech, and Language Processing, IEEE Transactions\non, vol. 16, no. 2, pp. 458–466, 2008.\n[2] M. Goldenberg, Modern school for snare drum . Hal\nLeonard, 1955.\n[3] P. Herrera, A. Yeterian, and F. Gouyon, “Automatic\nclassiﬁcation of drum sounds: A comparison of fea-\nture selection methods and classiﬁcation techniques,”\ninMusic and Artiﬁcial Intelligence , vol. 2445 of Lec-\nture Notes in Computer Science , pp. 69–80, Springer\nBerlin Heidelberg, 2002.\n[4] J. Paulus and T. Virtanen, “Drum transcription with\nnon-negative spectrogram factorisation,” in Proceed-\nings of the 13th European Signal Processing Confer-\nence, p. 4, 2005.\n[5] E. Battenberg and D. Wessel, “Analyzing drum pat-\nterns using conditional deep belief networks,” in Pro-\nceedings of the International Conference on Music In-\nformation Retrieval , 2012.[6] M. Barthet, P. Depalle, R. Kronland-Martinet, and\nS. Ystad, “Analysis-by-synthesis of timbre, timing, and\ndynamics in expressive clarinet performance,” Music\nPerception , vol. 28, no. 3, pp. 265–278, 2011.\n[7] M. Lagrange, M. Raspaud, R. Badeau, and G. Richard,\n“Explicit modeling of temporal dynamics within mu-\nsical signals for acoustical unit similarity,” Pattern\nRecognition Letters , vol. 31, no. 12, pp. 1498–1506,\n2010.\n[8] A. Tindale, A. Kapur, G. Tzanetakis, and I. Fujinaga,\n“Retrieval of percussion gestures using timbre classiﬁ-\ncation techniques,” in Proceedings of the International\nConference on Music Information Retrieval , pp. 541–\n544, 2004.\n[9] O. Lartillot and P. Toiviainen, “A matlab toolbox for\nmusical feature extraction from audio,” in Interna-\ntional Conference on Digital Audio Effects , pp. 237–\n244, 2007.\n[10] E. M. Schmidt, R. V . Migneco, J. J. Scott, and Y . E.\nKim, “Modeling musical instrument tones as dynamic\ntextures,” in Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA), 2011 IEEE Workshop\non, pp. 329–332, IEEE, 2011."
    },
    {
        "title": "Evaluating OMR on the Early Music Online Collection.",
        "author": [
            "Laurent Pugin",
            "Tim Crawford"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415946",
        "url": "https://doi.org/10.5281/zenodo.1415946",
        "ee": "https://zenodo.org/records/1415946/files/PuginC13.pdf",
        "abstract": "The Early Music Online (EMO) collection consists of about 300 printed music books of the sixteenth century held at the British Library. They were recently digitized from microfilms and made available online. In total, about 35,000 pages were digitized. This paper presents an optical music recognition (OMR) evaluation on the EMO collection. Firstly, the content of the collection is reviewed, looking at the type of music notation and the type of printing technique. Secondly, for the books for which it is possible (260 books), an OMR evaluation performed using the Aruspix OMR software application is presented. For each book, one randomly selected page of music was processed and the recognition rate was computed using a corrected transcription of the page. This evaluation shows very promising results for large-scale OMR on the EMO or similar collections. The paper also highlights critical points that should be taken into account in such an enterprise.",
        "zenodo_id": 1415946,
        "dblp_key": "conf/ismir/PuginC13",
        "keywords": [
            "British Library",
            "sixteenth century",
            "printed music books",
            "digitalization",
            "Optical Music Recognition (OMR)",
            "Aruspix OMR software",
            "British Library",
            "critical points",
            "large-scale OMR",
            "similar collections"
        ],
        "content": "EVALUATING OMR ON THE EARLY MUSIC ONLINE COLLECTION Laurent Pugin Tim Crawford Swiss RISM Office laurent.pugin@rism-ch.org Goldsmiths College, \u0000\b\u001dUniversity of London t.crawford@gold.ac.uk ABSTRACT The Early Music Online (EMO) collection consists of about 300 printed music books of the sixteenth century held at the British Library. They were recently digitized from microfilms and made available online. In total, about 35,000 pages were digitized. This paper presents an optical music recognition (OMR) evaluation on the EMO collection. Firstly, the content of the collection is re-viewed, looking at the type of music notation and the type of printing technique. Secondly, for the books for which it is possible (260 books), an OMR evaluation per-formed using the Aruspix OMR software application is presented. For each book, one randomly selected page of music was processed and the recognition rate was com-puted using a corrected transcription of the page. This evaluation shows very promising results for large-scale OMR on the EMO or similar collections. The paper also highlights critical points that should be taken into account in such an enterprise. 1. INTRODUCTION Over the last decade, libraries around the world have been digitizing their collections extensively, making available online not only books but also music scores. For books, digitization projects most often include an optical character recognition (OCR) step that renders the content of the images searchable in a similar way, for example, to Google Books. Enabling full-text access has radically changed the search paradigm for the user; we no longer search in the same way with the complete content of books as we used to with only the restricted metadata available in a library catalogue. When music scores are digitized, however, they usually remain searchable only through textual metadata, since no transcription of the content is made available during the digitization process. This is mostly due to the fact that optical music recogni-tion (OMR), the equivalent of OCR for music, is a very challenging task [11]. Furthermore, performing OMR on a large scale is particularly complicated because of the heterogeneous nature of music notation and of different types of music scores. It makes it difficult to set up a so-lution that works acceptably well in all cases. However, preliminary attempts to perform OMR on a large scale on the IMSLP Petrucci Music Library have shown interest-ing results even if at this stage they are still lacking a clear and systematic evaluation [13].  With historical documents in general, performing OCR remains a challenge. In particular, some steps of the OCR process remain critical, in particular the preprocessing of documents that are often highly degraded, and the layout analysis. More recently, large research projects have fo-cused on OCR for historical documents, such as the IMPACT project, with the aim of developing specific tools for the task [2]. For historical music documents, the leading OMR project is SIMSSA, currently with a focus on the handwritten repertoire in neumes and square nota-tion (before 1500) used for experimenting new methods and testing innovative online tools [7].  This paper focuses on printed music from the 16th cen-tury, a period of enormous expansion in the availability of music for domestic use. It was made possible by the development of a typographic technique adapted from Gutenberg’s invention of typesetting. This remained by far the most widely-used method for printing music until the beginning of the 18th century when it was superseded by engraving. Typographic music printing was invented by Ottaviano Petrucci in 1501 and simplified by Pierre Attaingnant in 1527. The former introduced a technique using two or three passes through the press, while At-taingnant developed a single-impression technique that made the process commercially sustainable, with the con-sequence that the number of printed scores increased dramatically from then on. Printed music scores of the time represent a valuable and rich source of material for musicologists, musicians and historians alike. We present in this paper an evaluation of OMR on the Early Music Online collection, a set of about 300 books held at the British Library and recently digitized from mi-crofilms [12]. The evaluation is twofold. First we evalu-ated which of the books in such a collection could be pro-cessed using the Aruspix OMR software application de-veloped specifically for typeset music prints.1 In particu-lar, we were interested to see what percentage of a library collection like this can be processed with existing tech-nology. For this, we looked at the type of music notation, the type of format and the type of printing technique. With this in hand, we were able to define a set of books                                                              1 <http://www.aruspix.net> \n Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  © 2013 International Society for Music Information Retrieval     to be used for the second part of our evaluation, which was to estimate what recognition rate we can obtain in an OMR workflow without human correction of the data. In the next section, we introduce the EMO data set. The experiments for evaluating the OMR recognition rates and the results are presented in the following sec-tions. Remarks on future work conclude the paper. 2. THE EMO COLLECTION In 2011, the British Library together with Royal Hol-loway, University of London and the UK group of the Repertoire International des Source Musicales (RISM) conducted a digitization project of sources held at the British Library. In total, 324 volumes of 16th-century printed music, mostly individual voice part-books of vo-cal music, were re-catalogued, digitized and made availa-ble online together with very rich metadata that includes information about the title pages, the dedicatees, the printer and the printing place, and more. It comprises ap-proximately 35,000 pages for a total of about 10,000 pieces of music. The books chosen for this digitization project were selected from the RISM B/I and B/II Series covering printed anthologies. Anthologies (collections of music by multiple composers) were selected as a priority because their content is not as well known as that of indi-vidual prints (RISM Series A/I). The books were digit-ized from the microfilm collection of the British Library and are available under the JISC Collections Open Edu-cation User Licence version 1.0. 2.1 Notation types Most of the books in the EMO collection are in mensural music notation on five line staves, the most common no-tation of the time. The collection also contains twenty-seven books of lute tablature (e.g., K.1.c.11)1 and about five books of organ tablature (e.g., K.8.h.22). Six books are interesting cases with a mix of mensural notation and lute tablature (e.g., D.250 and K.2.h.12). There is also one book that contains a very rare case of a mixture of five-line mensural notation and four-line square notation, printed by Etienne Gueynard in Lyon in 1528 (K.9.a.23). 2.2 Book formats The large majority of the books of the EMO collection are part-books, a standard format of the time. In part-books, each voice or instrumental part is printed in a separate gathering. Sometimes, two or more voices are printed in the same book, for example when a book in four voices (i.e., four part-books) also contains pieces with more voices. In these cases, the different voices ap-pear as in a choir book for the singers or musicians to be able to sing or play together by sharing the part-book. The EMO collection also contains proper choir books                                                              1 The reference for the books is their signature at the British Library. Images can be accessed from the RISM UK website (www.rism.org.uk). (e.g., K.9.a.11) where all voices are printed in a single book. Finally, the EMO collection contains ten table books mostly printed by Jacques Moderne in Lyon (e.g., K.10.a.7). The table books are similar to choir books with the main difference that the voice-parts are not all printed in the same direction. They were intended to be laid on a table with musicians sitting on different sides of the book. All the table books in EMO contain parts printed upside-down, but books with parts in three or four directions also exist. 2.3 Printing techniques About 300 books in the EMO collection were printed us-ing the single impression typographic technique intro-duced by Attaingnant. His innovation was to use type-faces where the staff lines and the notes were printed at the same time; this became the most widely-used tech-nique for printed music for nearly two centuries. The EMO collection also contains 13 books printed with the multiple impression technique, mostly printed by Petruc-ci (e.g., K.1.d.12), one of the very few printers who used this technique. There are also seven books printed from engraved plates by Simone Verovio (K.8.b.17) who was one of the first music engravers, and five books in wood-cut mostly by Andrea Antico (e.g., K.8.b.7).  Figure 1. A page printed in multiple impressions by Petrucci left out of the experiments. The processing of pages printed with this technique is problematic when ornate letters are superimposed on the music staves. 3. EXPERIMENTS 3.1 Data set for the OMR evaluation By looking at the different characteristics in terms of no-tation type, book format and printing technique, we were able to determine which were the books that could be processed successfully with the OMR tool that was to be used for the evaluation. We selected all the books in men-sural notation printed with the single impression tech-nique.  We left out the books in multiple impressions, even though they could also have been processed, albeit cer-tainly yielding less accurate results. As shown on Figure 1, ornate letters are sometimes superimposed on the mu-sic staves, which makes small areas of the page badly recognized by the OMR software application used for our evaluation. We also left out lute tablatures and books printed from engraved plates that are not appropriate for the OMR software application. Finally, we also left out \n   the table books, the books with a mixture of lute and mensural notation and also the book with square notation. \n Figure 2. The 260 books selected for the OMR evalua-tion were printed in 17 different cities, reflecting the im-portance of the music printing centres of the time. We ended up with a total of 260 books representing 80% of the EMO collection. These 260 books were print-ed by no less than 48 different printers coming from 17 cities (plus two from unknown locations). Interestingly, the provenance of the books reflects quite precisely the music printing production of the different cities at that time (see Figure 2); in particular, more than a third of the books were printed in Venice [3]. The diversity of the data set can also be visualized by looking at the different font shapes. In Table 1, we pre-sent the 18 different G-clef shapes found in the pages we processed for our evaluation.   Amadino  Antico  Ballard  Donangeli  Ang. Gardano  Gerlach  Hucher  Moderne  Petrejus  Phalèse  Scotto  Short   Susato  Vissenaken  Waelrant  [unknown]  [unknown]  [unknown] Table 1. The font and woodcut shapes in the books used for the evaluation are highly variable as illustrated by the 18 different G-clef shapes appearing in the data set. 3.2 Procedure The experiments were performed using the original gray-scale images in uncompressed TIFF format at 400dpi, the resolution used by the British Library during the EMO digitization process. This resolution, however, does not provide accurate information about the original document size because of all the different parameters that were involved in the making of the microfilms and the scanning. As an indication, looking at the staff height in pixels, we noticed in the EMO images that the staff height goes from 60 up to 300 pixels, with an average value of 122 pixels and a median value of 108 pixels. Of course, this is only an indication since the actual staff sizes on the sources themselves vary. In the EMO images, each image always contains two pages. Since the OMR tool used for our experiments works on single page images, the images had to be split. The images were split automatically by selecting a little more than half the image width for each page (55% of the width). Because the books are always centered in the EMO images and because the tool performs a border re-moval, this simple method worked well in all cases. For each of the 260 books of our data set, one page of music was randomly selected.  For the books where the random selection process did not retrieve a page containing music (9 books), the first subsequent page of music was selected by hand. The recognition process in the tool used for the exper-iments includes a binarization step that converts the gray-scale image into a black and white one. This step has been shown to be critical on degraded music documents [8]. The OMR tool offers two options for the binariza-tion, one for documents in good condition (B1), and an-other based on an algorithm specifically designed for highly degraded documents with marked bleed-through (B2). The B1 algorithm is the Brink & Pendock 1996 al-gorithm, and B2 its modified three-class version, Pugin 2007, the two best-performing algorithms described in [4].  The choice of the binarization option is left to the user and is not determined by the tool. In our experiments, we allowed the evaluator to choose in advance the algorithm that seemed the most appropriate for each book based on a visual evaluation of its degradation. B1 was applied to 159 books, and B2 to 101. In order to evaluate how ap-propriate the choice of the evaluator was, we also pro-cessed all the pages for which B2 was selected with the B1 options and compared the results. All the pages were processed without human interac-tion. That is, without any processing indication or correc-tion other than the binarization method selection. In that sense, the results provide a clear indication of what would be obtained with this technology on a large-scale OMR process involving minimal human intervention for pre-paring the process and no data correction at all. \n   4. RESULTS All but 2 of the 260 pages were successfully recognized. Only two pages produced unusable results because the staff detection step in the OMR process failed. The two pages were taken from books printed by Giorgio Marescotti (C.219.a) and Antonio Gardano (K.3.l.7). \n \n Figure 3. For two pages in the data set the recognition process failed. With the first, both the quality of the typeface used for printing and the quality of the micro-film were poor. With the second, the bleed-through was too strong and the microfilm quality poor. Both failures can be explained by the poor quality of the documents as shown in Figure 3 for the prints by Marescotti and Gardano. In the first case, the quality of the print itself was very bad because it had been printed with a deteriorated typeface that produced extremely ir-regular staff lines. The quality of the microfilm and of the image was also problematic. In the second case, the book was highly degraded, with very marked bleed-through. Nothing in the layout of the documents indicates that the recognition would have been problematic, had they been in better condition. For all the pages that were processed successfully (258), the OMR transcription was fully corrected by hand in order to be able to compute recall and precision fol-lowing the evaluation procedure previously used for the tool [10]. This method uses a standard best alignment ap-proach for evaluation of the number of substitutions, de-letions and insertions. For all symbols, exact matching is required, e.g., for a music note, a match occurs only when both pitch and duration are the same. It is also important to mention that the misrecognition of a clef does not af-fect the evaluation of the subsequent pitches, even though technically pitches would be incorrect – it is also not rare to see cases where the clef is actually wrong in the source itself. \n Figure 4. Aside from a few pages that produced poor results, the quartiles show that three quarters of the pag-es produced a recall rate between 85% and 100% and a precision rate between 76% and 95%. The median recall rate is 90% and the median preci-sion rate is 83%. Some pages did produce bad results. However, it can be seen that this is the case for only a few of them. Although the lowest recall rate is just 43%, the lower recall rate quartile is 85%, as shown on Figure 4. We noticed that the lowest recall rate was obtained on a fairly atypical page; it contained only two staves with few music symbols on them and also has added handwrit-ten slurs, which explain the poor results. Looking at the results by printer did not highlight very significant differences. Figure 5 shows the quartiles for the five printers for which we had more than 10 books in the data set. Overall, it seems that document degradation has more impact on the results than the printers, although the difference is slight.  Some of the books in the data set were printed using nested typefaces. With this technique, types with not only five lines but also four or three lines are used together with line elements overlapping several types horizontally. The technique had the advantage of requiring a smaller number of type shapes. It was used mostly in the Low Countries, in France and in Germany, by printers such as Robert Ballard and Tielman Susato. Our experiments show that the nested technique used by some printers do not cause major problems. Though this might be an ex-planation why the results obtained on the prints by Bal-lard are more variable, the results obtained on prints by Susato (19 books) are the same as that obtained on the prints by Antonio Gardano (30 books) with an average recall rate of about 92% and an average accuracy of 86%. \nRecallPrecision 0 10 20 30 40 50 60 70 80 90 100   \n Figure 5. The quartiles for the prints with more than 10 books in the data set shows differences, albeit small ones. The results on the prints of Ballard are more variable and can be lower. For the binarization algorithm, the results show that the choice made by the human evaluator before the pro-cessing of the book was appropriate in most cases. More precisely, the B2 binarization option gave better results than B1 in 80% of the cases where it was chosen by the evaluator. In the 21 cases where this was an inappropriate choice, it produced a recall rate significantly lower (by more than 5%) in one third of the cases. Figure 6 shows a page where B2 did improve the results and a case where it did not. In the first case, the recall rate was 69% with B1 and 91% with B2. In the second case, it was 97% with B1 but only 83% with B2, undoubtedly because the bleed-through is not marked enough.    Figure 6. Two pages for which selecting the B2 binariza-tion option did (above) and did not (below) improve the recognition results. For the case where algorithm B2 improved the recog-nition results, we can clearly see looking at the binarized images shown on Figure 7 that B1 was not appropriate, in particular where the ornate letter printed on the other side of the page clearly appears through the paper.   \n \n Figure 7. The binarization results with B1 (above) and B2 (below). The B2 option was clearly more appropriate for removing the bleed-through, in particular where the ornate letter is printed on the other side. Correcting the results for this evaluation highlighted a handful of characteristics that evidently caused recogni-tion errors. Prints with a strophic repertoire, mostly French chansons and lieder, with multi-line lyrics printed under the notes, often caused pre-processing imprecisions that then had an impact on the final recognition process. The problem with multi-line lyrics was that some of them were not properly recognized as text and the software thus attempted to read them as music symbols. Oblique ligatures (appearing on 6 pages) were inconsistently rec-ognized. Other rarities that caused problems were passag-es with an unusual number of ledger lines (1 page) and the aforementioned handwritten slurs (1 page). In terms of document condition, the typical problems that locally affected the recognition process are: the presence of li-brary stamps (2 pages), stains (4 pages), scratched-out notes (1 page), pages with extremely light inking (2 pag-es), or curvature on the edge of the page (9 pages). Addi-tionally, in a few cases, because of very high bleed-through (5 pages), the accuracy of the corrected transcrip-tion used for the evaluation is questionable due to reada-bility issues. 5. CONCLUSION AND FUTURE WORK By evaluating OMR on the EMO collection, this paper sets a baseline for large-scale projects in the field. Our evaluation showed that about 80% of the collection can be processed and that we can expect a recall rate of be-Antonio Gardano (30)Robert Ballard (27)Girolamo Scotto (24)Tielman Susato (19)Angelo Gardano (12) 40 50 60 70 80 90 100\n   tween 85% and 100% for three quarters of the pages. In comparison, the Bibliothèque Nationale de France re-tained a minimal recognition rate of 60% for OCR results to be included as full-text [5]. This was based on an eval-uation that showed this to be the limit for results to be useful for the user. Although searching text is different from searching music, and the usability of the data needs to be clearly estimated, our evaluation already shows an interesting potential for making the EMO or similar col-lections searchable. Furthermore, the evaluation does not take into account the improvement that can be obtained with adaptive OMR techniques that have proven to be extremely valuable in such cases [9]. The EMO collection is certainly an excellent example of what we can expect to find in library collections of similar size. Studying it was informative about which types of sources can be problematic. Whilst 20% of the sources had to be left out, there are other state-of-the-art tools that could be used to process some of them, in par-ticular the lute tablature [6]. For others, specific modules would need to be developed, for example for the layout analysis of table books, while books such as the ones printed from engraved plates are certainly more problem-atic. We can also notice that some categories that had to be left out are rather highly represented in EMO, quite likely because it contains only anthologies. If we compare this with the RISM A/I Series of individual prints, out of about 4,200 prints inventoried for the 16th century, around 3% are lute tablatures (10% in EMO), 0.6% are prints by Petrucci in single impression (3%) and only three prints by Verovio in engraving are inventoried (2%). Of course, this does not take into account the numerous re-editions to be found in the A/I Series. It does, however, show that the 20% exclusion part we ended up with in EMO is cer-tainly an upper limit if 16th century prints are considered on a larger scale and 5% to 10% is a more likely percent-age range on a collection with individual prints. In terms of workflow, detecting which pages are miss-ing music would be necessary for avoiding preparatory work. Experiments have already been conducted on this [1] and the SISSMA project is currently working on simi-lar issues. Our experiments have also shown that strophic music was not always properly handled in the prepro-cessing. This confirms it is always worth improving the layout analysis because it is a critical step in the process. For the binarization, having the user select the binariza-tion algorithm for a book is acceptable because it is a quick task. The cases where B2 was a bad choice could certainly have been avoided with better training of the user. However, user-selected binarization is not optimal because it assumes the degradation to be identical throughout the book, which is of course not always the case. Having the most appropriate binarization algorithm selected automatically would be a valuable improvement. Future work will also include reassembling the parts for reconstructing the score, which will open the door to polyphonic processing of the data. 6. REFERENCES [1] D. Bainbridge, and T. Bell: “Identifying music documents in a collection of images,” in Proceeding of the ISMIR 2006 Conference, pp. 47–52. [2] H. Balk, and L. Ploeger: “IMPACT: Working together to address the challenges involving mass digitization of historical printed text,” OCLC Systems & Services, 25(4), pp. 233–248, 2009. [3] J. Bernstein: Print Culture and Music in Sixteenth-Century Venice, Oxford, Oxford University Press, 2001. [4] J. A. Burgoyne, L. Pugin, G. Eustace, and I. Fujinaga, “A comparative survey of image binarisation algorithms for optical recognition on degraded musical sources,” in Proceedings of the ISMIR 2007 Conference, pp. 509–12. [5] G. Cron: “Corpus.” Presentation, Journée IMPACT, BnF, Paris, September 19, 2011.  [6] C. Dalitz, and C. Pranzas: “German Lute Tablature Recognition,” in Proceedings of the ICDAR 09, pp. 371–375. [7] A. Hankinson, J. A. Burgoyne, G. Vigliensoni, and I. Fujinaga: “Creating a large-scale searchable digital collection from printed music materials,” in Proceedings of the 21st ACM Conference on World Wide Web, pp. 903–908. [8] L. Pugin, J. A. Burgoyne, and I. Fujinaga: “Goal-directed evaluation for the improvement of optical music recognition on Early music prints,” in Proceedings of the ACM-IEEE JCDL 2007, pp. 303–04. [9] L. Pugin, J. A. Burgoyne, and I. Fujinaga, “MAP adaptation to improve optical music recognition of early music documents using hidden Markov models,” in Proceedings of the ISMIR 2007 Conference, pp. 513–16. [10] L. Pugin, J. Hockman, J. A. Burgoyne, and I. Fujinaga, “Gamera versus Aruspix: Two optical music recognition approaches,” in Proceeding of the ISMIR 2008 Conference, pp. 419–24. [11] A. Rebelo, I, Fujinaga, F. Paszkiewicz, A. R. Marcal, C. Guedes, and J. S. Cardoso: “Optical music recognition: state-of-the-art and open issues,” International Journal of Multimedia Information Retrieval, 1(3), pp. 173–190, 2012. [12] S. Rose: “Early Music Online,” Early Music Performer, 30, pp. 22–25, 2012. [13] V. Viro: “Peachnote: Music score search and analysis platform,” in Proceedings of the ISMIR 2011 Conference, pp. 359–362."
    },
    {
        "title": "Combining Modeling Of Singing Voice And Background Music For Automatic Separation Of Musical Mixtures.",
        "author": [
            "Zafar Rafii",
            "François G. Germain",
            "Dennis L. Sun",
            "Gautham J. Mysore"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415066",
        "url": "https://doi.org/10.5281/zenodo.1415066",
        "ee": "https://zenodo.org/records/1415066/files/RafiiGSM13.pdf",
        "abstract": "Musical mixtures can be modeled as being composed of two characteristic sources: singing voice and background music. Many music/voice separation techniques tend to focus on modeling one source; the residual is then used to explain the other source. In such cases, separation performance is often unsatisfactory for the source that has not been explicitly modeled. In this work, we propose to combine a method that explicitly models singing voice with a method that explicitly models background music, to address separation performance from the point of view of both sources. One method learns a singer-independent model of voice from singing examples using a Non-negative Matrix Factorization (NMF) based technique, while the other method derives a model of music by identifying and extracting repeating patterns using a similarity matrix and a median filter. Since the model of voice is singer-independent and the model of music does not require training data, the proposed method does not require training data from a user, once deployed. Evaluation on a data set of 1,000 song clips showed that combining modeling of both sources can improve separation performance, when compared with modeling only one of the sources, and also compared with two other state-of-the-art methods.",
        "zenodo_id": 1415066,
        "dblp_key": "conf/ismir/RafiiGSM13",
        "keywords": [
            "Musical mixtures",
            "singing voice",
            "background music",
            "Non-negative Matrix Factorization (NMF)",
            "singer-independent model",
            "repeating patterns",
            "median filter",
            "separation performance",
            "singer-independent model",
            "model of music"
        ],
        "content": "COMBINING MODELING OF SINGING VOICE AND BACKGROUND\nMUSIC FOR AUTOMATIC SEPARATION OF MUSICAL MIXTURES\nZafar Raﬁi1, Franc ¸ois G. Germain2, Dennis L. Sun2,3, and Gautham J. Mysore4\n1Northwestern University, Department of Electrical Engineering & Computer Science\n2Stanford University, Center for Computer Research in Music and Acoustics\n3Stanford University, Department of Statistics\n4Adobe Research\nzafarrafii@u.northwestern.edu ,ffgermain,dlsun g@stanford.edu ,gmysore@adobe.com\nABSTRACT\nMusical mixtures can be modeled as being composed of\ntwo characteristic sources: singing voice and background\nmusic. Many music/voice separation techniques tend to\nfocus on modeling one source; the residual is then used to\nexplain the other source. In such cases, separation per-\nformance is often unsatisfactory for the source that has\nnot been explicitly modeled. In this work, we propose\nto combine a method that explicitly models singing voice\nwith a method that explicitly models background music,\nto address separation performance from the point of view\nof both sources. One method learns a singer-independent\nmodel of voice from singing examples using a Non-negative\nMatrix Factorization (NMF) based technique, while the\nother method derives a model of music by identifying and\nextracting repeating patterns using a similarity matrix and\na median ﬁlter. Since the model of voice is singer-inde-\npendent and the model of music does not require training\ndata, the proposed method does not require training data\nfrom a user, once deployed. Evaluation on a data set of\n1,000 song clips showed that combining modeling of both\nsources can improve separation performance, when com-\npared with modeling only one of the sources, and also com-\npared with two other state-of-the-art methods.\n1. INTRODUCTION\nThe ability to separate a musical mixture into singing voice\nand background music can be useful for many applica-\ntions, e.g., query-by-humming, karaoke, audio remixing,\netc. Existing methods for music/voice separation typically\nfocus on estimating either the background music, e.g., by\ntraining a model for the accompaniment from the non-vocal\nsegments, or the singing voice, e.g., by identifying the pre-\ndominant pitch contour from the vocal segments.\nSome methods estimate the background music by train-\ning a model on the non-vocal segments in the mixture,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.identiﬁed manually or using trained vocal/non-vocal clas-\nsiﬁers. Ozerov et al. used Bayesian models to train a model\nfor the background music from the non-vocal segments,\nwhich they then used to train a model for the singing voice\n[7]. Han et al. used Probabilistic Latent Component Anal-\nysis (PLCA) to also train a model for the background mu-\nsic, which they then used to estimate the singing voice [2].\nOther methods estimate the background music directly,\nwithout prior vocal/non-vocal segmentation, by assuming\nthe background to be repeating and the foreground (i.e.,\nthe singing voice) non-repeating. Raﬁi et al. used a beat\nspectrum to identify the periodically repeating patterns in\nthe mixture, followed by median ﬁltering the spectrogram\nof the mixture at period rate to estimate the background\nmusic [9]. Liutkus et al. used a beat spectrogram to further\nidentify the varying periodically repeating patterns [6].\nOther methods instead estimate the singing voice by\nidentifying the predominant pitch contour in the mixture.\nLi et al. used a pitch detection algorithm on the vocal\nsegments in the mixture to estimate the predominant pitch\ncontour, which they then used to derive a time-frequency\nmask to extract the singing voice [5]. Hsu et al. also used a\npitch-based method to model the singing voice, while ad-\nditionally estimating the unvoiced components [3].\nOther methods are based on matrix decomposition tech-\nniques. Vembu et al. used Independent Component Anal-\nysis (ICA) and Non-negative Matrix Factorization (NMF)\nto decompose a mixture into basic components, which they\nthen clustered into background music and singing voice us-\ning trained classiﬁers such as neural networks and Support\nVector Machines (SVM) [12]. Virtanen et al. used a pitch-\nbased method to estimate the vocal segments of the singing\nvoice, and then NMF to train a model for the background\nmusic from the remaining non-vocal segments [14].\nOther methods estimate both sources concurrently. Dur-\nrieu et al. used a source-ﬁlter model to parametrize the\nsinging voice and a NMF model to parametrize the back-\nground music, and then estimated the parameters of their\nmodels jointly using an iterative algorithm [1]. Huang et\nal. used Robust Principal Component Analysis (RPCA) to\njointly estimate background music and singing voice, as-\nsuming the background music as a low-rank component\nand the singing voice as a sparse component [4].\nIn this work, we propose a method for modeling thesinging voice, which learns a singer-independent model\nof voice from singing examples using a NMF based tech-\nnique. We then propose to combine this method with a\nmethod for modeling the background music, which de-\nrives a model of music by identifying and extracting re-\npeating patterns using a similarity matrix and a median\nﬁlter. Combining a method that speciﬁcally models the\nsinging voice with a method that speciﬁcally models the\nbackground music addresses separation performance from\nthe point of view of both sources.\nThe rest of the article is organized as follows. In Section\n2, we present a method for modeling singing voice. In Sec-\ntion 3, we review an existing method for modeling back-\nground music, and we propose combining the two methods\nto improve music/voice separation. In Section 4, we eval-\nuate the method for modeling the singing voice and the\ncombined approach on a data set of 1,000 song clips, and\nwe compare them with the method for modeling the back-\nground music alone, and two other state-of-the-art meth-\nods. Section 5 concludes this article.\n2. MODELING SINGING VOICE\nIn this section, we present a method for modeling the sing-\ning voice. Because singer-speciﬁc training examples are\ngenerally not available for the music/voice separation meth-\nods, models for the singing voice are typically based on a\npriori assumptions, e.g., it has a sparse time-frequency rep-\nresentation [4], it is accurately modeled by a source-ﬁlter\nmodel [1], or it is reasonably described by pitch [5].\nRecently, universal models were proposed as a method\nfor incorporating general training examples of a sound class\nfor source separation when speciﬁc training examples are\nnot available [11]. We use these ideas to model the singing\nvoice using a universal voice model , learned from a corpus\nof singing voice examples. Since the formulation of uni-\nversal voice models is based on matrix factorization meth-\nods for source separation, we begin by reviewing Non-\nnegative Matrix Factorization (NMF).\n2.1 NMF for Source Separation\nThe magnitude spectrogram Xis a matrix of non-negative\nnumbers. We assume that the spectrum at time t,Xt, can\nbe approximated by a linear combination of basis vectors\nwi, each capturing a different aspect of the sound, e.g.,\ndifferent pitches, transients, etc.:\nXt\u0019KX\ni=1hitwi\nThe collection of basis vectors W=\u0002\nw1:::wK\u0003\ncan\nbe regarded as a model for that sound class, since all pos-\nsible sounds are assumed to arise as linear combinations of\nthese basis vectors. Likewise, H= (hit)can be regarded\nas the activations of the basis vectors over time. In matrix\nnotation, this can be expressed as:\nX\u0019WH:NMF attempts to learn WandHfor a given spectrogram\nX, i.e., it solves the optimization problem:\nminimize\nW;HD(XjjWH )\nsubject to the constraints that WandHare non-negative.\nDis a measure of divergence between XandWH .\nTo use NMF to separate two sources, say, singing voice\nand background music:\n1. Learn WVusing NMF from isolated examples of\nthe singing voice.\n2. Learn WBusing NMF from isolated examples of\nthe background music.\n3. Use NMF on the mixture spectrogram X, ﬁxing\nW=\u0002\nWVWB\u0003\n, and learning HVandHB.\n4. Estimates of the singing voice-only and background\nmusic-only spectrograms can be obtained from\nWVHVandWBHB.\nA more detailed description of this approach can be found\nin [10], although the authors use an equivalent probabilistic\nformulation (PLCA) instead of NMF.\nOf these tasks, steps 1 and 2 pose the greatest challenge.\nWhile it may be possible to use the non-vocal segments of\nthe background music as isolated training data of the back-\nground music, it is rare to ﬁnd segments in music where\nthe voice is isolated. Source separation is still possible\nin this setting where training data of only one source is\navailable—one simply learns WVtogether with HVand\nHBin step 3. This is the approach taken in [2, 7], but it\nrequires a sufﬁciently accurate prior vocal/non-vocal seg-\nmentation and a sufﬁcient amount of non-vocal segments\nto effectively learn a model of the background music.\n2.2 Universal Voice Model\nOne alternative when training data of a speciﬁc singer is\nnot available is to learn a model from a corpus of singing\nvoice examples. The universal model is a prescription for\nlearning a model from general training examples and incor-\nporating the model in NMF-based source separation [11].\nThe idea is to independently learn a matrix of basis vec-\ntors for each of Msingers from training data of the in-\ndividual singers. This yields Mmatrices of basis vectors\nW1;:::;WM. The universal voice model is then simply\nthe concatenation of the matrices of basis vectors:\nWV=\u0002\nW1:::WM\u0003\nThe hope is that an unseen singer is sufﬁciently simi-\nlar to one or a blend of a few of these singers, so that the\nuniversal voice model can act as a singer-independent sur-\nrogate for singer dependent models.\nIn applying the universal voice model for source sepa-\nration, we make the assumption that the activation matrix\nfor the singing voice\nHV=2\n64H1\n...\nHM3\n75is block sparse, i.e., several of the Hi\u00110. This is neces-\nsary because the number of singers is typically large, and\nthe matrix factorization problem can be underdetermined.\nThe block sparsity is a regularization strategy that incorpo-\nrates the structure of the problem; it captures the intuition\nthat only a few voice models should be sufﬁcient to explain\nany given singer. We achieve block sparsity by adding a\npenalty function \nto the objective function to encourages\nthis structure. \u0015controls the strength of the penalty term.\nminimize\nW;HD(XjjWH ) +\u0015\n(HV) (1)\nAs in [11], we choose the Kullback-Leibler divergence for\nD:\nD(YjjZ) =X\ni;jYijlogYij\nZij\u0000Yij+Zij\nand a concave penalty on the `1norm of the block:\n\n(HV) =MX\ni=1log(\u000f+jjHijj1)\nThe algorithm for optimizing (1) is known as Block KL-\nNMF. Further details can be found in [11].\n3. COMBINED APPROACH\nIn this section, we review an existing method for modeling\nthe background music, and we propose to use it to reﬁne\nthe residual from the singing voice modeling.\n3.1 Modeling Background Music\nA number of methods have been proposed to estimate the\nbackground music, without prior vocal/non-vocal segmen-\ntation, by assuming the background to be repeating and\nthe foreground (i.e., the singing voice) to be non-repeat-\ning. REPET-SIM is thus a generalization of the REpeat-\ning Pattern Extraction Technique (REPET)1, a simple ap-\nproach for separating the repeating background from the\nnon-repeating foreground in a mixture, by identiﬁcation\nof the repeating elements and the smoothing of the non-\nrepeating elements.\nIn particular, REPET-SIM uses a similarity matrix to\nidentify the repeating elements in the mixture - which ide-\nally correspond to the background music, followed by me-\ndian ﬁltering to smooth out the non-repeating elements -\nwhich ideally correspond to the singing voice [8]. Unlike\nthe earlier variants of REPET that use a beat spectrum or\nbeat spectrogram to identify the periodically repeating pat-\nterns [6, 9], REPET-SIM uses a similarity matrix and is\nthus able to handle backgrounds where repeating patterns\ncan also happen non-periodically.\n3.2 Combined Approach\nIn order to improve the music/voice separation that we ob-\ntain from using the universal voice model alone, we pro-\npose cascading the model with REPET-SIM. The idea is\n1http://music.eecs.northwestern.edu/research.php?project=repet\nBlock\nKL-NMFWiener\nﬁlter\nREPET-SIMWiener\nﬁlterFigure 1 . Combined approach which takes in the spectro-\ngram of a mixture Xand returns reﬁned estimates of the\nspectrogram of the singing voice XVand the background\nmusicXB\nthat the universal voice model speciﬁcally models the sing-\ning voice and, through the residual, provides a preliminary\nestimate of the background music, which can then be re-\nﬁned by feeding it to REPET-SIM. The pipeline is shown\nin Figure 1, and detailed below.\nThe universal voice model ﬁrst outputs an estimate for\nthe magnitude spectrogram of the singing voice X(1)\nV, and\na residualX(1)\nBcorresponding to the background estimate,\nwhich are initially ﬁltered into X(2)\nVandX(2)\nBby using\nWiener ﬁltering, as follows:\nX(2)\nV=\f\f\f\f\fX(1)\nV\nX(1)\nV+X(1)\nB\fX\f\f\f\f\f\nX(2)\nB=\f\f\f\f\fX(1)\nB\nX(1)\nV+X(1)\nB\fX\f\f\f\f\f\nwhereXdenotes the complex spectrogram of the mixture\nand\fthe Hadamard (component-wise) product. Wiener\nﬁltering is used here to reduce separation artifacts. Note\nthat we only use the magnitudes of the estimates here.\nThe background estimate from the universal voice model\nX(2)\nBis then fed to REPET-SIM which reﬁnes it into X(3)\nB.\nThe estimates for the complex spectrogram of the singing\nvoiceXVand the background music XBare ﬁnally ob-\ntained by ﬁltering X(2)\nVandX(3)\nBusing Wiener ﬁltering, as\nfollows:\nXV=X(2)\nV\nX(2)\nV+X(3)\nB\fX\nXB=X(3)\nB\nX(2)\nV+X(3)\nB\fX\n4. EV ALUATION\nIn this section, we evaluate the method for modeling the\nsinging voice and the combined approach on a data set of\n1,000 song clips. We also compare them with the method\nfor modeling the background music alone, as well as two\nother state-of-the-art methods.4.1 Data Set\nThe MIR-1K data set2consists of 1,000 song clips in the\nform of split stereo WA V ﬁles sampled at 16 kHz, with the\nbackground music and the singing voice recorded on the\nleft and right channels, respectively. The song clips were\nextracted from 110 karaoke Chinese pop songs performed\nby 8 female and 11 male singers. The durations of the clips\nrange from 4 to 13 seconds [3].\nWe created a set of 1,000 mixtures by summing, for\neach song clip, the left (background music) and right (sing-\ning voice) channels into a monaural mixture\n4.2 Performance Measures\nThe BSS Eval toolbox3consists of a set of measures that\nintend to quantify the quality of the separation between a\nsource and its estimate. The principle is to decompose an\nestimate into a number of contributions corresponding to\nthe target source, the interference from unwanted sources,\nand the artifacts such as “musical noise.”\nBased on this principle, the following measures were\nthen deﬁned (in dB): Sources to Interferences Ratio (SIR),\nSources to Artifacts Ratio (SAR), and Sources to Distor-\ntion Ratio (SDR) which measures the overall error [13].\n4.3 Competitive Methods\nDurrieu et al. proposed a method4based on the modeling\nof a mixture as an instantaneous sum of a signal of inter-\nest (i.e., the singing voice) and a residual (i.e., the back-\nground music), where the singing voice is parametrized as\na source-ﬁlter model, and the background music as an un-\nconstrained NMF model [1]. The parameters of the models\nare then estimated using an iterative algorithm in a formal-\nism similar to NMF. A white noise spectrum is added to the\nsinging voice model to better capture the unvoiced compo-\nnents. We used an analysis window of 64 milliseconds, a\nwindow size of 1024 samples, a step size of 32 millisec-\nonds, and 30 iterations.\nHuang et al. proposed a method5based on Robust Prin-\ncipal Component Analysis (RPCA) [4]. RPCA is a method\nfor decomposing a data matrix into a low-rank component\nand a sparse component, by solving a convex optimiza-\ntion problem that aims to minimize a weighted combina-\ntion of the nuclear norm and the L1norm. The method\nassumes that the background music typically corresponds\nto the low-rank component and the singing voice typically\ncorresponds to the sparse component.\n4.4 Training Universal Models\nOur experiments used a leave-one-out cross validation ap-\nproach. For each of the 19 singers, we learned a univer-\nsal model using NMF on the other 18 singers, with dif-\nferent choices for the number of basis vectors per singer:\nK= 5;10;20;30;40;50;60.\n2http://sites.google.com/site/unvoicedsoundseparation/mir-1k\n3http://bass-db.gforge.inria.fr/bss eval/\n4http://www.durrieu.ch/research/jstsp2010.html\n5https://sites.google.com/site/singingvoiceseparationrpca/4.5 Parameters\nWe used a Hamming window of 1024 samples, correspond-\ning to a duration of 64 milliseconds at a sampling fre-\nquency of 16 kHz, with 50% overlap.\nFor REPET-SIM6, pilot experiments showed that a min-\nimal threshold of 0, a maximal order of 50, and a minimal\ndistance of 0.1 second gave good separation results.\nFor the universal voice model, pilot experiments showed\nthat different settings of K,KB(number of background\nmusic basis vectors), and \u0015yielded optimal results for dif-\nferent measures (see Section 4.2) of the separation quality\nof singing voice and background music. We considered\nK= 5;10;20;:::;60,KB= 5;10;20;30;50;80, and a\nlogarithmic grid of \u0015values.\n4.6 Comparative Results\nFigures 2, 3, and 4 show the boxplots of the distributions\nfor the SDR, SIR, and SAR (in dB) for the background mu-\nsic (left plot) and the singing voice (right plot) estimates,\nfor the method of Durrieu et al. ( Durrieu ), the method of\nHuang et al. ( Huang ), REPET-SIM alone ( REPET ), uni-\nversal voice model alone ( UVM ), and the combination of\nuniversal voice model and REPET-SIM ( combo ). The hor-\nizontal line in each box represent the median of the distri-\nbution, whose value is displayed above the box. Outliers\nare not shown. Higher values are better.\nWe used two parameter settings for the universal voice\nmodel: one that gave the best SDR for the background mu-\nsic estimates ( K= 20 ,KB= 5, and\u0015= 1448 ), and\none that gave the best SDR for the singing voice estimates\n(K= 10 ,KB= 5, and\u0015= 2896 ). The boxplots then\nshow the results for the background music estimates (left\nplots) and the singing voice estimates (right plots) for the\nparameter settings that gave the best SDR, for the universal\nvoice model ( UVM ) and the combination ( combo ).\nThe plots show that the universal voice model alone,\nfor the right parameter settings, achieves higher SDR than\nREPET-SIM and the other state-of-the-art methods, for both\nthe background music and the singing voice estimates. Com-\nbining the universal voice model with REPET-SIM typi-\ncally yields further improvement.\nIf we focus on SIR, for the background music estimates,\nthe universal voice model alone achieves higher SIR than\nREPET-SIM and the other competitive methods; the com-\nbination further increases the SIR. For the singing voice\nestimates, the universal voice model alone achieves higher\nSIR than REPET-SIM and the method of Huang et al., but\nthe combination does no better than the universal voice\nmodel alone.\nOn the other hand, if we focus on SAR, for the back-\nground music estimates, the universal voice model alone\nhas slightly lower SAR than REPET-SIM and the other\ncompetitive methods; the combination further decreases\nthe SAR. For the singing voice estimates, the universal\nvoice model alone has higher SAR than the method of Dur-\nrieu et al.; the combination further improves the results.\n6http://music.eecs.northwestern.edu/includes/projects/repet/codes/repet sim.mFigure 2 . Box plots of the distributions for the SDR (dB).\nThese results show that, given the right parameter set-\ntings, the universal voice model is particularly good at re-\nducing in one source the interference of the other source,\nhowever at the expense of adding some artifacts in the es-\ntimates. This is related to the SIR/SAR performance trade-\noff commonly seen in source separation.\nThe results also show that combining the universal voice\nmodel with REPET-SIM helps to increase the SIR for the\nbackground music estimates and the SAR for the singing\nvoice estimates, but at the expense of decreasing the SAR\nfor the background music estimates and the SIR for the\nsinging voice estimates. This is related to the music/voice\nperformance trade-off commonly seen in music/voice sep-\naration. In other words, the combination helps to reduce in\nthe background music estimates the interference from the\nsinging voice but at the expense of introducing some arti-\nfacts in the estimates. On the other hand, it helps to reduce\nartifacts in the singing voice estimates, at the expense of\nintroducing interference from the background music.\n4.7 Statistical Analysis\nWe compared the SDR of the background music and sing-\ning voice estimates across the different methods using a\ntwo-sided paired t-test. The universal voice model alone\nachieved a signiﬁcantly higher SDR on the background\nmusic than the three state-of-the-art methods: the closest\ncompetitor was REPET-SIM ( t= 3:92,p < : 0001 ). The\ncombination represented a signiﬁcant improvement over\nthe universal model alone ( t= 19:4,p\u00190). A similar\nstory is true for the SDR of the singing voice estimates:\nthe universal voice model alone is signiﬁcantly better than\nany of the existing methods, with the method of Durrieu\net al. the closest competitor ( t= 6:13,p\u00190), and the\ncombination represents a signiﬁcant improvement over it\n(t= 13:8,p\u00190).\nIn terms of the SIR of the background music estimates,\nFigure 3 . Box plots of the distributions for the SIR (dB).\nthe combination is signiﬁcantly better than the universal\nvoice model alone ( t= 37:7,p\u00190), which is signif-\nicantly better than any of the existing methods, with the\nclosest competitor being REPET-SIM ( t= 7:75,p\u00190).\nFor the SIR of the singing voice estimates, the universal\nvoice model is not signiﬁcantly different from the method\nof Durrieu et al. ( t=\u00000:29,p=:77), but signiﬁcantly\nbetter than the other existing methods, and also the combi-\nnation (t= 20:1,p\u00190).\nFinally, for the SAR of the background music estimates,\nthe universal voice model is competitive with REPET-SIM\n(t=\u00001:26,p= 0:21), but signiﬁcantly worse than the\nother competitive methods ( t= 9:61andt= 13:2). On\nthe other hand, in terms of the SAR of the singing voice es-\ntimates, the combination performs signiﬁcantly better than\nthe universal voice model ( t= 50:1), which in turn is sig-\nniﬁcantly better than the method of Durrieu et al. ( t=\n9:69). However, both are signiﬁcantly worse than the other\ntwo competitors, the closest being the method of Huang et\nal. (t=\u000015:6).\nNote that there are 7 tests for each conﬁguration of the\nthree measures (SDR, SIR, SAR) and the two sources (back-\nground music and singing voice): comparing the universal\nvoice model and the combination to each of the three com-\npetitors, and then comparing the universal voice model to\nthe combination. Therefore, we are implicitly conducting\na total of 3\u00022\u00027 = 42 tests. All of the ﬁndings above\nremain signiﬁcant at the \u000b=:05level if we use a Bonfer-\nroni correction to adjust for the 42 tests, corresponding to\na rejection region of jtj>3:25. These results conﬁrm the\nﬁndings in Figures 2, 3, and 4.\n5. CONCLUSION\nIn this work, we proposed a method for modeling the singing\nvoice. The method can learn a singer-independent model\nfrom singing examples using a NMF based technique. WeFigure 4 . Box plots of the distributions for the SAR (dB).\nthen proposed to combine this method with a method that\nmodels the background music. Combining a method that\nspeciﬁcally models the singing voice with a method that\nspeciﬁcally models the background music addresses sepa-\nration performance from the point of view of both sources.\nEvaluation on a data set of 1,000 song clips showed\nthat, when using the right parameter settings, the univer-\nsal voice model can outperform different state-of-the-art\nmethods. Combining modeling of both sources can fur-\nther improve separation performance, when compared with\nmodeling only one of the sources.\nThis work was supported in part by NSF grant number\nIIS-0812314.\n6. REFERENCES\n[1] Jean-Louis Durrieu, Bertrand David, and Ga ¨el\nRichard. A musically motivated mid-level representa-\ntion for pitch estimation and musical audio source sep-\naration. IEEE Journal on Selected Topics on Signal\nProcessing , 5(6):1180–1191, October 2011.\n[2] Jinyu Han and Ching-Wei Chen. Improving melody ex-\ntraction using probabilistic latent component analysis.\nIn36th International Conference on Acoustics, Speech\nand Signal Processing , Prague, Czech Republic, May\n22-27 2011.\n[3] Chao-Ling Hsu and Jyh-Shing Roger Jang. On the\nimprovement of singing voice separation for monau-\nral recordings using the MIR-1K dataset. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n18(2):310–319, February 2010.\n[4] Po-Sen Huang, Scott Deeann Chen, Paris Smaragdis,\nand Mark Hasegawa-Johnson. Singing-voice separa-\ntion from monaural recordings using robust principal\ncomponent analysis. In 37th International Conferenceon Acoustics, Speech and Signal Processing , Kyoto,\nJapan, March 25-30 2012.\n[5] Yipeng Li and DeLiang Wang. Separation of singing\nvoice from music accompaniment for monaural record-\nings. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing , 15(4):1475–1487, May 2007.\n[6] Antoine Liutkus, Zafar Raﬁi, Roland Badeau, Bryan\nPardo, and Ga ¨el Richard. Adaptive ﬁltering for mu-\nsic/voice separation exploiting the repeating musical\nstructure. In 37th International Conference on Acous-\ntics, Speech and Signal Processing , Kyoto, Japan,\nMarch 25-30 2012.\n[7] Alexey Ozerov, Pierrick Philippe, Fr ´ed´eric Bimbot,\nand R ´emi Gribonval. Adaptation of Bayesian models\nfor single-channel source separation and its applica-\ntion to voice/music separation in popular songs. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 15(5):1564–1578, July 2007.\n[8] Zafar Raﬁi and Bryan Pardo. Music/voice separation\nusing the similarity matrix. In 13th International So-\nciety for Music Information Retrieval , Porto, Portugal,\nOctober 8-12 2012.\n[9] Zafar Raﬁi and Bryan Pardo. REpeating Pattern Ex-\ntraction Technique (REPET): A simple method for\nmusic/voice separation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 21(1):71–82, Jan-\nuary 2013.\n[10] Paris Smaragdis, Bhiksha Raj, and Madhusudana\nShashanka. Supervised and semi-supervised separation\nof sounds from single-channel mixtures. In Indepen-\ndent Component Analysis and Signal Separation , pages\n414–421. Springer, 2007.\n[11] Dennis L. Sun and Gautham J. Mysore. Universal\nspeech models for speaker independent single channel\nsource separation. In 38th International Conference on\nAcoustics, Speech and Signal Processing , Vancouver,\nBC, Canada, May 26-31 2013.\n[12] Shankar Vembu and Stephan Baumann. Separation of\nvocals from polyphonic audio recordings. In 6th Inter-\nnational Conference on Music Information Retrieval ,\npages 337–344, London, UK, September 11-15 2005.\n[13] Emmanuel Vincent, R ´emi Gribonval, and Cedric\nF´evotte. Performance measurement in blind audio\nsource separation. IEEE Transactions on Audio,\nSpeech and Language Processing , 14(4):1462–1469,\nJuly 2006.\n[14] Tuomas Virtanen, Annamaria Mesaros, and Matti\nRyyn ¨anen. Combining pitch-based inference and non-\nnegative spectrogram factorization in separating vo-\ncals from polyphonic music. In ISCA Tutorial and Re-\nsearch Workshop on Statistical and Perceptual Audi-\ntion, pages 17–20, Brisbane, Australia, 21 September\n2008."
    },
    {
        "title": "Exploring the Relation Between Novelty Aspects and Preferences in Music Listening.",
        "author": [
            "Andryw Marques Ramos",
            "Nazareno Andrade",
            "Leandro Balby Marinho"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416660",
        "url": "https://doi.org/10.5281/zenodo.1416660",
        "ee": "https://zenodo.org/records/1416660/files/RamosAM13.pdf",
        "abstract": "The discovery of new music, e.g. song tracks and artists, is a central aspect of music consumption. In order to assist users in this task, several mechanisms have been proposed to incorporate novelty awareness into music recommender systems. In this paper, we complement these efforts by investigating how the music preferences of users are affected by two different aspects of novel artists, namely familiarity and mainstreamness. We collected historical data from Last.fm users, a popular online music discovery service, to investigate how these aspects of novel artists relate to the preferences of music listeners for novel artists. The results of this analysis suggests that the users tend to cluster according to their novelty related preferences. We then conducted a comprehensive study on these groups, from where we derive implications and useful insights for developers of music retrieval services.",
        "zenodo_id": 1416660,
        "dblp_key": "conf/ismir/RamosAM13",
        "keywords": [
            "music consumption",
            "novelty awareness",
            "music recommender systems",
            "user preferences",
            "novel artists",
            "familiarity",
            "mainstreamness",
            "historical data",
            "Last.fm users",
            "novelty related preferences"
        ],
        "content": "EXPLORING THE RELATION BETWEEN NOVELTY ASPECTS AND\nPREFERENCES IN MUSIC LISTENING\nAndryw Marques\nUniversidade Federal\nde Campina Grande\nandrywmr@lsd.ufcg.edu.brNazareno Andrade\nUniversidade Federal\nde Campina Grande\nnazareno@computacao.ufcg.edu.brLeandro Balby\nUniversidade Federal\nde Campina Grande\nlbmarinho@computacao.ufcg.edu.br\nABSTRACT\nThe discovery of new music, e.g. song tracks and artists,\nis a central aspect of music consumption. In order to assist\nusers in this task, several mechanisms have been proposedto incorporate novelty awareness into music recommender\nsystems. In this paper, we complement these efforts by in-\nvestigating how the music preferences of users are affected\nby two different aspects of novel artists, namely familiar-\nity and mainstreamness. We collected historical data from\nLast.fm users, a popular online music discovery service,\nto investigate how these aspects of novel artists relate to\nthe preferences of music listeners for novel artists. Theresults of this analysis suggests that the users tend to clus-ter according to their novelty related preferences. We then\nconducted a comprehensive study on these groups, from\nwhere we derive implications and useful insights for de-velopers of music retrieval services.\n1. INTRODUCTION\nThe discovery of new songs and artists that one likes is a\ncentral aspect of music consumption. To a higher or lesserdegree, all music listeners look for novelty over time. Dueto the huge music collections presently available to listen-ers, it becomes increasingly difﬁcult to sift for novel andrelevant music. There is thus a potential for efﬁcient and\nnovelty-aware retrieval services that assist listeners.\nBoth commercial systems and research efforts have made\nprogress in incorporating novelty in music retrieval sys-tems (e.g.. [7,10,11]). In general, novelty concerns an itempreviously unknown to a consumer. For example, a moviestill not watched or a band still not listened to. However,it is also possible to extend this unifaceted view of nov-elty. For example, an item that is both previously unknown\nand has a different style from all other previously known\nitems brings to a consumer an extra dimension of novelty.The distinction of extra dimensions in novelty is relevantas consumers might have preferences for novel items alongmultiple of such dimensions: some users may prefer novel\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieval.items that are similar (or familiar) to their current pref-\nerences, but not mainstream (or popular), and vice-versa.Although sometimes mentioned in the literature, this mul-\ntifaceted view of novelty remains largely unexplored in the\nMusic Information Retrieval ﬁeld.\nIn this work we conduct an exploratory analysis of the\nimpact of different dimensions of novelty in the prefer-ences of music listeners. Our ultimate goal is to elucidatewhat kinds of novelties would be preferred by music lis-\nteners, thus paving the way for more efﬁcient and informa-tive music retrieval services. More concretely, we examine\nhow two important aspects of music novelty – familiarity\nand mainstreamness – affect the preferences of music lis-teners to the new artists they discover.\nOur analysis relies on historical music listening data\nfrom 17,000 Last.fm users (Section 3), and on a model of\nlistening behavior and novelty aspects we propose (Sec-\ntion 4). On the one hand, our results suggests that there is\nno global correlation between familiarity or mainstream-ness and novelty relevance for the listeners in our sam-ple. On the other hand, when considered individually, mostlisteners do have a signiﬁcant correlation between eitherfamiliarity or mainstreamness of novel artists heard andthe relevance they see in these artists (Section 5). Thissuggests that novelty-based personalized music retrieval\nsystems likely have more chances of success than non-personalized ones.\nAnother consequence of this result is that listeners form\ngroups concerning their novelty-related preferences. Thisobservation motivates us to perform a cluster analysis onthe listeners. This analysis unveils seven archetypical pro-\nﬁles that explain how different groups have preferences forthe different aspects of novelty we consider (Section 6).\n2. RELATED WORK\nSeveral studies point to the relevance of considering nov-\nelty in music recommendation systems. In general, differ-\nent efforts agree that it is necessary to evaluate such sys-tems not only by their accuracy and recall, but also by howmany new and relevant items are introduced to a user [4,5].In materializing this view, several studies in the literaturediscuss how to build recommender systems that introducenovelty adequately [7, 10, 11].\nSpeciﬁcally about novelties, V argas et al. [8] put for-\nward a formal framework to analyze the relation betweenusers, items, and novelty in a recommendation system. The\nauthors make a distinction between choice (a user picks anitem), discovery (a user is introduced to an novel item) andrelevance (a user likes an item).\nRelated to the discovery, novel items can be deﬁned in\na twofold manner [1]. On the one hand, novel items have\n(usually content-based) characteristics not shared by itemspreviously declared as relevant by the user [7, 11]. On the\nother hand, novel items are deﬁned in terms of popularity\namong users, mainly non popular items. This attribute ofitems is inﬂuential in their discovery potential [2, 4], thusinterplaying with their novelty.\nWith respect to the reaction to novelty in music, a re-\nlated ﬁeld is that of the reaction to new opinions (in gen-eral). Munson and Resnick [6] ran experimental studiesthat suggest that online users can be clustered into threedistinct subgroups with respect to their preferences for newopinions: diversity-seeking, challenge-averse and support-seeking.\nIn general, although past work has utilized the concept\nof novelty for recommending music and other types of items,and has sometimes dealt with the relation between noveltyand diversity or popularity, the literature currently lacksstudies speciﬁcally formalizing novelty aspects and relat-ing them to the relevance of novel items. This study con-tributes to ﬁll this gap, formalizing the concepts of famil-\niarity and mainstreamness of novel music artists in relation\nto a listener. According to V argas et al.’s framework, we re-\nlate these two aspects with the relevance of items chosen\nby listeners so as to inform the design of discovery mech-\nanisms. Finally, our results with respect to different types\nof listeners with respect to their reaction to novelty bears\nsimilarities to Munson and Resnick’s results.\n3. DATA COLLECTED\nOur dataset is comprised of two parts: the ﬁrst contains the\nsubjects whose behavior will be analyzed, together with\ntheir historical listening habits, and the second concernsmetadata about the artists listened by the chosen subjects.All data was collected from Last.fm through its publicly\navailable data access API.\nFor the remainder of this study, we differentiate be-\ntween the experiment period , set as the six months between\nMarch to September 2012, the observation period , which\nincludes the experiment period and also the following sixmonths, and the prior listening history , which is the year\npreceding the experiment period. The distinction betweenexperiment and observation period is used to control thebias in relevance measuring, and is further discussed inSection 4.4. Figure 1 illustrates the periods on a timeline.\n3.1 Subject data\nOur goal is to identify and collect data about a sample\nof Last.fm users which has been exposed to novel artistsduring a period of interest. Starting with the proﬁle ofthe ﬁrst author, a snowball sampling procedure was con-ducted on the Last.fm friendship network until 100,000\nFigure 1 : Time periods used in the experiment.\nusers were identiﬁed. Next, data about the artists listened\nto by the users prior and during the experiment period werecollected. The former is collected as the set of the 200artists most listened by the user since he or she joinedLast.fm, plus the union of the sets of 100 most listenedartists by the user in each week of his or her prior listen-ing history. The data collected about the listening habitsduring the observation period is the union of the sets of the\n100 most listened artists by the user in each week in this\nperiod.\nAfter this data collection, a ﬁltering process was con-\nducted, with the goal of selecting the users who (i) have\nhighly active listening habits, (ii) are likely to have in-formed a large portion of these habits to the system wecollected the data from, (iii) have experienced a numberof novelties in their listening that enable us to investigatethe relation between novelty characteristics and user pref-erences, and (iv) likely gear most of their music listening,instead of using a Last.fm’s radio created by a recommen-dation algorithm\n1.\nFor (i) and (ii) we kept only users who had at least 100\nartists in their prior listening history, and which were activein at least three quarters of the weeks in the observation pe-riod, having at least 100 song executions in a week to beconsidered active. For (iii), we selected the users with at\nleast 10 novel artists in the experiment period that were lis-\ntened to at least 10 times each. For (iv), we infer that a user\ndoes not chieﬂy use the Last.fm recommendations if that\nuser listened 15 or more songs of a same artist in a week.\nOur assumption is that this denounces a level of gearingincompatible with radio licensing. Finally, we ﬁltered out\nalso a small number of registered users with unrealistically\nhigh music listening frequency (more than 16 hours a day\nduring the observation period). This process produced a\nsample with 17,183 Last.fm users.\n3.2 Artist data\nFor each artist present in the subjects’ listening data, artist\npopularity and tag data was collected from Last.fm. Pop-\nularity data was collected as the number of users who lis-\ntened the artist as informed by Last.fm on March 10th 2013.A tag is a label given by a user to the artist, ranging, inLast.fm, from music genres (eg. rock, samba) to mood(eg. sad, lively) and other contextual metadata (eg. sum-\nmer, lovesong). Each tag has a popularity, reﬂecting howoften users have assigned the tag to a given artist. After\n1http://www.lastfm.com/listencollecting tag data from all artists in the subjects listening\ndata, we ﬁlter out unpopular tags, deﬁned as having popu-larity lower than 0.15 of the most popular tag for the artist.Furthermore, tags denoting personal circumstances, suchasseen live and favorite were manually removed.\n4. CHARACTERIZING LISTENER PROFILES\nAND NOVELTIES\nTo examine how different characteristics of novel items af-\nfect their relevance for listeners, we resort on three con-\nstructs we now describe in turn: a model of a listener pro-\nﬁle, a set of dimensions on which we consider novel items\nmay vary, and a deﬁnition of relevance.\n4.1 Novel Items\nFor our experiment, the items considered are artists, and\nan artist is novel for a subject if two conditions are met:(a) this artist is absent from the subject’s listening historyprior to the experiment period and (b) the artist was lis-tened to at least ﬁve times in a week by this subject during\nthe experiment period. The experiment period is used for\nidentifying novel artists listened by the subject, while thewhole observation experiment period is used for an unbi-ased evaluation of how much attention the subject payed to\nthese novel items (a process detailed in Section 4.4).\nWith this deﬁnition, we identify 652,511 novel items in\nour data, with a subject discovering on average 38.3 (std.\ndev. 31.2) novel artists during our measurement.\n4.2 Listener Proﬁles\nWe model each listener proﬁle as a set of clusters of artists\nin his or her listening history prior to the experiment pe-riod. These clusters are obtained applying the DBScanclustering algorithm [3] to the set of artists known to thelistener are not considered to be novel to the user in ourexperiment.\nArtists are represented as vectors, where each vector\ncomponent represents the number of times a given tag was\nassigned to the artist being considered. More formally,letAbe the set of artists, and Tthe set of tags. Let f:\nA×T→Rdenote the frequency of which a given tag\nt∈Twas assigned to a given artist a∈A.\n2Now, the\nvector representing an artist a∈Ais deﬁned as /vectora:=\n(f(a,t1),f(a,t2),...,f (a,t|T|))The cosine similarity be-\ntween two artists is deﬁned, as usual, as:\ncos(/vectora,/vectora/prime): =/angbracketleft/vectora,/vectora/prime/angbracketright\n/bardbl/vectora/bardbl/bardbl/vectora/prime/bardbl(1)\nThe clusters are then computed based on the cosine sim-\nilarity between the vector representations of artists as men-\ntioned above. The two parameters of the DBScan algo-\nrithm – the minimum number of points for a cluster to be\nconsidered and the minimum similarity for two points in\na cluster – were empirically deﬁned as 3 and 0.875 (resp.)\n2http://www.lastfm.com/apiafter consulting an online community of music aﬁciona-dos\n3about which of several clustering solutions better de-\nscribed members’ taste proﬁles. Proﬁles obtained through\nthis process for our subjects have an average of 5.5 (std.dev. 2.9) clusters found among the artists present in thesubject’s listening history.\n4.3 Characteristics of novelties\nWe consider novel items/artists have two other aspects of\nnovelty besides being unknown to a listener: familiarityand mainstreamness. The former captures the notion thatcharacteristics of the music from an artist may or may notbe familiar to a listener. The latter models the intuition thatthe artist, although not listened, may be known to the lis-tener through other media and the general public acclaim,\nie. mainstream.\n4.3.1 Familiarity\nFamiliarity is deﬁned based on the similarity between an\nartist and those that compose a listener proﬁle. This mod-\nels the intuition that for a jazz fan, novel heavy metal artistsare likely to sound less familiar than a novel artists from asubgenre of jazz. Formally, let P:={C\n1,...,C n}denote a\nlistener’s proﬁle, formed by the clusters of artists Ci. Also,\nlet/vectorcibe the centroid of the cluster Ci, andpibe the pro-\nportion of all song executions by the listener in his or herprior listening history that were from artists in C\ni. Then,\nthe familiarity between an artist aand a listener proﬁle P\nis the weighted arithmetic mean of the similarity betweenthe artist and the centroids /vectorc\ni, withpias weights:\nfam(a,P)=/summationtextn\ni=1cos(/vectora,/vectorci)×pi/summationtextn\ni=1pi(2)\nFigure 2(a) displays the cumulative distribution of the\nfamiliarity of all novel artists in our experiment to their lis-\ntener proﬁles. Notice that there is an overall skew towardsmore familiar artists.\n4.3.2 Mainstreamness\nFor our experiment, mainstreamness is deﬁned as the log\nof the overall popularity of the artist in Last.fm. This def-\ninition aims to capture how likely it is that the artist and\nan opinion about the artist are known to a subject beforethe subject has carefully listened to the artist. This knowl-edge likely comes from other medias, such as mentionsin newspapers or television, and from the subject’s socialnetwork. Our hypothesis is that some users may have apreference for mainstream novel items known to be likedby the general public. Because the artists popularity dis-tribution observed is highly skewed, our analysis uses the\nlog of this popularity in all results reported. Figure 2(b)displays the cumulative distribution of the mainstreamness\nof all novel artists in our experiment. This distribution has\na skew towards highly mainstream artists.\n3A facebook group of music releases named ’Revista Billboard’0.000.250.500.751.00\n0.00 0.25 0.50 0.75 1.0 0Familiarit yCumulative Fre q\n(a) Familiarity.0.000.250.500.751.00\n0246MainstreamnessCumulative Fre q\n(b) Mainstreamness.\nFigure 2 : Cumulative distribution of Familiarity and\nMainstreamness for all novel artists in the experiment pe-riod.\n4.4 Preferences for novel items\nWe measure subjects’ preferences for different types of\nnovel artists gauging how much or how often each novel\nartist was listened to during the observation period. An\nartist/item is thus said more relevant than another if theformer was listened to more times or more frequently than\nthe latter during a time span.\nTo put forward this approach we consider two metrics\nfor item relevance in our analysis. Let δbe the period be-\ntween the ﬁrst time a subject listened to a novel artist andthe end of our observation period, then (a) the total atten-\ntion a subject gives to an artist is the total number of execu-\ntions of songs of this artist during δdivided by the number\nof weeks in δ; and (b) the period of attention of a subject\nto a novel artist is the fraction of the number of weeks in δ\nin which the subject listened at least once to the artist. Be-cause novel artists are found over a period of time, some of\nthese artist have a smaller time window in our experiment\nperiod in which they can be listened to. We tackle this is-\nsue with two measures. First, taking δas the denominator\nof attention metrics limits the potential bias in the count-ing of song executions or weeks of attention. Second, asmentioned in Section 4.1, only novelties discovered in theexperiment period are considered in the analysis, but thewhole observation period is used to count song executions\nand weeks of attention. This gives each novel item a min-imum of six months of observation in our traces, which\nlimits possible biases caused by too short observations.\nFigure 3 shows the cumulative distribution of the total\nattention log and period of attention for each novelty found\nby the subjects in our experiment. In both cases, attention\nis concentrated on a small proportion of the novel itemsdiscovered.\n0.000.250.500.751.00\n0.00 0.25 0.50 0.75 1.0 0Period of Attention Cumulative Fre q\n0.000.250.500.751.00\n11 0 0Total Attention Lo g Cumulative Fre q\nFigure 3 : Cumulative distribution of the Total Attention\nLog and Period of Attention for each noveltyTable 1 : Correlation (Spearman’s coefﬁcient) between\nnovelty characteristics and relevance, analyzing all novel\nitems together.\nDimensions PoA F M\nTotal Attention 0.74 0.04 0.04Period of Attention (PoA) - 0.04 0.05\nFamiliarity (F) - - 0.01\nMainstreamness (M) - - -\n5. PREFERENCES FOR NOVELTY\nCHARACTERISTICS\nOur central research question is to understand how consid-\nering different aspects of novelty can enhance our under-standing about listener preferences. To address this ques-\ntion, we ﬁrst evaluate whether there is a correlation be-tween novelty characteristics – familiarity and mainstream-\nness – and relevance – total attention and attention period.\nTable 1 shows that analyzing all novel items together,\nthere seems to be no relevant correlation between noveltycharacteristics and relevance that is valid for all subjects.On the other hand, analyzing the correlation between nov-\nelty characteristics and relevance for the novel items of\neach subject individually, there is a different pattern. For\neach pair of variables we examine, there is a signiﬁcantcorrelation (above 0.15 or below -0.15) between the vari-\nables for approximately one third of all subjects (Figure\n4). In aggregate, 70% of all subjects have in their data a\nsigniﬁcant correlation between at least one of the novelty\ncharacteristics and a measure of relevance.\nTaken together, our results point that while there is no\noverall common behavior regarding subjects’ preferences\nfor different types of novelty, most users have some prefer-\nence for a type of novelty in their listening behavior .\n6. LISTENER GROUPS WITH RESPECT TO\nNOVELTY\nThe analysis of the correlation between novelty types and\npreferences indicates that there are different types of lis-tener in our data. In this section we apply clustering analy-\nsis to delve further in the identiﬁcation and analysis of such\ngroups. For that, we use only the set of subjects for which\nat least one of the correlations analyzed in the previous\nsection is not in the [-0.15; 0.15] interval (total=12,114).\nOur analysis uses the Ward hierarchical clustering method\n[9] considering normalized versions of two dimensions that\ndescribe each subject’s preferences for novelty aspects and\ntwo dimensions that measure the subject’s usual musicaltaste. The dimensions related to preferences are (a) thecorrelation between total attention devoted to a novel artistand the artist’s familiarity for the listener and (b) the cor-relation between total attention to a novel artist and theartist’s mainstreamness. To represent usual tastes, we use(c) the average mainstreamness of the artists in the lis-tener’s proﬁle, and (d) the number of clusters present inthe subject’s listening proﬁle. Upon examination, the cor-(a) Period of Attention and Main-\nstreamness\n(b) Period of Attention and Famil-iarity\n (c) Total Attention and Familiarity\n (d) Total Attention and Main-streamness\nFigure 4 : Distribution of the Spearman correlations coefﬁcient between novelty characteristics and relevance in each\nlistener’s data. Shaded areas highlight the portion of subjects with correlation higher than 0.15 or lower than -0.15.\n5.705.60−0.12−0.14\n4.405.800.210.16\n6.905.30−0.110.17\n4.085.600.17\n3.405.600.19−0.11\n9.105.500.180.00\n3.904.800.10−0.03Fond of surprises Mainstream upholder Mainstream explorer Crowd follower\nNiche radical Highly eclectic Underground#clustersavg Mainstreamnesscor(fam, TA)cor(mainst, TA)\n#clustersavg Mainstreamnesscor(fam, TA)cor(mainst, TA)\n−2−10 1 2 −2−10 1 2 −2−10 1 2\nvariable z −scorefactors\nnovelty−related\nprofile−related0.00\nFigure 5 : Centroids of the seven clusters found in the analysis. V ariables as normalized as the z-score (horizontal axis),\nwhere zero represents the average across all listeners, and the unit of variation is a standard deviation in the consideredmetric. In the vertical axis, fam stands for familiarity, mainst for maisntreamness and TA for total attention. The numberson the chart are the denormalized values.\nrelations between novelty aspects and period of attention\nfor novel artists were discarded as redundant dimensions\nfor the clustering analysis.\nApplying this approach points to satisfying clustering\nsolutions with respect to intra and intergroup heterogeneity\nthat have between 5 and 8 clusters. Further examinationconsidering the descriptive power of the solutions leads tothe choice of the 7-cluster solution, whose centroids areshown in Figure 5. Analyzing these centroids, we labelledthe groups as follows:\n1.F ond of surprises : The largest cluster (n=2,738), con-\ntains subjects with a distinct preference for novelty\nthat is both unfamiliar and of lower mainstreamness.\n2.Mainstream upholder : Subjects with a listening his-\ntory marked by mainstream artists, and which clearly\nprefer novelty of higher mainstreamness and famil-iarity (n=1,552).\n3.Mainstream explorer : Listeners with noted prefer-\nence for novel artists from the mainstream, but who\nvalue new artists that are not familiar. Seem to be\nexploring and discovering relevant artists among the\nmainstream that were previously outside their pro-ﬁle, which is itself formed by a high number of clus-\nters (n=1,535).\n4.Crowd follower : Subjects with a limited number of\nclusters in their listening proﬁle, and with a clearpreference for mainstream novelty (n=2130).5.Niche radical : Listeners who habitually focus on a\nsmall number of clusters of artists, and which have a\nstrong preference for novelty familiar and of below-\naverage mainstreamness, likely in one or more niches(n=1,172).\n6.Highly eclectic : Somewhat the opposite of niche rad-\nicals, these are subjects whose listening proﬁles hadhigh number of artist clusters, and who value noveltyalready familiar (n=1,874).\n7.Underground : Subjects who usually listen to artists\nfar from the mainstream, have a relatively homoge-neous listening proﬁle, and prefer novelty close tothis proﬁle (n=1,143).\nInterestingly, the largest single cluster in our results —\nand simultaneously one of the most distinct of them — isthat of subjects fond of surprises. These are listeners whoexhibit a clear preference for diversity and novelty amonglesser known artists. Our results point that subjects in this\ngroup are interested in discovering new artists in the long\ntail of popularity, and at the same time exploring unfamiliar\ngenres, even though their proﬁle is, on average formed bymainstream artists. As an example of fond of surprises,a listener has a proﬁle composed by Indie artists, but he\npreferred as novel artists some Country unpopular artists.\nDiametrically opposite to subjects in this cluster, those\nin the niche follower group seem eager to discover moremusic similar to that they have listened in the past. More-\nover, what was listened in the past is focused on a smallarea of the artists space, and novelty is more appreciated ifit has low mainstreamness.\nNot surprisingly, taken together, subject groups related\nto a marked preference for mainstream novel artists (main-\nstream upholders, mainstream explorers and crowd follow-\ners) form a large portion (43%) of our sample. Never-theless, we see a distinction between listeners who havea markedly mainstream listening history prior to the exper-iment (mainstream upholders), and those who don’t. As\nan example of mainstream upholder, a listener has a pro-\nﬁle composed by popular Indie artists and he liked others\npopular Indie artists as novel items.\nMainstream explorers seem to be subjects who discov-\nered a set of new mainstream artists during the experimentperiod that are different from their previous proﬁle. As aninstance of this group, a listener has a proﬁle composed ofSludge Metal and Hardcore artists, but he preferred some\npopular Hip-Hop artists as novel items. Crowd followers,\non the other hand, are subjects with a simpler proﬁle previ-\nous to the experiment, and which enjoined novelty accord-ing to mainstreamness, irrespective of similarity to theirprevious proﬁles.\nThe underground cluster seems to capture listeners who\ndevote their attention to a set of artists extremely differ-ent from the mainstream. Finally, the highly eclectic clus-ter models the proﬁle of listeners who transit among manyartist clusters, but which have marked preferences for nov-elty based on this diverse proﬁle.\n7. DISCUSSION AND IMPLICATIONS\nThe main objective of this study is to investigate the use\nof multiple dimensions of novelty to further understand\nlisteners’ preferences regarding novel artists. This under-standing, in turn, is aimed at improving the design of musicinformation retrieval systems.\nOur main ﬁndings are threefold. First, there is no over-\nall correlation between familiarity of mainstreamness and\nrelevance in the novel artists discovered by our subjects.\nSecond, when considered individually, most subjects had a\nclear correlation between either familiarity or mainstream-\nness and the relevance seen in novel artists. Third, it ispossible to cluster listeners with some correlation betweennovelty aspects and preferences in seven groups that revealhow different audiences approach novelty.\nConsidered together, these results point to the need of\na personalized approach in assisting a listener to discover\nrelevant novel artists. Moreover, this personalization shouldbe related not only to which other artists have been en-joyed before, but should be also aware that some listen-ers have clear preferences regarding how mainstream orfamiliar novel artists are. The information that these di-mensions are relevant to model listeners’ behavior, and thatit is highly variable among a listener population should beconsidered in the design of future mechanisms. On anotherperspective, the groups we ﬁnd to describe the archetypicalbehaviors among our subjects can be used to develop inter-\nfaces or mechanisms that target different types of users.\nThere are a number of opportunities on our study and\ndirections in which future work could expand it. Notedly,considering data from a population outside Last.fm is nec-essary to evaluate the generalizability of our results. Also,our clustering is aimed at a descriptive analysis. Finding\nthe listener clustering that provides maximum efﬁcacy gain\nto a predictive model is likely a fruitful avenue of work.Finally, the validation of the use of familiarity and main-streamness as dimensions of novelty in an experiment of\nmusical recommendation is necessary to further validatethe use of multiple dimensions to address novelty.\n8. REFERENCES\n[1] A. Bellogin, I. Cantador and P . Castells “A Study of\nHeterogeneity in Recommendations for a Social Music\nService,” Proc. of the 1st HetRec , pp. 1–8, 2010.\n[2] `O. Celma, P , Herrera: “A new approach to evaluat-\ning novel recommendations,” Proc. of the ACM RecSys\n2008 , pp. 179–186, 2008.\n[3] M. Ester, H. Kriegel, J. Sander and X. Xu: “A density-\nbased algorithm for discovering clusters in large spatialdatabases with noise,” Proc.of the ACM KDD, pp. 226-\n231, pp. 226–231, 1996.\n[4] J. L. Herlocker, J. A. Konstan, L. G. Terveen and J. T.\nRiedl “Evaluating collaborative ﬁltering recommender\nsystems,” ACM Trans. Inf. Syst. , V ol. 22, No. 1, pp. 5–\n53, 2004.\n[5] S. M. McNee, J. Riedl and J. A. Konstan “Being ac-\ncurate is not enough: how accuracy metrics have hurtrecommender systems,” Proc. of CHI ’06 , pp. 1097–\n1101, 2006.\n[6] S. Munson and P . Resnick “Presenting diverse polit-\nical opinions: how and how much,” Proc. of CHI ,\npp. 1457–1466, 2010.\n[7] M. Nakatsuji, Y . Fujiwara, A. Tanaka, T. Uchiyama, K.\nFujimura, T. Ishida: “Classical music for rock fans?:\nnovel recommendations for expanding user interests,”\nProc. of the 19th ACM CIKM , pp. 949–958, 2010.\n[8] S. V argas and P . Castells: “Rank and relevance in nov-\nelty and diversity metrics for recommender systems,”\nProc. of the ACM RecSys 2011 , pp. 109–116, 2011.\n[9] J. H. Ward Jr. “Hierarchical Grouping to Optimize an\nObjective Function,” Journal of the American Statisti-\ncal Association , V ol. 58, No. 301, pp. 236-244, 1963.\n[10] Y . C. Zhang, D. O. S ´eaghdha, D. Quercia and T. Jam-\nbor: “Auralist: introducing serendipity into music rec-ommendation,” Proc. of the WSDM , pp. 13–22, 2012.\n[11] C. Ziegle, S. M. McNee, J. A. Konstan, G. Lausen:\n“Improving recommendation lists through topic diver-\nsiﬁcation,” Proc. of the 14th WWW , pp. 22–32, 2005."
    },
    {
        "title": "Hierarchical Classification of Carnatic Music Forms.",
        "author": [
            "H. G. Ranjani",
            "T. V. Sreenivas"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415182",
        "url": "https://doi.org/10.5281/zenodo.1415182",
        "ee": "https://zenodo.org/records/1415182/files/RanjaniS13.pdf",
        "abstract": "We address the problem of classifying a given piece of Carnatic art music into one of its several forms recognized pedagogically. We propose a hierarchical approach for classification of these forms as different combinations of rhythm, percussion and repetitive syllabic structures. The proposed 3-level hierarchy is based on various signal processing measures and classifiers. Features derived from short term energy contours, along with formant information are used to obtain discriminative features. The statistics of the features are used to design simple classifiers at each level of the hierarchy. The method is validated on a subset of IIT-M Carnatic concert music database, comprising of more than 20 hours of music. Using 10 s audio clips, we get an average f-ratio performance of 0.62 for the classification of the following six typesof Carnatic art music: /AlApana/, /viruttam/, /thillAna/, /krithi/, /thaniAvarthanam/ and /thAnam/.",
        "zenodo_id": 1415182,
        "dblp_key": "conf/ismir/RanjaniS13",
        "keywords": [
            "Carnatic art music",
            "pedagogical forms",
            "hierarchical approach",
            "signal processing measures",
            "classifiers",
            "rhythm",
            "percussion",
            "repetitive syllabic structures",
            "3-level hierarchy",
            "formant information"
        ],
        "content": "HIERARCHICAL CLASSIFICATION OF CARNATIC MUSIC FORMS\nRanjani. H. G.\nDept. of Electrical Communication Engineering,\nIndian Institute of Science, Bangalore - 12, India\nranjanihg@ece.iisc.ernet.inT. V . Sreenivas\nDept. of Electrical Communication Engineering,\nIndian Institute of Science, Bangalore - 12, India\ntvsree@ece.iisc.ernet.in\nABSTRACT\nWe address the problem of classifying a given piece of\nCarnatic art music into one of its several forms recognized\npedagogically. We propose a hierarchical approach for\nclassiﬁcation of these forms as different combinations of\nrhythm, percussion and repetitive syllabic structures. The\nproposed 3-level hierarchy is based on various signal pro-\ncessing measures and classiﬁers. Features derived from\nshort term energy contours, along with formant informa-\ntion are used to obtain discriminative features. The statis-\ntics of the features are used to design simple classiﬁers at\neach level of the hierarchy. The method is validated on\na subset of IIT-M Carnatic concert music database, com-\nprising of more than 20 hours of music. Using 10 s au-\ndio clips, we get an average f-ratio performance of 0.62\nfor the classiﬁcation of the following six typesof Carnatic\nart music: /AlApana/, /viruttam/, /thillAna/, /krithi/, /thani-\nAvarthanam/ and/thAnam/ .\n1. INTRODUCTION\nCarnatic classical music is an aesthetic art form of South\nIndia. A typical Carnatic concert is rich in music content,\nand has structures at various levels. For example, use of\nvarious /rAga/1(melodic framework) and /tALa/ (rhyth-\nmic framework) in a concert is well known [1]. Addition-\nally, the concert can be lead-vocal or lead-instrumental in\nnature [2]. Apart from this, a typical concert comprises of\na mix of various forms of Carnatic music, such as, /AlA-\npana/, /thAnam/, /krithi/, /viruttam/, /thani-Avarthanam/,\n/thillAna/ and/swarakalpana/ [3]. All of these forms fall\nunder any of two broad categories: /manOdharma/ (ex-\ntempore) or /kalpita/ (those already composed) rendering\n[1, 4].\nIn this work, we explore different features from the mu-\nsic signal to classify the given piece of Carnatic vocal con-\ncert music in one of the several forms, i.e., /AlApana/ ,\n/thAnam/ ,/krithi/ ,/viruttam/ ,/thani-Avarthanam/ or\n1All the Carnatic music terms are written in emphasized text within\n/./, to indicate phonetic pronounciation\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval./thillAna/ . This kind of classiﬁcation will ﬁnd application\nin segmenting a concert into different parts, as many stu-\ndents of music and connoisseurs ( /rasikA/ ) want to concen-\ntrate on one of the parts for either learning or enjoyment.\nSimilarly, people would like to access these different parts\nof a concert from a database of Carnatic music, again for\nspeciﬁc listening or improvising.\nTo the best of our knowledge, there is little or no work\nto automatically classify a given piece of Carnatic music\ninto its various forms or classes. There exists no single\nfeature set that can classify all the classes, owing to the\nvarying nature of the signals. In addition, as can be ex-\npected of music, a sub-set of the classes exhibit common\noverlapping structures, while the rest differ. A hierarchical\napproach can organize the classes to groups and attribute\neach such group with a discriminative feature set based on\nthe domain-knowledge. We propose acoustic features to\nrepresent and distinguish each class at each hierarchy level.\nThe paper is organized as below: Details of the forms of\nmusic is elaborated in Section 2. In section 3, we present\nthe proposed hierarchical structure. The feature set ex-\ntracted and the design of classiﬁers are detailed in Section\n4. Section 5 summarizes the experiments along with the\nresults followed by discussion and conclusions in section\n6.\n2. CARNATIC MUSIC FORMS\nCarnatic music has a long, rich and illustrious tradition and\nhence many forms of the music exist through the /guru-\nshishya parampara/ [1, 4]. Broadly, in a concert which\nlasts about 2-3 hours, some of the main forms are presented\nusing construed musical ideas of the performer. These are\nsung (or played) in the context of different /rAga/ and dif-\nferent /sAhitya/ (lyrics) to create an enjoyable concert for\nthe audience.\nWe expand on the details of some of the forms in a Car-\nnatic music concert. These are, however, by no means ex-\nhaustive. One minute long example excerpts, for each of\nthese forms, can be downloaded from:\nhttp://www.ece.iisc.ernet.in/ \u0018ranjanihg/audio/index.html\n2.1/AlApana/\n/AlApana/ is a purely extempore melodic form of music.\nThe exposition of a /rAga/ (melodic framework of Indian\nclassical music) with no rhythm is called the /AlApana/ . It\ncomprises of a sequence of phrases sung intended to create\na mood for the subsequent composition, i.e., /krithi/ . The\nperformer chooses the phrases based on the grammar ofthe/rAga/ and elaborates systematically. The duration of\nan/AlApana/ can vary from 1 - 45 minutes and primarily\ndepends on the taste and mood of the performer and also\nthe subsequent composition. Typical vocal syllables used\nin/rAga AlApana/ are/ta/, /da/, /na/ and /ri/ .\nIn vocal concerts, the /AlApana/ comprises of extem-\npore presentation of phrases by the lead artist, which would\nbe pursued by the melodic-instrumental accompanist(s);\noccassionally this is followed by a solo /AlApana/ perfor-\nmance by the melodic-instrument accompanist(s) too.\n2.2/thAnam/\nA/thAnam/ is also a melodic improvisation form in Car-\nnatic music. In its present day form, /thAnam/ is rendered\nas a part of /rAgam-thAnam-pallavi/ (or popularly RTP)\nof a concert. A /thAnam/ blends melody with medium-\npaced rhythm ( /laya/ ) and brings about the intricacies of\nthe/rAga/ . The syllables used are /aa/, /nam/, /tham/,\n/na/, /thom/, /tha/, /nom/ , such that the audience perceive\nthe word ‘ /anantham/ ’ (endless) and ‘ /Anandam/ ’ (happi-\nness). Most often, /thAnam/ is rendered without percus-\nsion instruments.\n2.3/viruttam/\nA/viruttam/ is a devotional and metrical verse in one of\nthe (Indian) languages, sung as improvised music. It gen-\nerally is elaborated in a /rAga/ or a /rAgamAlika/ (where\nsequence of /rAgas/ used for each verse of a given compo-\nsition). Though the verse has a metric structure, /viruttam/\nis devoid of rhythm in the rendering and hence may not be\nidentiﬁable in the song.\n2.4/thani-Avarthanam/\nIn Carnatic concerts, /thani-Avarthanam/ are percussion\nsolo performances which exhibit creative and technical skill.\nIt, generally, follows the main performance in a concert. In\npresence of multiple percussionists, alternate performances\nfollowed by group is the convention. Through out the per-\nformance the tempo of the performance is maintained.\n2.5/thillAna/\nA/thillAna/ is a popular and energetic performance, orig-\ninated for dance performance, and realized during the end\nof a music concert. A major part of this rhythmic per-\nformance, comprises of a limited set of beat synchronous\nmelodic utterances of meaningless rhythmic syllables. Few\nexample syllables are /dheem/ /ta/ /na/ /dhir/ , /tha/ /ki/ /ta/\n/jham/ , and the likes. Another feature of this form is that\nthe ﬁnal verse of a ‘ /thillAna/ ’ consists of lyrics similar to\na/krithi/ .\n2.6/krithi/\nA/krithi/ in Carnatic music is a pre-composed piece of\nmusic or /kalpita sangeeta/ . It comprises of sub-structures:\n/pallavi/ ,/anupallavi/ and/charana/ which roughly corre-\nspond to refrain and verses in Western music. Some com-\npositions also include /chittai swaras/ , which are made of\nonly the Indian solfege syllables i.e.,( /sa/ /ri/ /ga/ /ma/ /pa/\n/da/ /ni/ ).2\n2Some more ﬁner aspects to a /krithi/ such as /niraval/ ,/varnam/ ,\n/mangalam/ ,/gIthe/ exist, which we have currently grouped into one class\n/krithi/ .2.7/swara kalpana/\nA/swara kalpana/ is also an improvised melodic perfor-\nmance with rhythm and the rendition uses only Indian solfege\nsyllables. The performer must conform to the grammar of\nthe chosen raga along with the rules of swarakalpana. This\nis an extempore performance or /manOdharma sangeeta/ ,\nthough for a naive listener, this form is very similar to\nthe/chittai swara/ section of the /krithi/ , which is /kalpita\nsangeeta/ .\n3. PROPOSED APPROACH\nFrom the above discussed forms, it is clear that we have\nto examine both melodic properties as well as rhythmic as-\npects, in grouping the different Carnatic music forms. Fur-\nther, we can exploit certain speciﬁc syllable vocalizations\nwhich have become well accepted conventions among sing-\ners. Because of such high level features of the music form,\nwe cannot ﬁnd one feature set which will permit us to do\na single level multi-class classiﬁcation. Additionally, fea-\ntures chosen for discriminating between a chosen set of\nclasses, may have no impact on other classes, owing to\nthe presence of simultaneous structures among the differ-\nent classes. Instead, we resort to different feature set at dif-\nferent levels and propose a hierarchical approach to clas-\nsify a given piece of Carnatic vocal music into one of the\nabove forms (Section 2).\nFigure 1 . [Color online] Proposed hierarchical classiﬁca-\ntion of Carnatic music forms. Text in blue indicates pro-\nposed hierarchy classiﬁcation.\nThe classes are hiearchically divided as shown in Fig-\nure 1. At the root level (Node R), a grouping based on\nwhether the form has a /tALa/ framework or not, leads\n/thAnam/ ,/krithi/ ,/thillAna/ ,/swarakalpana/ and/thani-\nAvarthanam/ in the former (Node B) and /AlApana/ and\n/viruttam/ in the latter (Node A). The /tALa/ -based mu-\nsic pieces can be further classiﬁed based on whether they\ncontain percussion instruments (in a concert) along with\nmelody leading to a divide between /thillAna/ ,/krithi/ &\n/thani-Avarthanam/ and/thAnam/ , seen as Nodes C and\nD in Figure 1. However, the presence of percussion for\n/swara-kalpana/3is a choice of the percussion artist and\nhence can belong to either of the two nodes. At Node C, we\ncan further introduce another branch based on the presence\n3,*It is not yet clear as to how to distinguish this from a possible /chit-\ntai swara/ and hence, we consider this as a part of /krithi/ , in this work.of the instrument, and thus separate /thani-Avarthanam/\nfrom the rest, if presence of percussion-only instrument\nis detected. This can be approached by incorporating in-\nstrument or instrument class identiﬁcation [12]. However,\nthe current approach intends to minimize the depth of the\nhierarchy so as to reduce propagation of errors [13].\n4. FEATURE SETS AND CLASSIFIERS\nWe consider features relevant to each node in the hierarchy\nso as to emphasize the discriminative capabilities between\nthe select classes.\n4.1 Presence or absence of /tALa/ : (Root Node R)\n4.1.1 Rhythmogram\nConsider atthsegment of a music signal x(n)as,\ns(t;m) = [x(n);n= (t\u00001)HE+m],m= (1 :NE),\nwhereNEis the segment length and HEis the segment\nhop. LetEtbe the short-time energy of the tthsegment of\nthe music-signal, calculated as Et=PNE\nm=1s2(t;m). The\nwindow length NE, corresponding to 32ms, and a win-\ndow hopHEto10msare considered, similar to speech\nanalysis, motivated by the suitable frequency and time res-\nolutions, respectively.\nConsider the sequence, [ Et,t= (1 :N)], which we\nfurther segment as: E(r;\u001c) = [Et; t= (r\u00001)Hac+\n\u001c]; \u001c= (1 :Nac), whereNacis the energy segment length\nandHacis the energy segment hop. We refer to the short-\ntime energy signal as the ‘novelty signal’. The parameters\nHacandNacmust be chosen to suit the slowest tempo in\nthe music. We have chosen, Nacto correspond to 10s\nandHacto50ms. We, then, compute the time-varying\nautocorrelation function of E(r;\u001c)given as:REE(r;k) =PNac\u0000k\n\u001c=1E(r;\u001c)E(r;\u001c+k), and,k= 0 : (Nac\u00001). This\nis the autocorrelation of the rthnovelty signal segment for\nthekthlag. Figure 2 shows this novelty-autocorrelation\nfunction as it evolves over time.\nFigure 2 . [Color online] Plot of jREE(r;k)j, the\nnovelty-autocorrelation function for an audio clip con-\nsisting of /alapana/ followed by /krithi/ , as a 3D plot.\nColor=jREE(;)j.\nThe peaks in autocorrelation lags capture the long term\nperiodicity of the energy signal, and are indicative of the\nmetric structure of the /tALa/4. This has been refered to\nas the rhythmogram [5]. (The method described above\ndiffers from [5], in taking autocorrelation of short-time\nenergy in former, while use of autocorrelation of onsets\n4It can be seen that slower the tempo, the peaks in jREE(r; k)jwill\nbe farther apart.is considered in [5].) The rhythmogram can be made dis-\ncrete by picking Ppeaks of the rhythmogram for every rth\nsegment of the energy signal. We refer to this as thresh-\nolded binary-rhythmogram, denoted as Rb(r;flig)where\nfligindicates the ithpeak location for i= [1 :P]./tALa/\nstructure, if present, can be well observed using this as a\nfeature as shown in Figure 3.\nIn the Indian music context, tempo is a choice of the\nsinger/ performer apart from being related to the compo-\nsition.5Thus, for Node-R classiﬁcation, we only need a\nmeasure to check the presence of /tALa/ ; hence, we can use\nsimple features (unlike in the literature which is mainly for\nWestern music [8, 9] and uses the tempo to aid in classiﬁ-\ncation or segmentation).\nFigure 3 . Plot ofRb(r;flig)for the audio clip correspond-\ning to Figure 2, for P= 10\nFigure 4 . Plot of variation of Spectral Flatness Measure\nfor the data corresponding to Figure 3, for P= 10\n4.1.2 Classiﬁcation at Root Node R\nWe can see from Figure 3 that the peak locations, flig,\nshow consistency in regions of music corresponding to pres-\nence of /tALa/ as against those which do not have /tALa/ .\nBy constructing a histogram of the peak locations, we can\nexpect peaky distributions, Pr(k)for regions containing\n/tALa/ , while the non- /tALa/ sections would have wider\nspread in the histogram. Motivated by this, we use spectral\nﬂatness measure (SFM or the GM:AM ratio) of the his-\ntogram ofRb(r;flig)which can be represented as:\nPr(k) =# of peaks with li=k;overr= 1 :R\nTotal # of peaks at all k\nSFM (r) =[QK\nk=1Pr(k)]1\nK\n1\nKPK\nk=1Pr(k);and,K,Nac\u00001\nThe evolution of SFM across time for the music clip con-\nsidered in Figure 3 is shown in Figure 4. We propose\na low SFM as a score to indicate the presence of /tALa/ ,\nwith 0.5 value denoting boundary between the two classes.\nThe effectiveness of the estimated distribution of peaks can\n5It can be argued that picking Ppeaks makes the feature invariant to\ntempo w.r.t further processingbe increased by considering more points from Rb(r;flig).\nHence, a trade-off between performance and duration, R\ncan be expected. A value corresponding to 10sis chosen\nforR.\n4.2 Percussive Vs Non-percussive: (Node B)\n4.2.1 Onset strength\nPercussion instruments give rise to stronger onsets, than\nmelody based instruments, owing to their sharp attack pat-\ntern [6, 7]. The method to calculate onsets is briefed as\nfollows: The energy signal, Et, used for rhythmogram ear-\nlier is again considered. The positive ﬁrst order difference\nfunction,E0(r;\u001c)+= (E(r;\u001c)\u0000E(r;\u001c\u00001))+6is com-\nputed; depending on the instrument and its corresponding\nattack pattern, this difference will be large (for percussive)\nor small (for non-percussive). The positive ﬁrst order dif-\nferences can be an estimate of onset strength at the rth\nframe, or,E0(r; \u001c)+2R+.\nLetPu(E0\n+jr)denote histogram of E0(r;\u001c)+, (onset\nstrength) for a 10 s audio clip containing solo voice (singing\nwith /tALa/ ) and solo voice with percussion is shown in\nFigure 5. It can be seen that these distributions can be mod-\neled using an exponential distribution with a signiﬁcantly\ndifferent parameter, \u0015.\n(a) (b)\nFigure 5 . Histograms Pu(E0jr)for (a) soft onsets and (b)\nhard onsets of /tALa/ class of data from randomly chosen\n10ssegments of non-percussive and percussive kind.\nEstimating the parameter of the exponential distribu-\ntion, using the maximum likelihood framework, we get:\n\u0015+(r)=NosP\n\u001cE0(r;\u001c)+, whereNosis the number of samples\nin the segment considered. Figure 6 shows a sample plot\nof variation of \u0015+(r)over successive rsegments, for an\naudio concert clip reﬂecting the change from /thAnam/ to\n/krithi/ . The parameter is calculated for every 0.1 s, for a\nwindow size Noscorresponding to Nac, ie., 10 s.\nFigure 6 .\u0015+(r)variation as a function of data segment,\nestimated for an audio clip transiting from /thanAm/ to\n/krithi/ .\nA similar approach is seen in [10], where a Gaussian\ndistribution is used to check the presence of onsets. Our\n6We deﬁne a function, (x)+,max(0 ; x)approach differs not only in using exponential distribution,\nbut also in using it for discriminating between soft and hard\nonsets.\nFigure 7 . [Color Online] Histograms of, Pu(\u0015+jr),\ndepicting parameter distributions for 2 audio ﬁles repre-\nsenting (red) non-percussive and (black) percussive /tALa/\nclass.\n4.2.2 Classiﬁcation at Node B\nFrom Section 4.2.1, estimated \u0015+parameter is seen to be\neffective in discriminating between forms of music (con-\ntaining /tALa/ ) that contain percussive and non-percussive\ninstruments. Characterizing the same, across varied perfor-\nmances, shows that \u0015+distribution for percussive instru-\nments is as shown in black and in red for non-percussive,\nshown in Figure 7. The deviation from the prototype pa-\nrameters characterized by mean and variance of the distri-\nbutions, provides correctness scores of classiﬁcation.\n4.3/AlApana/ Vs/viruttam/ : (leaf nodes of A)\n4.3.1 Formant peaks as features\nThe differentiator between /AlApana/ and/viruttam/ music\npieces is that, the former uses limited syllables compared\nto the latter. /viruttam/ , which gives more importance to\narticulation, so that the lyrics are heard clearly. (F1;F2)\nare estimated using Praat software [11] (Burg’s method)7,\nusing a segment length Nf= 32ms, and segment hop\nHf= 10ms. Figure 8 depicts a histogram of (F2\nF1)\nof both the classes. It can be observed that there is lesser\n(F2\nF1)variation in the alapana signal. We can thus use the\nrate of movement of (F2\nF1)as a discriminative feature set in\nthe rhythm-less form of Carnatic music.\nFigure 8 . Histogram of (F2\nF1)for a 10 s of audio containing\n(a)/viruttam/ (b)/alapana/ for a 10ssecond clips from\nrandomly chosen example clips of the classes.\n4.3.2 Classiﬁcation for /AlApana/ and/viruttam/\nSection 4.3.1 illustrated the effectiveness of (F1;F2)to\ndiscriminate between /AlApana/ and/viruttam/ . It is ob-\nserved that AlApana is more likely to have realizations of\n7Complexity in estimating reliable formants for high pitched sounds\nis well known and is beyond the scope of current paper./a/, yielding a choice (F2\nF1)a\u0018N(\u0016a;\u001ba), with param-\neters (\u0016a;\u001ba) = (2:5;0:3), with suitable Bayes’ decision\nboundary. Thus, classiﬁcation is based on maximum like-\nlihood, where the likelihood is calculated for the ratio of\nmean of the formants extracted from 10 s data.\n4.4/thani-Avarthanam/ Vs /thillAna/ Vs /krithi/ (leaf\nnodes of C)\n4.4.1 Cessation strength as feature\nBetween /krithi/ and/thillAna/ , from Sections 2.5, 2.6 and\naudio clips, one can prominently observe that /thillAna/ ,\ninspite of presence of vocals, give a stronger perception\nof beat cycle than in the case of a /krithi/ . On closer ob-\nservation, one can attribute it to the regular cessation of\nenergy in /thillAna/ , owing to the syllables used (which\nmainly comprise of prominent stops as onsets of the syl-\nlables). This cessation of energy can be gauged as ‘off-\nset’. We observe that the strength of this offset is higher\nfor/thillAna/ than for /krithi/ . Using terminology similar\nto Section 4.2.1, we can express E\u000e(r;\u001c) = (E(r;\u001c)\u0000\nE(r;\u001c\u0000\u000e))\u00008, whereE\u000e(r;\u001c)2R\u0000. The parameter\n\u000e, is required as the cessation cannot be instantaneous, re-\nquiring\u000eduration to effect a noticable change; or in other\nwords, it is a gradual offset. A sample histogram depicting\nthis is shown in Figure 9. Hence, we can use the exponen-\ntial parameter, \u0015\u0000, as a parameter to differentiate between\n/krithi/ and/thillAna/ , which consist of sustained energy\nowing to vocal or instrumental content.\n(a) (b)\nFigure 9 . Histogram Pu(E\u000ejr)ofE\u000e(r;\u001c)for a 10 s of\naudio containing (a) /krithi/ (b)/thillAna/\n4.4.2 Dynamic range of the energy contours\nThe spectral content of /thani-Avarthanam/ , the percussion-\nonly performance, comprises of mainly short lived bursts\nof energy, or “vertical lines” in the spectrogram. A periodic\nand large variation in the intensity as a function of time can\nbe observed due to strong onsets and offsets, and no sus-\ntaining period, resulting in lower average intensity of the\naudio signal. Figure 10, depicts the distribution of average\nintensity in 10sclips of /thani-Avarthnam/ and/thillAna/ ,\nwhich can be used as a feature to distinguish these classes.\n4.4.3 Classiﬁcation combining the offset and dynamic\nrange\nSimilar to Section 4.2.1, estimated \u0015\u0000parameter can dis-\ncriminate between /thani-Avarthanam/ and/thillAna/ against\n/krithi/ . Discrimination between /thani-Avarthanam/ and\n/thillAna/ uses mean energy, \u0016(E), as a feature.\n8We deﬁne, (x)\u0000,min(0 ; x)\nFigure 10 . Histogram of average intensity, \u0016(E), in a\n10sof audio containing (blue) /thillAna/ and (red) /thani-\nAvarthanam/\n4.5/thanam/ (leaf node of D)\nFor the present, /thanam/ is the only class considered at\nnode D and hence on classifying a particular piece to con-\ntain /tALa/ without percussion at Node B, leads to this\nclass in the proposed schema.\n5. EXPERIMENTS AND RESULTS\nAll experiments are carried out on excerpts of vocal Car-\nnatic music concerts. The database used comprises of ran-\ndomly chosen 100 ﬁles from IITM concert database [14],\namounting to 12 vocal concerts, by a total of 9 artistes (6\nmale and 3 female artists), and a total of \u001820 hours du-\nration. The tonic frequencies ranges from 129 to 205 Hz.\nThe ground truth is generated after manually listening to\nthe music pieces. The number of occurences and its dura-\ntion, of each of the Carnatic form in the database, is de-\ntailed in Table 1. Also indicated are the number of non-\noverlapping 10 s clips of each of the music form, which\nis used for evaluating the algorithm. The inherent skew-\nness in class distributions is to be noted. Change from one\nform to another in a Carnatic music concert is customary,\nand hence, any given 10sclip can comprise of at most 2\nclasses. In such cases, the considered ground truth gives\nweightage to the temporally dominant class.\nThe results of the classiﬁcation at each node is shown\nin Table 2. The performance of the proposed hierarchical\nmodel is evaluated at each node of the hierarchy, taking\ninto account the performance of the parent nodes. How-\never, errors at all nodes are given equal weightage, sim-\nilar to “ﬂat classiﬁcation” evaluation [13]. The perfor-\nmance of the algorithm for each 10 s audio data is mea-\nsured using the f-ratio9. The f-ratio is only indicative\nof the predictability of the algorithm to a particular class,\nand is invariant to tn, which is required in the unbalanced\ndistribution of multi-class. Hence, we consider accuracy,\nA=tp+tn\ntp+tn+fp+fnalso as a performance metric, which is\nnot invariant to tn[15].\nThough the performance of the proposed approach de-\npends on the size of audio data analysed, we report results\nfor 10 s clip only. An average f-ratio of 0.62 is seen for the\nproposed hierarchical classiﬁcation, which is quite promis-\ning, given the complexity of the high level structures in the\n9f-ratio is F,tp\ntp+(fp+fn\n2)while precision is P,tp\ntp+fp. Recall,\nR,tp\ntp+fn, &tp; fp; fn indicating the number of true positive, false\npositive & false negatives respectively.music signal being examined. Also, the performance was\nobserved to be not affected in terms of gender of the singer\nor the tonic variations, (at nodes R, B, C and D) as feature\nset considered is mostly independent of pitch.\nDetailed analysis shows that /tAnam/ ,/AlApana/ and\n/viruttam/ are the most confused classes. This is because\nthefp(of/tALa/ class) at Node R, trickle down to leaf\nnode D always. This can be expected, as leaf nodes of A\nalways have ‘soft’ onsets, thus reducing the predictability\nscore of /thAnam/ . Similarly, fnerrors (of /tALa/ class,\nmostly comprising of /tAnam/ class), and have higher syl-\nlabic repetitive features similar to that of /AlApana/ . Also,\nit is seen that /thani-Avarthanam/ ,/thillAna/ and/krithi/\ndisplay better performance inspite of being a node below\nin the hierarchy. /krithi/ shows least recall performance\nwhich has been observed to be due to assumption of /chittai-\nswaras/ and/swara-kalpana/ in the /krithi/ class, result-\ning in misclassiﬁcation to /thillAna/ class. Also, the per-\nformance at each node is sensitive to choice of decision\nboundary. Hence, it is crucial to have additional discrim-\ninative features along with high performance classiﬁers at\neach node of the hierarchy. The hierarchical framework\nis proposed for vocal concerts, and a generalizable frame-\nwork across vocal and instrumental concerts is to be con-\ntemplated.\nTable 1 . Details of Carnatic music forms in the chosen\ndataset\nCarnatic Form Duration(min) # of segments\n/AlApana/ 373 2236\n/viruttam/ 50 302\n/thillAna/ 23 136\n/krithi/ 666 3996\n/thani-Avarthanam/ 66 396\n/thAnam/ 22 132\nTable 2 . Performance of the algorithm on the Carnatic mu-\nsic forms\nCarnatic Form Precision Recall f-ratio Accuracy\nPresence of /tALa/ 0.89 0.97 0.92 0.88\nPercussive 0.94 0.86 0.88 0.85\n/viruttam/ 0.80 0.60 0.63 0.80\n/AlApana/ 0.45 0.79 0.47 0.67\n/thillAna/ 0.87 0.65 0.7 0.74\n/krithi/ 0.93 0.5 0.72 0.71\n/thani-Avarthanam/ 0.95 0.7 0.75 0.94\n/thAnam/ 0.4 0.68 0.50 0.81\n6. CONCLUSIONS\nWe have proposed a hierarchical approach to classiﬁca-\ntion of Carnatic music forms, using signal processing fea-\ntures derived from energy contours, formant movements\nand parametric statistic distribution of such features. Based\non the parameters of the statistical distributions, simple\nclassiﬁers are proposed. An initial set of feature-vectors\nare designed, considering the “production model” of each\nparticular class. An average f-ratio of 0.62 and accuracy\nof 0.77 is obtained, thus reﬂecting the promising nature ofthe feature vectors. The hierarchical approach to group the\nclasses, based on the knowledge of the data is intuitive, and\nhelps deriving better feature vectors for within the group\nclassiﬁcation. Hence, the trade-off between reduced accu-\nracy with increasing depth of the hierarchy, against com-\nplex features for a ﬂat single level classiﬁcation can be ob-\nserved as expected. More sophisticated classiﬁers in an\nautomatic supervised framework can be developed based\non these features to get better performance.\n7. REFERENCES\n[1] B. Benary: “Composers and Tradition in Karnatic Music,”\nAsian Music, V ol. 3( 2), pp 42-51, 1972.\n[2] K. L. Armand and A. L. Armand: “One Hundred Years of\nMusic in Madras: A Case Study in Secondary Urbanization,”\nEthnomusicology, V ol. 27( 3), pp. 411-438, 1983.\n[3] R. Morris: “Variation and Process in South Indian Mu-\nsic: Some Kritis and their Sangati,” Music Theory Spectrum\nV ol. 23( 1), pp. 74–89, 2001.\n[4] Indira V Peterson: “The ‘KRTI’ as an integrative cultural\nform: Esthetic experience in the religious songs of two south\nIndian classical composers,” Journal of South Asian Litera-\nture, V ol. 19( 2),pp 165–179, 1984.\n[5] K. Jensen: “A Causal Rhythm Grouping,” Computer Music\nModeling and Retrieval , V ol. 3310, pp 83–95, 2005.\n[6] Moore, B. C. J. and Glasberg, B. R. and Baer, T.: “A model\nfor the prediction of thresholds, loudness, and partial loud-\nness,” Journal of the Audio Engineering Society ,V ol. 45.4,\npp 224–240, 1997.\n[7] J. P. Bello, L. Daudet et al.,: “A Tutorial on Onset Detection\nin Music Signals,” IEEE Trans. Speech and Audio Process-\ning, V ol 13.5, pp 1035–1047, 2005\n[8] D. W. Ellis: “Beat Tracking by Dynamic Programming”,\nJournal of New Music Research , pp 51–60 ,2007.\n[9] G. Tzanetakis and P. Cook: “Musical Genre Classiﬁcation of\nAudio Signals,” IEEE Trans., Speech and Audio Processing ,\nV ol. 10(5), pp 293–302, 2002.\n[10] C. Duxbury, M. Sandler and M. Davies: “A Hybrid Approach\nTo Musical Note Onset Detection”, Proc. of 5th Int. Confer-\nence on Digital Audio Effects (DAFx) , pp 33–38, 2002.\n[11] Boersma, P. and Weenink, D: “Praat: doing phonetics by\ncomputer [Computer program],” Version 5.3, 2013.\n[12] G. Grindlay and D.P.W. Ellis: “Transcribing Multi-\ninstrument Polyphonic Music with Hierarchical Eigeninstru-\nments” IEEE Journal Of Selected Topics In Signal Process-\ning, V ol. 5 (6), pp 1159–1169, 2011.\n[13] E. P. Costa, A. C. Lorena et.al,: “A Review of Performance\nEvaluation Measures for Hierarchical Classiﬁers,” Proc. of\nthe Association for the Advancement of Artiﬁcial Intelli-\ngence , pp 1-6, 2007.\n[14] A. Bellur, V . Ishwar and H. Murthy: “A knowledge based\nSignal Processing Approach to Tonic Identiﬁcation in Indian\nClassical Music,” Proc. 2nd CompMusic Workshop , pp 113–\n118, 2012.\n[15] M. Sokolova, G. Lapalme: “A systematic analysis of perfor-\nmance measures for classiﬁcation tasks,” Proc Information\nProcessing and Management , V ol 45, pp 427–437, 2009."
    },
    {
        "title": "The Role of Audio and Tags in Music Mood Prediction: A Study Using Semantic Layer Projection.",
        "author": [
            "Pasi Saari",
            "Tuomas Eerola",
            "György Fazekas",
            "Mathieu Barthet",
            "Olivier Lartillot",
            "Mark B. Sandler"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418049",
        "url": "https://doi.org/10.5281/zenodo.1418049",
        "ee": "https://zenodo.org/records/1418049/files/SaariEFBLS13.pdf",
        "abstract": "Semantic Layer Projection (SLP) is a method for automatically annotating music tracks according to expressed mood based on audio. We evaluate this method by comparing it to a system that infers the mood of a given track using associated tags only. SLP differs from conventional auto-tagging algorithms in that it maps audio features to a low-dimensional semantic layer congruent with the circumplex model of emotion, rather than training a model for each tag separately. We build the semantic layer using two large-scale data sets – crowd-sourced tags from Last.fm, and editorial annotations from the I Like Music (ILM) production music corpus – and use subsets of these corpora to train SLP for mapping audio features to the semantic layer. The performance of the system is assessed in predicting mood ratings on continuous scales in the two data sets mentioned above. The results show that audio is in general more efficient in predicting perceived mood than tags. Furthermore, we analytically demonstrate the benefit of using a combination of semantic tags and audio features in automatic mood annotation.",
        "zenodo_id": 1418049,
        "dblp_key": "conf/ismir/SaariEFBLS13",
        "keywords": [
            "Semantic Layer Projection (SLP)",
            "automatically annotating music tracks",
            "expressed mood based on audio",
            "conventional auto-tagging algorithms",
            "maps audio features to a low-dimensional semantic layer",
            "circular model of emotion",
            "crowd-sourced tags from Last.fm",
            "editorial annotations from I Like Music (ILM) production music corpus",
            "predicting mood ratings",
            "benefit of using a combination of semantic tags and audio features"
        ],
        "content": "THE ROLE OF AUDIO AND TAGS IN MUSIC MOOD PREDICTION: A\nSTUDY USING SEMANTIC LAYER PROJECTION\nPasi Saari\u0003, Tuomas Eerola\u0003, Gy¨orgy Fazekasy, Mathieu Barthety, Olivier Lartillot\u0003, Mark Sandlery\n\u0003Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyv ¨askyl ¨a, Finland\nyCentre for Digital Music, Queen Mary University of London, United Kingdom\n\u0003ffirstname.lastname g@jyu.fi,yffirstname.lastname g@eecs.qmul.ac.uk\nABSTRACT\nSemantic Layer Projection (SLP) is a method for auto-\nmatically annotating music tracks according to expressed\nmood based on audio. We evaluate this method by com-\nparing it to a system that infers the mood of a given track\nusing associated tags only. SLP differs from conventional\nauto-tagging algorithms in that it maps audio features to\na low-dimensional semantic layer congruent with the cir-\ncumplex model of emotion, rather than training a model\nfor each tag separately. We build the semantic layer us-\ning two large-scale data sets – crowd-sourced tags from\nLast.fm, and editorial annotations from the I Like Music\n(ILM) production music corpus – and use subsets of these\ncorpora to train SLP for mapping audio features to the se-\nmantic layer. The performance of the system is assessed\nin predicting mood ratings on continuous scales in the two\ndata sets mentioned above. The results show that audio is\nin general more efﬁcient in predicting perceived mood than\ntags. Furthermore, we analytically demonstrate the beneﬁt\nof using a combination of semantic tags and audio features\nin automatic mood annotation.\n1. INTRODUCTION\nOur daily experiences with music, together with strongly\ncorroborated research evidence [1], suggest that music has\na remarkable ability to induce as well as to express emo-\ntions or moods. For this reason, the mood associated with\na musical piece is often a key aspect in music listening.\nThis provides clear motivations for creating Music Infor-\nmation Retrieval (MIR) systems to organize, navigate or\naccess music collections based on mood. These systems\ntypically rely on mood models and appropriately selected\nmachine learning techniques [2,3]. Among several models\nproposed for emotions, the Circumplex model [4, 5] con-\nnecting mood terms to underlying emotion dimensions of\nvalence (positive / negative) and arousal (active / passive)\nis one of the most popular [6]. On the other hand, Thay-\ners variant [7] of this model suggests dimensions of ten-\nsion and energy diagonal to arousal and valence. However,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.training machine learning models that automatically asso-\nciate musical pieces with moods require high quality hu-\nman mood annotations that are laborious to create, hence\ntypically limited in amount.\nMood-related tags, i.e., free-form labels applied to art-\nists, albums, tracks, etc., are abundantly available from\npopular online services such as Last.fm1, while editorial\ntrack-level mood tags are vital in large production music\ncatalogues. However, due to issues related to noise and\nambiguity in semantic relations between tags, uncovering\nreliable mood representations from tag data requires typ-\nically ﬁltering and semantic analysis [8, 9]. Previous re-\nsearch showed that semantically processed information us-\ning track-level Last.fm tags is congruent with listener rat-\nings of valence, arousal, tension and various mood terms\n[10]. In a test set of 600 popular music tracks, moderate to\nhigh ( :47< r < : 65) correlation was found using the Af-\nfective Circumplex Transformation (ACT) technique, that\nis based on Latent Semantic Analysis (LSA) and the cir-\ncumplex model of emotions. These results outperformed\nseveral conventional semantic analysis techniques, and no-\ntably, raw tag frequency scores ( :16< r < : 47). The ro-\nbustness of ACT was also demonstrated in [11], by apply-\ning the technique to editorial tags from a production music\nlibrary of about 250,000 tracks.\nIn a wider context, modelling mood, and thus estimat-\ning mood tags may be seen as a speciﬁc form of auto-\ntagging, which is a popular research topic in MIR. A sys-\ntem is typically trained using audio features extracted from\na collection of tracks and their associated tags. Then, the\ntrained model is utilised to label new untagged tracks auto-\nmatically given their features. Typical auto-tagging studies\nhave trained models independently for each tag [12–14],\nomitting semantic associations between tags, while results\nin [15] and [16] showed that post-processing auto-tags ac-\ncording to their semantic similarity increases the perfor-\nmance. These techniques have produced promising results\nfor mood tags, possibly due to the use of cleanly-labeled\ntag data collected for research purposes. As shown in [10],\na considerable semantic gap exists between raw crowd-\nsourced mood tags and veriﬁed listener ratings. However,\nsemantic computing provides a promising direction, not\nyet exploited to the full extent in auto-tagging, for captur-\ning reliable information from large tag collections.\nPrevious studies in auto-tagging have compared predict-\n1http://www.last.fmed tags with human-labeled tags as a way to assess per-\nformance. By considering listener ratings as ground-truth\nrather than tags, in this paper we analytically compare auto-\ntags to actual tags as predictors. The two representations\nrelate to two distinct assumptions in tag estimation: ei-\nther human-labeled tags or audio is available for each new\ntrack. Our aim is to challenge these assumptions by high-\nlighting the beneﬁt of semantic computing in the context\nof music mood auto-tagging. Semantic Layer Projection\n(SLP) proposed in [17] provides a robust method for pro-\njecting the audio feature space to multi-dimensional se-\nmantic layer based on ACT. SLP followed by linear regres-\nsion with the projected feature components outperformed\nstate-of-the-art regression models in predicting listener rat-\nings of valence in 600 tracks from Last.fm. In this paper we\nevaluate the beneﬁts of SLP in auto-tagging using two cor-\npora: tracks and crowd-sourced mood tags from Last.fm\ntags as well as tracks and curated editorial mood tags ob-\ntained from the I Like Music (ILM) production music cata-\nlogue. We predict listener ratings of moods in separate test\nsets extracted from these corpora.\nThe rest of the paper is organised as follows: Section 2\ndescribes the tag data and the ACT technique for building\nthe semantic space of moods based on the tags, the set of\naudio features, and the SLP technique for predicting mood\nin new tracks based on ACT and the features. Section 3\ngives a detailed account of the experimental setup, the data\nsets used for SLP evaluation, baseline techniques, and the\nmethod for comparing mood prediction based on tag or au-\ndio information of new tracks. Section 4 shows the results\nof the experiments and conclusions are drawn in Section 5.\n2. METHODOLOGY\n2.1 Affective Circumplex Transformation\nWe used two sources of tracks and tags in the analysis:\n259,593 tracks from Last.fm and 226,344 tracks from I\nLike Music (ILM) production music catalogue, associated\nwith 357 and 288 mood terms, respectively. To create these\ndata sets, tags associated to track sets from the two sources\nwere ﬁrst lemmatized and identiﬁed from a vocabulary of\n560 mood terms, aggregated from mood words obtained\nfrom selected research papers in affective sciences, music\npsychology and MIR, as well as from the Allmusic.com\nweb service. In both data sets, tracks with only one tag,\nand tags associated with less than 100 tracks were then ex-\ncluded. Finally, the tag data was normalised using term\nfrequency-inverse document frequency (TF-IDF) weights.\nA detailed account of the data sets and the above process\nis given in [10, 11].\nThe following process was applied to Last.fm and ILM\nsets separately. To uncover semantic similarity between\nindividual mood terms, a low-rank approximation of the\nTF-IDF matrix was computed using Singular Value De-\ncomposition (SVD) and Multidimensional Scaling (MDS)\nas in [10]. SVD decomposes a sparse TF-IDF matrix N\ninto orthogonal matrices UandV, and a diagonal ma-\ntrixSwith singular values in decreasing order, such thatN=USVT. A rank kapproximation of Nis then com-\nputed by \u0016Nk=UkSk(Vk)T, where each row vector Uk\ni\nrepresents the terms wiwithkrelative weights for each\ndimension. Similarly, Vk\njrepresents track tjaskrelative\nweights. Based on the rank kapproximation, dissimilarity\nbetween terms wiandw^ican be computed using the cosine\ndistance between the Uk\niSkandUk\n^iSkvectors. To repre-\nsent mood terms explicitly in a low-dimensional space that\nresembles the arousal-valence space, MDS was applied on\nthe term distances to obtain a three-dimensional conﬁgu-\nration. The choice of using three dimensions instead of\ntwo is motivated by the debate around whether two dimen-\nsions is enough to capture relevant variance in moods. Past\nresearch have proposed various candidates for the third di-\nmension, such as dominance, potency, or movement.\nNext we applied the Affective Circumplex Transforma-\ntion (ACT) to conform the MDS conﬁguration to the space\nofarousal andvalence (A V), using A V values of 101mood\nterms given in [4, p. 1167] and [5, p. 54]. This technique\ntakes advantage of the Procrustes transformation [18] in-\nvolving translation, reﬂection, orthogonal rotation, and iso-\ntropic scaling using sum of squared errors as goodness-\nof-ﬁt. The motivation for this is to i)increase the inter-\npretability of the MDS conﬁguration, and ii)enable di-\nrect prediction of arousal and valence from the semantic\nspace. The technique yields a mood term conﬁguration\nxi= (x1;i; x2;i; x3;i); i= 1; :::; nterms . A subset of\nLast.fm and ILM mood term conﬁgurations are visualised\nin Fig. 1 (with k= 16 ). The frequencies of the terms\nacross tracks (co-occurence counts) range from 110 (“vin-\ndictive”) to 79,524 (“chill”) for Last.fm, and 346 (“narra-\ntive”) to 39,892 (“uplifting”) for ILM. Each track jwas\nprojected onto the resulting space by taking the Euclidean\nmean of the term positions, weighted by the sparse TF-IDF\nvector qjof the track:\ntj= (\u0006 iqj;ixi)=(\u0006iqj;i): (1)\nFinally, explicit mood-term-speciﬁc weights for the track\nwith position tjwere computed using:\nPj;i= (xi=jxij)\u0001tj; (2)\nwhereas arousal and valence for a track was estimated di-\nrectly by using the positions along corresponding dimen-\nsions. Tension was obtained by projecting tracks along the\ndirection (-1,1,0) as suggested in [7] (see Fig. 1). The anal-\nysis in [10] showed that the value of the rank parameter\nkin SVD computation has minor effect on ACT perfor-\nmance. Therefore we chose to use a heuristically selected\nvalue k= 16 in our analysis.\n2.2 Audio Feature Extraction\nAudio features describing dynamics (RMS energy, Low-\nenergy ratio, Attack time, Attack slope), rhythm (Fluctu-\nation pos. & mag., Event density, Pulse clarity, Tempo),\npitch (avg. pitch, Chromagram unwrapped centroid), har-\nmony (Key clarity, Mode [majorness], Harmonic change,\nRoughness), timbre (Brightness, Irregularity, Zerocross-\nings, Spectral Centroid, Flatness, Skewness, Entropy, FluxRussell\nThayer\n   CALMNESS    TIREDNESS   ENERGY    TENSION   ACTIVE\n   PASSIVE   POSITIVE    NEGATIVE\npower\nfreeblack\ndeepbrutal\nsmoothgayhumor\nepicfunsilly\nsexyamusing\nabstract\ngriefcampdemonicbizarre\npleasureguilty\nmartialsavage\ndarkapocalypticfast bouncy\nparty\nsoulfulgenial\nfeel good\nslowcoldmechanical happy\npureinteresting\nlaid backmonumentalsweet\nmysteriousstrong\nnostalgia\nmellowquirky\ngratitude\nsofttragichauntingmoving\neasy\nmelancholyspooky\nromantic\nemotionalloud\nterror\nchillsentimentallight\nreligiousenergeticangry\nvindictive\nsolemnself consciousplayful\nharsh\nfrenzywistful\nconcious\ntrippyaggressive\ncalmintense\nhypnoticsleaze\nsoothingpassionate\ndreamysardonic\natmospheric\nrelaxingeclecticenigma\netherealmeditation(a) Last.fm.\nRussell\nThayer\n   CALMNESS    TIREDNESS   ENERGY    TENSION   ACTIVE\n   PASSIVE   POSITIVE    NEGATIVE\nproud\npatriotictriumphantheroic\nmajesticdetermined\nsoaring\nhopefulepicstrong\ninspiringsuccess\neuphoric\noptimisticfree\nmourning\nlyricaluptempo\nhopelessdramatic\nreflectiveelegantexciting\nsaddeeppowerful\nintenseupbeat\nseriousominousenergetic\nmenacing\nuplifting\npeacefulsoftatmosphere\ngentlesparsenarrative\neasyhappy\ndreamydarkfun\nlaid backmarching\nlight eerie\nmysterious\nromanticrelaxedmystical\nslowhypnotic exotic\nnostalgiasmoothquirky\nsexyeccentric\n(b) ILM.\nFigure 1 . Two ﬁrst dimensions (valence–arousal) of the\nthree-dimensional mood term conﬁgurations obtained with\nACT ( k= 16 ) for (a) Last.fm and (b) ILM.\nand Spread), and structure (Spectral, Rhythmic and Regis-\ntral repetition) as well as 13 MFCCs, \u0001MFCCs, and \u0001(\u0001)\nMFCCs were extracted from the data sets presented in Ta-\nble 1 using the MIRtoolbox [19]. To characterise tracks\nusing audio features, statistical means and standard devia-\ntions were computed for each feature extracted over short\n50% overlapping time frames, yielding a 128 element fea-\nture vector for each track. For the features describing the\nrhythmic repetition and zero crossing rate, we used longer\nframe lengths of 2s, whereas for chromagram-based fea-\ntures such as the repetition of register, key clarity, centroid,\nmode, harmonic change, and roughness we used a frame\nlength of 100ms. For other features the frame length was\n46.4ms, except for low-energy ratio which is a track-level\nfeature by deﬁnition.\nMoodPSemantic layertAudio featuresfAEq. (2)j^j^j^Figure 2 . Mapping process in SLP for a novel track repre-\nsented by audio features ^fj.\nLast.fm ILM\nSET10K SET600 SET5K SET205\n# Tracks 9,662 600 4,692 205\n# Terms 357 357 288 288\nTerm density (%) 1.59 2.43 1.86 2.38\nTable 1 . Statistics of the mood term sets.\n2.3 Semantic Layer Projection\nSemantic Layer Projection (SLP), originally proposed in\n[17], is a technique for the automatic annotation of audio\ntracks with mood using audio features. SLP is trained on\na large collection of audio features and associated tag in-\nformation. The difference between SLP and conventional\nauto-tagging is that audio features are not directly mapped\nto individual tags, but to three-dimensional semantic rep-\nresentation of the mood space obtained by ACT. The map-\nping is determined by a training stage using the Partial\nLeast Squares (PLS) method. PLS has been found efﬁcient\nat handling high dimensional and collinear input variables,\nand it is shown to be robust when using a large number of\nobservations [20].\nGiven a set Fofmaudio features related to ntracks\nFn\u0002m= (f1; f2; :::; f n), and a semantic layer represen-\ntation Tn\u00023= (t1; t2:::; tn)for the corresponding tracks\n(see Eq. 1), the mapping matrix Abetween FandTis\ndetermined using PLS so that T\u0019AF. We optimize the\nnumber of components in PLS by applying (50, 100)-fold\ncross-indexing [21]. Cross-indexing tackles problems of\nmodel overﬁtting when choosing the optimal parameteri-\nsation from several candidates.\nThe explicit mood of a previously unseen track repre-\nsented by audio features ^fjare estimated by ﬁrst ^tj=A^fj,\nand then by ^Pj;i= (xi=jxij)\u0001^tjas in Eq. 2. This process\nis summarised in Fig. 2.\n3. EXPERIMENTAL SETUP\n3.1 Data Sets\nFor both data sources, Last.fm and ILM, the semantic mod-\nels are built from the full set of tracks (250,000 approx.),\nwhereas mappings between audio features and the seman-\ntic space in SLP are trained on subsets of these large cor-\npora, namely SET10K and SET5K. The performance of\nthe model is evaluated using listener ratings of perceived\nmoods in separate test sets: SET600 and SET205. Statisti-\ncal measures of these data sets in terms of semantic mood\ncontent are summarised in Table 1.\nSET10K consists of 9,662 tracks and was also used in\n[17]. The set was sampled from the Last.fm corpus in a\nbalanced manner by i)optimising mood variance in termsof track projections in the ACT space, ii)favouring tracks\nwith many listeners according to Last.fm, and iii)includ-\ning only unique artists. The audio content of SET10K con-\nsists of 15-30s Last.fm preview clips. The clips are typ-\nically samples of full tracks in the 128kB/s mp3 format,\nstarting from 30s-60s into the beginning. We assume that\nthese samples are sufﬁciently representative of the whole\ntracks. SET600 collected in [10] consists of 15s excerpts of\n600 popular music tracks containing no overlapping artists\nwith SET10K, and no tracks overlapping with the large\nLast.fm corpus. The set was fetched from Last.fm in a sim-\nilar balanced manner as SET10K, with additional balanc-\ning across multiple popular music genres (jazz, pop, rock,\nelectronic, folk and metal), and favouring tracks with many\nassociated mood tags. SET600 was annotated in a listening\ntest [10], with 59 participants rating the excerpts in terms\nof perceived mood expressed by music. Moods were rated\nin nine point bipolar Likert-scales for the mood dimensions\nof valence (negative / positive), arousal (calm / energetic),\nand tension (relaxed / tense), as well as in unipolar scales\nfor individual mood terms atmospheric, happy, dark, sad,\nangry, sensual, and sentimental.\nThe 4,692 tracks from SET5K were picked up randomly\nfrom the ILM production music catalogue by i)keeping\ntracks with a duration of at least 60s (in order to discard\nshort instances of the tracks), and ii)discarding instrumen-\ntal stems, i.e. individual tracks from multitrack recordings.\nSix main genres were represented (jazz, dance, rock, elec-\ntronic, folk and orchestral). 30s audio clip versions of the\ntracks were produced in the 128kB/s mp3 format. SET205,\ndescribed in [11], consists of 205 clips of 30s duration\nfrom SET5K. The tracks were sampled in a similar fash-\nion as for the Last.fm test set, but without taking listener\nstatistics into account. The set was annotated by 46 partici-\npants in a similar manner as SET600, but for bipolar scales\nof valence (negative / positive), arousal (calm / energetic),\ntension (relaxed / tense), dominance (submissive / domi-\nnant), romance (cold / romantic), and humour (serious /\nfunny) [11].\nFeatures extracted from SET10K and SET5K were nor-\nmalised using the z-score transform. All feature values\nwith more than 5 standard deviations from zero were con-\nsidered outliers and truncated to the extremes [\u00005;5]. The\nfeatures associated with SET600 and SET205 were then\nnormalised according to the means and standard deviations\nof the larger feature sets.\n3.2 Modelling Techniques\nTo show the efﬁciency of the mappings from audio fea-\ntures to the semantic layer, we compare SLP to two base-\nline techniques (BL1 and BL2) aiming at predicting mood\nratings of e.g. valence, arousal, and tension in the test cor-\npora. Prediction rates are computed as squared correlation\ncoefﬁcients ( R2) between the estimates and ratings over\nthe test sets. The difference between the three techniques\nlies in how the semantic relationships between mood terms\nare exploited in the modelling. BL1 uses mappings be-\ntween audio features and individual mood terms directly, inorder to predict mood ratings for the corresponding terms\nin the test corpora. This is analogous to the techniques used\nin [12–14]. BL2 uses mappings between audio features\nand individual mood terms to predict each (term-track) pair\nin the test corpora. Test tracks are then projected using Eq.\n1 and Eq. 2 based on the inferred tags. This is analogous\nto the techniques presented in [15,16]. The SLP technique\nhas been described in Section 2.3.\nIn short, BL1 does not use information about mood term\nrelationships at all, while BL2 exploits the semantic infor-\nmation after producing a mapping from audio features to\nmood terms. SLP, on the other hand, maps audio features\ndirectly to the semantic layer.\nMappings in BL2 were trained for terms appearing at\nleast ten times in SET10K and SET5K, amounting to 287\nand 201 terms, respectively. Since valence, arousal, or ten-\nsion are not explicitly modeled by BL1 (and no tags “va-\nlence” or “arousal” exist in either of the tag corpora), we\nuse terms corresponding to the bipolar labels of the mood\nscales in the listening tests for modelling these ratings.\nTags “positive”, “energetic”, and “relaxing” / “relaxed”\nwere applied more often than tags “negative”, “calm”, and\n“tense” in both SET10K and SET5K, so we use the afore-\nmentioned tags to model the corresponding mood dimen-\nsions. Similarly, for dominance, romance, and humour that\nwere rated in bipolar scales in SET205, we use tags “pow-\nerful”, “romantic”, and “funny”.\nEvaluating the role of tags and audio in predicting moods\nis achieved by comparing SLP and ACT prediction rates.\nWhile both of these techniques rely on the same seman-\ntic representation of moods, for each novel track, SLP uses\nonly audio features and automatically inferred moods. ACT\nhowever uses actual tags associated with the track. We use\nthese techniques in conjunction by computing the weighted\nmean of these two estimates for each track, and comparing\nthat to the mood ratings. We vary the weights [w;1\u0000w]\n(w2[0;1]) for the techniques so that the case w= 0corre-\nsponds to using ACT, whereas the case w= 1corresponds\nto using SLP.\n4. RESULTS AND DISCUSSION\n4.1 Evaluation of SLP\nTable 2 presents the comparison of SLP with the base-\nline methods. In case of Last.fm, prediction rates of SLP\nspan from moderate ( R2= 0:248for happy) to consid-\nerably high ( R2= 0:710for arousal). SLP consistently\noutperforms both baseline methods, except in one case,\nwhere BL1 gives marginally higher performance for sad\n(R2= 0:313). The differences between the baseline tech-\nniques and SLP are however small for the arousal, angry,\nand sensual dimensions. We also note that valence and re-\nlated moods (happy, sad, and angry) are the most difﬁcult\nto predict with all of the models, and in turn, arousal is\nthe easiest to predict. This is consistent with past studies\nin music emotion recognition [22]. Although BL1 suffers\nfrom the lack of explicit tags for valence, arousal, and ten-\nsion to infer explicit predictions, results for the seven moodBL1 BL2 SLPLast.fmValence 0.045 0.244 0.322\nArousal 0.693 0.662 0.710\nTension 0.198 0.469 0.560\nAtmospheric 0.075 0.541 0.581\nHappy 0.073 0.183 0.248\nDark 0.264 0.314 0.370\nSad 0.313 0.295 0.310\nAngry 0.475 0.465 0.497\nSensual 0.505 0.523 0.546\nSentimental 0.218 0.354 0.390\nMean 0.286 0.405 0.453ILMValence 0.156 0.330 0.486\nArousal 0.680 0.672 0.718\nTension 0.478 0.501 0.588\nDominance 0.461 0.376 0.352\nRomance 0.274 0.301 0.351\nHumour 0.209 0.362 0.502\nMean .376 .424 .499\nTable 2 . Prediction rates ( R2) for the Last.fm and ILM test\nsets using SLP and two baseline methods (BL1 and BL2).\nFor each dimension, best scores are reported in bold.\nterms show that exploiting semantic associations between\ntags is highly beneﬁcial. Moreover, as SLP outperforms\nBL2 for all mood dimensions, mapping tags to the seman-\ntic layer directly rather than projecting individual auto-tags\nto the layer is efﬁcient.\nIn the case of the ILM data sets, our results show pat-\nterns that are highly consistent with those of Last.fm –\nin general SLP outperforms the baseline methods, while\nBL1 obtains the lowest performance, on average. How-\never, the performance for valence is considerably higher\n(R2= 0:486) than for the Last.fm data set. A clear ex-\nception to this pattern is the higher performance of BL1\nfor dominance ( R2= 0:461) compared to the other tech-\nniques. Since dominance is not directly captured either by\nthe tags or the semantic layer dimensions, using other tags\nthan “powerful” would have changed the modelling. In\nfact, tags “airy”, “intimate”, and “soft” yielded the high-\nest performance for SLP ( R2>0:57), the tag “relaxed”\nyielded the highest performance for BL1 ( R2= 0:493),\nand the tag “airy” yielded the highest performance for BL2\n(R2= 0:543).\nOverall, the results show the advantage of mapping au-\ndio features directly to a semantic layer to train predictive\nmodels for moods. This solution provides increased per-\nformance over methods not exploiting semantic associa-\ntions at all, or projecting auto-tags to the semantic layer in\na later stage, after mapping from audio features to mood\ntags.\n4.2 Tags vs. Audio Features in Mood Prediction\nTo assess the importance of tags and audio in conjunction,\nsystematic evaluation of using SLP and ACT separately or\nin conjunction using the weights was carried out. Overall,\nthe results of such comparisons (see Fig. 3 and Table 3)\nﬁrst suggest that the predictions driven by audio features\nalone yield better performance. However, the combina-\ntion of audio features and tags lead to a notable increase,\nespecially for moods that are the most difﬁcult for SLP\n0 0.25 0.5 0.75 10.20.30.40.50.60.70.8\nWR2VALENCE\n  \nLast.fm\nILM\n0 0.25 0.5 0.75 10.20.30.40.50.60.70.8\nWR2AROUSAL\n  \nLast.fm\nILM\n0 0.25 0.5 0.75 10.20.30.40.50.60.70.8\nWR2TENSION\n  \nLast.fm\nILM(a) Valence\n0 0.25 0.5 0.75 10.20.30.40.50.60.70.8\nWR2VALENCE\n  \nLast.fm\nILM\n0 0.25 0.5 0.75 10.20.30.40.50.60.70.8\nWR2AROUSAL\n  \nLast.fm\nILM\n0 0.25 0.5 0.75 10.20.30.40.50.60.70.8\nWR2TENSION\n  \nLast.fm\nILM (b) Arousal\nFigure 3 . Prediction rate obtained when relying on differ-\nent information in Last.fm and ILM test sets: tags ( w= 0),\naudio ( w= 1), or combination ( 0< w < 1).\n(valence, happy, and sad). For Last.fm, the mean of the\nmaximum performance when using audio and tags in con-\njunction is higher ( R2= 0:531) compared to the individual\nuse of tags ( R2= 0:334) and audio ( R2= 0:453). Similar\npatterns can be observed with the ILM data, for which au-\ndio content-based methods outperform tag-based methods\nfor all mood scales ( 0:062and0:164increases in mean R2\nwhen both audio and tags are used in conjunction, com-\npared to audio and tags alone, respectively). As can be\nseen on Fig. 3, the optimal weight for the combination\nvaries to a small degree in both data sets, but lies around\n0.70 (mean). In other words, the best prediction of mood is\nachieved when the acoustic features are attributed a higher\nweight and are supplemented by tag data, both projected\nvia a semantic space.\nHowever, there are signiﬁcant exceptions to the conclu-\nsions drawn from simply tallying up the prediction rates\nacross the models and data sets. In the Last.fm data set,\naudio features are actually worse than tags in explaining\nthe ratings of the valence and happy dimensions. This is in\nline with a number of previous studies in mood prediction\nwith audio features, such as [22], and may have to do with\nthe fact that valence is an elusive concept in music, and\nmaybe particularly dependent on music genres. Further re-\nsearch that extends the mutual patterns between mood and\ngenre is required to untangle such speciﬁc results.\n5. CONCLUSIONS\nIn this study, we demonstrated that mood prediction is efﬁ-\ncient when relying on large-scale music tag data and audio\nfeatures, and is boosted by exploiting semantic modelling.\nThe results suggest that higher prediction rates are achiev-\nable using the semantic layer projection (SLP) technique\nwhen compared to baseline techniques related to conven-\ntional auto-tagging that do not incorporate semantic mod-\nelling into mappings from audio features.\nWe conclude that building large-scale predictive models\nfor moods in music can be done more efﬁciently for certain\nmood dimensions by relying on audio features rather thanTags max( R2)w AudioLast.fmValence 0.388 0.492 0.57 0.322\nArousal 0.416 0.741 0.80 0.710\nTension 0.392 0.618 0.71 0.560\nAtmospheric 0.298 0.607 0.83 0.581\nHappy 0.357 0.429 0.53 0.248\nDark 0.328 0.506 0.71 0.370\nSad 0.300 0.393 0.58 0.310\nAngry 0.221 0.518 0.84 0.497\nSensual 0.371 0.584 0.73 0.546\nSentimental 0.271 0.422 0.72 0.390\nMean 0.334 0.531 0.70 0.453ILMValence 0.418 0.571 0.69 0.486\nArousal 0.500 0.764 0.78 0.718\nTension 0.497 0.667 0.69 0.588\nDominance 0.271 0.386 0.73 0.352\nRomance 0.261 0.386 0.75 0.351\nHumour 0.437 0.590 0.67 0.502\nMean 0.397 0.561 0.72 0.499\nTable 3 . Prediction rate for the Last.fm and ILM test sets\nusing tags (ACT), audio (SLP), or a weighted combination.\nassociated tags. This is supported by the higher overall\nperformance of audio compared to tags, and by the overall\nstable performance of the predictions between the models\nin two different data sets, crowd-sourced tags from Last.fm\nand a curated production music corpus (ILM). These data\nsets consisted of nearly 250,000 tracks each, out of which\ndifferent subsets were carefully utilized in model training\nand evaluation. The results also imply that mood tags for\nnovel tracks are not crucial for the automatic annotation of\ntracks along most mood dimensions. However, for moods\nrelated to valence, the use of tags yields a considerable in-\ncrease in the predictive performance when combined with\naudio feature-based estimations. In the future we will fac-\ntor in music genre to the approach presented here.\nAcknowledgements This work was partly funded by\nthe Academy of Finland (The Finnish Centre of Excel-\nlence in Interdisciplinary Music Research) and the TSB\nproject 12033 - 76187 Making Musical Mood Metadata\n(TS/J002283/1).\n6. REFERENCES\n[1] J. A. Sloboda and P. N. Juslin. Music and Emotion , chapter\nPsychological Perspectives on Music and Emotion, pages 71–\n104. Oxford University Press, New York, 2001.\n[2] Z. Fu, G. Lu, K. M. Ting, and D. Zhang. A survey of audio-\nbased music classiﬁcation and annotation. IEEE Transactions\non Multimedia , 13(2):303 –319, 2011.\n[3] M. Barthet, G. Fazekas, and M. Sandler. Multidisciplinary\nperspectives on music emotion recognition: Recommenda-\ntions for content- and context-based models. In Proc. of the\n9th Int. Symposium on Computer Music modelling and Re-\ntrieval (CMMR) , pages 492–507, 2012.\n[4] J. A. Russell. A circumplex model of affect. Journal of Per-\nsonality and Social Psychology , 39(6):1161–1178, 1980.\n[5] K. R. Scherer. Emotion as a multicomponent process: A\nmodel and some cross-cultural data , pages 37–63. CA: Sage,\nBeverly Hills, 1984.[6] T. Eerola and J. K. Vuoskoski. A review of music and emo-\ntion studies: Approaches, emotion models and stimuli. Music\nPerception , 30(3):307–340, 2012.\n[7] R. E. Thayer. The Biopsychology of Mood and Arousal. Ox-\nford University Press, New York, USA, 1989.\n[8] C. Laurier, M. Sordo, J. Serra, and P. Herrera. Music mood\nrepresentations from social tags. In Proceedings of 10th In-\nternational Conference on Music Information Retrieval (IS-\nMIR) , pages 381–86, 2009.\n[9] M. Levy and M. Sandler. A semantic space for music derived\nfrom social tags. In Proceedings of 8th International Confer-\nence on Music Information Retrieval (ISMIR) , 2007.\n[10] P. Saari and T. Eerola. Semantic computing of moods based\non tags in social media of music. IEEE Transactions on\nKnowledge and Data Engineering , In press 2013.\n[11] P. Saari, M. Barthet, G. Fazekas, T. Eerola, and M. Sandler.\nSemantic models of mood expressed by music: Compari-\nson between crowd-sourced and curated editorial annotations.\nInIEEE International Conference on Multimedia and Expo\n(ICME 2013): International Workshop on Affective Analysis\nin Multimedia , 2013.\n[12] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Se-\nmantic annotation and retrieval of music and sound effects.\nIEEE Transactions on Audio, Speech, and Language Process-\ning, 16(2):467–476, 2008.\n[13] M. I. Mandel and D. PW Ellis. A web-based game for col-\nlecting music metadata. Journal of New Music Research ,\n37(2):151–165, 2008.\n[14] M. I. Mandel and D. P. Ellis. Multiple-instance learning for\nmusic information retrieval. In Proceedings of 9th Interna-\ntional Conference of Music Information Retrieval (ISMIR) ,\npages 577–582, 2008.\n[15] R. Miotto and G. Lanckriet. A generative context model for\nsemantic music annotation and retrieval. IEEE Transactions\non Audio, Speech, and Language Processing , 20(4):1096–\n1108, 2012.\n[16] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere. Au-\ntotagger: A model for predicting social tags from acoustic\nfeatures on large music databases. Journal of New Music Re-\nsearch , 37(2):115–135, 2008.\n[17] P. Saari, T. Eerola, G. Fazekas, and M. Sandler. Using seman-\ntic layer projection for enhancing music mood prediction with\naudio features. In Sound and Music Computing Conference ,\n2013.\n[18] J. C. Gower and G. B. Dijksterhuis. Procrustes problems , vol-\nume 3. Oxford University Press Oxford, 2004.\n[19] O. Lartillot and P. Toiviainen. A matlab toolbox for musical\nfeature extraction from audio. In Proceedings of the 10th In-\nternational Conference on Digital Audio Effects , 2007.\n[20] S. Wold, M. Sj ¨ostr¨om, and L. Eriksson. Pls-regression: a ba-\nsic tool of chemometrics. Chemometrics and intelligent lab-\noratory systems , 58(2):109–130, 2001.\n[21] P. Saari, T. Eerola, and O. Lartillot. Generalizability and sim-\nplicity as criteria in feature selection: Application to mood\nclassiﬁcation in music. IEEE Transactions on Speech and Au-\ndio Processing , 19(6):1802 –1812, 2011.\n[22] Y . H. Yang, Y . C. Lin, Y . F. Su, and H. H. Chen. A regression\napproach to music emotion recognition. IEEE Transactions\non Audio, Speech, and Language Processing , 16(2):448–457,\n2008."
    },
    {
        "title": "Inter and Intra Item Segmentation of Continuous Audio Recordings of Carnatic Music for Archival.",
        "author": [
            "Padi Sarala",
            "Hema A. Murthy"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1414892",
        "url": "https://doi.org/10.5281/zenodo.1414892",
        "ee": "https://zenodo.org/records/1414892/files/SaralaM13.pdf",
        "abstract": "The purpose of this paper is to segment carnatic music recordings into individual items for archival purposes using applauses. A concert in carnatic music is replete with applauses. These applauses may be inter-item or intra-item applauses. A property of an item in carnatic music, is that within every item, a small portion of the audio corresponds to the rendering of a composition which is rendered by the entire ensemble of lead performer and accompanying instruments. A concert is divided into segments using applauses and the location of the ensemble in every item is first obtained using Cent Filterbank Cepstral Coefficients (CFCC) combined with Gaussian Mixture Models (GMMs). Since constituent parts of an item are rendered in a single raga, raga information is used to merge adjacent segments belonging to the same item. Inter-item applauses are used to locate the end of an item in a concert. The results are evaluated for fifty live recordings with 990 applauses in total. The classification accuracy for inter and intra item applauses is 93%. Given a song list and the audio, the song list is mapped to the segmented audio of items, which are then stored in the database.",
        "zenodo_id": 1414892,
        "dblp_key": "conf/ismir/SaralaM13",
        "keywords": [
            "applauses",
            "segmentation",
            "archival",
            "concert",
            "carnatic music",
            "ensemble",
            "audio",
            "composition",
            "raga",
            "database"
        ],
        "content": "INTER ANDINTRA ITEMSEGMENTATION OFCONTINUOUS AUDIO\nRECORDINGSOF CARNATICMUSICFORARCHIVAL\nPadiSarala\nComputer ScienceandEngineering\nIndian Institute of Technology,Madras\npadi.sarala@gmail.comHemaA.Murthy\nComputer ScienceandEngineering\nIndian Institute of Technology,Madras\nhema@cse.iitm.ac.in\nABSTRACT\nThe purpose of this paper is to segment carnatic music\nrecordings into individual items for archival purposes\nusing applauses. A concert in carnatic music is replete\nwith applauses. These applauses may be inter-item or\nintra-item applauses. A property of an item in carnatic\nmusic, is that within every item, a small portion of the\naudio corresponds to the rendering of a composition\nwhich is rendered by the entire ensemble of lead\nperformer and accompanying instruments. A concert is\ndivided into segments using applauses and the location of\nthe ensemble in every item is ﬁrst obtained using Cent\nFilterbank Cepstral Coefﬁcients (CFCC) combined with\nGaussian Mixture Models (GMMs). Since constituent\nparts of an item are rendered in a single raga,raga\ninformation is used to merge adjacent segments belonging\nto the same item. Inter-item applauses are used to locate\nthe end of an item in a concert. The results are evaluated\nfor ﬁfty live recordings with 990 applauses in total. The\nclassiﬁcationaccuracyforinterandintraitemapplausesi s\n93%. Given a song list and the audio, the song list is\nmapped to the segmented audio of items, which are then\nstoredinthe database.\n1. INTRODUCTION\nIndian classical music consists of two popular traditions,\nnamely, Carnatic and Hindustani. In most of the carnatic\nmusic concerts audio recordings are continuous and\nunsegmented1. A concert in carnatic music is replete\nwith applauses. As most Indian music is improvisational,\nthe audience applauds the artist spontaneously. Thus, in a\ngiven performance, there can be a number of applauses\neven within a single item. The general structure of a\nconcert is shown in Figure 1. As shown in Figure a\nconcert is made up of a number of different items. Each\nitem can optionally have a solo vocal, a solo violin, a solo\npercussion (referred to as Thani in the Figure) but a\n1”http://www.sangeethapriya.org ”\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbear this noticeand thefull citation ontheﬁrst page.\nc/circlecopyrt2013International Society forMusic InformationRetrieval .composition is mandatory. Generally, the audience\napplauds the artist at the end of every item. Owing to the\nspontaneity that can be leveraged by an artist in a carnatic\nmusic concert, the concert is more like a dialogue\nbetween the artist and the audience [10]. Aesthetic\nphrases are immediately acknowledged by the audience.\nOwing to this aspect, applauses can occur anywhere in the\nconcert. Different types of items and their constituent\nsegments are shown in Figure 2. An item can be i) a\nsingle composition, ii) a vocal followed by the violin and\nthen a composition, iii) a composition followed by\nThaniAvarthanam, iv) a ragam tanam pallavi (RTP) that\nconsists of solo pieces of vocal , violin, and the pallavi,\nwhich is equivalent to a composition for the purpose of\nanalysis. The composition itself can be rendered in\ntandem by a lead performer, followed by accompanying\ninstruments. For better understanding of carnatic music\nterms,werefer thereader tothesupplemental material2.\nItemStartApplause\nVocal\nSoloViolin\nSoloCompo−\nsitionEnd\n(percussion)ThaniStart of\nConcert\nFigure 1:General structureofcarnatic musicconcert.\nFrom the Figure 2 it is clear that a composition\nsegment is mandatory in an item. Furthermore an item\ncan consist of one or more composition segments as\nshown in Figure 3. This is especially true for the Main\nSong in a concert, where different parts of the\ncomposition lend itself to signiﬁcant improvisation. This\nleads to the composition being fragmented into a number\nof segments owing to applauses. A composition segment\nis also the last segment in an item. Since compositions are\nmandatory in every item, the begin and end of an item can\nbe found in the vicinity of composition segments. To\ndetermine whether a sequence of composition segments\nbelong to the same item or not, information about ragais\nrequired. In general, an item can be performed in single\nraga. The characteristics of ragacan be detected by\n2”http://xa.yimg.com/kq/groups/2785355/\n2047532645/name/Carnatic+music+terminology.pdf ”melodic histograms [6]. So, to identify different items ,\nmelodic histograms can be used. This will not work when\nan item is intrinsically performed in multiple ragas. Such\nitems are quite rare in a concert and seldom have multiple\ncomposition segments like viruttamor a vocal solo sung\ninisolation.\nThe ﬁrst task for archival of carnatic music recordings\nrequires that the audio should be segmented into\nindividual items. A ﬁner segmentation of the concert into\nVocal solo, Violin solo, Song (Composition), and\npercussion solo, vocal-violin tandem, different percussi on\ninstruments in tandem may be of relevance to a keen\nlearner, listener or researcher. In this paper, an attempt i s\nmade to determine the ﬁne segments using applauses. The\nﬁne segments are ﬁrst labeled using supervised Gaussian\nMixture Models (GMM). Most of the segments in an item\nare rendered in single raga,ragainformation is used to\nmerge the adjacent segments belonging to same item.\nGiven an item list (text), the items of audio are aligned.\nThis results in a database where the items can be queried\nusing thetext corresponding tothat of the item.\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13\n/g14/g15/g8/g7/g13/g16/g17/g18/g2/g19/g11/g10\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g18/g6/g2/g10/g6/g8\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13\n/g14/g15/g8/g7/g13/g16/g17/g18/g2/g19/g11/g10\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g18/g6/g2/g10/g6/g8\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g20/g21/g11/g8/g6\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13\n/g14/g15/g8/g7/g13/g16/g17/g22/g11/g23/g11/g3/g24 /g14/g18/g2/g19/g11/g10/g17\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g20/g11/g8/g11/g3/g24 /g14/g18/g6/g2/g10/g6/g8/g17\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g25/g11/g10/g10/g11/g26/g6/g24\n/g14/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8/g17\n/g9/g4/g4/g10/g11/g12/g5/g13\n/g14/g15/g8/g7/g13/g16/g17/g15/g7/g13/g3\n/g27/g11/g6/g8/g24/g28/g2/g8/g23 /g22/g20/g25\nFigure 2:Different types ofitems inacarnatic musicconcert.\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13\n/g14/g15/g8/g7/g13/g16/g17/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g18/g19/g11/g16/g11/g3/g20/g2/g21\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13\n/g14/g15/g8/g7/g13/g16/g17/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g22/g13/g16/g13/g23/g11/g10/g20/g2/g21/g20\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g18/g19/g11/g16/g11/g3/g20/g2/g21\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13\n/g14/g15/g8/g7/g13/g16/g17/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13/g14/g15/g8/g7/g16/g11/g17\n/g22/g13/g16/g13/g23/g11/g10/g20/g2/g21\n/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\n/g9/g4/g4/g10/g11/g12/g5/g13\n/g14/g15/g8/g7/g13/g16/g17/g1/g2/g3/g4/g2/g5/g6/g7/g6/g2/g8\nFigure 3:Different types of compositions in a carnatic music\nconcert.\nThe remaining part of the paper is organised as\nfollows. In Section 2 we discuss the process of\nsegmentation of a concert and identiﬁcation of different\ncomposition segments . Section 2 also discusses previous\nwork for locating the applauses in a concert using\ndifferent spectral domain features [11]. Based on\ncomposition segments in a concert usage of pitch\nhistograms to merge the composition segments belonging\nto a same ragais discussed in Section 3. Section 4discusses for a given meta-data in terms of text\ncorresponding to items in a concert the meta-data is used\nto index the audio. In Section 5 we discusses about the\ndatabase used for the experimental evaluation and the\nresults obtained for segmentation and, inter and intra item\nclassiﬁcation. Finally, Section 6concludes thework.\n2. SEGMENTATION OFCARNATIC MUSIC\nCONCERT\nIn Subsection 2.1 we discuss identiﬁcation of applause\nlocations in a concert using different features. And\nSubsection 2.2 explains how these identiﬁed applause\nlocations are used for segmentation of a concert for\nidentifying thecomposition segments.\n2.1 Applause Analysis\nIn a carnatic music concert applauses can occur at the end\nof an item or within an item, as explained in Section 1.\nThe paper [11] discusses applause identiﬁcation in\ncarnatic music and its applications to archival of carnatic\nmusic. The paper also discusses the use of Cumulative\nSum (CUSUM) [3] to highlight the important aspects of a\nconcert. Spectral domain features like spectral ﬂux and\nspectral entropy are used for detecting the applause\nlocations for the given concert. Figure 4 shows applause\nlocations (peaks) in a 3 hour concert using CUSUM of\nspectral entropy as the feature. This concert has 25\napplauses and 9 items. The complete details about the\napplause identiﬁcation and thresholds used for detection\nof applauses areexplained inSection 5.\n0 1 2 3 4 5 6 7 8 9 10\nx 10505001000150020002500Applause locations for a concert using Spectral entropy\nTimeCusum of Spectral Entropy\n  \nSpectral entropy\nApplause locations\nFigure 4 :Applause locations for a concert using spectral\nentropyfeature.\n2.2 Identifyingthe Composition Segment Locations in\naConcert\nIn order to ﬁnd inter or intra item applauses we need to\nﬁnd change in ragaamong all the segments. As explained\nin Section 1, the end of an item is always after the\ncomposition segment. Therefore, composition segment\nlocations probably indicates an end of an item. Owing to\nthis, composition segments are identiﬁed in a concert. In\norder to ﬁnd the composition segments in a concert,\nGMMs are built for four classes, namely a Vocal solo, a\nViolin solo, a Composition ensemble and\nThaniAvarthanam using Cent Filterbank based Cepstral\nCoefﬁcients (CFCC).\nIn carnatic music each singer renders every item with\nrespect to a reference called Tonic ( sadja) [1]. Any\nmelodic analysis of Indian music therefore requires anormalisation with the tonic. Within a concert the lead\nperformer chooses the tonic and the accompanying\ninstruments are also tuned it. Across concerts the same\nmusician can have different tonic. Nevertheless, the tonic\nchosen for a concert is maintained throughout the concert\nusing an instrument called the tambura that provides the\ndrone. Therefore, the analysis of a concert depends on the\ntonic. CFCCs are proposed in this paper to perform\nnormalisation on the spectrum. The fundamental\ndifference between Mel Frequency Cepstral Coefﬁcients\n(MFCC) and CFCC is that, in CFCC the frequency\nspectrum of the signal is mapped to the cent scale (after\nnormalising with tonic) as opposed to the Mel scale.\nExtraction ofCFCCs isdetailed below:\n1. The audio signal isdivided intoframes.\n2. The short-time discrete Fourier is computed for each\nframe.\n3. The power spectrum is then multiplied by a bank of\nﬁlters that are spaced uniformly in the tonic normalised\ncent scale. The cent scaleisdeﬁned as:\ncent= 1200×log2/parenleftbiggf\ntonic/parenrightbigg\n(1)\n4. The energy in each ﬁlter is computed. The logarithm of\nﬁlterbank energies iscomputed.\n5. Discrete cosine transform (DCT-II) of log ﬁlter bank\nenergies iscomputed.\n6. The cepstral coefﬁcients obtained after DCT\ncomputation areused as features forbuilding the GMMs.\nAlternatively, the chroma ﬁlterbanks can also be used .\nThe chroma ﬁlterbanks discussed in [7], is primarily for\nthe Western classical music where the scale is\nequitemperament and is characterised by a unique set of\n12 semitones, subsets of which are used in performances.\nAs indicated in [12], Indian music pitches follow a just\nintonation rather than an equitemperament intonation.\nIn [9], it is shown that even just intonation is not adequate\nbecause pitch histogram across all ragas of carnatic music\nappearstobemoreorlesscontinuous. Toaccountforthis,\nthe chroma ﬁlterbanks include a set of overlapping ﬁlters.\nFurthermore, the ﬁlters can span less than a semitone, to\naccount for the fact that in Indian music two adjacent\nsvarasneed not be separated by a semitone. Cepstral\ncoefﬁcients derived using the Chroma ﬁlterbanks are\nreferredtoas ChromaFCC.\nFigure 5 shows MFCC, ChromaFCC and CFCC\nfeatures for three types of segments, Vocal, Violin, and\nComposition. As shown in the Figure 5 the mel\nﬁlterbanks emphasises mostly timbre, the chroma\nﬁlterbanks emphasises the svaras, and the cent ﬁlterbanks\nemphasises both svaraand the timbre around the melodic\nfrequency. In this paper, we therefore use CFCC to\ndistinguish different types of segments. To ﬁnd CFCC\ncepstral coefﬁcients we need to identify the tonic for\nevery concert. To this extent pitch histograms are used to\nﬁnd the tonic as explained in [1]. Table 1 ( Subsection\n5.1) provides concerts for each singer and the tonic values\nidentiﬁed using the pitch histograms. GMMs [2] are\nMel Filterbank (Hz)\nTime in Seconds040008000\n0.0 1.0 2.0Song\n 10 12 14 16 18 20 22 24 26 28 30\nMel Filterbank (Hz)\nTime in Seconds040008000\n0.0 1.0 2.0Violin\n 10 12 14 16 18 20 22 24 26 28\nMel Filterbank (Hz)\nTime in Seconds040008000\n0.0 1.0 2.0Vocal\n 10 12 14 16 18 20 22 24 26 28 30 32Cent Filterbank (cents)\nTime in Seconds-2400024004800\n0.0 1.0 2.0\n 16 18 20 22 24 26 28 30 32\nCent Filterbank (cents)\nTime in Seconds-2400024004800\n0.0 1.0 2.0\n 12 14 16 18 20 22 24 26 28 30\nCent Filterbank (cents)\nTime in Seconds-2400024004800\n0.0 1.0 2.0\n 12 14 16 18 20 22 24 26 28 30 32Chroma Filterbank (cents)\nTime in Seconds04008001200\n0.0 1.0 2.0\n 40 45 50 55 60 65 70\nChroma Filterbank (cents)\nTime in Seconds04008001200\n0.0 1.0 2.0\n 35 40 45 50 55 60 65\nChroma Filterbank (cents)\nTime in Seconds04008001200\n0.0 1.0 2.0\n 35 40 45 50 55 60 65 70\nFigure 5 :Time frequency representations of Composition\n(Song),Vocal, ViolinusingMFCC,CFCCandChromaFCC.\nfrequently used for speaker veriﬁcation and identiﬁcation ,\nand for segmentation tasks. In this paper, GMMs are used\nto segment the given concert using CFCC features.\nComplete details about the training of GMMs,\nsegmentation of the concert and its accuracy is explained\ninSection 5.\n3. INTER AND INTRA ITEM CLASSIFICATION\nAs explained in Subsection 2.2, composition segment\nlocations might probably indicate the end of an item.\nFigure 3 shows that a composition can be fragmented into\nnumber of segments owing to applauses. As a result, an\nitem can have one or more composition segments. We\nﬁrst explain how the adjacent composition segments\nbelonging to same the ragaare merged into a single item\nusing the pitch histograms. Following it we provide an\nalgorithm used for classifying the inter-item and\nintra-item applauses for ﬁnding the number of items in a\nconcert for archival purpose.\n0 100 200 300 400 500  \nPitch Histogram\nPeak positions\n0 100 200 300 400 500\na) Pitch Histograms of two Compositions with Same Raga  \nPitch Histogram\nPeak Positions\n0 100 200 300 400 500\nb) Pitch Histograms of two Compositions with Different Raga  \nPitch Histogram\nPeak Positions\n0 100 200 300 400 500  \nPitch Histogram\nPeak Positions\nFigure 6:Pitch histograms for same raga and different raga\nsegments.\nPitch is an auditory sensation in which a listener\nassigns musical tones to relative positions on a musicalscale based primarily on the frequency of vibration. Raga\nis one of the melodic modes used in Indian classical\nmusic.Ragauses a sequence of svarasupon which a\nmelody is constructed [9]. However, the way the svaras\nare approached and rendered in musical phrases and the\nmood they convey are important in deﬁning a ragathan\nthesvarasthemselves. The note locations themselves are\nnot absolute in carnatic music. Even for two ragas with\nthe same set of notes, the rendered note locations can be\ndistinct. As the purpose in this paper is not to identify the\nragas but to detect the change in raga, pitch histograms\nare used [6]. Also, for the given task, it is irrelevant if the\npitch corresponding to that of the lead artist or the\naccompanying artistsispicked up.\nYin algorithm [5] is used for extracting the pitch for\nthe segments. Pitch histograms for a segment show the\nrelative notes sung by a musician [6]. Based on the pitch\nhistograms, adjacent segments can be compared. Figure 6\n(a) shows the pitch histogram for the adjacent segments\nbelonging to the same raga, while Figure 6 (b) shows the\npitch histogram for the adjacent segments that belong to\ndifferent ragas. It can be observed that, in the pitch\nhistograms corresponding to that of the same ragathe\npositions of the peaks are more or less the same. On the\nother hand, for adjacent segments belonging to different\nragas, the peaks are different. Clearly, the heights of the\npeaks are not necessarily be the same. Further, some\npeaks may be missing. This primarily means that in that\nspeciﬁc segment, the musician’s improvisation was\nrestricted to a set of notes. Similarity between adjacent\nsegments can be calculated by warping along the cent axis\nusingdynamic programming [8]. Ifthesimilaritymeasure\nbetween the adjacent segments is above threshold ‘ α’,\nthen adjacent segments belong to different ragas\notherwise they belong to the same raga. Finding a single\nthreshold ‘ α’ across all the concerts is crucial. A line\nsearch is performed and a threshold is chosen empirically.\nFor theconcerts inquestion, ‘ α’ value ‘750’ ischosen.\nWenowsummarisetheoverallalgorithmdistinguishing\ninter-item and intra-itemapplauses as follows:\n1. Applauses are identiﬁed for a concert using spectral\ndomain features.\n2. GMMs are built for Vocal solo, Violin solo,\nComposition ensemble and ThaniAvarthanam using\nCFCC based cepstral coefﬁcients.\n3. Audio segments between a pair of the applauses are\nlabeled as Vocal solo, Violin solo, Composition, and\nThaniAvarthanam usingthetrained GMMs.\n4. The composition segments are located and the pitch\nhistograms are calculated forthe composition segments.\n5. Similarity measure is computed on the warped\nhistograms of the adjacent composition segments. If the\nsimilarity measure is above the threshold ‘ α’, then the\ncomposition segment isinter-item otherwise, intra-item.\n6. Based on the inter-item locations, intra-item segments\naremergedintothecorrespondingitems. Aconcertisthus\nsegmented intoitemsfor archival purposes.Figure 7 shows the overall procedure for identifying\nthe inter and intra items in a concert. In this Figure, the\nﬁrst column corresponds to the audio with the applause\nlocations marked. The second column indicates the\ndifferent segments based on applause locations. The third\ncolumn corresponds to the labeling of all these segments\ninto Vocal solo, Violin solo, Composition and\nThaniAvarthanam. In the last column, some of the\nsegments are merged based on the similarity measure.\nObserve that in Figure 7 two composition segments 4 and\n5 are merged into single item 4 based on the distance\nmeasure.\n/g1/g2/g3/g4/g5/g6/g7/g8/g9/g1/g7/g6/g1/g3/g1\n/g10/g7/g11/g12/g5/g8/g13/g10/g7/g11/g12/g5/g8/g13/g1/g1/g14/g6/g13/g5/g8/g1/g15/g7/g12/g3/g13/g16/g11/g17/g1\n/g14/g18/g18/g19/g3/g20/g21/g5/g1/g15/g7/g12/g3/g13/g16/g7/g11/g21\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g22\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1 /g24\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g17/g1 /g25\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9\n/g26/g7/g12/g3/g19/g1\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g26/g16/g7/g19/g16/g11\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g27\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g28\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g29\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9\n/g26/g7/g12/g3/g19/g1\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g26/g16/g7/g19/g16/g11/g1\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1 /g30\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g31/g32/g3/g11/g16/g1\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1 /g33\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9\n/g26/g7/g12/g3/g19/g1\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g26/g16/g7/g19/g16/g11/g1\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1 /g34\n/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1 /g22/g35\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1 /g22/g22\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1 /g22/g24\n/g23/g11/g13/g5/g8/g1/g23/g13/g5/g9/g36/g5/g17/g9/g5/g11/g13/g1/g22\n/g36/g5/g17/g9/g5/g11/g13/g1/g25/g36/g5/g17/g9/g5/g11/g13/g1/g24\n/g36/g5/g17/g9/g5/g11/g13/g1/g27\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g28\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g29\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g30\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g33\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g34\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g22/g35\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g22/g22\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g22/g24\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g22/g25\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g22/g27/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g22\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g1/g24\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g25\n/g26/g7/g12/g3/g19/g1/g1\n/g26/g16/g7/g19/g16/g11\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g27\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g1/g28\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g29\n/g26/g7/g12/g3/g19\n/g26/g16/g7/g19/g16/g11\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g30\n/g31/g32/g3/g11/g16\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g33\n/g26/g7/g12/g3/g19\n/g26/g16/g7/g19/g16/g11\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g34\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g22/g35\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g22/g22\n/g10/g7/g9/g18/g7/g21/g16/g13/g16/g7/g11/g1/g22/g24/g36/g5/g17/g9/g5/g11/g13/g1/g22/g28\n/g36/g5/g17/g9/g5/g11/g13/g1/g1/g22/g29\n/g36/g5/g17/g9/g5/g11/g13/g1/g22/g30\n/g36/g5/g17/g9/g5/g11/g13/g1/g22/g33\n/g36/g5/g17/g9/g5/g11/g13/g1/g22/g34/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5\n/g14/g18/g18/g19/g3/g20/g21/g5/g10/g7/g11/g12/g5/g8/g13/g1/g14/g6/g13/g5/g8/g1\n/g36/g5/g17/g9/g5/g11/g13/g1/g10/g19/g3/g21/g21/g16/g6/g16/g12/g3/g13/g16/g7/g11/g10/g7/g11/g12/g5/g8/g13/g1/g14/g6/g13/g5/g8/g1/g23/g37/g5/g11/g13/g16/g6/g38/g16/g11/g17/g1\n/g23/g11/g13/g5/g8/g1/g3/g11/g37/g1/g23/g11/g13/g8/g3/g1/g23/g13/g5/g9/g21\n/g23/g13/g5/g9/g1/g22\n/g23/g13/g5/g9/g1/g24\n/g23/g13/g5/g9/g1/g25\n/g23/g13/g5/g9/g1/g27\n/g23/g13/g5/g9/g1/g28\n/g23/g13/g5/g9/g1/g29\n/g23/g13/g5/g9/g1/g30\n/g23/g13/g5/g9/g1/g33\n/g23/g13/g5/g9/g1/g34\nFigure7:Overallprocedureforidentifyinginterandintraitems\ninaconcert.\n4. MAPPING THE COMPOSITION LISTTO\nITEMS\nIn many performances, the meta-data is available for a\nconcert in terms of composition lists from the audience3.\nThe composition list obtained from the audience is\nmatched to the items obtained from the algorithm\ndiscussed in Section 3. Figure 8 shows the mapping of the\ncomposition list obtained from the audience to inter-items\nfor archival purposes. We evaluate 10 live recordings of\nmale and female singers for mapping. The details of\nmapping and the mismatches (if any) are explained in\nSection 5.\n5. EXPERIMENTAL EVALUATION\nIn this section we ﬁrst give brief introduction to the\ndatabase used for our study and then describe the\nexperimental setup. We then provide the results obtained\nfromvarious steps of thealgorithm.\n5.1 Database Used\nFor evaluation purpose, 50 live recordings of male and\nfemale singers are taken4. All recordings correspond to\n3”http://www.rasikas.org ”\n4These live recordings were obtained from a personal collect ion of\naudience, musicians. These were made available for research p urposes\nonly./g1/g2/g3/g4/g5/g1/g2/g3/g4/g5/g6/g4/g7/g4/g8/g2/g2/g9/g10/g4/g11/g4/g12/g5\n/g13/g14/g11/g3/g15/g16/g4/g17/g18/g4/g19/g15/g5/g20/g8/g15\n/g21/g4/g19/g19/g14/g5/g22/g15/g8/g4/g23/g24/g15/g5/g12/g5\n/g25/g15/g26/g15/g7/g27/g10/g17/g4/g5/g25/g11/g14/g28/g4/g16/g4/g29\n/g30/g4/g11/g4/g9/g4/g4/g29/g4/g8/g4/g19/g4/g5/g12\n/g31/g4/g28/g15/g19/g4/g11/g4/g18/g4/g19/g15/g5/g20/g8/g15\n/g32/g27/g3/g4/g11/g8/g24/g4/g19/g4/g5/g32/g15/g11/g15/g9/g24/g4/g29/g5/g12\n/g33/g15/g19/g8/g27/g17/g4/g29/g5/g25/g14/g28/g4/g16/g4/g29\n/g30/g4/g16/g4/g17/g4/g5/g32/g11/g4/g24/g4/g5/g12\n/g20/g26/g24/g4/g19/g4/g5/g16/g4/g19/g26/g4/g5/g34/g24/g4/g4/g28/g14\n/g35/g19/g8/g14/g16/g27/g5/g21/g2/g2/g5/g36/g4/g19/g4/g9/g14/g5/g12\n/g5/g31/g4/g17/g18/g4/g19/g15/g5/g20/g8/g15\n/g25/g37/g13/g5/g6/g27/g7/g5/g38/g16/g4/g17/g4/g15/g3/g4/g19/g15/g5/g3/g15/g19/g4/g5/g28/g4/g19/g15/g5\n/g29/g4/g8/g24/g14/g16/g4/g11/g4/g5/g3/g2/g19/g15/g5/g22/g2/g19/g5/g26/g4/g4/g29/g4/g11/g4/g15/g39/g5\n/g20/g8/g15/g38/g26/g15/g9/g11/g4/g5/g19/g4/g8/g4/g15/g39/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g40\n/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g41\n/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g42\n/g22/g27/g23/g4/g17\n/g22/g15/g27/g17/g15/g19\n/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g43\n/g25/g4/g7/g4/g29\n/g37/g4/g19/g4/g29\n/g13/g4/g17/g17/g4/g3/g15/g5\n/g38/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g44/g39\n/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g5/g40/g45\n/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g5/g40/g40/g22/g15/g27/g17/g15/g19/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g46\n/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g47\n/g22/g27/g23/g4/g17\n/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g48\n/g37/g24/g4/g19/g15/g20/g3/g4/g11/g26/g24/g4/g19/g4/g29\n/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g49/g36/g4/g15/g19/g5\n/g30/g27/g19/g7\n/g25/g37/g13\n/g20/g9/g24/g26/g4/g28/g4/g8/g15/g38/g30/g4/g19/g23/g24/g4/g11/g4/g5/g1/g4/g8/g24/g4/g11/g4/g39/g12/g5\n/g25/g4/g7/g4/g29/g4/g17/g15/g16/g4/g5/g20/g8/g15/g5/g50/g26/g2/g29/g5/g40\n/g50/g26/g2/g29/g5/g41\n/g50/g26/g2/g29/g5/g42\n/g50/g26/g2/g29/g5/g43\n/g50/g26/g2/g29/g5/g46\n/g50/g26/g2/g29/g5/g47\n/g50/g26/g2/g29/g5/g48\n/g50/g26/g2/g29/g5/g49\n/g5/g1/g4/g26/g4/g51/g4/g9/g2/g5/g30/g27/g19/g7/g5/g52/g15/g9/g26 /g52/g15/g9/g26/g5/g27/g53/g5/g50/g26/g2/g29/g9\n/g54/g11/g15/g7/g15/g19/g4/g17/g5/g5\n/g30/g27/g19/g7/g5/g52/g15/g9/g26/g52/g15/g9/g26/g5/g27/g53/g5/g50/g26/g2/g29/g9/g5/g55/g24/g15/g23/g24\n/g5/g4/g11/g2/g5/g36/g4/g28/g28/g2/g8/g5/g26/g27/g5\n/g54/g11/g15/g7/g15/g19/g4/g17/g5/g30/g27/g19/g7/g5/g52/g15/g9/g26/g56/g14/g17/g17/g5/g34/g27/g19/g23/g2/g11/g26\n/g37/g24/g15/g17/g17/g4/g19/g4/g5/g12\n/g57/g4/g29/g14/g19/g4/g31/g4/g17/g18/g4/g19/g15/g5/g5/g20/g8/g15/g34/g27/g29/g28/g27/g9/g15/g26/g15/g27/g19/g5/g5/g40/g41/g50/g26/g2/g29/g5/g44\nFigure 8:Figure illustrates how the songs list is mapped to list\nofinter-items.\nconcerts where the lead performer is a vocalist. Each\nconcert is about 2-3 hours long. The total number of\napplauses across all the concerts is 990, where 394\napplauses are inter-item applauses and 596 applauses are\nintra-item applauses. The 50 concerts consist of 394\nitems. All recordings are sampled at 44.1KHz sampling\nfrequency with 16 bit resolution. For feature extraction,\nthe analysis window chosen is 100msecand hop size is\nset to10msecTable 1 gives the details of the database\nused. It can be observed that for a given singer, there are\ndifferent possible tonic values, calculated using pitch\nhistograms.\nSinger Name No. ofConcerts Duration(Hrs) No. ofApplause DifferentTonic\nMale 1 4 12 89 158,148,146,138\nFemale 1 4 11 81 210,208\nMale 2 5 14 69 145,148,150,156\nFemale 2 1 3 16 198\nMale 3 4 12 113 145,148\nFemale 3 1 3 15 199\nMale 4 26 71 525 140,138,145\nMale 5 5 14 62 138,140\nTable 1:Database used for study, different Tonic values\nidentiﬁed for each singer usingpitchhistograms.\n5.2 Experimental Setup\n1. For all the 50 recordings spectral ﬂux and spectral\nentropy features are extracted with a window size of\n100msecwith overlap of 10msec. Applauses are\nmarked for all the recordings using the\nSonic-Visualizer [4]. These marked applauses are used as\nthe ground truth for ﬁnding the applause detection\naccuracy.\n2. To build the GMMs for the segmentation of the concert\nVocal, Violin, ThaniAvarthanam, and Composition\nsegments are manually segmented from the database.\nFrom male (female) recordings three segments are chosen\nfor training the GMM for each class. MFCC,\nChromaFCC, and CFCC features of 20 dimensions are\nextracted. GMMs with 32 mixtures are built for each of\nthe four classes.\n3. All the 50 recordings are segmented based on applause\nlocations and all these segments are manually labeled as\nVocal, Violin, ThaniAvarthanam, and Composition.Labeled data is used as the ground truth for ﬁnding the\nsegmentation performance5. After segmenting the\nconcerts based on the applauses, there are 990 segments\nin total and these segments are tested against the GMMs.\nAllthese segments arelabeled using1-best result.\n4. For each concert, composition segments are\nhighlighted using GMMs, and for these segments pitch is\nextracted using Yin algorithm [5]. These segments are\nmerged into a single item if they belong to the same raga\nusingthe pitchhistograms.\n5. The performance measure used for evaluating each of\nthe applause detection , segmentation performance and\ntheinter-item,intra-item classiﬁcation is\nAccuracy =CorrectlyPredicted Data\nTotal Amount of Data×100%\n5.3 Experimental Results\nTheanalysisofﬁndingtheendofitemlocationsforallthe\nconcerts isdone inthe following threestages.\n1. As explained in Section 3, ﬁrst, applauses are located\nfor using spectral domain features like spectral entropy\nand spectral ﬂux. Table 2 shows the decision thresholds\nfor applause and music discrimination and the applause\ndetection accuracy based on the same thresholds. The\napplause detection accuracy iscalculated at framelevel.\nFeature Thresholdrange Accuracy (%)\nSpectralFlux(Nonorm) 0.2-1.0 85%\nSpectral Flux(Peaknorm) 0.35-1.0 96%\nSpectral Entropy 0.79-1.0 95%\nTable 2:Decision thresholds for applause and music\ndiscriminationandapplause detection accuracy.\n2. Secondly, based on applause locations all concerts are\nsegmented. The segments are labeled using GMMs with\nCFCC features. Table 3 shows the segmentation\nperformance using MFCC, ChromaFCC, and CFCC\nfeatures. Separate GMMs are built for male and female\nsingers. Table clearly shows that CFCC features performs\nwell in locating the composition segments in the concert.\nThe overall segmentation accuracy for 50 concerts,\nincluding male and female recordings, is95%.\nModel MFCC ChromaFCC CFCC\nMale singers 78% 60% 90%\nFemalesingers 92% 70% 97%\nTable3:SegmentationperformanceusingMFCC,ChromaFCC\nandCFCC.\n3. Finally, the composition segments in a concert are\nmerged into a single item if they belong to the same raga\nusing the pitch histograms. Table 4 shows the inter-item\nand intra-item classiﬁcation accuracy for every singer.\nThe overall classiﬁcation accuracy for the 50 concerts is\nfound to be 93%. We also evaluate our approach for\nmapping the songs list to the set of items which we obtain\nfrom the algorithm. Table 5 also shows the mapping\n5Labeling was done by the ﬁrst author and veriﬁed by a professi onal\nmusicianperformance. This mapping is evaluated for 10 live\nrecordings of performances by six musicians for which\nsongs listswereavailable.\nMusician Intra-item Inter-item Accuracy (%)\nMale1 65 24 90\nFemale 1 50 31 100\nMale2 37 32 100\nFemale 2 10 6 98\nMale3 75 38 93\nFemale 8 7 99\nMale4 317 227 93\nMale5 33 29 100\nTable 4:Interand intraitemclassiﬁcation accuracy.\nMusician No. of concerts Actualinter items Predictedinteritems Mapping accuracy (%)\nMale 1 1 6 7 96\nFemale1 1 9 9 100\nMale 2 1 7 7 100\nMale 3 2 19 21 97\nMale 4 2 13 14 97\nMale 5 3 19 19 100\nTable 5:Performance of mapping the songs list to the set of\nitems.\nIn a concert every artist can optionally render a\nragamalika , where a single item can be performed in\ndifferent ragas. When a single composition is sung in\nragamalika , the algorithm will still work, since the\napplause will be at the end of the item. On the other hand,\nfor items like viruttam orragam tanam pallavi with\nmultiple ragas, owing to the extensive improvisational\naspects in them, a number of applauses can occur\nintra-item. Such special cases are limits of the algorithm.\nEvery concert may have one such item. For these types of\nitemsthelyricscouldbesameorthemetercouldbesame.\nFor such cases, the algorithm can be extended to include\nmatching of the lyrics. The last item in a concert is a\nclosure composition. This is identical in all concerts and\nis called the mangalam . When a musician continues\nwithout a pause before closure composition, the\nmangalam is combined with the penultimate item in the\nconcert. This isnot considered as an errorinthis paper.\n6. CONCLUSION AND FUTURE WORK\nIn this work we proposed an algorithm for automatically\nsegmenting continuous audio recordings of carnatic music\ninto items for archival purpose. An item in a concert has a\nstructure and each segment has speciﬁc spectral\nproperties. These properties are exploited to label the\nsegments. In particular, a new feature called CFCC\nperforms well for labeling the segments when compared\nto MFCC and ChromaFCC features. Given meta-data in\nterms of names of items in a concert, the audio and the\nmeta-data are aligned. The directions for future work will\nbe, we need to evaluate the inter-item and intra-item\nclassiﬁcation technique for the special cases like\nragamalika ,RTP,viruttam, and a vocal solo (sung in\nisolation), where adjacent segments belong to different\nragas but correspond to a single item. This requires\nadditional analysis in terms of lyrics, meter which are\nquite nontrivial.7. ACKNOWLEDGEMENTS\nThisresearchwaspartlyfundedbytheEuropeanResearch\nCouncil under the European Unions Seventh Framework\nProgram, as part of the CompMusic project (ERC grant\nagreement 267583).\n8. REFERENCES\n[1] Ashwin Bellur, Vignesh Ishwar, Xavier Serra,\nand Hema A. Murthy. A knowledge based signal\nprocessing approach to tonic identiﬁcation in indian\nclassicalmusic.In InternationalCompMusicWokshop ,\nInstanbul, Turkey, 2012.\n[2] Christopher M. Bishop. Pattern Recognition and\nMachine Learning (Information Science and\nStatistics) , chapter 7. Springer-Verlag New York,\nInc., Secaucus, NJ, USA, 2006.\n[3] B E Brodsky and B S Darkhovsky. Non-parametric\nMethods in change-point problems . Kluwer Academic\nPublishers, New York, 1993.\n[4] C Cannam, C Landone, M Sandler, and J.P Bello. The\nsonic visualiser: A visualisation platform for semantic\ndescriptors from musical signals. In 7th International\nConference on Music Information Retrieval (ISMIR-\n06), Victoria, Canada, 2006.\n[5] A De Cheveigne and H Kawahara. Yin, a fundamental\nfrequency estimator for speech and music. Journal\nof the Acoustical Society of America , page\n111(4):19171930, 2002.\n[6] P Chordia and A Rae. Raag recognition using pitch-\nclass and pitch-class dyad distributions. In Proc. of\nISMIR, pages 431–436, 2007.\n[7] Dan Ellis. Chroma feature analysis and synthesis.\nhttp://www.ee.columbia.edu/ ˜dpwe/\nresources/Matlab/chroma-ansyn , 2007.\n[8] Toni Giorgino. Computing and visualizing dynamic\ntime warping alignments in r: The dtw package.\nJournal of StatisticalSoftware , 31(7):1–24, 82009.\n[9] T M Krishna and Vignesh Ishwar. Svaras, gamaka,\nmotif and raga identity. In Workshop on Computer\nMusic, Instanbul, Turkey, July2012.\n[10] M V N Murthy. Applause and aesthetic experience.\nhttp://compmusic.upf.edu/zh-hans/node/151, 2012.\n[11] Padi Sarala, Vignesh Ishwar, Ashwin Bellur, and\nHema A Murthy. Applause identiﬁcation and its\nrelevance to archival of carnatic music. In Workshop\non Computer Music , Instanbul, Turkey, July2012.\n[12] JSerra,GKKoduri, MMiron,andXSerra. Tuningof\nsung indian classical music. In Proc. of ISMIR , pages\n157–162, 2011."
    },
    {
        "title": "Groove Kernels as Rhythmic-Acoustic Motif Descriptors.",
        "author": [
            "Andy M. Sarroff",
            "Michael A. Casey"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417903",
        "url": "https://doi.org/10.5281/zenodo.1417903",
        "ee": "https://zenodo.org/records/1417903/files/SarroffC13.pdf",
        "abstract": "The “groove” of a song correlates with enjoyment and bodily movement. Recent work has shown that humans often agree whether a song does or does not have groove and how much groove a song has. It is therefore useful to develop algorithms that characterize the quality of groove across songs. We evaluate three unsupervised tempo-invariant models for measuring pairwise musical groove similarity: A temporal model, a timbre-temporal model, and a pitchtimbre-temporal model. The temporal model uses a rhythm similarity metric proposed by Holzapfel and Stylianou, while the timbre-inclusive models are built on shift invariant probabilistic latent component analysis. We evaluate the models using a dataset of over 8000 real-world musical recordings spanning approximately 10 genres, several decades, multiple meters, a large range of tempos, and Western and non-Western localities. A blind perceptual study is conducted: given a random music query, humans rate the groove similarity of the top three retrievals chosen by each of the models, as well as three random retrievals.",
        "zenodo_id": 1417903,
        "dblp_key": "conf/ismir/SarroffC13",
        "keywords": [
            "groove",
            "enjoyment",
            "bodily movement",
            "algorithm",
            "characterize",
            "quality",
            "tempo-invariant",
            "musical groove",
            "similarity",
            "datasets"
        ],
        "content": "GROOVE KERNELS AS RHYTHMIC-ACOUSTIC MOTIF DESCRIPTORS\nAndy M. Sarroff\nDartmouth College\nDepartment of Computer Science\nsarroff@cs.dartmouth.eduMichael Casey\nDartmouth College\nDepartments of Computer Science and Music\nmichael.a.casey@dartmouth.edu\nABSTRACT\nThe “groove” of a song correlates with enjoyment and bod-\nily movement. Recent work has shown that humans often\nagree whether a song does or does not have groove and how\nmuch groove a song has. It is therefore useful to develop\nalgorithms that characterize the quality of groove across\nsongs. We evaluate three unsupervised tempo-invariant\nmodels for measuring pairwise musical groove similarity:\nA temporal model, a timbre-temporal model, and a pitch-\ntimbre-temporal model. The temporal model uses a rhythm\nsimilarity metric proposed by Holzapfel and Stylianou, while\nthe timbre-inclusive models are built on shift invariant prob-\nabilistic latent component analysis. We evaluate the mod-\nels using a dataset of over 8000 real-world musical record-\nings spanning approximately 10 genres, several decades,\nmultiple meters, a large range of tempos, and Western and\nnon-Western localities. A blind perceptual study is con-\nducted: given a random music query, humans rate the groove\nsimilarity of the top three retrievals chosen by each of the\nmodels, as well as three random retrievals.\n1. INTRODUCTION\nThe propensity to move to music in a particular way is\nwidespread and fundamental to our experience of music\nlistening and enjoyment. Anyone who has spontaneously\nbopped their head, clapped their hands, jumped the pogo,\nswayed their cigarette lighter in the air, or tapped their ﬁn-\ngers or toes to music has shared this common experience of\nnear-involuntary musical response. Yet this aspect of mu-\nsic has been little studied in music information retrieval.\nThe phenomenon has been variously described as ﬂow\n[3], sensorimotor synchronization [9], feel [16], and groove\n[8, 11, 12, 16]. Although related, the concept of groove is\ndifferent from beat, which is the property of a predictable\nunderlying periodic pulse [17]. The degree of groove is\ncorrelated with the degree to which the music induces the\ndesire to move rather than the location and frequency of\nperiodic entrainment.\nWe propose a new algorithm that extracts groove ker-\nnels—underlying audio patterns that correlate with the propen-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.sity to move. We use these features to measure groove sim-\nilarity between pieces of music. We deﬁne groove similar-\nity as that aspect of the sound pattern that induces, within\na subject, the desire to move in the same way .\nWe conducted a groove similarity experiment using hu-\nman subjects. The experiment evaluated three automatic\ngroove extraction and similarity algorithms: an extant tempo-\ninvariant rhythm similarity measure [6], and two versions\nof a proposed tempo-and-shift invariant groove kernel ex-\ntraction system. The proposed system is inspired by the\nwork of [18,20] which we extend with a full groove-oriented\nsystem architecture.\nOur evaluation with human subjects used a diverse dataset\nof real-world audio ﬁles. Results show that our system\nretrieves music that corresponds more closely to human\njudgements about groove similarity than random baselines.\nIn summary, the primary contributions of this work are:\n\u000fa new groove kernel feature based on shift and time-\nscale invariant Probabilistic Latent Component Anal-\nysis,\n\u000fa new dataset consisting of over 8,000 musical audio\ntracks in 10 contrasting genres,\n\u000fevaluation of three extraction and retrieval systems\nand two random baselines against human groove sim-\nilarity judgments.\nTo the best of our knowledge, this is the ﬁrst work to detail\nan unsupervised system architecture for groove features\nand experimentally evaluate it on human subjects. The fol-\nlowing section provides background and motivation. Sec-\ntion 3 describes how groove kernels are extracted from au-\ndio ﬁles. Experimental evaluation is presented in Section\n4, followed by concluding remarks in Section 5.\n2. BACKGROUND\n2.1 Groove\nUnderlying the operational character of groove is the psy-\nchological sensorimotor synchronization of auditory stim-\nuli and human movement. In their study Janata et al. [9]\nshowed that subjects strongly agreed that groove was de-\nscribed by the extent to which music induces movement,\na positive affect due to such proclivity, and a feeling of\nbeing part of the music. From a sensorimotor perspec-\ntive [12] deﬁned groove as “wanting to move some partof the body in relation to some aspect of the sound pat-\ntern”. Other studies consider timing deviations [1], or tem-\nporal discrepancies [11], with respect to precise metro-\nnomic timing as the source of the groove, relating these\nto expressiveness and proclivity for motion. Pressing de-\nscribes groove, or feel, as a “ﬁrmly structured temporal\nmatrix” [16]. It is a temporal foundation and an emergent\nphenomenon formed out of concurrent recurring pulses (a\nstable sense of tempo), perception of a cycle of time that\nlasts for 2 or more pulses, and is effective in engaging syn-\nchronization of bodily movement.\nFollowing [16] we take the position that groove induces\ncharacteristic responses in subjects and these responses stem\nfrom speciﬁc repeated acoustic patterns. Substantially dif-\nferent patterns induce different tendencies of motion, there-\nfore the feel or the groove is different. Music that grooves\nis characterized by strong repetition. Therefore also fol-\nlowing [16], we expect to observe a foundational “tempo-\nral matrix” that expresses the acoustic pattern correspond-\ning to a particular groove at the time scale of roughly two\nbars. We hypothesize that such foundational patterns are\ninvariant to shifts in time (i.e. within a song) and shifts of\ntempo (i.e. between songs).\n2.2 Beat, Meter, Rhythm\nTo express invariance to shifts in tempo the description of\ngroove must be normalized to the concept of beat. Align-\nment of a temporal matrix to the beat is not enough for\ncomparisons between musical excerpts. There must also\nbe a way to normalize for the phase of a temporal pattern\nwith respect to beat hierarchy, or meter. There are two ap-\nproaches to this problem: bar extraction and circular shift-\ning of the temporal matrix. If we wish to represent groove\nas a multi-bar pattern, then we must rely on circular shift-\ning.\nHolzapfel et al. [6] addresses tempo invariant represen-\ntations of rhythm at multiple time scales, therefore charac-\nterizing multi-scale rhythm. This work is unique in that it\nprovides a scale-invariant song-level rhythm descriptor for\nmusic. Holzapfel et al. show that music with similar albeit\ncomplex rhythmic structure may be successfully catego-\nrized, even when the tempos are rather different. There-\nfore we see their algorithm as a candidate representation\nfor groove similarity.\n2.3 Rhythm Retrieval and Classiﬁcation\nWhile groove retrieval has not been explicitly treated by\nthe music information retrieval community, rhythm clas-\nsiﬁcation and retrieval has been studied in recent years.\nRhythm similarity metrics typically extract a rhythm de-\nscriptor that exhibits tempo-invariant properties. For in-\nstance [5] measures pairwise rhythm similarity using beat\nspectra based upon beat-synchronous low level features.\nPattern segmenting is used in conjunction with dynamic\ntime warping of acoustic features for pairwise rhythm sim-\nilarity in [13].\nThe annotation of a large-scale rhythm-based dataset\nis expensive. Several authors have leveraged the musicrecordings available at the Ballroom Dancer’s website1\nwhich have tempo and genre annotations. With this dataset\nauthors have presented the results of genre classiﬁcation\ntasks using rhythm descriptors such as amplitude envelopes\nof bar/beat synchronous features [4]; log-scale autocorre-\nlation of onset strength signals [10]; ﬂuctuation patterns\n[15]; and spectral rhythm patterns [14]. The success of\nmany of these approaches is augmented when tempo meta-\ndata from the dataset is included. Hence the experimental\nresults reported often reﬂect a semi-supervised approach.\n2.4 Shift-Invariant Representation\nTo extract the most salient repeated aspects of the music we\nuse shift-invariant probabilistic latent component analysis\n(SI-PLCA) [18]. A convolutional variant of non-negative\nmatrix factorization (NMF), SI-PLCA places NMF in an\nexplicitly Bayesian framework and extracts time-frequency\ncomponents that are stable to shifts in time or frequency.\nOur work focuses on time-shift invariant PLCA. Given\na nonnegative matrix V, time-shift invariant PLCA factor-\nizesVsuch that V\u0019P\nkzkWk\u0003hk,kis an index to\nthe factor components, Zis a diagonal matrix containing\nmixing coefﬁcients, and \u0003is the convolutional operator. In\nour models we extract one component. Hence z= 1,W\nis a two-dimensional matrix, and his a vector. We refer to\nWas a kernel and has an activation function locating the\nkernel at multiple positions in a track.\nWeiss and Bello [20] used SI-PLCA to evaluate song\nstructure segmentation in a Beatles data set. They em-\nployed chroma features to extract multiple phrase-level blocks\nwithin songs whereas we use constant-Q spectral and cep-\nstral features to extract single rhythmic kernels at the bar\nlevel using different sparseness constraints. Finally, we as-\nsessed kernels in a groove similarity task with a large and\ndiverse dataset using human evaluators.\n3. SYSTEM ARCHITECTURE\nThe groove kernel is built in four stages. In the ﬁrst stage,\nbar and beat detection is performed. In the second stage,\nbeat-synchronous features are extracted and bar/beat acti-\nvation templates are generated. The third module estimates\nmeter. The fourth stage extracts a shift-invariant groove\nkernel. The overall architecture is depicted in Figure 1.\nThe details of each stage are described below.\n3.1 Beat and Bar Tracking\nGiven a discrete time audio signal, we perform bar and beat\ntracking using the Queen Mary bar and beat tracker2re-\nported in [2,19]. The meter of the audio is a required input\nparameter for the bar tracker. Since we do not know the\nmeter of a given musical audio ﬁle, we run the bar tracker\ntwice: once assuming 3/4 meter and again assuming 4/4\nmeter. We note that changing the value of the input param-\neter to the beat/bar tracker does not affect the estimated\n1http://www.ballroomdancers.com/\n2Available as a Vamp plugin at http://isophonics.net/\nQMVampPlugins .Groove KernelSmoothingSI-PLCAMeter?SI-PLCA   × 2Activation FunctionsFeaturesBar / Beat TrackingSongI.II.III.IV .Figure 1 . Overview of system architecture.\nlocations of the beats, only the indices that represent esti-\nmated bar onsets. The Queen Mary beat tracker has a re-\nported accuracy of 73.6% when metrical level is not taken\ninto account. The downbeat detector has a reported ac-\ncuracy of 52.6%. Beat and downbeat tracking is an open\nproblem and these results are comparable to the state of the\nart.\n3.2 Beat Synchronous Features and Activation\nTemplates\nWe extract frequency domain beat synchronous features.\nIn this work, we use two feature types—the Constant Q\nFourier Transform (CQFT) and the Low-Quefrency Con-\nstant Q Fourier Transform (LCQFT). The CQFT is com-\nputed by applying a log frequency-spaced ﬁlterbank to the\nShort Time Fourier Transform (STFT) of the audio signal.\nThe LCQFT is computed by transforming the CQFT of the\naudio signal to the cepstral domain, applying a low-pass\nlifter, and inverting the signal back to the log frequency\ndomain, analogous to MFCCs.\nThere are several parameters to choose when extract-\ning the low level features. In this work, our audio has a\nsample rate of 22050 Hz; we use 2048-point FFTs over\nhamming-windowed frames of audio. The hop size is dy-\nnamically determined based upon estimated beat locations.\nThe duration of each estimated beat is allocated 16 feature\nframes. The CQFT is computed using 24 bands per oc-\ntave beginning at the approximate frequency of the musi-\ncal note C2, yielding 178 CQFT coefﬁcients per frame. We\nuse 15 lower cepstral coefﬁcients of the CQFT to compute\nthe LCQFT (the ﬁrst cepstral coefﬁcient is ignored). A lin-\near pre-emphasis is placed over the frequency channels to\ngive higher weight to higher frequencies. This weighting\nhelps SI-PLCA avoid placing too much probability on the\nlower frequencies.\nThe CQFT preserves pitch information while reducing\nspectral resolution at higher frequencies. The liftering stage\nof the LCQFT effectively removes much of the pitch infor-\nmation from the signal while retaining the timbre infor-\nmation. Hence the CQFT-based model is a pitch-timbre-\ntemporal model, while the LCQFT-based model is a timbre-\ntemporal model. We refer to the respective models as G1\nandG2in the rest of this paper.\nWe generate several activation templates based upon the\nH\ncSTM\nG1\nFramesFreq.\nG2\nFramesFreq.Figure 2 . Left: Hdescriptor. The xaxis plots the scale\ncoefﬁcient, with 1\u0014c\u0014100, averaged across all frames.\nMiddle: G1descriptor. Right: G2descriptor.\nestimated locations of beats and bars. The templates are\nused as priors over the activation function hfor meter es-\ntimation and groove kernel extraction. The amplitudes of\nthe spikes sum to one. A duple meter and a triple me-\nter activation temple are generated without regard to the\nbar locations. These have a spike every second and third\nbeat, respectively. The templates are used for meter esti-\nmation. A duple and triple meter activation template are\nalso generated for groove kernel extraction. The duple-\nmeter activation template has a spike every fourth beat and\nthe triple meter activation template has a spike every third\nbeat. These templates are organized such that the ﬁrst spike\nis centered on the estimated location of the ﬁrst bar line.\n3.3 Meter Estimation\nWe do not know a priori what the meter of a given song\nis. To extract an effective groove representation in the fol-\nlowing stage, we set the size of the kernel based upon a\nmeter assumption. In this stage, we make a decision about\nthe meter assumption using the log probabilities of two SI-\nPLCA models.\nThe meter estimation activation templates are given as\nprior probabilities to two independent SI-PLCA models:\none with a triple, and the other with a duple meter assump-\ntion. The triple-meter model has a kernel window size of\n96 frames (2 bars in 3/4). The duple-meter model has a ker-\nnel window size of 128 frames (2 bars in 4/4). The triple\nand duple models are run until convergence, with updates\nto the activation functions allowed. There are no sparsity\nconstraints imposed on the model optimizations. However,\nsince the initial activation functions are sparse, the ﬁnal ac-\ntivation functions are also sparse. The meter of the song is\nchosen according to whichever model has the highest log\nprobability after convergence.\n3.4 Groove Kernel\nOnce the meter has been chosen, we extract a groove ker-\nnel using a ﬁnal stage of SI-PLCA. We provide a new ini-\ntial activation template hto the model in which there is an\nimpulse every 4 or 3 beats, based upon the assumed meter.\nNote that a duple meter model now has activations every4 beats, instead of 2. We also use the bar estimations pro-\nduced in the second stage to center the activations at the\nonsets of 4/4 or 3/4 bars. The window size is set to two\nbars.\nWeiss and Bello [20] have suggested that the optimal\nwindow size and meter may be learned by setting a sloping\nprior over an initial h. We tried this approach using varying\ninitializations of h, slope degrees, initial window sizes, and\nsparsity parameters. In informal listening tests we found\nthat our method provided better qualitative results when\nthe bar and beat tracker was accurate.\nOnce a groove kernel has been extracted we smooth it\nalong the time axis using a gaussian window. In this paper\nG1has no smoothing and G2is smoothed with a gaus-\nsian window having a standard deviation of 1 frame. Since\nour features have 16 frames per beat this window places\napproximately 95% of the window over 4 frames, or 1/16\nnote.\nWe do not know whether the phase of the groove kernel\nis aligned with respect to a latent two-bar groove structure\nof the music. Therefore for every groove kernel we enter a\nzero-phase and a circularly-shifted 1/2-phase version into\nour database.\n4. EXPERIMENTS\n4.1 Dataset\nWe built a dataset consisting of thousands of songs to eval-\nuate the algorithms presented in this paper. All data is pub-\nlicly available and we will provide the aggregate dataset\nand all associated metadata upon request. The data collec-\ntion steps are summarized below.\nWe used the Echo Nest developer’s API3to construct\na list of 10,000 song titles across 10 genres and 10,000\nunique artists. We began by querying the top styles in Echo\nNest’s database. An Echo Nest “style” is a search term\nassociated with artists. Styles are essentially genres; the\ntop ranked styes are those that Echo Nest believes yield\nthe strongest search results. We will refer to Echo Nest\nstyles as genres hereafter. We handpicked 10 genres from\nthe highest ranked members of the list that we associated\nwith having groove and variety. Table 1 shows the genres\nwe selected, along with their Echo Nest rank.\nFor each genre we queried 1000 unique artists that were\nalso cross-indexed with the 7digital4database, ranked by\ngenre relevance. For each artist we queried 1 unique song\nthat was in the 7digital database, ranked by Echo Nest’s\nhighest “danceability” estimation.\n7digital is a commercial music distribution service that\nmaintains .mp3 previews for most of the songs in their cat-\nalogue. We downloaded all previews in our list from the\n7digital website. While sampling the dataset we discov-\nered anomalous ﬁles. We ﬁltered these out, resulting in\n8249 unique song/artist clips each between 30 and 60 sec-\nonds long. We believe that the dataset dually exhibits a\nwide representation of groove and low redundancy.\n3http://developer.EchoNest.com/\n4http://us.7digital.com/Rank and Genre\n1 rock 2 elec-\ntronic3 hip\nhop6 jazz 14 pop\n17 reggae 19 funk 88 latin\njazz168 world 179 coun-\ntry\nTable 1 . Echo Nest ranks and genres used in this work.\n4.2 Models\nWe investigated three models, designated H,G1, and G2.\nHyields a temporal rhythm descriptor. G1yields a pitch-\ntimbre-temporal groove kernel. G2yields a timbre-temporal\ngroove kernel.\nThe Hmodel is the scale invariant rhythm descriptor\npresented by Holzapfel and Stylianou in [7]. Hcomputes\nmultiple Direct Scale Transforms (DSTs) on the autocor-\nrelation function of an Onset Strength Signal. We follow\nthe procedure outlined in [7]. The DST is computed over\na range of scale coefﬁcients. The value of the maximum\ncoefﬁcient is denoted as C. Holzapfel and Stylianou show\nthat the optimal value of Cis related to the source material,\nbut a value of C >80achieves nearly constant accuracy in\ntheir rhythm similarity tasks. The Hmodel sets C= 100 .\nThe ﬁnal descriptor is the average of the scale transform\nmagnitudes across frames.\nThe other two models are G1andG2. Their architecture\nand parameterization are described in Section 3. Note that\nthe key differences between G1andG2are that G1uses\nCQFT features. G2is built with LCQFT features and has\nsmoothing over the groove kernel.\nFigure 2 graphically depicts three extractions from the\nsame song clip using H(left), G1(middle), and G2(right).\nObserve that the Hdescriptor is a vector of Scale Trans-\nform Magnitude (STM) against a range of scaling coef-\nﬁcients (denoted c).G1andG2exhibit different images\neven though they are extracted on the same audio clip. G1\nhas ﬁner-grained detail in the temporal domain and a sus-\ntained tone with harmonics in the upper third of the image.\nThe rhythmic structure is apparent in G2, but the tonal and\ntemporal detail has been smoothed.\n4.3 Methods\nA subset of 100 musical queries—10 from each genre—\nwere randomly selected from the dataset. For each query\nand each model the top-3 nearest neighbors were selected\n(excluding the same song), as measured by cosine simi-\nlarity. Retrievals for the G2model were additionally re-\nstricted to a have an estimated tempo difference of 8 BPM\nto limit the range of tempo variation.\nThere were two sets of random retrievals. The R1re-\ntrieval set has 3 songs chosen at random for each query.\nEach retrieval in the R2set has an estimated tempo dif-\nference from its associated query of less than or equal to\n8 BPM. Tempos were estimated by computing the me-\ndian beat onset differences derived from the bar and beat\ntracker. Random retrieval sets were not restricted by genre.\nThere were two types of participants: solicited and anony-mous. Solicited participants were paid if they completed\nthe entire experiment. Both types of participants were pre-\nsented the same web-based experiment interface. Partici-\npants were randomly assigned one of ten genre-based test\nsubsets. A test subset consists of 10 same-genre queries\nand 5 retrieval sets. We collected 2,436 ratings from so-\nlicited participants and 236 ratings from anonymous par-\nticipants. There were 56 unique human evaluators that par-\nticipated in our experiment.\nThe experiment required that participants utilize a quiet\nlistening room or headphones. Participants were presented\nwith the following deﬁnition of groove [9]: “The groove\nis that aspect of the music that induces a pleasant sense of\nwanting to move along with the music.”\nWe are unaware of any studies in the literature on the\nperception of groove similarity . We therefore asked partic-\nipants to consider the similarity of groove based upon the\ngiven deﬁnition. The experimental interface further stated,\n“Please try to avoid judging groove similarity based upon\nsong genre. For instance, you may ﬁnd that two songs are\nfrom different genres, but you would move your body in a\nsimilar way to them. You should rate these songs as having\nhigh groove similarity.”\nEach participant was presented query-retrieval pairs from\ntheir test subset in random order. The audio clips were ap-\nproximately 5 seconds in duration, corresponding to the\nexpected duration of a two bar motif. They were asked\nto rate groove similarity on a coarse scale using radio but-\ntons having the labels “Not Similar”, “Somewhat Similar”,\nand “Very Similar”. These ratings were later assigned nu-\nmerical values from the set f0;1;2g. We denote these as\n“coarse” ratings. Participants were also asked to rate the\ngroove similarity of each pair on a ﬁne scale with a slider.\nThe slider had a range of [0;100], but the slider’s numerical\nvalue was not exposed to the user. We denote the ratings\nas “Fine”.\nA participant was required to listen to each pair of au-\ndio clips at least once and assign ratings before moving\nto the next comparison. Multiple listens were permitted.\nWe note that the experimental design was modeled after\nthe MIREX Audio Music Similarity and Retrieval5eval-\nuation procedure. One minor difference is that the Mirex\nAudio Similarity task asks its evaluators to rate the top-5\nranked songs per query and model. Due to limited human\nresources, we restricted the retrieval space to top-3.\n4.4 Results\nFigure 3 shows the mean coarse and ﬁne ratings per re-\ntrieval set. The red cross hairs show standard error. Table\n2 shows the results of pairwise t-tests between appropriate\nbaselines and models. Note that G2may only be directly\ncompared with R2; these were the retrieval instances where\nthe search space was restricted by tempo difference.\nThe ﬁrst thing that we notice is that participants were\npessimistic about the groove similarity of query-retrieval\npairings. The mean coarse value across all ratings was\n5http://www.music-ir.org/mirex/wiki/Audio_\nMusic_Similarity_and_RetrievalR1-H R1 -G1 R2 -G2\nCoarse 4:52\u000210\u000050:0038 0:1160\nFine 1:66\u000210\u000040:0094 0 :0142\nTable 2 . Pairwise t-testp-values. Boldface indicates a p-\nvalue less than 0.05.\nCoarse\n0.60.70.80.91\nR1 R2 H G1 G2Fine\n35404550\nR1 R2 H G1 G2\nFigure 3 . Mean coarse and ﬁne ratings by algorithm. The\nred cross hairs show standard error about the mean.\n0.780; the mean ﬁne rating was 40.909. Users were more\nlikely to rate a pair of songs as being not similar or some-\nwhat similar than very similar. The songs in the dataset\nspanned a range of 10 base genres. Several participants\nexpressed that they had difﬁculty cognitively separating\ngenre and preference from groove. Indeed, Janata et al.\nhave shown that enjoyment is correlated with groove [9].\nWe are not aware of a study that evaluates correlation be-\ntween genre and groove similarity.\nSecondly we observe that all models retrieve groove-\nsimilar songs better than random selection when the re-\ntrieval space is unrestricted by tempo. We have learned\nfrom Janata et al. that humans are able to reliably detect\nthe presence of groove. Our results support the hypothesis\nthat humans may also reliably detect groove similarity.\nWe ﬁnd that G1andHperform competitively. When\nadjusting for multiple comparisons using the Tukey-Kramer\nmethod each performs signiﬁcantly better than R1(at 95%\nconﬁdence), but they share similar statistical distributions\nwith each other for coarse and ﬁne ratings.\nWe notice that groove similarity ratings jump upward\nfor random retrieval when the space is limited to an 8 BPM\ntempo difference from the query. Humans are more likely\nto rate two arbitrary songs to have similar groove if they\nare close in tempo.\nThe only model that was evaluated with a restricted tempo\nspace was G2. As can be seen in Figure 3 and Table 2,\nthis model performed signiﬁcantly better than the tempo-\nrestricted random set on ﬁne evaluations. We do not know\nwhether the increased performance of G2is due to a (pitch-\nfree) low-level feature or the gaussian smoothing of the\ngroove kernel. Our intuition leads us to believe that smooth-\ning had a signiﬁcant impact. The kernels are fairly high-\ndimensional. By smoothing them, neighbors that were once\ndistant due to ﬁne differences in temporal structure become\nless distant (cf. Figure 2).5. CONCLUSIONS\nGroove is associated with the often pleasurable induction\nof bodily movement to music. There are an increasing\nnumber of rhythm similarity and classiﬁcation algorithms\nin the literature, yet groove encompasses a higher-level\nconstruct involving sensorimotor interaction stemming from\nrepeated acoustic patterns. We presented a new groove ker-\nnel feature based on shift and tempo invariance. We asked\nhumans to evaluate the groove kernel and another rhythm\nsimilarity model in a groove similarity retrieval task using\na diverse collection of real-world music recordings span-\nning 10 base genres.\nTheHandG1models give groove similarity rankings\nthat are signiﬁcantly better than random retrieval. The G2\nmodel performs signiﬁcantly better than random retrieval\nwhen the retrieval space is limited to an 8 BPM absolute\ndifference from the query.\nWe note that all three models—the temporal Hmodel\nand our proposed timbre-inclusive SI-PLCA based models—\nare constructed in an unsupervised manner. Building human-\nannotated collections of music is expensive. Hence there\nis higher value associated with models that do not rely on\nhuman annotation.\nOur models rely on beat-synchronous features derived\nfrom the automatically estimated bar and beat estimations.\nAs noted in Section 3 beat and downbeat estimation is not\na solved problem. We may assume that there is error in the\nestimated beat and bar locations. Unfortunately, this error\nis necessarily propagated forward through every stage of\nthe groove kernel models. We expect that our proposed\nmodels will perform better as beat and downbeat detection\nimproves.\nThe groove kernel activation templates were restricted\nto 3/4 and 4/4 meters. While the dataset included a large\nselection of world music, it is possible that the learned ker-\nnels did not ﬁt a signiﬁcant portion of the dataset. We ex-\npect that the algorithm could be improved with enhanced\nmeter detection.\nThe experimental design did not allow for a direct com-\nparison between the HandG2orG1andG2methods. We\ntherefore cannot draw conclusions regarding the impact of\nthe tempo-restricted space on these methods. We also can-\nnot state conclusively the contribution that smoothing has\non the G2model since this effect was not studied.\nOur human subject study had a limited number of hu-\nman participants with respect to the number of song queries.\nThere were 100 queries each associated with 3 models and\ntwo random baselines. An improved study would include\nthe effects of pairwise perceived genre similarity, song pref-\nerence, and other potential biases. Further investigation is\nneeded into the relationship between how much groove is\nperceived and groove similarity. Future work will include\na larger scale human evaluation with the intent to address\nthese important issues.\nTo the best of our knowledge, this paper provides the\nﬁrst human-based groove similarity retrieval task. Exper-\nimental results suggest that the groove kernel presents a\npromising direction for exploration of groove metrics.6. ACKNOWLEDGMENTS\nThis project has been supported by a Google Faculty Re-\nsearch Award and by a Neukom Institute for Computa-\ntional Science Graduate Fellowship.\n7. REFERENCES\n[1] J.A. Bilmes. Timing is of the essence: perceptual and computational\ntechniques for representing, learning, and reproducing expressive\ntiming in percussive rhythm. Master’s thesis, Massachusetts Institute\nof Technology, 1993.\n[2] M.E.P. Davies and M.D. Plumbley. A spectral difference approach to\ndownbeat extraction in musical audio. In Proc. EUSIPCO , 2006.\n[3] O. de Manzano, T. Theorell, L. Harmat, and F. Ullen. Psychophysi-\nology of ﬂow during piano playing. Emotion , 10:301–311, 2010.\n[4] S. Dixon, F. Gouyon, and G. Widmer. Towards characterisation of\nmusic via rhythmic patterns. In Proc. ISMIR , volume 5, 2004.\n[5] J. Foote, M. Cooper, and U. Nam. Audio retrieval by rhythmic simi-\nlarity. In Proc ISMIR , volume 3, pages 265–266, 2002.\n[6] A. Holzapfel and Y . Stylianou. A scale transform based method\nfor rhythmic similarity of music. In Proc. ICASSP , pages 317–320.\nIEEE, 2009.\n[7] A. Holzapfel and Y . Stylianou. Scale transform in rhythmic similarity\nof music. Audio, Speech, and Language Processing, IEEE Transac-\ntions on , 19(1):176–185, 2011.\n[8] V . Iyer. Embodied mind, situated cognition, and expressive micro-\ntiming in african-american music. Music Perception , 19:387– 414,\n2002.\n[9] P. Janata, S.T. Tomic, and J.M. Haberman. Sensorimotor coupling\nin music and the psychology of the groove. Journal of Experimental\nPsychology: General , 141(1):54–75, 2012.\n[10] J.H. Jensen, M.G. Christensen, and S.H. Jensen. A tempo-insensitive\nrepresentation of rhythmic patterns. In Proc. EUSIPCO , 2009.\n[11] C. Keil and S. Feld. Music grooves . University of Chicago Press,\nChicago, IL, 1994.\n[12] G. Madison. Experiencing groove induced by music: Consistency\nand phenomenology. Music Perception , 24:201–208, 2006.\n[13] J. Paulus and A. Klapuri. Measuring the similarity of rhythmic pat-\nterns. In Proc. ISMIR , volume 2, 2002.\n[14] G. Peeters. Spectral and temporal periodicity representations of\nrhythm for the automatic classiﬁcation of music audio signal. Au-\ndio, Speech, and Language Processing, IEEE Transactions on ,\n19(5):1242–1252, 2011.\n[15] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and G. Widmer. On\nrhythm and general music similarity. In Proc. ISMIR , volume 9,\n2009.\n[16] J. Pressing. Black atlantic rhythm: Its computational and transcul-\ntural foundations. Music Perception , 19:285–310, 2002.\n[17] E. Scheirer. Tempo and beat analysis of acoustic musical signals.\nJournal of the Acoustical Society of America , pages 588–601, 1998.\n[18] P. Smaragdis, B. Raj, and M. Shashanka. Sparse and shift-invariant\nfeature extraction from non-negative data. In Proc. ICASSP , pages\n2069–2072, 2008.\n[19] A.M Stark, M.E.P Davies, and M.D. Plumbley. Real-time beat-\nsynchronous analysis of musical audio. In Proc. DAFx , 2009.\n[20] R.J. Weiss and J.P. Bello. Unsupervised discovery of temporal struc-\nture in music. Selected Topics in Signal Processing, IEEE Journal of ,\n5(6):1240–1251, oct. 2011."
    },
    {
        "title": "Coupling Social Network Services and Support for Online Communities in Codes Environment.",
        "author": [
            "Felipe Mendonça Scheeren",
            "Marcelo Soares Pimenta",
            "Damián Keller",
            "Victor Lazzarini"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415604",
        "url": "https://doi.org/10.5281/zenodo.1415604",
        "ee": "https://zenodo.org/records/1415604/files/ScheerenPKL13.pdf",
        "abstract": "In recent years, our research group has been investigating the use of computing technology to support noviceoriented computer-based musical activities.  CODES (Cooperative Music Prototyping Design) is a Web-based environment designed to allow novice users to create musical prototypes through combining basic sound patterns. This paper shows how CODES has been changed to provide support to some concepts originally from of Social Networks and also to Online Communities having Music Creation as intrinsic motivation.",
        "zenodo_id": 1415604,
        "dblp_key": "conf/ismir/ScheerenPKL13",
        "keywords": [
            "novice-oriented",
            "computing technology",
            "novice users",
            "musical prototypes",
            "Social Networks",
            "Online Communities",
            "Music Creation",
            "intrinsic motivation",
            "Web-based environment",
            "CODES"
        ],
        "content": "COUPLING SOCIAL NETWORK SERVICES AND SUPPORT FOR ONLINE COMMUNITIES IN CODES ENVIRONMENT Felipe M. Scheeren Marcelo S. Pimenta Damián Keller Victor Lazzarini Institute of Informatics UFRGS, Brazil {felipems,mpimenta} @inf.ufrgs.br Amazon Center for Music Research UFAC, Brazil dkeller@ccrma.stanford.edu National Univ. of Ire-land, Maynooth, Ireland victor.lazzarini @nuim.ie  ABSTRACT In recent years, our research group has been investigating the use of computing technology to support novice-oriented computer-based musical activities.  CODES (Cooperative Music Prototyping Design) is a Web-based environment designed to allow novice users to create mu-sical prototypes through combining basic sound patterns.  This paper shows how CODES has been changed to provide support to some concepts originally from of So-cial Networks and also to Online Communities having Music Creation as intrinsic motivation. 1. INTRODUCTION During the last few years, our research group has been investigating the use of computing technology to support novice-oriented computer-based musical activities. The development of this support has followed an interdisci-plinary approach, and involves a multidisciplinary team of experts in Computer Music, Human-Computer Interac-tion (HCI) and Computer Supported Cooperative Work (CSCW). We are particularly interested in the conver-gence of technological support for cooperative creative activities, part of the field called “networked music”[20]. Network music allows people to explore the implications of interconnecting their computers for musical purposes. Because networked music works result from the conver-gence of social and technological aspects of Internet, this area has attracted the interest of the music technology community and the existing applications have evolved towards sophisticated projects and concepts including, for example, real-time distance performance systems, and various systems for multi-user interaction and collabora-tion. CODES is also a networked music system -  CODES is a Web-based environment designed to support music creation by means a process called Cooperative Music Prototyping (CMP) - b u t  w i t h  s p e c i a l  f o c u s  o n  music novices. Similarly to other Rich Internet Applications such as YouTube, MySpace, and Flickr – t h a t  h a v e  t u r n e d  t h e  passive user into an active producer of content, bringing into the picture new purposes, like engagement, enter-tainment and self-expression – CODES makes possible a novice be actor if their own musical experiences in music The main motivation of our work is the b e l i e f  t h a t  n o  previous musical knowledge should be required for participating in creative musical activities.  The goal of this paper is to present and discuss several concepts developed by our research group concerning how CODES has been changed for coupling social net-works services and for providing support to an online community for cooperative music making. Our environ-ment started out as a website that people could use to cre-ate their music interactively and cooperatively, but it has grown into a more g e n e r a l  o n l i n e  c o m m u n i t y  o f  p e o p l e  allowing to build an audience around music experimenta-tion, music creation, (music) knowledge sharing, and en-tertainment. 2. CODES ENVIRONMENT Differently from YouTube, Flickr, and even MySpace, where people only publish their “already ready” content, CODES is a system for music creation, instead of a sys-tem just for publishing music. CODES offers a high level music representation and user interface features to foster easy direct manipulation (drag-and-drop) of icons repre-senting sound patterns (predefined MP3 samples with 4 seconds of duration), combining them to create (new) simple musical pieces – called Music Prototypes (MPs.) Using adequate support features, CODES users can create, edit, share and publish MPs in their group or on the Web. These shared MPs can be repeatedly tested, lis-tened to, and modified by the partners, who cooperate on MP refinement. Users can start a new MP by choosing the name and the musical style they want. The selection of a musical style allows CODES to filter the sound pat-terns offered to the user. However, since all styles are available from the sound library, mixing sound patterns from different styles within the same musical prototype is still possible. Edition in CODES includes actions like “drag-and-drop” sound patterns from the sound library to the editing area, “move,” “organize,” “delete,” “expand” the duration,  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  © 2013 International Society for Music Information Retrieval     and “collapse” sound patterns to listen to the final result – a MP. When sharing a MP, the “owner” user can invite CODES users to use a search engine or may send explicit invitations via e-mail to non-members asking them for cooperation. When someone accepts such an invitation, the user becomes a prototype partner and can edit the MP like the owner does. The prototypical nature of CODES is designed and built to provide a novice-oriented perspective. At any time users can listen to the musical prototype and link arguments to their decisions. Thus, all prototype partners can discuss and change ideas about each step of the pro-totype refinement, in order to understand each other’s de-cisions. When someone considers that the resulting sounds are good, a “publication request” can be triggered and the group may discuss and deliberate about the publi-cation of this musical prototype in the CODES home page. This activity is called musical prototype publishing. As an alternative to publishing their music, users may ex-port their musical prototype, and share it at will. Thus, a novice may experiment with music by combining, listen-ing and rearranging pre-defined sound patterns to create the MPs. Furthermore, CODES users may cooperate with partners in a cyclical and collaborative process of proto-type creation - the CMP - through customized awareness, argumentation, and negotiation mechanisms until a final consensual prototype stage is reached. 2.1 Awareness Mechanisms in CODES Through CODES, anyone can draft, test, modify, and lis-ten to MPs. These actions can be done by the first author and their partners that cooperate in the refinement of the MP. This implies a focus not only on community man-agement (i.e., discovering, building, or maintaining vir-tual communities) but also on experimenting and partici-pating in specific design practices using a suitable inter-action vocabulary. This process suggests the existence of noteworthy distinct kinds of cooperation activities. Sys-tems aiming to provide effective support for these differ-ent activities have to meet specific requirements. Aware-ness and conflict resolution are already considered critical issues in general CSCW systems. However, mechanisms existing in other systems need some adaptation to take into account the idiosyncrasies of the CMP context. The ultimate goal is to provide actual cooperation, social knowledge construction, argumentation, and negotiation among the actors of the MP design activities. This type of cooperation is supported by a set of mechanisms bor-rowed from the Software Engineering and HCI areas and specially adapted for CODES, namely awareness, music prototyping rationale, authorship, version control, and conflict resolution [19]. 2.2 Related Work The most representative systems found in the literature of collective musical creation or music experimentation on the Web concerned with musical experimentation by nov-ices are: Daisyphone [4], PitchWeb [9], Web- Drum [5], Public Sound Objects – PSOs [2], and JamSpace [11]. A brief description of each system and a comparative analy-sis according to various criteria (categorized as techno-logical and architectural, computer music related, HCI related, CSCW related) may be found at [15]. In sum-mary, the main drawback of existing networked music systems comes from the lack of focus on effective coop-eration: in our viewpoint, effective collaboration is di-rectly obtained through adequate adoption of techniques such as argumentation, authorship (allowing users to know their original contributions at anytime), interaction trace, awareness, and group memory. When these tech-niques are not adequately considered or explored within interaction design (taking into account real needs and tasks of the novice users for cooperative musical interac-tions), networked music systems do not provide support for effective novice usage [16]. 2.3 CODES Principles CSCW concepts and tools have made more obvious the existence of a more connective, cooperative, and collec-tive nature of creativity rather than the prevailing focus on the individual. The creative processes being highly fuzzy, the programming of cooperative tools for creativ-ity and innovation should be adaptive and flexible. Creativity models in music have been heavily influ-enced by general models of creativity such as [10-23]. While some creativity models for music emphasize the cognitive dimensions of music creation [6-7], others con-sider its material dimension as an integral part of music creation [3-8]. In fact, to assess musical creativity is a very difficult task because social context, physical con-text, and personal factors shape the creative act and may determine the function and the dynamic of the creative processes involved [14]. Given this complex scenario, to put into practice collaboration, flexibility and multicul-turality [18], we have adopted two principles which have been confirmed by findings obtained during CODES de-velopment and usage: (a) Music creation by novices should be prototypical; and (b) Music creation by novices should be cooperative [16]. Within prototypical music creation, novices can draft an initial musical sketch (a simple MP) which can be tested, modified and listened to, applying a cyclical re-finement process until a final stage is reached. In the mu-sic literature, “draft” is the term commonly applied to ini-tial creative products. However, here the emphasis is on the cyclical prototyping process and not on the product itself. Consequently, in this chapter “prototype” and “draft” are equivalent. The prototyping process clearly resembles the incremental software development cycles adopted in the industry. Since music creation can be con-sidered a design activity, it seems natural and straight-   forward to adopt a prototypical process to model this ac-tivity. In cooperative music creation, the refinement of an initial musical idea is a consequence of the collaboration of the author with her partners. In this context, what mat-ters is not necessarily the musical quality of the finished work, but giving the possibility of a creative experience to a larger community of participants. The members build a social network by explicit invitation to cooperate until a final consensual MP stage is reached. 3. FROM CODES TO SOCIAL CODES CODES was conceived originally to be a Computer Sup-ported Cooperative Work (CSCW) system with a design based on cooperation and interaction concepts, so the evolution towards SNS and communityware is straight-forward. Even though support systems for both group types — t e a m s  a n d  c o m m u n i t i e s  — h a v e  d e v e l o p e d  in-dependently, both areas have something in common: the contact facilitation with unknown and known collabora-tors [13]. While community support systems concentrated mostly on the building process, i.e. finding people with similar interests, CSCW focused on the collaboration process, i.e. the synchronization and exchange of infor-mation in the context of a specific team task. Like [21], we are convinced that awareness can be a common base for community support systems to improve contact build-ing as well as for CSCW to maintain group work at a high performance level. Therefore, we need to address how we extend CODES from traditional CMP support and CSCW perspective towards online human commu-nity support. Successful online communities motivate online par-ticipation. An online community provides people a place to come together using the Internet: it is always on and is a more accessible way to keep in touch with people who are geographically far or with those who have conflicting schedules. Every online community has (sometimes implicit) rules that may specify how to participate and to engage with the community — f r o m  p e r i p h e r a l  p a r t i c i p a t i o n  (“lurker”) to explicitly recognized participation (“leader”). CSCW technologies can provide tools for supporting these roles. In the case of communities, these tools are used in combination, including text-based posts and chat rooms and forums that use voice, video, or avatars. The community metaphor can create several different functions for encouraging social interaction in communi-ties [12]: 1. Knowing each other; 2. Sharing preference and knowledge; 3. Generating consensus; 4. Supporting everyday life; 5. Assisting social events. A virtual community is a social network of individuals who interact through specific media, potentially crossing geographical and political boundaries in order to pursue mutual interests or goals. One of the most pervasive types of virtual community includes Social Networking Serv-ices (SNS), which consist of various online communities. Nowadays, there are lots of SNS with focus on music lis-tening and sharing, rather than music creation. YouTube and Vimeo are video sites that feature musical contents. Some SNS, as Myspace, have a social character, merging social networks aspects with content distribution. Face-book includes several musical pages and bands profiles. Even Apple tried to foster a musical social network, known as Ping. Traditionally, a SNS essentially consists of a represen-tation of each user (often a profile), his/her social links, and a variety of additional services. CODES provides a distinct vision of a SNS having music as its intrinsic mo-tivation, combining the traditional features (profiling, so-cial links) to different and specific features related to CMP. Thus, CODES has three different levels of viewing and interaction: a. Level 1: Public level, like an broadcasting channel, as Myspace, where posts are posted on to a “bulletin board” for everyone, without resorting to messaging users individually. Thus, people who do not know each other can check some data (personal information and proto-types) that the users select to publish, making this infor-mation available to any visitor; b. Level 2: Restricted one-way level, as on Twitter or Google+, when users may subscribe to other users’ posts — this is known as following and subscribers are known as followers. If a user follows someone, this is a non-mutual relationship, where who is being followed does not need to follow back. The follower can see someone else’s timeline as an RSS feed and also access MPs and even modify it (if this option is allowed in the MP), but these modifications are just local and temporary. It is also possible to repost a post from another user, and share it with one’s own followers; c. Level 3: Partnership level, when two people follow each other, it establishes a collaborative relation, where they can, not only suggest, but also edit the prototypes together. In this case, they are named ‘partners.’  CMP is an activity that involves people creating groups and working together on an MP as a shared work-space. In CODES, a cooperative musical prototype is ini-tiated by someone that creates a new prototype, elabo-rates an initial contribution, and asks for the collaboration of other “partners” by sending explicit invitations. Partners who accept the invitation can participate in the collaborative musical manipulation and refinement of the prototype. The group can publish the final or partial results of their CMP in the public spaces (level 1 above), in which interested users could discover it and follow    (level 2 above) or join the collaboration as new partners (level 3 above). In order to avoid undesired dependencies, inconsisten-cies and conflicts between contributions, preserving authorship among the several contributions of a commu-nity, CODES implements a particular layer-oriented ver-sion management mechanism. In this approach, each layer represents one partner’s view, and the union of partners’ contributions (a combination of layers) results in a cooperative MP version. Any partner can browse be-tween the contributions, independently of the creator, keeping the creator’s original ideas and authorship. It is also possible to edit another user’s contribution, by issu-ing an explicit “modification request” to a partner. The interested reader can find more details in [19]. The basic idea of our CMP process is that members cooperate not only by means of explicit conversation and explicit actions on a shared object space, but also by in-terpreting the messages and actions of other actors in ac-cordance with the model of their thinking and acting, which has been built up in the course of their interaction. A shared objects space involves prototype-oriented in-formation, which comprises all information about musi-cal prototypes, including their composition (combination of sound patterns, versions formed by layers) and social-oriented information (including interactions between ac-tors during the process). 3.1 CODES Social: Features and UI Description In our social networking services version of CODES, we choose to apply several well known HCI interface guide-lines to create a dynamic and creative environment, pro-viding SNS for the emergence of communities and ena-bling knowledge sharing by means of rich interaction and argumentation mechanisms associated with the MPs evo-lution. Several basic CODES features are similar to con-ventional features of Social Networking sites. Most often, individual users are encouraged to create profiles contain-ing various information about themselves. To protect user privacy, social networks usually have controls that allow users to choose who can view their profile, contact them, add them to their list of contacts, and so on. Users can upload pictures of themselves to their profiles, post blog entries for others to read, search for users with similar interests, and compile and share lists of contacts. In addi-tion, user profiles have a section dedicated to comments from friends and other members. CMP is a simple cyclical process including the follow-ing activities: (a) MP creation, (b) MP edition, (c) MP sharing, and (d) MP publishing. Through a Music Proto-typing Rationale (MPR) mechanism — based on the De-sign Rationale concept from HCI — each user may asso-ciate comments (i.e. an idea or an observation) and argu-ments (pro or cons) to any action on any MP. The argu-ments can be addressed to a specific partner or to the whole group. Due to the exploratory nature of CODES usage, MPR is one of its most important characteristics, allowing users to perceive and analyze group members’ actions on music prototypes (“to understand WHAT my partners are doing”) and the reasoning behind these ac-tions (“to understand WHY my partners are doing it”). The CODES interface was designed to strike a balance between user interfaces that are so easy-to-use that they end up depleting their expressiveness, and others that are so complicated that they discourage beginners. The CODES user interface has three levels of interaction for different user profiles: (a) Public Level, (b) MP Editing Level, and (c) Sound Pattern Editing Level. The lowest level of CODES — sound pattern editing — is a kind of “piano roll” editor, having no social-oriented features. Therefore, we will only discuss the other two levels. At the public level, anyone (including non-members) can access and explore musical prototypes, by searching and listening. One of the goals at this level is to encour-age the potential audience to become CODES members, and encourage members to publish their musical proto-types to foster the formation of a virtual community fo-cusing on music. \n Figure 1. CODES main page The main page at the public level is divided into four areas (see Figure 1): • (A) Top Pane: System logo, Dashboard (the main page, with user’s timeline, which shows followings updates), My Prototypes (user’s MP and the ones user is collabo-rating with), Compose (opens the CODES MP Editing Level interface), People (shows a list of CODES users and a search box), Groups (where users can choose to fol-low discussion groups or create new ones), Milestone (where users can create a set of to-dos to each MP), About page, Notification area and a dropdown menu with user settings. • (B) Left Pane: User’s name and picture with logout and three kinds of streams: Stories (the main timeline    view), Topics (users and followings discussions about it’s prototypes) and users MP’s To-dos. • (C) Main Pane: where the actions happen. All the things that you can choose in the other panes will appear on this one. Here, users will see and edit their prototypes, following or follower lists, the search results, etc. • (D) Bottom Pane: Systems’ information – not shown. The user’s main view is different of the user’s profile view. The left pane has a bigger image of the person, with links for his music gallery and friends list. And the con-tent pane shows new updates, such as new music, col-laborations, followings, or followers. The MP Editing Level is the most important level of the system. At this level, users can create and edit their MPs cooperatively (see Figure 2). The edition of a MP in fact is a simple task. The sound patterns are dragged from the sound library — a  r e g i o n  h a v i n g  i c o n s  r e p r e s e n t i n g  music instruments organized in folders named ‘rock,’ ‘funk,’ ‘jazz,’ etc. — a n d  d r o p p e d  i n t o  t h e  M P  e d i t i n g  area — the biggest region above. The sound patterns dis-played in the editing area are played from left to right. At any time, the user can play the MP existing in the MP ed-iting area using the execution control buttons: Play, Re-wind. The basic action at this level is to add or remove sound patterns within the editing area, as well as to change their sequence, size, combination, and position. Each author’s contribution in the shared workspace is identified by color: the edges of icons of sound patterns are colorful, with the same color chosen by the user at the registration. A detailed description of all features related to MP edition can be found in [15]. \n Figure 2. Musical Prototyping editing page Every new MP is a priori private, so without authori-zation no one is able to see/listen it. However, in “My Prototypes”, the user can see all MPs it has created and all the MPs it is collaborating with (as a partner). At the preferences panel for each MP, the user can select the musical style of each composition (multiple tags are al-lowed), as well as one of the following distribution op-tions: Private (No one but the owner will be able to ed-it/listen this MP), Collaborative ( P e o p l e  w i l l  b e  a b l e  t o  listen and edit the parts of the MP that the owner sets as allowed to edit), Public (Everybody can listen, Collabora-tors can edit it and followers too), Closed (Everybody can listen, but only the author can edit). Several difficulties were addressed in this work for bridging the gap between groups of novice users and the possibility of creating online communities for music making over the Web. Indeed, one of the main challenges was the combination of cooperation-oriented mechanisms (for effective collaboration in a CMP) with community-oriented mechanisms (for effective community manage-ment). 4. CONCLUSION Starting from an environment for Cooperative Activities for musical creation, this work has discussed two main challenges of networked music systems for novices: a) how to provide support for social  network services in CODES environment and; b) how to create Online Communities having music as intrinsic motivation. The first challenge is specially relevant with respect to provide different mechanisms — w e l l-known in social networks sites but not known by most of users of net-worked music systems at all — in CODES environment. The second challenge is important for providing new fea-tures and motivations for social relationships. Since peo-ple have always found listening, performing, or creating music significant in their lives, whether for enjoyment or for social cohesion, we think such online community re-lated to Music creation by novices is an interesting en-deavour because music making in modern life tends to be left in the hands of the professional artists, musicians, and singers. CODES users does not need to have previous musical knowledge for participating in musically creative activi-ties. Obviously, providing support for non-musicians or for musicians are not the same thing [17]. Musician-oriented systems usually include full and complex infor-mation, concepts, and interface functionalities that are part of the “musician’s world” and usually not understood by ordinary users. In our work such knowledge-intensive skills are replaced by strong commitment with communi-cation, cooperation and sharing by members of a commu-nity. Through CODES, novices may have the opportunity to be – like experienced musicians are – the actors of their own musical experiences. This means they can draft sim-ple musical pieces that can be modified, and repeatedly listened to, both by the first authors and by their partners that will be cooperating in the refinement of the MP. This implies a focus not only on community management (i.e.,    discovering, building or maintaining virtual communities) but also on experimenting and participating in specific design practices using a suitable interaction vocabulary. Moreover, this suggests the existence of noteworthy distinct kinds of cooperation activities, and that systems aiming to provide effective support for these different ac-tivities should also meet different requirements. 5. REFERENCES [1] Adobe: \"Flex,\" Retrivied May 10, 2013 from http://www.adobe.com/products/flex.html [2] A. Barbosa: \"Public sound objects: A shared environment for networked music practice on the web,\" Organised Sound, Vol. 10, No. 3, pp. 233–242, 2005 [3] S. Bennett: \"The process of musical creation: Interview with eight composers,\" Journal of Research in Music Education, No. 24, pp. 3–13, 1976. [4] N. Bryan-Kinns and P. G. T. Healey: \"Daisyphone: Support for remote music collaboration,\" Proceedings of the 2004 Conference on New Interfaces for Musical Expression, pp. 27-30, 2004. [5] P. Burk: \"WebDrum,\" Retrieved May 02, 2013, from http://www.transjam.com/webdrum [6] C. W. Chen: \"The creative process of computer-assisted composition and multimedia composition: Visual images and music,\" (PhD Thesis). Royal Melbourne Institute of Technology, Australia, 2006 [7] D. Collins: \"A synthesis process model of creative thinking in music composition\" Psychology of Music, Vol. 33, No. 2, pp. 193–216, 2005. [8] C. Dingwall: \"Rational and intuitive approaches to music composition: The impact of individual differences in thinking/learning styles on compositional processes,\" (Bachelor Dissertation Thesis). University of Sydney, Australia, 2008. [9] W. Duckworth: \"Making music on the web,\" Leo-nardo Music Journal, Vol. 1, No. 9, pp. 13–17, 1999. [10] H. Gardner: \"Creating minds,\" BasicBooks, New York, NY, 1993. [11] M. Gurevich: \"Jamspace: Designing a collaborative networked music space for novices,\" Proceedings of NIME, pp. 118–123, 2006. [12] T. Ishida: \"Community computing: Collaboration over global information networks,\" John Wiley and Sons, New York, NY, 1998. [13] T. Ishida (ed): \"Community computing and support systems: social interaction in network communities,\" Lecture Notes in Computer Science, Vol. 1519, 1998 [14] D. Keller, M. H. Lima, M. S. Pimenta and M. Queiroz: \"Assessing musical creativity: Material, procedural, and contextual dimensions,\" Proceedings of the 21st Conference of the Brazilian National Association of Research and Post-Graduation in Music (ANPPOM), 2011 [15] E. M. Miletto: \"CODES: An interactive novice-oriented web-based environment for cooperative musical prototyping,\" (PhD Thesis), Federal University of Rio Grande do Sul, Brazil. 2009.(available at http://hdl.handle.net/10183/2281) [16] E. M. Miletto, M. S. Pimenta, F. Bouchet, J.-P. Sansonnet and D. Keller: \"Principles for music creation by novices in networked music environments,\" Journal of New Music Research, Vol. 40, No.3, 2011 [17] E. M. Miletto, L. V. Flores, M. S. Pimenta, J. Rutily and L. Santagada: \"Interfaces for musical activities and interfaces for musicians are not the same: The case for codes, a web-based environment for cooperative music prototyping,\" Proceedings of the 9th International Conference on Multimodal Interfaces, pp. 201-207, 2007. [18] M. Pimenta, D. Keller, E. Miletto, L. V. Flores and G. G. Testa: \"Technological Support for Online Communities Focusing on Music Creation: Adopting Collaboration, Flexibility and Multiculturality from Brazilian Creativity Styles,\" pp. 283-312, In: N. A. Azab (Org.): Cases on Web 2.0 in Developing Countries, IGI Global, Hershey, 2013.  [19] M. S. Pimenta, E. M. Miletto and L. V. Flores: \"Cooperative mechanisms for networked music,\" Future Generation Computer Systems, Vol. 27, No. 1, pp. 100–108, 2011 [20] M. Schedel and J. P. Young: \"Editorial,\" Organised Sound, Vol. 10, No. 3, pp. 181–183, 2005. [21] J. Schlichter, M. Koch and C. Xu: \"Awareness - The common link between groupware and community support systems.\" Proceedings of CCSS, pp. 78-94, 1998. [22] W3C: \"Web standards,\" Retrieved May 10, 2013 from http://www.w3.org/standards/ [23] G. Wallas: \"The art of thought,\" Harcourt Brace and World, New York, NY, 1926."
    },
    {
        "title": "Learning Binary Codes For Efficient Large-Scale Music Similarity Search.",
        "author": [
            "Jan Schlüter"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416508",
        "url": "https://doi.org/10.5281/zenodo.1416508",
        "ee": "https://zenodo.org/records/1416508/files/Schluter13.pdf",
        "abstract": "Content-based music similarity estimation provides a way to find songs in the unpopular “long tail” of commercial catalogs. However, state-of-the-art music similarity measures are too slow to apply to large databases, as they are based on finding nearest neighbors among very high-dimensional or non-vector song representations that are difficult to index. In this work, we adopt recent machine learning methods to map such song representations to binary codes. A linear scan over the codes quickly finds a small set of likely neighbors for a query to be refined with the original expensive similarity measure. Although search costs grow linearly with the collection size, we show that for commercialscale databases and two state-of-the-art similarity measures, this outperforms five previous attempts at approximate nearest neighbor search. When required to return 90% of true nearest neighbors, our method is expected to answer 4.2 1-NN queries or 1.3 50-NN queries per second on a collection of 30 million songs using a single CPU core; an up to 260 fold speedup over a full scan of 90% of the database.",
        "zenodo_id": 1416508,
        "dblp_key": "conf/ismir/Schluter13",
        "keywords": [
            "binary codes",
            "commercial catalogs",
            "content-based music similarity",
            "commercial-scale databases",
            "exact nearest neighbor search",
            "linear scan",
            "machine learning methods",
            "nearest neighbors",
            "non-vector song representations",
            "state-of-the-art similarity measures"
        ],
        "content": "LEARNING BINARY CODES FOR EFFICIENT\nLARGE-SCALE MUSIC SIMILARITY SEARCH\nJan Schl ¨uter\nAustrian Research Institute for Artiﬁcial Intelligence, Vienna\njan.schlueter@ofai.at\nABSTRACT\nContent-based music similarity estimation provides a\nway to ﬁnd songs in the unpopular “long tail” of com-\nmercial catalogs. However, state-of-the-art music similar-\nity measures are too slow to apply to large databases, as\nthey are based on ﬁnding nearest neighbors among very\nhigh-dimensional or non-vector song representations that\nare difﬁcult to index.\nIn this work, we adopt recent machine learning methods\nto map such song representations to binary codes. A lin-\near scan over the codes quickly ﬁnds a small set of likely\nneighbors for a query to be reﬁned with the original expen-\nsive similarity measure. Although search costs grow lin-\nearly with the collection size, we show that for commercial-\nscale databases and two state-of-the-art similarity measures,\nthis outperforms ﬁve previous attempts at approximate near-\nest neighbor search. When required to return 90% of true\nnearest neighbors, our method is expected to answer 4.2\n1-NN queries or 1.3 50-NN queries per second on a collec-\ntion of 30 million songs using a single CPU core; an up to\n260 fold speedup over a full scan of 90% of the database.\n1. INTRODUCTION\nContent-based music similarity measures allow to scan a\ncollection for songs that sound similar to a query, and could\nprovide new ways to discover music in the steadily grow-\ning catalogs of online distributors. However, an exhaustive\nscan over a large database is too slow with state-of-the-art\nsimilarity measures. A possible solution are Filter-and-\nReﬁne indexing methods: To ﬁnd the knearest neighbors\n(k-NN) to a query, a preﬁlter returns a small subset of the\ncollection, which is then reﬁned to the kbest items therein.\nHere, we consider the following scenario: (1) We have\na commercial-scale music collection, (2) we want to return\non average at least a fraction Qof the items an exhaustive\nscan would ﬁnd, and (3) we cannot afford costly compu-\ntations when a song enters or leaves the collection (ruling\nout nonparametric methods, or precomputing all answers).\nWe then search for the fastest indexing method under these\nconstraints.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.Compared to existing work on approximate k-NN search,\nwhat makes this quest special is the nature of state-of-the-\nart music similarity measures, and a low upper bound on\ndatabase sizes: The largest online music store only offers\n26 million songs as of February 2013, while web-scale im-\nage or document retrieval needs to handle billions of items.\nAmong the ﬁrst approaches to fast k-NN search were\nspace partitioning trees [1]. McFee et al. [12] use an ex-\ntension of k-d trees on 890,000 songs, reporting a 120 fold\nspeedup over a full scan when missing 80% of true neigh-\nbors. No comparison to other methods is given.\nHash-based methods promise cheap lookup costs. Cai\net al. [2] apply Locality-Sensitive Hashing (LSH) [6] to\n114,000 songs, but do not evaluate k-NN recall. Torralba\net al. [23] learn binary codes with Restricted Boltzmann\nMachines (RBMs) for 12.9 mio. images, achieving 80%\n50-NN recall looking at 0.2% of the database and deci-\nsively outperforming LSH. Using similar techniques, other\nresearchers learn codes for documents [16] and images [8,\n13], but, to the best of our knowledge, never for songs.\nPivot-based methods map items to a vector space only\nusing distances to landmark items. Rafailidis et al. [15]\napply L-Isomap to 9,000 songs. Schnitzer et al. [19] apply\nFastMap to 100,000 songs, achieving 80% 10-NN recall\nlooking at 1% of the collection. Ch ´avez et al. [3] map items\nto byte vectors with good results, but on our data, their\napproach needs 50% of a collection to ﬁnd 80% of 1-NN.\nIn what follows, we will adapt the most promising meth-\nods for application on two music similarity measures and\nevaluate their performance under our scenario.\n2. FILTER-REFINE COST MODEL\nThe cost of answering a query using a ﬁlter-and-reﬁne ap-\nproach can be decomposed into the cost for running the\npreﬁlter and the cost for reﬁning the selection to kitems:\ncost \fltref(n;k) = cost \flt(n;k) + cost ref(n\flt(n;k);k)\nWe assume that the reﬁne step is a linear scan over the\ncandidates returned by the preﬁlter, picking the kbest:\ncost ref(nf;k) =nf\u0001(R+ log(k)\u0001S); (1)\nwhereRis the cost for computing the distance of a query\nto a candidate, and log(k)\u0001Sis the cost for updating a\nk-element min-heap of the best candidates found so far.1\n1This model is not entirely correct, as the heap is generally not up-\ndated for each item. However, for k\u0014100\u001cnf, we found the sorting\ncost to be indeed linear in nf, which is all we need for our argument.As a baseline method meeting our requirement of ﬁnd-\ning on average a fraction Qof the true neighbors, we adopt\na zero-cost preﬁlter returning a fraction Qof the dataset:\ncost baseline (n;k) = cost ref(Q\u0001n;k)\nUnder these assumptions, using a preﬁlter gives the fol-\nlowing speedup factor over the baseline:\nspu(Q;n;k ) =cost ref(Q\u0001n;k)\ncost \fltref(n;k)\n=\u0012cost \flt(n;k) + cost ref(n\flt(n;k);k)\nQ\u0001cost ref(n;k)\u0013\u00001\n=Q\u0001\u0012cost \flt(n;k)\ncost ref(n;k)+n\flt(n;k)\nn\u0013\u00001\n=Q\u0001\u0000\n\u001at(n;k) +\u001as(n;k)\u0001\u00001(2)\nWe see that making the preﬁlter fast compared to a full\nscan (small\u001at) is just as important as making the ﬁlter se-\nlective (small \u001as). More speciﬁcally, we need to minimize\nthe sum of the two ratios to maximize the speedup factor.\nAs we will see in our experiments, some existing meth-\nods put too much emphasis on a fast preﬁlter, resulting in\n\u001at(n;k)being orders of magnitude smaller than \u001as(n;k)\nespecially for large databases. In this work we will bal-\nance the two ratios better to maximize the performance.\n3. MUSIC SIMILARITY MEASURES\nFor our experiments, we choose two very different similar-\nity measures: One based on high-dimensional vectors, and\nanother based on Gaussian distributions.\n3.1 Vector-Based Measure\nSeyerlehner et al. [20] propose a set of six Block-Level Fea-\ntures to represent different aspects of a song’s audio con-\ntent, totalling in 9448 dimensions. These features work\nwell for genre classiﬁcation and tag prediction [21], and\nsimilarity measures based on them ranked among the top\nthree algorithms in the MIREX Audio Music Similarity\n(AMS) tasks 2010–2012. For the similarity measure, the\nsix feature vectors are individually compared by Manhat-\ntan distance, and the resulting feature-wise distances are\ncombined to form the ﬁnal similarity estimation.\nTo combine the feature-wise distances, they must be\nbrought to similar scale. Instead of ﬁnding six appropriate\nscaling factors on an arbitrary dataset, Seyerlehner et al.\nnormalize the feature-wise distance matrices for the han-\ndled collection: This Distance Space Normalization (DSN)\nprocesses each distance matrix entry by subtracting the\nmean and dividing by the standard deviation of its row and\ncolumn.2The six normalized matrices are added up and\nnormalized once again to form the ﬁnal similarities.\nWhile the normalizations seem unnecessarily complex,\nFlexer et al. [5] recently showed that they remove hubs –\nitems appearing as neighbors of undesirably many other\nitems – and are vital to achieve state-of-the-art results.\n2When it is infeasible to compute full distance matrices, the song-\nwise distance statistics can be approximated from a random subset of the\ncollection and stored with each feature vector.3.2 Gaussian-Based Measure\nAs a second method, we use the timbre model proposed\nby Mandel and Ellis [11]: Each song is represented by the\nmean vector and covariance matrix of its frame-wise Mel-\nFrequency Cepstral Coefﬁcients (MFCCs).3Song dis-\ntances are computed as the symmetrized Kullback-Leibler\ndivergence between these multivariate Gaussian distribu-\ntions [17, p. 24], and normalized with DSN.\nThis measure does not reach state-of-the-art performance\non its own, but forms the main component of [14], which\nranked among the top two algorithms in the MIREX AMS\ntasks 2009–2012. Furthermore, it is easy to reproduce and\nallows direct comparison to Schnitzer et al. [18, 19].\n4. INDEXING METHODS\nWe will evaluate seven different methods for fast k-NN\nsearch: One oblivious to the indexed dataset, four based\non song models and two based on song similarities.\n4.1 Locality-Sensitive Hashing (LSH)\nFor vectors in an Euclidean space, the family of projec-\ntions onto a random line, followed by binary threshold-\ning or ﬁxed-width quantization, is locality-sensitive : For\nsuch projections, two close items are more probable to be\nmapped to the same value than two items far apart [6].\nLSH usesL\u0001Kprojections to map each item xitoLdis-\ncreteK-dimensional vectors hl(xi). UsingLconventional\nhash-table lookups, it can quickly ﬁnd all items xjmatch-\ning a queryqin at least one vector, 9l\u0014Lhl(q) =hl(xj).\nHere, this serves as a preﬁlter for ﬁnding neighbor can-\ndidates. Increasing Kmakes it more likely for candidates\nto be true nearest neighbors, but strongly reduces the can-\ndidate set size. Increasing Lcounters this, but increases\nquery and storage costs. As a complementary way to in-\ncrease the number of candidates, Multi-probe LSH [10]\nconsiders items with a close match in one of their vectors.\n4.2 Principal Component Analysis (PCA)\nPCA ﬁnds a linear transformation y=W0xof Euclidean\nvectors xi2X to a lower-dimensional space minimizing\nthe squared reconstruction errorP\nikxi\u0000W W0xik2\n2.\nNearest neighbors in the low-dimensional space are good\ncandidates for neighbors in the original space, so a linear\nscan over items in the low-dimensional space serves as a\nnatural preﬁlter. The candidate set size can be tuned at will\nto achieve a target k-NN recall. Increasing the dimension-\nality of the space allows to reduce the candidate set size,\nbut increases preﬁlter costs.\n4.3 Iterative Quantization (ITQ)\nITQ [7] ﬁnds a rotation of the PCA transformation mini-\nmizing squared reconstruction error after bit quantization\nof the low-dimensional space:P\nikxi\u0000Wb(W0xi)k2\n2,\nwhere bi(z)is1for positiveziand0otherwise.\n3Speciﬁcally, we use frames of 46 ms with 50% overlap, 37 Mel bands\nfrom 0 Hz to 11025 Hz and retain the ﬁrst 25 MFCCs.It can serve as a preﬁlter just like PCA, but using bit\nvectors reduces computational costs for the linear scan.\nFor compact bit codes, neighbors within a small hamming\ndistance of a query can alternatively be found with a con-\nstant number of conventional hash table lookups.\n4.4 PCA Spill Trees\nK-d Trees [1] are binary trees recursively partitioning a\nvector space: Each node splits the space at a hyperplane,\nassigning the resulting half-spaces to its two child nodes.\nSpill Trees [9] allow the half-spaces to overlap, making it\nless likely for close items to be separated. McFee et al. [12]\nadditionally propose to choose hyperplanes perpendicular\nto a dataset’s principal components, and to strongly restrict\nthe depth of the tree. In the resulting PCA Spill Tree , each\nitem ends up in one or more leaves, with similar items of-\nten sharing at least one leaf. Locating the leaves for an\nitem is linear in the database size [12, Sec. 3.6], but can be\navoided by precomputing all leaf sets.\nAs in [12], we regard all items in the leaf sets of a query\nto be candidate neighbors. The candidate set size can be\nincreased by decreasing the tree depth or by increasing the\noverlap at each node.\n4.5 Auto-Encoder (AE)\nAn AE ﬁnds a nonlinear transformation of inputs to a low-\ndimensional or binary code space and back to the input\nspace, minimizing the difference between inputs and re-\nconstructions (e.g., `2distance for Euclidean input vec-\ntors). Similar to PCA and ITQ, candidate neighbors to a\nquery can quickly be found in the code space.\nThe transformation is realized as an artiﬁcial neural net-\nwork and can be optimized with backpropagation. For\ndeep networks, it is helpful to initialize the network weights\nusing Restricted Boltzmann Machines (RBMs). Salakhut-\ndinov et al. [16] were the ﬁrst to use a deep AE for ap-\nproximate nearest neighbor search, under the term Seman-\ntic Hashing , and describe the method in detail.\n4.6 Hamming Distance Metric Learning (HDML)\nHDML [13] ﬁnds a nonlinear transformation to a binary\ncode space optimized to preserve neighborhood relations\nof the input space. Speciﬁcally, for any triplet (x;x+;x\u0000)\nof items for which xis closer tox+than tox\u0000in the input\nspace, it aims to have xcloser tox+than tox\u0000in the\ncode space. Again, the transformation is realized as an\nartiﬁcial neural network, optimized with backpropagation,\nand HDML can be used as a preﬁlter just like ITQ or AE.\n4.7 FastMap\nFastMap [4] maps items to a d-dimensional Euclidean space\nbased on their (metric) distances to dpreviously chosen\npivot pairs in the input space. Schnitzer et al. [19] show\nhow to apply FastMap to Gaussian-based models and pro-\npose an improved pivot selection strategy we will adopt.\nFastMap serves as a preﬁlter like PCA, but supports\nnon-vector models as it is purely distance-based.5. EXPERIMENTS\nWe will now compare the seven indexing methods empiri-\ncally, conducting a range of retrieval experiments.\n5.1 Dataset and Methodology\nFrom a collection of 2.5 million 30-second song excerpts\nused in [18, 19], we randomly select 120k albums of 120k\ndifferent artists. We use 10k albums (124,013 songs) for\ntraining, 20k albums (246,117 songs) for validation and\nthe remaining 90k albums (1,101,737 songs) for testing. In\naddition, we use 20k albums (253,347 songs) of the latter\nas a smaller test set.\nFor each applicable combination of similarity measure\nand indexing method, we will train different parameteriza-\ntions of the method on the training set and determine the\nspeedup over the baseline (Eq. 2) for retrieving on aver-\nage 90% of the 1 or 50 nearest neighbors on the validation\nset. We will then evaluate the best parameterizations on\nthe small test set to ensure we did not overﬁt on the vali-\ndation set, and use the large test set to assess the methods’\nscalability.\n5.2 Vector-based Measure\nTo be able to compute the speedup, we ﬁrst determine\nthe costs of the similarity measure.4Computing 1 mil-\nlion 9,448-dimensional Manhattan distances takes 2.361 s,\nﬁnding the (indices of) the smallest 100 distances takes\n1.17 ms, and both costs scale linearly with the collection\nsize, as assumed in Eq. 1. Costs for the approximate DSN\nare negligible (see Sect. 3.1). For preﬁlters based on a lin-\near scan, computing 1 million 80-dimensional `2distances\ntakes 22 ms, and computing 1 million 1024-bit hamming\ndistances takes 9.8 ms. The costs of ﬁnding the best candi-\ndates in a linear scan depend on the candidate set size; we\nwill use separate measurements for each case.\nPCA: We start by evaluating PCA as a preﬁlter, as it\nproved useful as a preprocessing step for most other ﬁlters\nas well. To mimic how the similarity measure is combined\nfrom six features, we ﬁrst apply PCA to each feature sep-\narately, compressing to about 10% of its size, then rescale\neach feature to unit mean standard deviation (this brings\nthe distances to comparable ranges, and forms good inputs\nfor the AE later) and stack the compressed features to form\nan 815-dimensional vector. Finally, we apply another PCA\nto compress these vectors to a size suitable for preﬁltering.\nIn Table 1, we see that this cuts down query costs: For\nretrieving 90% of the true nearest neighbors, preﬁltering\nwith a linear scan over 40-dimensional PCA vectors takes\n\u001at= 0:56% the time of a full scan and only needs to exam-\nine\u001as= 0:26% of the database afterwards, resulting in a\n110 fold speedup over the baseline ( 0:9=(0:0052+0:0026) ,\nEq. 2). For retrieving 50-NN, it needs a larger candidate\nset, increasing the preﬁlter costs (higher sorting costs to\nﬁnd the candidates), but still achieving a 47 fold speedup.\n4All timings are reported on an Intel Core i7-2600 3.4 GHz CPU with\nDDR3 RAM, use a single core, and leverage A VX/POPCNT instructions.\nImplementations are in C, carefully optimized to maximize throughput.Method1-NN 50-NN\n\u001at(%)\u001as(%) spu \u001at(%)\u001as(%) spuPCA20 dim 0.38 0.59 93x 0.58 1.85 37x\n40 dim 0.56 0.26 110x 0.71 1.20 47x\n80 dim 1.01 0.18 76x 1.16 1.12 40xLSH8 bit 0.00 17.23 5x 0.00 23.82 4x\n16 bit 0.00 6.66 14x 0.00 11.00 8x\n20 bit 0.00 4.68 19x 0.00 8.42 11xmp-LSH128x16 bit 0.83 11.12 8x 0.83 34.18 3x\n64x32 bit 0.83 6.03 13x 0.83 12.23 7x\n32x64 bit 0.83 3.65 20x 0.83 7.59 11x\n16x128 bit 0.83 3.58 20x 0.83 7.11 11x\n1x256 bit 0.10 3.85 23x 0.10 7.98 11x\n8x256 bit 0.83 2.80 25x 0.83 6.22 13xITQ64 bit 0.03 5.43 16x 0.03 9.94 9x\n128 bit 0.05 4.74 19x 0.05 8.27 11x\nSpill Tree 0.00 10.25 9x 0.00 21.27 4xAE64 bit 0.03 2.14 41x 0.03 4.40 20x\n128 bit 0.05 0.57 144x 0.05 2.47 36x\n256 bit 0.10 0.24 265x 0.10 1.28 65x\n512 bit 0.21 0.14 258x 0.21 0.93 79x\n1024 bit 0.42 0.09 177x 0.42 0.70 81xFastMap40 dim 0.77 1.56 39x 1.11 3.72 19x\n80 dim 1.20 1.36 35x 1.53 3.43 18x\n128 dim 1.73 1.20 31x 2.03 3.07 18x\nTable 1 . Results for the vector-based music similarity\nmeasure on the validation set of 246,117 songs: Ratio of\npreﬁlter time to full scan ( \u001at), ratio of candidate set to\ndataset size ( \u001as) and resulting speedup over baseline (spu)\nfor retrieving 90% of 1 and 50 true nearest neighbors.\nVarying the vector dimensionality changes the tradeoff be-\ntween\u001atand\u001as, but does not improve the speedup.\nLSH: We apply different versions of LSH to the 815-\ndimensional intermediate PCA representation.5First, we\nfollow Slaney et al. [22] to compute optimal quantization\nwidth, dimensionality and table count for 90% 1-NN recall\nunder the assumption that all projections are independent\n(it suggests 92.192, 25 and 430, respectively). To reach our\ntarget 1-NN recall, we need a 3-fold increase in table count\nand obtain\u001as= 16:52%, which is not competitive. Turn-\ning to binary LSH, we ﬁx the dimensionality Kto 8, 16 or\n20 bit and increase Luntil we reach 90% recall (for 20 bits\nand 50-NN, we need 8353 hash tables). Even assuming\nzero preﬁlter costs, speedup is far below PCA. As a third\nalternative, we use a simple version of multi-probe LSH:\nWe ﬁxLandK, but consider all buckets within a ham-\nming distance of rto the query in any of the tables. We\nincreaserto reach the target k-NN recall, still achieving\nmoderate speedups of up to 25x only.\nITQ directly builds on the PCA transform above, but\nmaps items to bit vectors. Instead of directly tuning the\n5PCA is a useful stepping stone as the DSN (Sect. 3.1) invalidates any\ntheoretical guarantees of LSH ﬁnding the nearest neighbors in the original\nspace. Directly working on the 9448-dimensional vectors, rescaling the\nsix components to comparable range, consistently gave worse results.\n0.1% 1% 10% 100%\ncandidate set size / database size0%20%40%60%80%100%true 50-NN among candidatesAE, 1024 bit\nAE, 256 bit\nPCA, 40 dim\nFastMap, 40 dim\nmp-LSH, 4x256 bit\nITQ, 128 bit\nPCA Spill TreeFigure 1 . 50-NN recall versus candidate set size for the\nvector-based music similarity measure on the test set of\n253,347 songs, averaged over all 253,347 possible queries.\ncandidate set size, we consider all items in a hamming ball\nof radiusraround the query (in code space), and tune r.\nThis avoids the sorting costs for ﬁnding the candidates.\nITQ has small \u001at, but large\u001as, resulting in low speedups.\nPCA Spill Tree: We build a tree with spill factor 0.1\n(the best performing in [12]) and adjust the depth to reach\nour target recall. Assuming zero preﬁlter costs, it achieves\npoor speedups as it needs very large candidate sets.\nAE: We train a deep AE on the 815-dimensional inter-\nmediate PCA representation, pretrained with stacked RBMs\nas in [8]. We use an encoder architecture of 1024-256-128-\n64 layers for the shorter codes, 1024-512 and 2048-1024\nfor the two longer codes.6We encourage binary codes by\nadding noise in the forward pass as in [16]; especially for\n128 bits and more, this worked better than thresholding as\nin [8]. For 256 bits and less, it also helped to encourage\nzero mean code unit activations as in [13, Eq. 12].\nWe use the learned codes as in ITQ. We obtain a preﬁlter\nwhich is both faster than PCA and more selective, achiev-\ning a 265 fold speedup for 1-NN and 81 fold speedup for\n50-NN queries. Note how the accuracy of longer codes\npays off for 50-NN, while shorter codes win for 1-NN.\nHDML did not yield any improvement over AE.\nFastMap is about twice as fast as LSH or ITQ, but falls\nbehind AE and PCA.\nWe evaluate the best-performing instantiations of each\nmethod on the small test set and ﬁnd results to be very\nsimilar to Table 1. As the relative preﬁlter costs \u001atstay\nthe same anyway, we only show how the candidate set size\n\u001asand 50-NN recall interact (Fig. 1). We can see that the\n1024-bit AE again only needs about 0.7% of the dataset to\nﬁnd 90% of 50-NN, and we see that AE and PCA perform\nbest over a wide range of target recall values. Besides,\ncomparison with [12, Fig. 4] shows that our PCA Spill Tree\nperforms similar to its ﬁrst publication.\n6Results are robust to the exact architecture as long as there is at least\none layer before the code layer, and the ﬁrst layer is wide enough.Method1-NN 50-NN\n\u001at(%)\u001as(%) spu\u001at(%)\u001as(%) spuPCA20 dim 6.18 16.03 4x 10.96 29.80 2x\n40 dim 6.00 14.04 4x 11.11 28.79 2xAE64 bit 0.06 11.89 8x 0.06 19.36 5x\n128 bit 0.11 6.95 13x 0.11 15.77 6x\n1024 bit 0.91 4.76 16x 0.91 13.13 6xHDML128 bit 0.11 1.46 57x 0.11 3.73 23x\n256 bit 0.23 1.37 56x 0.23 3.93 22x\n2x128 bit 0.23 1.15 65x 0.23 3.12 27x\n4x128 bit 0.45 1.09 58x 0.45 3.02 26x\n1024 bit 0.91 1.20 43x 0.91 4.65 16xFastMap40 dim 2.03 2.62 19x 3.37 6.48 9x\n80 dim 2.78 1.85 19x 3.85 4.94 10x\n128 dim 3.98 1.81 16x 5.03 4.85 9x\nTable 2 . Results for the Gaussian-based music similarity\nmeasure on the validation set of 246,117 songs.\n5.3 Gaussian-based Measure\nAgain, we ﬁrst determine the costs of the similarity mea-\nsure: Computing 1 million symmetric Kullback-Leibler\n(sKL) divergences between 25-dimensional full-covariance\nGaussian models takes 1.085 s, using precomputed inverse\ncovariance matrices as in [17, Ch. 4.2]. Note that most in-\ndexing methods evaluated above are vector-based and not\napplicable to Gaussian models, so we expect the most from\nHDML and FastMap, but still try AE and PCA to be sure.\nAE: In order for learned codes to be useful, they must\nreﬂect the input space. For the input space at hands, it\nseems natural to learn codes by minimizing the sKL di-\nvergence between inputs and reconstructions. For this to\nwork, the AE must be forced to output valid covariance\nmatrices \u0006, otherwise it quickly learns to produce recon-\nstructions that push the sKL divergence unboundedly be-\nlow zero. We solve this by representing models in terms of\nthe mean vector and Cholesky decomposition of \u0006(mul-\ntiplying the reconstructed Cholesky decomposition by it-\nself transposed always gives a positive-semideﬁnite \u0006), but\nour sKL-optimizing AEs only learn to reconstruct the cen-\ntroid of all training data. Interestingly, however, ordinary\n`2-optimizing AEs beneﬁt from the modiﬁed input rep-\nresentation. Using the same architectures as in Sect. 5.2\nand a similar preprocessing (we separately compress mean\nvectors and Cholesky decompositions with PCA to 99.9%\nvariance, then scale to unit mean standard deviation), we\nobtain moderate speedups of up to 16x.\nPCA on the same representation performs poorly.\nHDML learns codes from triplets of items (x;x+;x\u0000),\nsee Sect. 4.6. We select x+among thek+nearest neigh-\nbors ofx, andx\u0000outside the 500 nearest neighbors. Dur-\ning training, we gradually increase k+from 10 to 200. In-\nstead of training a randomly initialized network as in [13],\nwe ﬁne-tune the existing AEs. We obtain good results\nwith 128-bit codes, but longer codes do not improve the\nspeedup. To close the gap between \u001atand\u001as, we instead\n0.1% 1% 10% 100%\ncandidate set size / database size0%20%40%60%80%100%true 50-NN among candidatesHDML, 4x128 bit\nHDML, 128 bit\nFastMap, 80 dim\nAE, 1024 bit\nPCA, 40 dimFigure 2 . 50-NN recall versus candidate set size for the\nGaussian-based music similarity measure on the test set of\n253,347 songs.\nMethod1-NN 50-NN\n\u001at(%)\u001as(%) spu\u001at(%)\u001as(%) spuHDML128 bit 0.11 1.19 69x 0.11 3.14 28x\n2x128 bit 0.23 0.93 78x 0.23 2.61 32x\n4x128 bit 0.45 0.88 67x 0.45 2.08 36xFastMap40 dim 1.56 0.94 36x 2.17 2.27 20x\n80 dim 2.53 0.85 27x 3.14 2.19 17x\n128 dim 3.67 0.69 21x 4.20 1.86 15x\nTable 3 . Results for the Gaussian-based music similarity\nmeasure on the test set of 1.1 million songs.\nemploy multiple 128-bit codes handled as in mp-LSH, ob-\ntaining an up to 65 fold speedup over the baseline.\nFastMap is faster than AE, but slower than HDML. Re-\nsults fall a bit behind [19] because unlike Schnitzer et al.,\nwe evaluate against nearest neighbors found with DSN.\nAgain, Fig. 2 demonstrates that our conclusions also\nhold for the test set and a wide range of target recall values.\n5.4 Scalability\nFinally, we evaluate how the best-performing approaches\nscale with the collection size. For the large test set of 1.1\nmillion songs, it is computationally infeasible to compute\nthe exact DSN, and we do not want to evaluate an approxi-\nmate retrieval algorithm against approximate ground truth.\nThus, we will limit ourselves to the Gaussian-based mea-\nsure, omitting the DSN altogether (as in [19]).\nFrom Table 3, we ﬁnd that the results scale better than\nlinearly, because all methods need smaller candidate sets.\nFor FastMap, the improvement is partly explained by eval-\nuating against non-DSN neighbors: On the validation set,\nthis alone improves the speedup by about 60% (it does not\nimprove HDML, which seemingly learned the DSN well).\nStill extrapolating linearly from the validation set to 30\nmillion songs, the best methods are expected to answer\n4.2 1-NN queries or 1.3 50-NN queries per second on the\nvector-based measure, and 2.2 1-NN queries or 0.9 50-NNqueries per second on the Gaussian-based one, using a sin-\ngle CPU core, with 90% true nearest neighbor recall.7\n6. DISCUSSION\nWe have shown how to learn binary codes for song rep-\nresentations of two state-of-the-art music similarity mea-\nsures that are useful for fast k-NN retrieval. Furthermore,\nwe have demonstrated that for collection sizes encountered\nin MIR, a k-NN index based on a linear scan can outper-\nform sublinear-time methods when we require a particular\naccuracy – even more so as scan-based methods are em-\nbarrassingly parallel and can be easily distributed or per-\nformed on a GPU. Note that our experiments explicitly tar-\ngetted commercial-scale collections and song-level search.\nFor user collections, PCA or FastMap will be preferable\nas they can quickly adapt to any dataset; training AE and\nHDML took 20 and 180 minutes, respectively. For sim-\nilarity search on a ﬁner scale (e.g., 10-second snippets),\ncollections could grow to require sublinear-time methods.\nFor future work, it may be worthwhile to evaluate the\nbinary representation in different scenarios: Do we obtain\nqualitatively good results using the codes alone, omitting\nthe reﬁne step? Do the codes prove useful for classiﬁcation\nor clustering?\n7. ACKNOWLEDGEMENTS\nThe author would like to thank Mohammad Norouzi for\npublishing his HDML implementation, and Maarten Grach-\nten for fruitful discussions.\nThis research is supported by the Austrian Science Fund\n(FWF): TRP 307-N23. The Austrian Research Institute for\nArtiﬁcial Intelligence is supported by the Austrian Federal\nMinistry for Transport, Innovation, and Technology.\n8. REFERENCES\n[1] J. L. Bentley. Multidimensional binary search trees used\nfor associative searching. Commun. ACM , 18(9):509–517,\nSeptember 1975.\n[2] R. Cai, C. Zhang, L. Zhang, and W.-Y . Ma. Scalable music\nrecommendation by search. In Proc. of the 15th Int. Conf. on\nMultimedia (ACM-MM) , 2007.\n[3] E. Ch ´avez, K. Figueroa, and G. Navarro. Proximity search-\ning in high dimensional spaces with a proximity preserving\norder. In Proc. of the 4th Mexican Int. Conf. on Artiﬁcial In-\ntelligence (MICAI) , 2005.\n[4] C. Faloutsos and K.I. Lin. Fastmap: A fast algorithm for in-\ndexing, data-mining and visualization of traditional and mul-\ntimedia datasets. In Proc. of the ACM SIGMOD Int. Conf. on\nManagement of Data , 1995.\n[5] A. Flexer, D. Schnitzer, and J. Schl ¨uter. A mirex meta-\nanalysis of hubness in audio music similarity. In Proc. of the\n13th Int. Soc. for Music Information Retrieval Conf. (ISMIR) ,\nPorto, Portugal, 2012.\n7This assumes all models are held in main memory, which can be\npractically achieved by distributing queries over a cluster. Preliminary ex-\nperiments also indicate that the vector-based models can be compressed\nto 10% of their size with a minor impact on accuracy (also cf. [21]), re-\nmoving a possible memory bandwidth bottleneck for the reﬁne step.[6] A. Gionis, P. Indyk, and R. Motwani. Similarity search in\nhigh dimensions via hashing. In Proc. of the 25th Int. Conf.\non Very Large Data Bases , 1999.\n[7] Yunchao Gong and Svetlana Lazebnik. Iterative quantization:\nA procrustean approach to learning binary codes. In Proc. of\nthe IEEE Int. Conf. on Computer Vision and Pattern Recog-\nnition (CVPR) , 2011.\n[8] A. Krizhevsky and G. Hinton. Using very deep autoencoders\nfor content-based image retrieval. In Proc. of the 19th Europ.\nSymp. on Artiﬁcial Neural Networks (ESANN) , 2011.\n[9] T. Liu, A. W. Moore, A. Gray, and K. Yang. An investiga-\ntion of practical approximate nearest neighbor algorithms. In\nNeural Information Processing Systems 17 (NIPS) . 2005.\n[10] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and K. Li. Multi-\nprobe LSH: Efﬁcient indexing for high-dimensional similar-\nity search. In Proc. of the 33rd Int. Conf. on Very Large Data\nBases , 2007.\n[11] M. Mandel and D. Ellis. Song-level features and support vec-\ntor machines for music classiﬁcation. In Proc. of the 6th Int.\nSoc. for Music Information Retrieval Conf. (ISMIR) , pages\n594–599, 2005.\n[12] B. McFee and G. Lanckriet. Large-scale music similarity\nsearch with spatial trees. In Proc. of the 12th Int. Soc. for\nMusic Information Retrieval Conf. (ISMIR) , 2011.\n[13] M. Norouzi, D. Fleet, and R. Salakhutdinov. Hamming dis-\ntance metric learning. In Neural Information Processing Sys-\ntems 24 (NIPS) . 2012.\n[14] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and G. Wid-\nmer. On rhythm and general music similarity. In Proc. of the\n10th Int. Soc. for Music Information Retrieval Conf. (ISMIR) ,\n2009.\n[15] D. Rafailidis, A. Nanopoulos, and Y . Manolopoulos. Nonlin-\near dimensionality reduction for efﬁcient and effective audio\nsimilarity searching. Multimedia Tools Appl. , 51(3):881–895,\n2011.\n[16] R. Salakhutdinov and G. Hinton. Semantic hashing. Int. Jour-\nnal of Approximative Reasoning , 50(7), 2009.\n[17] D. Schnitzer. Mirage – high-performance music similarity\ncomputation and automatic playlist generation. Master’s the-\nsis, Vienna Univ. of Technology, 2007.\n[18] D. Schnitzer. Indexing Content-Based Music Similarity Mod-\nels for Fast Retrieval in Massive Databases . PhD thesis, Jo-\nhannes Kepler Univ. Linz, Austria, 2011.\n[19] D. Schnitzer, A. Flexer, and G. Widmer. A ﬁlter-and-reﬁne\nindexing method for fast similarity search in millions of mu-\nsic tracks. In Proc. of the 10th Int. Soc. of Music Information\nRetrieval Conf. (ISMIR) , 2009.\n[20] K. Seyerlehner, G. Widmer, and T. Pohle. Fusing block-level\nfeatures for music similarity estimation. In Proc. of the 13th\nInt. Conf. on Digital Audio Effects (DAFx) , 2010.\n[21] K. Seyerlehner, G. Widmer, M. Schedl, and P. Knees. Auto-\nmatic music tag classiﬁcation based on block-level features.\nInProc. of the 7th Sound and Music Computing Conf. (SMC) ,\nBarcelona, Spain, 2010.\n[22] M. Slaney, Y . Lifshits, and J. He. Optimal parameters for\nlocality-sensitive hashing. Proceedings of the IEEE , 100(9),\n2012.\n[23] A. Torralba, R. Fergus, and Y . Weiss. Small codes and large\nimage databases for recognition. In Proc. of the IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR) , 2008."
    },
    {
        "title": "Learning Rhythm And Melody Features With Deep Belief Networks.",
        "author": [
            "Erik M. Schmidt",
            "Youngmoo E. Kim"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417185",
        "url": "https://doi.org/10.5281/zenodo.1417185",
        "ee": "https://zenodo.org/records/1417185/files/SchmidtK13.pdf",
        "abstract": "Deep learning techniques provide powerful methods for the development of deep structured projections connecting multiple domains of data. But the fine-tuning of such networks for supervised problems is challenging, and many current approaches are therefore heavily reliant on pretraining, which consists of unsupervised processing on the input observation data. In previous work, we have investigated using magnitude spectra as the network observations, finding reasonable improvements over standard acoustic representations. However, in necessarily supervised problems such as music emotion recognition, there is no guarantee that the starting points for optimization are anywhere near optimal, as emotion is unlikely to be the most dominant aspect of the data. In this new work, we develop input representations using harmonic/percussive source separation designed to inform rhythm and melodic contour. These representations are beat synchronous, providing an event-driven representation, and potentially the ability to learn emotion informative representations from pre-training alone. In order to provide a large dataset for our pre-training experiments, we select a subset of 50,000 songs from the Million Song Dataset, and employ their 3060 second preview clips from 7digital to compute our custom feature representations.",
        "zenodo_id": 1417185,
        "dblp_key": "conf/ismir/SchmidtK13",
        "keywords": [
            "deep learning",
            "fine-tuning",
            "supervised problems",
            "pretraining",
            "magnitude spectra",
            "acoustic representations",
            "harmonic/percussive source separation",
            "beat synchronous",
            "emotion informative representations",
            "pre-training experiments"
        ],
        "content": "LEARNING RHYTHM AND MELODY FEATURES WITH\nDEEP BELIEF NETWORKS\nErik M. Schmidt and Youngmoo E. Kim\nMusic and Entertainment Technology Laboratory (MET-lab)\nElectrical and Computer Engineering, Drexel University\nfeschmidt, ykim g@drexel.edu\nABSTRACT\nDeep learning techniques provide powerful methods for\nthe development of deep structured projections connecting\nmultiple domains of data. But the ﬁne-tuning of such net-\nworks for supervised problems is challenging, and many\ncurrent approaches are therefore heavily reliant on pre-\ntraining, which consists of unsupervised processing on the\ninput observation data. In previous work, we have in-\nvestigated using magnitude spectra as the network obser-\nvations, ﬁnding reasonable improvements over standard\nacoustic representations. However, in necessarily super-\nvised problems such as music emotion recognition, there\nis no guarantee that the starting points for optimization are\nanywhere near optimal, as emotion is unlikely to be the\nmost dominant aspect of the data. In this new work, we\ndevelop input representations using harmonic/percussive\nsource separation designed to inform rhythm and melodic\ncontour. These representations are beat synchronous, pro-\nviding an event-driven representation, and potentially the\nability to learn emotion informative representations from\npre-training alone. In order to provide a large dataset for\nour pre-training experiments, we select a subset of 50,000\nsongs from the Million Song Dataset, and employ their 30-\n60 second preview clips from 7digital to compute our cus-\ntom feature representations.\n1. INTRODUCTION\nDeep learning is rapidly becoming one of the most pop-\nular topics in the machine learning community, and such\napproaches offer powerful methods for ﬁnding deep struc-\ntured connections in data. But the success of these methods\noften hinges on pre-training, or unsupervised methods that\nare used to provide a starting point to perform gradient de-\nscent optimization. As there is no guarantee of convexity\nin these problems, ﬁnding a useful initial starting point is\nparamount, as the best case scenario is generally limited to\nﬁnding a good local minima.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.In previous work, we have looked into deep learning\nmethods for the prediction of musical emotion [1–3]. Deep\nbelief networks (DBNs) were trained on magnitude spectra\nobservations with the goal of predicting Arousal-Valence\n(A-V) coordinates, where valence indicates positive ver-\nsus negative emotion, and arousal indicates emotional in-\ntensity. Using these models, the individual layers were\ntreated as basis functions for feature extraction, and the\nlearned representations were shown to outperform standard\nmusic information retrieval (Music-IR) features (e.g., mel-\nfrequency cepstral coefﬁcients).\nBut in looking to further improve these approaches,\nmany questions remain in the pre-training methodology.\nUnsupervised methods such as restricted Boltzman ma-\nchine (RBM) pre-training reduce the dimensionality of\ndata based on the most prominent aspects. For instance,\nvery compelling results have been shown on text data,\nwhere reducing text documents to two dimensions related\ndirectly to document type [4]. If the goal is to build a doc-\nument type classiﬁer, then this approach will yield an ex-\ncellent starting position, but if it is document emotion we\nwish to model, then such a starting point may be no better\nthan random. The same is true in music; if we cannot have\nthe expectation of learning useful domains from unsuper-\nvised pre-training, then we should have low expectations\nfor the supervised ﬁne-tuning.\nIn this new work, we develop DBN input represen-\ntations speciﬁcally designed to allow the model to learn\nabout rhythm and melodic contour in an unsupervised\nfashion. Learning to understand these phenomena nec-\nessarily requires the ability to parse musical events, and\nwe therefore begin with beat tracking, such that the mod-\nels can be provided with a history of feature snapshots at\nmusically relevant time points. In all of our feature ex-\ntraction, we utilize harmonic/percussive source separation\n(HPSS) [5], allowing us to deconstruct the spectrum, sep-\narating out melody and harmonic sources from drums and\npercussive sources.\nWith the percussive spectra, we compute a beat syn-\nchronous percussion timbre feature, allowing us to parse\ndifferent drum sounds and construct rhythm models by an-\nalyzing a history of samples. For the harmonic spectra, we\ncompute a 48-dimensional beat synchronous chroma rep-\nresentation that allows the ability to track melodic contour\nover multiple octaves. In addition, we investigate the use\nof the 2-d FFT of beat synchronous chroma over four beatsegments, providing a shift (transposition) invariant feature\nfor melodic contour that has been shown to be successful\nin cover song recognition [6].\nIn order to provide a reasonable dataset for pre-training\nwe employ a set of 50,000, 30-60 second audio clips from\n7digital that were randomly selected from the Million Song\nDataset [7]. The DBNs are ﬁne-tuned for predicting mu-\nsical emotion using a publicly available dataset of time-\nvarying musical emotion data [8].\n2. BACKGROUND\nDeep learning and DBN based feature learning is a topic\nof expanding attention in the machine listening commu-\nnity [9]. Lee et al. was one of the ﬁrst to apply deep\nbelief networks to acoustic signals, employing an unsuper-\nvised convolutional approach [10]. Their system employed\nPCA to provide a dimensionality reduced representation of\nthe magnitude spectrum as input to the DBN and showed\nslight improvement over MFCCs for speaker, gender, and\nphoneme detection.\nHamel and Eck applied DBNs to the problems of mu-\nsical genre identiﬁcation and autotagging [11]. Their ap-\nproach used raw magnitude spectra as the input to their\nDBNs, which were constructed from three layers and em-\nployed ﬁfty units at each layer. The system was trained\nusing greedy-wise pre-training and ﬁne-tuned on a genre\nclassiﬁcation dataset, consisting of 1000, 30-second clips.\nThe learned representations showed reasonable increases\nin performance over standard feature representations on\nboth genre recognition and autotagging. The authors have\nalso found signiﬁcant improvement in moving to multi-\ntimescale representations [12, 13].\nBattenberg and Wessel applied conditional DBNs in\nmodeling drum patterns in recent work, which incorpo-\nrated an autoregressive time-varying restricted Boltzman\nmachine model that can be used for generating sequences\n[14]. One downside of the conditional RBM for the appli-\ncation discussed in this new work is that the input history\n(past samples) only contributes to the bias term between\nthe visible and hidden layer, and therefore the full informa-\ntion about rhythm may not be available in the upper hidden\nmodel layers.\n3. DATA COLLECTION\nIn this paper, we use a universal background model style\npre-training, initializing our models on a dataset of 50,000\nsongs, followed by ﬁne-tuning on a 240 song labeled\ndataset of 15-second clips annotated with A-V emotion.\n3.1 Unsupervised Pre-Training Data\nFor the unsupervised pre-training phase we seek to employ\na large dataset in order to expose our model to a wide dis-\ntribution of musical data. As such, we select a subset of\n50,000 tracks from the Million Song Dataset (MSD). As\nthe MSD includes only proprietary features, and we seek to\nhandcraft original domains, we employ their 30-60 secondpreview clips from the 7digital API1. In order to ensure\nquality audio, we ﬁrst download clips for the entire MSD\nand ﬁlter out any songs with less than 128 kbps MP3 bi-\ntrate, lower than 22050 Hz sampling rate, clips shorter than\n30 seconds, clips that were found to be silent, and ones that\nhad bad frames or ﬁle corruption issues.\n3.2 Supervised Fine-Tuning Dataset\nFor the supervised musical emotion training dataset, we\nemploy a corpus annotated in previous work using Ama-\nzon’s Mechanical Turk (MTurk) [8]. The dataset contains\n240, 15-second song clips that were sampled from a larger\ncorpus that was annotated at 1-second intervals using a\ngame-based approach. Each song clip was selected to en-\nsure a uniform distribution was provided across the four\nquadrants of the A-V space. The goals of the MTurk activ-\nity were to assess the effectiveness of the game and to de-\ntermine any biases created though collaborative labeling.\nOverall, the datasets were shown to be highly correlated,\nwith arousal r= 0:712, and valence r= 0:846. The\nMTurk dataset is available to the research community2\nand is densely annotated, containing 4;064label sequences\nin total ( 16:93\u00062:690ratings per song).\n4. ACOUSTIC REPRESENTATIONS\nAs previously discussed, learning to understand musical\nattributes, such as rhythm and melody, necessarily requires\nthe ability to parse musical events. As such, the success of\nthese methods hinges on our ability to accurately beat track\nmusic audio. All acoustic representations developed in this\nwork employ harmonic/percussive source separation. With\nbeat tracking, HPSS allows us to ﬁnd the best onsets pos-\nsible using percussive spectra. With rhythm features, it al-\nlows us to isolate just percussion (or percussive contribu-\ntions of other pitched instruments), and to create features\nbased on the timbre of percussion on the beat. Finally, with\npitch features, it allows us to isolate harmonic (pitched)\nsources when creating our chroma representations. Fig-\nure 1 shows the feature extraction process for each stage\nin our processing. Beat tracking is shown in the center,\nand percussion and pitch features are on the left and right,\nrespectively.\n4.1 Harmonic/Percussive Source Separation\nAs the time/frequency considerations are different for each\nof our feature extraction chains (i.e., beat tracking, pitch,\npercussion timbre), we must perform HPSS three times for\neach of our 50,000 pre-training songs. As a result, we elect\nto use an efﬁcient median ﬁltering based approach [5]. The\ngeneral idea of HPSS is that harmonic signals correspond\nto horizontal lines in the spectrogram (i.e., Fourier series)\nand percussive signals correspond to vertical lines (i.e., im-\npulses). In Figure 2, we show a simple audio example of a\nguitar and drums mix.\n1http://developer.7digital.net/\n2http://music.ece.drexel.edu/research/emotion/moodswingsturkSTFT\nHPSSBeatTrackingSTFTHPSSMFCCSTFTHPSSCQTBeatAggregation2DFFT\nInputAudioHop: 64Win: 2048Hop: 256Win: 512Hop: 256Win: 4096\nPercussiveSpectraPercussiveSpectraHarmonicSpectraPitchClassesPercussionTimbreBeatAggregationPercussionTimbreFeature2D FFTChromaFeatureBeat SynchronousChromaFeatureMelFilterbank128 MelBandsFigure 1 . Feature extraction process for percussion timbre\n(left), beat detection (center), and pitch chroma represen-\ntations (right).\n0 100 200 300 400 500 600\nFrame0246810Frequency (kHz)Original Magnitude Spectra\nFigure 2 . Original audio input spectra.\nMedian ﬁltering based HPSS performs two passes of\nmedian ﬁltering (vertically and horizontally) in order to\ngenerate spectral masks, and is therefore extremely efﬁ-\ncient. Figure 3 shows the HPSS separation for the spectro-\ngram shown in Figure 2.\n4.2 Beat Tracking\nOur beat tracking approach begins with an STFT with a 64\nframe hop (\u00183msec) to provide maximal time resolution,\nfollowed by the application of a 128 bin mel-spaced ﬁlter\nbank that provides vertical smoothing in the spectrum, thus\nmaking percussive onsets more prominent. Following the\nﬁlter bank, the onset proﬁle is computed via a multidimen-\nsional Laplace ﬁlter, and the ﬁlter means are used in an\nEllis-style beat tracker using librosa3[15].\n3https://github.com/bmcfee/librosa\n0246810Frequency (kHz)Harmonic Spectra\n0 100 200 300 400 500 600\nFrame0246810Frequency (kHz)Percussive SpectraFigure 3 . Harmonic percussive source separation.\n4.3 Percussion Timbre\nIn order to train a model to understand rhythm, we extract\na percussion timbre feature. This feature is shown in the\nleft column of Figure 1, where we ﬁrst extract the STFT\nwith a window size of 512 samples ( \u001823msec) and hop\nsize of 256 (\u001811:6msec). We then compute HPSS, fol-\nlowed by MFCCs of the percussive spectra, providing a\npercussion timbre feature (e.g., to differentiate the boom\nsound of a bass drum vs. the hit of a snare). We then beat\naggregate this feature such that the DBN is provided with\nevent-driven feature updates to learn rhythmic styles. As\nshown in Figure 4, we can parse rhythm visually.\n0 2 4 6 8 10 12 14\nBeat Frame051015MFCC DimensionBeat Synchronous Percussive Spectra MFCCs\nFigure 4 . Beat synchronous aggregation of mel-frequency\ncepstral coefﬁcients computed from the percussive spec-\ntrogram (percussion timbre).\n4.4 Pitch Chroma\nFollowing a similar pattern to the percussion timbre, we\nbegin our chroma representation with HPSS as well, but\nwith a much larger STFT window size. Here we use a\n4096 (\u0018186msec) window in order to provide reasonable\nfrequency precision on bass frequencies. Next, we apply a\nmagnitude constant-Q transform (CQT) ﬁlter bank starting\nat G 2(98Hz), the ﬁrst CQT ﬁlter that ﬁts comfortably into\nour STFT representation, and spanning four octaves up to\nF]6/G[6(1479.98). Figure 5 displays our 48-dimensional\nchroma representation.\nTo learn a model of how the chroma evolve, we will\nneed to present the DBN with multiple frames, and we\ntherefore elect to center those event around beats, as we\ncan have a reasonable expectation of a correlation with\nnote onsets. This also greatly reduces the number of train-0 100 200 300 400 500 600\nFrame010203040Pitch ChromaFour Octave ChromagramFigure 5 . Four octave pitch chroma.\ning frames in our dataset, making the approach more com-\nputationally feasible. Figure 6 shows the beat aggregation\nof our chroma feature.\n0 2 4 6 8 10 12 14\nBeat Frame010203040Pitch ChromaBeat Synchronous Chromagram\nFigure 6 . Beat synchronous chroma.\n4.5 Chroma 2-d FFT\nFor our DBN input representation, we investigate using the\nmagnitude 2-d FFT of our beat synchronous chroma rep-\nresentation. This feature was previously investigated in\nthe realm of cover song detection, where it was found to\nperform well using 75-beat patches, providing shift (trans-\nposition) and time invariant properties for melody within\na song [6]. Here we shorten this observation to just four\nbeats, with the goal of obtaining shift/transposition invari-\nance, but still retaining time information. Figure 7 shows\nthis feature, where it is computed for each shift of a four\nbeat window.\n0 2 4 6 8 10\nBeat Frame020406080Pitch Chroma2-d FFT of Beat Synchronous Chromagram\nFigure 7 . 2-d FFT of beat synchronous chroma.\n5. DEEP BELIEF NETWORKS\nA trained deep belief network shares an identical topology\nto a neural network, though they offer a far-superior train-\ning procedure, which begins with an unsupervised pre-\ntraining that models the hidden layers as restricted Boltz-\nman machines (RBMs) [4, 16, 17] . A graphical depiction\nof our ﬁrst layer RBM is shown in Figure 8, which uses\nfour beat synchronous frames of observations in the input.\nAn RBM is a generative model that contains only a single\nhidden layer, and in simplistic terms they can be thought\nof as two sets of basis vectors, one which reduces the di-\nmensionality of the data and the other that reconstructs it.\nBeat 1Beat 2Beat 3Beat 4VisibleLayerFeaturesHiddenLayerW•••Figure 8 . A restricted Boltzman machine with multiple\nbeat observations.\nRBMs are Markov random ﬁelds (MRFs) with hidden\nunits, in a two layer architecture where we have visible\nunits vand hidden units h. During pre-training, we learn\nRBMs “greedily,” where we learn them one at a time from\nthe bottom up. That is, after we learn the ﬁrst RBM we\nretain only the forward weights, and use them to create the\ninput for training the next RBM layer.\nFor our ﬁrst layer RBM we employ a Gaussian-\nbinomial RBM, where the visible data is represented as\na Gaussian distribution. The advantage of this representa-\ntion over the standard binomial-binomial is that a sigmoid\nfunction is not applied during inference when estimating\nthe visible layer from the hidden. With the standard bino-\nmial units, most visible values are forced to 0 or 1,\np(vi= 1jh) =\u001b(bi+X\njwijhj); (1)\nwhere the visible layer is v2R1\u0002I, the hidden layer h2\nR1\u0002J, and the model has parameters W2RI\u0002J, with\nbiases c2R1\u0002Jandb2R1\u0002I.\nThe Gaussian-binomial RBM allows a more continuous\nrange,\np(vijh) =N(bi+X\njwijhj;1): (2)\nDuring our greedy-wise pre-training we use Gaussian-\nbinomial RBMs at the ﬁrst layer, which presents continu-\nous data, but all subsequent layers use standard binomial-\nbinomial RBMs.\nFor the Gaussian-binomial RBM, we have an energy\ndistribution of the form,\nE(v;h) =X\ni2visible(vi\u0000bi)2\n2\u0000X\nj2hiddencjhj\u0000X\ni;jvihjwij;\n(3)\nand the standard binomial-binomial RBM has an energy\nfunction of the form,\nE(v;h) =\u0000X\ni2visiblebivi\u0000X\nj2hiddencjhj\u0000X\ni;jvihjwij:(4)\nAs in the typical approach to deep learning, after pre-\ntraining we form a multi-layer perceptron using only the\nforward weights of the RBM layers. As our goal is to learn\nfeature detectors for a regression problem, we lastly attach\na linear regression layer and report the prediction error for\nﬁne-tuning as the mean squared error of the estimators. Wetrained our DBNs using Theano,4a Python-based package\nfor symbolic math compilation.\n6. EXPERIMENTS AND RESULTS\nIn the following experiments, we investigate employing\nDBNs for rhythm and melody feature learning. For each\nDBN, we use shrinking layer sizes, where layer 0 contains\n75 nodes, layer 1 contains 50 nodes, and layer 2 contains\n25 nodes. The goal with this approach is to best take ad-\nvantage of the dimensionality reduction power of RBMs.\nFor our pre-training dataset, we use the 50,000 7dig-\nital preview clips described in Section 3.1, with beat syn-\nchronous features that are represented by taking every shift\nof a 4 beat window, and vectorizing the input as shown in\nFigure 8. For each feature type, we show the pre-training\nvisible data dimensionality and total number of examples\nbelow in Table 1.\nDBN Input Number of\nModel Domain Dimensionality Pre-Training Examples\nRhythm 80 4 ;645 ;595\nPitch Chroma 192 4 ;645 ;526\n2-d FFT Chroma 384 4 ;495 ;526\nTable 1 . DBN pre-training Data\nPre-training epochs of 5, 10, 20, and 50 are investigated\nfor all feature types with a learning rate of 10\u00005. The\nbest validation scores were found for 10 epochs with both\nthe chroma and 2-d FFT of chroma, and 50 epochs with\nthe percussion timbre representation. For each pre-trained\nmodel, we perform gradient descent back propagation ﬁne-\ntuning for each fold, where for each input example xi, we\ntrain the model to produce the emotion space parameter\nvector yi,\nyi= [\u0016a; \u0016v]: (5)\nIn performing ﬁne-tuning we note that our DBNs are\nbeat-synchronous, but our labeled data is annotated at one-\nsecond intervals. In order to ﬁne-tune our DBNs to predict\nemotion, we use linear interpolation to estimate the val-\nues of emotion on the beat. However, since we seek to\ncompare this method to that of previous work, it neces-\nsarily must be evaluated on the second-by-second data test\nset. Therefore, after DBNs are trained and layer-wise fea-\ntures are computed, we then aggregate DBN features over\nthe past 1-second, as is done with the standard feature do-\nmains, providing features at the same rate as the original\nlabels.\nWe evaluate these learned representations in the con-\ntext of multiple linear regression (MLR), as we have in-\nvestigated in prior work [18–20], where we develop re-\ngressors to predict the parameterization vector yiof a two-\ndimensional Gaussian in A-V space,\nyi= [\u0016a; \u0016v; \u001b2\naa; \u001b2\nvv; \u001b2\nav]: (6)\n4http://deeplearning.net/software/theano/In all supervised experiments, the model training is\ncross-validated 5 times, dividing the dataset into 50%\ntraining, 20% veriﬁcation, and 30% testing. To avoid the\nwell-known album-effect, we ensured that any songs that\nwere recorded on the same album were either placed en-\ntirely in the training or testing set. Note, for three songs\nin the dataset, the beat tracker returned less than four beats\nin the labeled portion of the song, and as a result they had\nto be removed from the sets. Those songs are IDs: 2996,\n5232, 6258. We post updated results for standard features\nover previous work in Table 2, and note that their removal\nleaves the results nearly unchanged [3].\nAs in previous approaches, we use Euclidean dis-\ntances to evaluate our A-V mean predictions in a normal-\nized space (i.e., axes bound between -0.5 and 0.5), and\nKullback-Liebler divergences to analyze our Gaussian pre-\ndictions. We note a slight difference in the KL divergence\ncalculation from our previous work,\nKL(pjjq) =1\n2\u0010\nlogj\u0006qj\nj\u0006pj+tr(\u0006\u00001\nq\u0006p)+\n(\u0016q\u0000\u0016p)T\u0006\u00001\nq(\u0016q\u0000\u0016p)\u0000d\u0011\n;(7)\nwhere in previous work we had omitted the1\n2multiplier\nterm (see [20]).\nFeature Average Mean Average KL\nType Distance Divergence\nMFCC 0:140\u00060:004 0 :642\u00060:069\nChroma 0:182\u00060:005 1 :654\u00060:143\nSpectral Shape 0:153\u00060:005 0 :755\u00060:074\nSpectral Contrast 0:139\u00060:005 0 :647\u00060:072\nENT 0:151\u00060:005 0 :700\u00060:079\nTable 2 . Emotion regression results from previous work\nfor ﬁfteen second clips.\nResults for the different DBN feature types are shown\nin Table 3. For each learned feature type, we investigate\nthat feature alone, as well as that feature in combination\nwith the others. As we pull the spectrum apart with HPSS\nto learn the different DBN features, it makes sense that we\nshould put the two domains back together for prediction.\nThe rhythm feature, which uses the percussion timbre as\ninput, is the best single performing feature at a mean error\nof 0.128, and the best result overall is when combining the\npitch and 2-d FFT of chroma at 0.113 mean error.\n7. DISCUSSION AND FUTURE WORK\nThis work presented a novel approach for training deep be-\nlief networks for understanding rhythm and melody. The\nﬁne-tuned DBN features easily outperformed any other\nsingular existing representation, and the combination of\nthe rhythm and melody DBN features outperformed any\nother system previously tested on this dataset.\nIn moving forward with deep learning approaches that\nrequire pre-training, we believe that it should be based\naround input observations from which high level musicalDBN DBN Pre-training Model Error Fine-tuning Model Error\nLayer Feature Type Mean Distance KL Divergence Mean Distance KL Divergence\nLayer 0 Rhythm 0:146\u00060:007 0 :681\u00060:083 0:139\u00060:009 0 :652\u00060:085\nLayer 1 Rhythm 0:148\u00060:007 0 :708\u00060:086 0 :132\u00060:013 0 :598\u00060:094\nLayer 2 Rhythm 0:156\u00060:006 0 :754\u00060:086 0:128\u00060:016 0 :582\u00060:104\nLayer 0 Pitch 0:160\u00060:004 0 :786\u00060:105 0:143\u00060:015 0 :678\u00060:122\nLayer 1 Pitch 0:161\u00060:005 0 :792\u00060:086 0 :131\u00060:022 0 :618\u00060:149\nLayer 2 Pitch 0:165\u00060:006 0 :815\u00060:095 0:129\u00060:024 0 :608\u00060:153\nLayer 0 2-d FFT Pitch 0:171\u00060:007 0 :889\u00060:103 0:148\u00060:014 0 :716\u00060:132\nLayer 1 2-d FFT Pitch 0:175\u00060:007 0 :926\u00060:116 0 :137\u00060:022 0 :669\u00060:166\nLayer 2 2-d FFT Pitch 0:175\u00060:006 0 :915\u00060:112 0:129\u00060:024 0 :620\u00060:170\nLayer 0 Rhythm+Pitch 0:145\u00060:006 0 :685\u00060:082 0:129\u00060:012 0 :604\u00060:091\nLayer 1 Rhythm+Pitch 0:147\u00060:007 0 :709\u00060:078 0 :117\u00060:019 0 :534\u00060:122\nLayer 2 Rhythm+Pitch 0:153\u00060:007 0 :743\u00060:089 0:114\u00060:022 0 :514\u00060:125\nLayer 0 Rhythm+2-d FFT Pitch 0:147\u00060:005 0 :707\u00060:075 0:131\u00060:013 0 :606\u00060:098\nLayer 1 Rhythm+2-d FFT Pitch 0:151\u00060:006 0 :739\u00060:077 0 :119\u00060:019 0 :552\u00060:128\nLayer 2 Rhythm+2-d FFT Pitch 0:156\u00060:007 0 :763\u00060:082 0:113\u00060:022 0 :514\u00060:131\nTable 3 . Emotion regression results for Mechanical Turk annotated clips. Rhythm features use percussion timbre as input,\npitch features use beat synchronous chroma, and 2-d FFT pitch features use our four beat 2-d FFT of chroma representation.\nFeature combination results are all early fusion based (concatenation of dimensions).\nideas like rhythm, melody, and harmony can easily be ex-\ntracted. Furthermore, as understanding these ideas nec-\nessarily requires the presentation of time-series data, fu-\nture approaches should further investigate the best way to\npresent this information to the ﬁrst DBN layer.\nIn continuing this work, we wish to further analyze the\noptimal number of beat synchronous frames to present to\nthe DBN input, as well as investigating smaller units of\nmusical events, such as eighth or sixteenth note feature up-\ndates. It would also be interesting to apply these learned\nfeatures in the context of a graphical model such as a con-\nditional random ﬁeld, as investigated in prior work [21].\n8. ACKNOWLEDGMENT\nThis work is supported by National Science Foundation\nawards IIS-0644151 and CNS-0960061.\n9. REFERENCES\n[1] E. M. Schmidt and Y . E. Kim, “Learning emotion-based acoustic\nfeatures with deep belief networks,” in IEEE WASPAA , New Paltz,\nNY , 2011.\n[2] ——, “Modeling the acoustic structure of musical emotion with deep\nbelief networks,” in NIPS Workshop on Music and Machine Learn-\ning, 2011.\n[3] E. M. Schmidt, J. Scott, and Y . E. Kim, “Feature learning in dynamic\nenvironments: Modeling the acoustic structure of musical emotion,”\ninISMIR , Porto, Portugal, October 2012.\n[4] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality\nof data with neural networks,” Science , vol. 313, no. 5786, pp. 504–\n507, July 2006.\n[5] D. FitzGerald, “Harmonic/percussive separation using median ﬁlter-\ning,” in DAFx , Graz, Austria, September 2010.\n[6] T. Bertin-Mahieux and D. P. W. Ellis, “Large-scale cover song recog-\nnition using the 2d fourier transform magnitude,” in ISMIR , Porto,\nPortugal, October 2012.\n[7] T. Bertin-Mahieux, D. P. W. Ellis, B. Whitman, and P. Lamere, “The\nMillion Song Dataset,” in ISMIR , Miami, FL, October 2011.\n[8] J. A. Speck, E. M. Schmidt, B. G. Morton, and Y . E. Kim, “A com-\nparative study of collaborative vs. traditional annotation methods,”\ninISMIR , Miami, Florida, 2011.[9] E. J. Humphrey, J. P. Bello, and Y . LeCun, “Moving beyond feature\ndesign: Deep architectures and automatic feature learning in music\ninformatics,” in ISMIR , Porto, Portugal, October 2012.\n[10] H. Lee, Y . Largman, P. Pham, and A. Y . Ng, “Unsupervised feature\nlearning for audio classiﬁcation using convolutional deep belief net-\nworks,” in NIPS , 2009.\n[11] P. Hamel and D. Eck, “Learning features from music audio with deep\nbelief networks,” in ISMIR , Utrecht, Netherlands, 2010.\n[12] P. Hamel, S. Lemieux, Y . Bengio, and D. Eck, “Temporal pooling\nand multiscale learning for automatic annotation and ranking of mu-\nsic audio,” in ISMIR , Miami, FL, October 2011.\n[13] P. Hamel, Y . Bengio, and D. Eck, “Building musically-relevant au-\ndio features through multiple timescale representations,” in ISMIR ,\nPorto, Portugal, October 2012.\n[14] E. Battenberg and D. Wessel, “Analyzing drum patterns using con-\nditional deep belief networks,” in ISMIR , Porto, Portugal, October\n2012.\n[15] D. P. W. Ellis, “Beat tracking by dynamic programming,” JNMR ,\nvol. 36, no. 1, pp. 51–60, March 2007.\n[16] G. E. Hinton, S. Osindero, and Y . Teh, “A fast learning algorithm\nfor deep belief nets,” Neural Computation , vol. 18, no. 7, pp. 1527–\n1554, 2006.\n[17] Y . Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy\nlayer-wise training of deep networks,” in NIPS , 2007.\n[18] E. M. Schmidt, D. Turnbull, and Y . E. Kim, “Feature selection for\ncontent-based, time-varying musical emotion regression,” in ACM\nMIR, Philadelphia, PA, 2010.\n[19] E. M. Schmidt and Y . E. Kim, “Prediction of time-varying musi-\ncal mood distributions from audio,” in ISMIR , Utrecht, Netherlands,\n2010.\n[20] ——, “Prediction of time-varying musical mood distributions using\nKalman ﬁltering,” in IEEE ICMLA , Washington, D.C., 2010.\n[21] ——, “Modeling musical emotion dynamics with conditional ran-\ndom ﬁelds,” in ISMIR , Miami, FL, 2011."
    },
    {
        "title": "An Experiment about Estimating the Number of Instruments in Polyphonic Music: A Comparison Between Internet and Laboratory Results.",
        "author": [
            "Michael Schoeffler",
            "Fabian-Robert Stöter",
            "Harald Bayerlein",
            "Bernd Edler",
            "Jürgen Herre"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417943",
        "url": "https://doi.org/10.5281/zenodo.1417943",
        "ee": "https://zenodo.org/records/1417943/files/SchoefflerSBEH13.pdf",
        "abstract": "Internet experiments in the fields of music perception and music information retrieval are becoming more and more popular. However, not many Internet experiments are compared to laboratory experiments, the consequence being that the effect of the uncontrolled Internet environment on the results is unknown. In this paper the results of an Internet experiment with 1168 participants are compared to those of the same experiment with 62 participants but previously conducted in a controlled environment. The comparison of the Internet and laboratory results enabled us to make a point on whether the Internet can be used for our experiment procedure. The experiment aimed to investigate the listeners ability to correctly estimate the number of instruments being played back in a given excerpt of music. The participants listened to twelve short classical and pop music excerpts each composed using one to six instruments. For each music excerpt the participants were asked how many instruments they could hear and how certain they were about their estimation.",
        "zenodo_id": 1417943,
        "dblp_key": "conf/ismir/SchoefflerSBEH13",
        "keywords": [
            "Internet experiments",
            "music perception",
            "music information retrieval",
            "uncontrolled Internet environment",
            "controlled environment",
            "experiment results",
            "Internet can be used",
            "listeners ability",
            "correctly estimate",
            "number of instruments"
        ],
        "content": "AN EXPERIMENT ABOUT ESTIMATING THE NUMBER OF\nINSTRUMENTS IN POLYPHONIC MUSIC: A COMPARISON BETWEEN\nINTERNET AND LABORATORY RESULTS\nMichael Schoefﬂer, Fabian-Robert St ¨oter, Harald Bayerlein, Bernd Edler, J ¨urgen Herre\nInternational Audio Laboratories Erlangen\nmichael.schoeffler@audiolabs-erlangen.com\nABSTRACT\nInternet experiments in the ﬁelds of music perception and\nmusic information retrieval are becoming more and more\npopular. However, not many Internet experiments are com-\npared to laboratory experiments, the consequence being\nthat the effect of the uncontrolled Internet environment on\nthe results is unknown. In this paper the results of an In-\nternet experiment with 1168 participants are compared to\nthose of the same experiment with 62 participants but pre-\nviously conducted in a controlled environment. The com-\nparison of the Internet and laboratory results enabled us to\nmake a point on whether the Internet can be used for our\nexperiment procedure. The experiment aimed to investi-\ngate the listeners ability to correctly estimate the number\nof instruments being played back in a given excerpt of mu-\nsic. The participants listened to twelve short classical and\npop music excerpts each composed using one to six in-\nstruments. For each music excerpt the participants were\nasked how many instruments they could hear and how cer-\ntain they were about their estimation.\n1. INTRODUCTION\nIn psychoacoustics, a sequence of sounds grouped by the\nauditory system is known as an “auditory stream” which\nwas coined by Bregman and Campbell [2]. In the past\ndecades, a lot of experimental work related to “auditory\nstreams” has been done [1]. A majority of these experi-\nments were psychoacoustically motivated, e. g. the stimuli\nused were mostly of simple type like sinusoids or noises.\nEspecially from a psychoacoustic point of view, music is\na very complex sound signal which contains high-level in-\nformation (e. g. instrumentation and song lyrics). When\nlistening to music, this high-level information is also men-\ntally processed by humans. For developing auditory mod-\nels, it could be helpful to know the maximum number of\ninstruments humans are able to estimate when listening to\nmusic.\nFor many types of music perception experiments such\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\n© 2013 International Society for Music Information Retrieval.as estimating the number of instruments, it is essential that\nthe selected participants represent a large population. As\nrecent research in cross-cultural music perception and cog-\nnition reveals: The perception of music is dependent on\nthe origin of people [13]. Besides the cultural background,\nother aspects like their profession might have an inﬂuence\nwhen estimating the number of instruments being played\nback, e. g. musicians might recognize instruments much\nmore easily since they are in touch with instruments in\ntheir everyday life. One of the advantages of Internet ex-\nperiments (also called web-based experiments or web ex-\nperiments) is that it is easier to gather participants with\ndifferent backgrounds and from different regions than in\nlaboratory experiments. For a summary of beneﬁts and\ndisadvantages of Internet experiments see [10]. In music\nperception a major argument against Internet experiments\nis that there is no control about the environment. With the\nspreading of mobile devices with Internet connections this\nargument becomes more apparent, since the environment\nof the participants can range from a quiet place at home to\na noisy place outside.\nBy comparing the results of the Internet experiment pre-\nsented in this paper to the results of the same experiment\nbut previously conducted in a controlled environment [14],\nwe contribute to answering the research question, whether\nthe Internet can be used for experiments in music percep-\ntion. Furthermore, subpopulations like headphones-users\nand loudspeaker-users are examined whether they lead to\nmore reliable responses.\n2. RELATED WORK\nThe ability of estimating the number of instruments is prob-\nably related to the ability of auditory stream segregation.\nAn overview of auditory stream segregation in general is\ngiven by Bregman [1] and Wang and Brown [15].\nIn 1989, Huron conducted a musically motivated exper-\niment related to stream segregation [8]. In his experiment\nhe asked the participants for the number of voices in ex-\ncerpts of organ music. Huron deﬁned a voice as a single\n“line” of sound, more or less continuous, that maintains\na separate identity in a sound ﬁeld or musical texture (an\noverview of voice deﬁnitions is given by Cambouropou-\nlos [3]). Huron came to the conclusion that the number of\ncorrectly identiﬁed voices is up to three. Based on Hurons\nwork, we carried out an experiment where we asked musi-cians and non-musicians how many instruments they could\nhear in short pieces of music [14]. In contrast to Huron we\naddressed a more general case where voices are played by\ndifferent instruments and not only by organ. In Huron’s\nexperiment the responses of the participants were time-\nvarying for each stimulus. Another difference is that in\nHuron’s experiment all six participants except one had a\nmusical background whereas our experiment was also at-\ntended by a large group of non-musicians.\nIt has been becoming more and more popular to use the\nInternet for experiments in music or auditory perception.\nOne of the ﬁrst auditory experiments were conducted by\nWelch and Krantz [16] in 1996. A web experiment related\nto MIREX tasks using Amazon Mechanical Turk has been\nconducted by Lee [9]. An experiment with a large atten-\ndance was done by Salganik et al. [12] in 2006. They had\nover 14.000 participants and examined the social inﬂuence\non participants in an artiﬁcial music market. An overview\nof recent Internet experiments is given by Reips [11].\n3. METHOD\n3.1 Stimuli\nFor the stimuli generation, MIDI ﬁles of music pieces from\nthe RWC database [7] were selected. The MIDI ﬁles were\nmodiﬁed so that each ﬁle has a speciﬁc number of instru-\nments being played back. Since the number of instruments\nbeing played back had to be as constant as possible, a so-\ncalled “instrumental stationarity” for each music piece was\ncalculated. The “instrumental stationarity” is a measure\nthat shows whether all instruments are played the whole\ntime for a given start position and length. See [14] for the\ndetailed equation of “instrumental stationarity” and more\ninformation about the stimuli generation. The RWC ﬁles\nwere manually ﬁltered a priori to exclude items dominated\nby lead instruments or singing voices. From the remaining\nitems thirteen excerpts of MIDI ﬁles were selected with\nthe help of the “instrumental stationarity” having one to\nsix simultaneously playing instruments. Table 1 shows the\nselected excerpts including their instrumentation. The du-\nration of each excerpt is around seven seconds. By cutting\nat note offsets we varied the lengths of the excerpts to make\nthem semantically more meaningful. Six items (notated as\nC0**) belong to the classical Western music genre whereas\nthe other items are of mixed genre. The MIDI excerpts\nwere humanized and rendered by a sequencer software uti-\nlizing state-of-the-art commercial sampling products into\nWA V ﬁles. The rendered ﬁles were processed with con-\nvolutive reverb to match the original recordings. In infor-\nmal listening tests the quality of the renderings was eval-\nuated. In addition, participants did not give negative feed-\nback about the artiﬁcialness of the items during the labo-\nratory experiment. Additionally a loudness normalization\nwas applied according to EBU-R128 [5]. To avoid spatial\ncues the ﬁles were downmixed to mono at 16 bit/44.1 kHz.RWC ID Start [s] Dur. [s] InstrumentationP\nJ021 46.5 6.6 Piano, Contrabass (pizz.) and Trumpet 3\nC001 0.0 9.0 Bassoon 1\nG047 35.3 8.3 Violoncello 1\nC016 0.9 7.6 Viola and Violoncello 2\nG068 132.4 6.6 Violin and Flute 2\nC018 240.4 5.4 French Horn, Piano and Violin 3\nG046 0.3 7.9 Contrabass, Piano and Violoncello 3\nC013 5.6 6.0 Flute, Viola, Violin and Violoncello 4\nG036 0.0 6.5 Acoustic Guitar, Electric Bass, Piano and Vi-\nolin4\nC012 112.0 6.0 Contrabass, Flute, Viola, Violin and Violon-\ncello5\nG037 67.1 7.0 Acoustic Guitar, Contrabass (pizz.), Flute,\nPiano and Tenor Sax5\nC001 147.8 6.0 Bassoon, Clarinet, Contrabass, French Horn,\nOboe and Violin6\nG028 17.5 6.5 Electric Bass, Electric Guitar, Flute, Piano,\nTrombone and Trumpet6\nTable 1 . Selected items from the RWC Music Database\n[7]. Item J021 was used as training item.\n3.2 Participants\nParticipation in the experiment was done by visiting the\nexperiment’s website1. The experiment was promoted in\nmailing lists, forums, social networks and by personal in-\nvitations. Most of the forums and mailing lists were audio-\nrelated. No material incentive was given to participants.\nFor motivating the participants a high score was added to\nthe experiment.\nIn total 1310 website visitors attended the experiment.\nWe identiﬁed 115 of them as participants who did the ex-\nperiment more than once by using a browser ﬁngerprint-\ning method (for more details in browser ﬁngerprinting, see\n[6]). In this case, only the ﬁrst trial is used in the re-\nsults analysis. Our browser ﬁngerprinting method created\na hash value by using the visitor’s browser settings, e. g.\nscreen resolution and installed plugins. In addition, we ex-\ncluded 27 participants since they gave at least one non-\nserious response. We deﬁned responses with zero instru-\nments (25 participants) and responses with more than 12\ninstruments (two participants) as invalid. After the screen-\ning we had 1168 valid participants.\nThe participants were asked by a questionnaire whether\nthey have a professional background in audio, play at least\none instrument (including singing) and are familiar with\nlistening tests. Detailed information about the participants\nare described in Table 2. Headphones were used by 571\nparticipants and loudspeakers were used by 597 partici-\npants.\n3.3 Materials and Apparatus\nThe main functionality of the experiment’s website was\nwritten in HTML5 and JavaScript. The website was tested\nfor all major web browsers and optimized for mobile de-\nvices and desktop computers. The default ﬁle format for\nthe stimuli was WA V . Since some browsers (e. g. Inter-\nnet Explorer) did not support WA V , MP3 (encoded with\n256kbits/s CBR with Fraunhofer Encoder) was used as al-\nternative ﬁle format. The alternative ﬁle format was only\n1http://www.audiolabs-erlangen.com/experiments/wice/Total Age group Musician Professional\n1168 0 [0-12] - -\n110 [13-19] 74 [yes] 12 [yes]\n62 [no]\n36 [no] 0 [yes]\n36 [no]\n889 [20-39] 463 [yes] 128 [yes]\n335 [no]\n426 [no] 46 [yes]\n380 [no]\n143 [40-59] 98 [yes] 32 [yes]\n66 [no]\n45 [no] 8 [yes]\n37 [no]\n26 [60+] 13 [yes] 6 [yes]\n7 [no]\n13 [no] 2 [yes]\n11 [no]\nTable 2 . Information about the participants.\nused when WA V was not supported by the browser.\n3.4 Procedure\nThe experiment started on February the 15th, 2013 and\nlasted until March the 15th, 2013.\nAt ﬁrst, participants ﬁlled out a short questionnaire. They\nwere asked which audio setup they are using, whether they\nregularly play any musical instruments or do singing, have\na background in professional audio, are familiar with lis-\ntening tests and which age group they belong to.\nAfter ﬁlling out the questionnaire, the participants did a\nshort training. The purpose of the training was to familiar-\nize the participants with the user interface and to give them\nthe option to adjust the volume. The training had one stim-\nulus with three instruments being played. The instruments\nwere piano, bass and trumpet. The participants were told\non the training page how many and which instruments are\nplayed back. During the training it was possible to listen\nto the stimulus unlimited times.\nFollowed by the training, the participants had to esti-\nmate the number of instruments being played in twelve\nstimuli. The experiment question was “How many differ-\nent instruments do you hear?”. Participants could listen\nto each stimulus up to three times. In addition, they were\nasked how certain they were in their response. They could\nchoose between “uncertain”, “certain” and “very certain”.\nThe user interface is shown in Figure 1.\nAfter the participants estimated the number of instru-\nments for all twelve stimuli, they were given a score based\non their performance. Besides their personal score, a per-\ncentile rank showed how each participant performed com-\npared to all the other participants.\n4. RESULTS\nThe independent variables are the number of instruments\nbeing played back ( Num Inst), whether a participant is mu-\nsical ( Musical ), professional in audio ( Professional ) and\nwhich setup was used ( Setup ). A participant is deﬁned as\nFigure 1 . Experiment User Interface.\nNumInst\nResp I= 1 I= 2 I= 3 I= 4 I= 5 I= 6 n\nR= 1 2025 373 5 18 13 12 2446\nR= 2 298 1642 810 736 451 382 4319\nR= 3 12 277 1343 1145 1093 1069 4939\nR= 4 1 43 158 386 645 680 1913\nR= 5 0 1 18 48 120 155 342\nR= 6 0 0 1 3 12 30 46\nR > 6 0 0 1 0 2 8 11\n14016 responses (1168 participants \u000112 items)\nProbability\nof\nRespCorrect0.87 0.70 0.57 0.17 0.05 0.01\nTable 3 . Responses from the participants. The cells with a\ngray background represent correct estimations.\nmusical ( Musical =true ) when he or she is regularly play-\ning an instrument (including singing). The same applies\nto being professional ( Professional =true ) which is set\nwhen the participant responded that he or she is a profes-\nsional in audio. The responses for the setup used can either\nbe headphones ( Setup =‘headphones ’) or loudspeaker\n(Setup =‘loudspeaker ’). The dependent variable is the\nparticipant’s estimation of the number of instruments being\nplayed back ( Resp ). A correct estimation is deﬁned as\nRespCorrect =(\n0ifNum Inst6=Resp\n1ifNum Inst=Resp: (1)\nTable 3 shows the responses of the participants for all stim-\nuli.\nFor testing hypotheses, a logistic regression model with\nthe response variable RespCorrect and the predictor vari-\nables Num Inst,Musical ,Professional andSetup was cal-\nculated. The estimated coefﬁcients, p-values and average\nmarginal effects are shown in Table 4. Average marginal\neffects in the regression model describe the increase in\nprobability for correctly estimating the number of instru-\nments when the predictor variable is increased by one level.\nCompared to the other coefﬁcients the average marginal ef-\nfect of Setup =‘headphones ’ is very low. By using head-\nphones instead of loudspeakers it is 1:35% more likely to\nestimate the number of instruments correctly.\nAs expected, participants who play an instrument or\ndo singing ( Musical =true ) performed slightly betterCoefﬁcient Estimate Std. Error z-value p-value Average\nMarginal\nEffects\n(Intercept) 1.5015 0.06836 21.963 <2e-16 0.1886\nNumInst= 2 -1.0293 0.07651 -13.453 <2e-16 -0.1293\nNumInst= 3 -1.6021 0.07463 -21.466 <2e-16 -0.2012\nNumInst= 4 -3.5674 0.08380 -42.572 <2e-16 -0.4481\nNumInst= 5 -4.8759 0.11290 -43.188 <2e-16 -0.6125\nNumInst= 6 -6.3061 0.19428 -32.459 <2e-16 -0.7922\nMusical =\ntrue0.5266 0.04932 10.676 <2e-16 0.0661\nProfessional =\ntrue0.3306 0.06234 5.303 1.14e-07 0.0415\nSetup =\n‘headphones ’0.1071 0.04823 2.220 0.0264 0.0135\n(Dispersion parameter for binomial family taken to be 1)\nNull deviance: 18816 on 14015 degrees of freedom\nResidual deviance: 11036 on 14007 degrees of freedom\nAIC: 11054\nNumber of Fisher Scoring iterations: 7\nMcFadden’s Pseudo R-squared: 0.413\nTable 4 . Logit regression model for response variable\nRespCorrect calculated from the data obtained by the In-\nternet experiment.\nthan non-musicians. According to the average marginal ef-\nfect their chance of estimating the number of instruments\ncorrectly is 6:61% more likely for all stimuli. A simi-\nlar increase for estimating the number correctly ( 4:15%)\ncan be seen for participants being a professional in audio\n(Professional =true ).\nThe average marginal effects of Num Instindicates up\nto which point humans are able to correctly estimate the\nnumber of instruments being played back. The average\nmarginal effect of Num Inst = 2 shows that it is 12:93%\nless likely to estimate correctly when listening to two in-\nstruments instead of one instrument. Furthermore, in case\nof three instruments being played back the probability of\nestimating the wrong number increases to 20:12%. The\nhighly negative average marginal effect of \u00000:4481 for\nNum Inst = 4 indicates that it is becoming very unlikely\nfor humans to estimate the number of instruments correctly\ncompared to estimating the number of one to three instru-\nments.\nFor a detailed analysis of the differences between the In-\nternet experiment and the laboratory experiment in a con-\ntrolled environment, a second logit regression model was\ncalculated. This logit regression model includes the data\nof the previous experiment which has responses of 62 par-\nticipants. Besides Num Instan additional predictor variable\nEnvironment was added which can have the values ‘ web’\nor ‘lab’ (described in Table 5). The second logit regres-\nsion model reveals that there are no signiﬁcant differences\n(p= 0:174) between the two experiments. The low aver-\nage marginal effect of \u00000:0184 also conﬁrms that the type\nof the conducted experiments is applicable for an Internet\nenvironment.\nFigure 2 depicts the mean probability for correctly esti-\nmating the number of instruments grouped by the environ-\nment. Since in [14] the differences between musicians and\nnon-musicians were emphasized, their data is also depicted\nin Figure 2. As the logit regression model indicated, theCoefﬁcient Estimate Std. Error z-value p-value Average\nMarginal\nEffects\n(Intercept) 2.0226 0.11718 17.260 <2e-16 0.2590\nNumInst= 2 -1.0135 0.07424 -13.652 <2e-16 -0.1298\nNumInst= 3 -1.5914 0.07223 -22.033 <2e-16 -0.2038\nNumInst= 4 -3.4786 0.08031 -43.316 <2e-16 -0.4454\nNumInst= 5 -4.8058 0.10919 -44.015 <2e-16 -0.6154\nNumInst= 6 -6.2481 0.19033 -32.827 <2e-16 -0.8000\nEnvironment =\n‘web ’-0.1435 0.10569 -1.358 0.174 -0.0184\n(Dispersion parameter for binomial family taken to be 1)\nNull deviance: 19826 on 14759 degrees of freedom\nResidual deviance: 11819 on 14753 degrees of freedom\nAIC: 11833\nNumber of Fisher Scoring iterations: 7\nMcFadden’s Pseudo R-squared: 0.404\nTable 5 . Logit regression model for response variable\nRespCorrect calculated from the data obtained by the In-\nternet experiment and the laboratory experiment.\n1 2 3 4 5 600:20:40:60:81\nNumber of InstrumentsMean of RespCorrectMusicians [lab]\nNon-Musicians [lab]\nAll [lab]\nMusicians [web]\nNon-Musicians [web]\nAll [web]\nFigure 2 . Probability of RespCorrect grouped by Internet\nexperiment (web) and laboratory experiment (lab). Solid\nlines represents the results of the Internet experiment and\ndashed lines represents the results of the laboratory exper-\niment.\ndifference in the performance of the participants between\nthe Internet experiment and the laboratory experiment are\nvery low. The participants in the laboratory experiment\nwere about 4.6% better in average for all stimuli than the\nparticipants of the Internet experiment. When looking into\nthe differences between musicians and non-musicians, the\noutcome for the Internet experiment and laboratory exper-\niment differ slightly. In the laboratory experiment musi-\ncians performed about 31.6% better than non-musicians\nand in the Internet experiment musicians performed about\n20.85% better.\nThe probability of correctly estimating the number of\ninstruments does not consider how close an estimation is\nto the actual number of instruments. This means that a\nparticipant who estimates wrong by one instrument for all\nitems has the same RespCorrect like a participant who is\nalways wrong by two instruments. Despite this work fo-1 2 3 4 5 6012\nNumber of InstrumentsMean of Absolute Deviationlab\nweb\nFigure 3 . The mean of the absolute deviation grouped\nby Internet Experiment (web) and laboratory experiment\n(lab).\ncuses on the correct estimation, the differences in the ab-\nsolute deviation to the correct number of instruments was\nalso analyzed. The absolute deviation is deﬁned as\nDevAbs=jNum Inst\u0000Respj: (2)\nFigure 3 depicts the mean of DevAbsfor the laboratory ex-\nperiment and the Internet experiment. A knee point can be\nseen for Num Inst= 3where the slope of DevAbschanges.\nTo conﬁrm the marginal differences between the Inter-\nnet experiment and laboratory experiment for DevAbs, a\nlinear regression model was calculated (see Table 6). Com-\npared to the predictor variable Num Inst, the coefﬁcient of\nEnvironment is very low.\nCoefﬁcient Estimate Std. Error z-value p-value\n(Intercept) 0.08535 0.02706 3.154 0.00161\nNumInst= 2 0.17683 0.01881 9.401 <2e-16\nNumInst= 3 0.30122 0.01881 16.014 <2e-16\nNumInst= 4 1.02154 0.01881 54.309 <2e-16\nNumInst= 5 1.67967 0.01881 89.297 <2e-16\nNumInst= 6 2.56667 0.01881 136.453 <2e-16\nEnvironment =\n‘web ’0.05481 0.02482 2.208 0.02724\nResidual standard error: 0.6597 on 14753 degrees of freedom\nMultiple R-squared: 0.6603, Adjusted R-squared: 0.6602\nF-statistic: 4779 on 6 and 14753 DF, p-value: <2.2e-16\nTable 6 . Linear regression model for DevAbscalculated\nfrom the data obtained by the Internet experiment and the\nlaboratory experiment.\nAnother response variable that was obtained from the\nparticipants was the certainty of their estimation. Figure 4\ndepicts the certainty values for the Internet experiment and\nthe laboratory experiment.\nFor testing whether the environment has a signiﬁcant\ninﬂuence on the certainty of the participants ( Certainty ),\na cumulative link model (also called ordered regression\nmodel) was calculated [4]. The cumulative link model is\n1 2 3 4 5 6012\nNumber of InstrumentsMean of Absolute Deviationlab\nweb\nFigure 3 . The mean of the absolute deviation grouped\nby Internet Experiment (web) and laboratory experiment\n(lab).\nthe differences between musicians and non-musicians, the\noutcome for the Internet experiment and laboratory exper-\niment differ slightly. In the laboratory experiment musi-\ncians performed about 31.6% better than non-musicians\nand in the Internet experiment musicians performed about\n20.85% better.\nThe probability of correctly estimating the number of\ninstruments does not consider how close an estimation is\nto the actual number of instruments. This means that a\nparticipant who estimates wrong by one instrument for all\nitems has the same RespCorrect like a participant who is\nalways wrong by two instruments. Despite this work fo-\ncuses on the correct estimation, the differences in the ab-\nsolute deviation to the correct number of instruments was\nalso analyzed. The absolute deviation is deﬁned as\nDev Abs=|Num Inst\u0000Resp |. (2)\nFigure 3 depicts the mean of Dev Absfor the laboratory ex-\nperiment and the Internet experiment. A knee point can be\nseen for Num Inst=3where the slope of Dev Abschanges.\nTo conﬁrm the marginal differences between the Inter-\nnet experiment and laboratory experiment for Dev Abs,a\nlinear regression model was calculated (see Table 6). Com-\npared to the predictor variable Num Inst, the coefﬁcient of\nEnvironment is very low.\nAnother response variable that was obtained from the\nparticipants was the certainty of their estimation. Figure 4\ndepicts the certainty values for the Internet experiment and\nthe laboratory experiment.\nFor testing whether the environment has a signiﬁcant\ninﬂuence on the certainty of the participants ( Certainty ),\na cumulative link model (also called ordered regression\nmodel) was calculated [4]. The cumulative link model is\nused since Certainty is an ordered dependent variable with\nthe possible values ‘uncertain’, ‘certain’ and ‘very certain’.\nThe predictor variables for the ordered regression modelCoefﬁcient Estimate Std. Error z-value p-value\n(Intercept) 0.08535 0.02706 3.154 0.00161\nNum Inst=2 0.17683 0.01881 9.401 <2e-16\nNum Inst=3 0.30122 0.01881 16.014 <2e-16\nNum Inst=4 1.02154 0.01881 54.309 <2e-16\nNum Inst=5 1.67967 0.01881 89.297 <2e-16\nNum Inst=6 2.56667 0.01881 136.453 <2e-16\nEnvironment =\n‘web’0.05481 0.02482 2.208 0.02724\nResidual standard error: 0.6597 on 14753 degrees of freedom\nMultiple R-squared: 0.6603, Adjusted R-squared: 0.6602\nF-statistic: 4779 on 6 and 14753 DF, p-value: <2.2e-16\nTable 6 . Linear regression model for Dev Abscalculated\nfrom the data obtained by the Internet experiment and the\nlaboratory experiment.\n1 2 3 4 5 6020406080100labweblabweblabweblabweblabweblabweb3.223.420.228.248.445.27.022.118.528.838.838.327.4\n48.451.655.638.743.532.9\n54.855.455.750.950.369.428.228.216.112.911.360.123.126.015.610.311.4Number of InstrumentsCumulative Certainty in Percentvery certaincertainuncertainFigure 4 . Differences in certainty between the Internet ex-\nperiment (web) and laboratory experiment (lab).\nareNum InstandEnvironment . In Table 7 is the cumulative\nlink model for Certainty described. Same as RespCorrect ,\nthe environment of the experiment has no signiﬁcant inﬂu-\nence on the dependent variable Certainty . Considering the\nnumber of participants and the comparable low coefﬁcient,\nthe environment had a very low inﬂuence on Certainty .\n5. DISCUSSION\nRegarding the ability of estimating the number of instru-\nments, the web experiment conﬁrmed the results of the lab-\noratory experiment [7]. Both experiments share the same\noutcome: The probability to correctly estimate up to about\nthree instruments is higher than 50%.\nIn our previous result analysis of the laboratory experi-\nment [7] we set the focus on the differences between mu-\nsicians and non-musicians. Between the Internet experi-\nment and the laboratory experiment, slightly different re-\nsults were obtained when looking into how musicians and\nnon-musicians performed (Figure 2). In the laboratory ex-\nperiment musicians performed much better compared toFigure 4 . Differences in certainty between the Internet ex-\nperiment (web) and laboratory experiment (lab).\nCoefﬁcient Estimate Std. Error z-value p-value\nNumInst= 2 -1.61273 0.05744 -28.078 <2e-16\nNumInst= 3 -1.43164 0.05701 -25.114 <2e-16\nNumInst= 4 -2.02315 0.05804 -34.858 <2e-16\nNumInst= 5 -2.47933 0.05901 -42.013 <2e-16\nNumInst= 6 -2.43573 0.05900 -41.283 <2e-16\nEnvironment =‘web ’ -0.01008 0.07355 -0.137 0.891\nThreshold coefﬁcient Estimate Std. Error z-value\nuncertain jcertain -2.90465 0.08434 -34.441\ncertain jverycertain -0.41072 0.08090 -5.077\nTable 7 . Logit cumulative link model of certainty that was\ncalculated from the data obtained by the Internet experi-\nment and the laboratory experiment.\nused since Certainty is an ordered dependent variable with\nthe possible values ‘uncertain’, ‘certain’ and ‘very certain’.\nThe predictor variables for the ordered regression model\nareNum InstandEnvironment . In Table 7 is the cumulative\nlink model for Certainty described. Same as RespCorrect ,\nthe environment of the experiment has no signiﬁcant inﬂu-\nence on the dependent variable Certainty . Considering the\nnumber of participants and the comparable low coefﬁcient,\nthe environment had a very low inﬂuence on Certainty .\n5. DISCUSSION\nRegarding the ability of estimating the number of instru-\nments, the web experiment conﬁrmed the results of the lab-\noratory experiment [14]. Both experiments share the same\noutcome: The probability to correctly estimate up to about\nthree instruments is higher than 50%.\nIn our previous result analysis of the laboratory experi-\nment [14] we set the focus on the differences between mu-\nsicians and non-musicians. Between the Internet experi-\nment and the laboratory experiment, slightly different re-\nsults were obtained when looking into how musicians and\nnon-musicians performed (Figure 2). In the laboratory ex-\nperiment musicians performed much better compared tonon-musicians than in the Internet Experiment. One rea-\nson seems to be that in the laboratory experiment 74.2%\nof the musicians had also a professional background in au-\ndio. In the Internet experiment only 27.5% of the musi-\ncians had a professional background in audio. Since audio\nprofessionals more often have to detect hardly audible dif-\nferences in audio ﬁles they are more trained in this ﬁeld.\nAs mentioned before, being a musician in our context does\njust mean that the participant plays an instrument without\nany information about his expert level or the time he or she\nspends on practicing.\nWhen examining the responses for items with the same\nnumber of instruments being played back, noticeable dif-\nferences between the genres were found. For the stimuli\nwith Num Inst\u00144the mean of RespCorrect was 53:0%\nand for non-classical items 62:5%. Since the experiment\nwas not designed for testing classical versus non-classical\nitems, we cannot make deﬁnitive statements about whether\nhumans are better in estimating instruments for a speciﬁc\nmusic genre. Moreover, we did not address this issue in\nour result analysis.\nOne of the main reasons for conducting the experiment\nwas to ﬁnd out which inﬂuence the Internet environment\nhas on the results. All three regression models which in-\ncluded data of both experiments (Table 5, 6 and 7) re-\nvealed that the Internet environment had only very minor\neffects on the results. Moreover, despite the large number\nof participants in both experiments, the predictor variable\nEnvironment was even not signiﬁcant in two out of three\nregression models.\nIt is often recommended to use headphones instead of\nloudspeakers for Internet experiments. From the relative\nlow average marginal effect of Setup (Table 4), it can be\nderived that the type of the setup had only minor effects on\nthe results of the Internet experiment.\nSurprising was the small number of participants who\nhad to be screened. We excluded 27 participants since they\ngave at least one non-serious response which is about 2.3%\n(1195 participants remained after excluding all trials which\nwere not the ﬁrst ones). Most of these excluded partici-\npants responded with either very high numbers (e. g. 99)\nor responded with zeros for their estimated number of in-\nstruments being played. We assume that especially par-\nticipants who responded with zero only wanted to get an\nimpression of the experiment.\n6. CONCLUSION\nThe Internet experiment presented in this paper conﬁrmed\nthe results of a previous laboratory experiment that hu-\nmans are able to correctly estimate up to around three in-\nstruments in music. Furthermore signiﬁcant differences in\nperformance between musicians and non-musicians found\nout by the previous experiment were conﬁrmed. The com-\nparison between the results of the Internet experiment and\nlaboratory experiment revealed that only minor differences\nbetween both environments exist. Using headphones in-\nstead of loudspeaker is often held to be important when\nconducting listening tests over the Internet. In this ex-periment the audio setup used had only a minor inﬂuence\non the results. According to these results, experiments in\nthe ﬁelds of music perception or music information re-\ntrieval related to our procedure are well suited for being\nconducted over the Internet.\n7. REFERENCES\n[1] Albert S. Bregman. Auditory Scene Analysis: The Perceptual\nOrganization of Sound . Bradford Books, MIT Press, Cam-\nbridge, 1990.\n[2] Albert. S. Bregman and Jeffrey Campbell. Primary auditory\nstream segregation and perception of order in rapid sequences\nof tones. Journal of experimental psychology , 89(2):244–9,\nAugust 1971.\n[3] Emilios Cambouropoulos. V oice and stream: Perceptual and\ncomputational modeling of voice separation. Music Percep-\ntion, 26(1):75–94, 2008.\n[4] Rune Haubo B. Christensen. Analysis of ordinal data with\ncumulative link models – estimation with the R-package or-\ndinal, 2012.\n[5] EBU. Loudness normalisation and permitted maximum level\nof audio signals (EBU Recommendation R 128), 2011.\n[6] Peter Eckersley. How Unique Is Your Web Browser? In\nPrivacy Enhancing Technologies Symposium (PETS 2010) ,\npages 1–18, Berlin, Germany, 2010.\n[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. RWC\nmusic database: Popular, classical, and jazz music databases.\nInProc. ISMIR , pages 287–288, 2002.\n[8] David Huron. V oice Denumerability in Polyphonic Music\nof Homogeneous Timbres. Music Perception: An Interdisci-\nplinary Journal , 6(4):361–382, 1989.\n[9] Jin Ha Lee. Crowdsourcing music similarity judgments using\nmechanical turk. In ISMIR , pages 183–188, 2010.\n[10] Ulf-Dietrich Reips. Standards for Internet-Based Experi-\nmenting. Experimental Psychology , 49(4):243–256, 2002.\n[11] Ulf-Dietrich Reips. Using the Internet to Collect Data. In Har-\nris Cooper, Paul M. Camic, Debra L. Long, A. T. Panter,\nDavid Rindskopf, and Kenneth J. Sher, editors, APA Hand-\nbook of Research Methods in Psychology, Vol 2: Research\ndesigns: Quantitative, qualitative, neuropsychological, and\nbiological , volume 2, chapter 17, pages 291–310. American\nPsychological Association (APA), Washington, US, 2012.\n[12] Matthew J. Salganik, Peter Sheridan Dodds, and Duncan J.\nWatts. Experimental study of inequality and unpredictabil-\nity in an artiﬁcial cultural market. Science (New York, N.Y.) ,\n311(5762):854–6, February 2006.\n[13] Catherine J. Stevens. Music perception and cognition: A re-\nview of recent cross-cultural research. Topics in cognitive sci-\nence, 4(4):653–667, 2012.\n[14] Fabian-Robert St ¨oter, Michael Schoefﬂer, Bernd Edler, and\nJ¨urgen Herre. Human ability of counting the number of in-\nstruments in polyphonic music. volume 19, page 035034.\nASA, 2013.\n[15] D. Wang and G. Brown. Computational auditory scene anal-\nysis: Principles, algorithms, and applications . Wiley-IEEE\nPress, 2006.\n[16] Norma Welch and John H. Krantz. The World-Wide Web as\na medium for psychoacoustical demonstrations and experi-\nments: Experience and results. Behavior Research Methods,\nInstruments, & Computers , 28(2):192–196, June 1996."
    },
    {
        "title": "Instrument Identification Informed Multi-Track Mixing.",
        "author": [
            "Jeffrey J. Scott",
            "Youngmoo E. Kim"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416578",
        "url": "https://doi.org/10.5281/zenodo.1416578",
        "ee": "https://zenodo.org/records/1416578/files/ScottK13.pdf",
        "abstract": "Although digital music production technology has become more accessible over the years, the tools are complex and often difficult to navigate, resulting in a large learning curve for new users. This paper approaches the task of automated multi-track mixing from the perspective of applying common practices based on the instrument types present in a mixture. We apply basic principles to each track automatically, varying the parameters of gain, stereo panning, and coarse equalization. Assuming all instruments are known, a small listening evaluation is completed on the mixed tracks to validate the assumptions of the mixing model. This work represents an exploratory analysis into the efficacy of a hierarchical approach to multi-track mixing using instrument class as a guide to processing techniques.",
        "zenodo_id": 1416578,
        "dblp_key": "conf/ismir/ScottK13",
        "keywords": [
            "digital music production",
            "complex tools",
            "learning curve",
            "automated multi-track mixing",
            "instrument types",
            "basic principles",
            "gain",
            "stereo panning",
            "coarse equalization",
            "listening evaluation"
        ],
        "content": "INSTRUMENT IDENTIFICATION INFORMED MULTI-TRACK MIXING\nJeffrey Scott and Youngmoo E. Kim\nMusic and Entertainment Technology Laboratory (MET-lab)\nElectrical and Computer Engineering, Drexel University\nfjjscott, ykimg@drexel.edu\nABSTRACT\nAlthough digital music production technology has become\nmore accessible over the years, the tools are complex and\noften difﬁcult to navigate, resulting in a large learning\ncurve for new users. This paper approaches the task of\nautomated multi-track mixing from the perspective of ap-\nplying common practices based on the instrument types\npresent in a mixture. We apply basic principles to each\ntrack automatically, varying the parameters of gain, stereo\npanning, and coarse equalization. Assuming all instru-\nments are known, a small listening evaluation is completed\non the mixed tracks to validate the assumptions of the mix-\ning model. This work represents an exploratory analysis\ninto the efﬁcacy of a hierarchical approach to multi-track\nmixing using instrument class as a guide to processing\ntechniques.\n1. INTRODUCTION\nThe pervasive use of digital tools for creating, recording,\nproducing and editing audio has led to a desire for in-\ncreased automation and efﬁciency of these tools. Although\nthere is a wide variety of digital audio workstations (DAW)\nand plug-in suites available, the level of expertise required\nto operate them proﬁciently necessarily inhibits many new-\ncomers from obtaining reasonable results even with a sig-\nniﬁcant amount of effort. This has led to an exploration in\nthe audio signal processing community for methods of au-\ntomatically analyzing audio and improving the perceived\nquality. Several signiﬁcant difﬁculties arise when attempt-\ning this task. The qualitative difference between the pref-\nerence of individuals, the wide range of timbre, dynamics\nand instrumentation and the multitude of production tech-\nniques available present ample hurdles to overcome.\nThis paper attempts to exploit some of the commonal-\nities between processing chains for a speciﬁc instrument\nclass, namely the drum kit. Figure 1 shows the general\nframework for the task. Given the identities of individ-\nual drum tracks (kick, snare, tom and overhead), we apply\nsome basic guidelines to make the kit sound more balanced\nusing spatial and spectral modiﬁcations.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.In order to apply the processing techniques, the labels\nof each drum track must be known. To this end, we provide\na simple classiﬁcation experiment to evaluate the difﬁculty\nof classifying these tracks in a real world situation. A lis-\ntening test is conducted to determine how well the model\nmixes the individual drum tracks. For the listening test, we\nuse the ground truth instrument labels to evaluate how well\nthe instrument based model mixes the tracks.\nThe remainder of the paper is organized as follows.\nPrior work in automatic mixing and instrument identiﬁca-\ntion is presented in Section 2. Details about the dataset are\noutlined in Section 3 and the processing employed to form\na drum mix is in Section 4. Sections 5 and 6 detail the\nclassiﬁcation experiment and listening evaluation, respec-\ntively.\n2. BACKGROUND\nInterest in automating multi-track production tasks has in-\ncreased in recent years, focusing on both real-time and of-\nﬂine models based on perceptual information, acoustic fea-\ntures and best practices [1, 2].\nSigniﬁcant work on cross-adaptive methods for multi-\ntrack mixing for speciﬁc parameters has been explored\nin [3–7]. The implementation of these models relies on\ncomputing time domain and spectral features on the tracks\npresent in a mixture and devising models to control mixing\nparameters based on the temporal and inter-track relations\nbetween the features. Psychoacoustic models of loudness\nand frequency masking are leveraged to inform the rela-\ntionships on a perceptual level and more closely approx-\nimate the signal characteristics that are processed by hu-\nmans.\nDrum Type Classiﬁcation\nInstrument Based\nProcessing\nFigure 1: Identiﬁcation informed mixing.Related work focuses on estimating production tech-\nniques from user examples as well as higher level con-\nstructs of musical timbre. Variation in the vocabulary used\nto describe music as well as individual artist and listener\npreference makes universal rules difﬁcult to derive. Ob-\nserving and recording user interaction with mixing tools\ncreates data that can be used to model individual prefer-\nences [8–10].\nThere have also been developments in learning mixing\nparameters from data. The authors in [11] use regression\nto estimate fader gains, equalization and compression from\nmixed audio signals. In a previous paper, we attempted to\nlearn dynamic mixing models directly from data [12].\nInstrument identiﬁcation is a problem that has been pop-\nular in Music-IR for many years. State of the art perfor-\nmance on datasets of individual samples of both pitched\nand un-pitched instruments has reached classiﬁcation ac-\ncuracies in the upper 90th percentile [13–15]. However,\nmost of these systems evaluate libraries of single, isolated\ninstrument tones. There has been some work that shows\ngood performance on longer excerpts of pitched instru-\nments [16], but there are not comparable results to indi-\nvidual sample classiﬁcation.\n3. DATASET\nThe dataset used throughout this paper consists of 135\nsongs across a variety of genres. The genres include\nAcoustic, Alternative, Country, Dance, Electronic, Hip-\nHop, Indie, Jazz, Rock and Metal. The songs were ob-\ntained from three primary sources: Weathervane Music1,\nSound on Sound2and a multi-track dataset used for song\nstructure segmentation [17]. Each track is converted to a\nmonaural source at 44.1kHz sampling rate and labeled with\nthe instrument present in the track.\nThe tracks in every song are labeled with the instrument\npresent by three individuals and the majority label for each\ntrack was retained as ground truth. The labelers are stu-\ndents in the music industry program at Drexel University.\nThe ﬁlenames for each audio track are used when possible\nand normalized to a standard label for a single instrument\nclass. Instrument classes are differentiated on a ﬁne level\n(clean/distorted electric guitar) and may be combined into\nsuperclasses (electric guitar) if desired. The electric gui-\ntar is a speciﬁc example where ﬁne level labels are desired\nsince the distorted and clean versions are treated very dif-\nferently by engineers and have much different roles in the\nmix. The dataset is publicly available online3.\n4. INSTRUMENT BASED PROCESSING\nIn this approach we attempt to codify some common prac-\ntices and apply them to multi-track drum audio. Sev-\neral professional and student mixing engineers were in-\nterviewed about the process of mixing audio and it was\n1http://weathervanemusic.org/\n2http://www.soundonsound.com/\n3http://music.ece.drexel.edu/research/AutoMixunsurprising to ﬁnd that all of them speciﬁed that their ap-\nproach is dependent upon the source material (i.e. genre,\ninstrumentation). It is quite difﬁcult to deﬁne a set of hard\nand fast rules for mixing audio yet there do exist some\ncommonalities that many agree upon. We apply some ba-\nsic techniques to improve the balance and quality of the\ndrums via stereo panning, ﬁltering and level adjustment.\nThe motivation for the processing techniques employed in\nthe following subsections are derived from the engineer in-\nterviews as well as authoritative sources on mixing [18,19].\nThere are several concerns when combining the sig-\nnals from multiple drum microphones to produce a mix-\nture. Problems with phase coherence between the different\nmicrophones can often occur and result in a comb ﬁlter-\ning effect applied to the instruments [18]. This is the case\nwith bleed (leakage) between microphones on different in-\nstruments as well as multiple microphones on a single in-\nstrument (as in the top/bottom heads of a snare drum). In\nproperly recorded material this effect is usually anticipated\nfor and dealt with during signal capture and therefore not\nconsidered in this paper.\nWe consider three proccessing areas: level balancing,\nstereo panning and equalization. Two basic approaches\nfor level adjustment are serial (faders down) and parallel\n(faders up) [18,19]. The serial approach invovles adding in\nlayers one at a time and the parallel approach starts with all\nlayers active and adjusts levels accordingly. We opt for the\nparallel approach where the level of each instrument track\nis evaluated individually against the rest of the mix. There\nare also two main approaches to using the ambient (over-\nhead/room) mics. One primarily uses the overheads as the\nmain drum signal and uses the individual instrument mics\nas reinforcement when needed. The alternate approach is\nto use the close microphones as the primary signal source\nand use the overhead microphones to increase the amount\nof cymbals and add ‘air’ to the mix. We opt to use the latter\napproach in this work.\nFor panning, one may start with a stereo spread of the\noverhead mics and pan the close microphones according\nto their position in that signal. Another common approach\nis to pan the kick and snare dead center since they are the\ndriving force of the rhythm section. This is the option we\nchoose in our model.\nThe equalization applied is minimal and was obtained\nfrom the interviews of engineers. The interviewees ex-\npressed reservation about making gereralizations without\nhearing the source material and knowing what other in-\nstruments are in the mixture, yet these are the same issues\nthey expressed with nearly all aspects of mixing, namely\nthat each session is different and must be approached in-\ndividually. Nevertheless, a ﬁltering scheme was developed\nto boost frequency ranges that often need boosting and cut\nfrequency ranges that often need attenuating. Ideally, this\nwould be done adaptively through comparing bandwise en-\nergy ratios and making adjustments accordingly.\nBefore processing, each track is analyzed to determine\nwhere the instrument is playing on each track. We only\nwant to compare signal characteristics where there is an ac-tive instrument in a track, not where there is just the noise\nﬂoor. Figure 2 depicts the computation of the active re-\ngions in each track. The ﬁrst four steps, full-wave recti-\nﬁcation, low pass ﬁltering, downsampling and smoothing\nwith a moving average ﬁlter produce the temporal envelope\nof the signal and the threshold determines active regions.\nAfter thresholding, any segments less than 150ms long are\ndiscarded.\n4.1 Stereo Panning\nIn [5, 20] a dynamic cross-adaptive model is used to ac-\ntively pan tracks as they come in and out of the instrument\nmixture based on several constraints related to spectral and\nspatial balance and masking. Here we attempt to lever-\nage common practices in drum kit panning and apply them\nto the individual tracks of a drum kit. This results in a\nstatic value being applied to the entire track for the dura-\ntion of the song regardless of the presence or absence of\ninstrument playing at any given time. Panning a drum kit\nis one aspect of mixing that is fairly consistent between en-\ngineers. Qualitatively, the stereo balance of the drum mix\nis as follows:\n1. Kick drum panned center\n2. Snare drum panned center\n3. Toms panned from left to right\n4. Overhead microphones panned left and right\nPanning is accomplished by applying the sine-cosine pan-\nning law\nLpan=cos(45\u000e\u0000\u0012) (1)\nRpan=sin(45\u000e\u0000\u0012): (2)\nHere\u00122[\u000045\u000e;45\u000e]and represents the angle offset from\nthe center of the stereo ﬁeld with \u000045\u000ebeing panned fully\nto the left and +45\u000epanned fully right. This method of\npanning maintains the perceived loudness of the signal as\nit is varied from left to right. Table 1 shows the parameter\nvalues used to pan the tracks.\nThe kick and snare drums are panned in the center of\nthe stereo ﬁeld. The toms are spaced linearly from left to\nright with 25\u000ebeing the maximum offset from the center\nposition. The overhead tracks are panned alternating left\nand right at the speciﬁed value in Table 1.\n4.2 Relative Levels\nAfter panning, the loudness of each track is computed and\ncompared against the loudness of the rest of the tracks to\ndetermine any boost or attenuation that is desired for each\nFull W ave \nRectify\nLow Pass\nFilter\n   M\nMoving\nAverage\nThreshold\nFigure 2: Processing chain to calculate the active areas of\nan instrument track.Instrument Class Panning Gain Values\nValue (\u0012 )f\u000b;\f;\u0015g\nKick Drum 0 (center) f0.9, 1.2, 2g\nSnare Drum 0 (center) f0.9, 1.2, 2g\nToms Spaced f-25, 25g f0.8, 1.3, 4g\nOverhead/Room f-35, 35g f0.8, 1.3, 4g\nTable 1: Mixing parameter values for individual drum\ntracks.\ntrack. The loudness of each track is calculated by ﬁltering\nthe signal using the inverse of the ISO 226 normal equal-\nloudness-level contours (at 75 phons) and then computing\nthe RMS energy over a 23ms window [21]. The level of\n75 phons was chosen based on preferred listening levels\nshown in [22]. The loudness of the target track (x loud)\nis compared to the loudness of the sum of the remaining\ntracks (yloud) and a loudness ratio is computed,\nrloud=1\nTTX\n\u001cx(\u001c)\nloud\ny(\u001c)\nloud; (3)\nwherexandyare in dB and T is the total number of short\ntime frames in the current song being analyzed. The loud-\nness ratio is then used to attenuate or boost the level of the\ntrack in question. The gain of the track is determined using\nthe following equation\ng= 10(\u00001\n\u0015log(rloud )): (4)\nEquation 4 offers control over the amount of level correc-\ntion that is applied to each instrument through the param-\neter\u0015. As\u0015increases, the amount of level correction is\nreduced as shown in Figure 3.\nLoudness is computed on each channel (L/R) after pan-\nning and the average of the loudness ratios is used to deter-\nmine the gain of the instrument. There are three parameters\nf\u000b;\f;\u0015gfor each instrument type that determine how the\nloudness ratio affects the gain, g, applied to the track. The\n\u000band\fparameters deﬁne thresholds for the loudness ra-\ntio necessary to apply loudness correction. For example,\nif we require rloud< \u000b orrloud> \f where\u000b= 0:8 and\n\f= 1:2 before applying gain g, then the track will have no\nlevel correction if rloud2[0:8;1:2]and will have loudness\ncorrection speciﬁed in Equation 4 otherwise. The parame-\nters in Table 1 are speciﬁed to err on the side of more kick\nand snare drum than overhead and tom microphones since\nthe kick and snare instruments are generally more promi-\nnent in rock music.\n4.3 Equalization\nThe desired frequency content for a speciﬁc instrument is\nvery genre dependent. For example in an electronic track\nthe kick drum generally contains more low frequency con-\ntent and may be prominent even into the sub-bass range. In\nheavy metal, the sound of the beater striking the kick drum\nis often desirable and the signal may need to be boosted in0 0.5 1 1.5 2 2.5 3−15−10−505101520Gain Contours for Values of λGain (dB)\nLoudness Ratio  \n2\n3\n5\n10Figure 3: Contours of gain attenuation for various \r.\nthe high-mid frequency range. For these reasons we chose\nto apply only subtle equalization based on some common\noperations. The kick drum has a 2dB boost from 1kHz-\n6kHz, a 2dB cut from 400Hz-900Hz and a 2dB boost of\n100Hz with a quality factor of 4.5. The snare drum has a\n3dB high shelving boost starting at 10kHz. These modi-\nﬁcations are designed to give the kick drum slightly more\npunch and the snare drum more brilliance.\n5. DRUM TYPE CLASSIFICATION\nFor an unknown set of tracks, the drums would need to be\nidentiﬁed to apply the common practices outlined above.\nHere we explore a preliminary experiment to classify a\ntrack in terms of the drum content it contains. The ap-\nproach is fairly standard for supervised learning and is\nmeant to serve as a benchmark of the difﬁculty of this par-\nticular dataset.\nA support vector machine (SVM) classiﬁer with radial\nbasis function (RBF) kernel is trained and evaluated via 5-\nfold cross validation using LIBSVM [23] . This is a four\nclass problem (C2f1;2;3;4g) with the four classes be-\ning kick drum, snare drum, tom-tom and overhead. The\nfeatures used in the experiment are listed in Table 2 and in-\nclude mel-frequency cepstral coefﬁcients (MFCC) (20 di-\nmensions), spectral features and time domain features as\nwell as information about the amount of time active au-\ndio is present in the track. The ﬁrst and second deriva-\ntives of each feature (non-singleton) is also included in\nthe dataset. This results in 138 total feature dimensions\nwhich is then reduced through principle components anal-\nysis (PCA). The classiﬁer achieved an average accuracy\nacross all folds of 0.504. For a four class problem, this\nFeatures Features (cont.)\nMFCC RMS\nCentroid Bandwidth\nFlux Zero-Crossing Rate\nNumber of Segments Inter Onset Interval\nSegment Length\nTable 2: Features used in drum type classiﬁcation.Production Familiarity Participants\nNone 4\nNovice 4\nIntermediate 6\nExpert 1\nTable 3: Listening test participant familiarity with audio\nmixing and production.\nresult is not particularly promising, but the model and fea-\ntures used are not as advanced as those in [13–16]. Al-\nthough the data is in multi-track format, there are still sev-\neral instruments present via the bleed of the microphones.\nFor the tom-tom drums, the majority of the track resem-\nbles an overhead microphone signal of low amplitude until\nthe drum is (with relative infrequency) struck. This type of\nreal-world situation increases the difﬁculty of performing\nclassiﬁcation.\n6. LISTENING EVALUATION\nTo evaluate the ability of the model to appropriately mix\nthe drum tracks together, a listening test is performed\nwhere participants noted their preference for the individ-\nual monaural tracks summed versus the mix generated with\nthe model. The ground truth instrument labels are used for\ngenerating the mixes using the model. Ten songs were se-\nlected at random from the dataset and a 15 second clip for\neach song was selected so that as many of the individual\ndrum tracks were active as possible. Most songs in the\ndataset do not have drum stems associated with them, only\nthe raw unmixed multi-track session and the ﬁnal profes-\nsional mix. The majority of songs that do have mixed drum\nstems are from the same studio and use very similar pro-\ncessing chains. Therefore to avoid over-representing that\nsubset we only included the summed mix and the auto-\nmatic mix across a larger number of sources.\nThe clip pairs were presented with the summed version\nand the automatically mixed version appearing in random\norder. Each participant was presented clip pairs one at a\ntime and asked which clip sounds more balanced. They\n1 2 3 4 5 6 7 8 9 10024681012Listening Test Results\nSongIDNumber of Ratings\n  \nAutoMix Preferred\nUnity Gain Preferred\nNo Preference\nFigure 4: Listening test results showing the number of rat-\nings for each clip pair.could choose Clip A, Clip B or No Preference. The partic-\nipants were asked to provide their level of experience with\naudio mixing and production, the distribution is shown in\nTable 3. Subjects are graduate and undergraduate students\nat Drexel in the music industry and engineering programs.\nMost subjects are male, with only two participants being\nfemale. There were 15 total participants in the study with\nabout half having little experience working with audio pro-\nduction and the other half having signiﬁcant experience.\nFigure 4 shows the results of the listening test. For six\nof the ten songs, the model is preferred over the summed\nmix and listeners prefer two of the ten monaural summed\nmixes. Songs 7, 8 and 10 contain some drum loops from\na library and do not adhere to the ‘standard‘ recording\ntechnique of having kick, snare, tom and overhead mi-\ncrophones. The dataset represents a variety of material\nfrom various sources and varying quality. Some material\nis recorded professionally and sounds reasonably balanced\nthrough just summing the tracks.\n7. DISCUSSION AND FUTURE WORK\nWe present an approach to mixing multi-track audio in an\nautomated fashion by incorporating common techniques\nbased on prior knowledge of instrumentation. The method\nobtains fair performance on a certain class of song in the\ndataset but is not able to gracefully handle inconsisten-\ncies in recording quality present the dataset. One caveat\nof working with multi-track audio is the lack of standard-\nization for recording sessions. This makes obtaining well\nlabeled consistent datasets to train models a difﬁcult task\nin itself. The work here demonstrates the possible potential\nof a hierarchical system that combines both best practices\nand common techniques of mixing engineers with more\nsophisticated models of instrument identiﬁcation, however\nthere is signiﬁcant room for improvement.\nFor the classiﬁcation task, there exist more advanced\nmethods in the literature, yet most apply to individual in-\nstrument samples and not full recorded tracks. Including\nmore information about the temporal evolution of a signal\nas well as taking advantage of the audio in multiple drum\ntracks while classifying each track could improve results\nsigniﬁcantly.\nGenre information plays a signiﬁcant role in the desired\ndrum sound for a given song. A jazz kit requires much dif-\nferent treatment than a dance or house drum beat, however\ngenre recognition is not a solved problem and the deﬁni-\ntions of genres are constantly evolving. This is an aspect\nof automatic mixing where it would make sense to expose\na parameter to the user and offer ‘presets’ similar to most\naudio plugins.\nMore adaptive methods can be used on the track level\nprocessing that computes the active segments and loudness\nas in [7]. Perhaps the most important aspect is further user\nevaluation and iteration based on listening test results. The\nultimate goal of automated mixing systems is to make the\nmix sound better to the user. Mixing audio often demands\nan iterative coarse-to-ﬁne approach where the engineer isconstantly making changes and then evaluating those deci-\nsions in the context of the mix [18, 19].\nThis is an introductory work that explores the poten-\ntial of a hierarchical approach to multi-track mixing us-\ning instrument class as a guide to processing techniques.\nWhile the classiﬁcation and listening evaluation results\nhave room for improvement, a system basing mixing de-\ncisions on the instruments in the mixture warrants further\ninvestigation.\n8. REFERENCES\n[1] J. D. Reiss, “Intelligent systems for mixing multichan-\nnel audio,” in Proceedings of the 17th International\nConference on Digital Signal Processing (DSP), Jul.\n2011, pp. 1–6.\n[2] E. Perez-Gonzalez and J. D. Reiss, “Automatic mix-\ning,” in DAFX: Digital Audio Effects, 2nd ed.,\nU. Z ¨olzer, Ed. John Wiley & Sons, Ltd, 2011, pp.\n523–549.\n[3] E. Perez-Gonzalez and J. D. Reiss, “Automatic gain\nand fader control for live mixing,” in IEEE Workshop\non Applications of Signal Processing to Audio and\nAcoustics, 2009.\n[4] ——, “Automatic equalization of multichannel audio\nusing cross-adaptive methods,” in 127th AES Conven-\ntion, 2009.\n[5] S. Mansbridge, S. Finn, and J. D. Reiss, “An au-\ntonomous system for multitrack stereo pan position-\ning,” in 133rd AES Convention, 2012.\n[6] ——, “Implementation and evaluation of autonomous\nmulti-track fader control,” in 132nd AES Convention,\n2012.\n[7] D. Ward, J. D. Reiss, and C. Athwal, “Multitrack mix-\ning using a model of loudness and partial loudness,” in\n133rd AES Convention, Oct. 2012.\n[8] A. T. Sabin and B. Pardo, “A method for rapid person-\nalization of audio equalization parameters,” Proceed-\nings of ACM Multimedia, pp. 769–772, 2009.\n[9] Z. Raﬁi and B. Pardo, “Learning to control a reverber-\nator using subjective perceptual descriptors,” in Pro-\nceedings of the 10th International Society for Music\nInformation Retrieval Conference, Kobe, Japan, Octo-\nber 26-30 2009, pp. 285–290.\n[10] B. Pardo, D. Little, and D. Gergle, “Building a person-\nalized audio equalizer interface with transfer learning\nand active learning,” in Proceedings of the 2nd Interna-\ntional ACM Workshop on Music Information Retrieval\nwith User-Centered and Multimodal Strategies. New\nYork, USA: ACM, 2012, pp. 13–18.\n[11] D. Barchiesi and J. Reiss, “Reverse engineering of\na mix,” Journal of the Audio Engineering Society,\nvol. 58, no. 7, pp. 563–576, 2010.[12] J. Scott, M. Prockup, E. M. Schmidt, and Y . E. Kim,\n“Automatic multi-track mixing using linear dynamical\nsystems,” in Proceedings of the 8th Sound and Music\nComputing Conference, Padova, Italy, 2011.\n[13] S. Scholler and H. Purwins, “Sparse approximations\nfor drum sound classiﬁcation,” IEEE Journal of Se-\nlected Topics in Signal Processing, vol. 5, no. 5, pp.\n933–940, 2011.\n[14] A. Tindale, A. Kapur, G. Tzanetakis, and I. Fujinaga,\n“Retrieval of percussion gestures using timbre classiﬁ-\ncation techniques,” in Proceedings of the 5th Interna-\ntional Society for Music Information Retrieval Confer-\nence, 2004.\n[15] Z. Fu, G. Lu, K. M. Ting, and D. Zhang, “A survey\nof audio-based music classiﬁcation and annotation,”\nIEEE Transactions on Multimedia, vol. 13, no. 2, pp.\n303–319, 2011.\n[16] S. Essid, G. Richard, and B. David, “Musical instru-\nment recognition by pairwise classiﬁcation strategies,”\nIEEE Transactions on Audio, Speech, and Language\nProcessing, vol. 14, no. 4, pp. 1401–1412, 2006.\n[17] S. Hargreaves, A. Klapuri, and M. Sandler, “Struc-\ntural Segmentation of Multitrack Audio,” IEEE Trans-actions on Audio, Speech, and Language Processing,\nvol. 20, no. 10, pp. 2637–2647, 2012.\n[18] M. Senior, Mixing Secrets for the Small Studio, 1st ed.\nFocal Press, 2011.\n[19] R. Izhaki, Mixing Audio: Concepts, Practices and\nTools, 1st ed. Elsevier Ltd., 2008.\n[20] E. Perez-Gonzalez and J. D. Reiss, “A real-time semi-\nautonomous audio panning system for music mixing,”\nEURASIP Journal on Advances in Signal Processing,\n2010.\n[21] International Standards Organization, ISO226: Normal\nequal-loudness-level contours, 2003.\n[22] W. E. Hodgetts, J. M. Rieger, and R. A. Szarko, “The\neffects of listening environment and earphone style on\npreferred listening levels of normal hearing adults us-\ning an mp3 player,” Ear and Hearing, vol. 28, no. 3,\npp. 290–297, 2007.\n[23] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for\nsupport vector machines,” ACM Transactions on Intel-\nligent Systems and Technology, vol. 2, pp. 27:1–27:27,\n2011."
    },
    {
        "title": "Annotating Works for Music Education: Propositions for a Musical Forms and Structures Ontology and a Musical Performance Ontology.",
        "author": [
            "Véronique Sébastien",
            "Didier Sébastien",
            "Noël Conruyt"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418199",
        "url": "https://doi.org/10.5281/zenodo.1418199",
        "ee": "https://zenodo.org/records/1418199/files/SebastienSC13.pdf",
        "abstract": "Web applications and mobile tablets are changing the way musicians practice their instrument. Now, they can access instantaneously thousands of musical scores online and play them while watching their tablet, put on their music stand. However musicians may have difficulties in getting appropriate tips and advice to play the chosen piece correctly. This is why we conceived a collaborative platform to annotate digital scores on tablets in previous work. However, we noticed that the current Music Ontology (MO) do not allow to tag these annotations appropriately. Thus, we present in this paper a proposition for a Musical Forms and Structures Ontology (MFSO) and a Musical Performance Ontology (MPO) based on music practice. A construction methodology and a model are first detailed. Then, a practical use case is presented. Lastly, inherent theoretical and practical difficulties encountered during the ontology framework’s conception are discussed.",
        "zenodo_id": 1418199,
        "dblp_key": "conf/ismir/SebastienSC13",
        "keywords": [
            "Web applications",
            "mobile tablets",
            "changing the way musicians practice",
            "accessing musical scores",
            "playing while watching",
            "music stand",
            "difficulty in getting tips",
            "conceived a collaborative platform",
            "Music Ontology",
            "proposition for a Musical Forms and Structures Ontology"
        ],
        "content": "ANNOTATING WORKS FOR MUSIC EDUCATION: \nPROPOSITIONS FOR A MUSICAL FORMS AND \nSTRUCTURES ONTOLOGY AND A MUSICAL \nPERFORMANCE ONTOLOGY \nVéronique Sébastien, Didier Sébastien, Noël Conruyt \nIREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 \nUniversity of Reunion Island, Saint-Denis, Réunion (FRANCE) \nveronique.sebastien/didier.sebastien/noel.conruyt@u niv-reunion.fr  \nABSTRACT \nWeb applications and mobile tablets are changing th e \nway musicians practice their instrument. Now, they can \naccess instantaneously thousands of musical scores o nline \nand play them while watching their tablet, put on t heir \nmusic stand. However musicians may have difficultie s in \ngetting appropriate tips and advice to play the cho sen \npiece correctly. This is why we conceived a collabo rative \nplatform to annotate digital scores on tablets in p revious \nwork. However, we noticed that the current Music On tol- \nogy (MO) do not allow to tag these annotations appr opri- \nately. Thus, we present in this paper a proposition  for a \nMusical Forms and Structures Ontology (MFSO) and a \nMusical Performance Ontology (MPO) based on music \npractice. A construction methodology and a model ar e \nfirst detailed. Then, a practical use case is prese nted. \nLastly, inherent theoretical and practical difficul ties en- \ncountered during the ontolog y framework’s  conception \nare discussed. \n1.  INTRODUCTION \nMore and more musicians share their scores and perf or- \nmances on dedicated Web platforms such as free- \nscores.com or musescore.com. Meanwhile music applica - \ntions dedicated to scores management are ported to tablet \ndevices (Tonara ™, Musescore ™, Finale Songbook ™). \nHowever, musicians still do not dispose of appropri ate \ntools to demonstrate their know-how on these scores : \nhow to play this difficult part? Which fingering sh ould I \nuse? This is why we designed a collaborative score anno- \ntation service working on tactile tablets [1]. It a llows us- \ners to illustrate abstract scores with multimedia c ontent \nshowing tips, exercises or questions directly linke d to the \nconcerned notes on the score. We also proposed a ma tch- \ning analyzer to automatically determine a score dif ficulty \nlevel [2]. But in order to suggest relevant annotat ions to \nperformers, we need to tag the latter appropriately . In- \ndeed, musical know-how is contextual: it relates to  a spe- cific piece with its structure and mood. But most t ech- \nniques can be reused in similar contexts with appro priate \nadaptations. Thus, correctly contextualized annotat ions \ncould be reused on different pieces sharing similar ities \n(genre, composer, patterns, etc.). It would also en able \ncomplex queries on instrumental issues (for instance : how \nto play scales? how to produce a soft but expressiv e \nsound? what is the best strategy to learn a piece b y \nheart?). \nWe could rely on social tags created by users (i.e.,  \nFolksonomies [3]). However, as noted by Sordo in hi s \nexperiment on musical genres and moods [4], the eme rg- \ning vocabulary is not always reliable, especially o n very \nspecialized terms. Considering our educational cont ext, \nreliability and accuracy are essential. These led u s to a \ncontrolled vocabulary based solution, with extensio n and \nadaptation possibilities according to the considere d case. \nTo do so, we propose to extend the existing Music O n- \ntology (MO) [11] with a Musical Forms and Structure s \nOntology (MFSO) and a Musical Performance Ontology \n(MPO), through a global Semiotic Annotation framewo rk \n(SA) (Figure 1). While the MO is dedicated to music al \nresources and events description for databases, the  MFSO \nfocuses on musical works structure analysis, and th e \nMPO on performances and instrumental techniques. \nHowever, if the MO and MFSO can be used to relate o b- \njective facts about music, the MPO deals with subje ctive \napproaches of a given piece. This is why we embed i t in a \nSA. We explain and justify the use of this Sign-bas ed \nframework in the next section. We then present our onto- \nlogical propositions in the third and fourth sectio n. A \nsimple use case is detailed in the fifth section. L astly, we \ndiscuss the difficulties we encountered in our atte mpt to \norganize musicological and practical instrumental t erms. \n \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on the f irst page.  \n© 2013 International Society for Music Information Re trieval  \n \nFigure 1 : Musical Ontologies global organization and \nroles. \n  \n \n2.  SEMIOTIC ANNOTATIONS ON WORKS \nEXTRACTS \nMusic, as any other ar- \ntistic field, deals more \nwith interpretations than \nwith formal knowledge. \nWhat makes it interest- \ning is that different art- \nists produce different \nperformances, somehow \nrecognizable, even if \nthey play the same \npiece. But explaining \nthese differences and \nhow to produce them is \na delicate task. Common formal representations are not \nsuited to do so for two main reasons. The first one  is that \ntextual descriptions cannot really convey emotions and \ngestures, even if they are important for search engi nes \nand knowledge bases. The second one is that they ar e not \nadapted to share subjective interpretations. For in stance, \nthe comment “I think that the 53421 fingering is mo re \nsupple for small hands” is difficult to represent accurately \ninto a machine language for the time being. \nTo overcome these issues, we propose to manage \nSigns rather than Knowledge. A Sign is a subjective  \ncommunication object composed of a content  (images, \ngestures, writings, sounds), its form  (structure, organiza- \ntion, context) and its sense  (interpretation, meaning) from \na subject  point of view at a given time [5]. For instance, a  \nsymbol is a particular type of graphical Sign, i.e. a form \nshared among a group of people. But a Sign can also  be a \nsimple gesture (e.g., a nod), with various meanings  ac- \ncording to its context (historical, or cultural). I n artistic \nfields, where practical know-how is essential, Sign s are \nessential to communicate different interpretations.  To \nmanage Signs in an information system, we link each  of \ntheir components to a digital element. Contents can  be \nembedded in multimedia data  (audio, video), form can \nconsist in its contextual information  (metadata, localiza- \ntion, selection), and sense consists in knowledge , repre- \nsented by a textual comment or a semantic descripti on for \nthe machine (i.e., tags from a structured vocabulary ). As \nshown on Figure 2, the Sign object can be represent ed as \na tetrahedron [5]. \nWe define a Semiotic Annotation (SA) framework to \ncapture these Signs on collaborative annotation pla tforms. \nFigure 3 presents the proposed model for Semiotic A nno- \ntation. To insure its integration to the current Se mantic \nWeb, the SA model is linked to top-level ontologies . The \nFRBR ontology allows addressing creative works and \ntheir different parts. Indeed, to create contextual ized an- \nnotations, we need to address precise parts of the dis- \ncussed Work . A WorkPart  can be any entity which is a \npart of another entity. We note that a Work  or a WorkPart  \nexists independently from its different Expressions  and \nManifestations . For instance, the first  Verse (WorkPart \nresource) in the Frères Jacques  canon ( Work resource) \ncan be discussed without reference to a particular perfor- \nmance of the tune ( Expression ), recorded on a particular album ( Manifestation ).  This is why we introduce a Selec- \ntion  concept to isolate the concerned WorkPart  on any \nembodiment of a Work . For example, in the musical field, \nit can be: \n- an extract of a MusicXML score: [n 1,n2,…,n n], where \nni are <note> elements defined by their bar number \nmi and their order of appearance k i in this bar (m i,k i), \n- an area of a PDF score: [(x 1,y 1),(x 2,y 2), …,(x n,y n)], \nwhere each (x i,yi) defines a summit of a selection \npolygon on the area of the score, \n- an extract of a video performance: [t 1,t 2] where t i is a \ntimecode of the video and t 1<t 2. \nNaturally, a WorkPart  resource can be composed of \nother Workpart  resources thanks to the frbr:part  relation. \nWe also take into account the fact that different u sers \nmay identify structures differently on a given piec e. For \ninstance, different musicologists will not necessar ily \nidentify the same structures, according to their res pective \ninterests (global structure, harmonic, rhythmic pat terns or \nthemes expositions). We thus introduce a partitioner  rela- \ntion to specify the person who extracts and names t he \npart. This person is not necessarily the one creati ng the \nannotations afterwards, as structuring pieces and a nnotat- \ning them can be distinct activities. Our model also  allows \nto link similar parts. To do so, the relatedTo  relation can \nbe specialized into more specific relations to indi cate how \ndifferent parts relate to each other (see applicati on to the \nmusical field in the next part). To name the part a ppropri- \nately, a taxonomy may be helpful according to the c on- \nsidered field (e.g., the Musical Forms and Structure s On- \ntology in the case of music). Once the part has been  clear- \nly extracted and identified, it can be annotated. T hus, a \nSemioticAnnotation  concept is proposed, including the \ndifferent components of the Sign (content, form, sen se, \naccording to a subject) presented previously. The s ense \ncomponent can be linked to a domain ontology in orde r to \nprovide a semantic description of the SA for furthe r pro- \ncessing. Of course, the subject is the creator of t he SA, \nwhich conveys his personal interpretation. Our SA i s per- \nceived as a specialization of the Annotation  concept from \nthe Open Annotation Model already in use in several  ap- \nplications (Utopia 1, YUMA 2 framework). However, the \nSA is more centered on interpretation comparisons. This \n                                                           \n1 http://getutopia.com/index.php, visited on the 06/ 05/2013. \n2YUMA Universal Media Annotator : https://github.com /yuma-\nannotation/, visited on the 06/05/2013. \n \nFigure 3: Semiotic Annotation model. \n \n \nFigure 2: Sign tetrahedron.  \n  \n \nis why we also define relations to characterize ada pta- \ntions, oppositions, and enhancements of SA. A SA ca n \nalso simply be a reaction to another SA, such as an  an- \nswer to a question, or a comment. Users can also kee p a \ntrace of their thought process by using versioning mecha- \nnisms on their annotations. Indeed, constructing an  inter- \npretation is generally an iterative process, where the sub- \nject enhances his performance with training and fee d- \nback. These relations are detailed in Figure 4. The  main \npurpose of such annotations is to steer, organize a nd \nshare helpful know-how emerging from practical cases . \nThis model can be applied to the musical field by r e- \nplacing the frbr:Work  concept by the more specialized \nmo:MusicalWork  concept. The WorkPart  concept will \nthus become the MusicalWorkExtract  concept (which is \nnot part of the current MO). But in the musical fie ld, it is \nimportant to name the detected parts when possible.  For \ninstance, a user could identify a Chorus , a Verse , a Fugue \nTheme , a Variation , a Leitmotiv , a Bass  part, a Canon \nVerse , a Solo , a Musical Bridge , etc. This is why we pro- \npose a conceptual model for a Musical Forms and Stru c- \ntures Ontology. \n3.  A MUSICAL FORMS AND STRUCTURES \nONTOLOGY \nNot to be confused with its genre, the form of a mu sical \npiece refers to its global organization and instrum entation \n[8]. For instance, the concerto form has three move ments \nand features a solo instrument. A form sometimes im plies \nthe presence of significant structures. For example,  a \nfugue necessarily contains a main theme, which will  be \nexposed at each voice, and then developed, inversed , \ntransposed in successive stretti. Besides, most basi c struc- \ntures have a type but not necessarily an explicit n ame \n(e.g., phrases, motifs, scales, arpeggios). While m usical \ngenres and moods are regularly studied in the MIR l itera- \nture (for instance [6] or [4]), musical forms and s tructures \nare less discussed. Indeed, the latter are rather a ddressed \nin musicological contexts and remain unexploited in  mu- \nsic recommendations systems and scores sharing com-\nmunities. Thus, we design a basic framework for a M usi- \ncal Forms and Structures Ontology (MFSO) to back up \nour Musical Performance Ontology with appropriate m u- \nsicological terms. As noted in an analog work on a Musi- \ncal Genres Taxonomy [6], achieving objectivity is a  diffi- \ncult task in a musical context. Concerning forms an d \nstructures, even specialists do not agree on some te rms \n(see discussion). This is why we only provide high- level \nconcepts for the time being, which are easier to di fferen- \ntiate, are well documented in musicological treatie s such \nas [8] and largely used by musicians. Our MFSO is thus designed to characterize any musi-\ncal work and its extracts (Figure 5). A Musical Wor k can \nhave a Musical Form (e.g., Sonata, Fugue, Song, Can on, \netc.). This form can be linked to a characteristic genre \n(e.g., Fugues and Sonata are generally associated t o clas- \nsical music), but there may be exceptions (e.g., cl assical \nstructures in symphonic rock). As pointed out previ ously \na form can also be linked to its characteristic str uctures. \nThese two links ( typicalOf  and hasStructure ) do not aim \nat restricting the annotation possibilities, but rath er at \nsuggesting appropriate terms to the annotator, from  basic \nmetadata. For instance, the title of the piece may indicate \nits form (example: “ Sonata KV545 ” by M ozart) which \nallows the annotation service to suggest appropriate  struc- \ntures to the user (example: Exposition , Development ). But \na musical extract does not necessarily have a name.  This \nis why all named structures inherit from the generi c Mu- \nsicalWorkExtract  concept. A MusicalWork  resource can \ncontain several MusicalWorkExtract  resources. These ex- \ntracts may be imbricated thanks to the frbr:part  relation. \nA musician can detail how an extract is written wit h the \ncontains  relation: is it a whole Phrase , a Motif , a simple  \nScale,  or a Sequence  of Chords ? \nA musical extract can also be labeled. Musicians ge n- \nerally structure a piece by associating alphabetica l labels \nto its different parts. This notation (or codificat ion) al- \nlows to clearly distinguish repeated parts (example : \nABABC, or ABAB’C meaning B’ is almost like B). The \nlabel  relation allows us to link our work to [9], which \nproposes notations conventions for structure labeli ng of \nmusical extracts. We insist on the distinction betw een a \nnotation and an annotation: a notation is a codific ation \n(i.e., a representation) of an object, while an annot ation is \na comment on an object. Thus, they are not related a pri- \nori, even if an annotation content can consist in a notation \nfragment.  \nThe relatedTo  relation allows to link distinct extracts. \nIt can be specialized in order to express that an e xtract \nintroduces , concludes , imitates, transposes, ornates or  \naccompanies another one. These relations can be auto- \nmatically associated with specific named structures . For \ninstance, a Variation  necessarily relates to a Theme  and \nshould be associated to it via the variation  relation. Other \nautomatisms can be implemented if a MusicXML repre-\nsentation of the piece is available. Indeed, notes, chords, \nscales and arpeggios can easily be extracted using the el- \n \nFigure 4: Relations between two Semiotic Annota tions. \n \n \nFigure 5: MFSO conceptual model. \n  \n \nements names (e.g., <chords>) or by detecting regular  \nstructures (e.g., ascendant or descendant thirds se quences \nfor arpeggios). Undefined regular patterns are more  deli- \ncate to automatically identify, even if some works exist \non this issue [10]. Some work has also been done on  de- \ntecting Fugue structures [12]. Now that the part ha s been \nextracted and named, it can be annotated. \n4.  A MUSICAL PERFORMANCE ONTOLOGY \nOur Musical Performance Ontology (MPO) aims at tag-\nging Semiotic Annotation appropriately in a musical  edu- \ncation context. It intervenes in the sense component  of \nthe SA (see Figure 7) to specify what themes are tr eated \nby the subject on the annotated extract. This organ ization \nhas been chosen because the MPO deals with subjecti ve \ninterpretations and advice (emotions, expression, f inger- \nings) rather than verifiable facts (artists, tracks , concerts), \nas the MO does. Embedding the MPO in a SA resource \nallows musicians to discuss their interpretations a nd pos- \nsibly enhance them. Tagged SA can then be exploited  by \nlearning agents to answer complex queries, discover in- \nteresting resources, link similar SA, generate exer cises or \nsuggest appropriate SA to help a student to learn a  new \npiece. For the time being, our proposition is not m eant to \nbe instrument-specific but is highly influenced by our ex- \nperience at the piano and guitar. However, it is po ssible \nto extend it according to other instruments requirem ents. \nThe MPO conception methodology is inspired by the A r- \nchonte methodology [7]. Recurrent linguistic units were \nextracted from recorded piano lessons. These units were \nclassified by an identity and differentiation proce ss and \neach obtained class was labeled (see Figure 6). We then \nnoticed that each label could be associated to one of the \nmain activity of music practice: listening, playing  and \nlearning. Thus, we used these three activities to d esign \nour MPO concepts tree. This ontology can be used to  se- \nmantically describe a Musical Expression (listening  activ- ity), an Instrumental Technique (playing activity) or an \nAssimilation Method (learning activity). Naturally,  these \nthree concepts are related: an Instrumental Techniq ue \nproduces a Musical Expression but requires an Assim ila- \ntion Method to be handled by a learner (Figure 7). Be- \nsides, the sociologist Megan Winget also highlights Ex- \npression and Technique as essential annotation type s in \nher study on musicians ’ annotation practices [13]. \nThe MusicalExpression  concept regroups all concepts \nwhich deal with musical writing ( Rhythm , Harmony , \nStructures , volume and tempo Dynamics ) and its \nresulting Sound . Various sound features can be described, \nsuch as pitch, duration, timbre, mood and articulat ion (i.e. \nlegato, staccato). The Structure concept regroups \nelements from the MFSO previously presented. Indeed , \nthe SA may discuss specific elements inside the sel ected \npart. For instance, a musician can extract all occu rences \nof a leitmotiv in a given musical work. This naming  task \nis realized at the MusicalWorkExtract  resource level, \nthanks to the Leitmotiv  concept from the MFSO. An other \nmusician can then annotate one of the occurences an d \ninsist on one of its intervals requiring more atten tion than \nthe others. This time, the structure identification  is at the \nSA level, but still requires the Interval  concept defined by \nthe MFSO. This is because the second structure \nidentification is more a personal approach of the p iece \nand will not necessarily catch the attention of a \nmusicologist as it does for a performer, who has to  \n“animate” the piece.  This approach gives more flexibility \nto the users, by allowing them to distinguish high level \nand meaningful structures (theme, phrases) from basi c \nelements (note level). Different relations exist be tween \nthese musical expression concepts but were not \nrepresented in Figure 7 for readability purposes. F or \nexample, Duration and Rhythm are strongly related, as \nextending a note duration (e.g., a fermata) has an impact \non the overall rhythmic organization around the not e. \nIdentified structures can also be linked to their h armonic \n(hasHarmony ) and rhythmic ( hasRhythm ) features. The \nHarmony concept can be linked to the Chord Ontology1 \nto specify the chords at stake in a standardized no tation. \nThe InstrumentalTechnique  concept allows the \nmusician to tag any instrument-related technical ma tter. It \ndeals with gestures, movements, fingerings and posit ions \n                                                           \n1 http://www.omras2.org/ChordOntology, visited on the  06/05/2013.  “We will study Espièglerie from Kabalevski. During t he learning(6), \nwe should try to keep the playful  and light-hearted tone(1) of this little \npiece. The main work(6) is based on fifths sequences(5).  \nConcerning text learning(6), there are several part s(5)  to \nconsider in this work. There is the first part(5) where the left and right \nhand(7)  play the same notes, namely two fifths(5) (plays (9)).  For the \nright hand(7) , we will develop(9) all notes but for the moment, it is \nbetter to play(9) only the fifths(5) like this, so that the notes are rapid- \nly assimilated(6) (plays the fifths). One should pay a ttention to the 2-5 \nfingering at the left hand(7) , which allows to get livelier and ligh ter \ndetached notes(1). Here is what you should obtain ( plays the part), \nhere both hands(7) play at the same time(5).  \nThat’s all for the  first part(5). This should be rapidly as- \nsimilated(6) , with the right nuances(4)(1). I played it slowly( 3) , but it \nshould not be a problem to play it at the right tempo(3) after repeat ing \nit 3 or 4 times(6)  (plays the fifths at the right tempo). It should be  \nplayed naturally(6)(1), and by heart(6) as soon as possible because it \nis rather difficult(6) to move on the keyboard and watch the \nscore(7)(8)  at the same time.”  \n1 Sound   2 Harmony   3 Rhythm   4 Dynamic  5 Structure  6 Assi milation   \n7 Gesture   8 Support   9 Relation  \n Figure 6: Piano lesson analysis example: extraction a nd \nclassification of significant linguistic units.  \n \nFigure 7: Musical Performance Ontology conceptual mod el. \n  \n \nwhich are essential to produce the desired musical \nexpression. As can be seen in Figure 7, an Instrume ntal \nTechnique involves a musician’s limbs or articulations \n(arms, hands, wrists, etc.) and a part of his instr ument (on \nthe piano: the keyboard, the pedals, the white keys,  the \nblack keys, etc. in singing: the vocal cords, the l ungs, the \nmouth, etc.). The produces  relation allows to link a \ntechnique to its resulting musical effect. In practi ce, the \nFingering  concept will certainly be the most used one, as \nit is one of the most important information for sig ht-\nreading musicians. If a MusicXML score of the piece  is \navailable, a fingering can be associated to a copy of the \nannotated extract (MusicXML 3.0 supports <fingering> \nnodes). Using copies instead of the original MusicX ML \nfile allows musicians to propose several fingering \nsolutions for a single part. The idea is to clearly  \ndistinguish the basic abstract notation (MusicXML n otes, \nprovided by editors) from the concrete annotations \ndemonstrating musical know-how (SA, provided by \nmusicians). \nThe Assimilation  concept deals with educational \nmechanisms, to help a student to handle an instrume ntal \ntechnique. In particular, it aims at providing a fr amework \nto create exercises based on rearrangements of the \noriginal score. For instance, piano teachers often reduce a \npart ’s  difficulty by dividing it into smaller parts (i.e. \nintervals). These elements are repeated three or fo ur times \nand then grouped by two, by three, etc…gradually \nincreasing the difficulty. We could thus generate t he \ncorresponding score for each step of the exercise. O ther \neducational mechanisms can be described in the \nAssimilation concept: beats counting, sight-reading  \nmethods, work calendar, learning by heart, etc. \nTo have a better understanding of how this ontology  \ncan be used and for what purposes, we propose a simp le \nuse case in the following section. \n5.  SA, MFSO AND MPO USE CASE \nWe propose a fictive use case to study how the prop osed \nSA, MFSO and MPO models can work together. A \nmusician wishes to annotate a score with the follow ing \nassertions: \n- “Here is an occurrence of leitmotiv A. I label it A 1.”  \n- “Here is an other occurrence of leitmotiv A. I labe l it \nA2.”  \n- “A 1 and A2 are almost similar, except A2 is a \ntransposition of A1.”  \n- “Both are based on the following rhythm: eighth –  \nsixteenth triplet – quarter.”  \n- “Beware of the articulation on A1’s first note : it is \nstaccato. It must be done with the whole arm to \nprevent a too sharp attack and prepare for the next  \nposition. It is also possible to lightly increase t he \nvolume on the ascendant arpeggio. But always keep \nthe solemn mood of the piece.”  \nFigure 8 represents the identified RDF 1 triplets. We \nhighlight the fundamental distinction between the s tudied \nobjects (the piece and its extracts, described with  the MO \n                                                           \n1 Resource Description Framework.  and MFSO) and their interpretations (semiotic \nannotations, described with the SA and MPO). The \nrelation between the two leitmotiv occurences \n(transposition) allows to deduce the type of A2. It  also \nallows to duplicate the explanations given for A1 o n A2, \nas long as tonal dependant concepts are not involve d \n(which is the case here). The “Rhythm URI” resource can \nbe linked to an appropriate rhythm notation resourc e \n(e.g., a MusicXML short fragment). Common \narticulations, dynamics and characters can be sugge sted \nin a list to facilitate the semantic description in put. The \ndescription of the arm gesture remains brief, but w ill \nena ble to return this annotation for requests such as “how \nto play staccato?”. The content and form components of \nthe SA were not represented in Figure 8 which focus es on \nsemantic description, but can be added by the user.  \nTypically, the staccato gesture should be demonstra ted \nwith a video resource (content component). The form  \ncomponent allows the user to select different ways of \nindexing his annotation. But did the musician use t he \nappropriate terms to describe his interpretation ? \n6.  DISCUSSION \nAs noted earlier, using appropriate words in music is a \ncomplex issue, mainly for subjectivity and context rea- \nsons. Naturally, musicology also aims at clearly de fining \nterms and analysis methods in this field. But somet imes, \nmusicologists themselves do not agree on appropriat e \nterms to designate structures. For instance, there is a \n“French school” and a n “Anglo -Saxon school”  with dis- \ntinct methods, vocabularies and notations customs. \nIndeed, the concept of “rule” in music is comple x. \nThere is a framework (mainly the tonal system in oc ci- \ndental music), a history, a culture, an aesthetic w hich \nwere built over time and various influences, but no  defi- \nnite rule. Thus, the line between form, genre and s tyle can \nbe thin in many circumstances. \nSome works exist in order to clarify the manipulate d \nterms meanings and to prevent confusion in musical writ- \n \nFigure 8: SA, MFSO and MPO use case example.  \n   \n \nings, such as the GDRM 1 initiative at Laval University. \nBut the most important factor is the context of use . For \nexample, the “pedal” can either designate a long bass \nnote, or a part of the piano. This example is espec ially \ninteresting because, even if the two meanings are d istinct, \nthey are somehow related and can explain why the sa me \nword is used: indeed, the piano pedal do help to pr oduce \na bass pedal in some cases. Both meanings thus impl y the \nidea of a “ long resonance ”.  For the time being, we pro- \npose to overcome this issue by making a distinction  be- \ntween the concept and its common designation. For t he \npedal example, we define two concepts: BassPedal  and \nPianoPedal , having both the designation “pedal”, corr e- \nsponding to the way they are commonly called by mus i- \ncians. \nThe difficulty also lies in the interrelation of th e de- \nfined concepts. As seen in the use case, musicians rarely \naddress one theme at a time, as all musical element s in- \nteracts with each other, and the musician should ha ve \ncontrol on each of them while playing. Providing an  ex- \ntensive semantic description in such case is very d ifficult. \nThis is why we rely on common concepts and try to e s- \ntablish simple relations between them. But more spe cific \nrelations can only emerge through intensive use of the \nannotation platform. This is why we rely on an iter ative \nconstruction of our descriptive model [14]. \nThus, our collaborative annotation platform aims at  \nfostering fruitful debates to help musicians confro nt their \nways of analyzing and practicing music. \n7.  CONCLUSION \nIn this paper, we proposed a Musical Forms and Struc - \ntures Ontology (MFSO) and a Musical Performance On-\ntology (MPO) to annotate performances and scores se - \nmantically. These ontologies aim at extending the s tand- \nard Music Ontology with musicological and musical \nknow-how concepts. To do so, we first introduced a Se- \nmiotic Annotation framework to allow users to descr ibe \npersonal interpretations of musical works and then intro- \nduced our MFSO. This allows musicians to name preci se- \nly the musical extract to annotate and its role in the work. \nMusical expressions, techniques and assimilation me th- \nods can then be described thanks to appropriate con cepts \nand relations from our MPO. This work notably aims at \nbuilding a music learning agent which can help musi cians \nto answer complex queries, discover interesting \nknowledge among large digital scores collections, g ener- \nate exercises or suggest appropriate SA to help a s tudent \non a new piece. \nNaturally, perspectives for this work include testi ng it \nwith musicians of all levels, which will allow us t o refine \nand extend our proposition. To do so, a collaborati ve se- \nmiotic annotation platform for mobile tablets is cu rrently \nbeing developed. \n                                                           \n1 http://www.mus.ulaval.ca/roberge/gdrm/, visited on the 06/05/2013.  8.  REFERENCES \n[1]  V. Sébastien, P. Sébastien, N. Conruyt, “@ -MUSE: \nSharing Musical Know-how Through Mobile Devices \nInterfaces” , 5th Conference on e-Learning Excellence in \nthe Middle East, Dubaï, 2012. \n[2]  V. Sébastien, H. Ralambondrainy, O. Sébastien, N. \nConruyt, “Score Analyzer: Automaticaly Determining \nScores Difficulty Level for Instrumental e- Learning” , \nProceedings of the International Conference on Music  \nInformation Retrieval , ISMIR, 2012. \n[3]  T. Vander Wal, \"Folksonomy\", Online posting , No 7, \n2007.  \n[4]  M. Sordo, “Semantic Annotation of Music Collections: A \nComputational Approach ”, PhD thesis report, Universitat \nPompeu Fabra, 2012. \n[5]  N. Conruyt, D. Grosser, R. Vignes-Lebbe, \"Knowledge \nDiscovery for Biodiversity: from Data Mining to Sign \nManagement\", International Congress on Environmental \nModeling and Software , Leipzig, Germany, 2012. \n[6]  F. Pachet and D. Cazaly, \"A Taxonomy of Musical \nGenres\", in Content-Based Multimedia Information Access \nConference (RIAO) , Paris, 2000. \n[7]  B. Bachimont, Ingénierie des connaissances et des \ncontenus, le numérique entre ontologies et document s . \n(Tr.: Knowledge and contents engineering, digital tec hnologies \nbetween ontologies and documents) , Lavoisier, Paris, 2007. \n[8]  A. Hodeir, Les Formes de la musique (Tr.: The Forms of \nMusic) , Presses Universitaires de France, 2012. \n[9]  F. Bimbot, E. Deruty, G. Sargent, and E. Vincent, \n\"Semiotic structure labeling of music pieces: concept s, \nmethods and annotation conventions\", in Proceedings of \nthe International Symposium on Music Information \nRetrieval , vol. 2, 2012. \n[10]  O. Lartillot: “Une analyse musicale automatique suivant \nune heuristique perceptive”  (Tr.: An automatic musical \nanalysis based on a perceptive heuristic) , 3ème Conférence \nInternationale Francop hone sur l’Extraction et la Gestion \ndes Connaissances , EGC 03, Lyon, 2003. \n[11]  Y. Raimond, S. Abdallah, M. Sandler, and F. Giasson: \n“The Music Ontology”, Proceedings of the International \nConference on Music Information Retrieval , ISMIR, 2007. \n[12]  M. Giraud, R. Groult, F. Levé,  “Detecting Episodes with \nHarmonic Sequences for Fugue Analysis ”, Proceedings of \nthe International Conference on Music Information \nRetrieval , ISMIR, 2012. \n[13]  M.A. Winget, \"Annotations on musical scores by \nperforming musicians: Collaborative models, interactive  \nmethods, and music digital library tool development\" , \nJournal of the American Society for Information Sci ence \nand Technology , vol. 59, no. 12, pp. 1878-1897, 2008. \n[14]  N. Conruyt and D. Grosser, \"Knowledge Management in \nEnvironmental Sciences with IKBS: Application to \nSystematics of Corals of the Mascarene Archipelago\", \nSelected Contributions in Data Analysis and \nClassification , pp. 333-343, 2007."
    },
    {
        "title": "A Video Compression-Based Approach to Measure Music Structural Similarity.",
        "author": [
            "Diego Furtado Silva",
            "Hélène Papadopoulos",
            "Gustavo Enrique De Almeida Prado Alves Batista",
            "Daniel P. W. Ellis"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417087",
        "url": "https://doi.org/10.5281/zenodo.1417087",
        "ee": "https://zenodo.org/records/1417087/files/SilvaPBE13.pdf",
        "abstract": "The choice of the distance measure between time-series representations can be decisive to achieve good classification results in many content-based information retrieval applications. In the field of Music Information Retrieval, two-dimensional representations of the music signal are ubiquitous. Such representations are useful to display patterns of evidence that are not clearly revealed directly in the time domain. Among these representations, self-similarity matrices have become common representations for visualizing the time structure of an audio signal. In the context of organizing recordings, recent work has shown that, given a collection of recordings, it is possible to to group performances of the same musical work based on the pairwise similarity between structural representations of the audio signal. In this work, we introduce the use of the CampanaKeogh distance, a video compression-based measure, to compare musical items based on their structure. Through extensive experiments, we show that the use of this distance measure outperforms the results of previous work using similar approaches but other distance measures. Along with quantitative results, detailed examples are provided to to illustrate the benefits of using the newly proposed distance measure.",
        "zenodo_id": 1417087,
        "dblp_key": "conf/ismir/SilvaPBE13",
        "keywords": [
            "distance measure",
            "content-based information retrieval",
            "Music Information Retrieval",
            "self-similarity matrices",
            "pairwise similarity",
            "CampanaKeogh distance",
            "video compression-based measure",
            "quantitative results",
            "examples",
            "benefits"
        ],
        "content": "A VIDEO COMPRESSION-BASED APPROACH TO MEASURE MUSIC\nSTRUCTURE SIMILARITY\nDiego F. Silva1,H´el`ene Papadopoulos2,Gustavo E.A.P.A. Batista1and Daniel P.W. Ellis3\n1Instituto de Ci ˆencias Matem ´aticas e de Computac ¸ ˜ao – Universidade de S ˜ao Paulo\n2Laboratoire des Signaux et Syst `emes (L2S), CNRS UMR 8506, France.\n3Department of Electrical Engineering - Columbia University\ndiegofsilva@icmc.usp.br ,helene.papadopoulos@lss.supelec.fr ,\ngbatista@icmc.usp.br ,dpwe@columbia.edu\nABSTRACT\nThe choice of the distance measure between time-series\nrepresentations can be decisive to achieve good classiﬁ-\ncation results in many content-based information retrieval\napplications. In the ﬁeld of Music Information Retrieval,\ntwo-dimensional representations of the music signal are\nubiquitous. Such representations are useful to display pat-\nterns of evidence that are not clearly revealed directly in the\ntime domain. Among these representations, self-similarity\nmatrices have become common representations for visual-\nizing the time structure of an audio signal. In the context of\norganizing recordings, recent work has shown that, given\na collection of recordings, it is possible to to group perfor-\nmances of the same musical work based on the pairwise\nsimilarity between structural representations of the audio\nsignal. In this work, we introduce the use of the Campana-\nKeogh distance, a video compression-based measure, to\ncompare musical items based on their structure. Through\nextensive experiments, we show that the use of this dis-\ntance measure outperforms the results of previous work us-\ning similar approaches but other distance measures. Along\nwith quantitative results, detailed examples are provided to\nto illustrate the beneﬁts of using the newly proposed dis-\ntance measure.\n1. INTRODUCTION\nWithin the last few years, time-series methods and algo-\nrithms have attracted the interest of many research com-\nmunities such as Data Mining and Information Retrieval.\nIndeed, processes of interest generally change over time,\nand the study of how these changes occur is a central issue\nto fully understand such processes.\nThe choice of the distance measure between time-series\nrepresentations can be decisive to achieve good classiﬁ-\ncation results in many content-based information retrieval\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.applications. Two distance measures commonly used in\ntime-series analysis are the Euclidean distance (ED) and\nthe Dynamic Time Warping distance (DTW). The DTW\ncan be understood as an extension of the ED, able to pro-\nvide nonlinear time scaling invariance, popularly known as\nwarping [3]. Those simple and well-known distances have\nbeen successfully applied to various kinds of problems and\nhave proven to be very hard to beat [9].\nSome time-series features are not evident in the time\ndomain. There is a large number of research papers that\npropose methods to explore alternative representations of\ntime-series, such as autocorrelation [1] and shapelets [20],\nin order to clarify speciﬁc features.\nIn the hot context of organization and retrieval of large\ncollections of music, the notion of similarity between mu-\nsic recordings is of great importance for many applica-\ntions such as music summarization [8] or cover song re-\ntrieval [11]. In general, the similarity between two record-\nings is measured by comparing their respective time-series.\nMusic audio signals are highly structured at different\ntime-scales (bar, phrase-level, sections, etc.) and exhibit\nrepetitive segments, e.g. the so-called ABA sonata form\n(exposition - development - recapitulation) in classical mu-\nsic. Since their introduction in the domain of audio in\n1999 [12], self-similarity matrices (SSMs) have become\ncommon representations for visualizing the time structure\nof an audio signal in terms of self-similarity and repeti-\ntions. Such two-dimensional representations are obtained\nby computing the pairwise similarity of an audio feature\nsequence (such as mel-frequency cepstral coefﬁcients [21]\nor chromas [2]), and allow putting in evidence patterns that\nare not clearly revealed directly in the time domain. For\ninstance, repeated patterns in the audio will appear as di-\nagonal stripes in the SSM. For more details and a review\nabout music structure, we refer the reader to [24].\nAmong the various applications based on cross-retrieval\ntasks, we are interested in this work in the problem of,\ngiven a piece of music as a query, to automatically retrieve\nfrom a given collection all performances (various interpre-\ntations) of the query. We focus on classical music. Two in-\nterpretations of the same piece will have a similar musical\ncontent, but they may differ in many ways. Besides articu-\nlation, phrasing and ornamentation, the global tempo maybe different from one performance to another. Local tempo\nvariations such as ritardandi may also exist. Moreover,\nother factors such as the recording conditions, the loud-\nness or the instrumentation may result in huge dynamical\nand spectral deviations between the two interpretations.\nDespite these variations, recent work has shown that it\nis possible to accurately measure the structural similarity\nbetween two recordings by computing the pairwise sim-\nilarity between their self-similarity matrices, without ex-\ntracting explicitly the underlying audio structure. In [16],\nthis approach is applied to the cover song detection prob-\nlem. In [22], this approach is used to build a retrieval sys-\ntem that searches a database for the musical piece that best\nmatches a given symbolic structural query. Of particular\ninterest to the present article are two closely related previ-\nous work that show that it is possible robustly group audio\nperformances of the same musical work by using a dis-\ntance that measures the pairwise similarity between their\nrespective structural representations [4, 14].\nIn order to compare two-dimensional objects directly,\nit is appropriate to use a distance measure speciﬁc to this\npurpose. An example of that is the Campana-Keogh (CK-\n1) distance [7], which uses the video compression as the\nbasis for estimating the dissimilarity between two images.\nIn this paper, we introduce the use of CK-1 to retrieve\nmusic recordings based on its structural similarity to an au-\ndio query. We show that the use of this distance measure\noutperforms the results of previous work using similar ap-\nproaches but other measures.\n2. STRUCTURAL SIMILARITY\nAs mentioned above, there are at least two studies that\nhave successfully explored the possibility to retrieve mu-\nsic recordings according to their structural similarity to an\naudio query. In this work, we introduce the use of the CK-1\nmeasure as an efﬁcient distance that can be used to accom-\nplish this task using self-similarity arrays.\nThis section describes each step of the proposed ap-\nproach. We consider the following retrieval scenario. Given\na query recording and a collection of music recordings that\ncontains various performances of the same piece as the\nquery, along with recordings of different compositions, we\naim at retrieving all the performances of the query music\npiece. To this end, we deﬁne the training and retrieval steps\nby the Algorithm 1 and Algorithm 2, respectively.\nAlgorithm 1 Training phase\nRequire: v= A collection of music recordings\nS []\nfori= 1!length (v)do\ns SSM (v[i])\nS concatenate (S; s)\nend for\n2.1 Feature Extraction\nThe ﬁrst step consists in extracting a set of features that\nprovides relevant information about the musical structure.\nAmong the various clues that humans use to determine\nthe structure of a music piece, the harmonic progressionAlgorithm 2 Retrieval phase\nRequire: q= A query recording\nS= The collection of SSM extracted in the training phase\ns SSM (q)\nD []\nfori= 1!length (S)do\nd dck1(s; S[i])\nD concatenate (D;fd; ig)\nend for\nL sortbydistance (D)\nreturn L\nis very important [6]. Since their introduction in 1999,\nthePitch Class Proﬁles [13] or chroma features [25] be-\ncame common features for describing the harmonic con-\ntent of music. The chroma features are, in general, 12-\ndimensional vectors that represent the spectral energy of\nthe pitch classes of the chromatic scale. They have been\nsuccessfully used for various content-based retrieval tasks,\nespecially in previous work on music similarity [4, 14].\nFollowing these approaches, we extract a sequence of\nchroma features, as well as two variants of chroma-like\nfeatures: the Chroma Energy Normalized Statistics (CENS)\nand the Chroma DCT-Reduced log Pitch (CRP). CENS\nfeatures involve an additional temporal smoothing and down-\nsampling step, leading to an increased robustness of the\nfeatures to local tempo changes. CRP features boost the\ndegree of timbre-invariance. We refer the reader to [23]\nfor more details. For chromagram computation, we used\nthe Matlab chroma toolbox1. All feature vectors are nor-\nmalized to have unit norm.\n2.2 Self-Similarity Matrix and Recurrence Plots\nIn order to analyze the music structure, we used a self-\nsimilarity matrix (SSM), deﬁned by Equation 1.\nS(i;j) =d(~ x(i);~ x(j)); i;j21\u0001\u0001\u0001N (1)\nwhereNis the length of the signal, ~ x(i)and~ x(j)are\nthe feature vectors at positions iandjof the signal, respec-\ntively, andd(\u0001;\u0001)is a similarity measure. In this work, we\nused the cosine distance deﬁned by Equation 2.\nd(~ x(i);~ x(j)) =<~ x(i);~ x(j)>\njj~ x(i)jj\u0001jj~ x(j)jj(2)\nA well-known variation of the SSM is called Recur-\nrence Plot (RP) [10], that introduces three parameters to\nthe SSM: an embedding dimension m, a time delay \u001cand\na closeness threshold \u000f. The general idea is that each vec-\ntor~ xis augmented by mobservations evenly spaced in \u001c\nunits of time. Therefore, we end up with a matrix X(i)2\n<m\u0002k, composed by ~ x(i),~ x(i+\u001c), . . . ,~ x(i+ (m\u00001)\u001c).\nA RP can be formally deﬁned according to Equation 3.\nRi;j= \u0002(\u000f\u0000d(X(i)\u0000X(j)));X(i)2<m\u0002k;\ni;j= 1::N\u0000m(3)\nwhere\u000fis a closeness threshold and \u0002is the Heaviside\nfunction (i.e. \u0002(z) = 0 ifz<0and\u0002(z) = 1 otherwise).\nFor a better understanding of these parameters and the use\nof closeness threshold, we recommend the reading of [15].\n1http://www.mpi-inf.mpg.de/resources/MIR/chromatoolbox/In short, RP is a thresholded version of the SSM, so that\nthe SSM is transformed into a binary matrix. The general\nidea of using the parameter \u000fis to discard spurious differ-\nences among observations that may hinder the visualiza-\ntion of relevant events. However, we note that setting such\na parameter is not an intuitive task, and one frequently has\nto rely on adhoc techniques to do so. As we will discuss in\nSection 5, our experiments show that the use of SSM and\nCK-1 distance can outperform the results obtained in the\nliterature with RP and Normalized Compression Distance.\n2.3 Compression Distances\nThe CK-1 distance [7] is based on the concept of Kol-\nmogorov complexity, proposed to quantify the randomness\nof discrete sequences. The Kolmogorov complexity K(x)\nof an object xis given by the size of the smallest program\ncapable to output xon a universal computer, such as a Tur-\ning machine [19]. Intuitively, K(x)is the minimal quantity\nof information required to generate a string xwith a pro-\ngram. Similarly, the Kolmogorov conditional complexity\nK(xjy)is the size of the smallest program to generate the\nsequencex, given a sequence yas auxiliary input.\nThese concepts are the basis to a metric called Infor-\nmation Distance, that is universal, in the sense that it sub-\nsumes other measures [5]. Although Information Distance\nhas attractive theoretical properties, it is uncomputable in\nthe general case. Therefore, several research papers have\nproposed approximations to this distance using compres-\nsion algorithms [17, 18].\nA widely used distance based on Kolmogorov complex-\nity is the Normalized Compression Distance (NCD) [18],\nthat uses standard compression algorithms to estimate the\nKolmogorov complexity. Let C(x)andC(y)be the sizes\nof sequences xandyafter they have been compressed, and\nC(xy)the compression size of the concatenation of both\nsequences. The NCD is deﬁned by Equation 4.\nNCD (x;y) =C(xy)\u0000minfC(x);C(y)g\nmaxfC(x);C(y)g(4)\nNCD has shown to provide good similarity estimates\nfor various applications in discrete domain, such as strings\nof DNA and natural language. NCD typically uses lossless\ncompression algorithms, which are well-suited for discrete\ndata. In short, lossless compression relies in ﬁnding exact\nrecurring sequences in data. However, images are com-\nposed by real-valued pixels and exact repeating patterns\nare rare. Another relevant issue is that generally the used\ncompression algorithms work over data sequences and are\nnot able to make use of spatial patterns present in images.\nTo overcome these limitations, the CK-1 distance makes\nuse of video compression algorithms. More speciﬁcally, it\nuses the MPEG-1 compressor. Such an algorithm explores\nrecurring patterns within a frame and/or between consec-\nutive frames to compress the video. When two consecu-\ntive frames are composed of similar images, the inter frame\ncompression step should be able to exploit this structure to\nproduce a smaller ﬁle size, which is therefore interpreted\nas signiﬁcant similarity. As digital video is an importantcommercial application, many efforts have been made to\nachieve high compression rates in video encoding, mak-\ning it a good approximation of the Kolmogorov conditional\ncomplexity.\nIn order to calculate a distance between two images, x\nandy, we can create a ﬁctional video with one frame for\neach image. Thus, CK-1 is deﬁned by Equation 5.\ndck1(x;y) =C(xjy) +C(yjx)\nC(xjx) +C(yjy)\u00001 (5)\nwhereC(xjy)is the size of a two-frame MPEG-1 video\ncomposed by images yandx, in this order.\n3. RELATED WORK\nIn [4], the author proposes an approach to retrieve music\nusing an intermediate representation of musical structure,\nwithout any explicit determination of such structure. The\napproach uses RP as representation of music structure and\nNCD as similarity measure between two plots. The pa-\nper performs a large-scale evaluation of the proposed ap-\nproach, including an exploration carried out to ﬁnd the best\nparameter conﬁguration, a comparison of front-end fea-\ntures, embedding and recurrence analysis strategies. Sup-\nported by such a large experimental evaluation, the author\nprovides evidence of the effectiveness of the approach.\nSubsequently, a similar method was proposed in [14],\nusing the Euclidean distance. Due to the simplicity of such\ndistance, the proposed method resulted in a more efﬁcient\napproach. However, temporal variations may cause recur-\nrence shifts and the Euclidean distance is very sensitive to\npatterns translations. To overcome this limitation, the au-\nthors used a technique to smooth the resulting plot. The re-\nsults, obtained on a dataset slightly different from [4] (with\na smaller number of recordings) also proved effective.\n4. EXPERIMENTAL SETUP\nWe conducted a broad experimental evaluation to assess\nthe effectiveness of the proposed method for music retrieval.\nIn order to evaluate and fairly compare our method to pre-\nvious work, we used the same data sets, and the same fea-\nture extraction parameter variation used in [4]. In addition,\nwe conducted experiments with a technique similar to that\npresented in [14], applying the binarization and blur meth-\nods over the SSM.\n4.1 Data Sets\nIn our evaluation phase, we used two datasets of the clas-\nsical repertoire, kindly provided by the author of [4]:\n\u000fThe ﬁrst dataset, referred in this paper as 123tracks ,\nis composed by 123 recordings of 19 different works\nfrom the classical and romantic period. Among these\ntracks, 56 are played on piano and the remaining 67\nare symphonic movements. In total there are 59 dif-\nferent conductors in this dataset;\u000fThe second dataset, referred in this paper as Mazurkas ,\nconsists in 29192recordings of 49 Chopin’s mazurkas\nfor piano. These were recorded by 135 different pi-\nanists.\n4.2 Parameter Conﬁguration\nThe CK-1 distance measure is parameter-free, as well as\nthe procedure to create the SSMs. However, CK-1 has a\nsmall caveat, it requires that all images must have the same\ndimensions. Since the dimensions of the SSM are propor-\ntional to the recordings durations, which are variable, it is\nnecessary to resize the feature sequences to a ﬁxed dimen-\nsiondbefore extracting the SSM. In our experiments, this\nis achieved by a resampling procedure, resulting in ﬁve dif-\nferent feature dimensions, d2f300, 500, 700, 900, 1100 g.\nFor each chroma-based feature, Chroma, CENS and CRP,\nwe used seven different analysis window lengths, resulting\nin seven feature rates, f2f0.333, 0.5, 1, 1.25, 2.5, 5, 10 g\nfeatures/second.\nTo facilitate the reading and writing of parameter con-\nﬁgurations, we adopted the notation Feat f=F;d=D, where\nFeat2fChroma;CENS;CRP gandFandDare the\nvalues of feature frequency ( f) and the feature vector di-\nmension (d), respectively.\n4.3 Evaluation\nAll the experiments in this work were evaluated using mean\naverage precision (MAP). Consider a collection Cconsist-\ning ofMitems, and a subset Q\u001aCcontainingndiffer-\nent performances of the same music piece. Given a query\nqi2Q, we build a ranked list by arranging the results in\nascending order according to the calculated distance be-\ntween all the pieces in the collection and the query qi. The\nAverage Precision (AP) is deﬁned by Equation 6.\nAP(qi) =1\nnMX\nr=1P(r)\n(r) (6)\nwhereris the rank in the ordered list, and\nP(r) =1\nrrX\ni=1\n(i) (7)\nand\n(r)is0if the work ris relevant and 1otherwise.\nFinally, the MAP is deﬁned by the mean of all AP values.\n5. RESULTS\n5.1 Results on the 123tracks dataset\nWe start presenting the results obtained with SSM and CK-\n1 distance on the 123tracks dataset. Tables 1, 2 and 3 report\nMAP results obtained by the proposed method, varying pa-\nrameters to calculate the SSMs using Chroma, CENS and\nCRP, respectively. Non-parametric paired Friedman and\n2This is the number of recordings presented in the ofﬁcial description\nof the dataset. However, there are pieces that are not available, and we\nfound a total of 2914 recordings. This condition also applies to previous\nwork, therefore does not affect the comparison of results.Dunnet post-hoc tests were applied to compare the statisti-\ncal difference between the results. Yellow-shaded cells are\nstatistically equivalent to the best result in each table.\nTable 1 .MAP results obtained with Chroma features\nFeature Feature dimension (d)\nrate (f) 300 500 700 900 1100\n10 0.914 0.915 0.904 0.900 0.897\n5 0.918 0.910 0.902 0.894 0.896\n2.5 0.930 0.922 0.898 0.890 0.885\n1.25 0.927 0.923 0.903 0.893 0.887\n1 0.929 0.928 0.913 0.903 0.889\n0.5 0.930 0.928 0.917 0.907 0.890\n0.33 0.930 0.927 0.918 0.907 0.888\nTable 2 .MAP results obtained with CENS features\nFeature Feature dimension (d)\nrate (f) 300 500 700 900 1100\n10 0.926 0.920 0.914 0.911 0.908\n5 0.943 0.929 0.923 0.917 0.915\n2.5 0.946 0.945 0.941 0.935 0.930\n1.25 0.943 0.943 0.937 0.933 0.930\n1 0.944 0.944 0.941 0.938 0.936\n0.5 0.946 0.944 0.940 0.940 0.942\n0.33 0.942 0.940 0.934 0.932 0.932\nTable 3 .MAP results obtained with CRP features\nFeature Feature dimension (d)\nrate (f) 300 500 700 900 1100\n10 0.930 0.936 0.937 0.932 0.935\n5 0.933 0.933 0.937 0.934 0.935\n2.5 0.933 0.940 0.936 0.935 0.934\n1.25 0.935 0.941 0.935 0.936 0.936\n1 0.933 0.936 0.932 0.934 0.937\n0.5 0.930 0.937 0.929 0.933 0.932\n0.33 0.929 0.937 0.929 0,932 0.931\nSince the parameter settings used in these experiments\nare the same as those used in [4], the results can be di-\nrectly compared. The results obtained with the proposed\nmethod outperformed the results of [4] for all parameter\nsettings. For instance, the best MAP result obtained with\nCK-1 and SSM is 0:946(CENS f=0:5;d=300). The com-\npeting method best result is 0:863(CRP f=10;d=700), a sig-\nniﬁcant difference of almost 9%. We must note that CK-1\nand SSM have no internal parameters to be searched for.\nMeanwhile, the recurrence plots used in [4] have three pa-\nrameters that need to be set. According to the author, after\na computational intensive search in the parameters space\nvarying the closeness threshold, the time delay and the em-\nbedding dimension, the best result obtained was 0:921. We\nnote that such result is still outperformed by our parameter-\nfree method.\nAlthough the performance difference is too small in or-\nder to claim a statistically signiﬁcant difference with high\nconﬁdence, there are other aspects that should be consid-\nered. The lack of internal parameters makes our method\nmuch simpler to use and having its results replicated by\nthe research community. Another relevant issue is that\nour method is very robust to changes of external param-\neters. A poor parameter choice in the feature extractionstep does not affect the performance signiﬁcantly. For in-\nstance, the worst result obtained by the proposed method is\n0:885, usingChroma f=2:5;d=1100 , while in [4], the worst\nresult is 0:225. Such invariability to the parameter setting\nis veriﬁed by the lack of statistically signiﬁcant differences\namong our best result and our remaining results when we\nvary the external parameters. We also obtained an excel-\nlent mean performance of 0.926 over all parameter values.\n5.2 Further Experiments\nWe mentioned the fact that the NCD may not be appro-\npriate to compare real value matrices. To prove this fact,\nwe applied the NCD on the SSMs obtained with the best\nparameter conﬁguration for Chroma, CENS and CRP, us-\ning the CK-1 distance. We also applied the NCD on the\nmatrices obtained by setting CRP f=10;d=700, the conﬁg-\nuration that achieved the best result in [4]. The results of\nthis experiment are shown in Table 4, as well as the results\nobtained using ED in the same conﬁgurations.\nTable 4 .Results obtained by applying NCD and ED on the SSM\nin some conﬁgurations.\nConﬁguration NCD ED\nChroma f=2:5;d=300 0.322 0.279\nCENS f=0:5;d=300 0.382 0.313\nCRP f=1:25;d=500 0.276 0.257\nCRP f=10;d=700 0.271 0.262\nWith this simple experiment, it is possible to note that\nED and NCD are not appropriate to compare SSM directly.\nHowever, in [14], it was shown that the ED can be effec-\ntively used to retrieve songs by preprocessing the SSM. We\ntake advantage of such idea to evaluate the use of CK-1\ndistance in this context.\nAs we did not have access to the code used in [14], we\njust simulated similar experiments by applying threshold\nand blur procedures. In other words, we did not apply path\nenhancement technique before applying the threshold. Our\ngoal is not to directly compare the results, because we do\nnot even have the same dataset. However, we can compare\nthe use of ED and CK-1 when some preprocessing opera-\ntions are applied on the SSMs.\nIn the binarization step (application of a threshold), we\nused the strategy to consider that k%of the closest points\nin the SSM represent recurrence. Thus, these points are\ntransformed into black ( 0) pixels in the resulting RP. The\nremaining become white ( 1). To evaluate different scenar-\nios, we used three different values for the threshold: k2\nf10, 25, 50g. Furthermore, we used a two-dimensional\ncircular averaging ﬁlter (Pillbox) to blur the image, using\nﬁve different ﬁlter sizes: l2f1, 5, 10, 20, 30g. Figure 1\nshows different representations of the same recording of\nthe Chopin’s Prelude Op.28 No.4. Plot (a)represents the\nSSM, plot (b)represents the RP (using 25% of the points)\nand plots (c&d)the result of applying a blur ﬁlter on the\nRP with four different sizes.\nFor the sake of space, we do not present all the results of\nour experiments. Brieﬂy, the best results obtained by the\nuse of distance ED in this context were better than those\nArgerich1975_Chopin_op28_4  \n(a) \n(c) (b) \n(d) CRP / f=1.25 / d=500  \na) SSM  \nb) RP k=25  \nc) RP k=25 l=10  \nd) RP k=25 l=30  \n \n \n0 1 \n0.5 \n0 1 \n0.5 0 1 \n0 1 \n0.5 500 \n0 250 500 \n0 250 \n500 \n0 250 500 \n0 250 \n0           250         500  0           250         500  \n0           250         500  0           250         500  \nFigure 1 .Four different representations of the same recording:\n(a) self-similarity matrix; (b) recurrence plot; (c) RP after appli-\ncation of a blur ﬁlter with size l= 30 .\nobtained in [4], which used the RP-NCD, in most scenar-\nios. However, the results obtained by CK-1 distance in the\nsame context were better than ED in all scenarios. The\nbest results obtained for each distance and each feature are\npresented in Table 5.\nTable 5 .Results obtained after applying a threshold and a blur-\nring effect in the recurrence plots. The value kis the percentage\nof black points and lis the size of the blur ﬁlter. The symbol \u0003in-\ndicates where CK-1 is statistically outperformed ED in the same\nparameter conﬁguration.\nk(%)l Distance MAP\nChroma f=2:5;d=30050 1CK-1 0.941\u0003\nED 0.816\n10 20CK-1 0.905\nED 0.872\nCENS f=0:5;d=30025 5CK-1 0.924\u0003\nED 0.829\n25 1CK-1 0.919\nED 0.910\nCRP f=1:25;d=50025 30CK-1 0.958\u0003\nED 0.893\n10 30CK-1 0.953\nED 0.941\n5.3 Results on Mazurkas Dataset\nWe performed experiments on the Mazurkas dataset to val-\nidate the results obtained in the previous experiments. We\nﬁrst chose the parameter conﬁguration CENS f=0:5;d=300\nsince it obtained the best classiﬁcation performance in the\n123tracks dataset. However, our method achieved a MAP\nof0:611, which we considered unsatisfactory. A more de-\ntailed analysis of the SSMs showed that in many cases the\nmatrices were not able to clearly represent the recording\nstructure. This was due to the fact that the cosine distance,\nused to extract matrices, resulted in short distances in many\ncases. Thus, the ﬁgure generated by such distances con-\ntains very dark colors when applied to a color scale be-\ntween 0and1.\nAfter analyzing the recordings, we can conclude that\nthe distances with small values can be directly related to\nthe frequency in which the features were extracted. A low\nfeature rate corresponds to a large analysis window, result-\ning in mixing several structurally distinct segments of mu-\nsic. Since many pieces in the dataset have a short dura-\ntion and numerous structural variations of short duration,their structure can only be accurately analyzed with higher\nfrequencies of feature extraction. To prove this fact, we\nperformed the same experiment using CENS f=1;d=300,\nachievingMAP = 0:760, without the need of normal-\nizing the SSM, similar to the best result achieved in [4],\nMAP = 0:767. The Figure 2 shows an example of the\ndifference between different feature extraction rates.\n(a) (b) \n0 1 \n0.5 \n0 1 \n0.5 300 \n0 150 \n0           150         300  300 \n0 150 \n0           150         300  \nFigure 2 . Self-similarity matrices of the same piece, but\ndifferent feature rates: (a) 0:5f=s; (b)1f=s;\nFinally, we used the same dataset to test the conﬁgura-\ntionCRP f=1:25;d=500 after the application of a threshold\n(k= 25 ) and a blur ﬁlter ( l= 30 ). This test was performed\nsince this conﬁguration had the best result on the 123tracks\ndataset. The effectiveness of the proposed method on this\nsetting was proved in this experiment. When analyzing\nthe structural similarity in this conﬁguration using ED, we\nreach the result of MAP = 0:652. However, when we\nused the CK-1 measure, we obtained MAP = 0:795. This\nresult is statistically superior to the results obtained by com-\npeting methods in the same dataset.\n6. CONCLUSION\nThis paper proposes a simple and parameter-free approach\nto recover music by its structural similarity. The only pa-\nrameters involved in our method are those required in the\nfeature extraction step. We also show that our method is\nrobust to a poor choice of these parameters, provided the\nparameter choices achieve meaningful SSM.\nThrough a wide experimental evaluation, we show that\nour method is superior to the techniques presented in previ-\nous work. Furthermore, CK-1 outperforms other distances\nregardless of pre-processing steps are performed in the sig-\nnal or structural representation.\nWe believe that the contribution of this paper is not lim-\nited to the presentation of a new method for retrieving mu-\nsic by structural similarity. We hope this work will en-\ncourage the scientiﬁc community to analyze signals from\nvarious domains using visual representations and image-\nfriendly distance measures.\n7. ACKNOWLEDGMENTS\nThis work was supported by grants #2011/04054-2, #2012/-\n18985-0 and #2012/50714-7, S ˜ao Paulo Research Founda-\ntion (FAPESP), and grant IIS-1117015, National Science\nFoundation (NSF).\n8. REFERENCES\n[1] A. Bagnall, L.M. Davis, J. Hills, and J. Lines. Transformation based\nensembles for time series classiﬁcation. In ICDM , 2012.[2] M.A. Bartsch and G.H. Wakeﬁeld. Audio thumbnailing of popular\nmusic using chroma-based representations. IEEE Trans. Multimedia ,\n7(1):96–104, 2005.\n[3] G.E.A.P.A. Batista, X. Wang, and E.J. Keogh. A complexity-invariant\ndistance measure for time series. In SDM , 2011.\n[4] J.P. Bello. Measuring structural similarity in music. IEEE Trans. Sp.\nAud. Proc. , 19(7):2013–2025, 2011.\n[5] C.H. Bennett, P. Gacs, Ming L., P.M.B. Vitanyi, and W.H. Zurek. In-\nformation distance. IEEE Trans. Inf. Theory , 44(4):1407–1423, 1998.\n[6] M. J. Bruderer, M. F. McKinney, and A. Kohlrausch. The perception\nof structural boundaries in melody lines of western popular music.\nMusicae Scientiae , 13(2):273–313., 2009.\n[7] B.J. L. Campana and E.J. Keogh. A compression based distance mea-\nsure for texture. In ICDM , pages 850–861, 2010.\n[8] M. Cooper and J. Foote. Automatic music summarization via similar-\nity analysis. In ISMIR , 2002.\n[9] H. Ding, G. Trajcevski, P. Scheuermann, X.e Wang, and E. Keogh.\nQuerying and mining of time series data: experimental comparison\nof representations and distance measures. VLDB , 2008.\n[10] J. P. Eckmann, Oliffson S. Kamphorst, and D. Ruelle. Recurrence\nplots of dynamical systems. Europhysics Letters , 4(9):973–977,\n1987.\n[11] D.P.W Ellis and G.E. Poliner. Identifying ”cover songs” with chroma\nfeatures and dynamic programming beat tracking. In ICASSP , vol-\nume 4, 2004.\n[12] J. Foote. Visualizing music and audio using self-similarity. In ACM\nMultimedia , pages 77–80, Orlando, Florida,, November 1999.\n[13] T. Fujishima. Real-time chord recognition of musical sound: a system\nusing common lisp music. In ICMC , 1999.\n[14] P. Grosche, J. Serr, M. Muller, and J.L. Arcos. Structure-based audio\nﬁngerprinting for music retrieval. In ISMIR , 2012.\n[15] J.S. Iwanski and E. Bradley. Recurrence plots of experimental data:\nTo embed or not to embed? Chaos: An Interdisciplinary Journal of\nNonlinear Science , 8(4):861–871, 1998.\n[16] T. Izumitani and K. Kashino. A robust musical audio search method\nbased on diagonal dynamic programming matching of self-similarity\nmatrices. In ISMIR , 2008.\n[17] E.J. Keogh, S. Lonardi, C.A. Ratanamahatana, L. Wei, S. Lee, and\nJ. Handley. Compression-based data mining of sequential data. Data\nMinining and Knowledge Discovery , 14(1):99–129, 2007.\n[18] M. Li, X. Chen, X. Li, B. Ma, and P.M.B. Vitanyi. The similarity\nmetric. IEEE Trans. Inf. Theory , 50(12):3250–3264, 2004.\n[19] M. Li and P. Vitanyi. An introduction to Kolmogorov complexity and\nits applications . Springer Verlag, second edition, 1997.\n[20] J. Lines, L.M. Davis, J. Hills, and A. Bagnall. A shapelet transform\nfor time series classiﬁcation. In SIGKDD , 2012.\n[21] B. Logan and A. Salomon. A music similarity function based on sig-\nnal analysis. In ICME , 2001.\n[22] B. Martin, M. Robine, and P. Hanna. Musical structure retrieval by\naligning self-similarity matrices. In ISMIR , 2009.\n[23] Meinard M ¨uller and Sebastian Ewert. Chroma Toolbox: MATLAB\nimplementations for extracting variants of chroma-based audio fea-\ntures. In ISMIR , Miami, USA, 2011.\n[24] J. Paulus, M. Mueller, and A. Klapuri. Audio-based music structure\nanalysis. In ISMIR , 2010.\n[25] G.H. Wakeﬁeld. Mathematical representation of joint time-chroma\ndistribution. In ASPAAI , 1999."
    },
    {
        "title": "A Meta-Analysis of the MIREX Structural Segmentation Task.",
        "author": [
            "Jordan B. L. Smith",
            "Elaine Chew"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415816",
        "url": "https://doi.org/10.5281/zenodo.1415816",
        "ee": "https://zenodo.org/records/1415816/files/SmithC13.pdf",
        "abstract": "The Music Information Retrieval Evaluation eXchange (MIREX) serves an essential function in the MIR community, but researchers have noted that the anonymity of its datasets, while useful, has made it difficult to interpret the successes and failures of the algorithms. We use the results of the 2012 MIREX Structural Segmentation task, which was accompanied by anonymous ground truth, to conduct a meta-evaluation of the algorithms. We hope this demonstrates the benefits, to both the participants and evaluators of MIREX, of releasing more data in evaluation tasks.",
        "zenodo_id": 1415816,
        "dblp_key": "conf/ismir/SmithC13",
        "keywords": [
            "Music Information Retrieval Evaluation eXchange (MIREX)",
            "essential function",
            "anonymity of datasets",
            "difficulty in interpretation",
            "Structural Segmentation task",
            "ground truth",
            "meta-evaluation",
            "benefits",
            "participants",
            "evaluators"
        ],
        "content": "A META-ANALYSIS OF THEMIREX STRUCTURE SEGMENTATION TASKJordan B. L. SmithElaine ChewQueen Mary, University of Londonjblsmith@eecs.qmul.ac.ukQueen Mary, University of Londonelaine.chew@eecs.qmul.ac.ukABSTRACTThe Music Information Retrieval Evaluation eXchange (MIREX) serves an essential function in the MIR com-munity, but researchers have noted that the anonymity of its datasets, while useful, has made it difficult to interpret the successes and failures of the algorithms. We use the results of  the 2012 MIREX Structural Segmentation task, which was accompanied by anonymous ground truth, to conduct a meta-evaluation of the algorithms. We hope this demonstrates the benefits, to both the participants and evaluators of MIREX, of releasing more data in evaluation tasks. Our aim is to learn more about the performance of the algorithms by studying how their success relates to prop-erties of the annotations and recordings. We find that some evaluation metrics are redundant, and that several algorithms do not adequately model the true number of segments in typical annotations We also use publicly available ground truth to identify many of  the recordings in the MIREX test sets, allowing us to identify specific pieces on which algorithms generally performed poorly and to discover where the most improvement is needed.1. INTRODUCTIONMIREX is a highly valued event in the MIR community. Modeled in large part on the evaluations conducted by the Text Retrieval Conference, its role is to establish bench-marks of  performance and to allow the community to compare the efficacy of  different approaches [5]. MIREX also stimulates competition and helps to drive innovation in areas that the community feels are valuable. At previous ISMIR conferences, the problems facing MIREX have been a frequent topic of discussion. Some of  these are challenges for any evaluation: e.g., the high cost of generating ground truth, and the legality of shar-ing the music in most test collections [5]. However, [14] points out some issues specific to MIREX, including the  problem of hidden data: namely, the results published by MIREX do not identify the songs used in the evaluation or provide metadata other than a general description of the corpus. For example, in the Audio Key Detection task, participants can see how often their algorithm makes different kinds of mistakes—e.g., being off  by a major fifth or by a relative key—but cannot see on which pieces their algorithm made the mistakes, or see other information related to the piece, such as the composer, instrumentation, or key. In his call for more meta-evaluation, Urbano points out that the hidden nature of  the MIREX testing data im-pedes the learning phase of  algorithm development [20]. That is, although MIREX evaluations allow the commu-nity to compare the performance of  state-of-the-art algo-rithms, it is difficult to learn from the mistakes of  the algorithms without any information about the ground truth. For example, although a researcher could learn that their key-detection algorithm makes many parallel key errors, they would have no idea in what situations their algorithm is more likely to make these kinds of errors. While improving MIREX by creating new ground truth or copyright-free datasets would be expensive, it would only require a change in publishing policies for MIREX to release more detailed evaluation data. In fact, such a change in policy was recently made for the Audio Structural Segmentation (SS) task: in 2012, MIREX posted not only the performance of each algorithm on each piece in the datasets, but the output of the algo-rithms and the matching annotation. This allows the community for the first time to look more deeply into the large-scale MIREX evaluation, and potentially identify patterns in the performance of the algorithms in order to improve them.. Urbano also recognized the need for more meta-evaluation in the MIR community [20]. Meta evaluation, or the analysis of  evaluation systems, is a popular subject in the text retrieval community (e.g., [4]) but has received less attention in MIR. A meta-evaluation investigates whether an evaluation experiment accomplishes its pur-pose: do the metrics measure the desired quantities or qualities? Is the experiment fairly and effectively estimat-ing the relative quality of different algorithms? With the release of  ground truth data in the 2012 SS task, we can attempt to answer some of these questions. The SS task was introduced to MIREX in 2009, and by then had already been the subject of  a meta-evaluation of performance metrics: the merits and short-comings of  over a dozen previously used metrics was discussed in [10], which proposed an additional two. In [6], an extended evaluation of  the algorithms submitted to the 2011 MIREX SS task, the authors accessed the algorithms submitted to MIREX in 2010 and tested them on the newly available SALAMI corpus [19]. They com-pared the algorithms’ performance on the large- and small-scale segmentation data, on the different genres within SALAMI and on the different MIREX datasets. With the new MIREX task came a greater interest in generating test collections and designing appropriate methodologies: [13] made recommendations that were adapted by [19] in the creation of the SALAMI dataset, while [3] conceived an alternative methodology, leading to the INRIA collections of annotations. \nPermission to make digital or hard copies of all or  part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for  profit or  commercial advantage and that copies bear this notice and the full citation on the first page. © 2013 International Society for Music Information Retrieval  In this article, we study the results of  the 2012 MIREX SS task for these two purposes: first, to learn about the failure modes of  the current state of  the art algorithms, and second, to appraise the success of  the task so far. In Section 2, we review the test sets, algo-rithms, and evaluation metrics involved in the SS task. In Section 3, we conduct a correlation analysis of  the SS evaluation results to find which metrics and basic fea-tures could be measuring the same quantities. In Section 4, we show how the ground truth released in 2012 allows us to identify many of  the anonymous pieces in the pub-lished results, and we survey some of  the easiest and hardest songs to analyze.2. THE STRUCTURE SEGMENTATION TASKIn this section we summarize the materials of  the 2012 SS task: the algorithms submitted, the test data used, and the evaluation metrics calculated.2.1  AlgorithmsFive algorithms were evaluated in the 2012 MIREX SS task: KSP [8], MHRAF [11], OYZS, SBV [16] and SMGA [17]. KSP was submitted with four different pa-rameter settings, and two versions of  SMGA were sub-mitted, resulting in nine algorithm runs. The algorithms are outlined in abstracts published with the MIREX re-sults, although no abstract is posted for OYZS, and the difference between the two versions of  SMGA is not specified in [17]. The four algorithms with abstracts are briefly compared below. Among the most important differences between the algorithms is each one’s hypothesis about what musical structure is. Using the terminology of [12], the KSP algo-rithm uses the states hypothesis, which holds that sec-tions are musically homogenous and distinct from one another. MHRAF uses the sequences hypothesis, which holds that sections are defined by distinct sequences. SMGA uses a combined approach, employing a novel feature representation that captures information about long-term repetitions and short-term homogeneity. Fi-nally, SBV is based on the novel “system and contrast” theory of musical structure described in [2]. The algo-rithm expects that sections will consist of 4 groups of  4 measures, with the fourth group either conforming to or contrasting with the system of musical relationships es-tablished by the previous three groups. Other important differences between the algorithms are:•MHRAF, SBV and SMGA all use harmonic features, while KSP uses a combination of  harmonic and timbral features.•All the algorithms except for MHRAF estimate boundaries first and then estimate segment labels; the MHRAF algorithm detects repetitions first and uses these to define the segmentation.•SBV is the only algorithm to employ a beat-detection step to align the analysis frames.•The parameters of  the SBV algorithm were set from a test on the INRIA annotations of  the RWC database, and a version of SMGA was previously tested on the Beatles and RWC datasets [18].2.2  MIREX dataThe 2012 SS task was evaluated using three datasets:•MIREX09: A set of  297 pieces introduced in 2009, with annotations taken from the EP [22], Isophonics [24] and TUT collections [26]. According to the analy-sis in Section 4, of the 343 distinct annotations in these collections, the MIREX test set includes pieces by The Beatles, Carole King, Michael Jackson, and Queen.•MIREX10: The RWC popular music database, which consists of  100 Japanese pop tunes, with annotations provided by AIST [7, 21] and by INRIA [3, 23]. The INRIA annotations only give boundaries and were first tested in 2010; the AIST annotations, introduced to MIREX in 2011, also provide section labels. INRIA annotations with segment labels were recently intro-duced [2].•MIREX12: The SALAMI data [25], introduced to MIREX 2012, and consisting of  approximately 859 pieces, over 500 of  them with annotations by two lis-teners [19]. The pieces include popular, classical, world and jazz recordings, publicly available live recordings taken from the Internet Archive, and some pieces bor-rowed from the Isophonics and RWC collections.Each of the collections were produced by a small number of  listeners (fewer than 10 each) annotating the structure they perceived, but differed in their methodology. The Beatles annotations were adapted from musicologist Al-lan Pollack’s descriptions [15]. Many of  the RWC anno-tations benefitted from a beat-tracked grid computed from a click track, and only the “obvious” sections were annotated, meaning there are some unannotated gaps in the descriptions [7]. Most SALAMI pieces were anno-tated by two listeners, and its annotations have separate layers for musical function, similarity at two timescales, and leading instrumentation. Finally, the INRIA annota-tions indicate boundaries according to a more concrete definition of segments [3].2.3  Evaluation metricsFor the SS task, MIREX published 14 of the most com-mon metrics reported in the literature: pairwise retrieval (precision pwp, recall pwr and f-measure pwf), proposed by [9]; over- and under-segmentation scores (SO and SU, respectively), proposed in [10]; Rand index (R), a metric for comparing partitions of  data first used for structural segmentation in the 2009 MIREX task; boundary re-trieval with a specified tolerance of 3 seconds (precision bp3, recall br3 and f-measure bf3) or 0.5 seconds (bp.5, br.5, bf.5); and the median distance from each true to the near-est claimed boundary (mt2c) and vice versa (mc2t). Since the output of  each algorithm is also available [27], it is possible to evaluate the algorithms with metrics not pub-lished by MIREX. We did so for 5 other metrics: average cluster purity (acp) and speaker purity (asp), and their summary metric called the K-measure (K), mentioned by [10] as a potential metric in MIR; and the fragmentation and missed-boundary scores (f and m, respectively) used by [1]. Some of  these metrics evaluate boundary estimation, and the others evaluate the grouping of  sections. Bound-ary estimation measures either penalize  over-segmentation, under-segmentation, or both. Similarly, grouping metrics either penalize the estimation of  spuri-ous similarity relationships, the omission of  true similar-ity relationships, or both. Table 1 summarizes the general purpose of the different metrics. Although each metric is  distinct, we expect the metrics in a single group will agree with each other.3. CORRELATION ANALYSISWith so many metrics we would like to know whether the metrics in fact measure different things. This problem can be posed in two ways: first, do the metrics differ in how they rank the algorithms? And second, do they differ in how they rank the difficulty of analyzing each recording? Since our data (the evaluation metrics) are not nor-mally distributed, we compute Kendall’s τ rather than the Pearson correlation. Consider all pairs of items in two ranked lists; if p is the probability that the lists agree on how to rank a pair, then τ = p – (1 – p) and ranges from τ = 1 for identical rankings to τ = –1 for reversed rankings. With independent lists of rankings, τ is a random variable with mean 0, and we can estimate the precision with which τ has been measured. In all the correlation plots that follow, we use the simple, conservative Bonferonni correction to determine which values for τ are signifi-cantly non-zero. Saying whether a given value of  τ indi-cates a “strong” or “weak” correlation remains a subjec-tive decision; we arbitrarily deem |τ| ≥ 0.8 as a strong correlation (for positive values, this means that two lists rank at least 9 in 10 pairs of  items the same way), |τ| ≥ 0.33 as a weak correlation (2 in 3 pairs are ranked the same), and |τ| < 0.33 as no correlation.3.1  Correlation among metricsThe agreement between the metrics when the algorithms are ranked according to the median grade achieved is shown in Figure 1a. The trio of  evaluation measures not used by MIREX (K, asp, acp) rank the algorithms very similarly to the pairwise retrieval metrics (pwf, pwr, and pwp, r e s p e c t i v e l y ) ,  s u p p o r t i n g  t h e i r  e x c l u s i o n  f r o m  MIREX evaluations for being redundant. We expected each metric to be most similar to other metrics measuring the same type of  error (under-segmentation, over-segmentation, and both together), but instead we find the over-segmentation metric SO is more similar to the sum-mary metrics pwf and K; and the intended summary met-ric R is more similar to the under-segmentation metrics SU, pwp and acp. We can also see whether the ranking of the recordings according to difficulty depends on the metric. Here the results (see Figure 1b) conform more to our expectations: K, asp and acp are again found to be redundant; SU and SO are grouped appropriately with pwr and pwp, but R now resembles an over-segmentation metric.Performing the same analysis with the boundary evaluation metrics, we again find that related metrics are somewhat redundant: bp3, 1-f, and mc2t are highly inter-correlated, as are br.5, br3, 1-m and mt2c (see Figure 2a). Interestingly, bp.5 does not correlate with bp3 or the other over-segmentation metrics; locating boundaries to within 3 seconds and to within 0.5 seconds are perhaps two dis-tinct skills. This discrepancy is not true for br.5 and br3, and hence is probably the cause of  the surprising finding that the boundary f-measure summary metrics (bf3 and bf.5) also do not intercorrelate significantly.When ranking the recordings (Figure 2b), the groups of  metrics (summary, over- and under-segmentation) are each consistent, but the summary metrics are also similar to the over-segmentation ones. Does this indicate that the algorithms are better at boundary precision than recall? In fact, the opposite is the case: mean bp3 and bp.5 were simply consistently worse for all algorithms.Lastly, while there is insufficient space to demonstrate it, the findings of this section were consistent across the datasets, albeit with some variation in significance levels. \nFigure 1a (top). Agreement (Kendall’s τ) between rankings of algorithms (according to median across all recordings) by different labelling metrics. All values of τ ≥ 0.33 are plotted. Shaded backgrounds indicate sig-nificance; bold values indicate strong agreements.Figure 1b (above). Agreement in the ranking of re-cordings by different labelling metrics.Table 1. Summary of the metrics by evaluation purpose.Purpose of the metricBoundary metricsLabelmetricsSummary metricbf3, bf.5pwf, R, KPenalize over-segmentation (spurious boundaries and omitted similarity relationships)bp3, bp.5,1–f, mc2tpwr, SO, aspPenalize under-segmentation (omitted boundaries and spurious similarity relationships)br3, br.5,1–m, mt2cpwp, SU, acp3.2  Correlation with ground truth propertiesThe preceding analysis uses only the evaluation data, not the additional ground truth information made avail-able for the 2012 SS task. With the ground truth we can push the correlation one step further and check whether there are simple properties of  the annotated and estimated descriptions that strongly influence the evaluation met-rics. For example, it could be that the algorithms simply find longer songs to be more difficult to analyze.We tested the correlation between all the preceding metrics and the length of the recording (len), as well as ten other properties. Four are properties of the annotation: number of segments (nsa), number of unique labels (nla), mean segment length (msla) and the number of segments per label (nspla). The next four are the same properties for the estimated description (nse, nle, msle, nsple). We also take the number of extra segments estimated (nse–nsa) as a “direct” over-segmentation measure for boundaries (ob), and likewise the number of extra labels (nle–nla) as the over-segmentation measure for labels (ol).The correlation between these properties reveals an interesting mismatch between the algorithms and the an-notations with regard to the mean segment length. In the annotations, song length correlates significantly with mean segment length (τ = 0.37), but hardly at all with the number of  segments (0.22). The pattern is reversed among the algorithm outputs, where song length barely correlates with mean segment length (0.25) but strongly with the number of  segments (0.72). It appears that the algorithms do not model how listeners tend to identify a number of  sections that is stable across most pieces: while the middle half of  the values of nsa ranges from 7 and 13 segments, the middle values for nse for most algo-rithms range from 11 to 20 segments. The two exceptions are MHRAF and OYZS, for which both msle and nse match the distributions seen in the annotations.This shortcoming of the algorithms is reflected in the strong dependence of  boundary estimation metrics on the mean segment length in the annotation (msla): worse val-ues of  bp, 1-f and mc2t, all of  which punish spurious boundary estimation, are correlated with longer annotated segments.Figure 3 also shows that the label evaluation metrics seem more sensitive to the number of unique labels in the annotation (nla). When nla is high, so are asp and SO (these reflect over-segmentation errors, which are more difficult to make with more fine-grained annotations), while pwp and acp are reduced, indicating a susceptibility to under-segmentation errors. This dependence suggests that algorithms have difficulty estimating the number of unique segment types heard by a listener.4. IDENTIFYING MIREX RECORDINGSIn the 2012 SS task, ground truth and the estimated analysis of  each algorithm were provided for each piece.  \nFigure 2a (top). Agreement in the ranking of algo-rithms by different boundary metrics.Figure 2b (above). Agreement in the ranking of re-cordings by different boundary metrics.\nFigure 3. Agreement in the ranking of recordings be-tween metrics and properties of the annotated and es-timated descriptions.Since the ground truth collections used by MIREX are publically available, we can try to identify the recordings in the evaluation by matching the anonymized ground truth published by MIREX with public ground truth. Before we identify the recordings, we acknowledge that there are advantages to keeping test data private: it is difficult for the designers of algorithms to overfit to hid-den data, which means the same data can be reused in successive years; this is useful since ground truth is ex-pensive to create. However, to learn from an evaluation with private data would require the task moderators to conduct meta-evaluation, which is also costly. Moreover, for the SS task, most of the annotation data is already public, and the Beatles and RWC datasets are already widely distributed—indeed, RWC was designed in order to be distributable at cost, without regard for copyright issues. Hence the main advantages of  private data do not apply to this task. The collections used in the SS task are summarized in Table 2. We downloaded all the publicly available anno-tations for these sources [21–26] to compile a grand pub-lic corpus, as well as all MIREX output and annotation for the SS task [27]. For every anonymous MIREX anno-tation, we searched for public annotations where the lengths of  the pieces differed by less than 15 seconds, and computed the boundary f-measure between them. If the boundary f-measure exceeded 0.99, we assumed a match was found. Checking many of the matches informally, it was clear the match was correct. The number of  annotations in the four test collections is provided in Table 1, as well as the number of  pieces that were positively matched with an existing annotation. The greatest number of  pieces missed were in the SA-LAMI collection. This is to be expected since half  of the SALAMI data remains private. Associating the MIREX results with actual recordings allows us to search for possible commonalities between the recordings that were “easiest” and “hardest” to anno-tate. The piece with the highest median pwf is The Beat-les’ “Her Majesty,” a 30-second song with just one sec-tion. When a song has just one section, any algorithm is guaranteed to get pairwise precision of  1, and the only boundaries in the song are within 3 seconds of its begin-ning and end, ensuring boundary recall of 1 as well. The next-best Beatles song, “I Will”, is an instance where both the states and sequences hypotheses apply well: the repeating sections are relatively homogeneous, but con-tain distinct harmonic sequences. Also, like “Her Maj-esty,” the song is short and contains few sections, reduc-ing the chance of under-segmentation. On the opposite end, the worst overall performance in the MIREX09 dataset is on songs by Queen and Michael Jackson. At the bottom is Jackson’s “They Don’t Care About Us”. The nine algorithms’ output for this song, along with the ground truth and pwf scores, are shown in Figure 4. This song is highly repetitive, especially har-monically, although the sung portions are very distinct from the instrumental sections: intro (before 0:42), outro (after 3:50), and interlude (2:40 to 3:10). Most of the es-timated descriptions discover this overall structure, but fail to differentiate between the similar verses and cho-ruses. Perhaps algorithms will need to employ more than just harmonic features to improve performance on a case like this. On the other hand, the annotation also labels the end of  the song differently from the chorus, even though they sound similar. That is, the annotation conflates musical similarity with musical function, the situation discussed by [13]. To improve performance, MIREX may wish to update its ground truth to reflect primarily musical simi-larity; or, algorithms should aim to characterize the se-mantic labels usually ignored in an analysis. Conspicuously, 17 of the easiest 20 songs (again, with respect to pwf) are Beatles tunes, while only 2 of  the most difficult 20 songs are—the rest being Michael Jackson, Queen and Carole King songs. Taking the median pwf across the algorithms and comparing this value for the 274 annotations identified as one of  these four artists, a Kruskal-Wallis test confirms that the groups differ. A multiple comparison test reveals that pwf is significantly greater for the Beatles group than the three others. The simplest explanation is that the songs by the other artists are simply more challenging to analyze than the bulk of the Beatles catalogue. However, this may be evidence that the community is overlearning on the Beatles dataset, which has been widely distributed and used as a test col-lection for at least 6 years.5. CONCLUSIONWe revisited the 2012 MIREX Structure Segmentation task to better understand the performance of the algo- \nFigure 4. Annotated ground truth and algorithm output for “They Don’t Care About Us” by Michael Jackson. The median pwf achieved by these algorithms was among the lowest in all of MIREX 2012.MIREX datasetDataset con-tentsNumber of piecesNumber of pieces identifiedmrx09Beatles, Queen, Michael Jackson297274mrx10_1RWC Popular100100mrx10_2RWC Popular100100salSALAMI1000674Table 2. Summary of the annotations identified in each corpusrithms and the behaviour of  the evaluation metrics. Using a correlation analysis, we identified the same metrics excluded from MIREX as redundant (K, asp, acp) and one as unstable and biased (R). Thanks to the release of the ground truth and algorithm output with the 2012 MIREX SS task, we were able to investigate the relation-ship between evaluation metrics and simple properties of the annotated and estimated descriptions, identifying the lack of  regularity in the number of segments per song as a hindrance to many submissions. We hope that this investigation serves as a positive example of the kind of learning that can be accomplished through meta-analysis. Other MIREX tasks could benefit from the release of algorithm output data and information about the ground truth. While the MIR community must weigh the value of  open evaluations with the cost of  new datasets, note that it is not necessary to release all the ground truth to benefit a meta-analysis: indeed, this analysis focused mainly on non-identifying parameters of the annotations (Section 3.2) and only a few high- and low-performing songs.6. ACKNOWLEDGEMENTSThis research was supported in part by the Social Sci-ences and Humanities Research Council of  Canada and a QMUL EPSRC Doctoral Training Account studentship.7. REFERENCES[1] Abdallah, S., Noland, K., Sandler, M., Casey, M., & Rhodes, C. 2005. Theory and evaluation of  a Bayesian music structure extractor. In Proc. ISMIR. London, UK. 420–5. [2] Bimbot, F., Deruty, E., Sargent, G., & Vincent, E. 2012. Semiotic structure labeling of  music pieces: Concepts, methods and annotation conventions. In Proc. ISMIR. Porto, Portugal. 235–40.[3] Bimbot, F., Le Blouch, O., Sargent, G., & Vincent, E. 2010. Decomposition into autonomous and comparable blocks: A structural description of  music pieces. In Proc. ISMIR. 189–94).[4] Buckley, C. and E. M. V oorhees. 2005. Retrieval system evaluation. In TREC: Experiment and Evaluation in Information Retrieval. E. M. V oorhees and D. K. Harman, eds. MIT Press, Cambridge, MA.[5] Downie, J. S. 2008. The music information retrieval evaluation exchange (2005-2007): A window into music information retrieval research. Acoustical Science and Technology 29, 247–55.[6] Ehmann, A. F., M. Bay, J. S. Downie, I. Fujinaga, and D. De Roure. 2011. Music structure segmentation algorithm evaluation: Expanding on MIREX 2010 analyses and datasets. In Proc. ISMIR. Miami, FL. 561–6.[7] Goto, M. 2006. AIST  annotation for the RWC music database. In Proc. ISMIR. Victoria, Canada. 359–60.[8] Kaiser, F., Sikora, T., & Peeters, G. 2012. MIREX 2012 - Music Structural Segmentation Task: IrcamStructure submission. In Late-breaking and demo session, ISMIR.[9] L e v y ,  M .  &  S a n d l e r ,  M .  2 0 0 8 .  S t r u c t u r a l  segmentation of  musical audio by constrained clustering. IEEE Transactions on Audio, Speech, and Language Processing, 16 (2). 318–26.[10] L u k a s h e v i c h ,  H .  2 0 0 8 .  T o w a r d s  q u a n t i t a t i v e  measures of evaluating song segmentation. In Proc. ISMIR. Philadelphia, PA. 375–80.[11] Martin, B., Hanna, P., Robine, M., & Ferraro, P. 2012. Structural analysis of harmonic features using string matching techniques. In Late-breaking and demo session, ISMIR.[12] Peeters, G. 2004. Deriving musical structures from signal analysis for music audio summary generation: “sequence” and “state” approach. In G. Goos, J. Hartmanis, and J. van Leeuwen (Eds.), Computer Music Modeling and Retrieval, 2771: 169–85. Springer Berlin / Heidelberg.[13] Peeters, G. & Deruty, E. 2009. Is music structure annotation multi-dimensional? A proposal for robust local music annotation. In Proceedings of the International Workshop on Learning the Semantics of Audio Signals. Graz, Austria. 75–90. [14] Peeters, G., Urbano, J., & Jones, G. J. F. 2012. Notes from the ISMIR 2012 late-breaking session on evaluation in music information retrieval. In Late-breaking and demo session, ISMIR.[15] Pollack, A. 2001. “Notes on ... Series.” http://www.icce.rug.nl/~soundscapes/DATABASES/AWP/awp-notes_on.shtml[16] Sargent, G., Bimbot, F., & Vincent, E. 2012. A music structure inference algorithm based on morphological analysis. In Late-breaking and demo session, ISMIR.[17] Serrà, J., Müller, M., Grosche, P., & Arcos, J. L. 2012a. The importance of detecting boundaries in music structure annotation. In Late-breaking and demo session, ISMIR.[18] Serrà, J., Müller, M., Grosche, P., & Arcos, J. L. 2012b. Unsupervised detection of  music boundaries by time series structure features. In Proceedings of the AAAI International Conference on Artificial Intelligence, Toronto, Ontario, Canada. 1613–9.[19] Smith, J. B. L., Burgoyne, J. A., Fujinaga, I., De Roure, D., & Downie, J. S. 2011. Design and creation of a large-scale database of  structural annotations. In Proc. ISMIR. Miami, FL. 555–60.[20] U r b a n o ,  J .  2 0 1 1 .  I n f o r m a t i o n  r e t r i e v a l  m e t a -evaluation: Challenges and opportunities in the music domain. In Proc. ISMIR. Miami, FL. 609–14.[21] AIST: http://staff.aist.go.jp/m.goto/RWC-MDB/AIST-Annotation/[22]  EP: http://www.ifs.tuwien.ac.at/mir/audiosegmentation.html#anchor_corpus[23] INRIA: http://musicdata.gforge.inria.fr/structureAnnotation.html [24] Isophonics: http://www.isophonics.net/content/reference-annotations[25] SALAMI: http://ddmal.music.mcgill.ca/research/salami/annotations[26] TUT: http://www.cs.tut.fi/sgn/arg/paulus/structure.html[27] MIREX data: http://nema.lis.illinois.edu/nema_out/mirex2012/results/struct/"
    },
    {
        "title": "Do Online Social Tags Predict Perceived or Induced Emotional Responses to Music?",
        "author": [
            "Yading Song",
            "Simon Dixon",
            "Marcus T. Pearce",
            "Andrea R. Halpern"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415212",
        "url": "https://doi.org/10.5281/zenodo.1415212",
        "ee": "https://zenodo.org/records/1415212/files/SongDPH13.pdf",
        "abstract": "Music provides a powerful means of communication and self-expression. A wealth of research has been performed on the study of music and emotion, including emotion modelling and emotion classification. The emergence of online social tags (OST) has provided highly relevant information for the study of mood, as well as an important impetus for using discrete emotion terms in the study of continuous models of affect. Yet, the extent to which human annotation reveals either perceived emotion or induced emotion remains unknown. 80 musical excerpts were randomly selected from a collection of 2904 songs labelled with the Last.fm tags “happy”, “sad”, “angry” and “relax”. Forty-seven participants provided emotion ratings on the two continuous dimensions of valence and arousal for both perceived and induced emotion. Analysis of variance did not reveal significant differences in ratings between perceived emotion and induced emotion. Moreover, the results indicated that, regardless of the discrete type of emotion experienced, listeners’ ratings of perceived and induced emotion were highly positively correlated. Finally, the emotion tags “happy”, “sad” and “angry” but not “relax” predicted the corresponding experimentally provided emotion categories.",
        "zenodo_id": 1415212,
        "dblp_key": "conf/ismir/SongDPH13",
        "keywords": [
            "Music",
            "communication",
            "self-expression",
            "emotion modelling",
            "emotion classification",
            "online social tags",
            "mood",
            "discrete emotion terms",
            "continuous models of affect",
            "human annotation"
        ],
        "content": "DO ONLINE SOCIAL TAGS PREDICT PERCEIVED OR INDUCED\nEMOTIONAL RESPONSES TO MUSIC?\nYading Song, Simon Dixon, Marcus Pearce\nCentre for Digital Music\nQueen Mary University of London\nfirstname.lastname@eecs.qmul.ac.ukAndrea Halpern\nDepartment of Psychology\nBucknell University\nahalpern@bucknell.edu\nABSTRACT\nMusic provides a powerful means of communication\nand self-expression. A wealth of research has been per-\nformed on the study of music and emotion, including emo-\ntion modelling and emotion classiﬁcation. The emergence\nof online social tags (OST) has provided highly relevant\ninformation for the study of mood, as well as an impor-\ntant impetus for using discrete emotion terms in the study\nof continuous models of affect. Yet, the extent to which\nhuman annotation reveals either perceived emotion or in-\nduced emotion remains unknown. 80 musical excerpts were\nrandomly selected from a collection of 2904 songs labelled\nwith the Last.fm tags “happy”, “sad”, “angry” and “re-\nlax”. Forty-seven participants provided emotion ratings\non the two continuous dimensions of valence and arousal\nfor both perceived and induced emotion. Analysis of vari-\nance did not reveal signiﬁcant differences in ratings be-\ntween perceived emotion and induced emotion. Moreover,\nthe results indicated that, regardless of the discrete type of\nemotion experienced, listeners’ ratings of perceived and in-\nduced emotion were highly positively correlated. Finally,\nthe emotion tags “happy”, “sad” and “angry” but not “re-\nlax” predicted the corresponding experimentally provided\nemotion categories.\n1. INTRODUCTION\nMusic provides a powerful means of conveying and evok-\ning feeling, and has attracted increasingly signiﬁcant re-\nsearch interest in the past decades [5]. People report that\ntheir primary motivation for listening to music lies in its\nemotional effects [10]. A study of recreational activities\n(watching television, listening to music, reading books,\nand watching movies) indicated that people listen to music\nmore often than any of the other activities [18]. The abil-\nity of identifying emotional content is established at very\nearly age; a 5-year-old child can discriminate happiness\nand sadness by tempo and mode [2]. Although a wealth\nof research has been performed on the study of music and\nemotion [5], many problems remain unsolved.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.In the study of emotion and music listening, one funda-\nmental distinction is between induced emotion (also known\nas felt emotion), which is the emotion experienced by the\nlistener, and perceived emotion (also known as expressed\nemotion), which is the emotion recognised in the music\n[8]. However, separating induced emotion from perceived\nemotion is not always straightforward. Previous studies\nhave suggested that music induces emotions similar to the\nemotional quality perceived in the music. Generally, in-\nduced emotion is more subjective and perceived emotion\ntends to be more objective [7,11] and it was found that the\nagreement of listener ratings of joy and sadness was higher\nthan those for anger and fear [22].\nTo describe musical emotions, two well-known and dom-\ninant models have arisen: the discrete model (also known\nas the categorical approach) and the dimensional model\n(also known as the valence-arousal model). The discrete,\nor categorical, model describes all emotions as being de-\nrived from a limited number of universal and innate basic\nemotions such as anger, happiness, sadness and fear [6,17].\nIn contrast, the dimensional model considers all affective\nterms as arising from independent neurophysiological sys-\ntems: one related to valence (a pleasure-displeasure con-\ntinuum) and the other to arousal (activation-deactivation)\n[19]. On one hand, the dimensional model has been criti-\ncised for its lack of differentiation when it comes to emo-\ntions that are close neighbours in the valence-activation\nspace, and for the limitation that participants can express\ntheir responses in terms of only two dimensions. On the\nother hand, the discrete model is criticised as being inad-\nequate to describe the richness of emotional effects. Both\ntheoretical frameworks, the categorical model and dimen-\nsional model, have however received empirical support [12,\n24], and a comparison of the two models was presented by\nEerola [4]. In recent years, a novel music-speciﬁc model\nderived from the Geneva Emotion Music Scale (GEMS),\nhas been developed for speciﬁcally music-induced emo-\ntions, which consists of 9 emotional scales - wonder, tran-\nscendence, tenderness, nostalgia, peacefulness, power, joy-\nful activation, tension and sadness [28].\nAlongside the emergence of music discovery websites\nsuch as Last.FM1in the past decade, social tags have re-\nceived increasing interest for the study of music and emo-\ntion [3, 15, 25]. Social tags are words or groups of words\n1http://www.last.fmsupplied by a community of internet users. They are more\nand more commonly used to aid navigation through large\nmedia collections [27], allowing users to get a sense of\nwhat qualities characterise a song at a glance [9]. Com-\npared with traditional human annotation by experts, se-\nmantic tags provide large-scale, cost-efﬁcient, rich and eas-\nily accessible source of metadata [23]. In addition, the\ninformation they provide is highly relevant to music in-\nformation retrieval, including genre, mood and instrument,\nwhich account for 70% of the tags [13].\nThough the use of social tags is a powerful tool which\ncan assist searching and the exploration of music [14], sev-\neral problems with tags have been identiﬁed, such as the\n“cold start” problem (new or unknown music has no tags),\nnoise, malicious tagging, and bias towards popular artists\nor genres [13]. There are a number of incentives and moti-\nvations for tagging, such as to aid memory, provide context\nfor task organisation, social signalling, social contribution,\nplay and competition, and opinion expression [1]. How-\never, we know very little about the criteria on which tag-\nging is based.\nTo our knowledge, the two facets of emotion commu-\nnication (perceived emotion and induced emotion) in mu-\nsic have rarely been studied in combination with semantic\ntags. The purpose of this paper is to explore the association\nbetween human-annotated tags and emotional judgements\nin perceived emotion and induced emotion based on the di-\nmensional model. This study also helps the mapping and\nmodelling of mood tags on the valence-arousal space. In\nthis paper, the following research questions are examined:\n(1) How do induced emotion and perceived emotion differ\nfrom each other in the ratings of valence and arousal for a\n2-dimensional model of emotion? (2) How well do seman-\ntic emotional tags reﬂect listeners’ perceived emotion and\ninduced emotion? (3) To what degree can the emotional\ntags be used to select stimuli for the study of music and\nemotion?\n2. METHOD\n2.1 Participants\nForty-seven English-speaking participants (male: 20; fe-\nmale: 27) took part in this study. They were recruited\nthrough various email lists (e.g. school lists, professional\nlists, and social media), and had ages ranging from 15 to\n54 years (age <18: 1; age 18-24: 22; age 25-34: 21;\nage 35-44: 1; age: 45-54: 2) with various educational\n(e.g. undergraduate, postgraduate), cultural (e.g. British,\nAmerican, French, Chinese, Canadian, Italian, Greek, Sri\nLankan) and musical training backgrounds (musician and\nnon-musician).\nTo assess the participants’ musical expertise, the Gold-\nsmiths Musical Sophistication Index questionnaire (GOLD-\nMSI) was given [16] (see section 2.3). Participants’ musi-\ncal training (life history of formal musical training) was\ncalculated using a provided template2giving a scale from\n9 to 63 (no formal training to formal training). A summary\nof the responses can be found in Table 1.\n2http://www.gold.ac.uk/music-mind-brain/gold-msi/Skill Min Max Median SD\nMusical Training 17 42 29 6.2915\nTable 1 . Summary of musical training\n2.2 Stimuli\nThe stimuli were selected from a collection of 2904 ex-\ncerpts retrieved from Last.FM and 7Digital3which have\nbeen used previously in music and emotion studies [20,21].\nEach excerpt had been tagged on Last.FM with one of\nthe four words “ happy ”, “sad”, “angry ” and “ relax ”. We\nrandomly chose a total of 80 excerpts from these four cat-\negories (n=20 from each category). The musical excerpts\nranged from recent releases back to 1960s, and covered a\nrange of Western popular music styles such as pop, rock,\ncountry, metal and instrumental. Each excerpt was either\n30 seconds or 60 seconds long (as provided by 7Digital),\nand it was played from a standard mp3 format ﬁle (bitrate:\n128 kbps or 64 kbps; sample rate: 22050 kHz or 44100\nkHz). This 80-excerpt dataset will be made available4, to\nenable further studies with this data and comparisons with\nthe current work.\nIn order to minimise the effect of song sequence and rat-\ning conditions (perceived and induced emotion), four dif-\nferent list conditions were constructed. The order of pre-\nsentation of the two rating conditions and two song blocks\n(m=40, 10 for each emotion category) was counterbalanced\nacross subjects. The songs in each block were randomly\ndistributed across participants [26]. See Table 2 for the\ngroup allocation. To remind the subjects of two different\nrating conditions, the questions were highlighted in differ-\nent colours (blue and red) and they were also counterbal-\nanced across groups.\nGroup Block 1 Block 2\nGroup 1 Induced emotion Perceived emotion\nGroup 2 Perceived emotion Induced emotion\nBlock 2 Block 1\nGroup 3 Induced emotion Perceived emotion\nGroup 4 Perceived emotion Induced emotion\nTable 2 . Group allocation among participants\n2.3 Procedure\nThe study was approved by the Research Ethics Commit-\ntee (REF: QMREC1019). The listening test was conducted\nonline5; participants only required internet access and a\nspeaker or headphones for this experiment. First, the par-\nticipants were asked to read the instruction page:\n1. Listen to the songs (they will last either 30 or 60\nseconds)\n3http://www.7digital.com/\n4https://code.soundsoftware.ac.uk/projects/emotion-recognition\n5http://isophonics.net/dimensional/test/2. After listening, please rate each piece on two dimen-\nsions: Valence (happy-sad continuum) and Arousal\n(excited-relaxed continuum)\n3. For each track, you may click the “stop” button of\nthe audio player if required\n4. Be careful, do not press too quickly, since you can\nonly listen to each song once\n5. Please answer all the questions; the test will take\nabout 40 mins to complete\nThe participants ﬁlled in a demographic form including\nname, age, gender, “type of music they are most familiar\nwith”, nationality, and “music culture they grew up with”\nas well as a selected Goldsmiths Musical Sophistication In-\ndex (GOLD-MSI) questionnaire (9 questions) to measure\nparticipants’ level of musical training. They responded to\neach excerpt (n=10 per page) and rated them on two di-\nmensions, valence (sad to happy) and arousal (relaxed to\nexcited). According to the session they chose, a pop-up\nwindow would appear reminding them, based on which\ncondition they needed to answer, “ How would you describe\nthe emotional content of the music itself? (expressed emo-\ntion) ” , and “ What emotion do you feel in response to the\nmusic? (felt emotion) ”. The whole experiment lasted about\n40 minutes without any breaks. However, the participants\nwere able to stop whenever they wanted.\n3. RESULTS\nThe data analysis was conducted using the Matlab 2012\nStatistics Toolbox. The results were aggregated across peo-\nple for song level analysis or aggregated across item for\nindividual level analysis.\n3.1 Song level analysis\n3.1.1 Comparison of valence and arousal ratings for\nperceived and induced emotion\nTo understand the effects of rating conditions (perceived\nemotion and induced emotion) and emotions (happy, sad,\nrelax6and angry) on the ratings of valence and arousal,\na two-way analysis of variance (ANOV A) was conducted.\nNo signiﬁcant difference was found for the ratings for the\ntwo conditions and the interaction of emotion and condi-\ntion. On the other hand, the emotion tag had a signiﬁcant\neffect on the ratings for valence and arousal (see Table 3).\n3.1.2 Correlation between the ratings for perceived\nemotion and induced emotion\nSection 3.1.1 showed that there was no signiﬁcant differ-\nence between ratings for perceived and induced emotion.\nCorrelation analyses were performed to study the relation-\nship between valence (respectively arousal) ratings for per-\nceived and induced emotion. Regardless of the discrete\nemotion tag, the listeners’ valence and arousal ratings were\nhighly positively correlated between perceived and induced\nemotion cases (valence: r= 0.9357, p<0.0001; arousal: r\n6The term “relax” was used in the data collection, which represented\nthe emotion “relaxed”Valence\nSource SS df MS F Prob >F\nCondition 1.93 1 1.93 1.14 0.29\nEmotion Tag 163.26 3 54.42 31.97 0\nInteraction 0.27 3 0.09 0.05 0.98\nArousal\nCondition 0 1 0.0002 0 0.99\nEmotion Tag 231.55 3 77.18 25.73 0\nInteraction 0.99 3 0.33 0.11 0.95\nNote: SS - the sums of squares, df - degrees-of-freedom\nMS - mean squares (SS/df), F - F statistics\nTable 3 . Two-way analysis of variance on the ratings of\nvalence and arousal\n= 0.9590, p<0.0001). The correlation of valence is shown\nin Figure 1.\nFigure 1 . Correlation between the valence ratings for in-\nduced and perceived emotion\n3.1.3 Correspondence between ratings and tags\nThe four emotion tags were chosen such that each occupies\na unique quadrant of the valence-arousal plane, as shown\nin Figure 2. Considering that these four basic emotions are\nwidely accepted across different cultures, we are able to as-\nsess the agreement between tags and participant ratings ac-\ncording to the extent that participants’ ratings correspond\nwith the quadrant belonging to the song’s tag.\nFor each song, the average of participants’ valence and\narousal ratings were calculated for both perceived and in-\nduced emotion, to give a centroid for each song. The quad-\nrant of this song centroid was then compared with the ex-\npected quadrant based on the emotion tag associated with\nthe song. The fraction of songs for which the centroid\nquadrant corresponded with that of the tag is shown in\nTable 4. In addition, the standard deviations (SD) of the\nvalence and arousal ratings for songs in each emotion cat-\negory were calculated.Figure 2 . Valence-Arousal model showing the quadrants\nof the four emotion tags used in this experiment.\nThe results are shown with the highest values shown in\nbold in Table 4. Apart from the excerpts tagged with “re-\nlax”, more than 55% of the average valence and arousal\nratings lie in the song’s corresponding tag quadrant. Fewer\nthan 20% of mean ratings for songs tagged “relax” were lo-\ncated in the correct quadrant. Moreover, the high standard\ndeviation of valence-arousal ratings for both perceived and\ninduced emotion were found, indicating that the ratings of\n“relax” excerpts were not consistent across songs. For all\nemotion tag categories, and for both perceived and induced\nemotion, it was found that valence ratings were more con-\nsistent than arousal ratings.\nHappy Sad Relax Angry\nInduced emotion\nRating=Tag 0.70 0.70 0.20 0.60\nValence SD 1.05 1.41 1.81 1.20\nArousal SD 1.56 1.45 2.46 1.82\nPerceived emotion\nRating=Tag 0.80 0.65 0.15 0.55\nValence SD 1.10 1.25 1.47 0.93\nArousal SD 1.34 1.32 2.04 1.56\nTable 4 . Agreement of valence-arousal ratings with tag\nquadrants, and spread of per-song ratings.\nFigure 3 presents the average participant ratings of each\nsong using the 2-dimensional model of emotion. The ﬁg-\nure shows a high level of agreement between ratings for\nperceived and induced emotion (note that each participant\nrated each song for only one of perceived and induced\nemotion), compared to the spread of songs corresponding\nto an emotion tag.\n3.1.4 Spread of participants’ ratings\nFor each song, the spread of participant ratings was com-\nputed as the mean distance between points (centroid of\nthe averaged ratings across participants and the ratings of\neach participant) in the valence-arousal plane. These val-ues were then averaged across songs labelled with each\nemotion tag, as shown in Table 5. The spread of ratings\nfor perceived and induced emotion were similar; likewise\nthe spreads for each emotion category were similar, with a\nslightly higher spread found for “sad” songs.\nHappy Sad Relax Angry\nInduced emotion 2.59 2.75 2.45 2.65\nPerceived Emotion 2.56 2.76 2.54 2.52\nTable 5 . The spread of the participant ratings across songs\nin each emotion category.\n3.2 Individual level analysis\n3.2.1 Individual agreement of participant ratings with the\nemotion tags\nThe analysis and results in section 3.1.3 were based on\nratings for each excerpt that were averaged across partici-\npants. To analyse the relationship between individual rat-\nings and emotion tags, we compute the fraction of ratings\nthat are in the same quadrant as the emotion tag for the\nsong, and compare this with the baseline of 25% for ran-\ndom choice of quadrants. The results are shown in Table\n6. Chi-square tests were used to test whether the agree-\nment with the emotion tag was signiﬁcantly above chance\nlevel.\nHappy Sad Relax Angry\nInduced emotion ***0.56 *0.47 0.25 **0.49\nPerceived emotion **0.57 0.42 0.25 **0.48\nTable 6 . Agreement of participant ratings with the quad-\nrant of the emotion tag for each category. Values above\nchance level according to \u001f2tests are shown for the fol-\nlowing signiﬁcance levels: * p < 0:05; **p < 0:01;\n***p<0:001.\nIt was found that the songs labelled with “happy” had\nthe highest agreement 55% ( p<0.001). Signiﬁcant results\nwere also found for tags “sad” ( p<0.05, induced emotion)\nand “angry” ( p<0.01). However, the agreement of partic-\nipant ratings and the expected quadrant for songs labelled\nwith “relax” was at the level of chance.\n3.2.2 Agreement among participants\nSince the tags associated with a song are not an absolute\nground truth, but are also generated by users, under un-\nknown conditions, we also look at the agreement of rating\nquadrants among the participants. The level of participant\nagreement is deﬁned as the fraction of participants whose\nratings are in the quadrant with the highest number of par-\nticipant ratings. This value has as a lower bound the agree-\nment with the tag quadrant, but can be higher if a greater\nnumber of participants agree on a quadrant other than that\nof the tag. The agreement of the individual dimensions\nof valence and arousal was also computed. The WilcoxonFigure 3 . Map showing the average valence-arousal ratings for each excerpt. Each pane shows the ratings and centroid for\none emotion tag. For each song, the points representing average ratings for perceived and induced emotion are connected\nby a line segment.\nSigned Rank test was used to compare the difference of\nagreement for both perceived and induced emotion. Re-\nsults are shown in Table 7. In comparison with Table 6,\nthe levels of participant agreement are higher, suggesting\nthat at least some of the tags do not correspond either with\nparticipants’ perceived or induced emotion.\nMean SD P-value\nInduced emotion 0.60 0.140.0032Perceived emotion 0.57 0.15\nInduced Valence 0.68 0.140.0211Perceived Valence 0.66 0.15\nInduced Arousal 0.77 0.140.0001Perceived Arousal 0.73 0.16\nTable 7 . Participant agreement among themselves\n3.2.3 The spread of song ratings for each emotion\ncategory\nSimilar to section 3.1.4, the spread of the song ratings for\neach person was computed for each emotion tag. Once\nagain, the highest values were found for the tag “relax”,\nbecause the responses for songs in this category vary more\nthan for songs in other categories. This also helps to ex-\nplain the low agreement with tags in section 6.\nHappy Sad Relax Angry\nInduced emotion 2.67 2.82 3.49 2.82\nPerceived emotion 2.67 2.61 3.15 2.55\nTable 8 . The spread of the song ratings for each emotion\ncategory.4. DISCUSSION\nThe main purpose of the paper was to investigate the asso-\nciations between categorical emotion tags (happy, sad, re-\nlax and angry) and musical judgements of perceived emo-\ntion and induced emotion using a 2-dimensional (valence-\narousal) model of emotion. First, two-way analysis of vari-\nance was used to test the effects of rating conditions and\nemotion categories on the ratings of valence and arousal.\nNo signiﬁcant difference was found between rating condi-\ntions, perceived and induced emotion, nor for the interac-\ntion of emotion categories and conditions. A correlation\nanalysis on the ratings for induced and perceived emotion\nshowed strong positive correlations between perceived and\ninduced emotion, for ratings of both valence ( p<0.0001\nr=0.94) and arousal ( p<0.0001 r=0.96). This suggests that\nlisteners will typically feel the emotions expressed by the\nsong, so if a song expresses happiness, it is likely that the\nlistener will feel happy as well [21].\nSecond, we studied the reliability of the mean value\namong participants’ ratings for predicting the emotion tag\n(happy, sad, relax and angry). The average valence and\narousal ratings for both induced and perceived emotion\nwere consistent with more than 55% of the happy, sad and\nangry tagged songs. However, the ratings of the songs la-\nbelled with “relax” tended to be less consistent, and did\nnot agree with the quadrant of the tag. This can be seen\nin the mapping of songs to the valence-arousal plane (Fig-\nure 3), which shows the average ratings for songs labelled\n“relax” spread across happy, sad and relax quadrants. In-\nterestingly, although 60% of the songs labelled with “sad”\nlie in correct quadrant, the spread of participants’ ratings\nwas higher than for the other emotion categories.\nThird, we analysed the relationship between emotion\ntags and individual participant ratings. The results werevery similar to the average ratings in section 3.1.3; the\nsongs labelled with “happy”, “sad” and “angry” had rat-\nings in the corresponding quadrants of the valence-arousal\nplane at a level that was signiﬁcantly above chance. For\nsongs tagged “relax”, however, the agreement of ratings\nwith the positive-valence, negative-arousal quadrant was\nat the level of chance for both perceived and induced emo-\ntion. Further, the spread of the ratings was also higher than\nfor the other categories. Comparing these four tags, regard-\nless of song or person, the excerpts tagged with “happy”\nare most likely to produce responses in the corresponding\nquadrant of the valence-arousal plane.\nFinally, the agreement among participants were com-\nputed. Gabrielsson noted a higher agreement among lis-\nteners for perceived emotion, due to its objectivity, over\ninduced emotion [8], and this was conﬁrmed in a study us-\ning the categorical model and the same musical excerpts as\nthe present study [21]. In this paper, we found that agree-\nment among participants was signiﬁcantly higher for in-\nduced emotion than for perceived emotion ( p<0.01). How-\never, we concede that the overall levels of agreement (60%\nfor induced emotion and 57% for perceived emotion) are\nnot high. As a partial explanation of discrepancies between\nemotion tags and user ratings, we observed lower levels of\nagreement among listeners for songs tagged “relax” than\nfor the classes which had higher agreement between tags\nand participant ratings.\nIn future studies we plan to investigate the different re-\nsponses in perceived and induced emotion, and compare\nthe use of the categorical and dimensional models of emo-\ntion.\n5. ACKNOWLEDGEMENTS\nWe acknowledge the support of the China Scholarship Coun-\ncil. We would like to thank the listeners and reviewers for\ntheir participation and comments.\n6. REFERENCES\n[1] M. Ames and M. Naaman. Why we tag: motivations for an-\nnotation in mobile and online media. In SIGCHI conference\non Human Factors in Computing Systems , 2007.\n[2] S. Dalla Bella, I. Peretz, L. Rousseau, N. Gosselin. A devel-\nopmental study of the affective value of tempo and mode in\nmusic. Cognition , 80(3):B1–10, 2001.\n[3] D. Eck, P. Lamere, T. Bertin-Mahieux, S. Green. Automatic\ngeneration of social tags for music recommendation. Ad-\nvances in Neural Information Processing Systems , 2007.\n[4] T. Eerola and J.K. Vuoskoski. A comparison of the discrete\nand dimensional models of emotion in music. Psychology of\nMusic , 39(1):18–49, 2010.\n[5] T. Eerola and J.K. Vuoskoski. A review of music and emo-\ntion studies: approaches, emotion models, and stimuli. Music\nPerception , 30(3):307–340, 2013.\n[6] P. Ekman. An argument for basic emotions. Cognition and\nEmotion , 6:169–200, 1992.\n[7] P. Evans and E. Schubert. Relationships between expressed\nand felt emotions in music. Musicae Scientiae , 12(1):75–99,\n2008.[8] A. Gabrielsson. Emotion perceived and emotion felt: same\nor different? Musicae Scientiae , 123–147, 2002.\n[9] M. Hoffman, D. Blei, et al. Easy as CBA: A simple proba-\nbilistic model for tagging music. In 10th International Soci-\nety for Music Information Retrieval , 2009.\n[10] P.N. Juslin and P. Laukka. Expression, perception, and induc-\ntion of musical emotions. Journal of New Music Research ,\n33(3):217–238, 2004.\n[11] K. Kallinen and N. Ravaja. Emotion perceived and emotion\nfelt: Same and different. Musicae Scientiae , 191–213, 2006.\n[12] G. Kreutz, U. Ott, D. Teichmann, P. Osawa, D. Vaitl. Using\nmusic to induce emotions: Inﬂuences of musical preference\nand absorption. Psychology of Music , 36(1):101–126, 2007.\n[13] P. Lamere. Social tagging and music information retrieval.\nJournal of New Music Research , 37(2):101–114, 2008.\n[14] M. Levy and M. Sandler. A semantic space for music derived\nfrom social tags. In 8th International Society for Music In-\nformation Retrieval , 2007.\n[15] M. Levy and M. Sandler. Music information retrieval us-\ning social tags and audio. IEEE Transactions on Multimedia ,\n11(3):383–395, 2009.\n[16] D. M ¨ullensiefen, B. Gingras, L. Stewart. Goldsmiths Musical\nSophistication Index (Gold-MSI) Technical report , 2012.\n[17] J. Panksepp. Affective neuroscience: The foundation of hu-\nman and animal emotions. Oxford University Press , 1998.\n[18] P.J. Rentfrow and S.D. Gosling. The Do Re Mi’s of every-\nday life: The structure and personality correlates of music\npreferences. Journal of Personality and Social Psychology ,\n84(6):1236–1256, 2003.\n[19] J.A. Russell. A circumplex model of affect. Journal of Per-\nsonality and Social Psychology , 39(6):1161–1178, 1980.\n[20] Y . Song, S. Dixon, M. Pearce. Evaluation of musical features\nfor emotion classiﬁcation. In 13th International Society for\nMusic Information Retrieval , 2012.\n[21] Y . Song, S. Dixon, M. Pearce, G. Fazekas. Using tags to se-\nlect stimuli in the study of music and emotion. In 3rd Inter-\nnational Conference on Music & Emotion , 2013.\n[22] M. Terwogt and F. Van Grinsven. Musical expression of\nmoodstates. Psychology of Music , 90–109, 1991.\n[23] D. Turnbull, L. Barrington, G. Lanckriet. Five approaches\nto collecting tags for music. In 9th International Society for\nMusic Information Retrieval , 2008.\n[24] S. Vieillard, I. Peretz, et al. Happy, sad, scary and peace-\nful musical excerpts for research on emotions. Cognition &\nEmotion , 22(4):720–752, 2008.\n[25] D. Wang, T. Li, M. Ogihara. Tags better than audio features?\nthe effect of joint use of tags and audio content features for\nartistic style clustering. In 11th International Society for Mu-\nsic Information Retrieval , 2010.\n[26] N. Welch and J.H. Krantz. The World-Wide Web as a\nmedium for psychoacoustical demonstrations and experi-\nments: Experience and results. Behavior Research Methods,\nInstruments,& Computers , 28(2):192–196, 1996.\n[27] X. Wu, L. Zhang, Y . Yu. Exploring social annotations for\nthe semantic web. In 15th International Conference on World\nWide Web , 2006.\n[28] M. Zentner, D. Grandjean, et al. Emotions evoked by the\nsound of music: characterization, classiﬁcation, and mea-\nsurement. Emotion (Washington, D.C.) , 8(4):494–521, 2008."
    },
    {
        "title": "Incremental Visualization of Growing Music Collections.",
        "author": [
            "Sebastian Stober",
            "Thomas Low",
            "Tatiana Gossen",
            "Andreas Nürnberger"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415110",
        "url": "https://doi.org/10.5281/zenodo.1415110",
        "ee": "https://zenodo.org/records/1415110/files/StoberLGN13.pdf",
        "abstract": "Map-based visualizations – sometimes also called projections – are a popular means for exploring music collections. But how useful are they if the collection is not static but grows over time? Ideally, a map that a user is already familiar with should be altered as little as possible and only as much as necessary to reflect the changes of the underlying collection. This paper demonstrates to what extent existing approaches are able to incrementally integrate new songs into existing maps and discusses their technical limitations. To this end, Growing Self-Organizing Maps, (Landmark) Multidimensional Scaling, Stochastic Neighbor Embedding, and the Neighbor Retrieval Visualizer are considered. The different algorithms are experimentally compared based on objective quality measurements as well as in a user study with an interactive user interface. In the experiments, the well-known Beatles corpus comprising the 180 songs from the twelve official albums is used – adding one album at a time to the collection.",
        "zenodo_id": 1415110,
        "dblp_key": "conf/ismir/StoberLGN13",
        "keywords": [
            "Growing Self-Organizing Maps",
            "Landmark Multidimensional Scaling",
            "Stochastic Neighbor Embedding",
            "Neighbor Retrieval Visualizer",
            "Incrementally integrate new songs",
            "Understand changes over time",
            "Interactive user interface",
            "Objective quality measurements",
            "User study",
            "Beatles corpus"
        ],
        "content": "INCREMENTAL VISUALIZATION OF GROWING MUSIC COLLECTIONS\nSebastian Stober, Thomas Low, Tatiana Gossen & Andreas N ¨urnberger\nData & Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE\nfstober, thomas.low, tatiana.gossen, andreas.nuernberger g@ovgu.de\nABSTRACT\nMap-based visualizations – sometimes also called projec-\ntions – are a popular means for exploring music collec-\ntions. But how useful are they if the collection is not static\nbut grows over time? Ideally, a map that a user is al-\nready familiar with should be altered as little as possible\nand only as much as necessary to reﬂect the changes of the\nunderlying collection. This paper demonstrates to what ex-\ntent existing approaches are able to incrementally integrate\nnew songs into existing maps and discusses their technical\nlimitations. To this end, Growing Self-Organizing Maps,\n(Landmark) Multidimensional Scaling, Stochastic Neigh-\nbor Embedding, and the Neighbor Retrieval Visualizer are\nconsidered. The different algorithms are experimentally\ncompared based on objective quality measurements as well\nas in a user study with an interactive user interface. In\nthe experiments, the well-known Beatles corpus compris-\ning the 180 songs from the twelve ofﬁcial albums is used –\nadding one album at a time to the collection.\n1. INTRODUCTION\nWithin the last decade, two-dimensional maps have be-\ncome a popular means for visualizing music collections.\nBy providing a collection overview which easily allows to\nidentify regions or neighborhoods of similar songs, they\nare especially helpful if users want to explore a collection\nwithout having to formulate an explicit query. There ex-\nists a large variety of approaches to compute maps. The\nmost popular ones in the ﬁeld of Music Information Re-\ntrieval (MIR) are Self-Organizing Maps (SOMs) [14] and\nMultidimensional Scaling (MDS) [15] as well as related\ntechniques such as Principle Component Analysis (PCA)\n[10]. Considerable effort has been made to improve the\nquality and usefulness of the generated maps – e.g., by\nadapting the underlying similarity measure based on user\nfeedback [11, 27], enriching the visualization with land-\nscape features such as islands [5, 13, 21, 22, 24] and moun-\ntain ranges [18,20], or correcting projection errors (caused\nby the inherent dimensionality reduction during map gen-\neration) with an adaptive distortion technique [29].\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.However, to the knowledge of the authors, the problem\nof changing collections has not yet been addressed appro-\npriately. To what extent does a map change when songs\nare added or removed? Could it even be necessary to re-\ncompute the map from scratch? Such questions need to\nbe answered as failing to support collection changes may\nsigniﬁcantly limit the usefulness of a MIR application in\nreal-world scenarios. Here, being able to add songs to an\nexisting map is more important than removal which is of-\nten trivial (at worst leading to blank spaces in the map) and\nan uncommon use case anyways. New songs that are sim-\nilar to existing ones should be embedded in the respective\nneighborhoods. At the same time, the map should also be\nable to deal with changes in music taste (adding songs from\nnew genres) and an increase of musical diversity. Ideally,\na map should be altered as little as possible and only as\nmuch as necessary to reﬂect the changes of the underlying\ncollection. Too abrupt changes in the topology might con-\nfuse the user who over time will get used to the location of\nspeciﬁc regions in the map.\nIn this paper, we compare several popular approaches\nfor generating two-dimensional maps of music collections\nwith respect to their ability to deal with growing collec-\ntions. Section 2 brieﬂy reviews the algorithms covered in\nthe comparison and points out related work. The evalua-\ntion has been two-fold: We conducted objective measure-\nments (Section 3) as well as a user study (Section 4). Sec-\ntion 5 summarizes our ﬁndings and draws conclusions.\n2. LAYOUT ALGORITHMS\n2.1 Multidimensional Scaling (MDS)\nGiven a set of data points, classical MDS [15] ﬁnds an\nembedding in the target space (here R2) that maintains\ntheir distances (or dissimilarities) as far as possible – with-\nout having to know their actual values. This way, it is\nalso well suited to compute a layout for spring- or force-\nbased approaches or as a method for vectorization (Sec-\ntion 2.3). MDS is closely related to PCA [33], which\nprojects data points simply onto the (two) axes of high-\nest variance termed principal components. In contrast to\nSOMs, both are non-parametric approaches that compute\nan optimal solution (with respect to data variance maxi-\nmization and distance preservation respectively) in ﬁxed\npolynomial time. Systems that apply PCA, MDS or similar\nforce-based approaches comprise [1,6,31], the fm4 Sound-\npark [5],MusicBox [16], and SoundBite [17].\nMDS does not support incremental collection changes.Instead, a new map has to be computed every time the col-\nlection grows. Even with little change of the collection, the\nresulting map may look very different because it could be\narbitrarily translated, rotated, and reﬂected without affect-\ning the pairwise distances. In order to remedy this issue,\nwe apply Procrustes analysis [7] to align each newly gen-\nerated map with the previous one. This method allows to\ntransform a set of data points through translation, rotation,\nand uniformly scaling such that it resembles another set as\nclosely as possible.\n2.2 Landmark Multidimensional Scaling\nLandmark MDS as described in [2] is a computationally\nefﬁcient approximation to classical MDS. The general\nidea of this approach is as follows: Given a sample set\nof landmark or pivot objects, an embedding into a low-\ndimensional space is computed for these objects using\nclassical MDS. Each remaining object can then be lo-\ncated within the output space according to its distances\nto the landmarks. Obviously, the quality of the projec-\ntion depends on the choice of the landmarks – especially\nif the landmark sample set is small compared to the size\nof the whole dataset. If the landmarks lie close to a low-\ndimensional subspace (e.g., a line), there is the chance of\nsystematic errors in the projection.\nAs described in [29], Landmark MDS can be applied\nto visualize growing music collections by using the initial\nsongs as landmarks. This way, the position of a song once\nadded to the map never changes. However, the landmark\nset may become less and less representative with increas-\ning collection size and possibly changing music taste. This\nmay have a signiﬁcant effect on the quality of the projec-\ntion as, e.g., already observed when applying Landmark\nMDS for vectorization [28].\n2.3 Growing Self-Organizing Map (GSOM)\nSOMs are commonly applied for structuring data collec-\ntions by clustering similar objects into identical or neigh-\nboring cells of a two-dimensional grid. In the ﬁeld of\nMIR, they have been used in a large number of applications\nlike the Islands of Music [22, 24], the MusicMiner [20] or\nnepTune [13]. A recent overview on SOM-related MIR\npublications is given in [30]. Growing Self-Organizing\nMaps (GSOMs) have the advantage that the structure of\nthe cell grid does not have to be speciﬁed prior to training.\nInstead, they can grow as needed and adapt incrementally\nto changes in the underlying collection whereas other ap-\nproaches may always need to generate a new structuring\nwhen the grid becomes too small. Growing hierarchical\nSOMs have been applied in [3, 23, 25]. In contrast, the\nBeatlesExplorer [27] uses a GSOM that grows by adding\nnew cells at the outer boundary. The same approach is\nused here as a ﬂat structure is desired. For the initializa-\ntion, a small hexagonal grid of 2\u00022cells is chosen. After\na regular training phase with a ﬁxed number of iterations,\nan internal error is computed for each cell as a measure\nof quality. To this end, the pairwise distance of the objects\ncontained in a cluster is used. Unless the maximum error isbelow a threshold which is speciﬁed as stopping criterion,\na new cell is added next to the border cell with the highest\nerror. Here, a very low threshold is used to produce a large\nmap with cells that only contain few songs. In order to\nreduce visual overlapping of songs in the same cell, song\ncoordinates are computed by taking the distance-weighted\nmean of the cell’s coordinates and those of its direct neigh-\nbors. This way, songs are placed slightly off-center.\nSOMs generally require the objects they process to be\nrepresented as vectors, i.e., elements of a vector space. As\nthe feature representation does not adhere to this condi-\ntion, some means of vectorization is required. This issue\nhas been discussed in depth in [28] with the recommenda-\ntion to use MDS for vectorization. For growing collections\nwhere only a fraction of the collection may be available\nfor the initial vectorization, the Landmark MDS approach\nhas to be used whereby the songs of the initial collection\nserve as landmarks. As observed in [28], this may cause a\nsigniﬁcant drop in the nearest neighbor retrieval precision\nif many new songs are added – especially if those are very\ndifferent from the initial ones.\n2.4 Stochastic Neighbor Embedding (SNE)\nRather than trying to maintain pairwise distances like\nMDS, the objective of SNE [9] is to preserve the proba-\nbilities of points being neighbors. To this end, probabil-\nity distributions pjjion how likely it is that the point j is a\nneighbor of point i are deﬁned based on the input space dis-\ntances using a Gaussian neighborhood function with width\n\u001b2\ni. The algorithm then tries to ﬁnd suitable coordinates\nin the output space that lead to (approximately) identical\nprobabilities qjjifor each pair of points. The Kullback-\nLeibler divergence\nDKL(pi;qi) =X\nj6=ipjjilogpjji\nqjji(1)\nbetween the probability distributions pjjiin the input space\nandqjjiin the output space serves as cost function. Starting\nfrom randomly initialized coordinates (close to the origin),\na conjugate gradient method is used for ﬁnding a (locally)\noptimal solution.\nSNE does not support changes in the collections. How-\never, it is possible to replace the random initialization and\ninstead use the coordinates from the current visualization\nfor those points already present. This way, the search for a\nnew solution (incorporating the new points) starts from the\ncurrent one. Whilst this does not give any guarantee that\nthe resulting map resembles the old one, chances are that\nthe difference is small.\n2.5 Neighbor Retrieval Visualizer (NeRV)\nAs shown in [32], SNE focuses on the cost of missing\nsimilar objects and – by using continuous probabilities to\nmodel neighborhood relationships – optimizes a smoothed\nversion of the mean recall,P\niDKL(pi;qi).1NeRV addi-\ntionally takes the mean smoothed precision into account by\n1Thesmoothed mean recall and the smoothed mean precision as in-\ntroduced in [32] are in fact cost functions to be minimized in contrast toconsidering also the cost of retrieving dissimilar objects,P\niDKL(qi;pi). The resulting combined cost function is\nE=\u0015X\niDKL(pi;qi) + (1\u0000\u0015)X\niDKL(qi;pi)(2)\nwhere the parameter \u00152[0;1]controls the trade-off be-\ntween precision and recall. For \u0015= 1, the cost function\nof SNE is obtained. Another important parameter is the\nscale of the neighborhoods, \u001b2\ni, for the computation of the\nprobability distributions pjjiandqjji. Starting with a large\nvalue and reducing it stepwise after each optimization step\nhelps to avoid local minima during the conjugate gradient\noptimization. Like SNE, NeRV does not support collection\nchanges but can be initialized with given coordinates.\n3. VISUALIZATION QUALITY MEASUREMENTS\n3.1 Test Collection: The Beatles Corpus\nFor our experiments, we used the well-studied corpus of\nthe 12 ofﬁcial albums of The Beatles containing 180 songs.\nThe albums were added one-by-one to the collection in\nthe order of their release. Distances between the songs\nwas computed as a simple linear combination over three\ncontent-based features: Using the CoMIRV A framework\n[26], we extracted Gaussian Mixture Models of the Mel\nFrequency Cepstral Coefﬁcients (MFCCs) [19] and the\n“ﬂuctuation patterns” described in [24]. Furthermore, we\nderived chord frequency distributions from the publicly\navailable ground truth annotations [8]. This feature combi-\nnation was chosen to capture common acoustic properties\nas well as to reduce the problem of hubs [4].\n3.2 Quality Measures\n3.2.1 Continuity and Trustworthiness\nA visualization can be considered trustworthy if theknear-\nest neighbors of a point on the display are also neighbors\nin the original space. Following [12], a respective measure\nof trustworthiness can be computed by:\nMtrustworthiness = 1\u0000C(k)NX\ni=1X\nj2Uk(i)(rij\u0000k)(3)\nwhererijis the rank of jin the ordering of the distance\nfromiin the original space, Uk(i)is the set of i’s false\nneighbors in the display, and C(k) = 2=(Nk(2N\u00003k\u0000\n1))is a constant for obtaining values in [0;1].\nAnalogously, the measure of continuity considers the k\nnearest neighbors in the original space and captures how\nwell they are preserved in the visualization [12]:\nMcontinuity = 1\u0000C(k)NX\ni=1X\nj2Vk(i)(^rij\u0000k)(4)\nHere, ^rijis the rank of jin the ordering of the distance\nfromiin the visualization and Vk(i)is the set of i’s true\nneighbors missing in the visualized neighborhood.\nthe traditional deﬁnition of precision and recall in information retrieval.\nFor simple “binary” neighborhood deﬁnitions, the Kullback-Leibler di-\nvergences and the precision-recall measures become equivalent [32].3.2.2 Mean Smoothed Rank-Based Precision and Recall\nAs a second pair of evaluation measures, it is possible to\ndirectly use mean smoothed precision and recall (described\nin Section 2.5) as error measures. However, these two er-\nrors have no upper bound and their scale depends on the\ndataset. Thus, it would only be possible to compare values\nfor the same sub-collection. As data-independent alterna-\ntives, the mean smoothed rank-based precision and recall\nhave been proposed in [32]. The idea is to replace the data-\ndependent distances in the computation of the probability\ndistributions by ranks, i.e., the nearest neighbor gets as-\nsigned a distance of 1, the second closest a distance of 2\nand so on. The worst case scenario of reversed ranks gives\nan upper bound that can be used for normalization so that\nall values lie in [0;1].2\n3.2.3 Mean Position Change\nFinally, a measure was needed to capture the amount of\nchange in the visualization. To this end, we computed the\naverage Euclidean distance between the initial and updated\ncoordinates of all points contained in two consecutive visu-\nalizations. In order to obtain normalized values, the point\nsets were scaled and translated to ﬁt into the unit square.\nThis resulted in a maximal position change ofp\n2for a\nsingle point.\n3.3 Results\nFigure 1 shows the values of the ﬁve quality measures for\neach step of adding another album to the collection. We\nchose neighborhoods of size k= 5for measurements. The\nsame parameter value was also used for the NeRV and SNE\ncost functions. The NeRV parameter \u0015was set to zero to\noptimize precision as contrast to SNE. Applying the MDS\nvectorization method on the initial collection, which only\ncontained the ﬁrst album (“Please Please Me”), yielded 13-\ndimensional real vectors to be processed by the GSOM.\nThis vectorization was used in all measurements. In com-\nparison to using the full dataset, which would result in 143\ndimensions, a small retrieval performance penalty of 1-4%\nwas observed. This matches with the 5% drop in the 10-\nnearest-neighbor retrieval precision reported in [28] for a\n14:166 ratio of landmarks and new songs. The penalty can\nbe expected to increase for more extreme ratios.\nConsidering only the overall visualization quality,\nNeRV clearly and consistently yielded the best results. It\nwas surprisingly only slightly outperformed in continuity\nand recall by SNE, which is speciﬁcally optimizing the lat-\nter measure while producing signiﬁcantly worse results for\ntrustworthiness and precision. In general, all methods did\nsimilarly well in continuity and recall, i.e., the number of\nactual neighbors being misplaced in the visualization (false\nnegatives) was rather low. For trustworthiness and preci-\nsion, the differences were much more obvious indicating\n2The mean smoothed rank-based measures are still error measures,\ni.e., a value of zero indicates optimal performance. To obtain consistency\nwith the traditional precision-recall measures, we additionally transform\nthem by f(x) = 1\u0000xas proposed in [32]. 0.7 0.75 0.8 0.85 0.9 0.95 1\n           continuity\n 0.7 0.75 0.8 0.85 0.9 0.95 1\n           trustworthiness\n 0.7 0.75 0.8 0.85 0.9 0.95 1\n           m.s. rank-b. precision\n 0.7 0.75 0.8 0.85 0.9 0.95 1\n           m.s. rank-b. recall\n 0 0.05 0.1 0.15 0.2 0.25 0.3\n 1  3  5  7  9  11m. position change\nnumber of albums in collectionGSOM\nMDS\nLandmark MDS\nNeRV\nSNEFigure 1 . Measurements of the visualization quality and\nincremental position change. In the top four diagrams,\nhigher values indicate better visualization quality whereas\nsmall values of change are desired in the bottom diagram.\nissues with neighbors in the visualization that did not be-\nlong to the input space neighborhood (false positives). Val-\nues were also less constant here. Especially for Landmark\nMDS, they decreased as more and more songs were added.\nA signiﬁcant drop in precision could be observed when the\neighth album (“Sgt. Pepper’s Lonely Hearts Club Band”)\nwas added to the collection.\nWhilst NeRV generated an excellent visualization in\neach step, the difference between them was often much\nhigher than for the other algorithms. Only the GSOM per-\nformed worse here. This might at least partly be due to the\nGSOM’s grid-based structure, which leads to a kind of po-\nsition quantization. Hence, when a song is re-assigned to\na neighboring cell (the smallest possible position change),\nits position already changes roughly by the grid cell width.\nLandmark MDS was the clear winner here with no posi-tion changes at all, but this came at the cost of degrading\nvisualization quality. MDS appears to offer a good com-\npromise: The amount of change was lower than for the\nother methods (except Landmark MDS) and it visibly de-\ncreased as the fraction of new songs became smaller. At\nthe same time, the recall and continuity value were com-\nparable to NeRV. Only trustworthiness and precision were\nabout 10% below NeRV.\n4. USER STUDY\nIn this section, we describe the design and results of a com-\nparative user study as qualitative evaluation. In order to\nreduce the effort for the participants, we only tested MDS,\nGSOM, and NeRV. Landmark MDS was not considered\nbecause there is no observable change for existing songs\nthat could be evaluated and SNE because of its close re-\nlation to the superior NeRV method. The following re-\nsearch questions were addressed: How well can users fol-\nlow the changes when adding a new album, what algorithm\ndo users prefer and why? The study was conducted using\nthe Beatles corpus and the same visualizations evaluated in\nthe quantitative measurements described in Section 3.\n4.1 Study Design\nThe user study was designed as follows: In a pre-interview,\nwe gathered the users’ demographic information, com-\nputer skills and their knowledge about The Beatles and\nmap-based visualizations. Then, a lab experiment with\nwithin-participants design was performed, i.e., each par-\nticipant viewed all three visualizations. We used a Latin\nsquare design to reduce the bias caused by the order in\nwhich participants were introduced to the different layout\nalgorithms, i.e., a third of the participants tested MDS ﬁrst,\nGSOM second and NeRV at last, whereas another third\nevaluated the algorithms in the order GSOM-NeRV-MDS\nand another third used the ordering NeRV-MDS-GSOM.\nAfter a short trial period of free interactions, we asked all\nparticipants to perform a memorization task in the form of\na game similar to thimblerig. At each step, starting with the\nﬁrst album of The Beatles, three songs were highlighted\nrandomly for ﬁve seconds as shown in Figure 2. Shortly\nafter, participants were asked to track these songs during\nthe transition of the visualization to the next step. The task\nwas to ﬁnd the previously highlighted songs in the updated\nlayout. There was no time limit. The number of errors\nmade by each user was recorded as a measurement for their\nperformance. Thus, for each algorithm, we gathered errors\nmade at each of the 11 (as we used 12 albums) steps. Af-\nter the test on each algorithm, users were asked how well\nthey were able to track the changes in the songs’ layout.\nFinally, we asked the participants what algorithm they pre-\nferred. The whole procedure took about 30 minutes.\n4.2 Study Results\nWe performed the user study with 19 subjects. They were\nbetween 23 and 38 years old (28 on average), eight women\nand eleven men. 68% of them were professional computerFigure 2 . Graphical user interface for the study.4Here, a\ncollection containing the ﬁrst two albums of The Beatles\nis visualized using the MDS algorithm. The participants\nwere asked to track all three highlighted songs (covers with\ngreen border) as they move during a transition.\nusers, whereas the rest had intermediate level. 68% of par-\nticipants were students or PhD students in computer sci-\nence. Only 21% were well acquainted with the music of\nThe Beatles, 47% were familiar with map visualization.\nThirteen participants chose MDS, six chose NeRV and\none chose GSOM as their favorite layout algorithm (one\nchose both MDS and NeRV). Users described MDS to re-\nsult in less positional changes, NeRV to better preserve\ncluster structures and GSOM to have less overlappings.\nThe results of the memorization task are shown in Figure 3.\nWith an increasing number of albums, it became more dif-\nﬁcult to ﬁnd the requested songs. At later steps, partic-\nipants noted that it was hard to select the desired songs\nbecause of many overlappings. Also, at some transitions,\ne.g., from six to seven albums, there are signiﬁcant differ-\nences between algorithms. In total, the layout generated\nby the MDS algorithm resulted into the least mistakes as\nshown in Figure 4. Unfortunately, our results are not sta-\ntistically signiﬁcant, presumably due to the low number of\nparticipants. However, there was a strong tendency to the\nMDS algorithm. Most participants (14) achieved their best\nresults using the MDS algorithm.\n5. CONCLUSIONS\nIn this paper, we evaluated ﬁve popular methods for pro-\nducing two-dimensional maps of music collections with\n4For better readability, the screenshot was taken using a\nmuch smaller screen resolution than in the user study. An on-\nline demo is available at http://demos.dke-research.de/\nbeatles-history-explorer/\n2 4 6 8 10 12\nnumber of albums in collection0.00.51.01.52.0errors\nMDS\nGSOM\nNeRVFigure 3 . Mean of memorization errors for each transi-\ntion, and conﬁdence intervals ( \u000b= 0:05).\nMDS GSOM NeRV\nalgorithm0.00.20.40.60.81.01.21.41.6errors\nFigure 4 . Mean memorization errors over all transitions,\nand conﬁdence intervals ( \u000b= 0:05).\nrespect to their ability to deal with incrementally growing\ncollections. Two of these, GSOMs and Landmark MDS,\nhave this ability by design. The other three, MDS, SNE,\nand NeRV, had to be slightly modiﬁed for this purpose.\nWe conducted objective measurements – tracking the vi-\nsualization quality and the change of positions over time\n– and a user study. Considering both the measured results\nand the feedback from the participants of the user study,\nMDS can be proclaimed as “winner”. This is much to our\nsurprise. We did not expect MDS (in combination with\nthe Procrustes analysis for alignment) would produce such\nnice incremental visualization updates as observed here.\nUnlike the second best, NeRV, MDS has furthermore the\nadvantage that it does not incorporate randomness and that\nit is guaranteed to ﬁnd a globally optimal solution.\nIt is possible though highly unlikely that the outcome\nwas in part caused by the choice of the collection. There-\nfore, we will conduct further experiments to verify the re-\nsults using different datasets and also different media like\ntexts and images. Another highly promising possibility for\nfuture work is to modify NeRV to better support incremen-\ntal collection changes – e.g., by introducing an additional\nweighted term to its cost function (Equation 2) that pe-\nnalizes position changes. This way, a new method could\nbe designed that results in less dramatic position changes\nbut maintains NeRV’s superior performance for trustwor-\nthiness and precision. For reproducibility and to encourage\npossible testing with further methods, the dataset (in par-\nticular the distance matrix used as input for the algorithms)\nwill be made publicly available.Acknowledgments We would like to thank all participants\nof the user study and J. Venna et al. for sharing their NeRV\nimplementation code. The work in this paper has been\nfunded in part by the German Federal Ministry of Educa-\ntion and Science (BMBF) through the Forschungscampus\nSTIMULATE under Grant No. FKZ:03FO16103A.\n6. REFERENCES\n[1] P. Cano, M. Kaltenbrunner, F. Gouyon, and E. Batlle.\nOn the use of fastmap for audio retrieval and browsing.\nInISMIR , 2002.\n[2] V . de Silva and J.B. Tenenbaum. Global versus local\nmethods in nonlinear dimensionality reduction. In Adv.\nin Neural Information Processing Systems 15 , 2002.\n[3] M. Dopler, M. Schedl, T. Pohle, and P. Knees. Access-\ning music collections via representative cluster proto-\ntypes in a hierarchical organization scheme. In ISMIR ,\n2008.\n[4] A. Flexer, D. Schnitzer, M. Gasser, and T. Pohle. Com-\nbining features reduces hubness in audio similarity. In\nISMIR , 2010.\n[5] M. Gasser and A. Flexer. Fm4 soundpark: Audio-based\nmusic recommendation in everyday use. In Proc. of 6th\nSound and Music Computing Conf. , 2009.\n[6] D. Gleich, M. Rasmussen, L. Zhukov, and K. Lang.\nThe World of Music: SDP layout of high dimensional\ndata. In Info Vis , 2005.\n[7] J.C. Gower and G.B. Dijksterhuis. Procrustes Prob-\nlems. Oxford University Press, 2004.\n[8] C. Harte, M.B. Sandler, S.A. Abdallah, and E. G ´omez.\nSymbolic representation of musical chords: A pro-\nposed syntax for text annotations. In ISMIR , 2005.\n[9] G.E. Hinton and S.T. Roweis. Stochastic neighbor em-\nbedding. In Adv. in Neural Information Processing Sys-\ntems 15 , 2002.\n[10] I.T. Jolliffe. Principal Component Analysis . Springer\nVerlag, 2002.\n[11] C. F. Julia and S. Jorda. SongExplorer: a tabletop ap-\nplication for exploring large collections of songs. In IS-\nMIR, 2009.\n[12] S. Kaski, J. Nikkila, M. Oja, J. Venna, P. Toronen, and\nE. Castren. Trustworthiness and metrics in visualiz-\ning similarity of gene expression. BMC Bioinformatics ,\n4(1):48, 2003.\n[13] P. Knees, T. Pohle, M. Schedl, and G. Widmer. Ex-\nploring Music Collections in Virtual Landscapes. IEEE\nMultiMedia , 14(3):46–54, 2007.\n[14] T. Kohonen. Self-organized formation of topologically\ncorrect feature maps. Biological cybernetics , 43(1):59–\n69, 1982.\n[15] J.B. Kruskal and M. Wish. Multidimensional Scaling .\nSage, 1986.\n[16] A.S. Lillie. Musicbox: Navigating the space of your\nmusic. Master’s thesis, MIT, 2008.[17] S. Lloyd. Automatic playlist generation and music li-\nbrary visualisation with timbral similarity measures.\nMaster’s thesis, Queen Mary Univ. of London, 2009.\n[18] D. L ¨ubbers and M. Jarke. Adaptive multimodal explo-\nration of music collections. In ISMIR , 2009.\n[19] M. Mandel and D. Ellis. Song-level features and sup-\nport vector machines for music classiﬁcation. In IS-\nMIR, 2005.\n[20] F. M ¨orchen, A. Ultsch, M. N ¨ocker, and C. Stamm.\nDatabionic visualization of music collections accord-\ning to perceptual distance. In ISMIR , 2005.\n[21] R. Neumayer, M. Dittenbach, and A. Rauber.\nPlaySOM and PocketSOMPlayer, alternative inter-\nfaces to large music collections. In ISMIR , 2005.\n[22] E. Pampalk, S. Dixon, and G. Widmer. Exploring mu-\nsic collections by browsing different views. In ISMIR ,\n2003.\n[23] E. Pampalk, A. Flexer, and G. Widmer. Hierarchical\norganization and description of music collections at the\nartist level. In Proc. of 9th Eu. Conf. on Research and\nAdvanced Technology for Digital Libraries , 2005.\n[24] E. Pampalk, A. Rauber, and D. Merkl. Content-based\norganization and visualization of music archives. In\nProc. of 10th ACM int. Conf. on Multimedia , 2002.\n[25] A. Rauber, E. Pampalk, and D. Merkl. Using psycho-\nacoustic models and self-organizing maps to create a\nhierarchical structuring of music by musical styles. In\nISMIR , 2002.\n[26] M. Schedl. The CoMIRV A Toolkit for Visualizing\nMusic-Related Data. Technical0 report, Johannes Ke-\npler University Linz, 2006.\n[27] S. Stober and A. N ¨urnberger. Towards user-adaptive\nstructuring and organization of music collections. In\nProc. of 6th int. workshop on Adaptive Multimedia Re-\ntrieval , 2008.\n[28] S. Stober and A. N ¨urnberger. Analyzing the impact of\ndata vectorization on distance relations. In 2011 IEEE\nInt. Conf. on Multimedia and Expo , 2011.\n[29] S. Stober and A. N ¨urnberger. Musicgalaxy: A multi-\nfocus zoomable interface for multi-facet exploration of\nmusic collections. In Exploring Music Contents , vol-\nume 6684 of LNCS , pages 273–302, 2011.\n[30] S. Stober and A. N ¨urnberger. Adaptive music retrieval\n– a state of the art. Multimedia Tools and Applications ,\n65(3):467–494, 2013.\n[31] R. van Gulik and F. Vignoli. Visual playlist generation\non the artist map. In ISMIR , 2005.\n[32] J. Venna, J. Peltonen, K. Nybo, H. Aidos, and S. Kaski.\nInformation retrieval perspective to nonlinear dimen-\nsionality reduction for data visualization. Journal of\nMachine Learning Research , 11:451–490, 2010.\n[33] C.K.I. Williams. On a connection between kernel pca\nand metric multidimensional scaling. Machine Learn-\ning, 46(1-3):11–19, 2002."
    },
    {
        "title": "Sparse Modeling for Artist Identification: Exploiting Phase Information and Vocal Separation.",
        "author": [
            "Li Su 0002",
            "Yi-Hsuan Yang"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417107",
        "url": "https://doi.org/10.5281/zenodo.1417107",
        "ee": "https://zenodo.org/records/1417107/files/SuY13.pdf",
        "abstract": "As artist identification deals with the vocal part of music, techniques such as vocal sound separation and speech feature extraction has been found relevant. In this paper, we argue that the phase information, which is usually overlooked in the literature, is also informative in modeling the voice timbre of a singer, given the necessary processing techniques. Specifically, instead of directly using the raw phase spectrum as features, we show that significantly better performance can be obtained by learning sparse features from the negative derivative of phase with respect to frequency (i.e., group delay function) using unsupervised feature learning algorithms. Moreover, better performance is achieved by using singing voice separation as a pre-processing step, and then learning features from both the magnitude spectrum and the group delay function. The proposed system achieves 66% accuracy in identifying 20 artists from the artist20 dataset, which is better than a prior art by 7%.",
        "zenodo_id": 1417107,
        "dblp_key": "conf/ismir/SuY13",
        "keywords": [
            "vocal sound separation",
            "speech feature extraction",
            "phase information",
            "group delay function",
            "unsupervised feature learning",
            "magnitude spectrum",
            "pre-processing step",
            "singing voice separation",
            "artist20 dataset",
            "66% accuracy"
        ],
        "content": "SPARSE MODELING FOR ARTIST IDENTIFICATION: EXPLOITING\nPHASE INFORMATION AND VOCAL SEPARATION\nLi Su and Yi-Hsuan Yang\nResearch Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan\nlisu@citi.sinica.edu.tw, yang@citi.sinica.edu.tw\nABSTRACT\nAs artist identiﬁcation deals with the vocal part of\nmusic, techniques such as vocal sound separation and\nspeech feature extraction has been found relevant. In\nthis paper, we argue that the phase information, which is\nusually overlooked in the literature, is also informative in\nmodeling the voice timbre of a singer, given the necessary\nprocessing techniques. Speciﬁcally, instead of directly\nusing the raw phase spectrum as features, we show that\nsigniﬁcantly better performance can be obtained by learn-\ning sparse features from the negative derivative of phase\nwith respect to frequency (i.e., group delay function) using\nunsupervised feature learning algorithms. Moreover, better\nperformance is achieved by using singing voice separation\nas a pre-processing step, and then learning features from\nboth the magnitude spectrum and the group delay function.\nThe proposed system achieves 66% accuracy in identifying\n20 artists from the artist20 dataset, which is better than a\nprior art by 7%.\n1. INTRODUCTION\nSinging voice is one of the most prominent characteristics\nin music. To model the vocal signal accurately, much\nof the effort has been focus on two topics: 1) extracting\nspeech-related features of the audio signal [25, 27], and 2)\nseparating human voice from the accompaniment [9,18,24,\n29].\nFor feature extraction in speech processing, the phase-\nbased features has been noticed with the application of\nspeaker recognition or the reconstruction of intelligible\nvoice [13, 21, 23, 26]. Instead of using phase only, these\nworks adopted the group-delay function (the negative deri-\nvative of phase by frequency), sometimes also merged\ntogether with amplitude-based features. On the other hand,\nthe application of phase on music has been limited mostly\nin signal-level, such as onset detection [2, 5, 16, 17] and\npitch tracking [10]. For high-level musical concepts, like\ngenre or the timbre of the vocal artist, phase information\nhas been rarely discussed. Since singing timbre is closely\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.related to characteristics of the speech signal, it is worth\ninvestigating the use of phase derivatives for artist identiﬁ-\ncation from popular songs.\nAs for vocal separation, one remarkable development\nemerged recently is the sparse and low-rank matrix decom-\nposition, also known as robust principal component anal-\nysis (RPCA) technique. It notices that the main melody\nand the accompaniment can be suitably regarded as the\nsparse and low-rank counterparts respectively in an audio\nspectrogram [15, 30], since the former involves only a few\nnotes at a time, and the latter are typically contributed by\nrepetition of metre and harmonic structure. This technique\nsatisﬁes the requirement of artist identiﬁcation, as such\napplication needs an efﬁcient algorithm to remove the\naccompaniment and preserve the vocal (mostly melody)\ninformation.\nRecent years have witnessed progression of sparse mod-\neling techniques [4, 6, 28], which allow for constructing a\nsuccinct representation of raw features as a combination of\nonly a few atoms learned from an external data collection\n[22]. The resulting signal reconstruction has been shown\nrobust to noise or corruptions of data. In consequence,\nsparse coding techniques have been applied in many ﬁelds,\nincluding MIR. For example, Yeh et al. [31] demonstrated\naccuracy on par with state-of-the-systems for genre classi-\nﬁcation using sparse features learned by sparse modeling\ntechniques.\nThis paper will investigate the sparse modeling tech-\nniques for singing voice separation and unsupervised fea-\nture learning of group-delay functions. In what follows, we\nwill provide the details of the proposed system in Section\n2, followed by experimental evaluations in Section 3. We\nwill discuss the main ﬁndings and limitations in Section 4\nand conclude the paper in Section 5.\n2. SYSTEM OVERVIEW\nFigure 1 shows the ﬂow diagram of the system we im-\nplemented for artist identiﬁcation. The system makes use\nof a collection of songs, named the “training corpus,” to\nbuild the audio dictionaries, which are used to compute the\nsparse representation of a querying audio signal. Prior to\ndictionary learning or sparse coding, frame-level features\nare extracted from the audio ﬁles. We consider both the\nmagnitude-based andphase-based features derived from\nthe short-time Fourier transform (STFT), by taking the log-\nmagnitude spectrogram as the amplitude-based feature,Figure 1 . Proposed artist identiﬁcation system.\nand the group-delay as the phase-based feature.\nTwo types of dictionaries, DSGandDGD, both of size\nm\u0002k, are respectively learned from the spectrograms and\ngroup-delay functions gathered from the training corpus\nusing the online dictionary learning (ODL) algorithm (see\nSection 2.2). When there is a large overlap of artists\nbetween the training corpus and the querying songs, the\ngeneralizability of the learned dictionary might be lim-\nited (this is the so-called transductive learning setup).\nTherefore, it is preferable to use a training corpus that is\nrepresentative enough, but is disjoint from the querying\nsongs in the experiments.\nAs for testing, we ﬁrst separate the query songs into\nsinging voice and music accompaniment using RPCA\ntechnique, one of the state-of-the-art algorithms for singing\nvoice separation [15]. Then, the log-magnitude spectro-\ngram and group-delay functions are then encoded by DSG\nandDGDrespectively by l1-regularized sparse coding,\nengendering the codewords \u000bSGand\u000bGD. Each input\nfeature is normalized to its Euclidean norm before sparse\ncoding. After sparse coding, bag-of-frames (BOF) features\nare obtained by summing over all the frame-based features\n\u000bSGand\u000bGD, respectively, thereby creating a histogram\nof the cumulative term occurrence of the dictionary atoms\n[19]. Then we perform feature fusion by concatenating the\ntwo BOF features encoded from spectrograms and group-\ndelay functions. Finally, the performance is evaluated\nby multi-class support vector classiﬁcation in a cross-\nvalidation scheme.\n2.1 Group Delay Function\nConsider a general representation of short-time Fourier\ntransform (STFT) of a time-domain signal x(t):\nSh\nx(t;!) =Z1\n\u00001x(\u001c)h\u0003(t\u0000\u001c)e\u0000j!\u001cd\u001c (1)\n= Mh\nx(t;!)ej\bh\nx(t;!); (2)\nFigure 2 . Examples of magnitude spectra, group-\ndelay functions and phase proﬁles for tenor vocal /ah/\nwith fundamental frequencies at C4, selected from RWC\nMusical Instrument Sound Database [11]. Bold solid line:\nnormal (less vibrato); thin solid line: vibrato; thin dashed\nline: falsetto.\nwhere Sh\nx(t;!)2Cis the two-dimensional STFT rep-\nresentation on time-frequency plane, h(t)is the window\nfunction, Mh\nx(t;!)and\bh\nx(t;!)of (1) are the amplitude\nand the phase of the STFT representation, respectively. By\ntaking the natural logarithm of Eq. (2), we obtain the log-\nmagnitude spectrogram in the real part and the phase the\nimaginary part:\nlog Mh\nx(t;!) = Re\u0000\nlog Sh\nx(t;!)\u0001\n; (3)\n\bh\nx(t;!) = Im\u0000\nlog Sh\nx(t;!)\u0001\n: (4)\nTaking the negative derivative of phase (4) with respect to\nfrequency, we have\n\u0000@\bh\nx(t;!)\n@!= Re\u0012\nt\u0000STh\nx(t;!)\nShx(t;!)\u0013\n; (5)\nwhere!= 2\u0019fis the angular frequency and T(\u0001)is the\noperator such that Th(t) =t\u0001h(t). The ﬁrst term t\ndenotes the current time, while the second term is deﬁned\nasgroup-delay function . Detailed derivation procedures\nof group-delay function can be found in [1, 12]. In this\nwork, the group-delay function is computed by the Time-\nFrequency Toolbox.1\nTo illustrate the effect of singing timbre on phase, in\nFigure 2 we show the spectral amplitudes, group-delay\nfunctions and phase proﬁles of three different examples\n(normal, vibrato and falsetto) of single vowel /ah/ sung\nby a tenor singer with the same fundamental frequencies\nC4. Since the fundamental frequencies of these three\nsounds are the same, the peaks in the amplitude spectra\nare mostly overlapped. At the spectral peak frequencies,\n1http://tftb.nongnu.org/the group-delay values of all three sounds are nearly zero.\nHowever, except for the spectral peaks, the group-delay\nfunction of falsetto sound is largely different from those of\nnormal and vibrato sounds. The group-delay of falsetto\nis more peaky, possibly because of the occurrence of\nthe transmission zeros (dips) evident from the magnitude\nspectrum, which mostly correspond to the spurious peaks\nseen in the group-delay function. We cannot observe\nthis kind of spurious peaks from the magnitude spectra\nalone, due to the non-stationary nature of the signal under\nanalysis (singing with vibrato), as well as the lack of\nsufﬁcient frequency resolution. In contrast, group-delay\nfunctions better indicate the characteristics in frequency\nbands with small-energy.\n2.2 Dictionary Learning and Sparse Coding\nThe atoms of a dictionary are typically learned from a\nlarge-scale training corpus. To overcome the difﬁculty of\nloading data into memory, the ODL algorithm is adopted\n[22]. ODL comes with a mini-batch mechanism that learns\nthe dictionary incrementally by using a part of the training\ncorpus in each update. Speciﬁcally, during the dictionary\nlearning process, the atoms are updated for each input\nfeature of the ﬁnite set of training signals Y= [y1;:::;yN]\nthrough the following joint optimization problem:\n^D= argmin\nD2C1\nNPN\ni=1\u00001\n2kyi\u0000D\u000bik2\n2+\u0015k\u000bik1\u0001\n;\n8j= 1;:::;k; dT\njdj\u00141;\n(6)\nwhere yi2Rmis thei-th frame-level feature (column\nvector) of the input data (i.e., either log Mh\nxor\bh\nx) from\nthe training corpus, \u000bi2Rkis the codeword, D2Rm\u0002k\nis the dictionary, and the atom djis thejth column vector\nofD. We can solve for Dand\u000biby minimizing one\nwhile keeping the other ﬁxed [22]. The optimization of\n\u000binvolves a typical l1-regularized sparse coding problem,\nwhich can be described as\n^\u000bt= arg min\n\u000btkxt\u0000D\u000btk2\n2+\u0015k\u000btk1: (7)\nThis problem has been well-studied in the machine learn-\ning and statistics ﬁelds, under different names such as\nthe basis pursuit [4] or the lasso problem [28]. In this\nwork, we use the open-source package SPAMS2and the\nLARS-lasso algorithm [6] for ODL and sparse coding,\nrespectively. The parameter \u0015is set to 1=pmas in [6].\n2.3 Source Separation\nGiven a monaural music signal, we ﬁrst compute its\nm\u0002nspectrogram Mh\nxthrough STFT. Then, we separate\nthe singing voice E(sparse components) from the music\naccompaniment A(low-rank components) by formulating\nthe problem as the following RPCA problem,\nmin\nA;E:Mhx=A+EkAk\u0003+\u0016kEk1; (8)\n2http://spams-devel.gforge.inria.fr/Corpus Original V ocal Accompaniment\nUSPOP 62.6% 65.9% 58.5%\nUSPOP2 61.7% 65.5% 56.6%\nMIR-1K 55.9% 63.4% 53.4%\nRWC 54.2% 59.9% 49.9%\nTable 1 . Average accuracy using log-magnitude spectro-\ngram BOF features for various audio signals and training\ncorpora. The vocal and accompaniment parts are separated\nby RPCA technique.\nwherek\u0001k\u0003denotes the trace norm of a matrix (the\nsum of its singular values), k\u0001k 1is thel1norm that\ndenotes the sum of the absolute values of matrix entries,\nand\u0016is a positive weighting parameter that can be set to\n1=p\nmax(m;n)as recommended in [3]. This algorithm\nis proven to be robust against gross errors and outliers,\nin comparison to its l2-regularized counterpart, the well-\nknown PCA algorithm. As Eq. (8) is convex, efﬁcient\nalgorithms such as accelerated proximal gradient (APG)\nand augmented Lagrange multipliers (ALM) [3, 20] can\nbe employed to compute AandEin an iterative fashion.\nOpen-source implementation of such solvers can be found\nfrom the Internet.3\n3. EXPERIMENT\n3.1 Dataset and Experimental Setup\nWe evaluate the performance of artist identiﬁcation using\nthe artist20 dataset [7],4which consists of six albums\n(1,413 songs in total) sung by 20 artists. For each song,\na 30-second length audio signal with both vocal and music\naccompaniment is clipped for evaluation. The clips are\nsampled at 16 kHz. Most of the songs in the artist20\ndataset can also be found in the uspop2002 dataset,5\nwhich contains over 7,000 Western Pop songs.\nFor computing the STFT, we use the Hanning window\nand try different values of the window size w(in terms\nof samples) and the hop factor h(the ratio of hop size\nand window size). For classiﬁer training and testing, the\nl2-regularized l2-loss support vector classiﬁer in LIBLIN-\nEAR6is employed for efﬁciency. To avoid album effect, a\nsix-fold jack-knife cross-validation scheme is conducted.\nEach fold contains only one album from every artist.\nAs there are many possible fold partitions, we perform\nthe random partitions for ten times to get the average\nclassiﬁcation accuracy. Two-tailed t-test is also performed\n(over the ten six-fold partitions) to evaluate whether the\nperformance difference between different methods or pa-\nrameter settings is signiﬁcant.\n3http://perception.illinois.uiuc.edu/\nmatrix-rank/\n4http://labrosa.ee.columbia.edu/projects/\nartistid/\n5http://labrosa.ee.columbia.edu/projects/\nmusicsim/uspop2002.html\n6http://www.csie.ntu.edu.tw/ ˜cjlin/liblinear/w/ GD,w= 512 w/ GD,w= 1024 w/ GD,w= 2048\nSG w/o GDh= 0.1h= 0.2h= 0.5h= 0.1h= 0.2h= 0.5h= 0.1h= 0.2h= 0.5\nw=\n1024h= 0.1 62.9% 63.3% 63.6% 64.0% 64.5% 64.3% 63.8% 64.6% 64.3% 63.8%\nh= 0.2 63.1% 62.1% 62.9% 64.2% 64.4% 65.0% 63.8% 65.4% 64.7% 64.3%\nh= 0.5 63.4% 58.1% 59.6% 62.1% 61.9% 62.5% 62.4% 64.9% 64.5% 63.9%\nTable 2 . Comparison of average accuracy among SG BOF features and fused SG + GD BOF features under various window\nsizes and hop factors. V ocal audio signal and MIR-1K training corpus are used. Data in bold style are those which achieve\nsigniﬁcant improvement in comparison to non-fused features under the same parameter settings.\nFigure 3 . Average accuracy (in %) for SG BOF (left)\nand GD BOF (right) features constructed under various\nwindow sizes and hop factors. V ocal audio signal and\nMIR-1K training corpus are used.\n3.2 Training Corpora\nAs discussed in Section 2, the size and the diversity of\nthe training corpus are important to the generalizability of\nthe learned dictionaries. We also have to pay attention to\nthe possible overlap between the training corpus and the\nquerying data (i.e., the dataset of the target classiﬁcation\nproblem; here artist20). To study the effect of training\ncorpora, we compared the performance of artist identiﬁ-\ncation using dictionaries learned from the following four\ntraining corpora: the whole uspop2002 dataset (USPOP),\nthe uspop2002 dataset excluding overlap with artist20\n(USPOP2), the MIR-1K dataset, and the whole vocal data\nin the RWC instrument dataset (RWC) [11]. MIR-1K\ncontains 1,000 song clips extracted from 110 Chinese Pop\nsongs released in karaoke format [14]; we use the vocal\nchannel to train the dictionary. RWC vocal data contains\nﬁve vowels (/a/, /i/, /u/, /e/, /o/) with various pitches and\nsinging techniques sung by eight female and ten male\nsingers, totaling more than 20,000 isolated notes. Among\nthe four corpora, USPOP has the largest overlap with\nartist20. USPOP2 has no overlap but the music genre is the\nsame (Western Pop). In contrast, the MIR-1K and RWC\nare considered more dissimilar from artist20.\nAs the ﬁrst evaluation, we consider only BOF features\ncomputed from the log-magnitude spectrogram, using w=\n1024 andh= 0:5. The sizekof the dictionary is set to1024. The evaluation result is shown in Table 1. The three\ncolumns of the table show the averaged accuracy using\nthe original signal and the separated signals (vocal and\naccompaniment). By comparing the result of the four rows\nalong the ﬁrst column, we see that USPOP and USPOP2\nlead to better accuracy comparing to MIR-1K and RWC.\nThis is not surprising as the last two datasets are less\nsimilar with artist20. Although USPOP performs slightly\nbetter than USPOP2, the performance difference is not\nsigniﬁcant under the t-test.\n3.3 Source Separation\nBy comparing the result of the three columns of Table\n1, also it can be observed that using the separated vocal\nsound generally improves the classiﬁcation accuracy, in\ncomparison to using the original audio signals. In contrast,\nusing the separated music accompaniment deteriorates the\naccuracy, which makes sense as the task emphasizes the\nvocal part of music. Two-tailed t-test shows that the im-\nprovement of V ocal over Original is signiﬁcant (p <0.001,\ndf=118). Moreover, when features are extracted from the\nvocal part, we see that the performance of using MIR-\n1K as the training corpus comes close to the case when\nUSPOP is used (63.4% vs 65.9%).\nTo ensure the evaluation reported here is general enough,\nwe use MIR-1K as the training corpus in the following\nexperiments. Moreover, the features are computed from\nthe separated vocal part of the song clips.\n3.4 Incorporating Group Delay\nFigure 3 shows the average accuracy of the BOF fea-\ntures computed from log-magnitude spectrogram (SG) and\ngroup-delay (GD) under various window sizes wand hop\nfactorsh. The sizes for the SG and GD dictionaries are\nboth set to 1024. Note that we use different y-axes for the\ntwo features. As the ﬁgure shows, the performance of GD\n(20–35%) is generally much worse than the performance\nof SG (61–64%), possibly due to the highly noisy parts of\nthe phase information. However, the result of GD is by\nno means random; the accuracies are signiﬁcantly better\nthan the random guess (whose accuracy is close to 5%).\nMoreover, we see a clear trend of the performance of GD\nwith respect to wandh: better result is obtained by using\nsmallerwand smaller h. In contrast, the performance of\nSG seems to be less sensitive to wandh.\nWe further experiment with the option of fusing the two\ntypes of features. The result is shown in Table 2, where weFigure 4 . Average accuracies of different SG + GD BOF\nfeatures by varying the dictionary size ( k) and the window\nsizes of SG feature. V ocal audio signal and MIR-1K\ntraining corpus are used. As for GD feature, the window\nsize is ﬁxed to 1024. Hop factors for both SG and GD are\n0.2.\ncompare the result without (‘w/o’) and with (‘w/’) using\nGD features with different values of wandh. We see the\nbest result is obtained when SG ( w=1024,h=0.2) and GD\n(w=2048,h=0.1) are fused, each using different values of\nwandh. Comparing to the case when GD is not used\n(i.e., the third column), the accuracy is improved from\n63.1% to 65.4%, a signiﬁcant improvement under the t-\ntest (p<0.05, df =118). Actually, signiﬁcant improvement\nis also observed for other cases, as indicated by the use of\nbold font weight in Table 2. This result shows that phase\ninformation is indeed useful for this task.\n3.5 Inﬂuence of Dictionary Size\nIt has been shown that the performance of dictionary-based\napproach can be improved by using a larger dictionary\n[31]. As this is only veriﬁed on spectrogram-based fea-\ntures before, in this experiment we test the effects of the\ndictionary size kon the accuracy of artist identiﬁcation\nusing the group-delay features. Instead of using the GD\nfeatures alone, we fuse it with SG features that is computed\nfrom a dictionary of size 1024, as this brings about better\nperformance. Figure 4 shows the results as the dictionary\nsize for GD ranges from 0 (no fusion) to 2048. When the\nwindow sizes wof SG are 512 or 1024, the performance\ngradually increases until reaching a plateau as we increase\nthe dictionary size of GD. Generally speaking, using larger\ndictionary is also beneﬁcial to phase-based features.\n3.6 Comparison with the Existing Work\nFinally, we compare our method with the GMM-based\nmodel proposed in [7], whose underlying frame-level\nfeature representation is based on the early fusion of the\nclassic MFCC and Chroma. Under a six-fold jack-knife\ncross-validation, the overall accuracy on artist20 was 56%\nfor MFCC features and 59% for MFCC+Chroma features.\nUsing the same fold partition and dictionaries learned fromseparated vocal parts of USPOP2, the proposed method\nreaches 66.0%, an improvement of 7%. Even if the\nless similar MIR-1K dataset is employed for dictionary\nlearning, the classiﬁcation accuracy reaches 65.5%.\n4. DISCUSSION\nIn view of the source-ﬁlter model of voice, group-delay\nfunction contains the information on the phase-distortion\nbehaviors of the vocal tract ﬁlter, which varies with the\nindividual. Differently, the magnitude part contributes\nto the distribution of the resonance peaks on the time-\nfrequency plane. Mathematically, the magnitude and the\nphase part of a STFT proﬁle are strongly related, but\nthey still have different characteristics [8]. Although the\ninformation provided by the magnitude part is prominent,\nthe incorporation of group-delay usually better explains the\ncharacteristics of the target signal. There are many pos-\nsibilities combining these two features, such as merging\nthe two features at frame level [13], combining at BOF\nlevel, or in decision stage. Therefore, the way by which\nthe features are combined and the relative weights of the\ntwo features are worthy for future study.\nThe use of singing voice separation generally improves\nthe modeling of either the magnitude or phase information.\nHowever, the RPCA technique adopted in this work is not\nperfect. Under many cases the sparse counterpart of RPCA\ntechnique contains not only vocal sources but predominant\ninstrument solo or the percussion sound. This issue might\nbe partially solved by performing vocal detection ﬁrst to\nexclude non-vocal segments.\n5. CONCLUSION\nIn this paper, we have proposed a novel artist identiﬁcation\nmethod based on sparse features learned from both the\nmagnitude and phase parts of the spectrum. The features\nare computed from the separated vocal part of music signal\nusing robust principal component analysis, in order to\nbetter model the characteristics of singing timbre. Our\nanalysis shows that both singing voice separation and un-\nsupervised feature learning are required steps for the fea-\ntures to be informative. Moreover, group-delay functions\ncontain information complementing spectrogram. Evalu-\nation on the artist20 dataset validates the effectiveness of\nthe proposed phase feature. It is hoped that the present\nwork can inspire more research towards the modeling of\nphase information of music signals, which might hold the\npromise of improving other MIR problems.\n6. REFERENCES\n[1] F. Auger and P. Flandrin. Improving the readability\nof time-frequency and time-scale representations by\nthe method of reassigment. IEEE Trans. Signal\nProcessing , 43(5):1068–1089, 1995.\n[2] E. Benetos and Y . Stylianou. Auditory spectrum-based\npitched instrument onset detection. IEEE Trans. Audio,\nSpeech, Language Process. , 18(8):1968–1977, 2010.[3] E. J. Cand `es, X. Li, Y . Ma, and J. Wright. Robust\nprincipal component analysis? J. ACM , 58(3):1–37,\n2011.\n[4] S. S. Chen, D. L. Donoho, Michael, and A. Saunders.\nAtomic decomposition by basis pursuit. SIAM J.\nScientiﬁc Computing , 20:33–61, 1998.\n[5] S. Dixon. Onset detection revisited. In Proceedings\nof the 9th International Conference on Digital Audio\nEffects (DAFx06) , 2006.\n[6] B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani.\nLeast angle regression. Annals of Statistics , 32:407–\n499, 2004.\n[7] D. Ellis. Classifying music audio with timbral and\nchroma features. In ISMIR , 2007.\n[8] E. Chassande-Mottin F. Auger and P. Flandrin. On\nphase-magnitude relationships in the short-time fourier\ntransform. IEEE Signal Process. Lett , 19(5):267–270,\n2012.\n[9] H. Fujihara, M. Goto, T. Kitahara, and H. G. Okuno.\nA modeling of singing voice robust to accompaniment\nsounds and its application to singer identiﬁcation\nand vocal-timbre-similarity-based music information\nretrieval. IEEE Trans. Audio, Speech, Language\nProcess. , 18(3):638–648, 2010.\n[10] M. Goto. A robust predominant-f0 estimation method\nfor real-time detection of melody and bass lines in cd\nrecordings. In IEEE ICASSP , 2000.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Music genre database and\nmusical instrument sound database. In ISMIR , 2003.\n[12] S. Hainsworth, M. Macleod, S. W. Hainsworth, and\nM. D. Macleod. Time frequency reassignment: A\nreview and analysis. Technical report, Cambridge\nUniversity Engineering Department and Qinetiq, 2003.\n[13] R. M. Hegde, H. A. Murthy, and V . R. R. Gadde.\nSigniﬁcance of the modiﬁed group delay feature\nin speech recognition. IEEE Trans. Audio, Speech,\nLanguage Process. , 15(1):190–202, 2007.\n[14] C.-L. Hsu and J.-S. R. Jang. On the improvement\nof singing voice separation for monaural recordings\nusing the mir-1k dataset. IEEE Trans. Audio, Speech,\nLanguage Process. , 18(2):310–319, 2010.\n[15] P.-S. Huang, S. D. Chen, P. Smaragdis, and\nM. Hasegawa-Johnson. Singing-voice separation from\nmonaural recordings using robust principal component\nanalysis. In Proc. IEEE ICASSP , 2012.\n[16] S. Abdallah C. Duxbury M. Davies J. P. Bello,\nL. Daudet and M. B. Sandler. A tutorial on onset\ndetection in music signals. IEEE Trans. Speech, Audio\nProcess. , 13(5):1035–1047, 2005.[17] A. Lacoste and D. Eck. A supervised classiﬁcation\nalgorithm for note onset detection. In EURASIP\nJournal on Advances in Signal Processing , 2007.\n[18] M. Lagrange, A. Ozerov, and E. Vincent. Robust\nsinger identiﬁcation using melody enhancement and\nuncertainty learning. In ISMIR , 2012.\n[19] M. Levy and M. Sandler. Music information retrieval\nusing social tags and audio. IEEE Trans. Multimedia ,\n11(3):383–395, 2009.\n[20] Z. Lin and et al. Fast convex optimization algorithms\nfor exact recovery of a corrupted low-rank matrix.\nTechnical Report UILU-ENG-09-2214, 2009.\n[21] L. Liu, J. L. He, and G. Palm. Effects of phase on\nthe perception of intervocalic stop consonants. Speech\nCommunication , 22:403–417, 1997.\n[22] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online\ndictionary learning for sparse coding. In ICML , pages\n689–696, 2009.\n[23] I. McCowan, D. Dean, M. McLaren, R. V ogt,\nand S. Sridharan. The delta-phase spectrum with\napplication to voice activity detection and speaker\nrecognition. IEEE Trans. Audio, Speech, Language\nProcess. , 19(7):2026–2038, 2011.\n[24] A. Mesaros, T. Virtanen, and A. Klapuri. Singer iden-\ntiﬁcation in polyphonic music using vocal separation\nand pattern recognition methods. In ISMIR , 2007.\n[25] T. L. Nwe and H. Li. Exploring vibrato-motivated\nacoustic features for singer identiﬁcation. IEEE Trans.\nAudio, Speech, Language Process. , 15(2):519–530,\n2007.\n[26] K. K. Paliwal and L. D. Alsteris. Further intelligibility\nresults from human listening tests using the short-time\nphase spectrum. Speech Communication , 48:727–736,\n2006.\n[27] L. Regnier and G. Peeters. Combining classiﬁcation\nbased on local and global features: application to\nsinger identiﬁcation. In DAFx-11 , pages 127–134, Sep.\n2011.\n[28] R. Tibshirani. Regression shrinkage and selection via\nthe lasso. J. Royal Statistical Soc. , 58:267–288, 1996.\n[29] W.-H. Tsai and H.-P. Lin. Background music removal\nbased on cepstrum transformation for popular singer\nidentiﬁcation. IEEE Trans. Audio, Speech, Language\nProcess. , 19(5):1196–1205, 2011.\n[30] Y .-H. Yang. On sparse and low-rank matrix decompo-\nsition for singing voice separation. In ACM MM , pages\n757–760, 2012.\n[31] C.-C. M. Yeh and Y .-H. Yang. Supervised dictionary\nlearning for music genre classiﬁcation. In ACM ICMR ,\n2012."
    },
    {
        "title": "A Methodology for the Comparison of Melodic Generation Models Using Meta-Melo.",
        "author": [
            "Nicolas Gonzalez Thomas",
            "Philippe Pasquier",
            "Arne Eigenfeldt",
            "James B. Maxwell"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1416092",
        "url": "https://doi.org/10.5281/zenodo.1416092",
        "ee": "https://zenodo.org/records/1416092/files/ThomasPEM13.pdf",
        "abstract": "We investigate Musical Metacreation algorithms by applying Music Information Retrieval techniques for comparing the output of three off-line, corpus-based style imitation models. The first is Variable Order Markov Chains, a statistical model; second is the Factor Oracle, a pattern matcher; and third, MusiCOG, a novel graphical model based on perceptual and cognitive processes. Our focus is on discovering which musical biases are introduced by the models, that is, the characteristics of the output which are shaped directly by the formalism of the models and not by the corpus itself. We describe META-MELO, a system that implements the three models, along with a methodology for the quantitative analysis of model output, when trained on a corpus of melodies in symbolic form. Results show that the models’ output are indeed different and suggest that the cognitive approach is more successful at the tasks, although none of them encompass the full creative space of the corpus. We conclude that this methodology is promising for aiding in the informed application and development of generative models for music composition problems.",
        "zenodo_id": 1416092,
        "dblp_key": "conf/ismir/ThomasPEM13",
        "keywords": [
            "Music Information Retrieval",
            "style imitation models",
            "Variable Order Markov Chains",
            "Factor Oracle",
            "MusiCOG",
            "system",
            "METAMELO",
            "quantitative analysis",
            "creative space",
            "generative models"
        ],
        "content": "A METHODOLOGY FOR THE COMPARISON OF MELODIC\nGENERATION MODELS USING META-MELO\nNicolas Gonzalez Thomas, Philippe Pasquier, Arne Eigenfeldt, James B. Maxwell\nMAMAS Lab, Simon Fraser University\nngonzale@sfu.ca, pasquier@sfu.ca\nABSTRACT\nWe investigate Musical Metacreation algorithms by ap-\nplying Music Information Retrieval techniques for com-\nparing the output of three off-line, corpus-based style imi-\ntation models. The ﬁrst is Variable Order Markov Chains ,\na statistical model; second is the Factor Oracle , a pattern\nmatcher; and third, MusiCOG , a novel graphical model\nbased on perceptual and cognitive processes. Our focus\nis on discovering which musical biases are introduced by\nthe models, that is, the characteristics of the output which\nare shaped directly by the formalism of the models and not\nby the corpus itself. We describe META-MELO, a system\nthat implements the three models, along with a method-\nology for the quantitative analysis of model output, when\ntrained on a corpus of melodies in symbolic form. Results\nshow that the models’ output are indeed different and sug-\ngest that the cognitive approach is more successful at the\ntasks, although none of them encompass the full creative\nspace of the corpus. We conclude that this methodology is\npromising for aiding in the informed application and devel-\nopment of generative models for music composition prob-\nlems.\n1. INTRODUCTION\nComputational Musicology has generally focused on study-\ning human composed music, however, algorithms for mu-\nsic generation provide a rich and relatively unexplored area\nfor study. As algorithmic and generative models grow in\nnumber and complexity, the task of selecting and applying\nthem to speciﬁc musical problems still remains an open\nquestion; for example, in the development of Computer\nAided Composition (CAC) environments.\nStylistic Imitation , a particular Musical Metacreative ap-\nproach [20], can be described as creativity arising from\nwithin a pre-established conceptual space. At times, this\nis referred to as Exploratory Creativity [5] and in practical\nmusical terms is concerned with generating new and origi-\nnal compositions that roughly cover the same space as the\ncorpus, thus ﬁtting a given musical style [2]. The concep-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\n© 2013 International Society for Music Information Retrieval.tual space of a style can be deﬁned by observing the mu-\nsical features which remain invariant across the corpus if,\nindeed, there are any such features.\nThe techniques applied for this task can be broadly cate-\ngorized into two methodological groups: corpus based and\nnon-corpus based methods. In the former, musical knowl-\nedge of the style is obtained through empirical induc-\ntion from existing music compositions (generally in sym-\nbolic MIDI format), using machine learning techniques.\nWhereas in the latter, this knowledge is provided by re-\nsearchers in the form of theoretical and/or rule-based rep-\nresentations.\nWe are concerned here with applying Music Informa-\ntion Retrieval (MIR) tools in a controlled setting for the\npurpose of understanding more completely how these meth-\nods behave in real world applications. For this study we\nhave chosen three corpus based models: the statistical Vari-\nable Order Markov Model (VOMM) [19], the Factor Or-\nacle (FO) pattern matcher [8], and MusiCog [15], a novel,\ncognitively inspired approach used for the suggestion and\ncontextual continuation (reﬂexive interaction) of musical\nideas in the notation-based CAC system Manuscore [14].\nThe main question that we address is: given three corpus-\nbased style-imitative models, which characteristics of the\noutput are shaped by the underlying models themselves\nand not by the corpus? That is, we aim to discover the mu-\nsical biases which arise from the formalism of the models.\nTo answer this we investigate how each model’s output is\ndifferent in a statistically signiﬁcant way.\nA second question that arises is: what is the appropri-\nate methodology for this research problem? We propose\na framework and methodology for generating and evaluat-\ning melodies in a controlled setting where all models share\nthe same fundamental conditions (Section 3.1). We use\ninter-model analysis to compare features from the melodic\noutput of each model to the corpus and to the output of all\nother models, and intra-model analysis to reveal informa-\ntion about the relationships between the melodies gener-\nated by a single model.\nOur contributions are: (1) META-MELO a MAX/MSP\nimplementation of the three models, used for melodic gen-\neration from a corpus (Section 3), (2) a methodology which\napplies Machine Learning and MIR techniques for model\noutput comparison (Section 5), (3) the results of a study\nwhere we apply this methodology (Section 6) and (4) a\ncorpus of classical, popular, and jazz melodies.\nFinally, we distinguish the tasks of composition frominterpretation and are concerned here only with the former.\n2. EXISTING WORK\nWe address the problem of music evaluation by using com-\nputational techniques to investigate model output in com-\nparison to human-composed corpora, but also in terms of\nmodel self-consistency. This analysis is useful for deter-\nmining which model output is truer to a corpus, and also\nfor discovering more precisely how the models differ. Our\nhypothesis is that they differ to a degree that is statistically\nsigniﬁcant, and that this difference has an effect that is per-\nceptible and can be described as a musical bias.\nIn previous work on computational evaluation, Manaris\net. al. [13] use artiﬁcial ‘music critics’ based on power-law\nmetrics trained on a corpus as a ﬁtness function for an evo-\nlutionary generative model. This example, as well as oth-\ners, generally do not consider the comparative analysis of\ndifferent music generation formalisms. On the other hand,\nBegleiter et. al. [4] compare the output of different VOMM\nalgorithms in the music domain with the goal of measur-\ning performance in terms of prediction; i.e., how precisely\na model is able to reproduce the corpus. In this case the\ndistinction is that the task of prediction is not the same as\nours of comparing models that generate novel pieces in the\nsame style.\nThe evaluation provided by Pearce and Wiggins [21]\nis very relevant to our work; they empirically compare\nthe output of three variants of Markov models trained on\nchorale melodies using musical judges. The stepwise re-\ngression also described provides directions for improving\nthe models by indicating the quantiﬁable musical features\nwhich are most predictive in their failure.\nOur approach empirically compares the output of three\nmethodologically distinct corpus-based music generation\nmodels (statistical, pattern-matching and cognitive), with-\nout the intervention of human listeners. One advantage of\nthis is that, by avoiding listening studies, methodologies\nmay be developed that models can incorporate for intro-\nspection in real-time generation. We also provide here a\nsimple and alternative technique for aiding the develop-\nment of the models using decision trees.\n3. META-MELO\nThe generative system implemented in MAX/MSP, which\nis independent from the comparative methodology, is avail-\nable for download together with the training corpus and a\nmore detailed model description than is provided here [22].\nWe follow with a presentation of this system, other com-\nponents used are shown in Figure 1.\n3.1 Music Representation and MIDI Parsing\nThe system uses a simple but ﬂexible representation for\nlearning melodic (monophonic) music inspired by the multi-\nviewpoint approach proposed by Conklin and Witten [7].\nThe symbols used for the Markov model and Factor Or-\nacle systems are derived from the concatenation of pitch\ninterval and onset interval information. Several attributescan be used to train the models, which brings immediate\nchallenges for the evaluation and comparison methodol-\nogy. The approach we have chosen is to restrict the study\nand the description of the system, for the purpose of com-\nparing the models in a controlled setting.\nIf the algorithms of the models are implemented in a\nsimple form, we expect to achieve a more transparent com-\nparison, but with likely less interesting musical output,\ntherefore reducing the value of the analysis. On the other\nhand, if more sophisticated implementations with musical\nheuristics are used for improving musical output, we will\nobtain results which are of poor generalization power with\nregards to the underlying models. It is worth noting that\nnone of the models contains an explicit model of tonality,\nthis is one of the reasons that we focus here on an analysis\nof the folk corpus: the relative harmonic and modulatory\nsimplicity mitigates this issue.\nFor the Markov and Factor Oracle implementations, the\nset of attributes is indexed and a dictionary is built for the\nset of symbols corresponding to that attribute. This way,\nwhen an event is observed which has not appeared before,\na new entry is created in the dictionary. An important func-\ntion in this initial stage is the quantization of temporal val-\nues to a reasonable minimum resolution of sixteenth notes.\nThis allows the parsing function to: (1) group and classify\nsimilar events as the same element in the dictionary and\n(2) avoid creating entries with negligible differences since\nnotation uses quantized representations.\nWe parse the melody by individual note events rather\nthan grouping by beats or bars for the purpose of obtaining\nan event-level granularity. Therefore there is no preserva-\ntion of beat or measure information. For example, if there\nare two eighth-notes in a beat, we create an event for each\nnote (note level) rather than one event with two notes (beat\nlevel). This will make evident certain biases that may oth-\nerwise be masked by parsing musical segments.\nSince MusiCOG is a cognitive model [15], it handles\nthe parsing of MIDI input using principles of music per-\nception and cognition which are not included in the other\ntwo models, therefore not requiring some of the prelimi-\nnary parsing described above.\n3.2 Corpus\nThe corpora consist of monophonic MIDI ﬁles from Clas-\nsical, Jazz, Popular, and Folk songs. These classes were\nselected for the purpose of investigating model behaviour\nin contrasting musical settings. We use a Finnish folk song\ncollection that is available for research purposes [10], and\nmanually created the other corpora by extracting melodies\nfrom MIDI ﬁles freely available on the web. For matters\nof space we present here an analysis on a subset of the folk\ncorpus alone, the complete collection of corpora is avail-\nable for download [22]. A subset of 100 pieces of 16 bars\nin length was selected from this corpus, totalling \u00186400\nnotes. It consists of 94 combined pitch-time interval types,\ntherefore a total of \u001870 samples of transitions, assuming\na uniform distribution.4. OVERVIEW OF THE MODELS\n4.1 Markov Models\nMarkov Chains are a widely used statistical approach. Two\nwell known real-time systems implementing these tech-\nniques are Zicarelli’s “M” [24] and Pachet’s “Continua-\ntor” [19]. The theoretical basis lies in the ﬁeld of stochas-\nticswhich studies the description of random sequences de-\npendent on a time parameter t. In their most basic form\nMarkov Chains describe processes where the probability\nof a future event Xt+1depends on the current state Xtand\nnot on previous events. In this way a sequence of musi-\ncal notes can be analyzed to obtain a set of probabilities\nwhich describe the transitions between states: in this case,\ntransitions between musical events.\nAs described by Conklin [6], perhaps the most common\nfrom of generation from Markov models is the so-called\n“random walk,” in which an event from the distribution is\nsampled at each time step, given the current state of the\nmodel. After each selection the state is updated to reﬂect\nthe selection. The memory ororder of the model is the\nnumber of previous states that is considered, and thus de-\nﬁnes the order of the Markov Chain. We implement a Vari-\nable Order Markov Model (VOMM) with a variability of\n1-4 events.\n4.2 Factor Oracle\nSince music can be represented in a symbolic and sequen-\ntial manner, pattern-matching techniques can be useful for\nthe learning and generation of pattern redundancy and vari-\nations, respectively. The Factor Oracle [1] is one example\nof a text and/or biological sequence search algorithm that\nhas been applied to music. It is an acyclic automaton with\na linear growth in number of transitions with regards to the\ninput pattern, which has been utilized in string searches [3].\nThere exists a construction mechanism [1] allowing the al-\nphabet to be grown sequentially and the complete structure\nincremented online, thus allowing for the search of string\nfactors in real-time.\nIt is important to note that neither the Markov Model\n(MM) nor the Factor Oracle (FO) will ever generate a tran-\nsition that is not in the corpus. Also, it is conceivable that\nknowledge of the formal properties of each model could\nbe used to evaluate model performance. However, as the\ncorpus grows in size, knowledge of the formal properties\nof the models alone is not of much aid in predicting their\nbehaviour: hence the need for an empirical evaluation.\n4.3 MusiCOG\nMusiCOG, created by Maxwell [15], models perceptual\nand cognitive processes, with a special focus on the for-\nmation of structural representations of music in memory.\nThe architecture is designed for the learning and genera-\ntion of musical material at various levels (pitch, interval\nand contour), with an emphasis on the interaction between\nshort- and long- term memory systems during listening and\ncomposition. As a cognitive model, with a complex hierar-\nchical memory structure, there are many possible ways togenerate output from MusiCOG. For this study, in order to\nreﬂect a similar systematic approach to the FO and MM,\nand to avoid music theoretical or compositional heuristics,\nwe selected a relatively simple stochastic approach which\nattempts to balance the application of both top-down (i.e.,\nstructural) and bottom-up (i.e., event transition) informa-\ntion.\nMusiCOG (MC) is a feedback system, capable of inter-\npreting its own output and modifying its behaviour accord-\ningly. As an online model, MC will normally learn from\nits own output, but an option to disable this behaviour has\nbeen added and applied to half of the generation examples\nused for all tests in the current study. This was done in or-\nder to bring MC closer in functionality to the MM and FO,\nwithout entirely negating important aspects of its design.\n5. METHODOLOGY\nThe analysis ﬁrst requires extracting features from the in-\nput and output melodies, selecting signiﬁcant features, and\ncalculating similarity measures. Then, k-means and t-tests\nare used for clustering, calculating confusion matrices, and\ndetermining signiﬁcant differences. Finally, we use Classic\nMulti-Dimensional Scaling (CMDS) for further investigat-\ning and interpreting the differences found. Figure 1 depicts\na diagram outlining the methodology proposed.\nMCSVMIDIMIDIMIDI processing & Feature ExtractionAnalytical Resultsk-meansMATLABMELCONVSIMILEMIDI-T oolboxt-testsCMDSMIDIjMIR & MIDItoARFFARFFCorporaMETA-MELOJ48(C4.5)WEKA\nFigure 1 . The components used, META-MELO is the gen-\nerative MAX/MSP system which is independent from the\nmethodology.\nMATLAB is used for most data processing scripts, fea-\nture extraction, CMDS and t-test calculations. SIMILE\n[11] and MELCONV are Windows command line programs\ndeveloped by Frieler for the conversion and comparison of\nMIDI monophonic ﬁles. The Matlab MIDI Toolbox [9]\nis used for a variety of MIDI processing and feature ex-\ntraction functions. WEKA [12] is used for selecting the\nmost signiﬁcant features (C4.5 Java clone: J48) and for\nfurther data analysis and exploration. MIDItoARFF [23]\nandjSymbolic [16], a module of the jMIR toolbox from\nthe same author, are also used for extracting features from\nMIDI ﬁles.\nIn Section 6.1 we describe the use of decision trees for\nselecting the features that best describe the differences in\nthe models. These features are then used in Section 6.2for visualizing the corpus and output of all models. In Sec-\ntion 6.3 and Section 6.4 we describe similarity analysis and\nCMDS respectively, used for evaluating the closeness of\nthe groups of melodies. Finally, we performed pairwise,\none-tailed t-tests for determining statistical signiﬁcance,\ndescribed in Section 6.5.\nWe trained each model with 100, 16 bar pieces from the\nfolk corpus and generated 32 pieces of 16 bars long from\neach model [22].\n6. RESULTS\n6.1 Decision Trees\nWe used WEKA [12] for generating a C4.5 decision tree\nand reducing our feature space. This method was useful\nfor arriving at an initial result in the search for musical\nbiases by detecting which features, amongst the many ex-\ntracted, distinguish the different models from one another\nand from the corpus. Figure 2 shows a tree learned on\nthe output from the models trained on the Folk corpus col-\nlection (CC). The three features arrived at by using the\nC4.5 decision tree are: (1) Compltrans , a melodic original-\nity measure which scores melodies based on a 2nd order\npitch-class distribution (transitions between pitch classes)\nobtained from a set of \u001815thousand classical themes. The\nvalue is scaled between 0 and 10 where a higher number in-\ndicates greater originality. (2) Complebm , an expectancy-\nbased model of melodic complexity which measures the\ncomplexity of melodies based on pitch and rhythm compo-\nnents calibrated with the Essen collection. The mean value\nis 5 and the standard deviation is 1, the higher the value the\ngreater the complexity of the melody. (3) Notedensity , the\nnumber of notes per beat. Details of these features can be\nfound in the MIDI Toolbox documentation [9].\nThe ﬁrst number in the leaf is the count of instances that\nreach that leaf, the number after the dash, if present, indi-\ncates the count of those instances which are misclassiﬁed\nalong with the correct class. Three aspects of the tree stand\nout:\n1. Most of MM, 25 melodies, are classiﬁed as FO and\nare therefore not easy to distinguish.\n2. The root ( Compltrans ) successfully separates 86%\nof FO and MM instances from 89% of CC and 100%\nof MC, an indication of greater similarity between\nCC and MC.\n3. The Notedensity feature seems to greatly aid in clas-\nsifying and distinguishing MC from CC where other\nfeatures are less successful. This type of analysis\nprovides valuable diagnostic insight on the MC model\nsince we can deduce that an increase in note density\non the output would potentially improve the imita-\ntion capabilities of the model.\n6.2 Originality and Complexity\nIn Figure 3 the corpus and the output instances for all mod-\nels are plotted using the Compltrans andComplebm fea-\ntures described in Section 6.1. The plot shows a clear\nCompltransNoteDensityCompltransComplebmComplebmCC (11)FO (58/3cc,25mm)> 7.48<= 7.48\nCC (49/6mc)> 4.22<= 4.5> 4.5<= 4.22> 0.88<= 0.88<= 6.3MC (4/1fo)> 6.3CC (7)CC (30)NoteDensity> 1.04Complebm<= 1.04MM (6/1mc,1fo)> 4.6<= 4.6MC (31/6cc,3mm)Figure 2 . C4.5 decision tree (J48).\n3.5$3.7$3.9$4.1$4.3$4.5$4.7$4.9$5.1$5.3$5.5$\n2.5$3.5$4.5$5.5$6.5$7.5$8.5$9.5$Expectancy*Based*Model*Of*Melodic*Complexity**\nMelodic*Originality*Measure*MC$MM$CC$FO$\nFigure 3 . Expectancy Based Complexity and Originality.\nFolk corpus (CC) and model output.\noverlap between the corpus and MC, whereas MM and FO\ncluster together with higher values on both dimensions.\n6.3 Similarity Analysis\nIt has been noted by M ¨ullensiefen and Frieler [17, 18]\nthat hybrid measures have a greater predictive power than\nsingle-attribute measures for estimating melodic similar-\nity. They provide an optimized metric ‘Opti3,’ which has\nbeen shown to be comparable to human similarity judge-\nments [18]. Opti3 is based on a weighted linear combina-\ntion of interval-based pitch, rhythmic, and harmonic simi-\nlarity estimates, normalized between 0-1, where a value of\n1 indicates that melodies are identical.\nThe corpus and the three sets of model output were ana-\nlyzed to establish the similarity between them (inter-corpus\nanalysis), as well as the diversity within the sets (intra-\ncorpus analysis). We use the Opti3 measure and calcu-\nlate the mean of the distances between each melody fromIntra and Inter-Model Melodic Similarity\nModel MM MC FO CC\nMarkov (MM) .206\nMusiCOG (MC) .183 .208\nFactor Oracle (FO) .166 .165 .198\nFolk Corpus (CC) .178 .174 .154 .201\nTable 1 . Mean melodic similarity of model output and cor-\npora using the “Opti3” similarity measure ( 1:0 =identity).\nIntra-model similarity is represented in the diagonal, lower\nvalue indicates higher diversity.\nthe set in the row against all melodies in the column set\n(cartesian product). Looking at Table 1, we can interpret\nthe diagonal as intra-model dissimilarity, the diversity of\neach set. Since a low value indicates higher diversity, MC\nis marginally the least and FO the most diverse of all sets,\nincluding the corpus. Furthermore, with this analysis, MM\nproduces the output which is most similar to the corpus.\nThis is the ﬁrst discrepancy that we observe in the results.\n6.4 Classic Multi-dimensional Scaling\nCMDS is a multivariate analysis process similar to Prin-\ncipal Component Analysis (PCA), used for visualizing the\nclustering of N-dimensional data. We calculated dissimi-\nlarity matrices from the similarity measures obtained with\n“Opti3”. In Figure 4 we can see that the horizontal axis\nseparates quite generally the corpus from the model out-\nput. Although the dimensions are not easy to interpret, it\nis evident that the models do not explore the full ‘creative’\nrange of the corpus. Correlating with the similarity analy-\nsis, the diversity of FO output is apparent as it occupies a\nbroader range in the space. It is worth noting that a similar\ntopology was observed when scaling a set of 100 melodies\nfrom each model [22].\n!0.4%!0.3%!0.2%!0.1%0%0.1%0.2%0.3%\n!0.4%!0.3%!0.2%!0.1%0%0.1%0.2%0.3%0.4%\nMC%MM%CC%FO%\nFigure 4 . Multi-dimensional scaling for all models and\ncorpus, using the optimized distance metric ‘Opti3’. The\ncorpus collection is marked as ‘CC’.t-test Results\nMM MC FO\nMusiCOG IVdist - -\nFactor Oracle No PCdist -\nFolk Corpus PCdist No PCdist\nTable 2 . T-test results for the folk corpus and model out-\nput, ‘No’ is indicated where no signiﬁcance was found,\notherwise the dimension where signiﬁcance exists. In these\ncases the resulting pvalues are all <0:001.\n6.5 Signiﬁcance Tests\nTable 2 shows the pairwise tailed t-tests that were per-\nformed on the 6 pairs of groupings of corpus and models\nfor determining difference across four dimensions: Pitch-\nclass distributions (PCdist), interval distribution (IVdist),\ncontour and rhythm ( meldistance function [9]). First, as\nin the similarity analysis, the mean distance between all\nmelodies in one set is measured. Second, the distance\nfrom each melody in this set is measured against all the\nmelodies in the set with which it is being compared. Fi-\nnally, the t-test is run on these two sets of measurements\n(further details are available [22]). Where we found signif-\nicance, it was at most in one dimension, either PCdist or\nIVdist. In those cases the pvalue is <0:001, Bonferroni\ncorrected. Results show that (1) MC is signiﬁcantly differ-\nent from both other models but not from the corpus, and\n(2) both MM and the FO are different from the corpus and\nundifferentiated between themselves. Although it is un-\nclear how much these results are affected, the assumption\nof independence is violated in this study and for this reason\nfurther exploration for a suitable analysis is required.\n7. CONCLUSION AND FUTURE WORK\nReturning to our broad deﬁnition of stylistic imitation, we\nexpect successful models to roughly cover the same space\nas the corpus. The CMDS diagram (Figure 4) shows graph-\nically that this is not occurring in our study, which clearly\nshows that the problem of stylistic imitation warrants fur-\nther research.\nWe have also shown that the task of investigating for\nsigniﬁcance in the differences of the output is valuable for\nvalidating closeness to the corpus. The decision trees in-\nform us, in musical features, where the important differ-\nences can be found.\nUnlike VOMM and Factor Oracle which have no mu-\nsical domain knowledge, MusiCOG is informed by music\nperception and cognitive science. This ‘knowledge bias’ in\nMusiCOG may result in output which is more true to the\ncorpus. As such, this encourages the continued investiga-\ntion into developing musical cognitive models.\nWe leave the following for future work: the application\nof the methodology to polyphonic music and models that\ninclude harmonic knowledge, an in-depth analysis of the\noutput of the models when trained on different corpora, and\nan evaluation of the behaviour of the models when combin-\ning stylistically diverse corpora (combinatorial creativity),and the exploration for a more suitable statistical analysis.\n8. ACKNOWLEDGEMENTS\nWe want to thank Klaus Frieler for providing us with his\ninsights and software for melodic analysis as also the re-\nviewers for the valuable comments. This research was\nmade possible by a grant from the Natural Sciences and\nEngineering Research Council of Canada (NSERC).\n9. REFERENCES\n[1] C. Allauzen, M. Crochemore, and M. Rafﬁnot. Factor\nOracle: A New Structure for Pattern Matching. In In\nProceedings of SOFSEM’99: Theory and Practice of\nInformatics , pages 291–306, Berlin, 2009.\n[2] S. Argamon, S. Dubnov, and K. Burns. The Structure of\nStyle: Algorithmic Approaches to Understanding Man-\nner and Meaning . Springer-Verlag, Berlin, 2010.\n[3] G. Assayag and S. Dubnov. Using Factor Oracles\nfor Machine Improvisation. Soft Computing - A Fu-\nsion of Foundations, Methodologies and Applications ,\n8(9):604–610, 2004.\n[4] R. Begleiter, R. El-Yaniv, and G Yona. On Prediction\nUsing Variable Order Markov Models. Journal of Arti-\nﬁcial Intelligence Research , 22:385–421, 2004.\n[5] M. A. Boden. Computer Models of Creativity. In\nHandbook of Creativity . ed. R. J. Sternberg, pages 351-\n372. Cambridge University Press, 1999.\n[6] D. Conklin. Music Generation From Statistical Mod-\nels. In Proceedings Of The AISB 2003 Symposium On\nArtiﬁcial Intelligence And Creativity In The Arts And\nSciences , pages 30–35, 2003.\n[7] D. Conklin and I. H. Witten. Multiple Viewpoint Sys-\ntems for Music Prediction. Journal of New Music Re-\nsearch , 24:51–73, 1995.\n[8] A. Cont, S. Dubnov, and G. Assayag. A Framework\nfor Anticipatory Machine Improvisation and Style Im-\nitation. In Anticipatory Behavior in Adaptive Learning\nSystems (ABiALS) . ABIALS, 2006.\n[9] T. Eerola and P. Toiviainen. MIDI Toolbox: MATLAB\nTools for Music Research . University of Jyv ¨askyl ¨a,\n2004. www.jyu.fi/musica/miditoolbox/ .\n[10] T. Eerola and P. Toiviainen. Suomen Kansan eSavel-\nmat. Digital Archive of Finnish Folk Tunes, 2004.\nhttp://esavelmat.jyu.fi/collection.\nhtml .\n[11] K. Frieler and D. M ¨ullensiefen. The SIMILE\nAlgorithms Documentation. Technical Report.\n2006. http://doc.gold.ac.uk/isms/mmm/\nSIMILE_algo_docs_0.3.pdf . Last visited on\nJuly 2013.[12] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-\nmann, and I. H. Witten. The WEKA Data Mining Soft-\nware: an Update. SIGKDD Explor. Newsl. , 11(1):10–\n18, November 2009.\n[13] B. Manaris, P. Roos, P. Machado, D. Krehbiel, L. Pel-\nlicoro, and J. Romero. A Corpus-Based Hybrid Ap-\nproach to Music Analysis and Composition. In Pro-\nceedings of the 22nd AAAI Conference on Artiﬁcial In-\ntelligence , pages 839–845. AAAI Press, 2007.\n[14] J. B. Maxwell, A. Eigenfeldt, and P. Pasquier. ManuS-\ncore: Music Notation-Based Computer Assisted Com-\nposition. In Proceedings of the International Computer\nMusic Conference (ICMC) , pages 357–364, 2012.\n[15] J. B. Maxwell, A. Eigenfeldt, P. Pasquier, and N. Gon-\nzalez Thomas. MusiCOG: A cognitive architecture for\nmusic learning and generation. Proceedings of the 9th\nSound and Music Computing conference (SMC 2012) ,\npages 521–528, 2012.\n[16] C. Mckay and I. Fujinaga. jSymbolic: A Feature Ex-\ntractor for MIDI Files. In International Computer Mu-\nsic Conference , pages 302–305, 2006.\n[17] D. M ¨ullensiefen and K. Frieler. Cognitive Adequacy\nin the Measurement of Melodic Similarity: Algorith-\nmic vs. Human Judgments. Computing in Musicology ,\n13(2003):147–176, 2004.\n[18] D. M ¨ullensiefen and K. Frieler. Melodic Similarity:\nApproaches and Applications. In Proceedings of the\n8th International Conference on Music Perception and\nCognition (CD-R) , 2004.\n[19] F. Pachet. The Continuator: Musical Interaction With\nStyle. Journal of New Music Research , 32(3):333–341,\n2003.\n[20] P. Pasquier, A. Eigenfeldt, and O. Bown. Proceed-\nings of the First International Workshop on Musical\nMetacreation. In Conjunction with the The Eighth An-\nnual AAAI Conference on Artiﬁcial Intelligence and\nInteractive Digital Entertainment. AAAI Technical Re-\nport WS-12-16. AAAI Press, 88 pages, 2012.\n[21] M. Pearce and G. Wiggins. Evaluating Cognitive Mod-\nels of Musical Composition. In Proceedings of the 4th\nInternational Joint Workshop on Computational Cre-\nativity , pages 73–80, 2007.\n[22] META-MELO. Online Resource. http://\nmetacreation.net/meta-melo/home.html\nLast visited on July 2013.\n[23] D. Rizo, P. J. Ponce de Le ´on, C. P ´erez-Sancho, A. Per-\ntusa, and J. M. I ˜nesta. A Pattern Recognition Approach\nfor Melody Track Selection in MIDI Files. In Proc. of\nthe 7th Int. Symp. on Music Information Retrieval IS-\nMIR, pages 61–66, 2006.\n[24] D. Zicarelli. M and Jam Factory. In Computer Music\nJournal , volume 11, pages 13–29. JSTOR, 1987."
    },
    {
        "title": "A Machine Learning Approach to Voice Separation in Lute Tablature.",
        "author": [
            "Reinier de Valk",
            "Tillman Weyde",
            "Emmanouil Benetos"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418279",
        "url": "https://doi.org/10.5281/zenodo.1418279",
        "ee": "https://zenodo.org/records/1418279/files/ValkWB13.pdf",
        "abstract": "In this paper, we propose a machine learning model for voice separation in lute tablature. Lute tablature is a practical notation that reveals only very limited information about polyphonic structure. This has complicated research into the large surviving corpus of lute music, notated exclusively in tablature. A solution may be found in automatic transcription, of which voice separation is a necessary step. During the last decade, several methods for separating voices in symbolic polyphonic music formats have been developed. However, all but two of these methods adopt a rule-based approach; moreover, none of them is designed for tablature. Our method differs on both these points. First, rather than using fixed rules, we use a model that learns from data: a neural network that predicts voice assignments for notes. Second, our method is specifically designed for tablature—tablature information is included in the features used as input for the models—but it can also be applied to other music corpora. We have experimented on a dataset containing tablature pieces of different polyphonic textures, and compare the results against those obtained from a baseline hidden Markov model (HMM) model. Additionally, we have performed a preliminary comparison of the neural network model with several existing methods for voice separation on a small dataset. We have found that the neural network model performs clearly better than the baseline model, and competitively with the existing methods.",
        "zenodo_id": 1418279,
        "dblp_key": "conf/ismir/ValkWB13",
        "keywords": [
            "machine learning",
            "voice separation",
            "lute tablature",
            "polyphonic structure",
            "automatic transcription",
            "symbolic polyphonic music",
            "rule-based approach",
            "tablature",
            "neural network",
            "features"
        ],
        "content": "A MACHINE LEARNING APPROACH TO VOICE \nSEPARATION IN LUTE TABLATURE \nReinier de Valk Tillman Weyde Emmanouil Benetos \nMusic Informatics Research Group \n Department of Computer Science \nCity University London \n{r.f.de.valk,t.e.weyde,Emmanouil.Benetos.1}@city.ac .uk \nABSTRACT \nIn this paper, we propose a machine learning model for \nvoice separation in lute tablature. Lute tablature is a \npractical notation that reveals only very limited \ninformation about polyphonic structure. This has \ncomplicated research into the large surviving corpus of \nlute music, notated exclusively in tablature. A solution \nmay be found in automatic transcription, of which voice \nseparation is a necessary step. During the last decade, \nseveral methods for separating voices in symbolic \npolyphonic music formats have been developed. \nHowever, all but two of these methods adopt a rule-based \napproach; moreover, none of them is designed for \ntablature. Our method differs on both these points. Fir st, \nrather than using fixed rules, we use a model that learns \nfrom data: a neural network that predicts voice \nassignments for notes. Second, our method is specifical ly \ndesigned for tablature—tablature information is included \nin the features used as input for the models—but it can \nalso be applied to other music corpora. We have \nexperimented on a dataset containing tablature pieces of \ndifferent polyphonic textures, and compare the results \nagainst those obtained from a baseline hidden Markov \nmodel (HMM) model. Additionally, we have performed a \npreliminary comparison of the neural network model with  \nseveral existing methods for voice separation on a smal l \ndataset. We have found that the neural network model \nperforms clearly better than the baseline model, and \ncompetitively with the existing methods. \n1.  INTRODUCTION \nThe lute, an instrument widely used from the early \nsixteenth to the mid-eighteenth century, has left us with  a \nconsiderable corpus of instrumental polyphonic music: \nover 860 print and manuscript sources survive, containing \napproximately 60,000 pieces [12]. This music is notated \nexclusively in lute tablature. Lute tablature is a pra ctical \nnotation that provides no direct pitch information and \nonly limited rhythmic information, but instead instruct s \nthe player where to place the fingers on the fretboar d and \nwhich strings to pluck (see Figure 1). It reveals very lit tle \nabout the polyphonic structure of the music it encodes, \nsince it specifies neither to which polyphonic voice th e tablature notes belong, nor what their individual durat ions \nare. Lute tablature’s “alien nature” [5] is the princi pal \nreason why, apart from a number of specialist studies, \nthis large and important corpus has so far escaped \nsystematic musicological research. \n \n \nFigure 1.  Excerpt of lute tablature in Italian style. \nTranscription into modern music notation—a format \nmuch more familiar to the twenty-first-century schola r or \nmusician—will increase the accessibility of the corpus, \nand, in fact, is the current modus operandi among those  \nstudying lute music. Transcribing tablature, however, is a \ntime-consuming and specialist enterprise. Automatic \ntranscription into modern music notation may provide a \nsolution. An important step in the process of (automatic)  \ntranscription of polyphonic music is voice separation, i. e., \nthe separation of the individual melodic lines (‘voices ’) \nthat together constitute the polyphonic fabric. Using \nmachine learning techniques, we have developed two \nmodels for voice separation in lute tablature—a neural \nnetwork model and a baseline hidden Markov model \n(HMM) model—which, with some modifications, can \nalso be applied to other music corpora.  \nThe outline of this paper is as follows: in Section 2, th e \nexisting methods for voice separation are discussed. In \nSection 3 the proposed models are introduced, and in \nSection 4 the dataset is presented. Section 5 is dedicate d \nto the evaluation of the models; in Section 6 the result s \nare discussed; and in Section 7 the performance of the \nneural network model is compared with that of several \nexisting methods. Concluding thoughts are presented in \nSection 8. \n2.  RELATED WORK \nDuring the last decade, several methods for separating \nvoices in symbolic polyphonic music formats have been \ndeveloped. 1 Except for two, described further below, all \nof these methods are rule-based. More concretely, they \nare based on at least one of two fundamental perceptual \nprinciples that group notes into voices, which have bee n \n                                                        \n1 In addition, a number of methods for voice separat ion in music in \naudio format exist—these, however, are left out of c onsideration here.   \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page.  \n© 2013 International Society for Music Information Retrieval    \n \nlabelled the Pitch Proximity Principle and the Temporal \nContinuity Principle by Huron [6]. These principles \nimply that the closer notes are to one another in ter ms of \npitch or time, respectively, the more likely they are \nperceived as belonging to the same voice. In addition, \nsome of the methods include supplementary perceptual \nprinciples. Although these methods vary considerably in \ntheir approach, in each of them, the perceptual principl es \nit is based on guide the voice assignment procedure.  \nTemperley [17] adopts an approach based on four \n‘preference rules,’ i.e., criteria to evaluate a possi ble \nanalysis. Two of these match the abovementioned \nprinciples; the other two prescribe to minimise the \nnumber of voices (New Stream Rule) and to avoid shared \nnotes (Collision Rule). Cambouropoulos [1] briefly \ndescribes an elementary version of a voice separation \nalgorithm based on the (Gestalt) principle of pitch \nproximity only. Chew and Wu [4] use a ‘contig’ \napproach, in which the music is divided into segments \nwhere a constant number of voices is active (the conti gs). \nThe voice fragments in the segments are then connected \non the basis of pitch proximity; voice crossings are \nforbidden. Szeto and Wong [16] consider voices to be \nclusters containing events proximal in the pitch and ti me \ndimensions, and model voice separation as a clustering \nproblem. The aim of their research, however, is to desi gn \na system for pattern matching, and not one for voice \nseparation. In their method, voice separation is only a \npre-processing step that prevents “perceptually \ninsignificant stream-crossing patterns” from being \nreturned by the system. Kilian and Hoos [9] present an \nalgorithm that is not intended primarily for correct voi ce \nseparation, but rather for creating “reasonable and \nflexible score notation.” Their method allows for \ncomplete chords in a single voice. In the method \npresented by Karydis et. al  [8], too, a ‘voice’ is not \nnecessarily a “monophonic sequence of successive non-\noverlapping notes” [2]. Rather, they prefer to use the te rm \n‘stream,’ which they define as “a perceptually \nindependent voice consisting of single or multi-note \nsonorities.” Hence, in addition to the ‘horizontal’ pi tch \nand time proximity principles, they include two ‘vertical  \nintegration’ principles into their method: the Synchro nous \nNote Principle (based on Huron’s Onset Synchrony \nPrinciple) and the Principle of Tonal Fusion (based on  \nHuron’s Tonal Fusion Principle). A new version of this \nalgorithm is described in Rafailidis et al. [14]. Madsen \nand Widmer [11], lastly, present an algorithm based \nprimarily on the pitch proximity principle, with some \nheuristics added to handle unsolved situations.  \nIn the remaining two methods, then, machine learning \ntechniques are used. Kirlin and Utgoff [10] describe a \nsystem that consists of two components: a ‘predicate,’  \nimplemented as a learned decision tree, that determines \nwhether or not two notes belong to the same voice, a nd a \nhard-coded algorithm that then maps notes to voices. \nJordanous [7] adopts a probabilistic approach based on a \nMarkov model, and presents a system that learns the \nprobability of each note belonging to each voice, as w ell as the probability of successive note pairs belonging t o \nthe same voice.  \nIn addition to these more recent methods, another rule-\nbased method—one designed specifically for automatic \ntranscription of German lute tablature—was developed as \nearly as the 1980s by Charnassé and Stepien [3]. In their \nresearch an approach was followed that combines expert \nknowledge encoded as rules with simpler heuristics. \nAlthough the results appear to be promising, the research \nseems to have ended prematurely.  \n3.   PROPOSED MODELS \nWe have implemented two models for voice separation in \ntablature. The first uses a discrete hidden Markov model \n[13] to predict voice assignments for complete chords; \nthe second uses a neural network (NN) to predict voice \nassignments for individual notes. The HMM model, in \nwhich the tablature chords are the only observations, is \nstraightforward and functions as a baseline model to \ncompare the neural network model against.  \nIn our method, as in most existing methods, we use the \nnotion of voice as a monophonic sequence of notes. In \ncontrast to most rule-based methods, however, we allow \nvoice crossings and shared notes (notes where two voices \nmeet at the unison), both of which are perceptually \nproblematic, but encountered frequently in polyphonic \nlute music. (This goes in particular for shared notes, \nwhich, especially in denser polyphony, are difficult to \nrealise technically on the lute. Although actual unisons  \nare sometimes used, a more idiomatic solution is to fin ger \nonly one note of the unison—a technique also witnessed \nin keyboard music. Such notes shall henceforth be \nreferred to as ‘shared single notes.’) Furthermore, unl ike \nmost existing methods, we assume in advance a \nmaximum number of possible voices (five). 2  \n3.1  HMM Model \nWe have used an HMM model in which the observations \nare the tablature chords, and the hidden states are the \nvoice assignments. Each chord c is represented by a \nvector of pitches (MIDI numbers), depending on the \nnumber of notes in the chord ranging in length from 1 t o \n4; each voice assignment qt for a given time frame t is \nrepresented by a vector of length 4. Here, each vector \nindex represents a voice and can take the values -1, …  , 3, \nwhere -1 denotes inactivity of the voice, and one of the  \nother numbers the sequence number in the chord of the \npitch that is assigned to that voice.   \nFor each training set used in cross-validation, we have \ncreated a transition probability matrix P(qt+1 |q t), denoting \nthe probability of having transitions between various  \nvoice assignments, an observation probability matrix \nP(ct|q t), denoting the probability of encountering chord ct \ngiven voice assignment qt, and an initial state distribution \nP(q1). Since a training set might contain no instances of  \ncertain chord-voice assignment combinations, we have \nmodified P(c t|q t) by including a small non-zero \n                                                        \n2 Because of technical and physical limitations of l ute and lutenist, more \nvoices are rare in lute music.    \n \nprobability for all cases where the number of pitches  in a \nchord is the same as the number of assigned pitches i n a \nvoice assignment. This way, we discourage the prediction \nof voice assignments in which too few or too many \npitches are assigned. Finally, the optimal voice \nassignment sequence is computed using the Viterbi \nalgorithm [13].  \nIt should be noted here that our HMM model is similar \nto Jordanous’s system, as described in [7]. Firstly, both  \nare probabilistic approaches, and second, only pitch-\nrelated observations from the training data are used. Th e \nmain difference between her system and our HMM model \nis that in the former, a Markov chain with an ad-hoc cost  \nfunction based on learned transition probabilities is used. \nJordanous herself notes that “[i]t would be interesting to \napply Hidden Markov Models . . . so that more of the \npreviously allocated notes can be used to assist in voice  \nallocation.” \n3.2  Neural Network Model \nIn the neural network model, the task of voice separation  \nis modelled as a classification problem where every \ntablature note is assigned to a voice—or, in the cas e of a \nshared single note, to two voices. We used a standard \nfeed-forward neural network with resilient \nbackpropagation (Rprop) [15] and sigmoid activation \nfunction, which provides a proven fast and robust \nlearning model. 3 The network consists of an input layer \nof 32 neurons, one hidden layer of 8 neurons, and an \noutput layer of five neurons, each of which represents a \nvoice. Having five output neurons enables us to use the \nnetwork for five-voice lute music;  however, because we \nare currently using a four-voice dataset, at the moment \nthe fifth neuron is never activated. Using the sigmo id \nfunction, the individual output neurons all have activation  \nvalues between 0 and 1; the neuron that gives the highes t \nactivation value determines the voice assignment \ndecision. Prior to the actual training and testing, we hav e \noptimised the regularisation parameter λ (0.00003) and \nthe number of hidden neurons (8) using a cross-validated  \ngrid search.  \nUsing cross-validation and regularisation, we have \ntrained in three runs, where each run consisted of 200 \ntraining epochs and the network weights were re-\ninitialised randomly at the start of each run. The model  \nfrom the training run in which the lowest error rate (s ee \nSection 5) was obtained, was selected for the validatio n \nstage.  \nIn the validation stage, the model traverses the \ntablature note by note, from left to right (always st arting \nwith the lowest note in a chord), and assigns the note s to \nvoices. The test process is linear, and previous voice \nassignments are not revised—except when an assignment \nconflict arises within a chord, i.e., when a note is \nassigned to a voice that was already assigned a note in the \nchord. Because we do not allow two notes within a cho rd \nto be assigned to the same voice, conflicts are sol ved \nusing a heuristic that reassigns the current note to a y et \n                                                        \n3 We use the implementation provided by the Encog fr amework. See \nhttp://www.heatonresearch.com/encog  (accessed May 2013).  unassigned voice. Since we have encountered only two \nconflicts in our experiments, we will not go into further \ndetails on this heuristic here. We assume that the low \nnumber of conflicts is due to the fact that the voice s \nalready assigned in the chord are given as a feature to the \nnetwork (see next section).  \n3.2.1  Features \nA 32-dimensional feature vector is generated for each \ntablature note, which contains two types of informati on \n(see Table 1). Features 1-12 contain only tablature \ninformation, and consist of (a) features encoding \ninstrument-technical properties of the note (1-8), and ( b) \nfeatures encoding information about the position of the \nnote within the chord (9-12). Features 13-32 contain \ninformation about the note’s polyphonic embedding: (c) \npitch and time proximities of the note to the previous note \nin each voice at the current onset time (13-27), and (d) \nthe voices that have already been assigned to previous \nnotes in the chord (28-32).  \nThree things should be noted here. First, features 13-\n27 encode, in essence, the principles that were labelled \nPitch Proximity- and Temporal Continuity Principle by \nHuron [6]. Second, for the calculation of features 13-32, \nin addition to tablature information, voice assignment \ninformation is needed. Third, the time window within \nwhich the information is extracted that is used for the \nvoice assignment decision, is presently still rather l imited \nas it reaches back only one note per voice.  \n \nTablature information \nNote information 7. isOrnamentation \n1. pitch  8. isOpenCourse \n2. course Chord information \n3. fret 9. numberOfNotesBelow \n4. minDuration 10. numberOfNotesAbove \n5. maxDuration 11. pitchDistanceToNoteBelow \n6. chordSize 12. pitchDistanceToNoteAbove \nPolyphonic embedding information \nPitch/time proximities 23-27. offsetOnsetProx  \n13-17. pitchProx  Voices already assigned \n18-22. interOnsetProx 28-32. voicesAlreadyAssigned \nTable 1.  Features for the NN model. \n4.  DATASET \nAt the moment, we are focusing on sixteenth-century lute \nmusic—more specifically, on intabulations, lute \narrangements of polyphonic vocal pieces. There are three \nreasons for this choice. First, intabulations are highl y \nrepresentative of the entire sixteenth-century corpus sinc e \nthey then formed the predominant lute genre. Second, \nsince the densest polyphonic structures in lute music are \nfound in intabulations, they constitute a sub-corpus that is \nchallenging for our research. Third, the use of \nintabulations provides an objective way of devising a \nground truth by polyphonically aligning the tablature and \nthe vocal pieces, whose voices are always notated \nseparately. We have thus transcribed a number of \ncarefully selected intabulations into modern music   \n \nnotation, and then converted these to MIDI, storing each  \nvoice in a separate file. The tablature encoding (in .t xt \nformat), together with the MIDI representation of the \nground truth, are given as input to the model.  \nThe dataset currently consists of nine intabulations,  all \nfor four voices (the most common intabulation format),  \nand contains pieces of different polyphonic texture: three \nimitative pieces, three ‘semi-imitative’ pieces (pieces  that \ncontain points of imitation, but whose structure is not  \ngoverned by them), and three free pieces. It comprises a \ntotal of 8892 notes divided over 5156 chords, single-note \nchords included (Table 2). \n \nPiece Texture Notes Chords \nOchsenkun 1558, \nAbsolon fili mi imitative 1184 727 \nOchsenkun 1558, In exitu \nIsrael de Egipto  imitative 1974 1296 \nOchsenkun 1558, Qui \nhabitat imitative 2238 1443 \nRotta 1546, Bramo morir free 708 322 \nPhalèse 1547, Tant que \nuiuray  free 457 228 \nOchsenkun1558, Herr \nGott laß dich erbarmen  free 371 195  \nAbondante1548, Mais \nmamignone semi-\nimitative 705 316 \nPhalèse 1563, Las on \npeult semi-\nimitative 777 395 \nBarbetta 1582, Il nest \nplaisir semi-\nimitative 478 234 \nTotals  8892 5156 \nTable 2.  The dataset used for the experiments. \n5.  EVALUATION \n5.1  Evaluation Metrics \nOur main evaluation metric is the error rate, which is the \npercentage of notes assigned to an incorrect voice. Th e \nerror rate is calculated by comparing, for each note, t he \npredicted voice assignment with the ground truth voice \nassignment. For the NN model, we use two modes of \nevaluation. In test  mode, we calculate the feature vectors \nwith which the model is evaluated using the ground truth \nvoice assignments. In  application  mode, which \ncorresponds to the ‘real-world situation’ where the \nground truth voice assignments are not provided, we \ncalculate the feature vectors using the voice assignment s \npredicted by the model. In application mode errors can \npropagate—once a note has been assigned to the wrong \nvoice(s), this will influence the decision process for  the \nassignment of the following notes or chords—typically \nresulting in higher error values. We thus distinguish \nbetween the test error , which is the error rate in test \nmode, and the application error , the error rate in \napplication mode. For the HMM model, we evaluate \nusing only a single metric that corresponds to the \napplication error in the NN model. \nFurthermore, for both models we use a tolerant and a \nstrict approach for calculating errors—a distinction that  \napplies to how shared single notes are handled. We distinguish between fully correct assignments (C), fully \nincorrect assignments (I) and three additional mixed \ncategories: one voice assigned correctly but the other \noverlooked (O); one voice assigned correctly but anothe r \nassigned superfluously (S); and one voice assigned \ncorrectly but the other assigned incorrectly (CI). Al l \npossibilities are listed in Table 3. In the tolerant \nevaluation approach, then, O, S, and CI are not counted \nas errors; in the strict approach they are counted as 0.5 \nerrors.  \n \nError category P(n) G( n) Possibility \nC O S CI I \nP is G  X     1 1 \nP is not G     X \nP is one of G   X    1 2 \nP is none of G     X \none of P is G   X   2 1 \nnone of P is G      X \nboth P are G X     \none of P is G     X  2 2 \nnone of P are G      X \nTable 3.  Error categories (P( n) = predicted voice(s) for \nnote n; G( n) = ground truth voice(s) for note n). \n5.2  Results \nWe have trained and evaluated both models on the \ncomplete dataset using nine-fold cross-validation, where \nthe folds correspond to the individual pieces in the data set \nand each piece serves as test set once. The results ar e \ngiven in Table 4. \n \nTolerant approach Strict approach  \nError (%) Std. dev. Error (%) Std. dev. \nNN model, test error \n11.52 3.41 12.87 3.63 \nNN model, application error \n19.37 5.43 20.67 5.61 \nHMM model, application error \n24.95 6.59 25.64 6.69 \nTable 4.  Averaged error rates (weighted) and standard \ndeviation in cross-validation. \n6.  DISCUSSION \nThe performance of the models is compared by means of \nthe application error rates. We see that the NN model  \noutperforms the HMM model by about 5 percentage \npoints—both when the tolerant and when the strict \napproach is applied. While the application error gives a \nrealistic idea of how well the NN model actually \nperforms, it is also interesting to have a look at the test \nerror, which reflects the performance of the model wh en \n‘perfect’ context information—context information \nderived directly from the ground truth voice \nassignments—is provided. A comparison of the test and \napplication mode informs us about error propagation in \nthe application mode. On the individual pieces, the test   \n \nerrors are approximately between one half and two-thir ds \nthe size of the application errors, meaning that each  \nmisassigned note propagates 0.5-1.0 times. The high \napplication errors might be explained at least partly by \nthe observation that the pieces with high application \nerrors contain many longer ornamental runs consisting of  \nsingle notes, which are highly characteristic for lute \nmusic. Thus, when the first note of such a run is ass igned \nto an incorrect voice, the following notes are very l ikely \nto be assigned to that voice as well. Because in such ca ses \nall notes are considered incorrect, single errors can \npropagate dramatically. However, the run as a whole will  \nbe assigned to a single voice, which is still a musica lly \nreasonable choice. This can be reflected using different  \nevaluation metrics such as soundness and completeness \n(see Section 7).  \nWe also observe that both models have problems \nhandling shared single notes. In the NN model, 118 of the \n129 shared single notes in the ground truth are assigned \nto only a single voice in test mode, and 114 in application  \nmode. Moreover, 120 notes are superfluously assigned to \na second voice in test mode, and 117 in application mode. \nWe are currently using a simple heuristic to determine \nwhether a note should be assigned to two voices: if t he \nsecond highest activation value in the network output \ndoes not deviate more than 5.0% (the ‘deviation \nthreshold’) from the highest activation value, the note  is \nassigned to both corresponding voices. Although the \ncurrent threshold leads to balanced results (118/114 \nshared single notes assigned erroneously to a single \nvoice, versus 120/117 non-shared notes assigned \nsuperfluously to two), the method for determining shared \nsingle notes could be improved. In the HMM model, \nthen, the number of shared single notes assigned \nerroneously to a single voice is in the same range (95); \nthe number of notes assigned superfluously to two voices, \nhowever, is much lower (27). With respect to handling \nshared single notes, the HMM model overall thus \nperforms better. \nVoice crossings constitute another problem. An \ninformal inspection shows that, in both models, most \nvoice crossings are not detected. In the NN model, the \nmain reason for this is that our features by design provide  \nlittle support for voice crossings. This might be improve d \nby including a ‘melodic Gestalt criterion’ in the form of \nfeatures that represent melodic shape in the model. The \ninclusion of such features goes hand in hand with an \nincrease of the information extraction window. \n7.  COMPARISON \nWe have compared our NN model with several of the \nexisting methods for voice separation for which results \nand evaluation metrics are documented [4, 7, 10, 11, 14]. \nUsing the same cross-validated procedure as above, but \nnow excluding tablature-specific features such as course \nand fret, we have trained and tested the NN model on a \nsmall dataset that is comparable to those used in the \nabove methods, and then evaluated the results using the \ndifferent evaluation metrics proposed. It must be noted \nthat the results of the comparison are only indicativ e, as the datasets used are similar but not identical and not a ll \nevaluation metrics are defined in detail.  \nOur dataset consists of the first five three-voice an d \nthe first five four-voice fugues of book I of Johann \nSebastian Bach’s Wohltemperirtes Clavier .4 This \ncollection of 48 preludes and fugues has been used, in \ntotal or in part, as the test set in most other meth ods we \ncompare with—the only exception being the one \ndescribed in [10], where the model is trained and tested \non excerpts of the (stylistically comparable) chaconne \nfrom Bach’s second violin partita (BWV 1004).  \nTo enable a comparison we use five evaluation \nmetrics: precision and recall, defined in [7] as “the \npercentage of notes allocated to a voice that correctly  \nbelong to that voice” (precision) and “the percentage of \nnotes in the voice that are successfully allocated to that \nvoice” (recall); soundness and completeness, defined in \n[10] as the percentage of adjacent note pairs in a \npredicted voice of which both notes belong to the same \nground truth voice (soundness) and, conversely, the \npercentage of adjacent note pairs in a ground truth voice \nof which both notes have been assigned to the same \npredicted voice (completeness); and Average Voice \nConsistency (AVC) as used by [4], which measures, “on \naverage, the proportion of notes from the same voice t hat \nhave been assigned . . . to the same voice.”  \n \nEvaluation metric (%)  Dataset \nP R S C A \nNN 10 fugues  \n(3-4vv) 83.12 \n 83.12 \n 94.07 \n 93.42 \n 82.67 \n \n[4]  48 fugues \n(3-5vv)     84.39 \n[7]  45 fugues \n(3-4vv) 80.88 80.85    \n[10]  Bach \nchaconne   88.65 65.57  \n[11] 30 Bach \nInventions  \n(2-3vv);  \n48 fugues \n(3-5vv)   95.94 70.11  \n[14]  4 fugues \n(3-4vv)  92.50    \nTable 5.  Comparison of the NN model with other \nmethods (P = precision; R = recall; S = soundness; C =  \ncompleteness; A = Average Voice Consistency).5  \nAs can be seen in Table 5, the results obtained by our NN  \nmodel are in a similar range as those reported for the \nother models, and at times better. Moreover, with an \napplication error of 16.87% (and a test error of 4.00%), \nthe NN model performs better than on tablature (cf. Ta ble \n4).  \n                                                        \n4 The dataset (in the form of MIDI files) was retrie ved from \nwww.musedata.org  (accessed July 2013).  \n5 In [11] it is stated that soundness and completene ss “as suggested by \nKirlin [and Utgoff]” were used as evaluation metric s; however, the \ntextual definitions given differ. We have not yet b een able to clarify this \ninconsistency, so we present the numbers and metric s exactly as in [11]. \n[14] use ‘accuracy’ as metric, whose definition mat ches that of recall.    \n \n8.  CONCLUSIONS AND FUTURE WORK \nIn this paper we propose a neural network model for \nvoice separation in lute tablature. This model is more \nflexible than the existing rule-based models in that it \nadapts to the data, and thus is less restricted with re gard \nto what needs to be fixed as a priori rules. The model \nclearly outperforms the baseline HMM model and also \nseems to be more robust. In addition, it performs \napparently competitively with the existing voice \nseparation methods we have compared it with; however, \nextended tests will be needed for a systematic \ncomparison. Although there is still room for \nimprovement, the results are sufficiently promising to  \ncontinue experimenting—not only with NN models, but \nalso with different HMM models. Issues that need to be \nsolved in particular are the high error propagation in the \nNN model’s application mode, which currently \ncomplicates a real-world application, the handling of \nshared single notes, and the detection of voice cross ings.     \nIn future work, we will therefore extend the current \nNN model by including more features and by expanding \nthe information extraction window. Additionally, we \nhave started working on an approach that does not assign \nindividual notes, but rather complete chords, to voices.  \nWith regard to the HMM model, we will experiment with \nmore complex models using Gaussian mixture HMMs \nand factorial HMMs. Lastly, we are planning to work \ntowards a more comprehensive and rigorous comparison \nof voice separation methods.      \n9.  ACKNOWLEDGEMENTS \nReinier de Valk is supported by a City University London \nPhD Studentship and Emmanouil Benetos is supported by \na City University London Research Fellowship.   \n10.  REFERENCES \n[1]  E. Cambouropoulos: “From MIDI to Traditional \nMusical Notation,” Proceedings of the AAAI \nWorkshop on Artificial Intelligence and Music , n.p., \n2000.     \n[2]  E. Cambouropoulos: “ ʻVoice’ Separation: Theo-\nretical, Perceptual and Computational Perspectives,” \nProceedings of the 9th International Conference on \nMusic Perception and Cognition , pp. 987-997, 2006. \n[3]  H. Charnassé and B. Stepien: “Automatic \nTranscription of German Lute Tablatures: An \nArtificial Intelligence Application,” Computer \nRepresentations and Models in Music , Ed. A. \nMarsden and A. Pople, Academic Press, London, pp. \n144-70, 1992. \n[4]  E. Chew and X. Wu: “Separating Voices in \nPolyphonic Music: A Contig Mapping Approach, ˮ \nComputer Music Modeling and Retrieval: Second \nInternational Symposium, Revised Papers , Ed. U. K. \nWiil, Springer, Berlin, pp. 1-20, 2004. [5]  J. Griffiths: “The Lute and the Polyphonist,” Studi \nMusicali , Vol. 31, No. 1, pp. 89-108, 2002. \n[6]  D. Huron: “Tone and Voice: A Derivation of the \nRules of Voice-Leading from Perceptual Principles,” \nMusic Perception , Vol. 19, No. 1, pp. 1-64, 2001. \n[7]  A. Jordanous: “Voice Separation in Polyphonic \nMusic: A Data-Driven Approach,” Proceedings of \nthe  International Computer Music Conference , n.p., \n2008. \n[8]  I. Karydis et al.: “Horizontal and Vertical \nIntegration/Segregation in Auditory Streaming: A \nVoice Separation Algorithm for Symbolic Musical \nData,” Proceedings of the 4th Sound and Music \nComputing Conference , pp. 299-306, 2007. \n[9]  J. Kilian and H. Hoos: “Voice separation—A Local \nOptimisation Approach,” Proceedings of the 3rd \nInternational Conference on Music Information \nRetrieval , n.p., 2002. \n[10]  P. Kirlin and P. Utgoff: “VoiSe: Learning to \nSegregate Voices in Explicit and Implicit \nPolyphony,” Proceedings of the 6th International \nConference on Music Information Retrieval , pp. \n552-557, 2005. \n[11]  S. T. Madsen and G. Widmer: “Separating Voices in \nMIDI,” Proceedings of the 7th International \nConference on Music Information Retrieval , n.p., \n2006. \n[12]  A. J. Ness and C. A. Kolczynski: “Sources of Lute \nMusic,” The New Grove Dictionary of Music and \nMusicians , 2nd ed., Ed. S. Sadie, Macmillan, \nLondon, pp. 39-63, 2001. \n[13]  L. R. Rabiner: “A Tutorial on Hidden Markov \nModels and Selected Applications in Speech \nRecognition,” Proceedings of the IEEE , Vol. 77, No. \n2, pp. 257-286, 1989. \n[14]  D. Rafailidis, E. Cambouropoulos, and Y. \nManolopoulos: “Musical Voice Integration/ \nSegregation: VISA  Revisited,” Proceedings of the \n6th Sound and Music Computing Conference , pp. \n42-47, 2009. \n[15]  M. Riedmiller and H. Braun: “RPROP—A Fast \nAdaptive Learning Algorithm,” Proceedings of the \nInternational Symposium on Computer and \nInformation Science , n.p., 1992. \n[16]  W. M. Szeto and M. H. Wong: “Stream Segregation \nAlgorithm for Pattern Matching in Polyphonic \nMusic Databases,” Multimedia Tools and \nApplications , Vol. 30, pp. 109-127, 2006. \n[17]  D. Temperley: The Cognition of Basic Musical \nStructures , The MIT Press, Cambridge, MA, 2001."
    },
    {
        "title": "A Study of Ensemble Synchronisation Under Restricted Line of Sight.",
        "author": [
            "Bogdan Vera",
            "Elaine Chew",
            "Patrick G. T. Healey"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417363",
        "url": "https://doi.org/10.5281/zenodo.1417363",
        "ee": "https://zenodo.org/records/1417363/files/VeraCH13.pdf",
        "abstract": "This paper presents a quantitative study of musician synchronisation in ensemble performance under restricted line of sight, an inherent condition in scenarios like distributed music performance. The study focuses on the relevance of gestural (e.g. visual, breath) cues in achieving note onset synchrony in a violin and cello duo, in which musicians must fulfill a mutual conducting role. The musicians performed two pieces – one with long notes separated by long pauses, another with long notes but no pauses – under direct, partial (silhouettes), and no line of sight. Analysis of the musicians’ note synchrony shows that visual contact significantly impacts synchronization in the first piece, but not significantly in the second piece, leading to the hypothesis that opportunities to shape notes may provide further cues for synchronization. The results also show that breath cues are important, and that the relative positions of these cues impact note asynchrony at the ends of pauses; thus, the advance timing information provided by breath cues could form a basis for generating virtual cues in distributed performance, where network latency delays sonic and visual cues. This study demonstrates the need to account for structure (e.g. pauses, long notes) and prosodic gestures in ensemble synchronisation.",
        "zenodo_id": 1417363,
        "dblp_key": "conf/ismir/VeraCH13",
        "keywords": [
            "quantitative study",
            "musician synchronisation",
            "ensemble performance",
            "restricted line of sight",
            "distributed music performance",
            "gestural cues",
            "note onset synchrony",
            "mutual conducting role",
            "two pieces",
            "notes separated by long pauses"
        ],
        "content": "A STUDY OF ENSEMBLE SYNCHRONISATION UNDER\nRESTRICTED LINE OF SIGHT\nBogdan Vera, Elaine Chew\nQueen Mary University of London\nCentre for Digital Music\nfbogdan.vera,eniale g@eecs.qmul.ac.ukPatrick G. T. Healey\nQueen Mary University of London\nCognitive Science Research Group\nph@eecs.qmul.ac.uk\nABSTRACT\nThis paper presents a quantitative study of musician syn-\nchronisation in ensemble performance under restricted line\nof sight, an inherent condition in scenarios like distributed\nmusic performance. The study focuses on the relevance of\ngestural (e.g. visual, breath) cues in achieving note onset\nsynchrony in a violin and cello duo, in which musicians\nmust fulﬁll a mutual conducting role. The musicians per-\nformed two pieces – one with long notes separated by long\npauses, another with long notes but no pauses – under di-\nrect, partial (silhouettes), and no line of sight. Analysis\nof the musicians’ note synchrony shows that visual contact\nsigniﬁcantly impacts synchronization in the ﬁrst piece, but\nnot signiﬁcantly in the second piece, leading to the hypoth-\nesis that opportunities to shape notes may provide further\ncues for synchronization. The results also show that breath\ncues are important, and that the relative positions of these\ncues impact note asynchrony at the ends of pauses; thus,\nthe advance timing information provided by breath cues\ncould form a basis for generating virtual cues in distributed\nperformance, where network latency delays sonic and vi-\nsual cues. This study demonstrates the need to account for\nstructure (e.g. pauses, long notes) and prosodic gestures in\nensemble synchronisation.\n1. INTRODUCTION\nIn ensemble performance, musicians rely on a complex\nmixture of non-verbal communication via visual and au-\nditory gestures (such as breathing) and the inherent tim-\ning information present within the acoustic signal of the\nperformance. Together, these cues contribute to the mu-\nsicians’ common perception of musical time, and allows\nthem to synchronize one with another. In certain cases,\nsuch as in distributed music performance, some of these\ncues are disrupted by factors such as network latency, which\nhas been shown to affect synchronisation between musi-\ncians and delay video transmissions so much so as to make\nvisual gestures ineffective. We are, therefore, interested\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.in understanding the effects of disruptions of visual com-\nmunication on ensemble performance, so as to advance re-\nsearch on assistive systems for distributed performance.\nThis paper presents a study on the effect of line-of-\nsight restriction between musicians working to synchro-\nnize onsets and interact in a performance. The remainder\nof the paper is organized as follows: Section 2 reviews re-\nlated work in ensemble interaction and networked perfor-\nmance; Section 3 describes the experimental design; Sec-\ntion 4 presents the analysis; results and conclusions follow\nin Sections 5 and 6.\n2. LITERATURE REVIEW\nMusical gesture analysis has been an area of interest for\nresearchers in music cognition, music performance, and\nhuman-computer interaction. McCaleb [1] compares en-\nsemble interaction to the communication paradigm (likened\nto a telephone or postal service), acknowledging that this\napproach has not yet been critiqued from the perspective\nof a performing musician.\nLeman and Godoy [2] performed a classiﬁcation of mu-\nsical gestures, distinguishing gestures that are part of sound\nproduction from those that are purely communicative and\nthose which simply accompany music (such as dancing).\nLim [3], as a step towards creating a robotic accompani-\nment system for ﬂute, identiﬁed start and end cues which\nserve to visually mark the onsets and offsets of notes, and\nbeat cues which are used to keep time during sustained\nnotes, all of which were motion based. Eye contact has\nalso been discussed as being important in ensemble syn-\nchronisation [4]. Breathing, as a musical gesture, has been\ntouched upon by Vines et al. [5], and mentioned as an im-\nportant cue in conversation, where it helps in coordinating\nturn taking.\nIn the context of network music performance, the ef-\nfects of audio latency itself have been studied by researchers\nsuch as Chafe and Gurevich [6], Chew et al. [7] and Schuett\n[8]. This research shows that when latencies higher than\naround 25 ms are present, the tempo tends to decrease\nand synchronisation is adversely affected. It is not under-\nstood what effects visual isolation has on the performance\nin these cases, though the DIP project reports iniitial ex-\nplorations of this area with attempts to provide distributed\nmusicians with visual cues via video streaming [9]. They\nfound that video latencies are to be too high for the trans-mitted gestural cues to be of much use, especially over\nlong distances. Solutions to the problem of latency based\non prediction were theorized by Chafe [10], and Sarkar’s\nTablaNet project [11] predicts tabla drum players’ strokes\nahead of time in order to create the appearance of zero\ntransmission latency. Further work on predicting drum\nstrokes has recently been done by Oda et al. focusing on\nestimating the velocity of drum mallets with high speed\ncameras and predicting their impact times [12]. Combin-\ning these ideas with Lim’s approach of predicting gestures,\nwe hypothesize that the issue of latency in video trans-\nmission could be ameliorated using predictive modeling of\ngestural cues.\n3. EXPERIMENTAL SET-UP\nTwo simple violin-cello duet pieces1were composed by\nVera for the experiment, and were played by a violin and\ncello duo under three different line of sight conditions. The\nparticipating musicians were both classically trained, and\nactive in chamber orchestras, but had never played together\nbefore. The ﬁrst condition, S1, involved normal perfor-\nmance with no line of sight obstruction, with the musicians\nlocated in their preferred positions. In the second scenario,\nS2, the musicians were made to face in opposite directions,\nremoving line of sight, but allowing auditory gestures such\nas breathing. In the third scenario, S3, a translucent curtain\nwas placed between the musicians, and their shadows were\ncast onto it by two bright studio lamps, allowing them to\nview only each other’s silhouettes, with no ﬁne details such\nas facial expression (see Figure 1). The musicians were\nasked to play the chosen pieces for the ﬁrst time, with little\nrehearsal time.\nThe pieces were specially designed with a focus on key\nexperimental features. The ﬁrst piece is relatively easy to\nplay (i.e. the musicians were not expected to greatly im-\nprove over time and they required almost no rehearsal to\nplay it). It consists of a sequence of very long notes (last-\ning two bars at a moderate tempo) followed by equally long\npauses. In the middle of the piece, the pauses are replaced\nby faster rhythmical dialogues between the performers, be-\nfore returning to the long separations. The aspect explored\nin this piece is the timing between the musicians at the be-\nginning of each note, where they have to cue each other\ninto a new section without relying on rhythmic informa-\ntion, the only exception being the middle section where the\nfast paced rhythms are expected to improve synchronisa-\ntion. The main hypothesis in this case is that lack of visual\ncontact would result in greater asynchrony between the on-\nsets of simultaneously sounded notes, and that asynchrony\nwould be reduced where timing information is carried by\nrhythmic patterns in the music itself.\nThe second piece is a similarly slow paced composi-\ntion, but without pauses. After four bars of solo cello, the\ntwo instruments play simultaneous notes, until later in the\npiece where some counterpoint is introduced between the\nparts. In this case, our hypothesis was that the presence\n1Scores available at http://tinyurl.com/nqhq2ppof a stronger rhythm and lack of pauses will result in less\nasynchrony, compared to the performance of the ﬁrst piece,\nwhen line of sight is affected.\nThe musicians were recorded playing 3 takes of each\npiece (four in the case of the ‘no line of sight’ recordings,\ndue to extra time at the end of the recording sessions), in\neach scenario, over two days. Due to time constraints, the\nmusicians played through each scenarios in sequence, and\nthus some improvement over time is expected as the mu-\nsicians became accustomed to playing the pieces. Both\ninstruments were recorded with attached pickups, in an at-\ntempt to isolate the two instruments as much as possible.\n4. ANALYSIS\nA quantitative analysis of the difference in time between\nthe note onsets of the performers’ simultaneously sounded\nnotes was performed, comparing their performances in the\nthree scenarios. As obtaining reliable note onsets from\nbowed instruments is difﬁcult with automatic methods, the\nonsets were hand annotated using Sonic Visualiser [13].\nEven when annotating onsets by hand, it can be difﬁcult\nto determine the exact onset time of soft notes. Notes\nplayed by bowed instruments have varying attack times,\nand one can, for example, choose either the start of the un-\npitched bowing sound or the moment when a fundamental\nfrequency becomes audible. In this case, the annotation fo-\ncused on the latter feature, using Sonic Visualiser’s adap-\ntive spectrogram to inspect the notes. The resulting set of\nonset time differences was then analyzed in Matlab, simply\nby subtracting the onset times of the violinist from those of\nthe cellist for simultaneous score notes.\n4.1 First Piece: Long Notes, Long Pauses\nFor this ﬁrst piece, the onset annotations for an example\nrecording are shown in Figure 2. In this case the onset\nannotations separate the piece into four-bar segments. No\nfast section onsets were considered for the initial analy-\nses. Fast section annotations marking two bar long sec-\ntions were later added to inspect the effects of rhythmic vs.\nnon rhythmic patterns on synchronization, and they were\ntreated as secondary to the longer notes, allowing a more\nfocused comparison between the onset times of the long\nnotes separated by pauses, and those linked by rhythmic\nsections.\nAsynchrony analysis for the three scenarios is visual-\nized in the box plots in Figure 4, showing the extent of\nasynchrony, which we deﬁne as the unsigned time differ-\nence between the onsets of ideally simultaneously sounded\nnotes, from all the recordings in each scenario. The results\nshow a median asynchrony of 52.1 ms in the normal line\nof sight scenario. This increases to 104.8 ms in the no line\nof sight scenario, showing a worsening of synchrony. In\nthe partial line of sight scenario, the median asynchrony\nwas 46.3 ms, which is slightly lower than in the normal\nline of sight scenario. Table 1 contains the p-values of\npairwise Kolmogorov-Smirnov tests between the scenar-\nios, showing that the scenario with no line of sight wasFigure 1 . The two musicians on either side of the shadow curtain in the partial LoS scenario\nFigure 2 . Example segmentations for one take of the ﬁrst\npiece (top - cello, bottom - violin)\nFigure 3 . Excerpt of the ﬁrst piece showing the long notes\nseparated by pauses before the rhythmical section\nsigniﬁcantly different from the others, and that the normal\nand partial line of sight conditions were not signiﬁcantly\ndifferent. Figure 5 shows the box plots of onset time dif-\nferences, without taking the absolute value. The analysis\nshowed that the violinist tended to play ahead of the cellist\n(i.e. most values are positive). Figure 6 shows median ab-\nsolute onset time difference per segment, for each scenario.\nFrom this graph it is notable that for segments 6, 7 and 8 –\nthe segments linked by rhythmic patterns – the musicians\nseem to have achieved better synchrony than in the rest of\nthe piece.\nScenario Pair S1 vs S2 S1 vs S3 S2 vs S3\nP-Value 0.0222 0.9360 0.0205\nTable 1 . Pairwise Kolmogorov-Smirnov p-values between\nasynchronies in each scenario for the ﬁrst piece\nFigure 4 . Asynchrony boxplots for each scenario for the\nﬁrst piece (y-axis is in seconds)\nFigure 5 . Onset time difference boxplots for each scenario\nfor the ﬁrst piece (y-axis is in seconds)\n4.2 Second Piece: Long Notes, No Pauses,\nCounterpoint\nThe same analysis was performed for the second piece.\nIn this case the segments examined were chosen to cor-\nrespond with all note onsets. Because the violin and cello\nparts contain many notes that do not have simultaneous on-\nsets, segmentation points from each part were replicated\nin the other part by automatically choosing time points at\nappropriate note subdivisions between adjacent segmen-\ntations. This provided a set of estimated segmentations\nbased on available data ensuring that both parts have com-Scenario Pair S1 vs S2 S1 vs S3 S2 vs S3\nP-Value 0.6055 0.0234 0.1072\nTable 2 . Pairwise Kolmogorov-Smirnov p-values between\nasynchronies in each ccenario for the second piece\nparable segmentation points.\nFigure 6 . Asynchrony against segment number for the ﬁrst\npiece\nFigure 7 . Segmentations for the second piece (top - cello,\nbottom - violin)\nAlthough comparing a real onset with an estimated one\ndoes not give a precise value for note onset synchrony, it\ndoes however give an indication of the performers’ degree\nof synchronisation to each other’s timing, i.e. a note played\nby the cellist that starts in the middle of the violinists’ held\nnote would have its onset time compared to the calculated\nmiddle of the violin’s two closest adjacent note onsets. The\nﬁrst four notes, which are played only by the cellist are not\nannotated. The segmentation points (before the addition of\nestimated segmentations) are shown in Figure 7.\nUnlike for the previous piece, the median asynchrony\ndecreases with each scenario, indicating that the effect of\nthe musicians getting better at playing the pieces was more\nsigniﬁcant than that of reduced line of sight. The me-\ndian asynchrony was 80 ms for the baseline scenario, 74.7\nms for the second, and 59.6 ms for the third. The paired\nKolmogorov-Smirnov test results, presented in Table 2, show\nthat there was no signiﬁcant worsening caused by reduced\nline of sight. We instead see a signiﬁcant improvement of\nsynchrony between the ﬁrst and last scenarios.\nFigure 8 . Excerpt of the second piece showing long notes\n(w/o pauses) in the cello joined by long notes in the violin\nFigure 9 . Asynchrony boxplots for each scenario for the\nsecond piece (y-axis is in seconds)\nFigure 10 . Onset time difference boxplots for each sce-\nnario for the second piece (y-axis is in seconds)\n4.3 Use of Breath for Cueing\nIn the recordings of the ﬁrst piece, it was notable that the\nviolinist took highly regular and audible breaths before\neach note following a pause, which the musicians identi-\nﬁed as important cues. To investigate the use of breath, an\nextra set of annotations was created, marking the start and\nend times of each breath, picked by investigating the spec-\ntrograms of the recordings. However, as the breaths do not\nhave clear onsets or offsets, this data may be noisy and us-\nable only at a fairly coarse level. An example annotated\nbreath sound is shown in Figure 12. This is a task that\ncould possibly be automated, for example using an algo-\nrithm like the one presented by Ruinskiy and Lavner [14].\nThe ﬁrst feature of interest was the set of breath start\ntimes as a ratio of pause length, or how far into the pauses\ndid the breaths tend to start. Another problem in this caseFigure 11 . Asynchrony against segment number for the\nsecond piece\nFigure 12 . Example of annotated breath sound on spectro-\ngram\nwas that ﬁnding the start times of the pause segments is\ndifﬁcult as string instruments do not always have distinct\nnote offsets. Annotation of these offsets was based on ﬁnd-\ning the point where the higher harmonics start to decay,\nas the fundamental often had a much longer decay, and\nthe musicians often left one string resonating through the\npause itself. Pause start times were then taken to be the\naverage offsets of the cellist and violinist’s previous notes.\nBreath position with respect to the violinist’s pause offset\n(i.e. note onset) was then expressed as a value between 0\nand 1 representing how far along a pause a breath occurs,\nas shown in Equation 1:\nBviolin;i =2bi;0\u0000ci;0\u0000vi;0\n2vi;1\u0000ci;0\u0000vi;0(1)\nwhere iis the pause index, bi;0is the breath onset time,\nci;0is the cello’s pause onset time, and vi;0andvi;1are\nthe violin’s pause onset and offset times, respectively. We\ncorrespondingly deﬁne Bcello as the breath position with\nrespect to the cello’s pause offset:\nBcello;i =2bi;0\u0000ci;0\u0000vi;0\n2ci;1\u0000ci;0\u0000vi;0: (2)\nFigure 13 shows the histogram of breath start positions\nfor all pauses from all recordings. The violinist mostly\nused breath gestures at the 0.76 point, which closely cor-\nresponds to the last half note in the 2-bar pause. To bet-\nter understand the effect of the breath cues, regression and\ncorrelation analysis was performed. This is shown in Fig-\nure 14, where Bv and Bc represent Bviolin andBcello, and\nFigure 13 . Histogram of breath position.\nFigure 14 . Correlation table of breath and pause variables\n(stars indicate signiﬁcance levels:\u00030:05;\u0003\u00030:01;\u0003\u0003\u00030:001)\nBdur is the duration of the breath as a ratio of the violinist’s\npause length; OTD is the difference between the violin and\nthe cello onset times, and Cdur and Vdur are the durations\nof the violinist’s and cellist’s pauses (a value greater than\nzero means that the violinist played before the cellist).\nFrom this analysis it is notable that the breath position,\ndespite its small range of variation, had a signiﬁcant ef-\nfect on the onset time difference. Later breath positions\nwith respect to the violin’s pause had a slight tendency to\ncorrespond with positive differences (meaning that the vio-\nlinist started ﬁrst), possibly by indicating a later start time\nto the cellist and making her start later. The correlation\nbetween the violinist’s breath position with respect to the\ncello pause time and the onset time difference was much\nmore signiﬁcant, and the two variables are inversely re-\nlated, higher values (nearer 0.75) essentially lowering the\ngap between the musicians’ onsets. We also see a very sig-\nniﬁcant inverse correlation between the breath’s start po-\nsition and the length of the breath sound. We also see a\npositive correlation between the length of the pause (from\nboth musician’s perspectives) and the position of the breath\nalong the pause, meaning that in longer pauses the breath\nstarted later. The correlations of interest are emphasized in\nFigure 14. These characteristics identify breath as a type\nof cue that could be used in a networked scenario to predict\na performer’s intent to begin a note, serving as the basis for\nsynthesis of virtual cues that can be sent ahead of time to\nbypass latency, in a manner similar to the rhythm predic-\ntion in Sarkar’s TablaNet project [9].5. RESULTS\nThe results of this study suggest that line of sight is im-\nportant in achieving good synchronization in a string duo,\nespecially when the music being played contains pauses\nduring which the musicians cannot easily track time. As\nthe partial line of sight scenario did not cause a signiﬁcant\ndecrease in synchrony, it appears that very simple body\nmotion was sufﬁcient for effective gestural cueing. In sce-\nnarios with restricted line of sight, performers can rely on\nnon-visual and extra-musical cues such as breath for syn-\nchronization. In this study, the leading musician issued\nbreath cues that were synchronized to their own perception\nof musical time, and served as advance warnings of note\nonset intent. The following musician then used this cue to\nestimate the beginning of the next note. Small variations in\nbreath onset within pauses were correlated with variations\nin note onset time delay, suggesting that musicians pay\nclose attention to these cues, and that mis-communication\nof timing by breathing too soon or too late can have direct\nconsequences on synchronization.\nWhen the music had no pauses and contained counter-\npoint and rhythm, the musicians did not exhibit worse syn-\nchronization in the absence of visual contact, suggesting\nthat auditory cues embedded in the music itself were sufﬁ-\ncient for synchronization.\n6. CONCLUSIONS\nThe ﬁndings of this study indicate a need for further re-\nsearch into the ﬁne dynamics of cues that are transmitted\nboth visually and sonically. Auditory features of interest\nmay be variations in dynamics or pitch in relation to crit-\nical synchronization points. Due to the improvement seen\nin the partial line of sight scenario, we propose that visual\ncues are likely more dependent on general motion than on\neye contact, facial expression, or other such ﬁne details.\nFurther work should focus on obtaining a larger dataset\nfor study, although the main difﬁculty is obtaining multi-\ntrack, acoustically isolated recordings done in controlled\nconditions. We intend to continue the study of ensemble\nsynchronization by including visual and motion tracking\ndata in our analysis, in order to discover the most impor-\ntant types of visual gestures and their relationship with the\nmusic being performed.\n7. ACKNOWLEDGEMENTS\nThe authors thank Laurel Pardue and Dr. Kat Agres for\nparticipating in the experiment. This project was funded\nin part by the Engineering and Physical Sciences Research\nCouncil (EPSRC).\n8. REFERENCES\n[1] M. McCaleb: “Communication or Interaction? Ap-\nplied environmental knowledge in ensemble perfor-\nmance,” Proceedings of the CMPCP Performance\nStudies Network International Conference , 2011.[2] R. I. Godoy, M. Leman: Musical Gestures Sound,\nMovement and Meaning , Routledge, 2010.\n[3] A. Lim: “Robot Musical Accompaniment: Real-time\nSynchronization using Visual Cue Recognition,” Pro-\nceedings of the IEEE/RSJ International Conference on\nIntelligent RObots and Systems (IROS) , 2010.\n[4] A. Williamon: “Coordinating Duo Piano Perfor-\nmance,” Proceedings of the Sixth International Con-\nference on Music Perception and Cognition , 2000.\n[5] B. W. Vines, M. M. Wanderley, C. L. Krumhansl, R L.\nNuzzo, D. J. Levitin: “Performance Gestures of Mu-\nsicians: What structural and emotional information do\nthey convey?” In A. Camurri, G. V olpe (eds.): Gesture-\nBased Communication in Human-Computer Interac-\ntion, LNCS 2915, Springer, 2004.\n[6] C. Chafe, M. Gurevich “Network Time Delay and En-\nsemble Accuracy: Effects of Latency, Asymmetry,”\nProceedings of the 117th AES Convention , San Fran-\ncisco, 2010.\n[7] E. Chew, A. Sawchuk, C. Tonoue, R. Zimmerman\n“Segmental Tempo Analysis of Performances in User-\nCentered Experiments in the Distributed Immersive\nPerformance Project,” Proceedings of the Sound and\nMusic Computing Conference , 2005.\n[8] N. Schuett “The Effects of Latency on Ensemble\nPerformance,” Undegraduate Honors Thesis, Stanford\nUniversity, 2009.\n[9] A.A. Sawchuk, E. Chew, R. Zimmermann, C. Pa-\npadopoulos and C. Kyriakakis “From Remote Me-\ndia Immersion to Distributed Immersive Performance,”\nProceedings of the ACM SIGMM 2003 Workshop on\nExperiential Telepresence , 2003.\n[10] C. Chafe “Tapping into the Internet as a Musi-\ncal/Acoustical Medium,” Contemporary Music Re-\nview, 2009.\n[11] M. Sarkar “TablaNet: a real-time online musical col-\nlaboration system for Indian percussion,” S.M. Thesis,\nMIT, 2007.\n[12] R. Oda, A. Finkelstein and R. Fiebrink “Towards Note-\nLevel Prediction for Networked Music Performance,”\nProceedings of the 13th International Conference on\nNew Interfaces for Musical Expression , 2013.\n[13] C. Cannam, C. Landone, M. Sandler “Sonic Visualiser:\nAn Open Source Application for Viewing, Analysing,\nand Annotating Music Audio Files,” Proceedings of\nthe ACM Multimedia 2010 International Conference ,\n2010.\n[14] D. Ruinskiy, Y . Lavner “An Effective Algorithm for\nAutomatic Detection and Exact Demarcation of Breath\nSounds in Speech and Song Signals,” IEEE Transac-\ntions On Audio, Speech, and Language Processing,\nVol. 15 , 2007."
    },
    {
        "title": "Optical Measure Recognition in Common Music Notation.",
        "author": [
            "Gabriel Vigliensoni",
            "Gregory Burlet",
            "Ichiro Fujinaga"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417024",
        "url": "https://doi.org/10.5281/zenodo.1417024",
        "ee": "https://zenodo.org/records/1417024/files/VigliensoniBF13.pdf",
        "abstract": "This paper presents work on the automatic recognition of measures in common Western music notation scores using optical music recognition techniques. It is important to extract the bounding boxes of measures within a music score to facilitate some methods of multimodal navigation of music catalogues. We present an image processing algorithm that extracts the position of barlines on an input music score in order to deduce the number and position of measures on the page. An open-source implementation of this algorithm is made publicly available. In addition, we have created a ground-truth dataset of 100 images of music scores with manually annotated measures. We conducted several experiments using different combinations of values for two critical parameters to evaluate our measure recognition algorithm. Our algorithm obtained an f-score of 91 percent with the optimal set of parameters. Although our implementation obtained results similar to previous approaches, the scope and size of the evaluation dataset is significantly larger.",
        "zenodo_id": 1417024,
        "dblp_key": "conf/ismir/VigliensoniBF13",
        "keywords": [
            "Automatic recognition",
            "bounding boxes",
            "measure extraction",
            "music notation",
            "optical music recognition",
            "barlines",
            "multimodal navigation",
            "ground-truth dataset",
            "f-score",
            "optimization"
        ],
        "content": "OPTICAL MEASURE RECOGNITION IN COMMON MUSIC NOTATION\nGabriel Vigliensoni, Gregory Burlet, and Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Technology (CIRMMT)\nMcGill University, Montr ´eal, Qu ´ebec, Canada\nfgabriel,ich g@music.mcgill.ca, gregory.burlet@mail.mcgill.ca\nABSTRACT\nThis paper presents work on the automatic recognition of\nmeasures in common Western music notation scores us-\ning optical music recognition techniques. It is important\nto extract the bounding boxes of measures within a music\nscore to facilitate some methods of multimodal navigation\nof music catalogues. We present an image processing al-\ngorithm that extracts the position of barlines on an input\nmusic score in order to deduce the number and position\nof measures on the page. An open-source implementation\nof this algorithm is made publicly available. In addition,\nwe have created a ground-truth dataset of 100 images of\nmusic scores with manually annotated measures. We con-\nducted several experiments using different combinations of\nvalues for two critical parameters to evaluate our measure\nrecognition algorithm. Our algorithm obtained an f-score\nof 91 percent with the optimal set of parameters. Although\nour implementation obtained results similar to previous ap-\nproaches, the scope and size of the evaluation dataset is\nsigniﬁcantly larger.\n1. INTRODUCTION\nOptical music recognition (OMR) is the process of con-\nverting scanned images of pages of music into computer\nreadable and manipulable symbols using a variety of image\nprocessing techniques. Thus, OMR is seen as a valuable\ntool that helps accelerate the creation of large collections\nof searchable music books.\nHowever, the automatic recognition of printed music\npresents several substantial challenges, including: A large\nvariability in the quality of analog or digital sources; the\ncommon superimposition of shapes within a score on staves,\nmaking it difﬁcult for computers to isolate musical ele-\nments and extract musical features that represent the con-\ntent; and ﬁnally, a large number of music symbols that can\ncreate a large pattern space [5]. Also, as noted in [7], a\ncommon source of OMR errors originate from the misin-\nterpretation of note stems or other vertical structures in the\nscore as barlines, or vice-versa, which leads to measure\nannotations with false positives or false negatives.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.In this project we aim to create an optical measure recog-\nnition algorithm capable of recognizing the physical lo-\ncation of barlines in a wide range of scores of common\nWestern music notation (CWMN), deriving the bounding\nboxes for the measures, and storing these elements in a\nsymbolic music ﬁle format. This allows us to relate the\nphysical location on the page to the structure of the music\nitself. Applications that employ optical measure recogni-\ntion can enhance interactions with music scores, and in-\nclude multimodal music presentation and navigation, such\nas synchronizing digitized scores with audio playback [8],\nor content-based retrieval systems, which allow users to\nquery the score by its measures [9]. In these applications,\nthe correct extraction of barlines is essential for a proper\nalignment of the different music representations.\nThe structure of this paper is as follows: Section 2 pres-\nents the structure and implementation details of the optical\nmeasure recognition algorithm we have developed. Sec-\ntion 3 describes the annotation methodology and design of\nthe ground-truth dataset that is used to evaluate the optical\nmeasure recognition algorithm. The evaluation procedure\nis presented in Section 4. We conclude in Section 5 with a\ndiscussion of our results and future work.\n2. MEASURE RECOGNITION ALGORITHM\nOur technique for locating the bounding boxes of mea-\nsures within a music score relies on several image pro-\ncessing functions and follows the task model proposed by\nBainbridge and Bell [1], which decomposes the problem\nof OMR into several key stages: image preprocessing and\nnormalization, stafﬂine identiﬁcation and removal, musical\nobject location, and musical reasoning.\nAfter preprocessing the input image, stafﬂines are re-\nmoved from the score and thin, long, vertical lines that\nhave the same horizontal position within a system of mu-\nsic are located, even if they are connected. The connected\nheight of these elements should approximately equal the\nheight of the system to which they belong. Our approach\nassumes that barlines are usually taller than the stem of\nnotes.\nWe experimented with several image processing algo-\nrithms in our optical measure recognition system. The\nGamera software framework [10] provides a ﬂexible and\nextensible environment for testing these methods and im-\nplementing new ones, if desired. Fig. 1 displays intermedi-\nary output of our measure recognition system at differentstages of processing, described in the following sections,\non a portion of an image selected from our dataset.\n2.1 Preprocessing\nBefore analyzing the input music score, the image must\nundergo two preprocessing steps: binarization and rotation\ncorrection. The binarization step coerces each pixel in the\nimage to be either black or white according to a speciﬁed\nthreshold parameter and is accomplished by using the Otsu\nbinarization algorithm [12]. The rotation correction step\nautomatically rotates skewed images and is accomplished\nby using the correct rotation method that is part of the\ndocument-preprocessing bundle toolkit for Gamera [13].1\n2.2 Staff grouping hint\nOur measure recognition algorithm requires prerequisite\ninformation, supplied by humans, that describes the struc-\nture of the staves on the music score being processed. This\ninformation, hereinafter referred to as the staff grouping\nhint, indicates how many staves are on the page, how many\nsystems are on the page, how staves are linked together\ninto systems, and whether barlines span the space between\ngroups of staves.\nThe staff grouping hint is a string that encodes the struc-\nture of staves. For example, consider a page of piano music\nconsisting of two staves that are broken into ﬁve systems,\nwhere barlines span the space between the two staves in\neach system, as in Fig. 2. The appropriate staff grouping\nhint for this page is (2|)x5 , where parentheses indicate\na group of staves, the pipe character |denotes that bar-\nlines span the space between staves in the group, and the x\ncharacter indicates the number of systems the staff group\nis broken into.2\nAlthough the staff group hint can be seen as a bottleneck\nbecause it requires human intervention, it is an important\ncomponent of our system because it is used to properly\nencode the output symbolic music ﬁle and for fault detec-\ntion of the staff detection algorithm, described in the next\nsection. Also, most multi-page scores do not change their\nsystem structure across pages, and so a hint created for one\npage can often be used for the whole score.\n2.3 Staff detection and removal\nAfter preprocessing the input image, a staff detection al-\ngorithm searches for staves on the music score and returns\nthe bounding box information for each staff. A stafﬂine re-\nmoval algorithm then discards the located stafﬂines from\nthe music score. However, staff detection and removal\nalgorithms yield variable results depending on the nota-\ntion style of the music score, image scan quality, and the\namount of noise (artifacts) present in the image. As a re-\nsult of the high variability of images in our dataset, we\ncould not rely on only one approach for detecting stafﬂines.\n1https://github.com/DDMAL/\ndocument-preprocessing-toolkit\n2More staff grouping hint examples can be accessed at\nhttp://ddmal.music.mcgill.ca/optical_measure_\nrecognition_staffgroup_hint_examples\nFigure 1 . Process of extracting bar candidates. a) Origi-\nnal image, b) stafﬂines removed on preprocessed image, c)\nmost frequent vertical black runs removed, d) barline can-\ndidates ﬁltered by aspect ratio, e) ﬁltering of bar candidates\nby vertical tolerance thresholding, f) ﬁnal barlines, and g)\nﬁnal retrieved measures superimposed on the original im-\nage.Low-quality scans and images with an excessive amount of\nartifacts frequently cause staff detection algorithms to fail,\nand so we implemented an error handling approach that\ntries one algorithm ﬁrst, and if that fails, an alternative is\nused instead. We consider a staff detection algorithm to\nhave failed when the number of detected staves does not\nequal the value derived from the provided staff grouping\nhint.\nFollowing previous research [15], we used the Music-\nStaves Gamera Toolkit because it offers a number of dif-\nferent algorithms for detecting the position of stafﬂines\nin an image, and also for removing them.3We used the\nMiyao andDalitz algorithms to perform staff detection.\nTheMiyao algorithm provides a precise method for deter-\nmining horizontal staff slope changes in inclined, broken,\nor non-linear staves by breaking the stafﬂine into equidis-\ntant segments and capturing the vertical position of each\nsegment [11]. The Dalitz staff detection algorithm [3] is\nalso capable of tracking non-straight lines by using long\nquasi-black run extraction and skelenotization (i.e., the rep-\nresentation of each stafﬂine as a one point thick continu-\nous path), but it does not break the stafﬂines into equidis-\ntant segments. Our approach for ﬁnding measures depends\nheavily on detecting the position of the staff, and so we im-\nplemented two approaches in case one of them fails. We\ntested both conﬁgurations (i.e., Dalitz-Miyao andMiyao-\nDalitz ), and concluded that the former arrangement yields\nsuperior performance with respect to our ground-truth data-\nset. After successful recognition of the position of staves,\nthe bounding box of each system can be calculated using\nthe provided staff grouping hint.\nFollowing the staff detection stage, stafﬂine removal\nmust be performed to eliminate each stafﬂine within the\nmusic score. This process is important because it isolates\nsuperimposed music symbols on staves, facilitating their\nrecognition. However, a comparative study established that\nthere is no single superior algorithm for performing stafﬂine\nremoval [4]. Using a number of different metrics on im-\nages with deformations, we observed that the performance\nof many algorithms for stafﬂine removal is similar, with\nno one technique being obviously better in general. Based\non our previous work [15], we chose the Roach & Tatem\nstafﬂine removal algorithm [14] in our optical measure recog-\nnition system. Fig. 1(b) shows the output of the stafﬂine\nremoval algorithm on a portion of a preprocessed image\nfrom our dataset.\n2.4 Locating barline candidates\nOnce the position of each staff and system is calculated\nand all stafﬂines have been removed, we ﬁlter short verti-\ncal runs of black pixels in order to remove ligatures, beams,\nand other elements on the page that are unlikely to be can-\ndidates for a barline. The most frequent run-length is cal-\nculated and is used in subsequent processing steps. Since\nremoving stafﬂines and short vertical runs frequently leaves\nunwanted artifacts on the page, we ﬁnally despeckle the\n3http://lionel.kr.hs-niederrhein.de/ \u0018dalitz/\ndata/projekte/stafflineimage to remove all connected components smaller than a\nthreshold value, which is dependent on the most frequent\nrun-length value.\nOnce we have removed the horizontal lines from the\nimage, we perform a connected components analysis to\nsegment all of the residual glyphs on the page. The re-\nsulting set of connected components is ﬁltered to only in-\nclude thin, vertical elements, which are referred to as bar\ncandidates . The discriminating feature for the selection of\nbar candidates is the aspect ratio : the relation between the\nwidth and the height of a component. Fig. 1(c) shows the\nresult of ﬁltering short vertical runs and despeckling the\nimage. Fig. 1(d) highlights bar candidates that have an ac-\nceptable aspect ratio.\nA bar candidate may be broken into several unconnected\nlines, depending on the quality of the original image, the\neffects of any of the intermediary processing steps, or sim-\nply from barlines that intentionally do not span an entire\nsystem. If bar candidates within a system have roughly the\nsame horizontal position, they are connected into a single\nbar candidate. The height of each connected bar candidate\nis calculated and compared to the height of the system to\nwhich it belongs; these heights should approximately be\nthe same. Moreover, the upper and lower vertical position\nof the bar candidate should lie sufﬁciently close to the up-\nper and lower vertical position of its system, respectively.\nIf the bar candidate fails to meet this criterion, the glyph is\ndiscarded. The sensitivity of this ﬁltering step is controlled\nby the vertical tolerance parameter. Fig. 1(e) shows a vi-\nsual representation of the vertical tolerance ﬁltering pro-\ncess.\nAn additional ﬁltering step addresses two common cases\nwhereby certain bar candidates are included as false pos-\nitives: The ﬁrst situation occurs when accidentals preface\nthe musical content on a staff. As most horizontal lines are\nremoved in previous processing steps, the vertical lines that\nremain in the accidentals are usually horizontally aligned\nacross the staves. Therefore, they are linked together into a\nsingle bar candidate, which results in a false positive. The\nsecond situation occurs when a double barline is consid-\nered as two bar candidates. Considering that the largest key\nsignature has seven accidentals spanning twice the vertical\nsize of the staff, we resolve both of the aforementioned is-\nsues by ﬁltering bar candidates that are less than twice the\nheight of the staff apart in the horizontal direction.\n2.5 Encoding the position of measures\nThe result of the ﬁltering processes is a set of recognized\nbarline candidates from an input page of music, as seen\nin Fig. 1(f). These barlines candidates are sorted accord-\ning to their system number and their horizontal position\nwithin the system. The bounding box for each measure\nis calculated by considering the location and dimensions\nof sequential barlines in each system, as seen in Fig. 1(g).\nThe resulting set of measure bounding boxes is encoded\nin the Music Encoding Initiative (MEI) ﬁle format.4The\nMEI ﬁle format is used because of its ability to record the\n4http://music-encoding.orgstructure of music entities as well as the physical position\nof all elements on the page [6]. Also encoded in this ﬁle is\nthe overall structure of the score that indicates which mea-\nsures belong to which system and which staves belong to\nwhich system.\n3. GROUND-TRUTH DATASET\nTo the best of our knowledge, there are no standard OMR\ndatasets that are complete with annotated measure bound-\ning boxes. Therefore, we created our own to test and eval-\nuate the performance of our optical measure recognition\nsystem. Our dataset consists of 100 pages extracted from\nthe International Music Score Library Project (IMSLP).5\nWe chose to extract images from IMSLP because of the\nquantity and diversity of the CWMN materials in its database.\nTo create this dataset we selected a random musical\nwork from IMSLP, downloaded a random score or part\nfrom this work, and ﬁnally, selected a page at random from\nthe score or part. As the purpose of our study is to lo-\ncate the position of measures on pages of music, images\nof blank pages, pages with mostly text, and pages with no\nmeasures were manually discarded and replaced. In the\ninitial draw of the dataset, images with these characteristics\naccounted for roughly 15 percent of the dataset. The set\nof downloaded images were in portable document format\n(PDF), which were processed using the libraries pyPDF6\nandpdfrw ,7and converted to the tagged image ﬁle format\n(TIFF) using the Adobe Acrobat Professional application.\n3.1 Measure annotations\nOnce the images in the dataset were transformed into the\ndesired TIFF format, we created a ground-truth dataset of\nmanually annotated bounding boxes for all measures on\neach page of music. We developed a Python application\nto perform the annotations.8The graphical user interface\nof the application displays an image to be annotated by a\nuser, who indicates the presence of a measure on the page\nby clicking on the top-left position of a measure and drag-\nging the mouse to the bottom-right corner of the measure.\nIn order to ensure that all stafﬂines are straight, the im-\nage displayed to the annotators was automatically rotated\nusing the same algorithm as in the preprocessing step of\nthe presented optical measure recognition algorithm. The\napplication encodes and saves the annotations as an MEI\nﬁle, using a similar structure as the output of the optical\nmeasure recognition algorithm.\nTwo annotators with musical training of at least 10 years\nwere hired to annotate the bounding box of each measure\noccurring in the entire dataset, as well as to provide a text\nﬁle containing the staff grouping hints for each image. The\nannotators were instructed to track the time and number of\npages they annotated per session, and to start annotating at\nopposite ends of the dataset to reduce the chances of error\n5http://imslp.org\n6http://pybrary.net/pyPdf\n7http://code.google.com/p/pdfrw\n8https://github.com/DDMAL/barlineGroundTruth\nFigure 2 . Manual measure annotations created using\nour standalone Python application for the image IM-\nSLP08436 , extracted from the International Music Score\nLibrary Project.\nin the initial pages of the dataset. On average the anno-\ntators required 10 minutes to annotate the measures and\ncreate the staff group hint for each page. There were few\ndiscrepancies between the two annotators; the most com-\nmon inconsistency was the staff group hint for complex\npages of music. The dataset consists of 2,320 annotated\nmeasures, with a mean of \u0016= 23.43 measures per page,\nand a standard deviation of \u001b= 21.34. Fig. 2 displays a\npage of music from our dataset with the measure annota-\ntions superimposed.\nEven with trained annotators, we encountered several\nchallenges in the creation of this ground-truth dataset. Sev-\neral recurrent issues arose during the annotation process,\nincluding how to interpret measures that are interrupted\nby a system break, how to annotate anacruses (“pick-up”\nnotes), how to annotate repetition measures, and how to an-\nnotate measures that indicate changes in key signature but\ncontain no notes. As our approach for ﬁnding measures on\nthe score relies only on visual cues, all of the aforemen-\ntioned cases are interpreted as separate measures. As such,\nwe considered a measure interrupted by a system break, as\nwell as an anacrusis, as two different measures. In addi-\ntion, repeated measures were considered a single measure.\nSimilar projects that recognize regions of a music scoreFigure 3 .f-score results of the optical measure recogni-\ntion algorithm. The x-axis displays different values of the\naspect ratio threshold parameter. The y-axis displays dif-\nferent values of the vertical tolerance threshold parameter.\nand synchronize these to audio playback have taken a sim-\nilar approach, not yet considering repetitions of particular\nparts of a musical work [9]. Finally, areas of systems that\ncontained the clef and accidentals, but no notes, were also\nconsidered to be a measure.\n4. ALGORITHM EVALUATION\nThe performance of our optical measure recognition algo-\nrithm was evaluated by computing precision, recall, and f-\nscore statistics for the automatically recognized measures\non each page of music in the ground-truth dataset. Since\nwe are not only concerned with the number of retrieved\nmeasures but also their size and physical position on the\npage, a measure is considered correctly recognized if its\nbounding box coordinates are within a quarter of an inch of\nits corresponding bounding box in the ground-truth mea-\nsure annotations.9\nExperiments were conducted to investigate the impact\nof different values of the two critical parameters of our op-\ntical measure recognition algorithm, namely the aspect ra-\ntioand the vertical tolerance parameters, described in Sec-\ntion 2. We iterated over a set of 100 combinations of these\nparameters and reported the resulting precision, recall, and\nf-score of our algorithm in each case.\nFig. 3 presents the results of the experiments across the\nentire dataset. It can be seen that the f-score value was\nhighly inﬂuenced by the aspect ratio parameter. When this\nparameter was <0:1thef-score of our algorithm sig-\nniﬁcantly decreased. This ﬁnding is intuitive because the\nappearance of elements on a music score are often vari-\nable, especially if it is handwritten. Consequently, it is un-\nlikely to encounter barlines that are perfectly vertical with\n9Measurements in inches are converted to pixels using the pixels per\ninch parameter from the metadata of each image in the dataset.a small, constant width; they are typically skewed due to\ndeformations in the image scan or contain artifacts result-\ning from intermediary processing steps of the optical music\nrecognition algorithm.\nThe vertical tolerance threshold parameter, on the other\nhand, was found to not signiﬁcantly affect the performance\nof the algorithm, especially when the aspect ratio thresh-\nold parameter was set to an optimal value. Only with ex-\ntremely low values of vertical tolerance (i.e., when the tol-\nerance was so small that the height of a bar candidate was\nexpected to be almost the same as the system’s height)\ndid this parameter decrease the performance of the system.\nHigh values of this parameter also decreased the perfor-\nmance, but to a lesser degree.\nOverall, the aspect ratio parameter had the most im-\npact on the performance of the algorithm, though, both pa-\nrameters exhibited a range of optimal values: aspect ratio\nthreshold 2[0:125;0:150] and vertical tolerance threshold\n2[0:325;1:00], which yielded an average f-score of 0.91\nacross the entire dataset. Similar barline recognition re-\nsults have been obtained by commercial OMR systems [2];\nhowever, in that study the evaluation dataset consisted of\nonly ﬁve images and the algorithms being evaluated were\nundisclosed. Furthermore, Fotinea et al. [5] reported simi-\nlar results on a dataset containing two pages of music.\nFinally, certain pages in the dataset failed with all com-\nbinations of parameter values. The quality of these pages\nwere generally quite poor and had discontinuous stafﬂines,\nwhich caused the staff detection algorithms to fail. Nev-\nertheless, these pages were still included in the algorithm\nevaluation and resulted in an f-score of zero. We believe\nthis accurately reﬂects how our system would perform in a\n“real-world” scenario.\n5. CONCLUSION\nWe have presented work on developing a system that per-\nforms optical measure recognition on CWMN scores. Our\napproach follows an OMR workﬂow that includes image\npreprocessing, staff removal, and musical glyph recogni-\ntion. Once all stafﬂines and short vertical runs of black\npixels are removed, the algorithm ﬁnds thin, vertical el-\nements on the page to form a set of barline candidates.\nSeveral heuristics were employed to ﬁlter this set of bar-\nline candidates into a ﬁnal set of barlines, which were then\nused to calculate the bounding boxes of measures on the\npage. Our algorithm solely identiﬁes measures on images\nof music scores, and thus, does not recognize other mu-\nsical symbols such as the repeat sign, which instructs the\nperformer to repeat a measure of music. This is problem-\natic for applications that intend to synchronize digitized\nscores with audio playback and is an issue to address in\nfuture versions of our measure recognition system.\nIn order to test and evaluate our system, we manually\nannotated measure positions in 100 random pages of music\nfrom IMSLP and compared the bounding boxes produced\nby our optical measure recognition algorithm to the manual\nannotations using several descriptive statistics. We con-\nducted several experiments to test different combinationsof two critical parameters of our algorithm and discovered\nthat the aspect ratio of a glyph is the most important dis-\ncriminating feature for barlines. With optimal parameters,\nour algorithm obtained 91 percent f-score across the entire\ndataset.\nAlthough our approach obtained similar results as pre-\nvious systems, the scope and size of our evaluation dataset\nis much larger than those in the literature. We hope that the\nopen-source, command line-based implementation of our\nsystem10can be easily integrated into existing OMR sys-\ntems, and will stimulate future work in this area and help\nother researchers discover new ways to extract meaningful\ninformation from images of music scores.\n6. ACKNOWLEDGEMENTS\nThe authors would like to thank our great development\nteam for their hard work: Nick Esterer, Wei Gao, and Xia\nSong. Special thanks also to Alastair Porter for his invalu-\nable insights at the beginning of the project. This project\nhas been funded with the generous ﬁnancial support of\ntheDeutsche Forschungsgemeinschaft (DFG) as part of the\nEdirom project, and the Social Sciences and Humanities\nResearch Council (SSHRC) of Canada.\n7. REFERENCES\n[1] Bainbridge, D., and T. Bell. 2001. The challenge of op-\ntical music recognition. Computers and the Humanities\n35 (2): 95–121.\n[2] Bellini, P., I. Bruno, and P. Nesi. 2007. Assessing op-\ntical music recognition tools. Computer Music Journal\n31 (1): 68–93.\n[3] Dalitz, C., T. Karsten, and F. Pose. 2005.\nStaff Line Removal Toolkit for Gamera.\nhttp://music-staves.sourceforge.net\n[4] Dalitz, C., M. Droettboom, B. Pranzas, and I. Fujinaga.\n2008. A comparitive study of staff removal algorithms.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence 30 (5): 753–66.\n[5] Fotinea, S., G. Giakoupis, A. Liveris, S. Bakamidis,\nand G. Carayannis. 2000. An optical notation recogni-\ntion system for printed music based on template match-\ning and high level reasoning. In Proceedings of the\nInternational Computer-assisted Information Retrieval\nConference , Paris, France, 1006–14.\n[6] Hankinson, A., L. Pugin, and I. Fujinaga. 2010. An\ninterchange format for optical music recognition ap-\nplications. In Proceedings of the International Society\nfor Music Information Retrieval Conference , Utrecht,\nNetherlands, 51–6.\n10https://github.com/DDMAL/barlineFinder[7] Knopke, I., and D. Byrd. 2007. Towards Musicdiff: A\nfoundation for improved optical music recognition us-\ning multiple recognizers. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference , Vienna, Austria, 123–6.\n[8] Kurth, F., M. M ¨uller, C. Fremerey, Y . Chang, and M.\nClausen. 2007. Automated synchronization of scanned\nsheet music with audio recordings. In Proceedings of\nthe International Society for Music Information Re-\ntrieval Conference , Vienna, Austria, 261–6.\n[9] Kurth, F., D. Damm, C. Fremerey, M. M ¨uller, and\nM. Clausen. 2008. A framework for managing multi-\nmodal digitized music collections. In Research and Ad-\nvanced Technology for Digital Libraries. Lecture Notes\nin Computer Science 5173: 334–45. Springer Berlin\nHeidelberg.\n[10] MacMillan, K., M. Droettboom, and I. Fujinaga. 2002.\nGamera: Optical music recognition in a new shell. In\nProceedings of the International Computer Music Con-\nference , La Habana, Cuba, 482–5.\n[11] Miyao, H., and M. Okamoto. 2004. Stave extraction\nfor printed music scores using DP matching. Journal of\nAdvanced Computational Intelligence and Intelligent\nInformatics 8 (2): 208–15.\n[12] Otsu, N. 1979. A threshold selection method from\ngray-level histograms. IEEE Transactions on Systems,\nMan and Cybernetics 9 (1): 62–6.\n[13] Ouyang, Y ., J. Burgoyne, L. Pugin, and I. Fujinaga.\n2009. A robust border dectection algorithm with ap-\nplications to medieval music manuscripts. In Proceed-\nings of the International Computer Music Conference ,\nMontr ´eal, Canada, 101–4.\n[14] Roach, J., and J. Tatem. 1988. Using domain knowl-\nedge in low-level visual processing to interpret hand-\nwritten music: An experiment. Pattern Recognition 21\n(1): 33–44.\n[15] Vigliensoni, G., J. A. Burgoyne, A. Hankinson, and I.\nFujinaga. 2011. Automatic pitch detection in printed\nsquare notation. In Proceedings of the International\nSociety for Music Information Retrieval Conference ,\nMiami, FL, 423–8."
    },
    {
        "title": "Musicbrainz for The World: The Chilean Experience.",
        "author": [
            "Gabriel Vigliensoni",
            "John Ashley Burgoyne",
            "Ichiro Fujinaga"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417951",
        "url": "https://doi.org/10.5281/zenodo.1417951",
        "ee": "https://zenodo.org/records/1417951/files/VigliensoniBF13a.pdf",
        "abstract": "In this paper we present our research in gathering data from several semi-structured collections of cultural heritage— Chilean music-related websites—and uploading the data into an open-source music database, where the data can be easily searched, discovered, and interlinked. This paper also reviews the characteristics of four user-contributed, music metadatabases (MusicBrainz, Discogs, MusicMoz, and FreeDB), and explains why we chose MusicBrainz as the repository for our data. We also explain how we collected data from the five most important sources of Chilean music-related data, and we give details about the context, design, and results of an experiment for artist name comparison to verify which of the artists that we have in our database exist in the MusicBrainz database already. Although it represents a single case study, we believe this information will be of great help to other MIR researchers who are trying to design their own studies of world music.",
        "zenodo_id": 1417951,
        "dblp_key": "conf/ismir/VigliensoniBF13a",
        "keywords": [
            "data gathering",
            "cultural heritage",
            "music database",
            "open-source",
            "user-contributed",
            "MusicBrainz",
            "artist name comparison",
            "experiment results",
            "world music",
            "MIR researchers"
        ],
        "content": "MUSICBRAINZ FOR THE WORLD: THE CHILEAN EXPERIENCE\nGabriel Vigliensoni1, John Ashley Burgoyne2, and Ichiro Fujinaga1\n1CIRMMT2ILLC\nMcGill University University of Amsterdam\nCanada The Netherlands\n[gabriel,ich]@music.mcgill.ca j.a.burgoyne@uva.nl\nABSTRACT\nIn this paper we present our research in gathering data from\nseveral semi-structured collections of cultural heritage—\nChilean music-related websites—and uploading the data\ninto an open-source music database, where the data can be\neasily searched, discovered, and interlinked. This paper\nalso reviews the characteristics of four user-contributed,\nmusic metadatabases (MusicBrainz, Discogs, MusicMoz,\nand FreeDB), and explains why we chose MusicBrainz as\nthe repository for our data. We also explain how we col-\nlected data from the ﬁve most important sources of Chilean\nmusic-related data, and we give details about the context,\ndesign, and results of an experiment for artist name com-\nparison to verify which of the artists that we have in our\ndatabase exist in the MusicBrainz database already. Al-\nthough it represents a single case study, we believe this\ninformation will be of great help to other MIR researchers\nwho are trying to design their own studies of world music.\n1. INTRODUCTION\nThousands of commercial and non-commercial websites\noffer information and metadata about different aspects of\nmusic and artists, for example, their recordings, biogra-\nphies, discographies, video clips, and other resources. How-\never, the data they provide is often disorganized and not\ninterlinked, and the websites disappear frequently. Hence,\nit is likely that the information collected over years can be\nlost. It seems sensible to gather all music-related data in a\ncentralized database that can be accessed by several web-\nsites or systems. The goal of our project is to take data\nfrom several semi-organized collections of Chilean-music\ncultural heritage—websites and databases which combined\nrepresent almost all music that has been composed and per-\nformed in Chile—and integrate it into an open-source mu-\nsic database, where information is easy to search and will\nlast for a longer time.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.1.1 Music Metadata\nMetadata is structured information that identiﬁes, describes,\nlocates, relates, and expresses several, different layers of\ndata about an information resource [3, 5, 8]. It can be of\nthree basic types: descriptive , for purposes such as iden-\ntiﬁcation and discovery; structural , for expressing rela-\ntions among resources; and administrative , for managing\nresources [8]. Descriptive music metadata commonly pro-\nvides information about recordings, expressing the song\ntitle and length, the artist name, and the release name of a\nmusical object, usually stored in a MP3 ID3 tag. Structural\nmusic metadata is used to document relationships within\nand among digital musical objects to allow navigation, such\nas the song’s order in an album or a playlist, linking song\nnames to video clips, artists to their biographies, and so\non. Administrative music metadata can include informa-\ntion such as software and hardware used to digitize the\nmusical resource, system requirements to read the ﬁles, use\nrestrictions or license agreements that constrain the use of\nthe resource. Thus, music metadata has been called a dig-\nital music commodity because it adds value to the musical\nobjects, mediating the experience of listeners with music\nand artists, helping them to browse, explore, sort, collect,\nuse, and ﬁnally enjoy music [4, 7].\nFor this project, we collected data about Chilean mu-\nsic from several websites and databases of different scope\nand size.1Centralizing and interlinking these resources\nshould create synergies in the data because one single query\nwill give access to all resources, creating relations that were\nnot established in their previous locations, thus contribut-\ning to a larger web of data. Hence, ﬁnding the best repos-\nitory capable in storing and expressing these data relation-\nships was considered signiﬁcant and crucial for the success\nof our endeavour.\n1.2 Music-Related Metadatabases\nAlthough there are many commercial, professionally re-\nviewed, online music libraries, we focused only on user-\nbuilt, open-data, music metadatabases. By contrast to com-\nmercial libraries, open databases provide user-access to en-\nter data and have similar types of licenses for the use of its\ndata as the Chilean websites. We present now a list of the\nmain services available for free, public use in terms of their\n1Data from all databases available at\nhttp://www.vigliensoni.com/McGill/BDMC/8_DATAScope Size Information\nresources storedRelations IDs Data\nquality\nassuranceLanguage\nguidelinesAPI Other\nFreeDB CD tracklists \u00182M albums Album, title, year,\ngenre, time offsetsDisc to genre FreeDB\npropriety\nDisc-IDUnspeciﬁed\nautomatic\nmethodNo No Limited search\nengine. Small, ﬁxed\nset of genres. Two\nartists can have the\nsame ID.\nMusicMoz Music-related factual\ndata and Internet\nlinks>136K items Artist’s biographies,\ndiscographies,\nproﬁle, reviews, and\narticles.\nMusic-related links\nand resourcesNo relations\nallowedArtist\nname-\nbased\nURIMusicMoz’s\nauthorized\neditorsNo No Many links are no\nlonger available.\nUnusual ontology of\nmusical categories.\nDiscogs Physical\ndiscographies and\nmusic releases>2M artists\n>3M albumsArtist, release,\nmaster, label, imageSmall set of\nrelationsArtist\nname-\nbased\nURICommunity-\nbased\nhigh-\nquality\ndataEnglish\nonlyRESTful,\nXML-\nbased\nAPI30s previews. Well\ndesigned solution for\nartist name variation.\nMarketplace\navailable. Could\nevolve to a\ncommercial site.\nMusicBrainz Any kind of music\nrelease>600K artists\n>1M releases\n>11M tracksCore entities (artist,\nlabel, recording,\nrelease,\nrelease-group,\nwork), entities, and\ntheir relationshipsAdvanced\nRelationships\ncan be\nexpressed for\nall core and\nexternal entitiesMB IDs are\nuniversally\nunique\nidentiﬁer\n(UUID)Community-\nbased\nhigh-\nquality\ndataGuidelines\nfor 30\nlanguagesRESTful,\nXML-\nbased\nAPIMB links data from\nall other\nmetadatabases.\nMB’s Advanced\nRelationships can be\nmapped into RDF.\nMB stores acoustic\nﬁngerprints.\nTable 1 . Comparison of free, user-built, open-data, music metadatabases: FreeDB ,MusicMoz ,Discogs , and MusicBrainz .\nscope, size, stored information resources, relations among\nthese resources, ID system, data quality and assurance, lan-\nguage guidelines, and API.\nFreeDB is a license-free database of CDs track-listings\nfrom user-contributed data, originally based on the Com-\npact Disc Database (CDDB).2\nMusicMoz is a user-contributed database that stores music-\nrelated, factual data and Internet links.3\nDiscogs is a large, user-populated database of discogra-\nphies and music releases from physical sources. It pro-\nvides data of high quality that is ensured by a strict in-\nput form mechanism and a large community of users.4\nMusicBrainz is a large, community-based, user-contribu-\nted metadatabase that stores the three aforementioned\ntypes of metadata for any kind of music release. Its\nhigh-quality database is managed by an open commu-\nnity of non-professionals that negotiate periodically and\nconsistently, with strict standards and routines, about\nthe orientations, developments, style guidelines, and\nmostly everything on MusicBrainz [4].5\nTable 1 shows a comparison of the four metadatabases.\nIt can be seen that, among all these databases, MusicBrainz\nis the database with the broadest scope, not being restricted\nto only physical copies, as in the case of Discogs, or CDs,\nas in FreeDB. Also, MusicBrainz is the only music database\ncapable of storing not only descriptive, but also structural\nand administrative metadata. This fact is critical for ex-\npressing the many relationships among musical resources,\n2http://www.freedb.org/\n3http://www.musicmoz.org/\n4http://www.discogs.com/\n5http://www.musicbrainz.org/as well as for providing efﬁcient ways of managing and\ninterlinking them. For example, the MusicBrainz univer-\nsal unique identiﬁer-based IDs (MBIDs) practically ensure\nunique identiﬁers for all its core entities (i.e., artist, la-\nbel, recording, release, release-group, and work), and they\nhave been already widely used for linking music data in the\nsemantic-web community. Furthermore, the MusicBrainz\ncommunity decided to develop a set Advanced Relation-\nships to describe relations between its core entities, and to\npublish all the MusicBrainz database, resources and their\nrelationships, as Linked Data. The recently-ﬁnished Linked-\nBrainz6subproject was intended to map all these rela-\ntionships into RDF [6]. MusicBrainz also stores acous-\ntic ﬁngerprints (PUIDs) that can be used to search for a\nresource even if its descriptive metadata is not available.\nUsers, as well as performance right organizations, could\nbeneﬁt from this feature for music rights identiﬁcation pur-\nposes. Moreover, MusicBrainz has also a strict, language-\nspeciﬁc, style guidelines for 30 different languages, which\nexplain how data should be formatted in their database.\nThis fact is very important to this project—and also to\nother similar projects coming from countries and regions\nwhere English is not the primary language—because it al-\nlows, and forces, the non-English-speaking users to fol-\nlow the correct standard for their language. Finally, Mu-\nsicBrainz is the most “open” of the four reviewed meta-\ndatabases because it provides methods to link data from\nother websites and databases to MusicBrainz (e.g., by ex-\ntracting CD track lists from FreeDB, linking artists’ images\nfrom Discogs, data from MusicMoz, Allmusic ,BBC Music\nandWikipedia ; or album covers and videos from Amazon\nandYouTube , respectively).\n6http://wiki.musicbrainz.org/LinkedBrainzBy analyzing all the aforementioned characteristics, we\ndecided to work with the MusicBrainz metadatabase in or-\nder to store and make available the corpus of music that we\nwant to work with: Chilean music.\n2. COLLECTING DATA ABOUT CHILEAN MUSIC\nOver the last ten years, several endeavors for creating web-\nsites devoted to Chilean music have been developed. These\nprojects have collected data about artists’ discographies,\nbiographies, video clips, and album and concert reviews.\nCurrently, however, there is no way of creating a common\nquery to retrieve all available data for a speciﬁc artist, al-\nbum, or song because these resources are neither central-\nized nor interlinked, do not have URIs, and the websites\ndepend on scarce sources of funding, and so sometimes\nthey have to shut down.\n2.1 Websites and Databases of Chilean Music\nDuring the past few years, Chilean music-related metadata\nhas been accessible through the following ﬁve major web-\nsites:\nBase de datos de la M ´usica Chilena (BDCH) (the Chi-\nlean Music Database) is a restricted digital music data-\nbase developed by the Sociedad Chilena del Derecho\nde Autor (SCD, the Chilean Society of Authors), the\nonly performance rights organization in Chile. The\nBDCH provides all associate radio stations across the\ncountry with a secure, fast, and easy-to-use website\nfor accessing and exploring the largest repository of\nChilean music. Radio stations can legally download\nand airplay music from the BDCH. The scope of its\ncollection is wide, ranging from rock, pop, and ballad,\nto classical, experimental, and jazz. Metadata for each\nsong has been manually generated by the artists or pro-\nducers themselves and cleaned afterwards by expert an-\nnotators. Data includes composer, author, interpreter,\nalbum, label, genre, and label.7\nMusicapopular (MP) is a website developed and main-\ntained by music journalists, advised by a musicologist,\nand funded by several short-term governmental grants.\nIts database is updated periodically and their authors\nhave a commitment for data accuracy and quality, which\nmakes MP a good source when looking for information\nabout Chilean music artists. It provides artists’ biogra-\nphies and discographies, the evolution of band mem-\nbers over time, birth and death dates for individuals,\nand start and end dates for bands. All its data is mostly\ninterlinked, but only within the website. Although MP\nhas a genre taxonomy tailored to Chilean music (e.g.,\nnueva canci ´on chilena ,m´usica chilota , orproyecci ´on\nfolcl´orica ), the mainstream genres, e.g., rock and pop,\ncomprise the bulk of the database.8\n7http://bdch.musica.cl/web_bdch/\n8http://www.musicapopular.cl/Mus (MUS) is a website devoted to new album and con-\ncert reviews. It was funded by the SCD and short-term\ngovernmental funds and was maintained periodically\nby music journalists. Although the website was shut\ndown on January 2012, its data can still be accessed\nwith the exact URLs of the resources. The site does not\nprovide any search methods.\nPortaldisc (PD) is a web portal where Chilean music of\nmany genres is sold. PD shares efforts with the SCD\nas well as MP, selling most of the catalog belonging\nto and reviewed in those collections. Its website pro-\nvides access to audio previews of all songs as well as\nshort album reviews. PD can be searched only by artist\nname.9\nVideoclipchileno (VCCL) is a website with a nearly com-\nprehensive collection of Chilean music video clips. It\nprovides not only links to the video clips and their di-\nrectors, but also contextual information about them. It\nis maintained periodically by the same group of mu-\nsic journalists that run MP, and they are waiting for\nnew governmental funding to improve the scope and\nfunctionality of the site. VCCL’s search engine ac-\ncepts a keyword that is searched across all ﬁelds in their\ndatabase.10\nMost data retrieved from the aforementioned websites\ncan be represented explicitly with the MusicBrainz meta-\ndata schema. Also, by means of creating advanced rela-\ntionships between resources from different sources, these\ncan be linked and accessed from the MusicBrainz web page\nor API.\n2.2 Data Harvesting and Parsing\nBecause most of the surveyed websites depend on external\nsources of funding, they can be short-lived and their data—\nand people’s work behind it—can be lost. For that reason,\nwe decided to harvest all data from the available websites.\nAlthough the nature of the data in all databases was dif-\nferent, it can be combined or its overlap can be used to ex-\ntract more accurate data. For example, the album reviews\ncan provide the track list, and so they can be used to ob-\ntain the actual list of songs for an speciﬁc album, or to help\nto disambiguate any difference in the track lists. Table 2\nshows the data types and approximate number of entries\nwe extracted by scraping the websites.11\nThere were some problems when scraping, parsing, and\nstoring the data from the websites.The ﬁrst problem was\nhandling all non-ASCII characters. These characters typi-\ncally arise because most of the words are written in Span-\nish, but also sometimes because of special characters that\nartists use for their names or for the names of their songs.\n8http://www.mus.cl/\n9http://www.portaldisc.cl/\n10http://www.vccl.tv/\n11Code and scripts available at\nhttp://www.github.com/vigliensoni/bbdd_much/Database Data retrieved\nBDCH 40,000 songs\n33,000 different songs\n3,300 artists\n3,000 albums\n400 record labels\n80 genres\nMP 1,500 bands\n1,800 individuals\n1,800 biographies\n40 genres\nMUS 500 albums\n300 interviews\n600 concerts\nPD 3,600 album reviews\nVCCL 1,600 video clips\nTable 2 . Approximate collection sizes and data types\nwithin the ﬁve major Chilean music databases.\nWe ended up using Unicode for representing all data in-\nternally. Another problem we faced was the many vari-\nations that a resource name can have across repositories,\nreleases, or even within the same website. For example,\ndifferent people can use alternative forms of an artist name\n(e.g., “Dj Bitman”, “DJ Bitman”, “Dj Bit Man”, and so\non). Also, artists themselves can use variations of their\nnames across several releases (e.g., “Bitman” and “DJ Bit-\nman”). Finally, many slight variations of the same resource\nname can exist across different repositories due to human\nerror when entering the data. Resolving these inconsisten-\ncies can be a tricky problem, for example, as in recent work\nfrom Angeles et al. [1] where they combined a metadata\nmanager software with ﬁngerprinting-based querying and\nstill obtained a low rate of consistency between resource\nnames among different databases. Libraries’ practice of\nusing authority ﬁles for identiﬁcation and disambiguation\nof catalog names is able to deal with these inconsistencies,\nhowever, most Chilean artist names are still not cataloged\nin institutional databases. This project tries to collect data\nfrom several sources, disambiguate name variations, and\nstore the data into a single searchable database. In Section\n3.1 we will detail a string matching-based experiment that\nhelped us take a step toward solving this problem.\n3. DATA MATCHING WITH MUSICBRAINZ\nAfter we had consolidated all data, we wanted to know\nhow many entries were (and were not) already in the Mu-\nsicBrainz database. Using the MusicBrainz web services,\nwe proceeded to compare our data with MusicBrainz and\nobtained 27, 23, and 21 percent of matches for artists, al-\nbums, and songs, respectively. However, among the artists,\nwe retrieved a large number that were false positives (i.e.,\nwrongly recognized as the queried artist when they did not\ncorrespond with the actual Chilean artist). In order to re-\nduce these inconsistencies, we decided to add constraints\non the resulting resources. The country name alone seemed\na good predictor for ﬁxing this problem, but we were dis-\ncouraged because currently only 22 percent of the totalnumber of artists in MusicBrainz have a country name as-\nsigned. Even worse, among them there are only 200 artists\nwith a CLcountry-code value, the ISO 3166-1 Alpha-2\ncode for Chile. A second problem we found was that some-\ntimes the actual, true positive result retrieved by the Lucene-\nbased MusicBrainz search server was not the one in the\nﬁrst position or the one with the highest score. We real-\nized that we would need to iterate over all retrieved results\nand compare the strings in order to see which one was the\nproper match.\nTo improve the number of true positives for our query,\nwe designed and ran an experiment considering the ad-\nvanced search method that MusicBrainz provides and the\ntwo aforementioned issues. The objective of the experi-\nment was to determine an ideal threshold that allows us to\nhave the largest precision and recall for the artist names.\nThat led us to three questions:\n\u000fHow many artists have an exact match (i.e., they are\nalready in the MusicBrainz database)?\n\u000fHow many artists do not match (i.e., they are not in the\nMusicBrainz database)?\n\u000fHow many artists match partially? Among these we\nneed to see what is the best threshold to obtain the\nlargest precision and recall.\n3.1 Approaches to Artist Name Matching\nOur approach for the experiment was two-fold. On the one\nhand, we created a query that consisted of the artist name\nplus the country code; we also looked for any comment\nwith the word Chile in the annotation ﬁeld for artist dis-\nambiguation (i.e., MusicBrainz’s own method for disam-\nbiguating similar artist names in its database). On the other\nhand, we hypothesized that for selecting the proper string\nfrom all the retrieved results, we could rely on measuring\nthe string difference between the query string and each one\nof the retrieved results: the one with the smallest difference\nwould be the true positive. Thus, to handle all nuances or\nvariations of the strings due to special Spanish characters\nor typographical errors when the artist name was entered,\nwe implemented two string metrics with three variations\neach:\nLevenshtein distance (L) permitted us to calculate the cost\nof the best sequence of edits to convert one string into\nthe other. We used it in its ratio form to consider the\nnumber of letters of the query, so we obtained a nor-\nmalized value, where 1 represents an exact match.\nJaro metric (J) also allowed us to calculate a normalized\nvalue that represents the number of edits needed to con-\nvert one string into the other, but it further weights pos-\nitively or negatively if source characters are or are not\npresent in the target string [2].\nNormal string (N) was a direct comparison of the origi-\nnal strings.ASCII-ﬁed string (A) allowed us to compare ASCII-ﬁed\nversions of the strings where all tilde and accents were\nremoved from the strings.\nASCII-ﬁed, lower-cased, no-space string (P) allowed us\nto compare strings where all characters were ASCII-\nﬁed and lower-cased, and from which the spaces were\nalso removed.\nHence, we ended up with the LN,LA,LP,JN,JA, and JP\nvariations of the similarity of the artist name query string\nagainst each one of the retrieved artist names by the Mu-\nsicBrainz web service. It should be noted that in all cases\nthe Lucene special characters were escaped before doing\nthe query, and so if an artist name had any of these charac-\nters, it was not considered.\n3.2 Experimental Procedure\nFor the experiment, we designed a testing subset with artist\nnames randomly chosen from the ones in our dataset. This\nsubset was created among those artists with string distances\nbetween the range of 0.75 and 1.00. The size of the dataset\nwas 800 entries, which represents roughly a quarter of the\ntotal number of artists in our dataset. We manually searched\nto see if the artists in the dataset already existed in the\nMusicBrainz database. This process was long and com-\nplex because there were many false positives among those\nartists with common and short names, such as Rachel ,Quo-\nrum,Criminal ,Twilight , and many others. The only way to\ndetermine if the query returned a true or false positive was\nsearching in the artist’s country, releases, relationships, or\nworks, and seeing if anything there matched some data in\nour database. We then ran the query for each entry, chose\nthe items with the largest metric values, and identiﬁed true\nand false positives, and false negatives. We then calculated\nprecision and recall for the whole subset at different thresh-\nolds, and calculated the error for each metric and threshold\nusing the bootstrapping technique. Figure 1 shows the pre-\ncision and recall for each threshold, metric, and variant.\nError curves at \u000b= 0.05 generated from a bootstrap sam-\nple of 1,000 replications of the original sample are also\nshown. These plots have to be understood as a series of\ntwo complementary runs. The plots in the left refer to pre-\ncision (upper left) and recall (lower left) for the compari-\nson of strings only. In this case, all retrieved artists with the\nsame name were considered as a true positive, even if they\nare not in fact the same artist. This approach was taken be-\ncause none of the six metrics can know in advance whether\nthe retrieved name denotes the speciﬁc person that we are\nlooking for. We can observe that if we were to be too strict\nwith the string distance threshold, the precision would be\nhigh but the recall would be low, as expected.\nIn terms of precision, it can be seen that Levenshtein\nstring distance (L) in its variants for the normal string (N),\nits ASCII-ﬁed version (A), and the ASCII-ﬁed, lower-cased,\nand no-spaced string version (P) performs better than the\nJaro (J) distance for thresholds between the range of 0.80\nand 0.90. The three different variants (N, A, P) do not\nmake statistically signiﬁcant differences in the results forany of the methods. However, in terms of recall, it can\nbe seen that the P versions of both L and J are the ones\nwith the best recall, especially when the thresholds become\nhigher. There is no statistically signiﬁcant difference be-\ntween the performance of P and A, but at high thresholds,\nN statistically signiﬁcantly underperforms both P and A.\nWe can conclude that for the best results, the queried string\nshould at least be ASCII-ﬁed and the Levenshtein distance\nshould be used. The threshold point to obtain the best pre-\ncision while still having a good recall can be found around\n0.90.\nThe two plots in the right of Figure 1 refer to the same\nexperiment and dataset but evaluated with respect to their\nreal-life context. In other words, in this second case, if the\nquery returned an artist with the same name but it referred\nto another artist in real life, the returned artist was con-\nsidered as a false instead of a true positive. Under these\nconditions, we reached a ceiling of precision at about 60\npercent, which establishes that some kind of veriﬁcation\nprocess is essential when using name matching in a real-\nworld application.\n4. CONCLUSIONS AND FUTURE WORK\nWe have done a review of user-contributed, music meta-\ndata libraries, and we have shown why we have chosen\nMusicBrainz as the metadatabase that we will use. We\nhave also shown the ﬁve websites that we used for col-\nlecting Chilean music-related metadata and given details\nabout the data that each website provides.\nWhen we tried to combine our database with MusicBra-\ninz, we realized that there was a large amount of false-\npositive noise in the results for each query. We tried several\ntypes of queries and determined that an advanced search\nmethod, including several ﬁelds at the same time, was re-\nquired, and also that it was necessary to iterate over all\nresults and perform a string comparison between the query\nand the retrieved query to look for the true result. Hence,\nwe developed an experiment whereby we created a random\nground-truth subset of artist names from our database. We\nthen queried these same entries with MusicBrainz, com-\npared the retrieved results with the ground truth, and then\ntried to deﬁne the optimum variation threshold that should\nbe accepted between the queried and the retrieved string in\norder to obtain the best precision and recall. We compared\ntwo different metrics with three variations each. For pre-\ncision, the Levenshtein ratio offered the best performance,\nstabilizing its curve close to a normalized value of 0.90,\nwhere 1.00 means that the two strings are identical. The\nthree variations did not matter. For recall, however, we\nestablished that the strings should, at least, be ASCII-ﬁed\nto obtain a better recall. Overall, the best string compar-\nison threshold on a normalized scale is close to 0.90, and\nthe method used should be Levenshtein distance with an\nASCII-ﬁed, lower-cased, no-spaced version of the strings.\nShort-term future work for this project will be to apply\nthese experimentally obtained values for the string com-\nparison across albums and songs. However, it is expected\nthat the results will be much better because we will knowFigure 1 . Precision and recall for artist names string comparison between the consolidated data from Chilean music\ndatabases and MusicBrainz. While upper and lower left plots show the results for the raw string comparison using Leven-\nshtein (L) and Jaro (J) string distances and three variants (N, A, P), the ones in the right show the results in real-life context,\nwhere false positives were discarded. Error curves show upper and lower limits for 1,000 populations replicated from the\noriginal sample using bootstrap at \u000b= 0.05.\nbeforehand if the album’s artist or song’s artist is already\nin the MusicBrainz database. For future work in the mid-\nterm, we will enter all data we collect into the MusicBrainz\ndatabase. The MusicBrainz API does not allow one to\nautomate this kind of process, but we experimented us-\ning a POST method to ﬁll the forms automatically, and\na user would simply need to review and submit the data\nto the database. By these means, all the data we collect\nwill be available from MusicBrainz. As a second mid-term\nproject, and by asking permission to the SCD, it would also\nbe possible to use the audio, thereby allowing audio anal-\nysis over the whole corpus of music. This kind of analy-\nsis would be beneﬁcial for everyone in the music-industry\nchain. For example, running structural music analysis to\nknow where the choruses are in a given song, or know-\ning the overall tempo of a song, would be of beneﬁt for\nthe radio stations that use the BDCH, or the general public\ninterested in creating a remix of a given song. In the long-\nterm, the experience gained and the techniques developed\nwill be applicable to other countries and cultures, to enable\nthem to merge their metadata to global central databases.\n5. ACKNOWLEDGEMENTS\nThis research was supported by the Social Sciences and\nHumanities Research Council, and by BecasChile Bicen-\ntenario, CONICYT (Comisi ´on Nacional de Ciencia y Tec-\nnolog ´ıa), Gobierno de Chile. The authors would like to\nthank Alastair Porter for sharing valuable knowledge about\nthe MusicBrainz web services and API, and to the Sociedad\nChilena del Derecho de Autor (SCD) for granting us access\nto their database.6. REFERENCES\n[1] Angeles, B., C. McKay, and I. Fujinaga. 2010. Dis-\ncovering metadata inconsistencies. Proceedings of the\nInternational Society for Music Information Retrieval\nConference . 195–200.\n[2] Bilenko, M., R. Mooney, W. Cohen, P. Ravikumar, and\nS. Fienberg. 2003. Adaptive name matching in infor-\nmation integration. Intelligent Systems, IEEE 18 (5):\n16–23.\n[3] Dublin Core Metadata Initiative. Retrieved April 6\n2013, from http://dublincore.org/metadata-basics/.\n[4] J. Hemerly. 2011. Making metadata: The case of Mu-\nsicBrainz. SSRN eLibrary 1982823.\n[5] International Federation of Library Associa-\ntions and Institutions. Digital Libraries: Meta-\ndata Resources . Retrieved April 6 2013, from\nhttp://archive.iﬂa.org/II/metadata.htm.\n[6] Jacobson, K., S. Dixon, and M. Sandler. 2010. Linked-\nBrainz: providing the MusicBrainz Next Generation\nSchema as Linked Data. Late-breaking demo session\nat the 11th International Society for Music Information\nRetrieval Conference .\n[7] J. W. Morris. 2012. Making music behave: Metadata\nand the digital music commodity. New Media & Soci-\nety14 (5):850–66\n[8] National Information Standards Organization. 2004.\nUnderstanding metadata . NISO Press, Bethesda, MD,\nUSA."
    },
    {
        "title": "A Corpus-Based Study on Ragtime Syncopation.",
        "author": [
            "Anja Volk",
            "W. Bas de Haas"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415846",
        "url": "https://doi.org/10.5281/zenodo.1415846",
        "ee": "https://zenodo.org/records/1415846/files/VolkH13.pdf",
        "abstract": "This paper presents a corpus-based study on syncopation patterns in ragtime. We discuss open questions on the ragtime genre and the potential of computational tools in addressing these questions, contributing to the fields of Musicology and Music Information Retrieval (MIR), and giving back to the ragtime enthusiasts community. We introduce the RAG-collection of around 11000 ragtime MIDI files collected, organised, and distributed by many ragtime lovers around the world. The collection is accompanied by a compendium, providing useful metadata on ragtime compositions. Using this collection and the compendium, we investigate syncopation patterns in ragtime melodies, for which we tailored a melody extraction algorithm. We test and confirm musicological hypotheses about the occurrence of syncopation patterns that are considered typical for ragtime on the extracted melodies. Thus, the paper presents a first step towards modelling typical characteristics of the ragtime genre, which is an important means for enabling automatic genre classification.",
        "zenodo_id": 1415846,
        "dblp_key": "conf/ismir/VolkH13",
        "keywords": [
            "syncopation patterns",
            "ragtime genre",
            "computational tools",
            "Musicology",
            "Music Information Retrieval",
            "RAG-collection",
            "melody extraction algorithm",
            "musicological hypotheses",
            "typical characteristics",
            "automatic genre classification"
        ],
        "content": "A CORPUS-BASED STUDY ON RAGTIME SYNCOPATION\nAnja Volk\nUtrecht University\nA.Volk@uu.nlW. Bas de Haas\nUtrecht University\nW.B.deHaas@uu.nl\nABSTRACT\nThis paper presents a corpus-based study on syncopation\npatterns in ragtime. We discuss open questions on the rag-\ntime genre and the potential of computational tools in ad-\ndressing these questions, contributing to the ﬁelds of Mu-\nsicology and Music Information Retrieval (MIR), and giv-\ning back to the ragtime enthusiasts community. We intro-\nduce the RAG-collection of around 11000 ragtime MIDI\nﬁles collected, organised, and distributed by many ragtime\nlovers around the world. The collection is accompanied\nby a compendium, providing useful metadata on ragtime\ncompositions. Using this collection and the compendium,\nwe investigate syncopation patterns in ragtime melodies,\nfor which we tailored a melody extraction algorithm. We\ntest and conﬁrm musicological hypotheses about the oc-\ncurrence of syncopation patterns that are considered typi-\ncal for ragtime on the extracted melodies. Thus, the paper\npresents a ﬁrst step towards modelling typical characteris-\ntics of the ragtime genre, which is an important means for\nenabling automatic genre classiﬁcation.\n1. INTRODUCTION\nThis paper presents a corpus-based study on ragtime syn-\ncopation using a collection of around 11000 ragtime MIDI\nﬁles. Corpus-based studies have the potential to quanti-\ntatively answer research questions in musicology using a\ndata-rich approach, thus providing a complementary strat-\negy to methods in Musicology. Moreover, they provide ex-\namples of how Music Information Retrieval (MIR) meth-\nods can successfully contribute to other research areas,\nwhich has recently been discussed as an important issue\nof interdisciplinarity in MIR [1,15]. In this paper we argue\nthat corpus-based studies also contribute to important re-\nsearch questions within MIR, such as similarity estimation\nand genre classiﬁcation.\nMauch and Dixon have carried out a corpus-based study\nof bar-length drum patterns in a collection of 48000 MIDI\nﬁles gathered from the Internet, motivated by the observa-\ntion that “comparatively little work in MIR has quantita-\ntively examined rhythms in symbolic data” [11, p. 164].\nThe paper argues that the outcome of such a study might\nbe of direct interest for musicians and musicologists. Con-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.clusions from the detected repeated drum patterns in this\ncollection are drawn on a general level, such as about the\ndifference between language and music with regard to rep-\netition. In this paper we argue that the outcome of a corpus-\nbased study is even more meaningful to musicians and mu-\nsicologists, if the questions addressed are meaningful for\nthe speciﬁc corpus investigated. Hence, we demonstrate\nwhat questions on ragtimes exist among musicologists and\nmusic lovers, and why a corpus-based study on ragtimes\nis therefore of vital interest to musicologists, music lovers\nand MIR researchers.\nIn this paper we introduce a corpus of around 11000\nragtime MIDI ﬁles as the dataset for our study on syncopa-\ntion patterns, the RAG1-collection. The MIDI ﬁles of this\ncollection have been produced, organised and collected by\nmany ragtime musicians and lovers all around the world\nand shared via the Internet. The collection has grown over\nmany years, and an accompanying Ragtime Compendium\nhas been created by Michael Mathew and colleagues with\nmetadata about the corresponding ragtime compositions\n(such as title, year, composer, original publisher, website\nof the MIDI ﬁle) [10]. The RAG-collection is the product\nof a shared effort of ragtime enthusiasts and hence presents\na heterogeneous collection of MIDI ﬁles of very different\nquality due to their different origins (such as original piano\nrolls that have later been translated into MIDI format, ac-\ntual performances of ragtimes or encodings of the notated\nscore). In this paper we undertake ﬁrst steps to use the\nRAG-collection as a basis for investigating characteristics\nof the ragtime genre employing MIR methods.\nThe contribution of the paper is threefold. First, we give\nan overview over open questions on ragtime as a genre\nrelevant to musicologists, MIR researchers, and ragtime\nlovers; we argue how MIR can productively contribute\nto answering these questions with a corpus-based study.\nSecond, we investigate syncopation patterns on the RAG-\ncollection as a means of understanding rhythmic character-\nistics of this genre. Speciﬁcally, we test and conﬁrm musi-\ncological hypotheses about the occurrence of so-called tied\nanduntied syncopation patterns in ragtimes during differ-\nent time periods of the ragtime era (1890-1919). Further-\nmore we show that these patterns remained important for\nragtimes in the period 1920-2012 and have been used even\nmore extensively. Third, we tailor MIR tools for melody\nﬁnding to the ragtime genre. By investigating syncopation\npatterns in ragtimes using a data-rich approach, we under-\ntake a ﬁrst step towards modelling typical characteristics of\n1Ragtime Admirers Groupthe ragtime genre, which is an important means for auto-\nmatic genre classiﬁcation based on similarity, contributing\nto a core topic in MIR.\n2. SIGNIFICANCE OF A CORPUS-BASED STUDY\nON RAGTIMES FOR MUSICOLOGY AND MIR\nIn this section we argue that a corpus-based study on rag-\ntimes contributes to important issues in both MIR and Mu-\nsicology regarding the question on what constitutes the\ngenre of ragtime.\n2.1 Open questions on ragtime as a genre\nWhile most people associate ragtime with the piano mu-\nsic by Scott Joplin, James Scott and Joseph Lamb, ragtime\nmusic in fact comprises a wide range of vocal, instrumen-\ntal, improvised and composed music with lots of different\nmusical inﬂuences (such as difference dance music styles\nlike march, two-step, habaneras, or tangos). Ragtime is\nconsidered a “hybrid from folk and written cultures” [5, p.\n65]; as a consequence, ragtime is both investigated in the\ncontext of “folk”, “popular” and “art” music [4, p. 142].\nAs of today, ragtimes present “thorny problems” to mu-\nsical scholars [4], one of those crucial problems concerns\nthe very identity of what is ragtime: “A major question that\nemerged in the course of this musical survey was how to\ndetermine what music could reasonably be considered rag-\ntime” [2, p. xviii]. As we will argue later, this question is\nnot only of scholarly interest, but also concerns the vital\ncommunity of ragtime lovers all over the world.\nOne of the main characterizations of ragtimes is related\nto syncopation, as Edward Berlin points out: “At the core\nof the contemporary understanding of ragtime ... was syn-\ncopation. The question ‘What is ragtime?’ was asked\nthroughout the period, and almost invariably explanations\nincluded a statement about syncopation” [2, p. 11]. How-\never, there are ragtimes that do not contain syncopation,\nand there exist lots of syncopated music that is not consid-\nered ragtime. Hence, the role of syncopation for the rag-\ntime genre needs still to be more speciﬁed, and additionally\nother features need to be described that contribute to rag-\ntime as a genre. One example of investigating the speciﬁc\nrole of syncopation in ragtime is that musical scholars have\ndetermined typical syncopation patterns in ragtime along-\nside with hypotheses on what patterns are most typical for\na certain period of ragtime [2, 7], which we will address in\nthis paper using computational approaches.\nClosely related to the question of what constitutes a\nragtime is the question about the different origins or mu-\nsical sources that contributed or are still contributing to\nragtime, and what subgroups within this broad genre can\nbe identiﬁed. There exist different theories as to what\nhas contributed to shaping the genre of ragtime, such as\ncoon-songs, cakewalk, two-steps, or dance music of the\nCaribbean or South America (such as danzas, habaneras,\nand tangos) and music characteristics of African Ameri-\ncan origins. With regard to piano ragtimes, Berlin distin-\nguishes at least three different subgroups: “piano rendi-\ntions of ragtime songs; ‘ragged’ versions of pre-existingunsyncopated music; and original, dance-oriented ragtime\ncompositions” [2, p. 63]. Furthermore he lists patriotic and\nfolk tunes as well as classical music pieces as common mu-\nsical sources that by being ragged have become members\nof the ragtime repertoire. In the U.S. many local Ragtime\nsocieties and festivals exist, and attempts have been made\nto distinguish stylistically different types of ragtimes de-\npendent on their local origin – however, this has not yet\nbeen achieved according to Berlin [3].\nMoreover, Berlin discusses that what has been consid-\nered ragtime by listeners in the past and present, has also\nbeen shaped by socio-cultural aspects: “My original search\nfor a working deﬁnition of ragtime was almost dwarfed\nas I became immersed in the conﬂicts of its day, concern-\ning ragtime’s origins, racial content, relevance to American\nmusic, innovative features, potential for ‘artistic’ develop-\nment, effects on cultural, moral, and physical well-being\nand the like” [3, p. xviii].\n2.2 The signiﬁcance of a corpus-based investigation of\nragtimes for musicology\nAn investigation of ragtimes using a large digital corpus\ncomprising ragtime pieces from the 1890s to present is\nnecessary in order to overcome current biases about this\ngenre due to a rather restricted perspective that comes from\nthe musical pieces that have been mainly investigated so\nfar. The years of roughly 1890s–1919 are considered as\nthe Ragtime era when the genre was established, and for\nthe general public a resurrection of the genre took place\nfollowing the ﬁlm “The Sting” in the 1970s. However,\nin fact ragtime music has been a lively music tradition\nalso in the years in between. Edward Berlin argued that\npublished anthologies of ragtimes over-represent “the best-\nknown composers” (Joplin, Scott, Lamb) from the Ragtime\nera, and that therefore in ragtime research the stylistic traits\nof their music has overweighted stylistic traits of other rag-\ntime compositions [2, p. 72]. Therefore, in order to get a\nmore balanced view of the ragtime genre as a whole, he\nhas carried out by hand a study of a random selection of\nragtimes at the Lincoln Center. Studying a large digital\nragtime corpus with computational tools is the logical next\nstep of broadening the perspective on ragtime. Moreover,\nattempts to deﬁne possible subgroups in the broad range of\nexisting ragtimes, provide a natural setting for research on\ngenre using a computational data-rich approach.\nInvestigating rhythmic patterns in ragtime also con-\ntributes to research on variation in music. Variation is a\nfundamental trait in music and is linked to the experience\nof similarity in music, a central concept in MIR [14]. Most\noften, variation has been studied in the context of Variation\nsets in Western classical music, where one musical idea is\nwidely exploited by the composer. However, as a means of\nestablishing similarity in music, it seems more appropriate\nto study the relation between variation patterns and style,\nas suggested by Meyer [12, p. 3]: “Style is a replication\nof patterning”. For ragtime there exist musicological hy-\npotheses on what patterns are characteristic for the genre;\nthe role of these patterns can be tested with computationalmethods. In this paper we test hypotheses about the occur-\nrence of a typical syncopation pattern in ragtime, which\nhas been dubbed as Leitmotiv2of ragtime by musicolo-\ngists [6]. We examine how often and in which variations\nwith respect to the location within the bar, this pattern oc-\ncurs within the RAG-collection.\n2.3 The signiﬁcance of a corpus-based investigation of\nragtimes for MIR and ragtime lovers\nThe question on how to determine with computational meth-\nods the genre of a given digitized musical piece lies at the\nheart of MIR research. At the same time, it is all too of-\nten very unclear what a reasonable argument would be to\nclassify a certain musical piece into a given genre category.\nThe genre of ragtime provides an interesting case study of\nlinking rhythmic and harmonic patterns that are discussed\nas being constituents of the genre to the occurrence of these\npatterns within a large digitized corpus of ragtime pieces.\nMoreover, the question of what constitutes a ragtime\nis not only interesting from a scholarly perspective, but is\nof interest to a rather large international ragtime fan com-\nmunity (with ragtime festivals and societies3existing) that\nshares lots of digital collections on the Internet: “the rag-\ntimers . . . have clubs, magazines, and an international fol-\nlowing. The subject owes an enormous amount to these in-\ndividual collectors and dedicated players who have helped\nto preserve the repertoire and keep it alive. New rags are\nbeing composed and played all the time” [5, p. 65].\nPiano rolls belong to the ﬁrst sources that preserved rag-\ntimes before they appeared as printed scores in sheet mu-\nsic. Many of these piano rolls have been later preserved\nas MIDI ﬁles, and along with many other compositions\nfor which MIDI ﬁles have been created and shared via\nthe Internet. We have now a collection of around 11000\nMIDI ﬁles that have been recognized as ragtime by ragtime\nlovers. It is of vital interest, also to this community of rag-\ntime fans, whether a given digital ﬁle might be considered\nas ragtime or into what subcategory of ragtime it might be-\nlong: “A discussion on what constitutes ragtime could ﬁll\nseveral web pages” [10]. While socio-cultural aspects cer-\ntainly contribute to constituting the ragtime genre, content-\nbased MIR-methods can assist in investigating in how far\nmusical characteristics contribute to the the identity of rag-\ntime. Moreover, MIR-methods can be used to identify vari-\nants of the same ragtime composition in different MIDI ﬁle\nrenditions existing on the Internet. Content-based MIR-\nmethods hence would be of great use for this community,\nsince metadata on ragtimes is rather problematic and not\nvery reliable (i.e. ragtime composers have used different\npseudonyms for different genres [10]). Hence, ragtimes\nprovide an interesting application area for content-based\nmethods on similarity in MIR.\n2by metaphorically extending the original concept of Leitmotiv from\nrecurring motifs that represent a certain person, idea etc. to recurring\nmotifs that represent a genre\n3e.g. http://www.westcoastragtime.com; http://indianarag.org;\nhttp://www.bohemragtime.com/en/act.html\n42\n42\n42\n42I I O I  . . . .  . . . .  . . . . \n. . . .  I I O I  . . . .  . . . . \n. . I I  O I . .  . . . .  . . . . \n. . . .  . . I I  O I . .  . . . . a)\nb)\nc)\nd)\ne)Figure 1 . (a) shows the syncopation pattern dubbed 121in\n[7] and its different shifts within the bar, leading to untied\nsyncopations in (b) and (c), and tiedsyncopations in (d)\nand (e). Behind the pattern scores we display the patterns\nused for matching: Idenotes an onset, Odenotes no onset,\nand.denotes a wild-card that matches everything. Every\nposition represents sixteenth note position within a bar.\n3. THE RAG-COLLECTION\nThe RAG-collection currently contains 11591 MIDI ﬁles\nof ragtimes gathered from the Internet. The Ragtime Com-\npendium [10] is an accompanying database on the iden-\ntiﬁcation of individual ragtime compositions (year, title,\ncomposer, publisher) and whether or not a MIDI ﬁle exists.\nThe Compendium lists around 15000 identiﬁed composi-\ntions (each entry corresponds to a unique ragtime composi-\ntion), out of these 5600 pieces have MIDI ﬁles. Hence, the\n11591 MIDI ﬁles contain lots of different MIDI renditions\nfor the same composition. The compendium “has evolved\nover several years, starting from very humble beginnings\nand now, thanks to the help of many ragtime enthusiasts, it\nis probably the most complete listing that exists” [10].\n4. A CORPUS-BASED STUDY ON SYNCOPATION\nFor the characterization of musical pieces as ragtimes, syn-\ncopation plays a major role, as described in section 2. To\nbe able to distinguish between pieces that contain syncopa-\ntions and that belong to the ragtime genre or not, more de-\ntailed investigations of syncopation patterns are necessary.\nAre ragtimes characterized by a higher amount of synco-\npation patterns than other genres, and/or are there speciﬁc\nsyncopation patterns prevalent in ragtimes more than in\nother music styles? Musicologists [2, 7] discuss a num-\nber of syncopation patterns typically occurring in ragtimes\nofthe ragtime era (1890s–1919), along with hypotheses\non what patterns occur most often in early (until 1901) or\nlate ragtimes (1902–1919) based on an extensive study of\nragtime pieces from this era by hand.\nIn this paper we are testing these hypotheses about typ-\nical syncopation patterns with a computational approach,\nas a ﬁrst step of exploring the role of syncopation in deﬁn-\ning ragtimes. Speciﬁcally, we test Berlin’s hypothesis that\nso-called untied syncopations are a typical characteristic\ntrait of early ragtimes, while tiedsyncopations have been\nintroduced in the late ragtimes starting roughly from 1902.\nHence, many early ragtimes use untied syncopations ex-\nclusively, while tied syncopations became the predominant\ntype of syncopation later [2, p. 128]. The difference be-\ntween tied and untied syncopation is discussed with thesyncopation pattern shown in Figure 1(a), which has been\ncalled the Leitmotiv of ragtime music [6] and called pat-\ntern 121in [7]. Shifting this pattern along the bar leads\nto either untied syncopation (Figures 1(b) and (c)) or tied\nsyncopation (Figures 1(d) and (e)).\nWe were unable to ﬁnd musicological hypotheses about\nthe role of these syncopation patterns in ragtimes that have\nbeen composed after 1919 (following the ragtime era).\nSince the amount of ragtime pieces composed dropped sig-\nniﬁcantly after 1919, one might hypothesize that by then\nthe consolidation of the ragtime style has taken place. Rag-\ntimes following this consolidation might therefore exhibit\nsimilar amounts of untied and tied syncopation as in the\nsecond half of the ragtime era (1902–1919). However,\nsince the extensive use of syncopation patterns was new to\nthe general musical audience at the beginning of the 20th\ncentury, one might also hypothesize that the amount of\nsyncopation patterns increases over time due to a common\nhabituation to syncopations. For instance, an increased use\nof syncopation in American popular music in general has\nbeen concluded from an empirical study in [9], using 437\nshort random samples from pieces dating back from 1890\nto 1939. Yet a third hypothesis arises from Berlin’s re-\nmarks on the erosion of ragtime’s “most distinctive qual-\nities” [2, p. 147] that started in the 1910s. The increased\nuse of dotted rhythms observed in late ragtimes “reduced\nthe need for syncopation in ragtime” [2, p. 148]. From this\nperspective, a decreased use of syncopation might be hy-\npothesized for the following years. We take advantage of\nthe RAG-collection containing ragtime pieces until 2012\nin order to test the role of these syncopation patterns in\nragtimes composed after the ragtime era.\n4.1 Data preparation\nFor testing hypotheses on syncopation patterns, we ﬁrst\nneed to pre-process the MIDI ﬁles of the RAG-collection.\n4.1.1 Melody ﬁnding\nWithin the ragtime genre most of the interesting rhythmi-\ncal material is in the melody, hence in the case of a piano\npiece, this mainly concerns the right hand part of the piece.\nThe accompaniment in the left hand is characterised by sta-\nble rhythmical patterns that follow the beat (also dubbed\noompah bass line). Since the melody is not annotated ex-\nplicitly in the MIDI ﬁles of the RAG-collection, we need\nto employ automatic melody extraction in order to split the\nmelody from the accompaniment. We use a version of the\nskyline algorithm for melody ﬁnding [16]; this algorithm\ntakes the highest sounding note when multiple notes sound\nsimultaneously. To overcome that highest notes from the\nleft hand are classiﬁed wrongly as part of the melody at\nsections where there is no melody, we set a lower limit: all\nnotes below the middle C are classiﬁed as accompaniment.\nWe evaluated our skyline implementation on a small se-\nlection of 435 songs of the RAG-collection. These songs\nwere selected automatically based on whether they were\nquantised, and had exactly two tracks. We ensured that\nthere were no duplicate ﬁles or tracks. In most cases theP R F\nMelody extraction :930:989:958\nMelody extraction w. dip-detection :972:984:978\nTable 1 . The Precision, Recall, and F-measure of the\nmelody extraction with and without dip-detection , evalu-\nated on a 435-piece subset of the RAG-collection.\nﬁrst track was the melody track, but in a few cases the\nmelody and accompaniment track were reversed. Hence,\nwe checked whether both the lowest and the highest notes\nin the melody were higher than the lowest and highest notes\nin the accompaniment. If this was not the case, we exam-\nined the ﬁle by hand. When the two tracks did not clearly\nreﬂect a melody and accompaniment, we removed it from\nthe selection. Next, we performed a melody extraction ex-\nperiment by creating one track containing all notes, per-\nforming the melody ﬁnding, and comparing the result to\nthe original melody track (our ground-truth melody). In\nsome cases the ground-truth melody track contained chords,\nsince we are mainly interested in rhythm, and did not tai-\nlor our skyline implementation for retrieving chords in the\nmelody track, we removed all but the highest chord notes.\nWe compared the extracted melody to our ground-truth\nmelody by calculating the precision, recall and F-measure\nfor every piece, and averaged these numbers over the total\nselection of 435 songs. The results are displayed on the\nﬁrst row of Table 1, and can be considered good. The high\nrecall indicates that almost all notes of the original melody\nare retrieved. The high precision shows in turn that we do\nnot include a lot of non-melody notes into our automati-\ncally extracted melody.\nAlthough the melody extraction could be considered\ngood, manual inspection of some extracted melodies\nrevealed that accompaniment notes were classiﬁed as\nmelody notes typically at occasions where syncopation in\nthe right hand occurs. In absolute numbers there were not\nmany of these spurious melody notes. However, since they\nbroke some of the syncopation patterns that we are inter-\nested in (see Figure 1), we extended our melody ﬁnding\nalgorithm with a dip-detection : most of the misclassiﬁca-\ntions can be recognised by a large interval down followed\nby a large interval up. Therefore, after performing the sky-\nline algorithm, we removed notes that were characterised\nby an interval down greater than 9 semitones followed by\nan interval up greater than 9 semitones.\nWe evaluate the melody extraction with dip-detection\non the same subset of the RAG-collection, the results are\ndisplayed in the second row of Table 1. They show that\nthe precision improves, while the recall hardly changes.\nOverall the skyline algorithm with dip-detection yields a\nnear-perfect result on this subset of the RAG-collection.\n4.1.2 Quantisation\nTo be able to analyse the rhythmical patterns in the ragtime\nmelodies, the MIDI data needs to be quantised appropri-\nately. Although most ragtime performers claim that rag-\ntime should be played with straight eighth notes, we founda large number of MIDI ﬁles that contained ragtimes per-\nformed with swing . Swing refers to a characteristic long-\nshort subdivision of the beat that is generally considered a\ncrucial aspect contributing to the quality of a jazz or pop\nperformance (see [8] for further information). To be able\nto capture and represent all of these different subdivisions\nof the beat properly, we shift all onsets onto a grid of 12\nequally spaced divisions per quarter note. To estimate how\nwell a ﬁle can be aligned to the metrical grid, we store the\nquantisation deviation for every note (in MIDI ticks), and\ncalculate the average. Because MIDI ﬁles can have dif-\nferent time resolutions, we divide the average deviation by\nthe quarter note length as speciﬁed in the MIDI ﬁle; this\nnormalised average quantisation deviation gives us a rea-\nsonable estimate of the quantisation error.\n4.1.3 Selection of relevant MIDI ﬁles\nSince we want to test hypotheses about the occurrence of\nsyncopation patterns in ragtime compositions stemming\nfrom speciﬁc time periods, we need to match the meta-\ndata in the compendium to the MIDI ﬁles in the RAG-\ncollection. This is done by comparing the ﬁle name with\nthe title ﬁeld in the compendium. To make the ﬁle name\ncomparison more robust, we removed all spaces, commas,\nparentheses etc. before doing a case-insensitive compari-\nson. Moreover, not all MIDI ﬁles are suitable for analy-\nsis: we require that a MIDI ﬁle has only one metre, which\nshould be2\n2,2\n4, or4\n4, and starting exactly at tick 0. Also,\nwe found that pieces after 1920 have on average a higher\nnormalised average quantisation deviation than pieces be-\nfore 1920. To eliminate this bias, we removed all pieces\nthat have a normalised average quantisation error above\n2% from the corpus. For many entries in the compendium\n(a single entry stands for a unique ragtime composition)\nthere exist multiple MIDI ﬁles in the collection. In order\nto not bias our study by using different MIDI renditions of\nthe same ragtime composition, we select only one MIDI\nrendition per composition, namely the ﬁle with smallest\naverage quantisation deviation.\nWe divided the data in two periods: the ragtime era\nperiod (1890–1919, 1806 pieces) and the modern period\n(1920–2012, 659 pieces). To be able to investigate changes\nin syncopation patterns in different time periods, we split\npieces from the ragtime era into the pieces up to 1901 (253\npieces) and from 1902 onwards (1553 pieces).\n4.2 Testing of syncopation hypothesis\nFor testing Berlin’s hypothesis on syncopations in the rag-\ntime era we calculate the proportion of tied and untied 121\nsyncopation patterns in the period 1894–1901 (253 pieces)\nand the period 1902–1919 (1553 pieces). All pitch infor-\nmation of the ragtime melodies is ignored, and we only\nexamine the onsets of the melodies. The melodies are\nsegmented in two bar segments that overlap exactly one\nbar.4Furthermore, based on the time signature, the pat-\nterns are scaled appropriately to match the predominant\npulse. As a consequence, in2\n2and4\n4metres the patterns\n4This is necessary since the tied pattern in Fig. 1(e) crosses a barline.\n0.000.050.100.150.200.25\n1890–19 01 1902–19 19 1890–19 19 1920–20 12Proportion of 121 syncopation patterns per bar Untied\nTiedFigure 2 . On the left side, the proportion of untied and tied\n121syncopation patters per bar averaged over all songs be-\nfore and after 1902 is displayed. On the right side we show\nthe distribution of tied and untied syncopation patterns for\nthe ragtime era and for modern ragtimes. The error bars\ndenote the standard error of the mean.\nin Figure 1 (a)–(e) are matched at the quarter note level.\nWe match all tied and untied patterns for all songs written\nbefore and after 1902, and average the results. The experi-\nment was implemented using the functional programming\nlanguage Haskell, and the code is available to the research\ncommunity on request.\nThe results displayed on the left side of Figure 2 show\nthat the number of tied patterns has almost doubled in the\nperiod 1902–1919, compared to the period 1890–1901. In\nthe same period, the number of untied patterns has de-\ncreased with approximately one third. We examined if\nthese differences were statistically signiﬁcant by perform-\ning two Wilcoxon rank-sum tests, and we can conﬁrm that\nthe difference in mean before and after 1902 are statisti-\ncally signiﬁcant for the untied pattern, p< 0:0001 , as well\nas for the tied pattern, p< 0:0001 .5\nExamining the difference in tied and untied patterns in\nthe entire ragtime era (1890–1919, 1806 pieces) reveals\nthat the differences are less pronounced (see the right side\nof Figure 2). The average 121proportion per bar is 0:11\nfor untied and 0:14for tied syncopation. We conﬁrm that\nthis difference is still statistical signiﬁcant at the \u000b= 0:01\nlevel with a Wilcoxon signed-rank test, Z=\u00005:57;p <\n0:0001 . If we examine the difference between the pro-\nportions of tied and untied patterns after the ragtime era\n(1920–2012, 659 pieces), we observe that the usage of syn-\ncopation patterns increases compared to the ragtime era.\nHence, we cannot conﬁrm the hypothesis that a consoli-\ndation of the ragtime style within the ragtime era led to\nthe same amount of syncopation patterns after 1919 (ex-\ncept for the fact that tied syncopation occur more often\nthan untied syncopation); neither can we observe a de-\ncreased use of these patterns, which would be in accor-\ndance with Berlin’s hypothesis that the increased use of\ndotted rhythms in late ragtimes reduced the need for syn-\ncopation. Especially the proportion of tied syncopation in-\ncreases signiﬁcantly, and the difference between tied and\nuntied syncopation is highly statistical signiﬁcant, Z=\n\u00008:62;p < 0:0001 . The increased use of these synco-\n5One might think that the large difference in sample size has affected\nthe signiﬁcance tests. We reassure the reader that the differences remain\nstatistically signiﬁcant if we take a random sample of 253 songs after\n1902 for tied, p <0:0001 , and untied, p <0:0001 , patterns.pation patterns in the modern era indicates that our second\nhypothesis on the habituation to syncopation by the gen-\neral musical audience in the 20th century allowing an even\nstronger use of syncopation in ragtimes, should indeed not\nbe rejected on the basis of the RAG-collection. However,\nsince the RAG-collection is much more complete for the\nragtime era than for the modern era, it is difﬁcult to predict\nwhether this result is representative for all existing mod-\nern ragtime pieces (including those for which no MIDI ﬁle\nexists yet).\n5. CONCLUSION AND FUTURE WORK\nIn this paper we have provided an overview of open ques-\ntions on ragtime as a genre, and have argued on how MIR-\nmethods can productively contribute to answering these\nquestions. As a ﬁrst step for ﬁnding characteristic musi-\ncal features of the ragtime genre, we have evaluated musi-\ncological hypotheses on syncopation patterns on the RAG-\ncollection. We introduced the RAG-collection that evolved\nfrom the shared effort of the Internet community of ragtime\nlovers, and introduced a number of preprocessing steps to\nbe able to parse the ragtime ﬁles of this collection for a\ncorpus-based study; speciﬁcally, we tailored the skyline\nmelody extracting algorithm successfully to ragtimes.\nWith our computational investigation of the occurrence\nof the 121-pattern we were able to conﬁrm Berlin’s hypoth-\nesis on the increased use of tied syncopations in compar-\nison to untied syncopations in the ragtime era after 1902.\nFurthermore, by extending our investigation to modern rag-\ntimes, we were able to show that these patterns remain\nimportant in ragtime also after 1920. We have found no\nevidence that the amount and kind of syncopation after the\nragtime era stays stable, rather we observe an increased use\nof these patterns, especially of tied syncopations. The in-\ncrease of syncopation in ragtime over time concluded from\nour analysis of 2465 compositions provides an interesting\ncomplementary ﬁnding to the increase of syncopation in\nAmerican popular music in 1890-1939 concluded in [9]\nfrom short samples extracted from 437 pieces.\nIn the near future, we will quantitatively test all occur-\nring kinds of syncopation patterns in ragtime and deter-\nmine the proportion of the 121-pattern among these pat-\nterns. This will contribute to evaluating whether the 121-\npattern can indeed be considered such a typical pattern in\nragtime, providing a promising candidate for typical rag-\ntime features for classiﬁcation tasks in MIR. For modelling\nsyncopation patterns as a characteristic musical feature of\nragtime, another next step is to consider the location of\nthese patterns within speciﬁc form sections, such as dis-\ncussed in [7] in order to establish different typologies of\nragtimes. Apart from rhythmic features, musicologists ar-\ngue that there exist also typical harmonic patterns in rag-\ntime, such that the study of chord progressions is a next\nlogical step. The strengthening of the syncopation by plac-\ning a chord rather than only a melody note at the right hand\npart provides yet another candidate of syncopation patterns\ntypically for ragtimes that we are going to evaluate.\nFor distinguishing syncopation patterns typical for rag-times and testing their discriminative power for automati-\ncally classifying ragtimes, we plan to extend our database\nwith jazz, pop and rock pieces, which constitute related\ngenres for which syncopation has been claimed being rel-\nevant [9, 13]. In a study of syncopation in rock, Temper-\nley argues that a “full understanding of rock requires con-\nsideration of social and cultural aspects”. This certainly\napplies also to ragtime music, and by employing content-\nbased MIR-methods for genre classiﬁcation, we plan to\ncontribute to the debate on how far a musical genre is both\na sociological construct and is rooted in musical character-\nistics that can be extracted from the musical content.\nAcknowledgments. We acknowledge the efforts of many ragtime\nenthusiasts that have contributed to the RAG-collection and the accom-\npanying Ragtime Compendium. Speciﬁcally, we thank Michael Mathew,\nwho has kindly provided us with the collection and the compendium. We\nthank F. Wiering and M.E. Rodr ´ıguez-L ´opez for providing valuable com-\nments on an earlier draft on this text. A. V olk and W.B. de Haas are sup-\nported by the Netherlands Organization for Scientiﬁc Research, through\nthe NWO-VIDI-grant 276-35-001 to Anja V olk.\n6. REFERENCES\n[1] J. J. Aucoutourier & E. Bigand: “Mel Cepstrum & Ann\nOva: The difﬁcult dialog between MIR and Music cognition.”\nISMIR-Proceedings , pp. 397–402, 2012.\n[2] E. A. Berlin: “Ragtime. A Musical and Cultural History.”\nUniversity of California Press, 1980.\n[3] E. A. Berlin: Review of “That American Rag: The Story of\nRagtime from Coast to Coast”, by David A. Jasen and Gene\nJones. American Music , V ol. 19, No. 4, pp. 474–476, 2001.\n[4] S. DeVeaux: Review of “Ragtime: Its History, Composers,\nand Music” by John Edward Hasse; “Reﬂections and Re-\nsearch on Ragtime” by Edward A. Berlin. Ethnomusicology ,\nV ol. 32, No. 2, pp. 142–145, 1988.\n[5] P. Dickinson: “The Achievement of Ragtime: An Introduc-\ntory Study with Some Implications for British Research in\nPopular Music”. Proceedings of the Royal Musical Associa-\ntion, V ol. 105, No. 2, pp. 63–76, 1978-1979.\n[6] F. Gillis: “Hot Rhythm in Piano Ragtime”. Music in the\nAmericas , pp. 91–104, 1967.\n[7] I. Harer: Ragtime. Versuch einer Typologie. Verlag Hans\nSchneider, Tutzing, 1989.\n[8] H. Honing & W.B. de Haas: “Swing Once More: Relating\nTiming and Tempo in Expert Jazz Drumming”, Music Per-\nception , V ol. 25, No. 5, pp. 471–476, 2008\n[9] D. Huron & A. Ommen: “An Empirical Study of Syncopa-\ntion in American Popular Music, 1890-1939”, Music Theory\nSpectrum , V ol. 28, No. 2, pp. 211–231, 2006.\n[10] M. Mathew: “A Ragtime Compendium”\nhttp://ragtimecompendium.tripod.com/\n[11] M. Mauch & S. Dixon: “A corpus-based study of rhythm pat-\nterns”, ISMIR-Proceedings , pp. 163–168, 2012.\n[12] L.B. Meyer: Style and Music: Theory, History, and Ideology ,\nUniversity of Chicago Press, 1989.\n[13] D. Temperley: “Syncopation in Rock: A Perceptual Perspec-\ntive”, Popular Music , pp. 19–40, 1999.\n[14] A. V olk, W.B. de Haas, & P. van Kranenburg: “Towards Mod-\nelling Variation in Music as a Foundation for Similarity”,\nProceedings of the 12th ICMPC , pp. 1085-1094, 2012.\n[15] A. V olk, & A. Honingh: “Mathematical and computational\napproaches to music: challenges in an interdisciplinary en-\nterprise” Journal of Mathematics and Music , V ol. 6, No. 2,\npp. 73–81, 2012.\n[16] A.L. Uitdenbogerd & J. Zobel: “Manipulation of music for\nmelody matching.” In Proceedings of the ACM Multimedia\nConference , pp. 235-240, 1998."
    },
    {
        "title": "The Audio Effects Ontology.",
        "author": [
            "Thomas Wilmering",
            "György Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415004",
        "url": "https://doi.org/10.5281/zenodo.1415004",
        "ee": "https://zenodo.org/records/1415004/files/WilmeringFS13.pdf",
        "abstract": "In this paper we present the Audio Effects Ontology for the ontological representation of audio effects in music production workflows. Designed as an extension to the Studio Ontology, its aim is to provide a framework for the detailed description and sharing of information about audio effects, their implementations, and how they are applied in realworld production scenarios. The ontology enables capturing and structuring data about the use of audio effects and thus facilitates reproducibility of audio effect application, as well as the detailed analysis of music production practices. Furthermore, the ontology may inform the creation of metadata standards for adaptive audio effects that map high-level semantic descriptors to control parameter values. The ontology is using Semantic Web technologies that enable knowledge representation and sharing, and is based on modular ontology design methodologies. It is evaluated by examining how it fulfils requirements in a number of production and retrieval use cases.",
        "zenodo_id": 1415004,
        "dblp_key": "conf/ismir/WilmeringFS13",
        "keywords": [
            "Audio Effects Ontology",
            "Music Production Workflows",
            "Studio Ontology",
            "Detailed Description",
            "Sharing of Information",
            "Realworld Production Scenarios",
            "Data about the use of audio effects",
            "Facilitates reproducibility",
            "Detailed analysis of music production practices",
            "Semantic Web technologies"
        ],
        "content": "THE AUDIO EFFECTS ONTOLOGY\nThomas Wilmering, Gy ¨orgy Fazekas, Mark B. Sandler\nCentre for Digital Music (C4DM)\nQueen Mary University of London\nthomas.wilmering@eecs.qmul.ac.uk\nABSTRACT\nIn this paper we present the Audio Effects Ontology for the\nontological representation of audio effects in music pro-\nduction workﬂows. Designed as an extension to the Studio\nOntology, its aim is to provide a framework for the detailed\ndescription and sharing of information about audio effects,\ntheir implementations, and how they are applied in real-\nworld production scenarios. The ontology enables captur-\ning and structuring data about the use of audio effects and\nthus facilitates reproducibility of audio effect application,\nas well as the detailed analysis of music production prac-\ntices. Furthermore, the ontology may inform the creation\nof metadata standards for adaptive audio effects that map\nhigh-level semantic descriptors to control parameter val-\nues. The ontology is using Semantic Web technologies that\nenable knowledge representation and sharing, and is based\non modular ontology design methodologies. It is evaluated\nby examining how it fulﬁls requirements in a number of\nproduction and retrieval use cases.\n1. INTRODUCTION\nThe development of tools and services for the realisation of\nthe Semantic Web has been a very active ﬁeld of research\nin recent years, with a strong focus on linking existing data.\nIn the ﬁeld of music information management, Semantic\nWeb technologies may facilitate searching and browsing,\nand help to reveal relationships with data from other do-\nmains. At the same time, many algorithms have been de-\nveloped to extract low and high-level features, which en-\nable the user to analyse music and audio in detail. The use\nof semantics in the process of music production however is\nstill a relatively new ﬁeld of research. With computer sys-\ntems and music processing applications becoming increas-\ningly powerful and complex in their underlying structure,\nsemantics can help musicians and producers in decision\nprocesses, and provide more natural interactions with the\nsystems.\nHerrera and Serra [9] stressed the potential of semantic\nsound descriptors for the development of new audio ap-\nplications in their work using MPEG-7 descriptors. They\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\n© 2013 International Society for Music Information Retrieval.asserted that ”there are [...] sound content-based process-\ning applications waiting to be developed once we have a\nrobust set of descriptors and structures for putting them\ninto relation and for expressing semantic concerns about\nsound.” We argue that Semantic Web technologies, such as\nSemantic Web ontologies and RDF are a superior choice\nfor the representation of metadata in audio production, be-\ncause they allow for a more ﬂexible and extensible rep-\nresentation of this heterogeneous information domain. Be-\nside, as de-facto standards of the future Web, they allow for\nsharing and linking structured information across different\ndomains. Ontology-driven knowledge management in mu-\nsic production has also been discussed in [2,7]. Within this\nﬁeld, the main focus of our study is the representation of\ninformation about audio effects.\nAudio effects play an integral part in modern music pro-\nduction. They modify an input signal and may be applied\nin order to enhance the perceived quality of a sound or to\nmake more drastic changes to it in the composition pro-\ncess. Employing music information retrieval (MIR) and\nSemantic Web technologies speciﬁcally for the control of\naudio effects has the potential of representing a signiﬁcant\nstep in their evolution. This work therefore has a good\npotential to address a phenomenon described by V oorvelt\n[17]: “in the context of popular music production, the equip-\nment in use generally trails the latest technological devel-\nopments.” Detailed descriptions of the use of audio ef-\nfects in a music production project can additionally facili-\ntate the reproduction of workﬂows, and add an additional\nlayer of depth to MIR. For instance, the ontology can help\nanswering queries such as: Which effects have been used\nin a music production project and what are the parameter\nsettings? Which effect implementations are available that\nsuit the needs for a speciﬁc workﬂow?\nThe Audio Effects Ontology presented in this paper is\ndesigned as an extension to the Music Ontology [14] and\nStudio Ontology [7], as well as our previous work detailed\nin [18]. First, we brieﬂy discuss the Semantic Web Tech-\nnologies underlying this work. Then we introduce the Au-\ndio Effects Ontology and provide an overview of its design\nand purpose. We discuss some applications in the domain\nof music production and music information retrieval, and\nﬁnally outline directions of future work.\n2. SEMANTIC WEB TECHNOLOGIES\nThe Semantic Web aims to bring intelligence to the Web\nby allowing machines to reason about Web content. Withthe proliferation of audio content on the Web, represent-\ning information about audio and its production is just as\nimportant as processing and linking text documents. The\nﬁrst step towards this goal is to represent information in a\nmachine interpretable format.\nA stack of technologies have been proposed for building\nthe Semantic Web. The Resource Description Framework\n(RDF) is a data model for describing statements using sub-\nject, predicate, object triples. These form an RDF graph\nwhen combined. When the elements of RDF statements\nare identiﬁed by uniform resource identiﬁers (URI), we\nobtain an interlinked, globally distributed “database”, the\nWeb of Linked Data. However, to enable querying or rea-\nsoning over linked data, we need languages that describe\nthe shared meaning of RDF graphs, in other words, repre-\nsent knowledge about the entities described in data sets.\n2.1 Ontologies and knowledge representation\nOntology languages like the RDF Schema language and\nthe Web Ontology Language (OWL) [1] allow for char-\nacterising entities in terms of their relationships. They\ndescribe a shared conceptualisation of a world [8] com-\nprised of individuals, classes and relations, with formal se-\nmantics that allow automated reasoning over RDF data ex-\npressed using an ontology. RDF Schema allows for deﬁn-\ning simple hierarchies of classes and properties with a set\nof constraints over their use, but without adequate logical\ngrounding. OWL reﬁnes this model by adding tools for\nrepresenting domain knowledge more precisely. For in-\nstance, we can characterise properties in terms of transi-\ntivity, symmetry or reﬂexivity. OWL constructs directly\ncorrespond to Description Logics (DLs), a family of logic-\nbased knowledge representation languages which in turn\nare based ﬁrst-order logic [10].\n3. THE STUDIO ONTOLOGY\nThe Studio Ontology is an OWL ontology for capturing\nthe nuances of record production by providing an explicit,\napplication and situation independent conceptualisation of\nthe studio environment [7]. It is presented as a modular\nframework of ontologies, which in turn are built on the\nMusic Ontology framework [14] and its components. It\nuses its core elements that allow for the representation of\ntime-based events (Event and Timeline ontologies), and\nthe workﬂow of music production in an editorial context\nsubsumed under broader terms deﬁned by the Functional\nRequirements for Bibliographic Records (FRBR) [13].\nThe Music Ontology allows for describing the music\nproduction workﬂow from composition to delivery, how-\never, it lacks some concepts to do so in sufﬁcient detail.\nThe Studio Ontology provides some of the necessary ex-\ntensions that form the foundation for a comprehensive rep-\nresentation of audio effects, and their application in music\nproduction. Here, we outline only those of its features and\ncomponents which make it suitable as basis for our work.\n•Foundational components : The Studio Ontology\nallows for characterising and describing the appli-cation of technological artefacts (devices) in music\nproduction. The Device Ontology provides a funda-\nmental device and device decomposition model, and\nentities for representing device states, such as vari-\nable device parameters at different levels of granu-\nlarity.\n•Complex device descriptions : The ontology pro-\nvides a model for describing complex devices such\nas signal processing tools and their interconnections.\nIt includes a four-layered abstract model of these de-\nvices resembling the FRBR model.\n•Core components : The ontology provides for de-\nscribing recording studios on the editorial level (e.g.\npersonnel and available equipment). It also provides\nfor describing signal processing workﬂows in the stu-\ndio using a parallel event and signal ﬂow utilising\nmusic production tools.\n•Domain speciﬁc extensions : The Studio Ontology\nsupports the provision of domain speciﬁc extensions.\nIt also provides some extensions for describing au-\ndio recording (e.g. microphones), mixing, editing\nand a core model for describing audio effects.\nThe application of audio effects to signals can be described\nusing the concept studio:Transform deﬁned by the Studio\nOntology. This concept represents an event that takes a\nsignal as a factor, and produces a transformed signal. This\nconcept may be subsumed in more speciﬁc effect ontolo-\ngies. The Studio Ontology sets aside the problem of deﬁn-\ning speciﬁc audio effects, their classiﬁcations, parameters\nand their application speciﬁc descriptions. The Audio Ef-\nfects Ontology ﬁlls this gap.\n4. THE AUDIO EFFECTS ONTOLOGY\nThe aim of the Audio Effects Ontology is the represen-\ntation of knowledge concerning audio effect implementa-\ntions and their application in the music production studio.\nFor instance, music software such as digital audio worksta-\ntions (DAW) and digital effects implementations, may sup-\nport the audio engineer by producing and reusing knowl-\nedge that is represented using the concepts and properties\ndeﬁned by the ontology.\n4.1 The Core Ontology\nThe core parts of the Audio Effects Ontology deﬁne con-\ncepts and properties for the description of audio effect im-\nplementations and how they are applied within the pro-\nduction process. This facilitates the incorporation of data\nabout the application of audio effects in online catalogues\nor content-based music recommendation systems, and al-\nlows for using an effects database in Semantic Web appli-\ncations at large. For instance, this facilitates the retrieval of\nsongs characterised by certain effects or effect types used\nin a production, and the reproduction of workﬂows.\nRather than signal processing devices, audio effects in\nour proposed ontology are conceptualised as physical and\nacoustical phenomena, that are represented on the same\nconceptual layer as the abstract Work entity in the FRBRmodel [13] of intellectual works. An effect is represented\nby the OWL class afx:Fx . Furthermore, the Fxclass can be\nlinked to effect types (see Section 4.2), thus adding mean-\ning to the audio effect that may not be given solely by an\nimplementation’s given name. A separate class serves the\npurpose of describing signal processing devices, such as\nsoftware implementations or hardware effect units.\nThe description of audio transformations in music pro-\nduction is another purpose of the ontology. To enable this\nfunctionality, the Audio Effects Ontology deﬁnes concepts\nfor describing the application of effects to a signal. These\nconcepts integrate seamlessly with the Studio Ontology\nthat already provides a class for transformations (see Fig-\nure 1).\nafx:Transformmo:Signalcon:InputTerminalafx:Implementationmo:Signalcon:OutputTerminalstudio:produced_signalstudio:consumed_signalstudio:signalstudio:signalstudio:effectdevice:outputdevice:inputStudio OntologyMusic OntologyConnectivity OntologyAudio Effects Ontologystudio:Transform\nstudio:EffectPluginrdfs:subclassOf\nrdfs:subclassOfafx:Fxafx:implementation_of\nFigure 1 . Transformation of an audio signal described us-\ning the Studio Ontology and Audio Effects Ontology.\nAtransform can be linked to an effect implementation\nusing the studio:effect property.Using this mechanism we\ncan express details about the effect implementation involved\nin a transformation and the device state at the time of the\ntransformation. The ontology provides additional concepts\nfor the description of implementation attributes, such as the\nplugin format, operating system, parameters and parameter\nsettings (see Table 1). Instead of being conceptualised as\nstatic individuals of the respective classes, operating sys-\ntems and plugin formats are conceptualised as subclasses,\nwhich enables a more detailed description of the execution\ncontext, for instance, by specifying a particular version\nnumber of an effect plugin. This implementation class may\nalso act as the connection to the afx:Fx class specifying the\neffect type with the property afx:implementation of. Asso-\nciating events on an audio signal timeline — using appro-\npriate Music Ontology terms — to a particular transform\nand its parameters, it is possible to state where a certain\neffect has been applied during the course of a track.\nFinally, it is a common practice to automate effect pa-\nrameters in music production, i.e. the parameters of a given\neffect may change over time. Modern DAWs are able to\nstore the automation data for this purpose. In order to rep-\nresent changing effect parameter values the Audio Effects\nOntology provides the class afx:State which represents asimilar concept as the device:State class, proposed in the\ncontext of consolidated reiﬁcation in [6]. It conceptualises\nvariable attributes and relationships of an audio effect. The\nafx:State class is a subclass of event:Event in the Event\nOntology1. This allows describing a region on a signal\ntimeline, during which a certain parameter setting is true.\nThe region is deﬁned by an entry andexitpoint. These are\nsubclasses of tl:Instant of the Timeline Ontology2.\n4.2 Audio Effects Classiﬁcation\nIn order to produce metadata describing the workﬂow and\nthe elements of an audio production, it is beneﬁcial to de-\nscribe which speciﬁc audio effect implementation has been\nused for a given transformation, and also to specify the\neffect type. This facilitates the comparison of workﬂows\nindependently from the tools that have been used in the\nstudio. The conceptualisation of effect types for instance\nenables the search for similarities in the use of audio effects\nin a database of music production data. While the classi-\nﬁcation of audio effects has several applications in music\nproduction, the heterogeneity of possible taxonomies, as\nwell as the many viable points of view for organising ef-\nfects present research problems that are not easily solved.\nCreating extensible ontologies provide a possible solution\nto this problem. Musicians and music producers have a\nlarge number of digital audio effects at their disposal, while\nover 70 types of effects have been identiﬁed in academic\nresearch [21]. There are different approaches to the task of\naudio effects classiﬁcation depending on a variety of fac-\ntors. For instance, we may group effects by their percep-\ntual attributes or classify them by their underlying signal\nprocessing implementations. The best classiﬁcation de-\npends on the intended use. A developer for example would\nprobably want to emphasise signal processing techniques,\nwhereas a musician would prefer to classify effects by their\nperceptual qualities. An example of inter-disciplinary ef-\nfect classiﬁcation has been proposed in [15], as part of\nan effort to facilitate communication and collaborations\nbetween DSP programmers, sound engineers, composers,\nperformers and musicologists. To address this issue, we in-\ncorporated several linked classiﬁcation systems subsumed\nunder the concept afx:Fx in our ontology. These are based\non different criteria, including technical aspects as well as\nperceptual attributes. As a result, an MIR system using\nthe ontology may answer questions such as: Which audio\neffects affect the timing of the audio material? Which pro-\nductions used delay-based audio effects?\n4.3 Effect Parameters\nRecognising the fact that not all audio effect implementa-\ntions adhere to parameter naming conventions, we extend\nour ontology with the Parameter Ontology module. We\nconceptualise effect parameters in such a way that we can\nassign a parameter type to a parameter that is linked to an\naudio effect implementation. We distinguish between two\ntypes of parameters: numerical parameters and indexed pa-\n1http://motools.sf.net/event/event.html\n2http://motools.sf.net/timeline/timeline.htmlconcept property range some subclasses/individuals\nProduct name dc:title literal (xsd:string) -\nFX format available as Format Vst, Au, Lv2, Rtas\nOperating system os Os Windows, MacOS, Linux\nFX type implementation of Fx class1:Chorus, class2:Bandpass\nFX technique technique Technique SchroederMoorer, PhaseV ocoder\nFX parameters parameter Parameter NumParameter, IndexedParameter\nFX preset preset Preset -\nAudio inputs audio inputs literal (xsd:int) -\nTable 1 . Some of the concepts and properties for the description of a digital audio effect implementation.\nrameters. The former is set by numerical values, while the\nlatter consists of a list of string values. For instance, a\nparameter may be used to specify a ﬁlter type or a wave-\nshape for an oscillator. We may want to query for effects\nthat have a speciﬁc type of parameter (e.g. the delay time\nor attack time). Consistent parameter names, which are\nnot necessarily given, are a prerequisite for efﬁcient com-\nparison. The Parameter Ontology solves this by provid-\ning concepts for parameter types. Instead of simply la-\nbelling a parameter with a literal stating its given name,\nlinking parameters to conceptualised parameter types fa-\ncilitates retrieving this information independently from the\nactual given parameter names. Furthermore, by specify-\ning the unit (for the delay time this may be milliseconds or\nseconds) we can compare and transfer settings across dif-\nferent implementations. To achieve this we use concepts of\nthe Quantities, Units, Dimensions and Data Types (QUDT)\nontology3. Figure 2 shows a description for a delay time\nparameter.\n:parameterafx:NumParameter\"0\"par:DelayTime\nrdf:typeafx:parameter_id\nliteralsQUDT/Units OntologyAudio Effects/Parameter Ontology\"delay\"qud:QuantityValuerdfs:labelafx:parameter_min_valuerdf:type\"1\"qud:numericValueunit:Millisecondsqud:unitpar:parameter_type\nFigure 2 . Partial description for a delay time parameter as\nit appears in an effect implementation.\n4.4 Provenance\nProvenance information describes entities, activities, and\npeople involved in producing data. It enables software\nagents to track changes to data, thus ensuring a level of\ntransparency and trust by providing information about the\nsources of data items. For instance, provenance informa-\ntion can consist of a statement about who created a partic-\nular resource. A detailed review of provenance ontologies,\nboth general and discipline-speciﬁc, is provided in [5].\nDigital audio effects alter audio data and consequen-\ntially the audio features associated with it. The studio on-\ntology already provides mechanisms for describing the au-\nthor, for instance an audio engineer, that was involved in\n3http://www.qudt.org/the creation of a music production. Moreover, relating a\ntransform with an effect implementation documents which\nsoftware device has been used in the process. We introduce\nadditional provenance properties in the Audio Effects On-\ntology for describing timed audio features that have been\nproduced or altered by the application of an audio effect.\nSince the Studio and Audio Effects ontologies are devel-\noped in the context of future intelligent audio worksta-\ntions that produce detailed metadata about the audio ma-\nterial and workﬂows in music production, the inclusion of\nthe provenance properties facilitates adding effect-speciﬁc\nmetadata to annotations of audio signals. For describing\nprovenance information in the DAFX ontology, we intro-\nduce subproperties subsuming properties of the The Open\nProvenance Model V ocabulary (OPMV), that is based on\nthe Open Provenance Model (OPM) [12]. In OPM, an arte-\nfactis deﬁned as an ”immutable piece of state” which may\nrefer to an actual physical object or a digital representation.\nAprocess is the action that creates artefacts, be it by acting\non an existing artefact or by being caused by one. Agent\ndescribes an entity involved in a process by enabling or\ninﬂuencing its execution. Edges denote causal dependen-\ncies between its source (the effect) and its destination (the\ncause). OPM can be used in combination with terms from\nthe Dublin Core speciﬁcation which on its own we found\nto be insufﬁcient for our requirements.\nafx:Transformafx:Implementationevent:Eventdc:Agentafx:derived_fromafx:developerdc:publisherafx:generated_byopmv:wasControlledBy\nFigure 3 . Provenance properties in the Audio Effects On-\ntology.\nUsing these properties of the Audio Effects Ontology\n(Figure 3), we can represent provenance information about\naudio features. For instance, an echo effect produces ad-\nditional note onsets, since it adds the delayed signal to the\noriginal. We can express that such an event (artefact) has\nbeen generated by a given transformaion (process), and\nthat the transformation was controlled by an audio effect\nimplementation, the echo effect (agent). We can express\nthat the existence of an audio feature produced by the trans-\nformation is dependent on a previously existing feature.\nFor instance, a delayed note onset event may be derived\nfrom an onset in the original audio material prior to trans-formation.\n5. APPLICATIONS\n5.1 Creation of Music Production Studio Databases\nfor Information Retrieval\nDesigned as an extension of the Studio Ontology, knowl-\nedge represented with the Audio Effects Ontology can be\nseamlessly integrated into a database using the Studio On-\ntology framework. This information may include details\nabout audio effects, such as the type of effects, their char-\nacteristics and parameter conﬁguration of speciﬁc instances\nof effects applied in a music production project. Including\ndetailed information about the application of audio effects\nmay facilitate retrieval for various purposes for music pro-\nduction. This also facilitates reproducibility of workﬂows\nconcerning the application of audio effects. Furthermore,\nby employing techniques similar to those applied in mu-\nsic recommendation systems based on Semantic Web tech-\nnologies [4] [11], the Audio Effects Ontology provides a\nframework for the retrieval of audio effects given a set of\nspeciﬁed criteria. These criteria can be technical aspects,\nas well editorial information such as the developers or ven-\ndors involved in audio effect implementations.\n5.2 Publishing effect data on the Semantic Web\nThe Audio Effects Ontology is capable of reﬁning the re-\ntrieval of songs based on production procedures as pro-\nposed in the context of the Studio Ontology framework.\nData about audio effects may be published as Linked Data\nresources. This allows for the creation of an audio effects\ndatabase on the Semantic Web, and facilitates the incor-\nporation of data about the application of audio effects in\nmusic productions in online catalogues and content-based\nmusic recommendation systems. For example, this enables\nthe retrieval of songs characterised by certain types of ef-\nfects, or the actual effect used in a track. Assuming a large\nenough database of production data, this also allows for\nmusicological research with regard to trends in the appli-\ncation of audio effects. Furthermore, plugin presets may\nbe shared on the Semantic Web and retrieved by users and\nagents helping students or the work of professional engi-\nneers.\n5.3 Adaptive Audio Effects\nPrevious implementations of adaptive audio effects [16]\nthat map high-level features stored in a database to con-\ntrol parameters either use proprietary non-standardised for-\nmats, or MPEG-7 descriptors for the representation of au-\ndio features (e.g. [3]). However, the majority of meta-\ndata standards only specify the syntax of documents, while\nthe semantics remain implicit and hardcoded in procedural\nsoftware. Using Semantic Web technologies on the other\nhand, provide a uniform way of encoding and linking infor-\nmation. The Audio Effects Ontology provides the means\nfor integrating adaptive audio effects seamlessly into a mu-\nsic production system that supports RDF based knowledge\nrepresentation and retrieval. Examples of this new class of\naudio effects are given in [19, 20].6. CASE STUDY: RETRIEVING AUDIO EFFECT\nSETTINGS BY QUERYING METADATA\nAudio effect play a crucial role in creating the right “sound”\nof a track in most contemporary musical genres. Repro-\nducing the application of effects including their exact pa-\nrameter settings is therefore a very important use case.\nThe Audio Effects Ontology covers the necessary con-\ncepts to represent the information about an effect transfor-\nmation in such a way that it is possible to query for a given\neffect and its parameter settings at a given temporal loca-\ntion relative to the audio signal. In the following we show\nan example of how we can query data represented with the\nAudio Effects Ontology to retrieve information about the\napplication of audio effects in a music production project,\nwhere production metadata is available using our ontology\nframework. We may want to investigate an audio effect\nfound in an annotated music production where one minute\ninto the song an echo effect has been applied to the guitar.\nUsing the appropriate SPARQL4queries we are able to re-\ntrieve the necessary information in order to identify com-\nparable effect plugins present in our studio setup. First, we\nquery the project database for the parameter types, settings\nand units of an echo effect applied on the speciﬁed track at\nthe speciﬁed time instant (Listing 1). We assume the time-\nline for the guitar track as :guitarTimeline starting at the\nbeginning of the project.\nSELECT ?parameter_type ?value ?unit\nWHERE {\n?transform a afx:Transform ;\nstudio:effect [ aafx:Implementation ;\nimplementation_of ?class1:Echo ;\nafx:state ?state ] .\n?event a event:Event ;\nevent:time [ atl; instant ;\ntl:timeline :guitarTimeline ;\ntl:at \"60.0s\"ˆˆxsd:duration ] .\n?state afx:entry ?event ;\nafx:parameter [ aafx:Parameter ;\npar:parameter_type ?parameter_type ;\nqud:value [ qudt:numericValue ?value ]\n;\nafx:unit ?unit ] . }\nListing 1. Query retrieving effect settings.\nThe parameter unit speciﬁcations can form the basis of\nthe conversion of settings between implementations hav-\ning parameters of the same type with different units. This\nmay be useful in case the implementations used originally\nare not available, and we wish to approximate the trans-\nformations with effects at our disposal in our studio. In\na second step we query the database describing our studio\nfacility for existing echo effect implementations having pa-\nrameters of the same type (Listing 2). The query retrieves\nall the echo effect implementations available in our studio\nsetup that have parameters of the same type as the one used\nin the production project in question. Since we also know\nthe respective units for the parameter values it is possible\nto transfer the settings for the retrieved effect implemen-\ntations. The information can act as a starting point for the\n4SPARQL is a recursive acronym for SPARQL Protocol and RDF\nQuery Language.SELECT ?publisher ?fx_name ?time_name\n?time_unit ?parameter_name ?parameter_unit\nWHERE {\n:ourStudio studio:equipment ?device ;\n?device a afx:Implementation ;\ndc:publisher [ fc:name ?publisher ] ;\ndc:title ?fx_name ;\nimplementation_of [ class1:Echo ] ;\nhas_parameter [ aafx:Parameter ;\nafx:parameter_type par:DelayTime ;\nrdfs:label ?time_name ;\nafx:unit ?time_unit ] ,\n[aafx:Parameter ;\nafx:parameter_type par:LowpassFilter ;\nrdfs:label ?lowpass_name ;\nafx:unit ?lowpass_unit ] . }\nListing 2. Query retrieving effect implementations.\napproximation of transformations performed by one imple-\nmentation of a given effect type with another implementa-\ntion of the same effect family.\n7. CONCLUSIONS AND FUTURE WORK\nWe presented a Semantic Web Ontology covering the do-\nmain of audio effects and their implementations that is de-\nsigned as an extension to the Studio Ontology framework.\nUsing the ontology it is possible to create detailed meta-\ndata about the application of effects in music production\nprojects and to classify and describe audio effect imple-\nmentations. The applications of the Audio Effects On-\ntology range from adaptive audio effects using high-level\nsemantic metadata, content-aware music production tools,\nand searchable audio effect databases. We have shown that\nquerying RDF databases storing information about projects,\nstudio equipment and available effect implementations en-\nables access to detailed information about workﬂows, and\nfacilitates their reproduction. Moreover, it allows for the\nanalysis and comparison of musical works with regards to\nthe use of audio effects.\nFuture work includes the development of software ap-\nplications that support the Studio Ontology framework, such\nas content-aware audio production tools that automatically\nretrieve information and annotate multitrack projects auto-\nmatically.\n8. REFERENCES\n[1] G. Antoniou and F. van Harmelen. Web ontology language:\nOWL. in S. Staab and R. Studer (eds.), Handbook on On-\ntologies, International Handbooks on Information Systems,\nSpringer-Verlag Berlin Heidelberg, pages 91–110, 2009.\n[2] K. Barkati, A. Bonardi, A. Vincent, and F. Rousseaux.\nGAMELAN: A knowledge management approach for digi-\ntal audio production workﬂow. In E. Mercier-Laurent, editor,\nProceedings of Artiﬁcial intelligence for Knowledge Manage-\nment Workshop (AI4KM) of ECAI , 2012.\n[3] O. Celma, E. G ´omez, J. Janer, F. Gouyon, P. Herrera, and\nD. Garcia. Tools for content-based retrieval and transforma-\ntion of audio using mpeg-7: the spofﬂine and the mdtools.\npresented at the AES 25th International Conference, London,\nUK, June 2004.[4] `O. Celma and X. Serra. FOAFing the music: Bridging the se-\nmantic gap in music recommendation. Lecture Notes in Com-\nputer Science , 4273, 2006.\n[5] L. Ding, J. Bao, J. Michaelis, J. Zhao, and D. L. McGuinness.\nReﬂections on provenance ontology encodings. Proceedings\nof the 3rd International Provenance and Annotation Work-\nshop (IPAW), Troy, New York, USA , June 2010.\n[6] G. Fazekas. Semantic Audio Analysis - Utilities and Applica-\ntions . PhD thesis, Queen Mary University of London, 2012.\n[7] G. Fazekas and M. B. Sandler. The studio ontology frame-\nwork. Proceedings of the 12th International Society for Music\nInformation Retrieval Conference (ISMIR 2011) , 2011.\n[8] M. R. Genesereth and N. J. Nilsson. Logical Foundations\nof Artiﬁcial Intelligence . Morgan Kaufmann Publishers, San\nMateo, CA, USA., 1987.\n[9] P. Herrera and X. Serra. Audio descriptors and descriptor\nschemes in the context of mpeg-7. Proceedings of the Inter-\nnational Computer Music Conference, Beijing, China , 1999.\n[10] I. Horrocks. Ontologies and the Semantic Web. Communica-\ntions of the ACM , V ol. 51(12):pp. 58–67., 2008.\n[11] K. Jacobson, G. Fazekas, Y . Raimond, and M. Smethurst.\nMusic and the web of linked data. Tutorial presented at the\n10th International Society for Music Information Retrieval\nConference (ISMIR), Kobe, Japan , 2009.\n[12] L. Moreau, B. Clifford, J. Freire, J. Futrelle, Y . Gil, P. Groth,\nN. Kwasnikowska, S. Miles, P. Missier, J. Myers, B. Plale,\nY . Simmhan, E. Stephan, and J. V . den Bussche. The open\nprovenance model core speciﬁcation (v1.1). Future Genera-\ntion Computer Systems , Preprint, July 2010.\n[13] M.-F. Plassard, editor. Functional Requirements For Bibli-\nographic Records : ﬁnal report / IFLA Study Group on\nthe Functional Requirements for Bibliographic Records , vol-\nume 19. K.G. Saur, 1998.\n[14] Y . Raimond, S. Abdallah, M. Sandler, and F. Giasson. The\nmusic ontology. Proceedings of the International Conference\non Music Information Retrieval , 2007.\n[15] V . Verfaille, C. Guastavino, and C. Traube. An interdisci-\nplinary approach to audio effect classiﬁcation. Proceedings\nof the 9th International Conference on Digital Audio Effects\n(DAFx-06), Montreal, Canada , September 2006.\n[16] V . Verfaille, U. Z ¨olzer, and D. Arﬁb. Adaptive digital au-\ndio effects (A-DAFx): A new class of sound transformations.\nIEEE Transactions on Audio, Speech and Language Process-\ning, 14(5), 2006.\n[17] M. V oorvelt. New sounds, old technology. Organised Sound ,\n5(2):67–73, 2000.\n[18] T. Wilmering, G. Fazekas, and M. B. Sandler. Towards on-\ntological representations of digital audio effects. Proceedings\nof the 14th International Conference on Digital Audio Effects\n(DAFx-11), Paris, France , 2011.\n[19] T. Wilmering, G. Fazekas, and M. B. Sandler. High level se-\nmantic metadata for the control of multitrack adaptive audio\neffects. presented at the 133rd Convention of the AES, San\nFrancisco, USA , 2012.\n[20] T. Wilmering and M. Sandler. RDFx: Audio effects util-\nising musical metadata. presented at the 4th IEEE Inter-\nnational Conference on Semantic Computing (IEEE ICSC\n2010), Pittsburgh, PA, USA , 2010.\n[21] U. Z ¨olzer. DAFX - Digital Audio Effects . J. Wiley & Sons,\n2nd edition, 2011."
    },
    {
        "title": "Simultaneous Unsupervised Learning of Flamenco Metrical Structure, Hypermetrical Structure, and Multipart Structural Relations.",
        "author": [
            "Dekai Wu"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1414772",
        "url": "https://doi.org/10.5281/zenodo.1414772",
        "ee": "https://zenodo.org/records/1414772/files/Wu13.pdf",
        "abstract": "We show how a new unsupervised approach to learning musical relationships can exploit Bayesian MAP induction of stochastic transduction grammars to overcome the challenges of learning complex relationships between multiple rhythmic parts that previously lay outside the scope of general computational approaches to music structure learning. A good illustrative genre is flamenco, which employs not only regular but also irregular hypermetrical structures that rapidly switch between 3/4 and 6/8 mediocompas blocks. Moreover, typical flamenco idioms employ heavy syncopation and sudden, misleading off-beat accents and patterns, while often elliding the downbeat accents that humans as well as existing meter-finding algorithms rely on, thus creating a high degree of listener “surprise” that makes not only the structural relations, but even the metrical structure itself, ellusive to learn. Flamenco musicians rely on both complex regular hypermetrical knowledge as well as irregular real-time clues to recognize when to switch meters and patterns. Our new approach envisions this as an integrated problem of learning a bilingual transduction, i.e., a structural relation between two languages—where there are different musical languages of, say, flamenco percussion versus zapateado footwork or palmas hand clapping. We apply minimum description length criteria to induce transduction grammars that simultaneously learn (1) the multiple metrical structures, (2) the hypermetrical structure that stochastically governs meter switching, and (3) the probabilistic transduction relationship between patterns of different rhythmic languages that enables musicians to predict when to switch meters and how to select patterns depending on what fellow musicians are generating.",
        "zenodo_id": 1414772,
        "dblp_key": "conf/ismir/Wu13",
        "keywords": [
            "unsupervised approach",
            "Bayesian MAP induction",
            "stochastic transduction grammars",
            "complex relationships",
            "multiple rhythmic parts",
            "flamenco genre",
            "hypermetrical structures",
            "irregular rhythms",
            "listener surprise",
            "bilingual transduction"
        ],
        "content": "SIMULTANEOUS UNSUPERVISED LEARNING OF FLAMENCO\nMETRICAL STRUCTURE, HYPERMETRICAL STRUCTURE, AND\nMULTIPART STRUCTURAL RELATIONS\nDekai Wu\nHKUST, Human Language Technology Center, Department of CSE, Hong Kong\ndekai@cs.ust.hk\nABSTRACT\nWe show how a new unsupervised approach to learning\nmusical relationships can exploit Bayesian MAP induction\nof stochastic transduction grammars to overcome the chal-\nlenges of learning complex relationships between multiple\nrhythmic parts that previously lay outside the scope of gen-\neral computational approaches to music structure learning.\nA good illustrative genre is flamenco, which employs not\nonly regular but also irregular hypermetrical structures that\nrapidly switch between 3/4 and 6/8 mediocompas blocks.\nMoreover, typical flamenco idioms employ heavy synco-\npation and sudden, misleading off-beat accents and pat-\nterns, while often elliding the downbeat accents that hu-\nmans as well as existing meter-finding algorithms rely on,\nthus creating a high degree of listener “surprise” that makes\nnot only the structural relations, but even the metrical struc-\nture itself, ellusive to learn. Flamenco musicians rely on\nboth complex regular hypermetrical knowledge as well as\nirregular real-time clues to recognize when to switch meters\nand patterns. Our new approach envisions this as an inte-\ngrated problem of learning a bilingual transduction, i.e., a\nstructural relation between two languages—where there are\ndifferent musical languages of, say, flamenco percussion\nversus zapateado footwork or palmas hand clapping. We\napply minimum description length criteria to induce trans-\nduction grammars that simultaneously learn (1) the multi-\nple metrical structures, (2) the hypermetrical structure that\nstochastically governs meter switching, and (3) the prob-\nabilistic transduction relationship between patterns of dif-\nferent rhythmic languages that enables musicians to predict\nwhen to switch meters and how to select patterns depend-\ning on what fellow musicians are generating.\n1. INTRODUCTION\nLittle work has been done on automatic algorithms for\nlearning across multiple parts in rhythmically complex mu-\nsic genres such as flamenco, despite a respectable history of\nwork on automatic meter finding utilizing a wide range of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page.\n© 2013 International Society for Music Information Retrieval.underlying modeling paradigms. Repetition of patterns is a\ncentral feature in many automatic meter finding algorithms\nsuch as that of Steedman [23]. Early approaches were rule-\nbased (e.g., Longuet-Higgins and Steedman [16]), while\nothers employed neural nets (e.g., Desain and Honing [6])\nor preference rules (e.g., Povel and Essens [19], Temper-\nley and Sleator [28]). More recent approaches are based\non probabilistic modeling, such as the generative models\nof Cemgil et al. [4]), Raphael [20], and Temperley [27].\nGenerally, however, these approaches are based on rela-\ntively simplistic assumptions about straightforward duple,\ntriple, or 4/4 meters accompanied by the regular occurrence\nof strong accents on or near the downbeat. These assump-\ntions are widely understood to apply primarily to Western\nmusic conventions rather than worldwide music genres that\ncan be rhythmically much more complex. Flamenco con-\nventions, for example, often defy for example what Ler-\ndahl and Jackendoff [14] termed the “Strong Beat Early”\nrule—omitting the downbeat accent is a typical idiom, and\nin fact the strong beat is often understood in flamenco to be\nlate. Moreover, flamenco rhythms employ continual meter\nswitching in both regular and irregular ways, with a com-\nplex hypermetrical language governing the switching, and\nmake frequent use of polyrhythm in addition.\nEven less work has been done to date on computational\napproaches to analysis of flamenco. Models such as those\nof Diaz-Banez et al. [7], Gomez and Bonada [12], Guas-\ntavino et al. [11], Mora et al. [17], or Thul and Toussaint\n[29] represent various intriguing attacks on specific aspects\nof flamenco, but do not attempt to actually induce the mu-\nsical structures.\nTo attack more complex rhythmic forms such as these,\nwe propose an approach based on unsupervised induction\nof stochastic transduction grammars. On one hand, this fol-\nlows the generative modeling paradigm of GTTM [14] and\nSteedman [24] or [25] in which various aspects of music\ncan each be modeled as languages that can be generated\nby formal grammars. On the other hand, to facilitate auto-\nmatic learning and scaling up of the models, we formulate\nthe task in terms of stochastic grammars that describe prob-\nabilistic models of musical structure.\nThe majority of previous work on stochastic grammat-\nical models for music employs flat Markov models and/or\nhidden Markov models (HMMs). For example, both the\nContinuator model of Pachet [18] and the Factor Oracle\nmodels of Assayag et al. [2] use Markov models to learnmusic improvisation conventions-an approach further ex-\nplored by François et al. [8] and [9]. A grammar induction\napproach for learning jazz grammars under Markovian as-\nsumptions is proposed by Gillick et al. [10]. Relatively\nlittle has been done on musical structure modeling using\nstochastic context-free grammars [13]. Related work on\nunsupervised learning of CCMs (a variant of SCFGs) for\nmusical grammars includes that of Swanson et al. [26], or\nthe Data Oriented Parsing approach of Bod [3].\nStochastic grammars are excellent for describing indi-\nvidual aspects of music. Much of music, however, is about\nthe loosely coupled relationships between multiple strands\nof different kinds of sequences taking place in parallel.\nOur new approach differs from previous stochastic gram-\nmatical models of music in that we (1) shift to bilingual\nstochastic transduction grammars instead of conventional\nmonolingual stochastic grammars, allowing us to model\nlearning structural relations between different musical lan-\nguages of separate percussive flamenco parts, and (2) ap-\nply a new grammar induction strategy that searches for the\nBayesian MAP (maximum a posteriori ) model encompass-\ning metrical relations, hypermetrical relations, and proba-\nbilistic transduction relations in a single integrated process.\n2. STOCHASTIC TRANSDUCTION GRAMMARS\nIn classic formal language theory, a transduction is a\nrelation between two languages, which is exactly what we\nwish to induce. A transduction grammar ortranslation\ngrammar (TG) is a bilingual grammar of transductions,\nand describes structured relations between two languages\n[1], [15]. (An equivalent term “synchronous grammar” used\nonly in computational linguistics is not as long established\nor widely understood throughout computer science.)\nThus stochastic transduction grammars are probabilis-\ntic bilingual grammars of transductions, and describe struc-\ntured relations between two languages probabilistically —\nwhich means that stochastic TGs do not suffer from the\noverly rigid constraints of non-stochastic transduction mod-\nels, and can be automatically learned [31]. In a stochas-\ntic transduction grammar, a probability distribution is im-\nposed over the space of possible derivations. This is typ-\nically done by associating a conditional probability with\neach rule, representing the probability that any nonterminal\nsymbol matching the left-hand-side of the rule generates\nchildren matching the right-hand-side of the rule. Tech-\nniques have been developed for numerous tasks utilizing\nstochastic transduction grammars including aligning bilin-\ngual corpora, unsupervised segmentation and annotation of\nbilingual corpora, automatic induction of bilingual corre-\nspondences, grammar induction for stochastic TGs, and so\non [32].\nIn this paper we will make use of stochastic monotonic\ntransduction grammars , which in terms of generative ca-\npacity sit in the hierarchy of transduction grammars be-\ntween stochastic finite-state transducers and linear inver-\nsion transduction grammars, as detailed in [32].\nA monotonic transduction grammar or MTG (equiva-\nlent to the “simple syntax-directed transduction grammar”or “simple SDTG” of Aho and Ullman) in normal form is\na tuple ⟨N;\u0006;∆; S; R⟩where Nis a finite nonempty set\nof nonterminal symbols, \u0006is a finite nonempty set of input\nlanguge symbols, ∆is a finite nonempty set of output lan-\nguage symbols, S2Nis the designated start symbol, and\nRis a finite nonempty set of syntax-directed transduction\nrules on the forms:\nS!A; A !φ; A !e/f\nwhere A2N,φ2NNN\u0003, andeandfareterminal\nsymbols representing musical event segments as follows.\nStrings in both the languages represent sequences of sym-\nbolic musical event tokens. A “sentence” is a full musical\npassage for a single instrumental part, whereas a “bisen-\ntence” is a matched musical passage with both instrumental\nparts. For convenience of musical interpretation, instead of\nwriting musical sequences using linguistic string notation,\nwe shall use conventional music staff notation for musical\nevent segments eandf, as in Figure 1. Just as we consider\nthe monolingual terminal symbols eandfto represent mu-\nsical event segments, we consider the bilingual e/fnota-\ntion to denote a biterminal symbol representing a paral-\nlel pair of musical event segments from different musical\ninstruments. Technically, e/f2(\u0006\u0003\u0002∆\u0003)\u0000(ϵ/ϵ), in\nwhich we exclude the degenerate case of pairing a zero-\nlength empty segment ϵwith another zero-length empty\nstring ϵto avoid unnecessary complications arising from\ninfinite recursion.\n3. TRANSDUCTION GRAMMAR INDUCTION\nIn this section we describe our new model for unsuper-\nvised induction of stochastic transduction grammars. For\nconcrete examples of the abstract model, see Section 4.\nMinimizing description length We begin with the over-\nall Bayesian model whose posterior we wish to maximize.\nWe seek the maximum a posteriori (MAP) model given the\ndata; that is, we attempt to optimize the posterior probabil-\nity following Bayes’ rule\nP(\bjD) =P(\b)P(Dj\b)\nP(D)\nwhere \bis the model and Dis the training data. The prior\nprobability of the data is constant during search, which gives\nus the following search problem:\nargmax\n\bP(\b)P(Dj\b)\nIn our case, the probability of the data given the model\ncan be determined through parsing since it is a grammar.\nThe prior of the model is, however, somewhat more com-\nplicated because it must incorporate the effect of both the\nstructure and the parameters of the model.\nP(\b) = P(\bG)P(\bSj\bG)P(\u0012\bj\bS;\bG)\n\bGis a global prior over possible model formalisms,\nwhich we set to be the space of possible monotonic trans-\nduction grammars, P(\bSj\bG)is a prior on the modelstructure given the model formalism, and P(\u0012\bj\bS;\bG)\nis a prior over the model parameters given the formalism\nand the structure. We approximate the prior over the model\nstructure using the description length of the model:\n\u0000log2(P(\bSj\bG))/DL(\bS)\nThe description length of a model is calculated by summing\nthe length of all the rules, where each unique (monolin-\ngual) terminal segment is efficiently given a unique Huff-\nman encoding. This avoids redundant double-counting of\nterminal segments that appear in more than one rule. The\nlength of a symbol is proportional to \u0000log2(1\nM)\nwhere\nM= 2 + N+ \u0006 + ∆ is the total number of symbols\n(Nis the number of nonterminals, \u0006is the size of the L0\nvocabulary and ∆is the size of the L1vocabulary). Thus,\nfor example, reducing the number of distinct nonterminals\nin a grammar reduces its description length.\nWe set the prior over the model parameters to be a uni-\nform Dirichlet distribution over right-hand sides given left-\nhand sides:\nP(\u0012\bj\bS;\bG) =N\u00001∏\ni=01\nB(\u000b0; \u000b1; :::; \u000b Rni\u00001)Rni\u00001∏\nj=0\u0012ni\n\b(j)\nwhere Nis the number of nonterminals, Rniis the set of\nrules where niis the left-hand side, and \u0012ni\n\bis a function\nthat gives the rule probabilities for rule where the left-hand\nside is ni. Fleshing out the search problem, we have:\nargmax\n\bG;\bS;\u0012\bP(\bG)P(\bSj\bG)P(\u0012\bj\bS;\bG)P(D j\bG;\bS; \u0012\b)\nRecall that are restricting \bGto monotonic transduction\ngrammars. We further divide the search into two phases:\na top-down rule segmentation phase, which focuses on the\nstructural induction to optimize P(\bSj\bG)andP(Dj\n\bG;\bS; \u0012\b), and a parameter tuning phase, which focuses\nonP(\u0012\bj\bS;\bG)andP(Dj\bG;\bS; \u0012\b).\nInitializing model structure The induction procedure starts\nwith a transduction grammar that memorizes the training\ndata as well as possible, and generalizes from there. The\ntransduction grammar that best fits the training data is the\none where the start symbol rewrites to the full sentence\npairs that it has to generate. It is also possible to add any\nnumber of nonterminal symbols in the layer between the\nstart symbol and the bisentences without altering the prob-\nability of the training data. We take initial advantage of this\nby allowing for one intermediate symbol so that the start\nsymbol conforms to the normal form and always rewrites\nto precisely one nonterminal symbol.\nOur initial model thus consists of the rule S!Aplus\nnumerous rules of the form A!e0::T/f0::Vwhere Sis\nthe start symbol, Ais the nonterminal, Tis the length of the\noutput sentence, and Vis the length of the input sentence.\nGeneralizing model structure In order to generalize the\ninitial monotonic transduction grammar we need to identifyparts of the existing biterminals that could be validly used\nin isolation, and allow them to combine with other seg-\nments. This is the very feature that allows a finite transduc-\ntion grammar to generate an infinite set of sentence pairs;\nwhen we do this, we move some of the probability mass\nwhich was concentrated in the training data out to other data\nthat are still unseen—i.e. we generalize from the training\ndata. The general strategy is to propose a number of sets of\nbiterminal rules and a place to segment them, estimate the\nposterior given the sets and commit to the best set. That\nis: we do a greedy search over the power set of possible\nsegmentations of the rule set. This intractable problem can\nbe reasonably efficiently approximated.\nThe key component in the approach is the ability to eval-\nuate the change in a posteriori probability if a specific seg-\nmentation was made in the grammar. This can then be\nextended to a set of segmentations, which only leaves the\nproblem of generating suitable sets of segmentations. Any\nsegment that can be reused maximizes the model prior. The\nmore rules we can find with shared biaffixes, the more likely\nwe are to find a good set of segmentations.\nOur algorithm takes advantage of the above observation\nby focusing on both the monolingual and bilingual affixes\n(i.e., prefixes or suffixes) found in the training data. Each\naffix orbiaffix defines a set of lexical rules paired up with\na possible segmentation. We evaluate the (bi)affixes by es-\ntimating the change in posterior probability associated with\ncommitting to all the segmentations defined by a (bi)affix.\nThis allows us to find the best set of segmentations, and\ncommit to as many of them as possible. Moreover, as we\ngenerate new nonterminal categories during this process,\nwe also use affixes and biaffixes to suggest possible merges\nof the nonterminal categories. This minimizes the parsing\nefforts, which are more expensive. A priority queue based\nagenda keeps track of possible candidates for rule segmen-\ntation and nonterminal category actions, and always greed-\nily commits at each step to the action that best improves\noverall posterior probability:\nG = the transduction grammar\nbiaffixes_to_rules = index of G's transduction rules by their (bi)affixes\nlhs_to_rules = index of G's transduction rules by their LHS nonterms\nagenda = [] // Priority queue of actions by their DL impact on G\nfor each affix or biaffix x in G :\ndelta = eval_seg_post(x, biaffixes_to_rules[x], G)\nif (delta < 0)\nagenda.add(SEGMENT, x, delta)\nwhile agenda.pop(act, x) < 0 :\nif (act == SEGMENT)\nreal_delta = eval_seg_post(x, biaffixes_to_rules[x], G)\nif (real_delta < 0)\nG, modified_rules = segment_rules(x, biaffixes_to_rules[x], G)\nfor each pair y of nonterms serving as LHS of modified_rules\nthat share a common (monolingual or bilingual) RHS :\ndelta = eval_merge_post(x, biaffixes_to_rules[x], G)\nagenda.add(MERGE, y, delta)\nelse if (act == MERGE)\nreal_delta = eval_merge_post(y, lhs_to_rules[y], G)\nif (real_delta < 0)\nG = merge_nonterms(y, lhs_to_rules[y], G)\nNote that both affixes and biaffixes are handled in biaf-\nfixes_to_rules (since an affix can be regarded as a spe-\ncial case of a biaffix where one of the two affixes is the\nempty string ϵ). We have written eval_seg_post andseg-\nment_rules as shorthand for the above-discussed evalua-\ntion of the impact of a rule segmentation action upon the\u0006\u0006\u0006 \u0006\u0006\u0006\u0006\u0006 \u0006\u0006 \u0006\u0006 \u0006\u0006\n\u0006\u0006\u0006 \u0006\u0006 \u0006\u0006 \u0006\u0006\u0006\u0006\u0006 \u0006\u0006 \u0006\u0006 \u0006\u0006\n\u0001\u0002\u0002 \u0001\u0001\u0001\u0002\u0002 \u0001 \u0001 \u0001\u0001\n\u0002\u0002 \u0001 \u0001 \u0002\u0002\u0002 \u0001 \u0002 \u0001\n\u0007\u0007\u0007\u0007\n\u0003\u0003\u0003\u0003\u0003\u0005\u0005\u0005\u0005\u0003\u0004 \u0004\u0005\u0004\u0005 \u0005\u0005\u0003\n\u0003\u0003 \u0003\u0003\u0003\u0003\u0005\u0005\u0003\u0005 \u0005\u0005\u0004 \u0004\u0005\u0005\u0003 \u0005\u0005\u0005\u0005\u0005\n\u0003\u0003\u0003\u0003\u0005\u0003\u0004\u0005 \u0003\u0004\u0005\n\u0003\u0003\u0003\u0005\u0004\u0005\u0005 \u0005\u0003\u0005\n\u0003\u0003\u0003\u0003\u0005\u0005\u0004\u0005 \u0005\u0005\u0003\n\u0003\u0003\u0003\u0005\u0005\u0003\u0004 \u0005\u0003\u0004\n\u0003\u0003\u0003\u0004\u0003\u0004\u0005 \u0005\u0005\u0003\n\u0003\u0003 \u0003\u0003\u0005\u0005\u0003\u0005 \u0005\u0005\u0004\n\u0003\u0003\u0004\u0005\u0005\u0003 \u0005\u0005\u0005\n\u0003\u0003\u0003\u0003 \u0003\u0003\u0003\u0005\u0005\u0003\u0004 \u0005\u0003\u0004 \u0005\u0005\u0004\u0005 \u0005\u0005\u0003\n\u0003\u0003\u0003\u0003 \u0003\u0003\u0003\u0003\u0005\u0005\u0004 \u0004\u0005\u0005 \u0005\u0003\u0005\u0005 \u0005\u0004\u0005\n\u0003\u0003\u0003 \u0003\u0003\u0003\u0003\u0003\u0005\u0005\u0003\u0005 \u0005\u0005\u0004 \u0004\u0005\u0005\u0003 \u0003\u0005\u0005\n\u0003\u0003\u0004\u0005\u0005\u0003\u0003\u0003\u0003\u0005\u0005\u0003\u0004 \u0004\u0005\u0004\n\u0003\u0003\u0003\u0005\u0005\u0005\u0003 \u0005\u0005\u0003\n\u0003\u0003\u0003\u0003\u0003\u0005\u0005\u0004 \u0004\u0005\u0005\n\u0003\u0003\u0003\u0004\u0005\u0005\u0005 \u0005\u0005\u0003\n\u0003\u0003\u0003\u0003 \u0003\u0005\u0005\u0003\u0004 \u0005\u0005\u0004\n\u0003\u0003\u0003\u0003\u0004\u0005\u0005\u0003 \u0003\u0005\u0005\n\u0003\u0004\u0005\u0005\nPalmasCajónPalmasCajón\nA\u0003ĺ\nA\u0003ĺ\u0003A A\nS\u0003ĺ\u0003A\nA\u0003ĺFigure 1 .Initial transduction grammar with two training examples (see text). Lexical transduction rules are shown with\ntheir biterminals in cajón andpalmas staves using standard music notation for sequences instead of character strings.\nposterior. This consists of removing the rule being seg-\nmented, inserting the three new rules (one structural and\ntwo terminal) along with two new nonterminals, and uni-\nformly distributing the segmented rule’s probability mass\nover the three new rules; exact mathematical details are\nin [21]. Similarly, the shorthand eval_merge_post and\nmerge_nonterms denote consolidating the probability mass\nof two nonterminal categories.\nThe change in the model prior is easy to estimate, as it\nis proportional to the change in grammar length when the\nold rule is removed and the new rules are inserted (keeping\nin mind that a rule can only be added once, so if it already\nexists inserting it will not change the description length).\nThe change in the probability of the data given the model\nis expensive to get through biparsing the data, so instead we\naccumulate enough statistics during biparsing to be able to\nmake an educated guess. We follow the analogous proce-\ndure after merging two nonterminals.\nOptimizing model parameters Although the iterative seg-\nmentation of the rules result in reasonable parameters, there\nis still room for improvement. In this phase we consider\nthe model structure to be fixed, and optimize the model\nparameters to give the highest possible posterior probabil-\nity, i.e., we fix \bS(to be what we arrived at using the al-\ngorithm described in the previous section), as well as \bG\n(which remains fixed as MTGs). The two remaining free\nfactors in the MAP are thus: P(\u0012\bj\bS;\bG)andP(Dj\n\bG;\bS; \u0012\b)—the prior over the parameters and the condi-\ntional probability of the data given the complete model.\nThe prior is, as described earlier, a uniform Dirichlet\ndistribution over all the rules, which can be described using\na concentration parameter. To get the conditional, we have\nto biparse the training data, and to maximize it, we per-\nform expectation maximization [5], as a special case of EM\nas specified for inversion transduction grammars by [30].\nThis requires biparsing, which we do with the cubic time\nbiparsing algorithm described in [22].\n4. RESULTS\nFor our experiments we chose the buleriás form of fla-\nmenco because of its metrical and hypermetrical complex-\nity. Various passages from multi-track recordings were col-\nlected, from which were taken approximately 5 minutes of\naligned cajón (box drum) and palmas (clapping percussion)\ntracks. Symbolic “tick” notes were extracted from the ca-jónandpalmas tracks via heuristics primarily relying on\na combination of volume and frequency range filters. For\nthecajón the notes were separated into “bass”, “tone”, and\n“tip” categories. For the palmas the notes were separated\ninto ordinary or accented notes. All notes were quantized\ninto 1/16th note intervals.\nAs no meaningful gold standard for mechanically eval-\nuating the quality of the learned model exists, only subjec-\ntive evaluations are possible. Moreover, variance in fla-\nmenco expectations is extremely large, and our subjective\nevaluations were so close to 100% accuracy so as to be\nswamped by statistical variance in human judgments. Rea-\nsons for the high accuracy of the model can best be seen by\ntracing specifically how it learns, looking at a small con-\ncrete subset of the training set.\nTwo short training passages are shown in Figure 1. Note\nthat in our induction method’s rule segmenting strategy,\nthe initial transduction grammar starts out containing one\nrule for each training example, each belonging to the same\ngeneric category Aas shown by the left-hand-side nonter-\nminal. In addition, the grammar contains the start rule and\na low-probability “glue rule” that allows arbitrary concate-\nnation of any valid sequences when all else fails.\nLearning metrical structure In the early iterations, the\nMDL-driven induction algorithm primarily works toward\nlearning transduction patterns that are a single full compas\nin length, i.e., a twelve beat cycle. This accurately mirrors\nthe primary (mixed meter) structure that is most common\nand most fundamental in flamenco.\nThe two lexical transduction rules in the initial transduc-\ntion grammar of Figure 1share no biaffixes, but the first\nhalf of the first lexical rule’s cajón part is repeated at the\nend of the second lexical rule’s cajón part. Thus, the first\ninduction decision is to segment both lexical transduction\nrules so as to gain the bits from efficiently encoding their\ncommon cajón sequence, instead of redundantly enumer-\nating the same string twice. This yields a revised grammar\nCajón\nPalmasPalmasCajónPalmasCajónPalmasCajón\nB\u0003ĺ\nA\u0003ĺ\u0003D E\nA\u0003ĺ\u0003B C\nA\u0003ĺ\u0003A A\nS\u0003ĺ\u0003A\nC\u0003ĺ\nD\u0003ĺ\nE\u0003ĺ\nwith shorter description length, that introduces new nonter-minals so as to generate the same transduction as before.\nIn the new grammar, a shared biaffix is found, between\nthe second half of the Crule and the first half of the E\nrule. Segmenting both rules, again introducing new non-\nterminals as needed, yields:\nCajón\nPalmas\nCajón\nPalmasPalmasCajónPalmasCajónPalmasCajón\nB\u0003ĺ\nS\u0003ĺ\u0003A\nA ĺ\u0003A A\nA ĺ\u0003B C\nA\u0003ĺ\u0003D E\nC\u0003ĺ\u0003F G\nE\u0003ĺ\u0003G H\nD\u0003ĺ\nF\u0003ĺ\nG\u0003ĺ\nH\u0003ĺ\nThis has created another shared biaffix—the new Hrule\nis a bisuffix of the Brule. Segmenting the Brule yields:\nCajón\nPalmas\nCajón\nPalmasPalmasCajónPalmasCajónPalmasCajón\nD\u0003ĺ\nS\u0003ĺ\u0003A\nA ĺ\u0003A A\nA ĺ\u0003B C\nA\u0003ĺ\u0003D E\nC\u0003ĺ\u0003F G\nE\u0003ĺ\u0003G H\nB ĺ\u0003I H\nF\u0003ĺ\nG\u0003ĺ\nH\u0003ĺ\nI\u0003ĺ\nNote that the algorithm has learned the full compas length\npatterns. Typically, in subsequent iterations, the induction\nprocess moves on to gradually learn mediocompas patterns\nthat are only six beats in length. Here, the next segmenta-\ntion, based on the shared bisuffix of the DandFrules and\nbiprefix of the G,H, andIrules, has this effect:\nPalmasCajónCajón\nPalmas\nCajón\nPalmasPalmasCajónPalmasCajónPalmasCajón\nJ\u0003ĺ\nS\u0003ĺ\u0003A\nA ĺ\u0003A A\nA ĺ\u0003B C\nA\u0003ĺ\u0003D E\nC\u0003ĺ\u0003F G\nE\u0003ĺ\u0003G H\nB ĺ\u0003I H\nD ĺ J K\nF ĺ K L\nG ĺ K M\nH ĺ K N\nI ĺ K O\nK\u0003ĺ\nL ĺ\nM\u0003ĺ\nN\u0003ĺ\nO\u0003ĺ\nAt this point, there are no more rules that can be seg-\nmented to lower the description length. However, we no-\ntice that it is still possible to improve the posterior prob-\nability of the model, because some of the newly created\nnonterminal categories might be merged in such a way as\nto improve the model structure prior P(\bSj\bG)by reduc-\ning the model’s description length in terms of the number\nof nonterminal categories, without introducing so much er-\nror that it excessively decreases the likelihood of the data\nP(Dj\bG;\bS; \u0012\b).\nThe nonterminals that have just been created generate\nfour candidates for merging, since the algorithm considers\neach pair of nonterms serving as the LHS of modified rules\nthat share a common RHS: J\u0019K,J\u0019O,K\u0019L, and\nM\u0019N. Of these, K\u0019Lgives by far the best improve-ment in posterior probability, so we replace all instances of\nLin the grammar with Kinstead.\nThe next best merge is J\u0019K, so we replace in turn\nall instances of K(including those formerly L) with J. In\nso doing, the updated rules for DandFbecome identical,\nboth now with J Jon the right-hand-side. Thus we propa-\ngate the merging to replace all instances of FwithD, at no\nadditional cost. Similarly, merging M\u0019Nslightly im-\nproves the posterior, and in so doing, the updated rules for\nGandHbecome identical so we also merge HintoG.\nMerging J\u0019K,K\u0019L, andM\u0019Ninherently intro-\nduces generalizations that are able to (bi)parse new exam-\nples outside the training set. The iteration terminates with-\nout merging J\u0019Owhich would introduce too much error\nto improve the posterior. The final transduction grammar\ninduced is thus:\nPalmasCajónCajón\nPalmas\nCajón\nPalmasPalmasCajónPalmasCajónPalmasCajón\nJ\u0003ĺ\nS\u0003ĺ\u0003A\nA ĺ\u0003A A\nA ĺ\u0003B C\nA\u0003ĺ\u0003D E\nC\u0003ĺ\u0003D G\nE ĺ G G\nB ĺ I G\nD ĺ J J\nG ĺ J M\nI ĺ J O\nJ\u0003ĺ\nJ ĺ\nM\u0003ĺ\nM\u0003ĺ\nO\u0003ĺ\nAccurately reflecting flamenco norms, induction has cat-\negorized the mediocompas patterns into three distinct non-\nterminal types: Jcomprises patterns in 6/8 meter, while\nMcomprises patterns in 3/4 meter, and Ois a polyrhyth-\nmic pattern that crosses both. The fact that a distinction\nbetween the two meters can be learned—even though our\ncurrent approach does not incorporate any explicit a priori\nmodel of accented pulses at constant repeated intervals—\narises from the transduction grammar induction’s natural\nintegration of metrical structure learning together with hy-\npermetrical structure learning, as discussed below.\nBetween the 6/8 and 3/4 transduction patterns, certain\ncommon palmas sequences appear in both. This correctly\nreflects conventional flamenco usage of palmas (playing\na similar function to clave patterns in Afro-Latin genres).\nHowever, the 3/4 transduction patterns also tend more fre-\nquently to relate certain palmas sequences that do not gen-\nerally appear with 6/8 patterns. Such patterns tend to have\nnotes that align more naturally with 6/8 accents. Palmas\nsequences are useful to learning because of their clave-like\nfunction, even though they are not as consistent as Afro-\nLatin clave, and are usually silent on what would be the\nstrong downbeat in most mainstream dance music forms.\nLearning hypermetrical structure Among the many fla-\nmenco forms, the buleriás style is particularly aggressive\nabout using mediocompas patterns in irregular ways. The\nlearned rule G!J Mmodels a regular full alternating\nmeter compas , while I!J Omodels the same polyrhyth-\nmically against a 6/8 mediocompas feel. The rule D!\nJ Jmodels a full compas in 6/8 mediocompas feel. Therules for B,C, and Emodel longer compas pairs, with\ntypical idioms such as staying in 6/8 meter until the final\nfourth mediocompas . This behavior is naturally emergent\nfrom the MDL-driven induction (even more so on larger\ntraining sets).\nLearning probabilistic transduction relationships The\ninduced transduction grammar is potentially useful in a wide\nvariety of applications, which we plan to investigate in de-\ntail in our next steps. Currently, the learned model can\nalready be used to predict suitable accompaniment for ei-\nther instrumental part. Given a previously unseen cajón se-\nquence, a Viterbi parse translates the sequence into the most\nprobable palmas sequence with nearly perfect accuracy.\nSimilarly, given a new palmas sequence, the model can\ntranslate it into the most probable cajón sequence, which is\ncurrently generally acceptable though not necessarily mu-\nsically optimal. Our future work will focus on further re-\nfining the predictive accuracy for accompaniment from a\nstylistic standpoint.\nThe new MDL-driven transduction grammar induction\nmethod we have introduced is the first to (1) exploit oppor-\ntunities to compress both monolingual affixes and bilingual\naffixes, (2) exploit regularities in either single language to\nhelp segment rules describing both languages, and (3) ex-\nploit both monolingual and bilingual regularities to induce\ncategories for longer hypermetrical patterns. We anticipate\nnumerous further applications beyond the flamenco genre.\n5. ACKNOWLEDGMENTS\nThis material is based upon work supported in part by the Hong Kong Research\nGrants Council (RGC) research grants GRF620811, FSGRF13EG28, GRF621008,\nand GRF612806; the Defense Advanced Research Projects Agency (DARPA) un-\nder BOLT contract no. HR0011-12-C-0016, and GALE contract nos. HR0011-06-\nC-0022 and HR0011-06-C-0023; and by the European Union under the FP7 grant\nagreement no. 287658. Thanks to Markus Saers, Karteek Addanki, Chi-kiu Lo, and\nMeriem Beloucif for assistance with implementation. Any opinions, findings and\nconclusions or recommendations expressed in this material are those of the authors\nand do not necessarily reflect the views of the RGC, EU, or DARPA.\n6. REFERENCES\n[1] Alfred V . Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation, and\nCompiling (Volumes 1 and 2) . Prentice-Halll, Englewood Cliffs, NJ, 1972.\n[2] Gérard Assayag, Georges Bloch, Marc Chemillier, Arshia Cont, and Shlomo\nDubnov. OMax Brothers: A dynamic topology of agents for improvization\nlearning. In First ACM Workshop on Audio and Music Computing Multimedia ,\npages 125–132, 2006.\n[3] Rens Bod. Stochastic models of melodic analysis: Challenging the gestalt prin-\nciples. Journal of New Music Research , 30(3), 2001.\n[4] Ali Taylan Cemgil, Bert Kappen, Peter Desain, and Henkjan Honing. On tempo\ntracking: Tempogram representation and Kalman filtering. Journal of New Mu-\nsic Research , 29(4):259–273, 2000.\n[5] Arthur Pentland Dempster, Nan M. Laird, and Donald Bruce Rubin. Maximum\nlikelihood from incomplete data via the EM algorithm. Journal of the Royal\nStatistical Society. Series B (Methodological) , 39(1):1–38, 1977.\n[6] Peter Desain and Henkjan Honing. Music, Mind, and Machines: Studies in Com-\nputer Music, Music Cognition, and Artificial Intelligence . Thesis Publications,\nAmsterdam, 1992.\n[7] J. Miguel Díaz-Báñez, Giovanna Farigu, Francisco Gómez, David Rappaport,\nand Godfried T. Toussaint. El compás flamenco: A phylogenic analysis. In\nBRIDGES: Mathematical Connections in Art, Music and Science , pages 61–70,\nSouthwestern College, Winfield, Kansas, Jul 2004.\n[8] Alexandre R.J. François, Elaine Chew, and Dennis Thurmond. Mimi - a musical\nimprovisation system that provides visual feedback to the performer. Technical\nReport 07-889, USC Computer Science Department, Apr 2007.[9] Alexandre R.J. François, Isaac Schankler, and Elaine Chew. Mimi4x: An inter-\nactive audio-visual installation for high-level structural improvisation. In IEEE\nInternational Conference on Multimedia and Expo (ICME 2010) , pages 1618–\n1623, 2010.\n[10] Jon Gillick, Kevin Tang, and Robert M. Keller. Machine learning of jazz gram-\nmars. Computer Music Journal , 34(3):56–66, Fall 2010.\n[11] Catherine Guastavino, Francisco Gómez, Godfried Toussaint, Fabrice Maran-\ndola, and Emilia Gómez. Measuring similarity between flamenco rhythmic pat-\nterns. Journal of New Music Research , 38(2):129–138, 2009.\n[12] Emilia Gómez and Jordi Bonada. Automatic melodic transcription of flamenco\nsinging. In Fourth Conference on Interdisciplinary Musicology (CIM08) , Thes-\nsaloniki, Greece, Jul 2008.\n[13] Karim Lari and Steve J. Young. The estimation of stochastic context-free gram-\nmars using the inside-outside algorithm. Computer Speech and Language , 4:35–\n56, 1990.\n[14] Fred Lerdahl and Ray Jackendoff. A Generative Theory of Tonal Music . MIT\nPress, 1983.\n[15] Philip M. Lewis and Richard E. Stearns. Syntax-directed transduction. Journal\nof the Association for Computing Machinery , 15(3):465–488, 1968.\n[16] Hugh Christopher Longuet-Higgins and Mark J. Steedman. On interpreting\nBach. Machine Intelligence , 6:221–241, 1971.\n[17] Joaquín Mora, Francisco Gómez, Emilia Gómez, Francisco Escobar-Borrego,\nand José Miguel Díaz-Báñez. Characterization and melodic similarity of a cap-\npella flamenco cantes. In 11th International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 351–356, 2010.\n[18] François Pachet. The continuator: Musical interaction with style. Journal of\nNew Music Research , 32(3):33–341, 2003.\n[19] Dirk-Jan Povel and Peter Essens. Perception of temporal patterns. Music ,\n2(4):411–440, Summer 1985.\n[20] Christopher Raphael. A hybrid graphical model for rhythmic parsing. Artificial\nIntelligence , 137(1-2):217–238, May 2002.\n[21] Markus Saers, Karteek Addanki, and Dekai Wu. Iterative rule segmentation un-\nder minimum description length for unsupervised transduction grammar induc-\ntion. In Adrian-Horia Dediu, Carlos Martín-Vide, Ruslan Mitkov, and Bianca\nTruthe, editors, First International Conference on Statistical Language and\nSpeech Processing (SLSP 2013) , volume 7978 of LNAI , pages 224–235, Tar-\nragona, Spain, Jul 2013. Springer.\n[22] Markus Saers, Joakim Nivre, and Dekai Wu. Learning stochastic bracketing in-\nversion transduction grammars with a cubic time biparsing algorithms. In 11th\nInternational Conference on Parsing Technologies (IWPT’09) , pages 29–32,\nParis, Oct 2009.\n[23] Mark J. Steedman. The perception of musical rhythm and metre. Perception ,\n6(5):555–569, 1977.\n[24] Mark J. Steedman. The formal description of musical perception. Music Per-\nception , 2:52–77, 1984.\n[25] Mark J. Steedman. The blues and the abstract truth: Music and mental models.\nIn A. Garnham and J. Oakhill, editors, Mental Models in Cognitive Science ,\npages 305–318. Erlbaum, 1996.\n[26] Reid Swanson, Elaine Chew, and Andrew S. Gordon. Supporting musical cre-\nativity with unsupervised syntactic parsing. In AAAI Spring Symposium on Cre-\native Intelligent Systems , 2007.\n[27] David Temperley. Music and Probability . MIT Press, 2007.\n[28] David Temperley and Daniel Sleator. Modeling meter and harmony: A\npreference-rule approach. Computer Music Journal , 23(1):10–27, 1999.\n[29] Eric Thul and Godfried T. Toussaint. On the relation between rhythm complex-\nity measures and human rhythmic performance. In Conference on Computer\nScience & Software Engineering (C3S2E ’08) , 2008.\n[30] Dekai Wu. Trainable coarse bilingual grammars for parallel text bracketing. In\nThird Annual Workshop on Very Large Corpora (WVLC-3) , pages 69–81, Cam-\nbridge, MA, Jun 1995.\n[31] Dekai Wu. Stochastic Inversion Transduction Grammars and bilingual parsing\nof parallel corpora. Computational Linguistics , 23(3):377–404, Sep 1997.\n[32] Dekai Wu. Alignment. In Nitin Indurkhya and Fred J. Damerau, editors, Hand-\nbook of Natural Language Processing , pages 367–408. Chapman and Hall /\nCRC, second edition, 2010."
    },
    {
        "title": "Spectral Correlates in Emotion Labeling of Sustained Musical Instrument Tones.",
        "author": [
            "Bin Wu 0013",
            "Simon Wun",
            "Chung Lee",
            "Andrew Horner"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1417443",
        "url": "https://doi.org/10.5281/zenodo.1417443",
        "ee": "https://zenodo.org/records/1417443/files/WuWLH13.pdf",
        "abstract": "Music is one of the strongest inducers of emotion in humans. Melody, rhythm, and harmony provide the primary triggers, but what about timbre? Do the musical instruments have underlying emotional characters? For example, is the well-known melancholy sound of the English horn due to its timbre or to how composers use it? Though music emotion recognition has received a lot of attention, researchers have only recently begun considering the relationship between emotion and timbre. To this end, we devised a listening test to compare representative tones from eight different wind and string instruments. The goal was to determine if some tones were consistently perceived as being happier or sadder in pairwise comparisons. A total of eight emotions were tested in the study. The results showed strong underlying emotional characters for each instrument. The emotions Happy, Joyful, Heroic, and Comic were strongly correlated with one another. The violin, trumpet, and clarinet best represented these emotions. Sad and Depressed were also strongly correlated. These two emotions were best represented by the horn and flute. Scary was the emotional outlier of the group, while the oboe had the most emotionally neutral timbre. Also, we found that emotional judgment correlates significantly with average spectral centroid for the more distinctive emotions, including Happy, Joyful, Sad, Depressed, and Shy. These results can provide insights in orchestration, and lay the groundwork for future studies on emotion and timbre.",
        "zenodo_id": 1417443,
        "dblp_key": "conf/ismir/WuWLH13",
        "keywords": [
            "Melody",
            "Rhythm",
            "Harmony",
            "Timbre",
            "Emotion",
            "Wind instruments",
            "String instruments",
            "Listening test",
            "Eight emotions",
            "Spectral centroid"
        ],
        "content": "SPECTRAL CORRELATES IN EMOTION LABELING OF SUSTAINEDMUSICAL INSTRUMENT TONESBin Wu1, Simon Wun1, Chung Lee2, Andrew Horner11Department of Computer Science and Engineering,Hong Kong University of Science and Technology, Hong Kong2The Information Systems Technology and Design Pillar,Singapore University of Technology and Design, 20 Dover Drive, Singapore 138682{bwuaa, simonwun}@cse.ust.hk, im.lee.chung@gmail.com, horner@cse.ust.hkABSTRACTMusic is one of the strongest inducers of emotion in hu-mans. Melody, rhythm, and harmony provide the primarytriggers, but what about timbre? Do the musical instru-ments have underlying emotional characters? For exam-ple, is the well-known melancholy sound of the Englishhorn due to its timbre or to how composers use it? Thoughmusic emotion recognition has received a lot of attention,researchers have only recently begun considering the rela-tionship between emotion and timbre. To this end, we de-vised a listening test to compare representative tones fromeight different wind and string instruments. The goal wasto determine if some tones were consistently perceived asbeing happier or sadder in pairwise comparisons. A to-tal of eight emotions were tested in the study. The re-sults showed strong underlying emotional characters foreach instrument. The emotions Happy, Joyful, Heroic, andComic were strongly correlated with one another. Theviolin, trumpet, and clarinet best represented these emo-tions. Sad and Depressed were also strongly correlated.These two emotions were best represented by the hornand ﬂute. Scary was the emotional outlier of the group,while the oboe had the most emotionally neutral timbre.Also, we found that emotional judgment correlates signiﬁ-cantly with average spectral centroid for the more distinc-tive emotions, including Happy, Joyful, Sad, Depressed,and Shy. These results can provide insights in orchestra-tion, and lay the groundwork for future studies on emotionand timbre.1. INTRODUCTIONMusic is one of the most effective forms of media for con-veying emotion. A lot of work has been done on emotionrecognition in music, considering such factors as melody[3], rhythm [18], and lyrics [10]. However, little attentionhas been given to the relationship between emotion andPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page.c\u00002013 International Society for Music Information Retrieval.timbre. Does the timbre of a particular musical instrumentarouse speciﬁc emotions? For example, the English hornoften plays a sad and melancholy character in orchestralmusic – is the melancholy character due to the instrument’stimbre, the melody composers feel inspired to write for theinstrument, or both? If listeners just heard an isolated tonefrom the English horn without a melodic context, would itsound more melancholy than other instruments? This pa-per addresses this fundamental question.Some previous studies have shown that emotion isclosely related to timbre. Scherer and Oshinsky foundthat timbre is a salient factor in the rating of synthetictones [17]. Peretzet al.showed timbre speeds up thediscrimination of emotion categories [15]. Bigandet al.reported similar results from their study of emotional sim-ilarities between one-second musical excerpts [4]. It wasalso found that timbre is essential to musical genre recog-nition and discrimination [2, 19].Little attention has been given to the direct connectionbetween emotion and timbre, though a study by Eerolaetal.was an excellent start [6]. They carried out listeningtests to investigate the correlations of emotions with tem-poral and spectral sound features. The study conﬁrmedstrong correlations between some features, especially at-tack time and brightness, and the emotional dimensions va-lence and arousal for one-second isolated instrument tones.Valence and arousal refer to how positive and energetica music stimulus sounds [21]. Despite the widespread useof these emotional dimensions in music research, com-posers may ﬁnd them vague and difﬁcult to interpret forcomposition and arrangement purposes. In our study, tomake the results intuitive for composers, the listening testsubjects compared sounds in terms of emotional categoriessuch as Happy and Sad. The use of emotional categorieshas been shown to be generally congruent with results ob-tained using the dimensional model in music [7].Moreover, we equalized the attacks and decays of thestimuli so that the temporal features attack time and de-cay time would not be factors in the subjects’ judgment.This modiﬁcation allowed us to isolate the effects of spec-tral features, such as the average spectral centroid, whichstrongly correlates with the perceptual brightness of sound.The next section describes the listening test in detail.We report the listening test results in Section 3. Section 4discusses applications of the results and questions arisingfrom our study.2. EXPERIMENTAL METHOD2.1 StimuliThe stimuli consisted of eight sustained tones from sev-eral wind and bowed string instruments: the bassoon (Bs),clarinet (Cl), ﬂute (Fl), horn (Hn), oboe (Ob), saxophone(Sx), trumpet (Tp), and violin (Vn). They were obtainedfrom the McGill and Prosonus sample libraries, except forthe trumpet tone, which had been recorded at the Univer-sity of Illinois at Urbana-Champaign School of Music. Allthe tones were used in a discrimination test carried outby Horneret al.[9], and six of them were also used byMcAdamset al.[14].The tones were used in their entirety, including the fullattack, sustain, and decay sections. They were nearly har-monic and had fundamental frequencies close to 311.1 Hz(Eb4). The original fundamental frequencies deviated byup to 1 Hz (6 cents), and were synthesized by additive syn-thesis at 311.1 Hz. They were stored in the format of 16-bitsamples at a 22050- or 44010-Hz sampling rate (dependingon the number of harmonics with signiﬁcant amplitudes).Duration, loudness, and harmonic frequency deviationswere equalized so that these factors would not inﬂuencethe results. Furthermore, to isolate the effects of spectralfeatures, the attacks and decays of the tone were equal-ized by time-stretching the actual attacks and decays. Asa result, the stimuli were standardized to last for 2 secondswith attacks and decays 0.05 seconds long.2.2 Subjects32 subjects without hearing problems were hired to takethe test. These undergraduate students ranged in age from19 to 24. Half of them had music training (that is, at leastﬁve years of practice on an instrument).2.3 Emotion CategoriesThe subjects compared the stimuli in terms of eight emo-tion categories: Happy, Sad, Heroic, Scary, Comic, Shy,Joyful, and Depressed. These terms were selected for theirrelevance to composition and arrangement by one of theauthors, who had received formal composition education.Their ratings according to the Affective Norms for EnglishWords [5] are shown in Figure 1 using the Valence-Arousalmodel. It is worth noting that Happy, Joyful, Comic, andHeroic form one cluster, and that Sad and Depressed formanother cluster.2.4 Listening TestEvery subject made pairwise comparisons of all the eightinstruments. During each trial, the subjects heard a pairof tones from different instruments and were prompted tochoose which tone more strongly aroused a given emotion.Figure 2 shows a screenshot of the listening test program.02468100246810Happy    Sad      Heroic   Scary    Comic    Shy      Joyful   DepressedValenceArousal\nFigure 1. Russel’s Valence-Arousal emotion model. Va-lence refers to how positive an emotion is. Arousal refersto how energetic an emotion is.Each combination of two different instruments was pre-sented in four trials for each emotion, and the listening testtotaledC82◊4◊8 = 896trials. The overall trial presenta-tion order was randomized.\nFigure 2. Listening test interface.Before the ﬁrst trial, the subjects read online deﬁnitionsof the emotion categories from the Cambridge AcademicContent Dictionary [1]. The listening test took about 1.5hours, with breaks every 30 minutes.The subjects were seated in a “quiet room” with lessthan 40 dB SPL background noise level. Residual noisewas mostly due to computers and air conditioning. Thenoise level was reduced further with headphones. Soundsignals were converted to analog by a Sound Blaster X-Fi Xtreme Audio sound card, and then presented throughSony MDR-7506 headphones at a level of approximately78 dB SPL, as measured with a sound-level meter. TheSound Blaster DAC utilized 24 bits with a maximum sam-pling rate of 96 kHz and a 108 dB S/N ratio.3. RESULTS3.1 Quality of ResponsesThe subjects’ responses were ﬁrst screened for inconsis-tencies. A subject’s consistency was deﬁned in the fourcomparisons of a pair of instruments A and B for a partic-ular emotion as follows:consistencyA,B=max(vA,vB)4(1)wherevAandvBare the number of votes the sub-ject gave to the two instruments respectively. A valueofconsistency=1represents perfect discrimination,whereas 0.5 represents random guessing. The mean of theaverage consistency of all subjects was 0.79.Predictably the subjects were only fairly consistent be-cause of the emotional ambiguities in the stimuli. We as-sessed the quality of subject responses further using a prob-abilistic approach. One probabilistic model, successful forimage labeling, was adapted to suit this study [20]. Theoriginal model took the difﬁculty of labeling and the ambi-guities in image categories into account. This was done toestimate the annotators’ expertise and the quality of theirresponses. Those who made low-quality responses wereunable to discriminate between image categories and wereconsidered random pickers. In our study, we veriﬁed thatthe two least consistent subjects made responses of thelowest quality. They were excluded from the results.We measured the level of agreement among the remain-ing subjects with an overall Fleiss’ Kappa statistic. It wascalculated at 0.22, which can be interpreted as indicatingfair agreement [12].3.2 Emotional JudgmentFigure 3 depicts the emotion “voting” results on a grayscale. Each row shows the percentage of positive votesan instrument received when compared to the other instru-ments for one particular emotion. The lighter the colorof a cell, the more positive votes its “row instrument” re-ceived when compared to its “column instrument”. For ex-ample, the bassoon was nearly always judged happier thanthe horn but usually not as happy as the clarinet.The subjects gave clear-cut votes to distinctive emo-tions such as Sad and Depressed, for which the voting pat-terns have a lot of contrast. On the other hand, there wereconsiderable ambiguities in the emotion comparisons forScary.The voting patterns for Sad, Depressed, and Shy weresimilar, suggesting that these emotions correlate with eachother. Likewise, Happy and Joyful form another group ofcorrelated emotions, which apparently includes Heroic andComic.We ranked the instruments in order of the number ofpositive votes they received for each emotion, and derivedscale values using the Bradley-Terry-Luce (BTL) model[11]. Table 1 lists the rankings of the instruments, whichcan be visualized on a BTL scale in Figure 4. The in-strument rankings for correlated emotions are similar. Thehorn and ﬂute ranked high for the sad emotions, whereasthe violin, trumpet, and clarinet ranked high for the happyemotions. Note that the oboe nearly always ranked in themiddle.The comparisons between instruments close in rank(e.g., the trumpet and the clarinet) were generally difﬁcult.The votes received by a pair of such instruments could beclose, corresponding to the grayest areas in Figure 3. Bycontrast, instruments ranking in different extremes (e.g.,the horn and clarinet) could receive quite different num-bers of votes, which correspond to bright and dark areas inFigure 3.Table 2 shows the spectral characteristics of the testtones such as average spectral centroid. Table 3 showsclose ties between test tone ranking and the average spec-tral centroid, as measured by the Spearman correlation co-efﬁcient. Emotional judgment correlated signiﬁcantly withaverage spectral centroid, except for the less distinctiveemotion Scary. For example, a high-centroid instrumentis likely to sound happier, and a low-centroid instrumentsadder. Emotional judgment did not have statistically sig-niﬁcant correlations with the other spectral characteristicsthat we tested.4. DISCUSSIONThe pairwise correlations between emotions in our listen-ing test are basically consistent with the pairwise relationsbetween emotional words in the Valence-Arousal model inFigure 1. Both show Scary as the biggest outlier. Bothshow Happy, Joyful, Heroic, and Comic in a one cluster,and Sad and Depressed in another group. The biggest dif-ference is that Shy is included in the sad group in this study,but it is emotionally-neutral in the Valence-Arousal model.The results were consistent with those of Eerola’sValence-Arousal results for musical instrument tones [6].Both show that musical instrument timbres carry cuesabout emotional expression that are easily and consistentlyrecognized by listeners. Both show that spectral cen-troid/brightness is a signiﬁcant component in music emo-tion.The main application motivating this study is to pro-vide guidelines for composers/arrangers in orchestration,especially for computer games, ﬁlm, and stage music thatneeds to reﬂect characterization and dramatic action. Theresults provide some clear guidelines in achieving the de-sired emotional impact. Composers/arrangers can choosetimbres that reinforce the desired emotion of their melodyto achieve the strongest emotional impact. For example,composers could combine a joyful melody with instru-ments that have inherently joyful timbres. On the otherhand, instrumental music, like opera arias, often includemixed emotions representing the complexity of charactersand dramatic situations. The results of this study also pro-vide a good starting point for achieving mixed emotions.For example, composers/arrangers might create a feelingof overall joyfulness mixed with an undertone of sadnessby skillfully combining an otherwise joyful melody withthe normally sad horn for a sophisticated mix of emotions.Happy    \n  BsClFlHnObSxTpVnBsClFlHnObSxTpVn\n00.20.40.60.81\nSad      \n  BsClFlHnObSxTpVnBsClFlHnObSxTpVn\n00.20.40.60.81\nHeroic   \n  BsClFlHnObSxTpVnBsClFlHnObSxTpVn\n00.20.40.60.81\nScary    \n  BsClFlHnObSxTpVnBsClFlHnObSxTpVn\n00.20.40.60.81\nComic    \n  BsClFlHnObSxTpVnBsClFlHnObSxTpVn\n00.20.40.60.81\nShy      \n  BsClFlHnObSxTpVnBsClFlHnObSxTpVn\n00.20.40.60.81\nJoyful   \n  BsClFlHnObSxTpVnBsClFlHnObSxTpVn\n00.20.40.60.81\nDepressed\n  BsClFlHnObSxTpVnBsClFlHnObSxTpVn\n00.20.40.60.81\nFigure 3. Comparison between instruments for each emotion. The lighter the color of a cell, the more positive votes its“row instrument” received when compared to its “column instrument”.In addition, instrument rankings for the different emo-tions may serve as a reference for combining instrumentsthat blend together or contrast with each other emotionally.Moreover, there are many other useful emotional labelsto test. For example, one listener commented that the hornwas not so much sad as mysterious. Is it the most mys-terious timbre? What makes the instrument sound mys-terious? Is it due to the reverberant reﬂections it makes?We could test the effect of reverberations by using tonesrecorded in an anechoic chamber and comparing them tothose recorded in halls with varying reverberation times. Ingeneral, how does reverberation inﬂuence emotion? Doesit make the sound more mysterious, majestic, or heroic?What is the effect of reverberation time and reverberationamount on emotion?From an audio engineering and sound reproductionpoint of view, how do spectral alterations inﬂuence emo-tion? For example, how does the spectral distortion ofplayback equipment inﬂuence emotion? How about theeffect of audio coding on emotion? For example, pastwork has shown that MP-3 compression with low bit-ratescan cause a large reduction in brightness in the saxophonewithout affecting other instruments [13]. Is the emotionalimpact changed in a corresponding way? We expect it tobe less joyful/happy – is it?For future work, it will be also fascinating to seehow emotion varies with different pitches, dynamic levels,brightness, and articulations. Do these parameters changeperceived emotion in a consistent way, or does it vary frominstrument to instrument? For example, we know thatincreased brightness makes a tone more dramatic (morehappy or more angry), but is the effect more pronouncedin some instruments and less so in others? For example,if the violin, which is the happiest instrument, is playedsoftly with less brightness, is it still happier than the horn,which is the saddest instrument, if the horn is played loudlywith maximum brightness? At what point are they equallyhappy? Can we normalize the instruments to equal happi-ness by simply adjusting brightness or other attributes? Inthe same way that we can normalize brightness by ﬁlteringor spectral tilting, can we normalize happiness by ﬁlteringor spectral tilting (or pitch or dynamic level)? How do the``````````RankingEmotionHappySadHeroicScaryComicShyJoyfulDepressed1Vn (5.23)Hn (8.80)Tp (3.38)Fl (2.95)Cl (3.44)Hn (8.09)Cl (7.30)Hn (8.70)2Tp (4.53)Fl (6.30)Cl (2.23)Hn (2.05)Tp (3.23)Fl (4.95)Tp (6.94)Fl (6.11)3Cl (4.42)Bs (3.75)Sx (1.93)Vn (2.02)Sx (2.84)Bs (3.85)Vn (5.35)Bs (4.08)4Sx (3.91)Ob (2.20)Vn (1.95)Tp (1.43)Ob (2.20)Ob (2.55)Sx (5.15)Ob (2.37)5Ob (3.10)Sx (1.70)Ob (1.65)Bs (1.40)Vn (1.63)Sx (2.01)Ob (3.79)Sx (1.68)6Fl (2.02)Tp (1.47)Hn (1.05)Sx (1.34)Bs (1.37)Tp (1.53)Bs (2.17)Tp (1.64)7Bs (2)Vn (1.08)Bs (1.03)Ob (1.28)Fl (1.30)Cl (1.49)Fl (1.84)Cl (1.20)8Hn (1)Cl (1)Fl (1)Cl (1)Hn(1)Vn (1)Hn (1)Vn (1)Table 1. Rankings of instruments for each emotion from strongest to weakest with Bradley-Terry-Luce scale values inbrackets.\nBsBsBsBsBsBsBsBsClClClClClClCl\nClFlFl\nFlFlFlFlFlFl\nHnHn\nHnHnHnHn\nHnHn\nObObObObObObObObSxSxSxSxSxSxSxSxTpTpTpTpTpTpTp\nTpVnVnVnVnVnVnVn\nVn012345678910\nHappySadHeroicScaryComicShyJoyfulDepressedBsClFlHnObSxTpVn\nFigure 4. Bradley-Terry-Luce scale values of the instruments for each emotion.happy spaces of the violin overlap with other instrumentsin terms of pitch, dynamic level, brightness, and articula-tion? In general, how does timbre space relate to emotionalspace?Emotion gives us a fresh perspective on timbre, help-ing us to get a handle on its perceived dimensions. It givesus a focus for exploring its many aspects. Just as timbreis a multidimensional perceived space, emotion is an evenhigher-level multidimensional perceived space deeper in-side the listener.5. ACKNOWLEDGMENTSThis work has been supported by Hong Kong Re-search Grants Council grants HKUST613111 andHKUST613112.6. REFERENCES[1] “happy, sad, heroic, scary, comic, shy, joyful and de-pressed”.Cambridge Academic Content Dictionary.Online: http://goo.gl/v5xJZ (17 Feb 2013).[2] Jean-Julien Aucouturier, Franc ¸ois Pachet, and MarkSandler. The way it sounds: timbre models for anal-ysis and retrieval of music signals.IEEE Transactionson Multimedia, 7(6):1028–1035, 2005.[3] Laura-Lee Balkwill and William Forde Thompson. Across-cultural investigation of the perception of emo-tion in music: Psychophysical and cultural cues.Musicperception, pages 43–64, 1999.[4] Emmanuel Bigand, Sandrine Vieillard, Franc ¸oisMadurell, Jeremy Marozeau, and A Dacquet. Multi-dimensional scaling of emotional responses to music:The effect of musical expertise and of the duration ofthe excerpts.Cognition & Emotion, 19(8):1113–1139,2005.[5] Margaret M Bradley and Peter J Lang. Affective normsfor english words (ANEW): Instruction manual and af-fective ratings.Psychology, (C-1):1–45, 1999.[6] Tuomas Eerola, Rafael Ferrer, and Vinoo Alluri. Tim-bre and affect dimensions: Evidence from affect andsimilarity ratings and acoustic correlates of isolatedinstrument sounds.Music Perception: An Interdisci-plinary Journal, 30(1):49–70, 2012.[7] Tuomas Eerola and Jonna K Vuoskoski. A comparison`````````````FeaturesInstrumentBsClFlHnObSxTpVnAverage Spectral Centroid3.38066.38433.49092.42294.27134.12184.25614.3502Spectral Flux3.92745.869712.6597.06536.16657.71084.489212.187Spectral Entropy0.483880.531370.573820.384680.444360.495270.571330.54972Spectral Spread12.62814.12112.5186.48397.428911.4298.166510.464Irregularity0.170811.0830.447380.185660.637740.857510.0586120.5557Roughness0.53790.544420.0175420.308090.110090.693870.0338968.9796Table 2. Spectral characteristics of the instrument test tones.XXXXXXXXXXTimbreEmotionHappySadHeoricScaryComicShyJoyfulDepressedAverage Spectral Centroids0.8333úú-0.9048úú0.6429ú-0.57140.7619úú-0.8810úú0.8571úú-0.8810úúSpectral Flux0.07140.1667-0.30950.5238-0.3810.0714-0.28570.0714Spectral Entropy0.5714-0.3810.19050.30950.2619-0.40480.4048-0.4048Spectral Spread0.119-0.3571-0.0238-0.30950.3095-0.26190.3333-0.2619Irregularity0.2619-0.45240.2381-0.59520.4286-0.3810.3571-0.381Roughness0.381-0.52380.3095-0.35710.2381-0.57140.381-0.5714Table 3. Spearman correlation between emotions and spectral features.úú:p<0.05;ú:0.05<p<0.1.of the discrete and dimensional models of emotion inmusic.Psychology of Music, 39(1):18–49, 2011.[8] Cardillo G. Fleiss’ kappa: compute the ﬂeiss’ kappafor multiple raters.http://goo.gl/0DFKV, 2007.[9] Andrew Horner, James Beauchamp, and Richard So.Detection of random alterations to time-varying musi-cal instrument spectra.The Journal of the AcousticalSociety of America, 116:1800–1810, 2004.[10] Yajie Hu, Xiaoou Chen, and Deshun Yang. Lyric-based song emotion detection with affective lexiconand fuzzy clustering method. InProceedings of ISMIR,volume 10, 2009.[11] David R Hunter. Mm algorithms for generalizedbradley-terry models.Annals of Statistics, pages 384–406, 2004.[12] Fleiss L Joseph. Measuring nominal scale agreementamong many raters.Psychological bulletin, 76(5):378–382, 1971.[13] Chung Lee, Andrew Horner, and James Beauchamp.Impact of mp3-compression on timbre space of sus-tained musical instrument tones.The Journal of theAcoustical Society of America, 131(4):3433–3433,2012.[14] Stephen McAdams, James W Beauchamp, andSuzanna Meneguzzi. Discrimination of musical instru-ment sounds resynthesized with simpliﬁed spectrotem-poral parameters.The Journal of the Acoustical Societyof America, 105:882, 1999.[15] Isabelle Peretz, Lise Gagnon, and Bernard Bouchard.Music and emotion: perceptual determinants, imme-diacy, and isolation after brain damage.Cognition,68(2):111–141, 1998.[16] James A Russell. Evidence of convergent validity onthe dimensions of affect.Journal of personality and so-cial psychology, 36(10):1152, 1978.[17] Klaus R Scherer and James S Oshinsky. Cue utilizationin emotion attribution from auditory stimuli.Motiva-tion and emotion, 1(4):331–346, 1977.[18] Janto Skowronek, Martin McKinney, and Steven VanDe Par. A demonstrator for automatic music mood es-timation. InProceedings of the International Confer-ence on Music Information Retrieval, Vienna, Austria,2007.[19] George Tzanetakis and Perry Cook. Musical genreclassiﬁcation of audio signals.Speech and Audio Pro-cessing, IEEE transactions on, 10(5):293–302, 2002.[20] Jacob Whitehill, Paul Ruvolo, Tingfan Wu, JacobBergsma, and Javier Movellan. Whose vote shouldcount more: Optimal integration of labels from labelersof unknown expertise.Advances in Neural InformationProcessing Systems, 22(2035-2043):7–13, 2009.[21] Yi-Hsuan Yang, Yu-Ching Lin, Ya-Fan Su, andHomer H. Chen. A regression approach to music emo-tion recognition.IEEE TASLP, 16(2):448–457, 2008."
    },
    {
        "title": "Bilevel Sparse Models for Polyphonic Music Transcription.",
        "author": [
            "Tal Ben Yakar",
            "Roee Litman",
            "Pablo Sprechmann",
            "Alexander M. Bronstein",
            "Guillermo Sapiro"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415914",
        "url": "https://doi.org/10.5281/zenodo.1415914",
        "ee": "https://zenodo.org/records/1415914/files/YakarSLBS13.pdf",
        "abstract": "In this work, we propose a trainable sparse model for automatic polyphonic music transcription, which incorporates several successful approaches into a unified optimization framework. Our model combines unsupervised synthesis models similar to latent component analysis and nonnegative factorization with metric learning techniques that allow supervised discriminative learning. We develop efficient stochastic gradient training schemes allowing unsupervised, semi-, and fully supervised training of the model as well its adaptation to test data. We show efficient fixed complexity and latency approximation that can replace iterative minimization algorithms in time-critical applications. Experimental evaluation on synthetic and real data shows promising initial results.",
        "zenodo_id": 1415914,
        "dblp_key": "conf/ismir/YakarSLBS13",
        "keywords": [
            "trainable",
            "sparse",
            "model",
            "automatic",
            "polyphonic",
            "music",
            "transcription",
            "unified",
            "optimization",
            "framework"
        ],
        "content": "BILEVEL SPARSE MODELS FOR POLYPHONIC MUSIC\nTRANSCRIPTION\nTal Ben Yakar,1Roee Litman,1Pablo Sprechmann,2Alex Bronstein,1and Guillermo Sapiro2\n1Tel Aviv University,2Duke University\ntalby10@gmail.com ,roeelitm@post.tau.ac.il ,bron@eng.tau.ac.il\nfpablo.sprechmann,guillermo.sapiro g@duke.edu\nABSTRACT\nIn this work, we propose a trainable sparse model for\nautomatic polyphonic music transcription, which in-\ncorporates several successful approaches into a uniﬁed\noptimization framework. Our model combines un-\nsupervised synthesis models similar to latent compo-\nnent analysis and nonnegative factorization with met-\nric learning techniques that allow supervised discrim-\ninative learning. We develop efﬁcient stochastic gra-\ndient training schemes allowing unsupervised, semi-,\nand fully supervised training of the model as well its\nadaptation to test data. We show efﬁcient ﬁxed com-\nplexity and latency approximation that can replace it-\nerative minimization algorithms in time-critical appli-\ncations. Experimental evaluation on synthetic and real\ndata shows promising initial results.\n1. INTRODUCTION\nThe goal of automatic music transcription (AMT) is\nto obtain a musical score from an input audio signal.\nAMT is particularly difﬁcult when the audio signal is\npolyphonic [12], as the harmonic relations and inter-\nactions in music signals challenges the detection of\nmultiple concurrent pitches. Polyphonic AMT is still\nconsidered an open problem and the state-of-the-art\nsolutions are far from the level of precision required\nin many applications. We refer the reader to [4] for\na detailed description of the open questions and chal-\nlenges in polyphonic AMT.\nWork partially supported by BSF, ONR, NGA, NSF, ARO, and\nAFOSR.\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.\nc\r2013 International Society for Music Information Retrieval.1.1 Prior work\nIn what follows, we brieﬂy review two main families\nof approaches recently used for AMT, which are par-\nticularly relevant for the present work. Reviewing all\nexisting AMT methods is beyond the scope of this pa-\nper; for recent surveys, we refer to the reader to [4,12]\nand references therein.\nBeing essentially a classiﬁcation task, music tran-\nscription has been addressed by classiﬁcation-based\napproaches. These techniques deﬁne a set of mean-\ningful features for pitch and onset detection, and feed\nthem to generic classiﬁcation schemes such as neural\nnetworks [6,15], deep believe networks [16], and sup-\nport vector machines [17]. In [16], the features them-\nselves were learned form the data. In [17], the au-\nthors argue that prior knowledge (such as harmonic-\nity) is not strictly necessary for achieving levels of\ntranscription accuracy comparable to the one obtained\nwith competing approaches, and that such assump-\ntions can be substituted with discriminative learning.\nWhile being feasible, the lack of insight makes such\npure learning-based systems hard to train, since they\nneed to infer from the training data all possible vari-\nations and combinations of pitches. In general, this\ntranslates into long off-line training times and requires\nhuge training sets.\nAnother family of recent approaches is based on\nspectrogram factorization techniques, such as non-\nnegative matrix factorization (NMF) [13], and its\nprobabilistic counterpart – probabilistic latent compo-\nnent analysis (PLCA) [20]. The basic idea, ﬁrst in-\ntroduced in [19], aims at factorizing a spectral repre-\nsentation of the signal X2Rn\u0002k, into a product of\nnon-negative factors, X\u0019DZ, where then\u0002pnon-\nnegative dictionary Dcontains templates of the indi-\nvidual pitches, and the p\u0002knon-negative factor Z\ncontains the corresponding activations for each frame.\nIdeally one would expect Zto resemble a piano-roll\nand reveal the active notes in each spectral frame.\nUnfortunately, this is not enough in practice. In or-\nder to overcome this problem, many approaches have\nproposed to regularize the factorization by includingsparsity [1], harmonicity, and smoothness [5, 22]. In\n[2], the authors propose a shift-invariant version of\nPLCA, where the dictionary contains note templates\nfrom multiple orchestral instruments. Including such\na regularizations usually signiﬁcantly improves the re-\nsults but also translates into slower coding schemes.\nThis contrasts with the discriminative approaches that,\nafter training, have very light computational costs. On\none hand, the generative nature of factorization ap-\nproaches allows them to handle the spectral super-\nposition of harmonically related pitches in a natural\nway. On the other hand, however, such generative ap-\nproaches seem less ﬂexible and more difﬁcult to adapt\nto speciﬁc settings compared to their discriminative\ncounterparts.\n1.2 Contributions\nIn this paper, we present an attempt to inject the gener-\native properties of factorization approaches into a dis-\ncriminative setting. We aim at establishing a bridge\nbetween pure learning and factorization-based meth-\nods. Speciﬁcally, we propose the coupled training of\na set of classiﬁers that detect the presence of a given\npitch in a frame of audio, taking as the input the acti-\nvations produced by a generative matrix factorization\nscheme. Instead of constraining the factorization al-\ngorithm, we design a very simple factorization method\ntrained to produce the optimal input to a binary clas-\nsiﬁer in the sense of the classiﬁcation performance.\nOnce trained, the simplicity of the proposed factor-\nization scheme allows to use fast approximation of\nsparse encoders, resulting in computation complex-\nity comparable to that of pure discriminative mod-\nels. With this implementation, the proposed method\nbridges between factorization and classiﬁcation meth-\nods, designing a neural network that solves a mean-\ningful factorization problem.\nWe formulate our model as a bilevel optimization\nprogram, generalizing supervised dictionary learning\ndevised for sparse coding schemes [14]. We also in-\ncorporate elements of metric learning into this super-\nvised sparse NMF setting in order to increase its dis-\ncriminative power. The proposed approach is natu-\nrally amenable to semi-supervised training regimes, in\nwhich unlabeled data can be used to adapt the system.\nFinally, the output of these classiﬁers is temporally\nsmoothed as is normally done in AMT [2, 17].\nIn Section 2 we present the proposed model cou-\npling the codes with the pitch classiﬁers. Then, in\nSection 3, we formulate its supervised variant as a\nbilevel optimization problem. In Section 4, we de-\nscribe how the proposed method can be signiﬁcantly\naccelerated. Section 5 shows how to incorporate the\nproposed scheme into higher-level temporal models.Experimental evaluation is reported in Section 6. Fi-\nnally, Section 7 concludes the paper.\n2. NON-NEGATIVE SPARSE MODEL\nLike the majority of music and speech analysis tech-\nniques, music transcription typically operates on the\nmagnitude of the audio time-frequency representation\nsuch as the short-time Fourier transform or constant-Q\ntransform (CQT) [8], as adopted in this work. Given\na spectral frame x2Rn\n+at some time, the transcrip-\ntion problem consists of producing a binary label vec-\ntory2f\u0000 1;+1gp, whosei-th element indicates the\npresence ( +1) or absence (\u00001) of thei-th pitch at that\ntime. We use p= 88 corresponding to the span of the\nstandard piano keyboard (MIDI pitches 21\u0000108).\nIn the proposed model, the output vector is pro-\nduced by applying a simple linear classiﬁer y=\nsign(Wz+a), parametrized by the p\u0002mmatrix W\nandp\u00021vector a, to them-dimensional feature vec-\ntorzobtained by solving the following non-negative\nsparse representation pursuit problem\nz(x) = (1)\narg min\nz\u001501\n2kM(x\u0000Dz)k2\n2+\u00151kzk1+\u00152kzk2\n2:\nHere, Dis ann\u0002mover-complete ( m > n ) non-\nnegative dictionary, whose columns represent differ-\nent templates for each of the individual pitches, and\nMis ar\u0002nmetric matrix ( r\u0014n).\nThe ﬁrst data ﬁtting term requires the data to be\nwell-approximated by a sparse non-negative combina-\ntion of the atoms of D, expressing the assumption that\nat each time, only a few pitches are simultaneously\npresent. Replacing the standard Euclidean ﬁtting term\nby a more general Mahalanobis metric parametrized\nby the matrix Mallows to give different weights to\ndifferent frequencies, as frequently practiced in music\nprocessing. The second term, whose relative impor-\ntance is governed by the parameter \u00151, actually pro-\nmotes the sparsity of the solution vector, while the\nthird term is added for regularization.\nPursuit problem (1) is a strictly convex optimiza-\ntion problem, which can be solved efﬁciently us-\ning (among other alternatives) a family of optimiza-\ntion techniques called proximal methods. We adopt\na non-negative variant of the iterative shrinkage-\nthresholding algorithm (ISTA) [9], summarized in Al-\ngorithm 1. While faster versions of this ﬁxed-step\nproximal method can be used to reach linear conver-\ngence rates, the discussion of these extensions is be-\nyond of the scope of this paper.\nWe observe that given a collection of spectral\nframes X= (x1;:::;xk), the solution of the pur-\nsuit problem aims at ﬁnding a non-negative factoriza-input : Data x, dictionary D, metric matrix M,\nparameters\u00151;\u00152, step size\u000b.\noutput : Non-negative sparse code z.\nDeﬁne H= (1\u0000\u00152\n\u000b)I\u00001\n\u000bMTDTDM ,\nG=1\n\u000bMTDT,t=1\n\u000b\u00151.\nInitialize z1=0andb1=Gx.\nfork= 1;2;::: until convergence do\nzk+1=\u001bt(bk)\nbk+1=bk+H(zk+1\u0000zk)\nend\nAlgorithm 1: Non-negative iterative shrinkage-\nthresholding algorithm (ISTA). \u001bt(b) = maxf0;b\u0000tg\ndenotes element-wise single-sided soft thresholding.\ntion of XintoDZ, thus being essentially an instance\nof a non-negative matrix factorization (NMF) problem\nwith a ﬁxed left factor D. The approach proposed in\nthe paper can be essentially viewed as a supervised\nversion of NMF.\nDenoting the parameters of the sparse model as\n\u0002=fD;Mg, and those of the linear classiﬁer as\n\b=fW;ag, the proposed pitch transcription system\ncan be expressed as y=y\b(z\u0002(x)), where z\u0002de-\nnotes the non-linear map produced by solving (1), and\ny\brefers to the application of the classiﬁer. In what\nfollows, we will address how to train and adapt the\nparameters \u0002and\bfor the AMT task.\nDictionary initialization. The initial dictionary is\nconstructed to contain spectral templates for each pos-\nsible pitch. The training of the dictionary is done by\nlearning a set of small sub-dictionaries, one per pitch,\nminimizing\nmin\nD2D;Z\u001501\n2kM(X\u0000DZ)k2\nF+\n\u00151kZk1+\u00152kZk2\nF;(2)\nwherek\u0001k Fdenotes the Frobenius norm, Dis the\nspace of appropriately sized non-negative matrices\nwith unit columns, and M=I. Additional constrains\nsuch as harmonicity can be included by changing D\nto be more restrictive. The initial dictionary can be\nconstructed for a speciﬁc instrument or for multiple\ninstruments as in [2], via simple concatenation.\nClassiﬁer initialization. Once the initial dictionary\nhas been trained, we can learn the classiﬁer parameters\n\b. To that end, we construct a training set Xcontain-\ning pairs of the form (x;y), of spectral frames with the\ncorresponding groundtruth pitch labels. Here, unlike\nin the unsupervised dictionary training, the best per-\nformance of the classiﬁer is obtained when the train-\ning set contains representative examples of chords and\npitch combinations.The classiﬁer is trained by minimizing\nmin\n\b1\njXjX\n(x;y)2X`(y\b(z);y) (3)\non the outputs z=z\u0002(x)of the pursuit algorithm.\nHere,`denotes a loss function penalizing for the mis-\nmatch between the ground truth labels and the actual\noutput of the classiﬁer. We use the logistic regression\nloss function `(y0;y) = log(1 + e\u0000yTy0). The mini-\nmization of (3) can scale to very large training sets by\nusing (projected) stochastic gradient descent (SGD)\ntechniques [7], which we adopt in all our experiments.\n3. BILEVEL SPARSE MODEL\nA striking disadvantage of the two-stage training de-\nscribed so far is the fact that the training of the dictio-\nnaryDaims at reducing the data reconstruction error\nkX\u0000DZkFrather than reducing the classiﬁcation er-\nror (3). Consequently, the dictionary trained in the\ninitial unsupervised regime is suboptimal in the sense\nof (3); furthermore, there is no natural way to train the\nmetric matrix M. The ultimate way to perform super-\nvised training of the entire system would be therefore\nby minimizing\nmin\n\u0002;\b1\njXjX\n(x;y)2X`(y\b(z\u0002(x));y) +\u0016jj\bjj2\nF(4)\nnot only with respect to the parameters \bof the clas-\nsiﬁer, but also with respect to the parameters \u0002of the\npursuit. This leads to a bilevel optimization problem,\nas we need to optimize the loss function `, which in\nturn depends on the minimizer of (1). Note that, as is\nstandard practice in machine learning, a regularization\nterm on the classiﬁer parameters is added to prevent\nover-ﬁtting.\nIn particular, one would need to compute the gra-\ndients of the loss with respect to the parameters \u0002=\nfD;Mgof the pursuit. Fortunately, zis almost every-\nwhere differentiable with respect to DandM[14].\nDenoting by \u0003the active set of z(i.e., the set of in-\ndices at which it attains non-zero values), we deﬁne\n\f\u0003= (DT\n\u0003MTMD \u0003+\u00152I\u0003)\u00001(rz`(y\b(z);y))\u0003;\nwhererz`is the gradient of the loss function with\nrespect to z. The elements of \foutside \u0003are set to\nzero. The gradients of `(y\b(z);y)with respect to D\nandMcan be expressed as\nrD`=MTM((x\u0000Dz\u0003)\fT\u0000D\fz\u0003) (5)\nrM`=MD \u0003\f\u0003(x\u0000Dz\u0003)T\u0000M(x\u0000Dz\u0003)DT\n\u0003:\nWe omit the derivation details due to lack of space,\nand refer the reader to [14] for a related discussion.We perform the minimization of (4) again by using\nSGD alternating descents on \bkeeping \u0002ﬁxed, and\non\u0002keeping \bﬁxed. \u0002and\bare initialized as de-\nscribed in Section 2. It is also worthwhile noting that\nthe minimization of the discriminative loss (4) with\nrespect to the matrix Mcan be viewed as a particular\nsetting of metric learning – a family of problems that\naim at designing task-speciﬁc metrics. In our case,\nwe design a Mahalanobis metric MTMsuch that the\npursuit with respect to it minimizes the classiﬁcation\nerrors.\nThe purely discriminative objective of (4) is sus-\nceptible to over-ﬁtting since the learned matrix Mwill\nnot aim at producing faithful data reconstructions. In\nthat case, the generative advantage of NMF would be\nlost. To avoid this problem, the minimization of (4)\ncan be regularized by adding an data reconstruction\nterm of the formkx\u0000Dz\u0002(x)k2\n2.\nWe distinguish between two training regimes: in\nthefully supervised setting, all samples in the training\nset come with label information y, and the training is\nperformed as described above. Since label informa-\ntion is often difﬁcult to obtain, in many practical cases\nonly some of the samples in the training set are la-\nbeled. We call such a setting semi-supervised . Given\na set of unlabeled data, Xu, we can change the learn-\ning process by augmenting the discriminative loss (4)\non the labeled data (eventually regularized with data\nreconstruction term), with the unsupervised term\nX\nx2Xu1\n2kx\u0000Dz\u0002(x)k2\n2+\u00151kz\u0002(x)k1+\u00152kz\u0002(x)k2\n2:\nThis new training scheme aims at producing a dictio-\nnaryDthat is good for classifying (and reconstruct-\ning) the labeled data but that can also used to sparsely\nrepresent the unlabeled data. Note that this regime can\nalso be used as a way to adapting the system to unseen\ntesting data. Both factorization- and classiﬁcation-\nbased approaches suffer a performance drop when the\ntesting data are not well represented by the training\nsamples. In this way, our system can be adapted to\nnew unseen (and unlabeled) data.\n4. FAST APPROXIMATION\nThe proposed approach relies on solving optimiza-\ntion problem (1) using an iterative method. One of\nthe drawbacks of such iterative schemes is their rel-\natively high computational complexity and latency,\nwhich is furthermore data-dependent. For exam-\nple, non-negative ISTA typically requires hundreds\nof iterations to converge. However, while the clas-\nsical optimization theory provides worst-case (data-\nindependent) convergence rate bounds for many fam-\nilies of iterative algorithms, very little is known about\n1001011020102030405060\nNumber of iterations / layersAccuracy (%)\n  \nISTA\nNN Encoder\nNN Encoder (adapted)Figure 1 .Accuracy of the optimization-based and neural\nnetwork encoders as a function of the number of iterations\nor layers. Evaluation was performed on dataset from [17].\nThe networks were trained in the unsupervised regime.\ntheir behavior on speciﬁc data, coming e.g., from a\ndistribution supported on a low-dimensional manifold\n– properties often exhibited by real data. Common\npractice of sparse modeling concentrates on creating\nsophisticated data models, and then relies on compu-\ntational and analytic techniques that are totally agnos-\ntic of the data structure.\nFrom the perspective of the pursuit process, the\nminimization of (1) is merely a proxy to obtaining\na highly non-linear map between the data vector x\nand the corresponding feature vector z. Adopting\nISTA as the iterative algorithm, such a map can be\nexpressed by unrolling the iterations into the compo-\nsitionf\u000ef\u000e\u0001\u0001\u0001\u000ef(0;Gx)ofTelementary op-\nerations of the form f: (z;b)7!(\u001bt(b);b+\nH(\u001bt(b)\u0000b));whereTis a sufﬁciently large num-\nber of iterations required for convergence. By ﬁxing\nT, we obtain a ﬁxed-complexity and latency encoder\n^zT;\t(x), parametrized by \t=fH;G;tg(recall\nthat ISTA deﬁnes the latter parameters as functions of\n\u0002=fD;Mg). Such an encoder can be thought of\nas a time-recurrent neural network, or a feed-forward\nnetwork with Tidentical layers.\nNote that for a sufﬁciently large T,^zT;\t\u0019z\u0002.\nHowever, when complexity budget constraints require\nTto be truncated at a small ﬁxed number, the output\nof^zT;\tis usually unsatisfactory, and the worst-case\nbounds provided by the classical optimization theory\nare of little use. However, within the family of func-\ntionsf^zT;\tg, there might exist better parameters for\nwhich ^zperforms better on relevant input data . These\nideas advocated by [11], have been recently shown\nvery effective in sound separation problems [21].\nAdapted to our problem, the encoder ^zT;\tcan be\ntrained in lieu of the iterative pursuit process in one of\nthe discussed regimes, using the standard backpropa-\ngation techniques to compute the gradients of the net-\nwork with respect to its parameters [11]. The training\nof the encoder can be achieved by minimize the dis-criminative objective\nmin\n\t;\b1\njXjX\n(x;y)2X`(y\b(^zT;\t(x));y) +\u0016jj\bjj2\nF(6)\nSimilar ideas can be used in the unsupervised set-\nting. Figure 1 shows, as a function of T, the per-\nformance of the exact pursuit (ISTA truncated after\nTiterations), and its approximation using the neural\nnetworks. About two orders of magnitude of speedup\nis observed.\nThis perspective bridges between two popular ap-\nproaches to music transcription: those based on ex-\nplicit data modeling and relying on optimization to\nsolve some kind of a representation pursuit problem,\nand those relying on pure learning of a neural net-\nwork. However, while training ^zT;\tis technically a\npure learning approach, it is very much rooted into the\nunderlying data model. First and most importantly,\nthe training objective is solving a matrix factorization\nproblem. Second, the architecture of the neural net-\nwork is derived from an iterative process that is guar-\nanteed to minimize a meaningful objective. Third,\nthe network comes with a very good initialization of\nthe parameters (as prescribed by ISTA). Since neural\nnetwork training is a highly non-convex optimization\nproblem, such an initialization is crucial.\n5. TEMPORAL REGULARIZATION\nIndependent analysis of spectral frames fails short of\nexploiting the temporal structures and dependencies\nof music signals. This prior knowledge can be incor-\nporated by temporally regularizing the output of the\nclassiﬁers. A popular way to achieve this is by adding\na post-processing stage based on hidden Markov mod-\nels (HMMs) [18]. Following [2, 17], in this work\nwe smooth the classiﬁer outputs using an independent\ntwo-state HMM for each pitch. We now think of the\noutput of the sparse coding Z= (z1;:::;zk)as a se-\nquence ofkinput observations. For each pitch p, the\nstates are represented as a sequence of hidden vari-\nablesqp= (qp\n1;:::;qp\nk)that take the value of +1in\nthe presence, or\u00001in the absence of the pitch, fol-\nlowing the convention used throughout the paper. The\nHMM aims at ﬁnding for each pitch, the optimal se-\nquence of states minimizing\nmin\nqpp(z1jqp\n1)p(qp\n1)kY\ni=2p(zijqp\ni)p(qp\nijqp\ni\u00001);(7)\nwhere the initial probabilities p(qp\n1)and the transition\nprobabilities p(qp\nijqp\ni\u00001)are learned from the data.\nThe probability of observing a sparse code given a\npitch state,p(zijqp\ni), can be obtained naturally fromTable 1 .Precision, recall, F1 and accuracy in percent on\nthe test data presented in [17] for the proposed approach\nunder different training regimes. For reference, the accu-\nracy obtained for three recent alternative methods is: 57.6\n% for [2], 56.5 % for [17], and 47.0 % for [3].\nTraining regime Pre. Rec. F1 Acc.\nSupervised 81.6 69.8 74.3 60.0\nSupervised+Fitting 79.7 69.9 73.7 59.2\nSemi-supervised 79.7 70.0 73.7 59.2\nSemi-supervised+Fitting 82.0 70.9 75.1 61.0\nthe output of the classiﬁers. The logistic classiﬁers\ncan be thought as generalized linear predictors for\nBernoulli variables, leading to p(zijqp\ni=y) = 1=(1+\ne\u0000yWTzi):Hence, maximizing the classiﬁer’s perfor-\nmance can be thought as maximizing the likelihood of\nthe true state in the HMM. Problem (7) is solved using\nthe Viterbi algorithm [18].\n6. EXPERIMENTAL EV ALUATION\nSimilarly to the factorization-based methods, the pro-\nposed model can be trained to transcribe pieces con-\ntaining mixtures of instruments by appropriately train-\ning the initial dictionaries. However, since the scope\nof the paper is rather a proof-of-concept than the de-\nsign of a full-featured AMT system, we limit the ex-\nperimental evaluation to piano recordings only.\nData. The system was tested on the Disklavier\ndataset proposed in [17]. For training of the initial dic-\ntionaries (pitch templates), two different piano types\nwere used from the MAPS dataset [10]. Then, the\nclassiﬁer and the dictionary were trained in the super-\nvised regime using the ﬁrst 30seconds of 50 songs\nfrom MAPS and the training data in [17]. Spec-\ntral frames were represented using CQT with 48fre-\nquency bins per octave.\nPerformance measures. We adopted the frame-\nbased accuracy measure proposed in [17], Acc =\nTP=(FP+FN+TP) , where TP(true positives) is the\nnumber of correctly predicted pitches, and FP(false\npositives) and FN(false negatives) are the number of\npitches incorrectly transcribed as ONorOFF, respec-\ntively. We also include the standard frame-based pre-\ncision, recall and F1measures.\nEvaluation. The proposed system was evaluated\nunder different training regimes. Training the sys-\ntem by solving the bilevel optimization problem (4)\nis referred to as Supervised , and Supervised+Fitting\nwhen using the additional data ﬁtting term described\nin Section 3. We also tested the capability of our sys-\ntem to use unlabeled data to unsupervisedly adapt to\nthe test data (the Semi-supervised+Fitting settings).\nThe obtained performance was compared against one\nsuccessful representative approach from each of themain existing philosophies. We used [2] to represent\nthe factorization-based approaches, and [17] for the\nclassiﬁcation-based ones. Both of these methods in-\nclude a post-processing stage very similar to the one\nused in the work. We also included a third method\nbased on onset detection [3]. It is worth mention-\ning that the training of [2] does not use the Disklavier\ntraining data given in [17]; the authors train their sys-\ntem using samples of three different piano types of the\nMAPS dataset.\nTable 1 summarizes the obtained results. The pro-\nposed method is competitive with the alternative ap-\nproaches. The inclusion of unlabeled data allows a\nsigniﬁcant improvement in the system performance.\nAdding the ﬁtting term seems to have more impact\nwhen unlabeled data are available. We attribute this\nto the reduction of over-ﬁtting risks that such a regu-\nlarization offers. In both semi- and fully-supervised\nregimes, the reconstruction properties of the dictio-\nnary are much better preserved with the mentioned ﬁt-\nting regularization.\n7. CONCLUSION\nWe showed a trainable bilevel non-negative sparse\nmodel model for polyphonic music transcription. Our\nmodel can be interpreted as a supervised variant of\nNMF as well as a ﬂavor of metric learning. We also\nshowed that the original iterative optimization-based\napproach can be efﬁciently approximated by ﬁxed-\ncomplexity feed-forward architectures that give a two-\norder-of-magnitude speedup at little expense of accu-\nracy. This creates an interesting relation between the\noptimization-based transcription methods, and those\nrelying on pure learning, which are traditionally dealt\nwith by two, practically disjoint, communities. The\napproach can naturally beneﬁt from the inclusion of\nunlabeled data via a semi-supervised training scheme.\n8. REFERENCES\n[1] S. Abdallah and M. Plumbley. Polyphonic music tran-\nscription by non-negative sparse coding of power spec-\ntra. In ISMIR , pages 10–14, 2004.\n[2] E. Benetos and S. Dixon. Multiple-instrument poly-\nphonic music transcription using a convolutive proba-\nbilistic model. In Sound and Music Computing Confer-\nence, pages 19–24, 2011.\n[3] E. Benetos and S. Dixon. Polyphonic music transcrip-\ntion using note onset and offset detection. In ICASSP ,\npages 37–40. IEEE, 2011.\n[4] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and\nA. Klapuri. Automatic music transcription: Breaking the\nglass ceiling. In ISMIR , 2012.\n[5] N. Bertin, R. Badeau, and E. Vincent. Enforcing har-\nmonicity and smoothness in Bayesian non-negative ma-\ntrix factorization applied to polyphonic music transcrip-\ntion. IEEE Trans. Audio, Speech, and Language Proc. ,\n18(3):538–549, 2010.[6] S. Bock and M. Schedl. Polyphonic piano note transcrip-\ntion with recurrent neural networks. In ICASSP , pages\n121–124, 2012.\n[7] L. Bottou. Large-scale machine learning with stochastic\ngradient descent. In COMPSTAT , pages 177–187, Au-\ngust 2010.\n[8] J. C. Brown. Calculation of a constant Q spectral trans-\nform. The Journal of the Acoustical Society of America ,\n89:425, 1991.\n[9] I. Daubechies, M. Defrise, and C. De Mol. An iterative\nthresholding algorithm for linear inverse problems with\na sparsity constraint. Communications on Pure and Ap-\nplied Mathematics , 57(11):1413–1457, 2004.\n[10] V . Emiya, R. Badeau, and B. David. Multipitch estima-\ntion of piano sounds using a new probabilistic spectral\nsmoothness principle. IEEE Trans. Audio, Speech, and\nLanguage Proc. , 18(6):1643–1654, 2010.\n[11] K. Gregor and Y . Lecun. Learning fast approximations\nof sparse coding. In ICML , 2010.\n[12] A. Klapuri. Multipitch analysis of polyphonic music and\nspeech signals using an auditory model. IEEE Trans. Au-\ndio, Speech, and Language Proc. , 16(2):255–266, 2008.\n[13] D. D. Lee and H. S. Seung. Learning parts of ob-\njects by non-negative matrix factorization. Nature ,\n401(6755):788–791, 1999.\n[14] J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary\nlearning. IEEE Trans. PAMI , 34(4):791–804, 2012.\n[15] M. Marolt. A connectionist approach to automatic tran-\nscription of polyphonic piano music. IEEE Trans. Mul-\ntimedia , 6(3):439–449, 2004.\n[16] J. Nam, J. Ngiam, H. Lee, and M. Slaney. A\nclassiﬁcation-based polyphonic piano transcription ap-\nproach using learned feature representations. In ISMIR ,\n2011.\n[17] G. E. Poliner and D. Ellis. A discriminative model for\npolyphonic piano transcription. EURASIP J. Adv. in Sig.\nProc. , 2007, 2006.\n[18] L. R. Rabiner. A tutorial on hidden Markov models and\nselected applications in speech recognition. Proc. IEEE ,\n77(2):257, 1989.\n[19] P. Smaragdis and J. Brown. Non-negative matrix factor-\nization for polyphonic music transcription. In WASPAA ,\npages 177–180, 2003.\n[20] P. Smaragdis, B. Raj, and M. Shashanka. A probabilistic\nlatent variable model for acoustic modeling. In NIPS ,\nvolume 148, 2006.\n[21] P. Sprechmann, A. Bronstein, and G. Sapiro. Real-time\nonline singing voice separation from monaural record-\nings using robust low-rank modeling. In ISMIR , 2012.\n[22] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch estima-\ntion. IEEE Trans. Audio, Speech, and Language Proc. ,\n18(3):528–537, 2010."
    },
    {
        "title": "Low-Rank Representation of Both Singing Voice and Music Accompaniment Via Learned Dictionaries.",
        "author": [
            "Yi-Hsuan Yang"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1418089",
        "url": "https://doi.org/10.5281/zenodo.1418089",
        "ee": "https://zenodo.org/records/1418089/files/Yang13.pdf",
        "abstract": "Recent research work has shown that the magnitude spectrogram of a song can be considered as a superposition of a low-rank component and a sparse component, which appear to correspond to the instrumental part and the vocal part of the song, respectively. Based on this observation, one can separate singing voice from the background music. However, the quality of such separation might be limited, because the vocal part of a song can sometimes be lowrank as well. Therefore, we propose to learn the subspace structures of vocal and instrumental sounds from a collection of clean signals first, and then compute the low-rank representations of both the vocal and instrumental parts of a song based on the learned subspaces. Specifically, we use online dictionary learning to learn the subspaces, and propose a new algorithm called multiple low-rank representation (MLRR) to decompose a magnitude spectrogram into two low-rank matrices. Our approach is flexible in that the subspaces of singing voice and music accompaniment are both learned from data. Evaluation on the MIR-1K dataset shows that the approach improves the source-to-distortion ratio (SDR) and the source-to-interference ratio (SIR), but not the source-to-artifact ratio (SAR).",
        "zenodo_id": 1418089,
        "dblp_key": "conf/ismir/Yang13",
        "keywords": [
            "magnitude spectrogram",
            "low-rank component",
            "sparse component",
            "instrumental part",
            "vocal part",
            "separation",
            "quality",
            "background music",
            "subspace structures",
            "clean signals"
        ],
        "content": "LOW-RANK REPRESENTATION OF BOTH SINGING VOICE AND\nMUSIC ACCOMPANIMENT VIA LEARNED DICTIONARIES\nYi-Hsuan Yang\nResearch Center for IT Innovation, Academia Sinica, Taiwan\nyang@citi.sinica.edu.tw\nABSTRACT\nRecent research work has shown that the magnitude spec-\ntrogram of a song can be considered as a superposition of\na low-rank component and a sparse component, which ap-\npear to correspond to the instrumental part and the vocal\npart of the song, respectively. Based on this observation,\none can separate singing voice from the background music.\nHowever, the quality of such separation might be limited,\nbecause the vocal part of a song can sometimes be low-\nrank as well. Therefore, we propose to learn the subspace\nstructures of vocal and instrumental sounds from a collec-\ntion of clean signals ﬁrst, and then compute the low-rank\nrepresentations of both the vocal and instrumental parts of\na song based on the learned subspaces. Speciﬁcally, we use\nonline dictionary learning to learn the subspaces, and pro-\npose a new algorithm called multiple low-rank representa-\ntion (MLRR) to decompose a magnitude spectrogram into\ntwo low-rank matrices. Our approach is ﬂexible in that the\nsubspaces of singing voice and music accompaniment are\nboth learned from data. Evaluation on the MIR-1K dataset\nshows that the approach improves the source-to-distortion\nratio (SDR) and the source-to-interference ratio (SIR), but\nnot the source-to-artifact ratio (SAR).\n1. INTRODUCTION\nA musical piece is usually composed of multiple layers\nof voices sounded simultaneously, such as human vocal,\nmelody line, bass line and percussion. These components\nare mixed in most songs sold in the market. For many\nmusic information retrieval (MIR) problems, such as pre-\ndominant instrument recognition, artist identiﬁcation and\nlyrics alignment, separating one source from the others is\nusually an important pre-processing step [6, 9, 13].\nMany algorithms have been proposed for blind source\nseparation in monaural music signals [21,22]. For the par-\nticular case of separating singing voice from music accom-\npaniment, it has been found that characterizing the music\naccompaniment as a repeating structure on which varying\nvocals are superimposed leads to good separation qual-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2013 International Society for Music Information Retrieval.ity [8,16,17,23]. For example, Huang et al. [8] found that,\nby decomposing the magnitude spectrogram of a song into\na low-rank matrix and a sparse matrix, the sparse compo-\nnent appears to correspond to the singing voice. Evaluation\non the MIR-1K data set [7] shows that such a low-rank\ndecomposition (LRD) method outperforms sophisticated,\npitch-based inference methods [7, 22].\nHowever, the low-rank and sparsity assumptions about\nthe music accompaniment and singing voice have not been\ncarefully studied so far. From mathematical point of view,\nthe low-rank component corresponds to a succinct repre-\nsentation of the observed data in a lower dimensional sub-\nspace, whereas the sparse component corresponds to the\n(small) fraction of the data samples that are far away from\nthe subspace [2, 11]. Without any prior knowledge of the\ndata, it is not easy to distinguish between data samples\noriginated from the subspace of music accompaniment and\nthose from the subspace of singing voice. Therefore, the\nlow-rank matrix resulting from the aforementioned decom-\nposition might be actually a mixture of the subspaces of vo-\ncal and instrumental sounds, and the sparse matrix might\ncontain a portion of the instrumental sounds such as the\nmain melody or the percussion sounds [23].\nBecause MIR-1K comes with “clean” vocal and instru-\nmental sources recorded separately at the left and right\nchannels, in our pilot study we tried LRD using principal\ncomponent analysis (PCA) [2] for the two clean sources,\nrespectively. Result shows that, contrary to the sparsity\nassumption, the vocal channel can also be well approxi-\nmated by a low-rank matrix. As Figure 1 exempliﬁes, we\nare able to reduce the rank of the singing voice and the mu-\nsic accompaniment matrices (by PCA) from 513 to 50 and\n10, respectively, with less than 40% loss in the source-to-\ndistortion ratio (SDR) [20].\nMotivated by the above observation, in this paper we\ninvestigate the quality of separation as a result of decom-\nposing the magnitude spectrogram of a song into “two”\nlow-rank matrices plus one sparse matrix. The ﬁrst two\nmatrices represent the singing voice and music accompa-\nniment in the subspaces of vocal and instrumental sounds,\nrespectively, whereas the last matrix contains data samples\ndeviated from the subspaces. Therefore, unlike existing\nmethods, the vocal part of a song is also modeled as a low-\nrank signal. Moreover, different subspaces are explicitly\nused for vocal and instrumental sounds.\nTo achieve the above decomposition, we propose a new\nalgorithm called multiple low-rank representation (MLRR),Figure 1 . (a) (b) The original, full-rank magnitude spec-\ntrograms (in log scale) of the vocal and instrumental parts\nof the clip ‘Ani 101’ in MIR-1K [7]. (c) (d) The low-rank\nmatrices of the vocal part (rank =50) and the instrumental\npart (rank =10) obtained by PCA. Such low-rank approxi-\nmation only incurs 40% loss in signal-to-distortion ratio.\nwhich involves an iterative optimization process that seeks\nthe lowest rank representation [2, 10, 11]. Moreover, in-\nstead of decomposing a signal from scratch, we employ an\nonline dictionary learning algorithm [12] to learn the sub-\nspace structures of the vocal and instrumental sounds in\nadvance from an external collection of clean vocal and in-\nstrumental signals. In this way, we are able to incorporate\nprior knowledge about the nature of vocal and instrumental\nsounds to the decomposition process.\nThe paper is organized follows. Section 2 reviews re-\nlated work on LRD its application to singing voice separa-\ntion. Section 3 describes the proposed algorithms. Section\n4 presents the evaluation and Section 5 concludes.\n2. REVIEW ON LOW-RANK DECOMPOSITION\nIt has been shown that many real-world data can be well\ncharacterized by low-dimensional subspaces [11]. That is,\nif we putn m-dimensional data vectors in the form of a\nmatrixX2<m\u0002n,Xshould have rank r\u001cmin(m;n),\nmeaning few linearly independent columns [2]. The goal\nof LRD is to obtain a low-rank approximation of Xin the\npresence of outliers, noises, or missing values [11].\nThe classical principal component analysis (PCA) [2]\nseeks a rank- restimateAof the matrix Xby solving\nmin\nAkX\u0000Ak\nsubject to rank (A)\u0014r;(1)\nwherekXkdenotes the spectral norm, or the largest singu-\nlar value ofX. This problem can be efﬁciently solved via\nsingular value decomposition (SVD) by using the rlargest\nsingular values [2].It is well-known that PCA is sensitive to outliers. To\nremedy this issue, robust PCA (RPCA) [2] uses the l1norm\nto characterize sparse corruptions and solves\nmin\nAkAk\u0003+\u0015kX\u0000Ak1; (2)\nwherek\u0001k\u0003denotes the nuclear norm (the sum of its sin-\ngular values),k\u0001k 1is thel1norm that sums the absolute\nvalues of matrix entries, and \u0015is a positive weighting pa-\nrameter. The use of nuclear norm as a surrogate of the\nrank function makes it possible to solve (2) by convex opti-\nmization algorithms such as accelerated proximal gradient\n(APG) or augmented Lagrange multipliers (ALM) [10].\nRPCA has been successfully applied to singing voice\nseparation [8]. Researchers found that the resulting sparse\ncomponent (i.e., X\u0000A) appears to correspond to the vocal\npart and the low-rank one (i.e., A) corresponds to the music\naccompaniment. More recently, Yang [23] found that the\nsparse component often contains percussion sounds and\nproposed a back-end drum removal procedure to enhance\nthe quality of the separated singing voice. Sprechmann et\nal.[17] considered both AandX\u0000Ato be non-negative\nand employed multiplicative algorithms to solve the re-\nsulting robust non-negative matrix factorization (RNMF)\nproblem. Efﬁcient, supervised or semi-supervised variants\nhave also been proposed [17]. Although promising result\nis obtained, none of the reviewed methods justiﬁed the as-\nsumption of considering singing voice as sparse.\nDurrieu et al. [3] proposed a non-negative matrix factor-\nization (NMF)-based method for singing voice separation\nthat regards the vocal spectrogram as an element-wise mul-\ntiplication of an excitation spectrogram and a ﬁlter spectro-\ngram. Many other NMF-based methods that do not rely on\nthe sparse assumption have also been proposed [14]. How-\never, we tend to focus on LRD-based methods that have\nsimilar form as RPCA in this work. The comparison with\nNMF-based methods is left as a future work.\nFinally, low-rank representation (LRR) [11] seeks the\nlowest rank estimate of data Xwith respect to D2<m\u0002k,\na “dictionary” that is assumed to linearly span the space of\nthe data being analyzed. Speciﬁcally, it solves\nmin\nZkZk\u0003+\u0015kX\u0000DZk1; (3)\nwhereZ2<k\u0002nandkdenotes the dictionary size. Since\nrank(DZ)\u0014rank(Z),DZ is also a low-rank recovery to\nX. As discussed in [11], by properly choosing D, LRR\ncan recover data drawn from a mixture of several low-rank\nsubspaces. By setting D=Im, them\u0002midentify matrix,\nthe formulation (3) reduces to (2). Although it is possible\nto use dictionary learning algorithms such as K-SVD [1]\nto learn a dictionary from data, Liu et al. [11] simply set\nD=X, using the data matrix itself as the dictionary. In\ncontrast, we extend LRR to the case of multiple dictionar-\nies and employ online dictionary learning (ODL) [12] to\nlearn the dictionaries, as described below.\n3. PROPOSED ALGORITHMS\nBy extending formulation (3), we are able to obtain the\nlow-rank representations of Xwith respect to multiple dic-Figure 2 . The spectra (in log scale) of the learned dictio-\nnaries (with 100 codewords) for (a) vocal and (b) instru-\nmental spectra, using online dictionary learning.\ntionariesD1;D2;:::;D\u0014, where\u0014denotes the number of\ndictionaries. Although it is possible to use a dictionary for\neach musical component (e.g., human vocal, melody line,\nbass line and percussion), we consider the case \u0014= 2and\nuse one dictionary for human vocal and the other for the\nmusic accompaniment.\n3.1 Multiple Low-Rank Representation (MLRR)\nGiven an input data Xand two pre-deﬁned (or pre-learned)\ndictionaries D12<m\u0002k1andD22<m\u0002k2(k1andk2\ncan take different values), MLRR seeks the lowest rank\nmatricesZ1andZ2by solving\nmin\nZ1;Z2kZ1k\u0003+\fkZ2k\u0003+\u0015kX\u0000D1Z1\u0000D2Z2k1;(4)\nwhere\fis a positive parameter. This optimization prob-\nlem can be solved by the method of ALM [10], by ﬁrst\nreformulating (4) as\nmin\nZ1;Z2;J1;J2;EkJ1k\u0003+\fkJ2k\u0003+\u0015kEk1\nsubject to X=D1Z1+D2Z2+E;\nZ1=J1; Z2=J2;(5)\nand then minimizing the augmented Lagrangian function\nL=kJ1k\u0003+tr(YT\n1(Z1\u0000J1)) +\u0016\n2kZ1\u0000J1k2\nF\n+\fkJ2k\u0003+tr(YT\n2(Z2\u0000J2)) +\u0016\n2kZ2\u0000J2k2\nF\n+\u0015kEk1+tr(YT\n3(X\u0000D1Z1\u0000D2Z2\u0000E))\n+\u0016\n2kX\u0000D1Z1\u0000D2Z2\u0000Ek2\nF;\n(6)\nwherek\u0001kFdenotes the Frobenius norm (square root of\nthe sum of the squares of its elements) and \u0016is a posi-\ntive penalty parameter. We can minimize (6) with respect\ntoZ1;Z2;J1;J2;E, respectively, by ﬁxing the other vari-\nables and then updating the Lagrangian multipliers Y1,Y2\nandY3. For example, J2can be updated by\nJ\u0003\n2=argmin\fkJ2k\u0003+\u0016\n2kJ2\u0000(Z2+\u0016\u00001Y2)k2\nF;\n(7)\nwhich can be solved via the singular value thresholding\n(SVT) operator [2], whereas Z1can be updated by\nZ\u0003\n1= \u0006 1\u0000\nDT\n1(X\u0000D2Z2\u0000E) +J1+\u0016\u00001(DT\n1Y3\u0000Y1)\u0001\n;\n(8)where \u00061= (I+DT\n1D1)\u00001. The update rule for the other\nvariables can be obtained in a similar way as described\nin [10, 11], mainly by taking the ﬁrst-order derivative of\nthe augmented Lagrangian function Lwith respect to the\nvariable. By using a non-decreasing sequence of f\u0016tgas\nsuggested in [10] (i.e., using \u0016tin thet-th iteration), empir-\nically we observe that the optimization usually converges\nin 100 iterations. After the decomposition, we consider\nD1Z1andD2Z2as the vocal and instrumental parts of the\nsong and discard the intermediate matrices E,J1andJ2.\n3.2 Learning the Subspace Structures of Singing and\nInstrumental Sounds\nThe goal of dictionary learning is to ﬁnd a proper repre-\nsentation of data by means of reduced dimensionality sub-\nspaces, which are adaptive to both the characteristics of\nthe observed signals and the processing task at hand [19].\nMany dictionary learning algorithms have been proposed,\nsuch askmeans and K-SVD [1, 19]. In this work, we\nadopt the online dictionary learning (ODL) [12], a ﬁrst-\norder stochastic gradient descent algorithm, for its low mem-\nory consumption and computational cost. ODL has been\nused in many MIR tasks such as genre classiﬁcation [24].\nGivenNsignalspi2<m, ODL learns a dictionary D\nby solving the following joint optimization problem,\nmin\nD;Q1\nNNX\ni=1\u00121\n2kpi\u0000Dqik2\n2+\u0011kqik1\u0013\n;\nsubject todT\njdj\u00141;qi\u00150;(9)\nwherek\u0001k 2denotes the Euclidean norm for vectors, Q\ndenotes the collection of the (unknown) nonnegative en-\ncoding coefﬁcients qi2<k, and\u0011is a regularization pa-\nrameter. The dictionary Dis composed of kcodewords\ndj2<m, whose energy is limited to be less than one. For-\nmulation (9) can be solved by updating DandQin an al-\nternating fashion. The optimization of qiinvolves a typical\nsparse coding problem that can be solved by the LARS-\nlasso algorithm [4]. Our implementation of ODL is based\non the SPAMS toolbox [12].1\nFigure 2 shows the dictionaries for vocal and instru-\nmental spectra we learned from a subset of MIR-1K, using\nk1=k2= 100 . It can be found that the vocal dictio-\nnary contains voices of higher fundamental frequency. In\naddition, we see more energy in the so-called “singer’s for-\nmant” (around 3 khz) from the vocal dictionary [18], show-\ning that the two dictionaries capture distinct characteristics\nof the signals. Finally, we also observe some atoms that\nspan almost the whole spectra in both dictionaries (e.g.,\nthe 12th codeword in the instrumental dictionary), possi-\nbly because of the need to reconstruct a signal by a sparse\nsubset of the dictionary atoms, by virtue of the l1-based\nsparsity constraint in formulation (9).\nIn principle, we can improve the reconstruction accu-\nracy (i.e., smallerkpi\u0000Dqik2in (9)) by using larger k[12],\nat the expense of increasing the computational cost in solv-\ning both (9) and (5). However, as Section 4.1 shows, larger\n1http://spams-devel.gforge.inria.fr/kdoes not necessarily lead to better separation quality,\npossibly because of the mismatch between the goals of re-\nconstruction and of separation.\nThe source codes, sound examples, and more details of\nthis work are available online.2\n4. EVALUATION\nOur evaluation is based on the MIR-1K dataset collected\nby Hsu & Jang [7].3It contains 1,000 song clips extracted\nfrom 110 Chinese pop songs released in karaoke format,\nwhich consists of a clean music accompaniment track and a\nmixture track. A total number of eight female and 11 male\namateur singers were invited to sing the songs, thereby cre-\nating the clean singing voice track for each clip. Each clip\nis 4 to 13 seconds in length and sampled at 16 khz. Al-\nthough MIR-1K also comes with human-labeled pitch val-\nues, unvoiced sounds and vocal/nonvocal segments, lyrics,\nand the speech recordings of the lyrics for each clip [7],\nthese information are not exploited in this work.\nFollowing [17], we reserved 175 clips sang by one male\nand one female singers (‘abjones’ and ‘amy’) for training\n(i.e., learning the dictionaries D1andD2), and used the re-\nmaining 825 clips of 17 singers for testing the performance\nof separation. For the test clips, we mixed the two sources\nvandalinearly with equal energy (i.e., 0 db signal-to-\nnoise ratio) to generate x, the mixture of sounds similar\nto the one available from commercial CDs. The goal is to\nrecovervandafromxfor each test clip separately.\nGiven a music clip, we ﬁrst computed its short-time\nFourier transform (STFT) by sliding a Hamming window\nof 1024 samples and 1/4 overlapping (as in [8]) to obtain\nthe spectrogram, which consists of the magnitude part X\nand the phase part P. We applied matrix decomposition\nusingXto get the separated sources. To synthesize the\ntime-domain waveforms ^vand^a, we performed inverse\nSTFT using the magnitude spectrogram of the separated\nsource and the phase Pof the original signal [5]. Because\nthe separated spectrogram may contain negative values, we\nconverted negative values to zero before inverse STFT.\nThe quality of separation is assessed in terms of the fol-\nlowing measures [20], which are computed for the vocal\npartvand the instrumental part a, respectively,\n\u000fSource-to-distortion ratio (SDR), which measures the\nenergy ratio between the source and the distortion\n(e.g.,vtov\u0000^v).\n\u000fSource-to-artifact ratio (SAR), which measures the\namount of artifacts of the source separation algo-\nrithm such as musical noise.\n\u000fSource-to-interference ratio (SIR), which measures\nthe interference from other sources.\nHigher values of these ratios indicate better separation qual-\nity. We computed these ratios by using the BSS Eval tool-\nbox v3.0,4assuming that the admissible distortion is a\n2http://mac.citi.sinica.edu.tw/mlrr\n3https://sites.google.com/site/\nunvoicedsoundseparation/\n4http://bass-db.gforge.inria.fr/\nFigure 3 . The quality of the separated (a) vocal and (b)\ninstrumental parts of the 825 clips in MIR-1K in terms of\nglobal normalized source-to-distortion ratio (GNSDR).\ntime-invariant ﬁlter [20]. As in [7], we compute the nor-\nmalized SDR (NSDR) by SDR (^v;v)\u0000SDR(x;v). More-\nover, we aggregate the performance over all the test clips\nby taking the weighted average, with weight proportional\nto the length of each clip [7]. The resulting measures are\ndenoted as GNSDR, GSAR, and GSIR, respectively (the\nlater two are not normalized).5\n4.1 Result\nWe ﬁrst compared the performance of MLRR with RPCA,\none of the state-of-the-art algorithms for singing voice sep-\naration [8]. We used ALM-based algorithm for both MLRR\nand RPCA [10]. For MLRR, we learned dictionaries from\nthe training set and evaluate separation on the test set of\nMIR-1K. Although it is interested to use different dictio-\nnary sizes for the vocal and instrumental dictionaries, we\nsetk1=k2=kin this study. For RPCA, we simply eval-\nuated it on the test set, without using the training set. The\nvalue of\u0015was set to either \u00150= 1=p\nmax(m;n), accord-\ning to [2] (recall that (m;n)is the size of the input matrix\nX), or 1, as suggested in [11]. We only use \u00150for RPCA\nbecause using 1 did not work. Moreover, we simply set \f\nto 1 for MLRR. For future work it would be interesting to\nuse different \fto investigate whether we want to penalize\nthe rank of one particular source more.6\nFigure 3 shows the quality (in terms of GNSDR) of the\nseparated vocal and instrumental parts using different al-\ngorithms, different values of the parameter \u0015and different\nvalues of the dictionary size k. We found that MLRR at-\ntains the best result when k= 100 for both parts (3.85 db\nand 4.19 db). The performance difference in GNSDR be-\n5Please note that in some previous work the older version BSS Eval\ntoolbox v2.1 was used [7, 8, 23], assuming that the admissible distortion\nis purely a time-invariant gain.\n6In fact, when \f= 1 one can combine Z1andZ2, reducing (4) to\n(3), and used an LRR-based algorithm to solve the problem as well.Table 1 . Separation quality (in db) for the singing voice\nMethod GNSDR GSIR GSAR\nRPCA (\u0015=\u00150) [8] 3.17 4.43 11.1\nRPCAh (\u0015=\u00150) [23] 3.25 4.52 11.1\nRPCAh+FASST [23] 3.84 6.22 9.19\nMLRR (k=100,\u0015=1) 3.85 5.63 10.7\ntween MLRR (when k= 100 ) and RPCA is signiﬁcant,\neither for the vocal or instrumental part, under one-tailed\nt-test (p-value <0.001; d.f. =1648).7\nFrom Figure 3, several observations can be made. First,\nit can be found that using larger kdoes not always lead\nto better performance, as discussed in Section 3.2. Sec-\nond, for the instrumental part, using k= 20 (\u0015=\u00150)\nalready yields high GNSDR (2.74 db), whereas for the vo-\ncal part we need to use at least k= 50 (\u0015= 1). This\nresult shows that we need more dictionary atoms to repre-\nsent the space of the singing voice, possibly because the\nsubspace of singing voice is of higher rank (cf. Figure 1).\nThe separation quality of the singing voice is worse (i.e.,\nlower than zero) when kis too small. Third, we saw that\nthe vocal and instrumental parts favor different values of \u0015\nfor MLRR, which deserves future study.8\nNext, we compared MLRR with the two algorithms pre-\nsented in [23], in terms of more performance measures.\nRPCAh is an APG-based algorithm that uses harmonic-\nity priors to take into account the similarity between si-\nnusoidal elements [23]; RPCAh+FASST employs Flexible\nAudio Source Separation Toolbox for removing the drum\nsounds in the vocal part [15]. Because FASST involves a\nheavy computational process, we set the maximal number\nof iterations to 100 in this evaluation.9\nResult shown in Tables 1 and 2 indicates that, except\nfor the GSIR for singing voice, MLRR outperforms all the\nevaluated RPCA-based methods [8,23] in terms of GNSDR\nand GSIR, especially for the music accompaniment. How-\never, we also found that MLRR introduces some artifacts\nand leads to slightly lower GSAR. This is possibly because\nthe separated sounds are linear combination of the dictio-\nnary atoms, which may not be comprehensive enough to\ncapture every nuance of music signals.\nFinally, to provide a visual comparison, Figure 4 shows\nthe separation result for RCA ( \u0015=\u00150), RCAh+FASST, and\nMLRR (k=100,\u0015=1) for the clip ‘Ani 101,’ focusing on\nlow frequency parts 0–4 khz. We saw that the recovered\nvocal signal well captures the main vocal melody, and that\ncomponents with strong harmonic structure are present in\nthe recovered instrumental part. We also observed undesir-\nable artifacts in the higher frequency components of MLRR,\nwhich should be the subject of future research.\n7We have tried imposing a nonnegative constraint on the dictionary D\n(c.f. Eq. 9) but this did not further improve the result.\n8It is fair to use different \u0015for the two sources; for example, if the\napplication is about analyzing singing voice, one can use \u0015=1.\n9We did not compare our result with another two state-of-the-art meth-\nods [17] and [16], because somehow we cannot reproduce the result for\nthe former and because the latter did not evaluate on MIR-1K. Moreover,\nplease note that the evaluation here is performed on 825 clips (excluding\nthose used for dictionary learning) instead of the whole MIR-1K.Table 2 . Separation quality for the music accompaniment\nMethod GNSDR GSIR GSAR\nRPCA (\u0015=\u00150) [8] 3.19 5.24 9.23\nRPCAh (\u0015=\u00150) [23] 3.27 5.31 9.30\nRPCAh+FASST [23] 3.21 5.24 9.30\nMLRR (k=100,\u0015=\u00150) 4.19 7.80 8.22\n5. CONCLUSION AND DISCUSSION\nIn this paper, we have presented a time-frequency based\nsource separation algorithm for music signals that consid-\ners both the vocal and instrumental spectrograms as low-\nrank matrices. The technical contributions we have brought\nto the ﬁeld include the use of dictionary learning algo-\nrithms to estimate the subspace structures of music sources\nand the development of a novel algorithm MLRR that uses\nthe learned dictionaries for decomposition. The proposed\nmethod is advantageous in that potentially more training\ndata can be harvested to improve the result of separation.\nAlthough it might not be fair to directly compare the per-\nformance of MLRR and RPCA (because the former uses an\nexternal dictionary), our result shows that we can still get\nsimilar separation quality without the sparse assumption on\nthe singing voice. However, because the separated sounds\nare linear combination of the atoms in the pre-learned dic-\ntionaries, there are some unwanted artifacts that are audi-\nble, which should be the subject of future work.\n6. ACKNOWLEDGMENTS\nThis work was supported by the National Science Council\nof Taiwan under Grants NSC 101-2221-E-001-017, NSC\n102-2221-E-001-004-MY3 and the Academia Sinica Ca-\nreer Development Award.\n7. REFERENCES\n[1] M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An\nalgorithm for designing overcomplete dictionaries for\nsparse representation. IEEE Trans. Signal Processing ,\n54(11):4311–4322, 2006.\n[2] E. J. Cand `es, X. Li, Y . Ma, and J. Wright. Robust\nprincipal component analysis? Journal of the ACM ,\n58(3):1–37, 2011.\n[3] J.-L. Durrieu, G. Richard, and B. David. An iterative\napproach to monaural musical mixture de-soloing. In\nProc. ICASSP , pages 105–108, 2009.\n[4] B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani.\nLeast angle regression. Annals of Statistics , 32:407–\n499, 2004.\n[5] D. Ellis. A phase vocoder in Matlab, 2002. [Online]\nhttp://www.ee.columbia.edu/ dpwe/resources/matlab/pvoc/.\n[6] H. Fujihara, M. Goto, J. Ogata, and H. G. Okuno.\nLyricsynchronizer: Automatic synchronization system\nbetween musical audio signals and lyrics. J. Sel. Topics\nSignal Processing , 5(6):1252–1261, 2011.Figure 4 . (a) The magnitude spectrogram (in log scale) of the mixture of singing and music accompaniment for the clip\n‘Ani 101’ in MIR-1K [7]; (b) (c) The groundtruth spectrograms for the two sources; the separation result for (d) (e) RPCA\n[8], (f) (g) RPCAh+FASST [23], and (h) (i) the proposed method MLRR ( k=100,\u0015=1) for the two sources, respectively.\n[7] C.-L. Hsu and J.-S. R. Jang. On the improvement of\nsinging voice separation for monaural recordings us-\ning the MIR-1K dataset. IEEE Trans. Audio, Speech &\nLanguage Processing , 18(2):310–319, 2010.\n[8] P.-S. Huang, S. D. Chen, P. Smaragdis, and\nM. Hasegawa-Johnson. Singing-voice separation from\nmonaural recordings using robust principal component\nanalysis. In Proc. ICASSP , pages 57–60, 2012.\n[9] M. Lagrange, A. Ozerov, and E. Vincent. Robust singer\nidentiﬁcation in polyphonic music using melody en-\nhancement and uncertainty-based learning. In Proc. IS-\nMIR, pages 595–560, 2012.\n[10] Z. Lin, M. Chen, L. Wu, and Yi Ma. The augmented\nLagrange multiplier method for exact recovery of cor-\nrupted low-rank matrices. Technical Report UILU-\nENG-09-2215, 2009.\n[11] G. Liu, Z. Lin, S. Yan, J. Sun, Y . Yu, and Y . Ma. Ro-\nbust recovery of subspace structures by low-rank repre-\nsentation. IEEE Trans. Pattern Anal. & Machine Intel. ,\n35(1):171–184, 2013.\n[12] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dic-\ntionary learning for sparse coding. In Proc. Int. Conf.\nMachine Learning , pages 689–696, 2009.\n[13] M. M ¨uller, D. P. W. Ellis, A. Klapuri, and G. Richard.\nSignal processing for music analysis. J. Sel. Topics Sig-\nnal Processing , 5(6):1088–1110, 2011.\n[14] G. Mysore, P. Smaragdis, and B. Raj. Non-negative\nhidden Markov modeling of audio with application to\nsource separation. In Int. Conf. Latent Variable Analy-\nsis and Signal Separation , pages 829–832, 2010.\n[15] A. Ozerov, E. Vincent, and F. Bimbot. A general ﬂexi-\nble framework for the handling of prior information inaudio source separation. IEEE Trans. Audio, Speech &\nLanguage Processing , 20(4):1118–1133, 2012.\n[16] Z. Raﬁi and B. Pardo. REpeating Pattern Extraction\nTechnique (REPET): A simple method for music/voice\nseparation. IEEE Trans. Audio, Speech & Language\nProcessing , 21(2):73–84, 2013.\n[17] P. Sprechmann, A. Bronstein, and G. Sapiro. Real-time\nonline singing voice separation from monaural record-\nings using robust low-rank modeling. In Proc. ISMIR ,\npages 67–72, 2012.\n[18] J. Sundberg. The science of the singing voice . Northern\nIllinois University Press, 1987.\n[19] I. To ˇsi´c and P. Frossard. Dictionary learning. IEEE Sig-\nnal Processing Magazine , 28(2):27–38, 2011.\n[20] E. Vincent, R. Gribonval, and C. F `evotte. Perfor-\nmance measurement in blind audio source separation.\nIEEE Trans. Audio, Speech & Language Processing ,\n16(4):766–778, 2008.\n[21] T. Virtanen. Unsupervised learning methods for source\nseparation in monaural music signals. In A. Klapuri\nand M. Davy, editors, Signal Processing Methods for\nMusic Transcription , pages 267–296. Springer, 2006.\n[22] D. Wang and G. J. Brown. Computational Auditory\nScene Analysis: Principles, Algorithms, and Applica-\ntions . Wiley-IEEE Press, 2006.\n[23] Y .-H. Yang. On sparse and low-rank matrix decompo-\nsition for singing voice separation. In Proc. ACM Mul-\ntimedia , pages 757–760, 2012.\n[24] C.-C. M. Yeh and Y .-H. Yang. Supervised dictio-\nnary learning for music genre classiﬁcation. In Proc.\nACM Int. Conf. Multimedia Retrieval , pages 55:1–\n55:8, 2012."
    },
    {
        "title": "Beyond NMF: Time-Domain Audio Source Separation without Phase Reconstruction.",
        "author": [
            "Kazuyoshi Yoshii",
            "Ryota Tomioka",
            "Daichi Mochihashi",
            "Masataka Goto"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.1415060",
        "url": "https://doi.org/10.5281/zenodo.1415060",
        "ee": "https://zenodo.org/records/1415060/files/YoshiiTMG13.pdf",
        "abstract": "This paper presents a new fundamental technique for source separation of single-channel audio signals. Although nonnegative matrix factorization (NMF) has recently become very popular for music source separation, it deals only with the amplitude or power of the spectrogram of a given mixture signal and completely discards the phase. The component spectrograms are typically estimated using a Wiener filter that reuses the phase of the mixture spectrogram, but such rough phase reconstruction makes it hard to recover high-quality source signals because the estimated spectrograms are inconsistent, i.e., they do not correspond to any real time-domain signals. To avoid the frequency-domain phase reconstruction, we use positive semidefinite tensor factorization (PSDTF) for directly estimating source signals from the mixture signal in the time domain. Since PSDTF is a natural extension of NMF, an efficient multiplicative update algorithm for PSDTF can be derived. Experimental results show that PSDTF outperforms conventional NMF variants in terms of source separation quality.",
        "zenodo_id": 1415060,
        "dblp_key": "conf/ismir/YoshiiTMG13",
        "keywords": [
            "nonnegative matrix factorization",
            "music source separation",
            "amplitude or power",
            "phase",
            "Wiener filter",
            "frequency-domain phase reconstruction",
            "time domain",
            "positive semidefinite tensor factorization",
            "source signals",
            "experimental results"
        ],
        "content": "BEYOND NMF: TIME-DOMAIN AUDIO SOURCE SEPARATION\nWITHOUT PHASE RECONSTRUCTION\nKazuyoshi Yoshii1Ryota Tomioka2Daichi Mochihashi3Masataka Goto1\n1National Institute of Advanced Industrial Science and Technology (AIST)\n2The University of Tokyo3The Institute of Statistical Mathematics (ISM)\n{k.yoshii, m.goto }@aist.go.jp tomioka@mist.i.u-tokyo.ac.jp daichi@ism.ac.jp\nABSTRACT\nThis paper presents a new fundamental technique for source\nseparation of single-channel audio signals. Although non-\nnegative matrix factorization (NMF) has recently become\nvery popular for music source separation, it deals only with\nthe amplitude or power of the spectrogram of a given mix-\nture signal and completely discards the phase. The compo-\nnent spectrograms are typically estimated using a Wienerﬁlter that reuses the phase of the mixture spectrogram, but\nsuch rough phase reconstruction makes it hard to recover\nhigh-quality source signals because the estimated spectro-\ngrams are inconsistent, i.e., they do not correspond to any\nreal time-domain signals. To avoid the frequency-domain\nphase reconstruction, we use positive semideﬁnite tensor\nfactorization (PSDTF) for directly estimating source sig-\nnals from the mixture signal in the time domain. Since PS-\nDTF is a natural extension of NMF, an efﬁcient multiplica-\ntive update algorithm for PSDTF can be derived. Experi-\nmental results show that PSDTF outperforms conventional\nNMF variants in terms of source separation quality.\n1. INTRODUCTION\nSource separation of music audio signals is a fundamental\ntask for music information retrieval (MIR). High-quality\nsource separation could help users ﬁnd their favorite songsaccording to the content (such as vocals or instruments) [1].\nIt would also let them enjoy active music listening [2] based\non the remixing of existing instrumental parts [1–4].\nNonnegative matrix factorization (NMF) [5] has recently\nplayed a key role in the source separation of single-channel\naudio signals. It can approximate a nonnegative matrix (the\namplitude or power spectrogram of a given mixture signal)as the product of two nonnegative matrices— a set of basis\nspectra and a set of the corresponding activations. Then the\ncomplex spectrogram of the mixture signal is decomposed\ninto a sum of source spectrograms by using a Wiener ﬁlter\nthat simply reuses the original phase. However, we cannot\nrecover high-quality source signals from the decomposed\nspectrograms having the unreal phase.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted w ithout fee provided that copies are\nnot made or distributed for proﬁt or c ommercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2013 International Society for Music Information Retrieval.\n\u001b\u0001%JTDSFUF\u0001'PVSJFS\u0001USBOTGPSN\u0001\t%'5\n\u0001NBUSJY\n\u001b\u0001$PNQMFY\u0001TQFDUSVN\u0001PG\u0001GSBNF\u001b\u0001-PDBM\u0001TJHOBM\u0001PG\u0001GSBNF\n/.'\u0001GPDVTFT\u0001POMZ\u0001PO\u0001EJBHPOBM\u0001FMFNFOUT\n\tOPUJDFBCMF\u0001DPSSFMBUJPO\u0001TUSVDUVSFT\u0001BSF\u0001JHOPSFE\n.BUSJY\u0001EBUB\u001b\u0001B\u0001TFU\u0001PG\u0001QPXFS\u0001TQFDUSB\u0001\n\tB\u0001TFU\u0001PG\u0001OPOOFHBUJWF\u0001WFDUPST\n5FOTPS\u0001EBUB\u001b\u0001B\u0001TFU\u0001PG\u0001DPWBSJBODF\u0001NBUSJDFT\u0001\n\tB\u0001TFU\u0001PG\u0001QPTJUJWF\u0001TFNJEFpOJUF\u0001NBUSJDFT\n14%5'\n<E#>'BDUPSJ[F'BDUPSJ[F\n/.'\nFigure 1 . PSDTF is a natural extension of NMF.\nConsiderable effort has been devoted to estimating con-\nsistent complex spectrograms that correspond to real time-\ndomain signals. To reconstruct the phase of a given ampli-\ntude spectrogram, Grifﬁn and Lim [6] proposed an iterative\nshort-time Fourier transform (STFT) method that estimatesa time-domain signal such that its amplitude spectrogram\nis closest to the given spectrogram. Le Roux et al. [7] pro-\nposed a cost function that evaluates the inconsistency of\na complex spectrogram and derived an efﬁcient algorithm\nfor minimizing the cost function [8]. Kameoka et al. [9],\non the other hand, formulated complex NMF for directly\nfactorizing a complex spectrogram. The cost function eval-uating the inconsistency could be integrated into complex\nNMF as suggested in [10]. Note that improved consistency\ndoes not always result in improved sound quality.\nTo circumvent the phase reconstruction, we use positive\nsemideﬁnite tensor factorization (PSDTF) [11] for time-\ndomain separation of single-channel audio signals. Instead\nof explicitly considering the wave shapes and phases of ba-\nsis signals, we focus on the statistical characteristics ( e.g.,\nperiodicity and whiteness) of those signals. More speciﬁ-\ncally, we assume that each basis signal follows a Gaussian\nprocess (GP) having a stationary kernel. A given mixture\nsignal consisting of multiple basis signals is thus locallymodeled by a GP with a convex combination of the cor-\nresponding kernels. These kernels can be estimated from\na set of local covariances of the mixture signal by using a\nmultiplicative update algorithm.\nWe can show that the time-domain PSDTF has an equiv-\nalent frequency-domain representation used for factorizing\na mixture spectrogram like NMF. As shown in Figure 1,\nPSDTF deals with a set of Hermitian positive semideﬁnite\nmatrices (outer products of complex spectra) for consider-\ning the correlations between frequency components. This\nis reasonable because the discrete Fourier transform (DFT)cannot perfectly decorrelate the frequency components of\nthe mixture signal. On the other hand, NMF focus only on\na set of the nonnegative diagonal elements of those matri-\nces (power spectra) by discarding the correlations between\nfrequency components. This indicates that PSDTF is a nat-\nural and elegant extension of NMF.\n2. FREQUENCY-DOMAIN SOURCE SEPARATION\nThis section aims to reveal the theoretical basis underlying\nnonnegative matrix factorization (NMF) in the context of\nsource separation. We review two popular variants of NMFcalled KL-NMF [12] and IS-NMF [13] and how these vari-\nants can be used for source separation.\n2.1 Nonnegative Matrix Factorization\nThe goal of NMF is to approximate a nonnegative matrix\nX∈R\nM×Nas the product of two nonnegative matrices\nW∈RM×KandH∈RK×Nas follows:\nX≈WHdef=Y, (1)\nwhereWandHrespectively represent a set of basis vec-\ntors and a set of the corresponding weight vectors, K/lessmuch\nmin(M,N )is the number of basis vectors, and Y∈RM×N\nrepresents a reconstruction matrix. Eq. (1) can be rewritten\nin an element-wise manner as follows:\nXmn≈K/summationdisplay\nk=1WmkHkndef=Ymn. (2)\nWe here let Yk\nmn=WmkHknbe a component reconstruc-\ntion such that Ymn=/summationtext\nkYk\nmn. A popular way to evaluate\nthe reconstruction error C(Xmn|Ymn)betweenXmnand\nYmnis to use the Bregman divergence [14] deﬁned as\nCφ(Xmn|Ymn)\n=φ(Xmn)−φ(Ymn)−φ/prime(Ymn)(Xmn−Ymn),(3)\nwhereφis a strictly convex function. This divergence is\nno less than zero and is zero only when Xmn=Ymn.T h e\nKullback-Leibler (KL) divergence ( φ(x)=xlogx−x)\nand the Itakura-Saito (IS) divergence ( φ(x)=−logx)a r e\nwell-known special cases of the Bregman divergence. To\nestimateWandHsuch that the cost function Cφ(X|Y)=/summationtext\nmnCφ(Xmn|Ymn)is minimized, we can use an efﬁcient\nmultiplicative update (MU) algorithm [15].\n2.2 Application to Source Separation\nThe goal of source separation is to decompose a given mix-\nture signal into the sum of Ksource signals. NMF enables\nus to perform source separation in the frequency domain.\nWe regard the nonnegative spectrogram of the mixture sig-\nnal as an Xfor which Mis the number of frequency bins\nandNis the number of frames. We then factorize the given\nspectrogram XasX≈WH ,w h e r eWandHrespec-\ntively represent a set of basis nonnegative spectra and a set\nof the corresponding temporal activations.\nA probabilistic formulation of NMF enables us to esti-\nmate latent source signals. Let S∈CM×Nbe the com-plex spectrogram of the mixture signal and Sk∈CM×N\nbe that of the k-th source signal. If the mixture signal is an\ninstantaneous mixture of Ksource signals, we can say\nS=K/summationdisplay\nk=1Sk. (4)\nGiven the mixture spectrogram S(observed variable), each\ncomponent spectrogram Sk(latent variable) can be esti-\nmated in a probabilistic manner as follows:\nE[Sk\nmn|Smn]=Yk\nmn\nYmnSmn=WmkHkn\n/summationtext\nkWmkHknSmn.(5)\nEq. (5) is known as Wiener ﬁltering in which the original\nphase ofSis directly attached to each Sk. The real-valued\nsource signal can then be recovered from E[Sk|S]by us-\ning the overlap-add synthesis method [16]. Note that the\ncomplex spectrogram of the resulting source signal is un-\nl i k e l yt ob ee q u a lt o E[Sk|S]because in general E[Sk|S]\nis an inconsistent spectrogram that does not correspond to\nany actual time-domain signals.\n2.3 Source Separation based on KL-NMF\nKL-NMF is used for factorizing the amplitude spectrogram\n[12], i.e.,Xmn=|Smn|. The cost function is given by\nCKL(Xmn|Ymn)=XmnlogXmn\nYmn−Xmn+Ymn. Note that\nKL-NMF is not theoretically justiﬁed for source separation\nbecause the cost function is scale-variant, i.e.,CKL(X|Y)/negationslash=\nCKL(αX|αY)for a positive number α.\nThe probabilistic model of KL-NMF can be formulated\nby assuming that each latent component |Sk\nmn|is Poisson\ndistributed with a mean parameter Yk\nmnas follows:\n|Sk\nmn|/vextendsingle/vextendsingleYk\nmn∼Poisson (Yk\nmn). (6)\nWe here assume that the condition for amplitude additivity\nis satisﬁed, i.e., that the phase of each Skis equal to that of\nS. Eq. (4) can then be written as |Smn|=/summationtext\nk|Sk\nmn|.U s -\ningXmn=|Smn|andYmn=/summationtext\nkYk\nmn, the reproducing\nproperty of the Poisson distribution gives\nXmn/vextendsingle/vextendsingleYmn∼Poisson (Ymn). (7)\nThis probabilistic model based on superimposed Pois-\nson variables {|Sk\nmn|}K\nk=1satisfying |Smn|=/summationtext\nk|Sk\nmn|\nenables us to calculate the expectation of each latent vari-able|S\nk\nmn|in a principled manner as follows:\nE[|Sk\nmn|/vextendsingle/vextendsingle|Smn|]=Yk\nmnY−1\nmn|Smn|. (8)\nSince the phase is assumed to be preserved, we get Eq. (5).\n2.4 Source Separation based on IS-NMF\nIS-NMF is used for factorizing the power spectrogram [13],\ni.e.,Xmn=|Smn|2. The cost function is CIS(Xmn|Ymn)=\nXmn\nYmn−logXmn\nYmn−1.IS-NMF is suitable for source sepa-\nration because the cost function is scale-invariant.\nThe probabilistic model of IS-NMF can be formulated\nby assuming that each latent component Sk\nmnis complex\nGaussian distributed with a variance Yk\nmnas follows:\nSk\nmn|Yk\nmn∼Nc(0,Yk\nmn). (9)5JNF\n4JOF\u0001XBWF\u0001XJUI\u0001UJNF\u000eWBSZJOH\u0001TDBMF-PDBM\u0001TJHOBMT\u0001FYUSBDUFE\nCZ\u0001VTJOH\u0001B\u0001TIPSU\u0001XJOEPX\nFigure 2 . Local signals xknandxkn/primehave different wave\nshapes and phases, but share the same periodicity.\nUsingSmn=/summationtext\nkSk\nmnandYmn=/summationtext\nkYk\nmn, the repro-\nducing property of the Gaussian distribution gives\nSmn|Ymn∼Nc(0,Ymn). (10)\nUsingXmn=|Smn|2, we get an exponential distribution:\nXmn|Ymn∼Exponential (Ymn). (11)\nThe probabilistic model based on superimposed Gaus-\nsian variables {Sk\nmn}K\nk=1satisfying Smn=/summationtext\nkSk\nmnen-\nables us to represent a posterior distribution of latent vari-ableS\nk\nmnas a Gaussian distribution whose mean and vari-\nance are given by Eq. (5) and\nV[Sk\nmn|Smn]=Yk\nmn−Yk\nmnY−1\nmnYk\nmn. (12)\n3. TIME-DOMAIN SOURCE SEPARATION\nThis section recasts the problem of source separation in the\ntime domain. We propose a probabilistic model of source-\nsignal superimposition and show how latent source signals\ncan be estimated in a probabilistic manner.\n3.1 Problem Speciﬁcation\nThe goal of source separation is to decompose a given mix-\nture signal into the sum of Ksource signals. This decom-\nposition is performed on a frame-by-frame basis. Suppose\nwe have a set of NsamplesO=[x1,···,xN]∈RM×N,\nwherexn∈RMis a local signal in the n-th frame ex-\ntracted from the mixture signal by using a window of size\nM. Eachxncan be decomposed as follows:\nxn=K/summationdisplay\nk=1xkn, (13)\nwherexknis a local signal extracted from the k-th source\nsignal. This time-domain formulation is equivalent to the\nfrequency-domain formulation given by Eq. (4). Although{x\nkn}N\nn=1are different, we assume that {xkn}N\nn=1share\nsome characteristics. For example, suppose the k-th source\nsignal is a sine wave whose scale varies over time as shown\nin Figure 2. Note that xknandxkn/prime(n/negationslash=n/prime)h a v ed i f f e r -\nentscales but have the same period . We factorize xkninto\nnonstationary and stationary factors as xkn=πknφkn,\nwhereπknis a coefﬁcient (scale) of a local signal φknex-\ntracted from the k-th stationary basis signal. Note that the\nbasis signal is assumed to vary over time according to sta-\ntionary characteristics ( e.g., periodicity and whiteness).\nGivenOas observed data, we aim to estimate a set of\nlatent signals {xkn}N\nn=1for eachk.T h ek-th source signal\ncan be obtained by the overlap-add synthesis method [16].\nWe do not need any frequency analysis such as short-term\nFourier transform (STFT) or inverse STFT.3.2 Probabilistic Formulation\nWe formulate a probabilistic model of Eq. (13). A key fea-\nture is to focus on the stationary characteristics of the basis\nsignal. Since the stationarity means that {φkn}N\nn=1are ex-\npected to have the same covariance, we put a multivariate\nGaussian prior shared over all frames as follows:\nφkn∼N (0,Vk), (14)\nwhereVk∈RM×Mis afullcovariance matrix. The mean\nis set to a zero vector because an audio signal is recorded\nas real numbers distributed on both sides of zero.\nWe can say that the k-th basis signal is Gaussian process\n(GP) distributed with a stationary (shift-invariant) kernel\nVk. Since in reality the signal exists over continuous time,\nit is essential to consider a probability distribution of the\ncontinuous signal. Such a distribution is a GP by deﬁnition\nbecause its marginal over any Mdiscrete time points is a\nGaussian distribution, as indicated by Eq. (14). If Vkis a\nperiodic kernel, {φkn}N\nn=1are expected to have a certain\nperiod but their phases and wave shapes can be different.\nWe will derive a likelihood of the observed signal xn.\nThe linear relationship xkn=πknφknand Eq. (14) lead\nto a likelihood of xknas follows:\nxkn|π,V∼N (0,π2\nknVk). (15)\nThen, using the reproducing property of the Gaussian dis-\ntribution and the linear relationship given by Eq. (13), we\nget the likelihood of xnas follows:\nxn|π,V∼N/parenleftBigg\n0,K/summationdisplay\nk=1π2\nknVk/parenrightBigg\n. (16)\nNote that Eq. (16) does not include φkn,i.e., all possibili-\nties ofφknare taken into account. This formulation frees\nus from explicitly considering the phase of φkn.W e h e r e\ndeﬁne some symbols as Hkn=π2\nkn≥0,Xn=xnxT\nn/followsequal\n0,a n dYn=/summationtext\nkHknVk/followsequal0,w h e r eΨ/followsequal0means that\nΨis a positive semideﬁnite (PSD) matrix. Then Eq. (16)\ngives the log-likelihood of Xnas follows:\nlogp(Xn|Yn)c=−1\n2log|Yn|−1\n2tr(XnY−1\nn),(17)\nwherec=represents equality except for the constant terms.\nGiven a tensor X=[X1,···,XN]∈RM×M×N,w e\naim to estimate H∈RK×NandV=[V1,···,VK]∈\nRM×M×Ksuch that the log-likelihood/summationtext\nnlogp(Xn|Yn)\nis maximized. As shown in Section 4, this is a special case\nof PSDTF in which each Xnis restricted to a rank-1 PSD\nmatrix (Xn=xnxT\nn). We can therefore use a multiplica-\ntive update algorithm described in Section 4.3.\n3.3 Probabilistic Decomposition\nAfterHandVare obtained, we can estimate a local signal\nxkn=πknφknin a probabilistic manner. Instead of esti-\nmatingφkn, we can directly calculate a Gaussian posterior\nofxknwhose mean and covariance are given by\nE[xkn|xn,H,V]=YnkY−1\nnxn, (18)\nV[xkn|xn,H,V]=Ynk−YnkY−1\nnYnk, (19)whereYnk=HknVk/followsequal0such that Yn=/summationtext\nkYnk/followsequal0.\nEq. (18) works as a Wiener ﬁlter that passes only a com-\nponent signal of xnmatching the characteristics of ker-\nnelVkwithout explicitly considering the phase and wave\nshape. Eqs. (18) and (19) formulated in the time domain\nlook similar in form to Eqs. (5) and (12) formulated in the\nfrequency domain. The key difference is that we consider\nthe covariance structure over frequency components ( e.g.,\nharmonic partials) when decomposing xn.\n3.4 Frequency-Domain Representation\nWe discuss a frequency-domain representation (Figure 1).\nLetF∈CM×Mbe the DFT matrix. Eq. (16) means that\nthe complex spectrum Fxn(linear transformation of xn)\nis complex-Gaussian distributed as follows:\nFxn|H,V∼Nc/parenleftBigg\n0,K/summationdisplay\nk=1HknFVkFH/parenrightBigg\n,(20)\nwhere the full covariance structure between frequency bins\nis considered. Note that FVkFHbecomes a diagonal ma-\ntrix ifVkis a circulant matrix. A trivial example is a case\nthatVkis an identity matrix, i.e.,φknis stationary white\nGaussian noise. If Vkis a periodic kernel and its size M\nis much larger than its period, Vkcan be roughly viewed\nas a circulant matrix. If FVkFHis diagonal, Eq. (20) re-\nduces to a probabilistic model of IS-NMF discarding the\ncovariance structure between frequency bins [13]. In real-\nity, however, Vkis considerably different from a circulant\nmatrix as shown in Section 5 (Figure 4 and Figure 5).\n4. POSITIVE SEMIDEFINITE TENSOR\nFACTORIZATION\nThis section explains a new tensor factorization technique\ncalled positive semideﬁnite tensor factorization (PSDTF),\nin a general-purpose way. As NMF approximates Nnon-\nnegative vectors (a matrix) as the convex combinations of\nKnonnegative vectors, PSDTF approximates NPSD ma-\ntrices (a tensor) as the convex combinations of KPSD ma-\ntrices. PSDTF is therefore a natural extension of NMF.\n4.1 Problem Speciﬁcation\nWe formalize the problem of PSDTF. Suppose we have as\nobserved data a three-mode tensor X=[X1,···,XN]∈\nRM×M×N, where each slice Xn∈RM×Mis a real sym-\nmetric positive semideﬁnite (PSD) matrix. Note that a sim-\nilar discussion can be applied even if Xn∈CM×Mis a\ncomplex Hermitian PSD matrix such that Xn=XH\nn.\nThe goal of PSDTF is to approximate each PSD matrix\nXnby a convex combination of PSD matrices {Vk}K\nk=1\n(Kbasis matrices) as follows:\nXn≈K/summationdisplay\nk=1HknVkdef=Yn, (21)\nwhereHkn≥0is a weight at the n-th slice. Eq. (21) can\nalso be represented as X≈/summationtext\nkhk⊗Vkdef=Y,where⊗\nindicates the Kronecker product.To evaluate the reconstruction error between PSD ma-\ntricesXnandYn, we propose to use a Bregman matrix\ndivergence [14] deﬁned as follows:\nCφ(Xn|Yn)\n=φ(Xn)−φ(Yn)−tr/parenleftbig\n∇φ(Yn)T(Xn−Yn)/parenrightbig\n,(22)\nwhereφis a strictly convex matrix function. In this paper\nwe focus on the log-determinant (LD) divergence ( φ(Z)=\n−log|Z|) [17] given by\nCLD(Xn|Yn)=−log/vextendsingle/vextendsingleXnY−1\nn/vextendsingle/vextendsingle+tr/parenleftbig\nXnY−1\nn/parenrightbig\n−M.(23)\nThis divergence is always nonnegative and is zero if and\nonly ifXn=Yn. The Itakura-Saito (IS) divergence over\nnonnegative numbers given by CIS(x|y)=−log(x/y)+\nx/y−1is a well-known special case when M=1,a n di t\nis often used for audio source separation.\nOur goal is to estimate H=[h1,···,hK]∈RN×K\nandV=[V1,···,VK]∈RM×M×Ksuch that the cost\nfunctionCLD(X|Y)=/summationtext\nnCLD(Xn|Yn)is minimized.\nNote that our model imposes the nonnegativity constraint\nonHand the positive semideﬁniteness constraint on V.\nThis speciﬁc model is called LD-PSDTF.\n4.2 Auxiliary Function Approach\nWe use the auxiliary function approach [15] for indirectly\nmaximizing the cost function CLD(X|Y)with respect to Y\n(i.e.,HandV) because of its analytical tractability. Let\nF(θ)is an objective function to be minimized with respect\nto a parameter θ. A function F+(θ,φ)satisfying\nF(θ)≤F+(θ,φ) (24)\nis an auxiliary function for F(θ),w h e r eφis an auxiliary\nparameter. It can be proved that F(θ)is non-increasing\nthrough the following iterative update rules:\nφnew←argminφF+(θold,φ), (25)\nθnew←argminθF+(θ,φnew). (26)\nThis algorithm is theoretically guaranteed to converge and\nin fact a similar idea was used for IS-NMF [18].\nTo derive an auxiliary function U(X|Y)forCLD(X|Y),\nwe need to use matrix-variate inequalities based on func-\ntion concavity and convexity. First, since f(Z) = log|Z|\nis concave, we calculate a tangent plane of f(Z)by using\na ﬁrst-order Taylor expansion as follows:\nlog|Z|≤log|Ω|+tr(Ω−1Z)−M, (27)\nwhereΩis an auxiliary PSD matrix (tangent point), Mis\nthe size of Z, and the equality holds when Ω=Z.F o r\na convex function g(Z)= tr(Z−1A)for any PSD matrix\nA, we can use a sophisticated inequality [19] as follows:\ntr/parenleftbigg/parenleftBig/summationtextK\nk=1Zk/parenrightBig−1\nA/parenrightbigg\n≤K/summationdisplay\nk=1tr/parenleftBig\nZ−1\nkΦkAΦT\nk/parenrightBig\n,(28)\nwhere{Zk}K\nk=1is a set of arbitrary PSD matrices, {Φk}K\nk=1\nis a set of auxiliary matrices that sum to the identity ma-\ntrix ( i.e.,/summationtext\nkΦk=I), and the equality holds when Φk=\nZk(/summationtext\nk/primeZk/prime)−1.Using Inequalities (27) and (28), we can derive an aux-\niliary function U(Xn|Yn)for Eq. (23) as follows:\nCLD(Xn|Yn)c=l o g|Yn|+tr/parenleftbig\nXnY−1\nn/parenrightbig\n≤log|Ωn|+tr/parenleftbig\nYnΩ−1\nn/parenrightbig\n−M\n+/summationtext\nktr/parenleftBig\nY−1\nnkΦnkXnΦT\nnk/parenrightBig\n(29)\n≤log|Ωn|+/summationtext\nktr/parenleftbig\nHknVkΩ−1\nn/parenrightbig\n−M\n+/summationtext\nktr/parenleftBig\nH−1\nknV−1\nkΦnkXnΦT\nnk/parenrightBig\ndef=U(Xn|Yn),\nwhereΩnis a PSD matrix and {Φnk}K\nk=1is a set of auxil-\niary matrices that sum to the identity matrix. The equality\nholds, i.e.,U(Xn|Yn)is minimized, when\nΩn=Yn,Φnk=YnkY−1\nn. (30)\n4.3 Multiplicative Update\nWe can derive multiplicative update (MU) rules that mono-\ntonically decrease the total auxiliary function U(X|Y)=/summationtext\nnU(Xn|Yn).W eh e r ea s s u m et r (Vk)=1 (unit trace)\nto remove the scale arbitrariness. If tr (Vk)=s, the scale\nadjustments Vk←1\nsVkandHkn←sHkndo not change\nCLD(Xn|Yn)andU(Xn|Yn). Letting the partial deriva-\ntive of Eq. (29) with respect to Hknbe equal to be zero and\nusing Eq. (30), we get the following update rule:\nHkn←Hkn/radicalBigg\ntr/parenleftbig\nY−1\nnVkY−1\nnXn/parenrightbig\ntr/parenleftbig\nY−1\nnVk/parenrightbig. (31)\nThen, letting the partial derivative with respect to Vkbe\nequal to be zero and using Eq. (30), we get\nVkPkVk=Vold\nkQkVold\nk, (32)\nwherePkandQkare PSD matrices given by\nPk=N/summationdisplay\nn=1HknY−1\nn,Qk=N/summationdisplay\nn=1HknY−1\nnXnY−1\nn.(33)\nEq. (32) can be solved analytically by using the Cholesky\ndecomposition Qk=LkLT\nk,w h e r eLkis a lower triangu-\nlar matrix. Finally, we get the following update rule:\nVk←VkLk(LT\nkVkPkVkLk)−1\n2LTkVk,(34)\nwhere the positive semideﬁniteness of Vkis ensured. Note\nthat a real matrix Ais said to be positive semideﬁnite if and\nonly ifA=ZZTis satisﬁed for some real matrix Z.\n4.4 Connection to IS-NMF and Source Separation\nLD-PSDTF reduces to IS-NMF if PSD matrices Xnand\nVkare restricted to diagonal matrices. Since the diagonal\nelements of an arbitrary PSD matrix are always nonnega-\ntive, the cost function given by Eq. (23) is decomposed as\nCLD(Xn|Yn)=/summationtext\nmCIS(Xnmm|Ynmm )and the MU rules\ngiven by Eq. (31) and Eq. (34) thus reduce to the MU rules\nof IS-NMF [15]. As described in [15], several algorithms\nsuch as an expectation-maximization (EM) algorithm and\na convergence-guaranteed MU algorithm can be used for\nIS-NMF. The same is true for LD-PSDTF, and for faster\nconvergence we derived the MU algorithm.\n,-\u000e/.'\n,-\u000e/.'\u0001XJUI\u0001JUFS45'5\n*4\u000e/.'\n*4\u000e/.'\u0001XJUI\u0001JUFS45'5\n-%\u000e14%5'\u0001\tQSPQPTFE\n-%\u000e14%5'\u0001BDIJFWFE\u0001UIF\u0001\nTJHOJGJDBOU\u0001JNQSPWFNFOUT\nPWFS\u0001,-\u000e/.'\u0001BOE\u0001*4\u000e/.'\u000f\n5IF\u0001JUFSBUJWF\u000145'5\u0001NFUIPE\nEFHSBEFE\u0001UIF\u0001RVBMJUZ\u000f\n4\"3 4%3 4*3\u0014\u0011\n\u0013\u0016\n\u0013\u0011\n\u0012\u0016\n\u0012\u0011<E#>\n\u0012\u000f\u0015\u0001E#\u0014\u000f\u001a\u0001E#\u0012\u000f\u0019\u0001E#\u0014\u000f\u0018\u0001E#\n\u0015\u000f\u0012\u0001E#\n\u0012\u000f\u0014\u0001E#\nFigure 3 . Source separation performance.\nTo use LD-PSDTF for source separation, we set Xn=\nxnxT\nn(rank-1 matrix) as shown in Section 3.2. Since the\nnegative of the log-likelihood given by Eq. (17) is equal\nto the cost function given by Eq. (23) except for constant\nterms, the maximum-likelihood estimates of HandVcan\nbe obtained by the MU algorithm for LD-PSDTF. Note that\ngeneral LD-PSDTF is formulated for any-rank matrixXn.\n5. EV ALUATION\nThis section reports a comparative experiment evaluating\nthe source separation performance of LD-PSDTF.\n5.1 Experimental Conditions\nWe used three mixture audio signals each of which was\nsynthesized using piano sounds (011PFNOM), electric gui-\ntar sounds (131EGLPM), or clarinet sounds (311CLNOM)\nrecorded in the RWC Music Database: Musical Instrument\nSound [20]. Each mixture signal was made by concatenat-ing seven 2.0-s isolated or mixture sounds (C4, E4, G4,\nC4+E4, C4+G4, E4+G4, and C4+E4+G4). The resulting\n14.0-s signals were sampled at 16kHz.\nThe task was to separate each mixture signal into three\nsource signals corresponding to C4, E4, and G4. The local\nsignals{x\nn}N\nn=1were extracted by using a Gaussian win-\ndow with a width of 512 samples ( M= 512 ) and a shifting\ninterval of 160 samples ( N= 1400 ). The PSD matrices V\nand their activations Hwere estimated by using the MU\nalgorithm with K=3 . For comparison, we used KL-\nNMF for amplitude-spectrogram decomposition and IS-\nNMF for power-spectrogram decomposition (Section 2.3and Section 2.4). The number of iterations was 100 in each\nmethod. We also tested the iterative STFT method [6] as\na phase reconstructor for NMF. We evaluated the quality\nof separated signals in terms of source-to-distortion ratio\n(SDR), source-to-interferences ratio (SIR), and sources-to-\nartifacts ratio (SAR) using the BSS Eval toolbox [21].\n5.2 Experimental Results\nThe experimental results showed the clear superiority of\nLD-PSDTF for source separation (Figure 3). The average\nSDR, SIR, and SAR were 17.7 dB, 22.2 dB, and 19.7 dB in\nKL-NMF, 19.1 dB, 24.0 dB, and 21.0 dB in IS-NMF, and23.0 dB, 27.7 dB, and 25.1 dB in LD-PSDTF.\n1We found\nthat the iterative STFT method degraded the quality of sep-\narated signals. This implies that the spectrogram consis-\ntency does not always lead to the perceived quality of au-\n1Audio ﬁles and a sample code are at the ﬁrst author’s website.\u0012\u0013\u0019\u0013\u0016\u0017\u0014\u0019\u0015\u0016\u0012\u0013\u0012\u0013\u0019\n\u0013\u0016\u0017\n\u0014\u0019\u0015\n\u0016\u0012\u0013\n\u0012\u0013\u0019\u0013\u0016\u0017\u0014\u0019\u0015\u0016\u0012\u0013\u0012\u0013\u0019\n\u0013\u0016\u0017\n\u0014\u0019\u0015\n\u0016\u0012\u0013\n\u0012\u0013\u0019\u0013\u0016\u0017\u0014\u0019\u0015\u0016\u0012\u0013\u0012\u0013\u0019\n\u0013\u0016\u0017\n\u0014\u0019\u0015\n\u0016\u0012\u0013\n\u0011\u0013\u0015\u0017\u0019\u0012\u0011\u0012\u0013\u0012\u0015\u0011\u0011\u000f\u0012\u0011\n\u0011\u0011\u000f\u0012\u0015\n\u0011\u0011\u000f\u0014\u0016\nY\u0012\u0011\u0013\u0011 \u0013 \u0015 \u0017 \u0019 \u0012\u0011 \u0012\u0013 \u0012\u0015 \u0011 \u0013 \u0015 \u0017 \u0019 \u0012\u0011 \u0012\u0013 \u0012\u0015#BTJT\u0001NBUSJY #BTJT\u0001NBUSJY #BTJT\u0001NBUSJY\n\"DUJWBUJPO\u0001WFDUPS \"DUJWBUJPO\u0001WFDUPS \"DUJWBUJPO\u0001WFDUPS\n0CTFSWFE\u0001NBUSJY 0CTFSWFE\u0001NBUSJY 0CTFSWFE\u0001NBUSJY 0CTFSWFE\u0001NBUSJY\u0011 \u0013 \u0015 \u0017 \u0019 \u0012\u0011 \u0012\u0013 \u0012\u0015 <T>\n%JTDPWFS\u0001CBTJT\u0001NBUSJDFT\u0001TVDI\u0001UIBU\u0001UIF\u0001MPH\u000eEFUFSNJOBOU\u0001EJWFSHFOD F\u0001JT\u0001NJOJNJ[FE0CTFSWFE\u0001NJYVSF\u0001TJHOBM\n1FSJPE\u0001PG\u0001$\u00151FSJPE\u0001PG\u0001$\u0015 1FSJPE\u0001PG\u0001$\u0015 1FSJPE\u0001PG\u0001&\u00151FSJPE\u0001PG\u0001&\u0015 1FSJPE\u0001PG\u0001&\u0015 1FSJPE\u0001PG\u0001(\u00151FSJPE\u0001PG\u0001(\u0015 1FSJPE\u0001PG\u0001(\u0015\nFigure 4 . Factorization of a piano mixture signal.\ndio signals, as suggested in [10]. We conﬁrmed that LD-\nPSDTF can appropriately estimate VandHfrom both de-\ncaying and sustained sounds (Figure 4 and Figure 5). The\nreason that each Xndoes not appear to be well approxi-\nmated by Yn(a convex combination of {Vk}K\nk=1)i st h a t\nthe cost function based on the LD divergence allows Ynto\noverestimate Xnwith a smaller penalty. A main limitation\nof LD-PSDTF is that its computational cost is O(KNM3)\nwhile the computational cost of NMF is O(KNM ).I nt h i s\nexperiment, LD-PSDTF spent several hours for analyzing\neach mixture signal on Xeon X5492 (3.4 GHz). Therefore,\nwe think that initializing LD-PSDTF by using basis vec-\ntors and their activations obtained by IS-NMF can reducethe computational cost and help avoid local minima.\n6. CONCLUSION\nThis paper presented log-determinant positive semideﬁnite\ntensor factorization (LD-PSDTF) as a natural extension ofItakura-Saito NMF (IS-NMF). We derived a convergence-\nguaranteed multiplicative update algorithm and showed the\nclear superiority of LD-PSDTF over NMF variants in terms\nof source separation quality.\nThere are several interesting directions. To separate mu-\nsic signals into instrument parts, we plan to fuse the source-\nﬁlter model into the framework of LD-PSDTF as in the\ncomposite autoregressive system [22]. We also plan to in-\nvestigate another variant of PSDTF based on the von Neu-\nmann divergence ( φ(Z)= tr(ZlogZ−Z)in Eq. (22))\nthat can be viewed as an extension of KL-NMF.\nAcknowledgment: This study was supported in part by JSPS\nKAKENHI 23700184, MEXT KAKENHI 25870192, and JST\nCREST OngaCREST.\n7. REFERENCES\n[1] K. Itoyama et al. Query-by-example music information re-\ntrieval by score-informed source separation and remixing\ntechnologies. EURASIP Journal , 2010. Article ID 172961.\u0011\u000f\u0011\u0017\u0011\u000f\u0011\u0017\u0011\u000f\u0011\u0019\n\u0012\u0013\u0019 \u0013\u0016\u0017 \u0014\u0019\u0015 \u0016\u0012\u0013\u0012\u0013\u0019\n\u0013\u0016\u0017\n\u0014\u0019\u0015\n\u0016\u0012\u0013\u0012\u0013\u0019 \u0013\u0016\u0017 \u0014\u0019\u0015 \u0016\u0012\u0013\u0012\u0013\u0019\n\u0013\u0016\u0017\n\u0014\u0019\u0015\n\u0016\u0012\u0013\u0012\u0013\u0019 \u0013\u0016\u0017 \u0014\u0019\u0015 \u0016\u0012\u0013\u0012\u0013\u0019\n\u0013\u0016\u0017\n\u0014\u0019\u0015\n\u0016\u0012\u0013\n\u0011\u0013\u0015\u0017\u0019 \u0012 \u0011 \u0012 \u0013 \u0012 \u0015\u0011\u0011\u0011Y\u0012\u0011\u0013\u0011 \u0013 \u0015 \u0017 \u0019 \u0012\u0011 \u0012\u0013 \u0012\u0015 \u0011 \u0013 \u0015 \u0017 \u0019 \u0012\u0011 \u0012\u0013 \u0012\u0015\"DUJWBUJPO\u0001WFDUPS \"DUJWBUJPO\u0001WFDUPS \"DUJWBUJPO\u0001WFDUPS#BTJT\u0001NBUSJY #BTJT\u0001NBUSJY #BTJT\u0001NBUSJY\nFigure 5 . Factorization of a clarinet mixture signal.\n[2] M. Goto. Active music listening interfaces based on signal\nprocessing. ICASSP , volume 4, pp. 1441–1444, 2007.\n[3] K. Yoshii et al. Drumix: An audio player with real-time\ndrum-part rearrangement func tions for active music listen-\ning. IPSJ Digital Courier , 3:134–144, 2007.\n[4] N. Sturmel et al. Linear mixing models for active listening\nof music productions in realistic studio conditions. AES Con-\nvention , 2012.\n[5] D. Lee and H. Seung. Algorithms for non-negative matrix\nfactorization. NIPS , pp. 556–562, 2000.\n[6] D. W. Grifﬁn and J. S. Lim. Signal estimation from mod-\niﬁed short-time Fourier transform. IEEE Trans. on ASLP ,\n32(2):236–243, 1984.\n[7] J. Le Roux, H. Kameoka, N. Ono, and S. Sagayama. Explicit\nconsistency constraints for STFT spectrograms and their ap-\nplication to phase reconstruction. SAPA , pp. 23–28, 2008.\n[8] J. Le Roux, H. Kameoka, N. Ono, and S. Sagayama. Fast sig-\nnal reconstruction from magn itude STFT spectrogram based\non spectrogram consistency. DAFx , pp. 397–403, 2010.\n[9] H. Kameoka et al. Complex NMF: A new sparse representa-\ntion for acoustic signals. ICASSP , pp. 45–48, 2009.\n[10] J. Le Roux et al. Consistent Wiener ﬁltering: Generalized\ntime-frequency masking respectin g spectrogram consistency.\nLVA/ICA , pp. 89–96, 2010.\n[11] K. Yoshii, R. Romioka, D. Mochihashi, and M. Goto. Inﬁnite\npositive semideﬁnite tensor fact orization for source separa-\ntion of mixture signals. ICML , pp. 576–584, 2013.\n[12] P. Smaragdis and J. Brown. Nonnegative matrix factorization\nfor polyphonic music transcription. WASPAA , 2003.\n[13] C. F ´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix\nfactorization with the Itakura-Saito divergence: With appli-cation to music analysis. Neu. Comp. , 21(3):793–830, 2009.\n[14] L. M. Bregman. The relaxation method of ﬁnding the com-\nmon points of convex sets and its application to the solution\nof problems in convex programming. USSR CMMP , 1967.\n[15] M. Nakano et al. Convergence-guaranteed multiplicative al-\ngorithms for non-negative matrix factorization with beta di-\nvergence. MLSP , pp. 283–288, 2010.\n[16] J. Allen and L. Rabiner. A uniﬁed approach to short-time\nFourier analysis and synthesis. IEEE , 1977.\n[17] B. Kulis, M. Sustik, and I. Dhillon. Low-rank kernel learning\nwith Bregman matrix divergences. JMLR , 10:341–376, 2009.\n[18] M. Hoffman et al. Bayesian nonparametric matrix factoriza-\ntion for recorded music. ICML , pp. 439–446, 2010.\n[19] H. Sawada, H. Kameoka, S. Araki, and N. Ueda. Efﬁcient\nalgorithms for multichannel extensions of Itakura-Saito non-negative matrix factorization. ICASSP , pp. 261–264, 2012.\n[20] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. RWC\nmusic database: Music genre database and musical instru-ment sound database. ISMIR\n, pp. 229–230, 2003.\n[21] E. Vincent, R. Gribonval, and C. F ´evotte. Performance mea-\nsurement in blind audio source separation. IEEE Trans. on\nASSP , 14(4):1462–1469, 2006.\n[22] H. Kameoka and K. Kashino. Composite autoregressive sys-\ntem for sparse source-ﬁlter representation of speech. ISCAS ,\npp. 2477–2480, 2009."
    },
    {
        "title": "Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013, Curitiba, Brazil, November 4-8, 2013",
        "author": [
            "Alceu de Souza Britto Jr.",
            "Fabien Gouyon",
            "Simon Dixon"
        ],
        "year": "2013",
        "doi": "10.5281/zenodo.7388670",
        "url": "https://doi.org/10.5281/zenodo.7388670",
        "ee": "https://zenodo.org/records/7388670/files/Indian Carnatic Classical Music.xlsx",
        "abstract": "This dataset contains the multitask annotation for the Ragas in the Carnatic style of Indian classical music. The multitasks present in the datasetare the Swaras set, Melakarta set, Aaroh set,Avroh set,Janak/janya, and Raag Id. The music for extracting the mel-spectrogram feature is taken from the Dunya corpus [1]. The dataset contains 40 Ragas, and each contains 12 music recordings.\n\n[1]Porter, Alastair, Mohamed Sordo, and Xavier Serra. Dunya: A system for browsing audio music collections exploiting cultural context. Britto A, Gouyon F, Dixon S. 14th International Society for Music Information Retrieval Conference (ISMIR); 2013 Nov 4-8; Curitiba, Brazil.[place unknown]: ISMIR; 2013. p. 101-6.. International Society for Music Information Retrieval (ISMIR), 2013.\n\n\n\n_________________________________________________________________________________________________________\nThis project was funded under the grant number: ECR/2018/000204 by the Science  Engineering Research Board (SERB).",
        "zenodo_id": 7388670,
        "dblp_key": "conf/ismir/2013",
        "keywords": [
            "Indian classical music",
            "Ragas",
            "Carnatic style",
            "Swaras set",
            "Melakarta set",
            "Aaroh set",
            "Avroh set",
            "Janak/janya",
            "Raag Id",
            "Dunya corpus"
        ]
    }
]