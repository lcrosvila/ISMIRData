[
    {
        "title": "Analyzing User Interactions with Music Information Retrieval System: An Eye-tracking Approach.",
        "author": [
            "Xiao Hu 0001",
            "Ying Que",
            "Noriko Kando",
            "Wenwei Lian"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527832",
        "url": "https://doi.org/10.5281/zenodo.3527832",
        "ee": "http://archives.ismir.net/ismir2019/paper/000049.pdf",
        "abstract": "There has been little research considering eye movement as a measure when assessing user interactions with music information retrieval (MIR) systems, whereas many studies have adopted conventional user-centered measures such as user effectiveness and user perception. To bridge this research gap, this study investigates users' eye movement patterns and measures with two music retrieval tasks and two interface presentation modes. A user experiment was conducted with 16 participants whose eye movement and mouse click behaviors were recorded through professional eye trackers. Through analyzing visual patterns of eye gazes and movements as well as various metrics in prominent Areas of Interest (AOI), it is found that users' eye movement behaviors were related to task type. Besides, the results also disclosed that some eye movement metrics were related to both user effective-ness and user perception, and influenced by user characteristics. It is also found that some eye movement and user effectiveness metrics can be used to predict user perception. This study allows researchers to gain a deeper insight into user interactions with MIR systems from the perspective of eye movement measure.",
        "zenodo_id": 3527832,
        "dblp_key": "conf/ismir/0001QKL19",
        "keywords": [
            "eye movement",
            "music information retrieval",
            "user interactions",
            "conventional user-centered measures",
            "research gap",
            "user experiment",
            "professional eye trackers",
            "visual patterns",
            "user effectiveness",
            "user perception"
        ]
    },
    {
        "title": "Deep-Rhythm for Global Tempo Estimation in Music.",
        "author": [
            "Hadrien Foroughmand Aarabi",
            "Geoffroy Peeters"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527890",
        "url": "https://doi.org/10.5281/zenodo.3527890",
        "ee": "http://archives.ismir.net/ismir2019/paper/000077.pdf",
        "abstract": "It has been shown that the harmonic series at the tempo frequency of the onset-strength-function of an audio signal accurately describes its rhythm pattern and can be used to perform tempo or rhythm pattern estimation. Recently, in the case of multi-pitch estimation, the depth of the input layer of a convolutional network has been used to represent the harmonic series of pitch candidates. We use a similar idea here to represent the harmonic series of tempo candidates. We propose the Harmonic-Constant-Q-Modulation which represents, using a 4D-tensors, the harmonic series of  modulation frequencies (considered as tempo frequencies) in several acoustic frequency bands over time. This representation is used as input to a convolutional network which is trained to estimate tempo or rhythm pattern classes. Using a large number of datasets, we evaluate the performance of our approach and compare it with previous approaches.  We show that it slightly increases Accuracy-1 for tempo estimation but not the average-mean-Recall for rhythm pattern recognition.",
        "zenodo_id": 3527890,
        "dblp_key": "conf/ismir/AarabiP19",
        "keywords": [
            "harmonic series",
            "tempo frequency",
            "audio signal",
            "rhythm pattern",
            "tempo estimation",
            "convolutional network",
            "multi-pitch estimation",
            "representation",
            "tempo candidates",
            "tempo modulation"
        ]
    },
    {
        "title": "Controlling Symbolic Music Generation based on Concept Learning from Domain Knowledge.",
        "author": [
            "Taketo Akama"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527936",
        "url": "https://doi.org/10.5281/zenodo.3527936",
        "ee": "http://archives.ismir.net/ismir2019/paper/000100.pdf",
        "abstract": "Machine learning allows automatic construction of generative models for music. However, they are learned from only the succession of notes itself without explicitly employing domain knowledge of musical concepts such as rhythm, contour, and fragmentation  consolidation. We approximate such musical domain knowledge as a function, and feed it into our model. Then, two decoupled spaces are learned: the extraction space that captures the target concept, and the residual space that captures the remainder. For monophonic symbolic music, our model exhibits high decoupling/modeling performance.  Controllability in generation is improved: (i) our interpolation enables concept-aware flexible control over blending two musical fragments, and (ii) our variation generation enables users to make concept-aware adjustable variations.",
        "zenodo_id": 3527936,
        "dblp_key": "conf/ismir/Akama19",
        "keywords": [
            "Machine learning",
            "Automatic construction",
            "Generative models",
            "Rhythm",
            "Contour",
            "Fragmentation consolidation",
            "Musical domain knowledge",
            "Function approximation",
            "Two decoupled spaces",
            "Monophonic symbolic music"
        ]
    },
    {
        "title": "Learning Soft-Attention Models for Tempo-invariant Audio-Sheet Music Retrieval.",
        "author": [
            "Stefan Balke",
            "Matthias Dorfer",
            "Lu\u00eds Carvalho",
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527782",
        "url": "https://doi.org/10.5281/zenodo.3527782",
        "ee": "http://archives.ismir.net/ismir2019/paper/000024.pdf",
        "abstract": "Connecting large libraries of digitized audio recordings to their corresponding sheet music images has long been a motivation for researchers to develop new cross-modal retrieval systems. In recent years, retrieval systems based on embedding space learning with deep neural networks got a step closer to fulfilling this vision. However, global and local tempo deviations in the music recordings still require careful tuning of the amount of temporal context given to the system. In this paper, we address this problem by introducing an additional soft-attention mechanism on the audio input. Quantitative and qualitative results on synthesized piano data indicate that this attention increases the robustness of the retrieval system by focusing on different parts of the input representation based on the tempo of the audio. Encouraged by these results, we argue for the potential of attention models as a very general tool for many MIR tasks.",
        "zenodo_id": 3527782,
        "dblp_key": "conf/ismir/BalkeDCAW19",
        "keywords": [
            "cross-modal retrieval systems",
            "embedding space learning",
            "temporal context tuning",
            "attention mechanism",
            "audio input",
            "tempo of the audio",
            "MIR tasks",
            "robustness of the retrieval system",
            "quantitative and qualitative results",
            "synthesized piano data"
        ]
    },
    {
        "title": "Augmenting Music Listening Experiences on Voice Assistants.",
        "author": [
            "Morteza Behrooz",
            "Sarah Mennicken",
            "Jennifer Thom",
            "Rohit Kumar",
            "Henriette Cramer"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527804",
        "url": "https://doi.org/10.5281/zenodo.3527804",
        "ee": "http://archives.ismir.net/ismir2019/paper/000035.pdf",
        "abstract": "Voice interfaces have rapidly gained popularity, introducing the opportunity for new ways to explore new interaction paradigms for music. However, most interactions with music in current consumer voice devices are still relatively transactional; primarily allowing for keyword-based commands and basic content playback controls. They are less likely to contextualize content or support content discovery beyond what users think to ask for. We present an approach to dynamically augment the voice-based music experience with background information using story generation techniques. Our findings indicate that augmentation can have positive effects on voice-based music experiences, given the right user context and mindset.",
        "zenodo_id": 3527804,
        "dblp_key": "conf/ismir/BehroozMTKC19",
        "keywords": [
            "Voice interfaces",
            "new ways to explore",
            "music interaction paradigms",
            "transactional interactions",
            "keyword-based commands",
            "basic content playback controls",
            "contextualization",
            "content discovery",
            "background information",
            "story generation techniques"
        ]
    },
    {
        "title": "Backtracking Search Heuristics for Solving the All-partition Array Problem.",
        "author": [
            "Brian Bemman",
            "David Meredith 0001"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527826",
        "url": "https://doi.org/10.5281/zenodo.3527826",
        "ee": "http://archives.ismir.net/ismir2019/paper/000046.pdf",
        "abstract": "Recent efforts to model the compositional processes of Milton Babbitt have yielded a number of computationally challenging problems. One of these problems, known as the \\textit{all-partition array problem}, is a particularly hard variant of set covering, and several different approaches, including mathematical optimization, constraint satisfaction, and greedy backtracking, have been proposed for solving it. Of these previous approaches, only constraint programming has led to a successful solution. Unfortunately, this solution is expensive in terms of computation time. We present here two new search heuristics and a modification to a previously proposed heuristic, that, when applied to a greedy backtracking algorithm, allow the all-partition array problem to be solved in a practical running time. We demonstrate the success of our heuristics by solving for three different instances of the problem found in Babbitt's music, including one previously solved with constraint programming and one Babbitt himself was unable to solve. Use of the new heuristics allows each instance of the problem to be solved more quickly than was possible with previous approaches.",
        "zenodo_id": 3527826,
        "dblp_key": "conf/ismir/Bemman019",
        "keywords": [
            "Milton Babbitt",
            "compositional processes",
            "all-partition array problem",
            "set covering",
            "mathematical optimization",
            "constraint satisfaction",
            "greedy backtracking",
            "constraint programming",
            "search heuristics",
            "practical running time"
        ]
    },
    {
        "title": "Generalized Metrics for Single-f0 Estimation Evaluation.",
        "author": [
            "Rachel M. Bittner",
            "Juan J. Bosch"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527916",
        "url": "https://doi.org/10.5281/zenodo.3527916",
        "ee": "http://archives.ismir.net/ismir2019/paper/000090.pdf",
        "abstract": "Single-f0 estimation methods, including pitch trackers and melody estimators, have historically been evaluated using a set of common metrics which score estimates frame-wise in terms of pitch and voicing accuracy. \"Voicing\" refers to whether or not a pitch is active, and has historically been regarded as a binary value. However, this has limitations because it is often ambiguous whether a pitch is present or absent, making a binary choice difficult for humans and algorithms alike. For example, when a source fades out or reverberates, the exact point where the pitch is no longer present is unclear. Many single-f0 estimation algorithms select a threshold for when a pitch is active or not, and different choices of threshold drastically affect the results of standard metrics. In this paper, we present a refinement on the existing single-f0 metrics, by allowing the estimated voicing to be represented as a continuous likelihood, and introducing a weighting on frame level pitch accuracy, which considers the energy of the source producing the f0 relative to the energy of the rest of the signal. We compare these metrics experimentally with the previous metrics using a number of algorithms and datasets and discuss the fundamental differences. We show that, compared to the previous metrics, our proposed metrics allow threshold-independent algorithm comparisons.",
        "zenodo_id": 3527916,
        "dblp_key": "conf/ismir/BittnerB19",
        "keywords": [
            "pitch trackers",
            "melody estimators",
            "frame-wise evaluation",
            "pitch accuracy",
            "voicing accuracy",
            "binary value",
            "ambiguous voicing",
            "threshold choice",
            "single-f0 estimation",
            "continuous likelihood"
        ]
    },
    {
        "title": "mirdata: Software for Reproducible Usage of Datasets.",
        "author": [
            "Rachel M. Bittner",
            "Magdalena Fuentes",
            "David Rubinstein",
            "Andreas Jansson 0001",
            "Keunwoo Choi",
            "Thor Kell"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527750",
        "url": "https://doi.org/10.5281/zenodo.3527750",
        "ee": "http://archives.ismir.net/ismir2019/paper/000009.pdf",
        "abstract": "There are a number of efforts in the MIR community towards increased reproducibility, such as creating more open datasets, publishing code, and the use of common software libraries, e.g. for evaluation. However, when it comes to datasets, there is usually little guarantee that researchers are using the exact same data in the same way, which among other issues, makes comparisons of different methods on the \"same\" datasets problematic.  In this paper, we first show how (often unknown) differences in datasets can lead to significantly different experimental results. We propose a solution to these problems in the form of an open source library, mirdata, which handles datasets in their current distribution modes, but controls for possible variability. In particular, it contains tools which: (1) validate if the user's data (e.g. audio, annotations) is consistent with a canonical version of the dataset; (2) load annotations in a consistent manner; (3) download or give instructions for obtaining data; and (4) make it easy to perform track metadata-specific analysis.",
        "zenodo_id": 3527750,
        "dblp_key": "conf/ismir/BittnerFRJCK19",
        "keywords": [
            "reproducibility",
            "open datasets",
            "code publishing",
            "common software libraries",
            "dataset variability",
            "mirdata library",
            "consistent data validation",
            "annotations loading",
            "data downloading",
            "track metadata analysis"
        ]
    },
    {
        "title": "Multi-Task Learning of Tempo and Beat: Learning One to Improve the Other.",
        "author": [
            "Sebastian B\u00f6ck",
            "Matthew E. P. Davies",
            "Peter Knees"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527850",
        "url": "https://doi.org/10.5281/zenodo.3527850",
        "ee": "http://archives.ismir.net/ismir2019/paper/000058.pdf",
        "abstract": "In this paper, we propose a multi-task learning approach for simultaneous tempo estimation and beat tracking of musical audio. The system shows state-of-the-art performance for both tasks on a wide range of data, but has another fundamental advantage: due to its multi-task nature, it is not only able to exploit the mutual information of both tasks by learning a common, shared representation, but can also improve one by learning only from the other. The multi-task learning is achieved by globally aggregating the skip connections of a beat tracking system built around temporal convolutional networks, and feeding them into a tempo classification layer. The benefit of this approach is investigated by the inclusion of training data for which tempo-only annotations are available, and which is shown to provide improvements in beat tracking accuracy.",
        "zenodo_id": 3527850,
        "dblp_key": "conf/ismir/BockDK19",
        "keywords": [
            "multi-task learning",
            "tempo estimation",
            "beat tracking",
            "musical audio",
            "state-of-the-art performance",
            "mutual information",
            "shared representation",
            "improvement",
            "skip connections",
            "tempo classification layer"
        ]
    },
    {
        "title": "The AcousticBrainz Genre Dataset: Multi-Source, Multi-Level, Multi-Label, and Large-Scale.",
        "author": [
            "Dmitry Bogdanov",
            "Alastair Porter",
            "Hendrik Schreiber 0001",
            "Juli\u00e1n Urbano",
            "Sergio Oramas"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527818",
        "url": "https://doi.org/10.5281/zenodo.3527818",
        "ee": "http://archives.ismir.net/ismir2019/paper/000042.pdf",
        "abstract": "This paper introduces the AcousticBrainz Genre Dataset, a large-scale collection of hierarchical multi-label genre annotations from different metadata sources. It allows researchers to explore how the same music pieces are annotated differently by different communities following their own genre taxonomies, and how this could be addressed by genre recognition systems. Genre labels for the dataset are sourced from both expert annotations and crowds, permitting comparisons between strict hierarchies and folksonomies. Music features are available via the AcousticBrainz database. To guide research, we suggest a concrete research task and provide a baseline as well as an evaluation method. This task may serve as an example of the development and validation of automatic annotation algorithms on complementary datasets with different taxonomies and coverage. With this dataset, we hope to contribute to developments in content-based music genre recognition as well as cross-disciplinary studies on genre metadata analysis.",
        "zenodo_id": 3527818,
        "dblp_key": "conf/ismir/BogdanovPSUO19",
        "keywords": [
            "AcousticBrainz Genre Dataset",
            "large-scale collection",
            "hierarchical multi-label genre annotations",
            "different metadata sources",
            "researchers exploration",
            "genre recognition systems",
            "music pieces annotation",
            "genre taxonomies",
            "folksonomies comparison",
            "music features"
        ]
    },
    {
        "title": "Self-Supervised Methods for Learning Semantic Similarity in Music.",
        "author": [
            "Mason Bretan",
            "Larry P. Heck"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527840",
        "url": "https://doi.org/10.5281/zenodo.3527840",
        "ee": "http://archives.ismir.net/ismir2019/paper/000053.pdf",
        "abstract": "Neural networks have been used to learn a latent \"musical space\" or \"embedding\" to encode meaningful features and provide a method of measuring semantic similarity between two musical passages. An ideal embedding is one that both captures features useful for downstream tasks and conforms to a distribution suitable for sampling and meaningful interpolation. We present two new methods for learning musical embeddings that leverage context while simultaneously imposing a shape on the feature space distribution via backpropagation using an adversarial component. We focus on the symbolic domain and target short polyphonic musical units consisting of 40 note sequences. The goal is to project these units into a continuous low dimensional space that has semantic relevance. We evaluate relevance based on the learned features' abilities to complete various musical tasks and show improvement over baseline models including variational autoencoders, adversarial autoencoders, and deep structured semantic models. We use a dataset consisting of classical piano and demonstrate the robustness of our methods across multiple input representations.",
        "zenodo_id": 3527840,
        "dblp_key": "conf/ismir/BretanH19",
        "keywords": [
            "Neural networks",
            "latent musical space",
            "semantic similarity",
            "musical embeddings",
            "context",
            "feature space distribution",
            "backpropagation",
            "adversarial component",
            "symbolic domain",
            "short polyphonic musical units"
        ]
    },
    {
        "title": "Microtiming Analysis in Traditional Shetland Fiddle Music.",
        "author": [
            "Estefan\u00eda Cano",
            "Scott Beveridge"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527856",
        "url": "https://doi.org/10.5281/zenodo.3527856",
        "ee": "http://archives.ismir.net/ismir2019/paper/000061.pdf",
        "abstract": "This work aims to characterize microtiming variations in traditional Shetland fiddle music.  These microtiming variations dictate the rhythmic flow of a performed melody, and contribute, among other things, to the suitability of this music as an accompaniment to dancing. In the context of Shetland fiddle music, these microtiming variations are often referred to as lilt.  Using a corpus of 27 traditional fiddle tunes from the Shetland Isles, we examine inter-beat timing deviations, as well as inter-onset timing deviations of eighth note sequences.  Results show a number of distinct inter-beat and inter-onset rhythmic patterns that may characterize lilt, as well as idiosyncratic patterns for each performer.  This paper presents a first step towards the use of Music Information Retrieval (MIR) techniques for modelling lilt in traditional Scottish fiddle music, and highlights its implications in the field of ethnomusicology.",
        "zenodo_id": 3527856,
        "dblp_key": "conf/ismir/CanoB19",
        "keywords": [
            "microtiming variations",
            "traditional Shetland fiddle music",
            "rhythmic flow",
            "melody",
            "suitability as an accompaniment",
            "dancing",
            "inter-beat timing deviations",
            "inter-onset timing deviations",
            "eighth note sequences",
            "distinct inter-beat and inter-onset rhythmic patterns"
        ]
    },
    {
        "title": "Data Usage in MIR: History &amp; Future Recommendations.",
        "author": [
            "Wenqin Chen",
            "Jessica Keast",
            "Jordan Moody",
            "Corinne Moriarty",
            "Felicia Villalobos",
            "Virtue Winter",
            "Xueqi Zhang",
            "Xuanqi Lyu",
            "Elizabeth Freeman",
            "Jessie Wang",
            "Sherry Cai",
            "Katherine M. Kinnaird"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527733",
        "url": "https://doi.org/10.5281/zenodo.3527733",
        "ee": "http://archives.ismir.net/ismir2019/paper/000001.pdf",
        "abstract": "The MIR community faces unique challenges in terms of data access, due in large part to country-specific copyright laws. As a result, there is an emerging divide in the MIR research community between labs that have access to music through large companies with abundant funds, and independent labs at smaller institutions who do not have such expansive access. This paper explores how independent researchers have worked to overcome limitations of access to music data without contributing to the crisis of reproducibility.  Acknowledging that there is no single solution for every data access problem that smaller labs face, we propose a number of possibilities for how the MIR community can bridge the gap between advancements from large companies and those within academia. As MIR looks towards the next 20 years, democratizing and expanding access to MIR research and music data is critical. Future solutions could include a  distributed MIREX system, an API designed for MIR researchers, and community-led advocacy to stakeholders.",
        "zenodo_id": 3527733,
        "dblp_key": "conf/ismir/ChenKMMVWZLFWCK19",
        "keywords": [
            "data access",
            "country-specific copyright laws",
            "emerging divide",
            "independent labs",
            "reproducibility crisis",
            "MIR community",
            "large companies",
            "smaller institutions",
            "democratizing access",
            "MIREX system"
        ]
    },
    {
        "title": "Harmony Transformer: Incorporating Chord Segmentation into Harmony Recognition.",
        "author": [
            "Tsung-Ping Chen",
            "Li Su 0004"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527794",
        "url": "https://doi.org/10.5281/zenodo.3527794",
        "ee": "http://archives.ismir.net/ismir2019/paper/000030.pdf",
        "abstract": "Musical harmony analysis is usually a process of unfolding and interpreting the hierarchical structure of music. Computational approaches to such structural analysis are still challenging, owing to the fact that the boundary between different harmonic states (such as chord functions) is not explicitly defined in the audio or symbolic music data. It is a novel approach to improve chord recognition by jointly identifying chord change using end-to-end sequence learning. In this paper, we propose the Harmony Transformer, a multi-task music harmony analysis model aiming to improve chord recognition through incorporating chord segmentation into the recognition process. The integration of chord segmentation and chord recognition is implemented with the Transformer, a deep sequential learning model yielding fruitful results in the field of natural language processing. A non-autoregressive decoding framework is also adopted here in aid of concatenating the two highly correlated tasks. Experiments of both chord symbol recognition and functional harmony recognition on audio and symbolic datasets demonstrate that explicitly learning the hierarchical structural information of musical data can facilitate and improve the harmony recognition.",
        "zenodo_id": 3527794,
        "dblp_key": "conf/ismir/ChenS19",
        "keywords": [
            "Musical harmony analysis",
            "Hierarchical structure",
            "Computational approaches",
            "Boundary between states",
            "Chord recognition",
            "Joint identification",
            "Sequence learning",
            "Chord segmentation",
            "Transformer model",
            "Non-autoregressive decoding"
        ]
    },
    {
        "title": "Deep Unsupervised Drum Transcription.",
        "author": [
            "Keunwoo Choi",
            "Kyunghyun Cho"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527774",
        "url": "https://doi.org/10.5281/zenodo.3527774",
        "ee": "http://archives.ismir.net/ismir2019/paper/000020.pdf",
        "abstract": "We introduce DrummerNet, a drum transcription system that is trained in an unsupervised manner. DrummerNet does not require any ground-truth transcription and, with the data-scalability of deep neural networks, learns from a large unlabeled dataset. In DrummerNet, the target drum signal is first passed to a (trainable) transcriber, then reconstructed in a (fixed) synthesizer according to the transcription estimate. By training the system to minimize the distance between the input and the output audio signals, the transcriber learns to transcribe without ground truth transcription. Our experiment shows that DrummerNet performs favorably compared to many other recent drum transcription systems, both supervised and unsupervised.",
        "zenodo_id": 3527774,
        "dblp_key": "conf/ismir/ChoiC19",
        "keywords": [
            "DrummerNet",
            "unsupervised training",
            "large unlabeled dataset",
            "transcriber learns transcription",
            "synthesizer reconstructs signal",
            "minimizes distance between signals",
            "compared favorably to others",
            "supervised and unsupervised systems",
            "audio signal reconstruction",
            "transcription without ground truth"
        ]
    },
    {
        "title": "Zero-shot Learning for Audio-based Music Classification and Tagging.",
        "author": [
            "Jeong Choi",
            "Jongpil Lee",
            "Jiyoung Park",
            "Juhan Nam"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527741",
        "url": "https://doi.org/10.5281/zenodo.3527741",
        "ee": "http://archives.ismir.net/ismir2019/paper/000005.pdf",
        "abstract": "Audio-based music classification and tagging is typically based on categorical supervised learning with a fixed set of labels. This intrinsically cannot handle unseen labels such as newly added music genres or semantic words that users arbitrarily choose for music retrieval. Zero-shot learning can address this problem by leveraging an additional semantic space of labels where auxiliary information about the labels is used to unveil the relationship between each other.  In this work, we investigate the zero-shot learning in music domain and organize two different setups of auxiliary information. One is using human-labeled attribute information based on Free Music Archive and OpenMIC-2018 datasets. The other is using general word semantic information based on Million Song Dataset and Last.fm tag annotations.  Considering a music track is usually multi-labeled in music classification and tagging datasets, we also propose a data split scheme and associated evaluation settings for the multi-label zero-shot learning.  Finally, we report experimental results and discuss the effectiveness and new possibilities of zero-shot learning in music domain.",
        "zenodo_id": 3527741,
        "dblp_key": "conf/ismir/ChoiLPN19",
        "keywords": [
            "audio-based music classification",
            "categorical supervised learning",
            "fixed set of labels",
            "zero-shot learning",
            "auxiliary information",
            "semantic space",
            "unseen labels",
            "music retrieval",
            "newly added music genres",
            "semantic words"
        ]
    },
    {
        "title": "Towards Explainable Music Emotion Recognition: The Route via Mid-level Features.",
        "author": [
            "Shreyan Chowdhury",
            "Andreu Vall",
            "Verena Haunschmid",
            "Gerhard Widmer"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527788",
        "url": "https://doi.org/10.5281/zenodo.3527788",
        "ee": "http://archives.ismir.net/ismir2019/paper/000027.pdf",
        "abstract": "Emotional aspects play an important part in our interaction with music. However, modeling this aspect in MIR systems has been notoriously challenging since emotion is an inherently abstract and subjective experience, thus making it difficult to quantify or predict in the first place, and to make sense of the predictions in the next. In an attempt to create a model that can give a musically meaningful and intuitive explanation for its prediction, we propose a VGG-style deep neural network that learns to predict emotional characteristics of a musical piece together with (and based on) human-interpretable, mid-level perceptual features. We compare this to predicting emotion directly with an identical network that does not take into account the mid-level features, and observe that the cost of going through the mid-level features is surprisingly low, on average. The design of our network allows us to visualize the effects of perceptual features on individual emotion predictions, and we argue that the small loss in performance in going through the mid-level features is justified by the gain in explainability of the predictions.",
        "zenodo_id": 3527788,
        "dblp_key": "conf/ismir/ChowdhuryPHW19",
        "keywords": [
            "emotion",
            "MIR systems",
            "abstract",
            "subjective experience",
            "quantify",
            "predict",
            "explainability",
            "perceptual features",
            "visualize",
            "explainability"
        ]
    },
    {
        "title": "Supervised Symbolic Music Style Translation Using Synthetic Data.",
        "author": [
            "Ondrej C\u00edfka",
            "Umut Simsekli",
            "Ga\u00ebl Richard"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527878",
        "url": "https://doi.org/10.5281/zenodo.3527878",
        "ee": "http://archives.ismir.net/ismir2019/paper/000071.pdf",
        "abstract": "Research on style transfer and domain translation has clearly demonstrated the ability of deep learning-based algorithms to manipulate images in terms of artistic style. More recently, several attempts have been made to extend such approaches to music (both symbolic and audio) in order to enable transforming musical style in a similar manner. In this study, we focus on symbolic music with the goal of altering the 'style' of a piece while keeping its original 'content'. As opposed to the current methods, which are inherently restricted to be unsupervised due to the lack of 'aligned' data (i.e. the same musical piece played in multiple styles), we develop the first fully supervised algorithm for this task. At the core of our approach lies a synthetic data generation scheme which allows us to produce virtually unlimited amounts of aligned data, and hence avoid the above issue. In view of this data generation scheme, we propose an encoder-decoder model for translating symbolic music accompaniments between a number of different styles. Our experiments show that our models, although trained entirely on synthetic data, are capable of producing musically meaningful accompaniments even for real (non-synthetic) MIDI recordings.",
        "zenodo_id": 3527878,
        "dblp_key": "conf/ismir/CifkaSR19",
        "keywords": [
            "deep learning",
            "style transfer",
            "domain translation",
            "music",
            "symbolic music",
            "audio",
            "supervised learning",
            "synthetic data",
            "encoder-decoder model",
            "musical accompaniments"
        ]
    },
    {
        "title": "humdrumR: a New Take on an Old Approach to Computational Musicology.",
        "author": [
            "Nathaniel Condit-Schultz",
            "Claire Arthur"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527910",
        "url": "https://doi.org/10.5281/zenodo.3527910",
        "ee": "http://archives.ismir.net/ismir2019/paper/000087.pdf",
        "abstract": "Musicology research is a fundamentally humanistic endeavor. However, despite the productive work of a small niche of humanities-trained computational musicologists, most cutting-edge digital music research is pursued by scholars whose primary training is scientific or computational, not humanistic. This unfortunate situation is prolonged, at least in part, by the daunting barrier that computer coding presents to humanities scholars with no technical training. In this paper, we present humdrumR (\"hum-drummer\"), a software package designed to afford computational musicology research for both advanced and novice computer coders. Humdrum is a powerful and influential existing computational musicology framework, including the humdrum syntax\u2014a flexible text data format with tens of thousands of extant scores available (Kern Scores)\u2014and the Bash-based humdrum toolkit. HumdrumR is a modern replacement for the humdrum toolkit, based in the data-analysis/statistical programming language R. By combining the flexibility and transparency of the humdrum syntax with the powerful data analysis tools and concise syntax of R, humdrumR offers an appealing new approach to would-be computational musicologists. HumdrumR leverages R's powerful metaprogramming capabilities to create an extremely expressive and composable syntax, allowing novices to achieve usable analyses quickly while avoiding many coding concepts that are commonly challenging for beginners.",
        "zenodo_id": 3527910,
        "dblp_key": "conf/ismir/Condit-SchultzA19",
        "keywords": [
            "humanistic",
            "humanities-trained",
            "computational musicology",
            "novice computer coders",
            "humdrumR",
            "humdrum syntax",
            "Bash-based humdrum toolkit",
            "R",
            "data-analysis/statistical programming language",
            "metaprogramming capabilities"
        ]
    },
    {
        "title": "LakhNES: Improving Multi-instrumental Music Generation with Cross-domain Pre-training.",
        "author": [
            "Chris Donahue",
            "Huanru Henry Mao",
            "Yiting Ethan Li",
            "Garrison W. Cottrell",
            "Julian J. McAuley"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527902",
        "url": "https://doi.org/10.5281/zenodo.3527902",
        "ee": "http://archives.ismir.net/ismir2019/paper/000083.pdf",
        "abstract": "We are interested in the task of generating multi-instrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation\u2014here we adapt it to the multi-instrumental setting. Transformers are complex, high-dimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to fit. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-voice scores from an early video game sound synthesis chip (the NES), which we find to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music. Despite differences between the two corpora, we find that this pre-training procedure improves both quantitative and qualitative performance for our primary task.",
        "zenodo_id": 3527902,
        "dblp_key": "conf/ismir/DonahueMLCM19",
        "keywords": [
            "Transformer",
            "piano score generation",
            "NES-MDB dataset",
            "four-voice scores",
            "NES sound synthesis chip",
            "pre-training technique",
            "heterogeneous music",
            "quantitative performance",
            "qualitative performance",
            "corpus differences"
        ]
    },
    {
        "title": "Community-Based Cover Song Detection.",
        "author": [
            "Jonathan Donier"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527790",
        "url": "https://doi.org/10.5281/zenodo.3527790",
        "ee": "http://archives.ismir.net/ismir2019/paper/000028.pdf",
        "abstract": "Audio-based cover song detection has received much attention in the MIR community in the recent years. To date, the most popular formulation of the problem has been to compare the audio signals of two tracks and to make a binary decision based on this information only. However, leveraging additional signals might be key if one wants to solve the problem at an industrial scale. In this paper, we introduce an ensemble-based method that approaches the problem from a many-to-many perspective.  Instead of considering pairs of tracks in isolation, we consider larger sets of potential versions for a given composition, and create and exploit the graph of relationships between these tracks. We show that this can result in a significant improvement in performance, in particular when the number of existing versions of a given composition is large.",
        "zenodo_id": 3527790,
        "dblp_key": "conf/ismir/Donier19",
        "keywords": [
            "audio-based",
            "cover song detection",
            "MIR community",
            "binary decision",
            "leveraging additional signals",
            "ensemble-based method",
            "many-to-many perspective",
            "graph of relationships",
            "significant improvement",
            "large number of existing versions"
        ]
    },
    {
        "title": "Cover Detection Using Dominant Melody Embeddings.",
        "author": [
            "Guillaume Doras",
            "Geoffroy Peeters"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527752",
        "url": "https://doi.org/10.5281/zenodo.3527752",
        "ee": "http://archives.ismir.net/ismir2019/paper/000010.pdf",
        "abstract": "Automatic cover detection -the task of finding in an audio database all the covers of one or several query tracks -has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use.  In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction -that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds.",
        "zenodo_id": 3527752,
        "dblp_key": "conf/ismir/DorasP19",
        "keywords": [
            "Automatic cover detection",
            "finding covers in audio database",
            "MIR community challenge",
            "practical problem for authors",
            "small datasets accuracy",
            "modern real-life audio corpora",
            "faster approaches",
            "lower accuracy for practical use",
            "neural network architecture",
            "track embedding vector representation"
        ]
    },
    {
        "title": "Towards Automatically Correcting Tapped Beat Annotations for Music Recordings.",
        "author": [
            "Jonathan Driedger",
            "Hendrik Schreiber 0001",
            "W. Bas de Haas",
            "Meinard M\u00fcller"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527778",
        "url": "https://doi.org/10.5281/zenodo.3527778",
        "ee": "http://archives.ismir.net/ismir2019/paper/000022.pdf",
        "abstract": "A common method to create beat annotations for music recordings is to let a human annotator tap along with them. However, this method is problematic due to the limited human ability to temporally align taps with audio cues for beats accurately. In order to create accurate beat annotations, it is therefore typically necessary to manually correct the recorded taps in a subsequent step, which is a cumbersome task. In this work we aim to automate this correction step by \"snapping\" the taps to close-by audio cues a strategy that is often used by beat tracking algorithms to refine their beat estimates. The main contributions of this paper can be summarized as follows. First, we formalize the automated correction procedure mathematically. Second, we introduce a novel visualization method that serves as a tool to analyze the results of the correction procedure for potential errors. Third, we present a new dataset consisting of beat annotations for 101 music recordings. Fourth, we use this dataset to perform a listening experiment as well as a quantitative study to show the effectiveness of our snapping procedure.",
        "zenodo_id": 3527778,
        "dblp_key": "conf/ismir/DriedgerSHM19",
        "keywords": [
            "automated correction",
            "snapping",
            "audio cues",
            "beat annotations",
            "listening experiment",
            "quantitative study",
            "novel visualization",
            "dataset",
            "beat tracking algorithms",
            "temporal alignment"
        ]
    },
    {
        "title": "Folded CQT RCNN For Real-time Recognition of Instrument Playing Techniques.",
        "author": [
            "Jean-Fran\u00e7ois Ducher",
            "Philippe Esling"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527908",
        "url": "https://doi.org/10.5281/zenodo.3527908",
        "ee": "http://archives.ismir.net/ismir2019/paper/000086.pdf",
        "abstract": "In the past years, deep learning has produced state-of-the-art performance in timbre and instrument classification. However, only a few models currently deal with the recognition of advanced Instrument Playing Techniques (IPT). None of them have a real-time approach of this problem. Furthermore, most studies rely on a single sound bank for training and testing. Their methodology provides no assurance as to the generalization of their results to other sounds. In this article, we extend state-of-the-art convolutional neural networks to the classification of IPTs. We build the first IPT corpus from independent sound banks, annotate it with the JAMS standard and make it freely available. Our models yield consistently high accuracies on a homogeneous subset of this corpus. However, only a proper taxonomy of IPTs and specifically defined input transforms offer proper resilience when addressing the \"minus-1db\" methodology, which assesses the ability of the models to generalize. In particular, we introduce a novel Folded Constant Q-Transform adjusted to the requirements of IPT classification. Finally we discuss the use of our classifier in real-time.",
        "zenodo_id": 3527908,
        "dblp_key": "conf/ismir/DucherE19",
        "keywords": [
            "deep learning",
            "timbre and instrument classification",
            "advanced Instrument Playing Techniques (IPT)",
            "real-time approach",
            "independent sound banks",
            "JAMS standard",
            "homogeneous subset",
            "taxonomy of IPTs",
            "Folded Constant Q-Transform",
            "real-time"
        ]
    },
    {
        "title": "Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN.",
        "author": [
            "Anders Elowsson",
            "Anders Friberg"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527864",
        "url": "https://doi.org/10.5281/zenodo.3527864",
        "ee": "http://archives.ismir.net/ismir2019/paper/000065.pdf",
        "abstract": "This paper presents a convolutional neural network (CNN) that uses input from a polyphonic pitch estimation system to predict perceived minor/major modality in music audio. The pitch activation input is structured to allow the first CNN layer to compute two pitch chromas focused on different octaves. The following layers perform harmony analysis across chroma and time scales. Through max pooling across pitch, the CNN becomes invariant with regards to the key class (i.e., key disregarding mode) of the music. A multilayer perceptron combines the modality activation output with spectral features for the final prediction. The study uses a dataset of 203 excerpts rated by around 20 listeners each, a small challenging data size requiring a carefully designed parameter sharing. With an R2 of about 0.71, the system clearly outperforms previous systems as well as individual human listeners. A final ablation study highlights the importance of using pitch activations processed across longer time scales, and using pooling to facilitate invariance with regards to the key class.",
        "zenodo_id": 3527864,
        "dblp_key": "conf/ismir/ElowssonF19",
        "keywords": [
            "convolutional neural network",
            "polyphonic pitch estimation system",
            "perceived minor/major modality",
            "music audio",
            "pitch chromas",
            "harmony analysis",
            "spectral features",
            "R2 score",
            "key class",
            "max pooling"
        ]
    },
    {
        "title": "Quantifying Musical Style: Ranking Symbolic Music based on Similarity to a Style.",
        "author": [
            "Jeffrey Ens",
            "Philippe Pasquier"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527952",
        "url": "https://doi.org/10.5281/zenodo.3527952",
        "ee": "http://archives.ismir.net/ismir2019/paper/000107.pdf",
        "abstract": "Modelling human perception of musical similarity is critical for the evaluation of generative music systems, musicological research, and many Music Information Retrieval tasks. Although human similarity judgments are the gold standard, computational analysis is often preferable, since results are often easier to reproduce, and computational methods are much more scalable. Moreover, computation based approaches can be calculated quickly and on demand, which is a prerequisite for use with an online system. We propose StyleRank, a method to measure the similarity between a MIDI file and an arbitrary musical style delineated by a collection of MIDI files. MIDI files are encoded using a novel set of features and an embedding is learned using Random Forests. Experimental evidence demonstrates that StyleRank is highly correlated with human perception of stylistic similarity, and that it is precise enough to rank generated samples based on their similarity to the style of a corpus. In addition, similarity can be measured with respect to a single feature, allowing specific discrepancies between generated samples and a particular musical style to be identified.",
        "zenodo_id": 3527952,
        "dblp_key": "conf/ismir/EnsP19",
        "keywords": [
            "Modelling human perception",
            "Evaluation of generative music systems",
            "Musicological research",
            "Music Information Retrieval tasks",
            "Computational analysis",
            "Gold standard",
            "Computational methods",
            "Scalability",
            "Online system",
            "StyleRank"
        ]
    },
    {
        "title": "Leveraging knowledge bases and parallel annotations for music genre translation.",
        "author": [
            "Elena V. Epure",
            "Anis Khlif",
            "Romain Hennequin"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527944",
        "url": "https://doi.org/10.5281/zenodo.3527944",
        "ee": "http://archives.ismir.net/ismir2019/paper/000103.pdf",
        "abstract": "Prevalent efforts have been put in automatically inferring genres of musical items. Yet, the propose solutions often rely on simplifications and fail to address the diversity and subjectivity of music genres. Accounting for these has, though, many benefits for aligning knowledge sources, integrating data and enriching musical items with tags. Here, we choose a new angle for the genre study by seeking to predict what would be the genres of musical items in a target tag system, knowing the genres assigned to them within source tag systems. We call this a translation task and identify three cases: 1) no common annotated corpus between source and target tag systems exists, 2) such a large corpus exists, 3) only few common annotations exist. We propose the related solutions: a knowledge-based translation modeled as taxonomy mapping, a statistical translation modeled with maximum likelihood logistic regression; a hybrid translation modeled with maximum a posteriori logistic regression with priors given by the knowledge-based translation. During evaluation, the solutions fit well the identified cases and the hybrid translation is systematically the most effective w.r.t. multilabel  classification metrics. This is a first attempt to unify genre tag systems by leveraging both representation and interpretation diversity.",
        "zenodo_id": 3527944,
        "dblp_key": "conf/ismir/EpureKH19",
        "keywords": [
            "genre inference",
            "diversity",
            "subjectivity",
            "knowledge-based translation",
            "statistical translation",
            "hybrid translation",
            "taxonomy mapping",
            "maximum likelihood logistic regression",
            "maximum a posteriori logistic regression",
            "prior distribution"
        ]
    },
    {
        "title": "A Dataset of Rhythmic Pattern Reproductions and Baseline Automatic Assessment System.",
        "author": [
            "Felipe Falc\u00e3o",
            "Baris Bozkurt",
            "Xavier Serra",
            "Nazareno Andrade",
            "Ozan Baysal"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3299663",
        "url": "https://doi.org/10.5281/zenodo.3299663",
        "ee": "http://archives.ismir.net/ismir2019/paper/000052.pdf",
        "abstract": "This work presents a novel dataset comprised of audio and jury evaluations for rhythmic pattern reproduction performances by students applying for a conservatory. Data was collected in-loco during entrance exams where students were asked to imitate a set of rhythmic patterns played by teachers. In addition to the pass or fail grades provided by the members of the jury during the exam sessions, a subset of the data was also evaluated by external annotators on a 4-level scale. A baseline automatic assessment system is presented to demonstrate the usefulness of the dataset. Preliminary results deliver an accuracy of 76% for a simple pass/fail logistic regression classifier and a mean average error of 0.59 for a linear regression grade estimator. The implementation is also made publicly available to serve as baseline for alternative assessments systems that may leverage the dataset.",
        "zenodo_id": 3299663,
        "dblp_key": "conf/ismir/FalcaoBSAB19",
        "keywords": [
            "audio",
            "jury evaluations",
            "rhythmic pattern reproduction",
            "conservatory",
            "entrance exams",
            "pass/fail grades",
            "external annotators",
            "baseline automatic assessment",
            "logistic regression classifier",
            "linear regression grade estimator"
        ]
    },
    {
        "title": "Modeling and Learning Structural Breaks in Sonata Forms.",
        "author": [
            "Laurent Feisthauer",
            "Louis Bigo",
            "Mathieu Giraud"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527828",
        "url": "https://doi.org/10.5281/zenodo.3527828",
        "ee": "http://archives.ismir.net/ismir2019/paper/000047.pdf",
        "abstract": "Expositions of Sonata Forms are structured towards two cadential goals, one being the Medial Caesura (MC). The MC is a gap in the musical texture between the Transition zone (TR) and the Secondary thematic zone (S). It appears as a climax of energy accumulation initiated by the TR, dividing the Exposition in two parts.   We introduce high-level features relevant to formalize this energy gain and to identify MCs. These features concern rhythmic, harmonic and textural aspects of the music and characterize either the MC, its preparation or the texture contrast between TR and S. They are used to train a LSTM neural network on a corpus of 27 movements of string quartets written by Mozart. The model correctly locates the MCs on 14 movements within a leave-one-piece-out validation strategy. We discuss these results and how the network manages to model such structural breaks.",
        "zenodo_id": 3527828,
        "dblp_key": "conf/ismir/FeisthauerBG19",
        "keywords": [
            "Expositions",
            "Sonata Forms",
            "Cadential goals",
            "Medial Caesura",
            "Climax of energy",
            "Rhythmic aspects",
            "Harmonic aspects",
            "Textural aspects",
            "LSTM neural network",
            "Leave-one-piece-out validation"
        ]
    },
    {
        "title": "Learning to Generate Music With Sentiment.",
        "author": [
            "Lucas Ferreira",
            "Jim Whitehead"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527824",
        "url": "https://doi.org/10.5281/zenodo.3527824",
        "ee": "http://archives.ismir.net/ismir2019/paper/000045.pdf",
        "abstract": "Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy.  A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous.",
        "zenodo_id": 3527824,
        "dblp_key": "conf/ismir/FerreiraW19",
        "keywords": [
            "Deep Learning",
            "Automatic Music Composition",
            "Sentiment Control",
            "Polyphonic Music",
            "Automatic Music Generation",
            "Symbolic Music",
            "Sentiment Analysis",
            "User Study",
            "Negative Pieces",
            "Intented Sentiment"
        ]
    },
    {
        "title": "Quantifying Disruptive Influence in the AllMusic Guide.",
        "author": [
            "Flavio V. D. de Figueiredo",
            "Nazareno Andrade"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527940",
        "url": "https://doi.org/10.5281/zenodo.3527940",
        "ee": "http://archives.ismir.net/ismir2019/paper/000102.pdf",
        "abstract": "Understanding how influences shape musical creation provides rich insight into cultural trends. As such, there have been several efforts to create quantitative complex network methods that support the analysis of influence networks among artists in a music corpus. We contribute to this body of work by examining how disruption happens in a corpus about music influence from the All Music Guide. A disruptive artist is one that creates a new stream of influences; this artist builds on prior efforts, but influences subsequent artists that do not build on the same prior efforts. We leverage methods devised to study disruption in Science and Technology, and apply them to the context of music creation. Our results point that such methods identify innovative artists, and that disruption is often uncorrelated with network centrality.",
        "zenodo_id": 3527940,
        "dblp_key": "conf/ismir/FigueiredoA19",
        "keywords": [
            "influences",
            "musical creation",
            "quantitative complex network methods",
            "analysis of influence networks",
            "disruption",
            "corpus about music influence",
            "All Music Guide",
            "disruptive artist",
            "prior efforts",
            "subsequent artists"
        ]
    },
    {
        "title": "Modelling the Syntax of North Indian Melodies with a Generalized Graph Grammar.",
        "author": [
            "Christoph Finkensiep",
            "Richard Widdess",
            "Martin Rohrmeier"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527844",
        "url": "https://doi.org/10.5281/zenodo.3527844",
        "ee": "http://archives.ismir.net/ismir2019/paper/000055.pdf",
        "abstract": "Hierarchical models of music allow explanation of highly complex musical structure based on the general principle of recursive elaboration and a small set of orthogonal operations. Recent approaches to melodic elaboration have converged to a representation based on intervals, which allows the elaboration of pairs of notes. However, two problems remain: First, an interval-first representation obscures one-sided operations like neighbor notes. Second, while models of Western melody styles largely agree on step-wise operations such as neighbors and passing notes, larger intervals are either attributed to latent harmonic properties or left unexplained. This paper presents a grammar for melodies in North Indian raga music, showing not only that recursively applied neighbor and passing note operations underlie this style as well, but that larger intervals are generated as generalized neighbors, based on the tonal hierarchy of the underlying scale structure. The notion of a generalized neighbor is not restricted to ragas but can be transferred to other musical styles, opening new perspectives on latent structure behind melodies and music in general. The presented grammar is based on a graph representation that allows one to express elaborations on both notes and intervals, unifying and generalizing previous graphand tree-based approaches.",
        "zenodo_id": 3527844,
        "dblp_key": "conf/ismir/FinkensiepWR19",
        "keywords": [
            "Hierarchical models",
            "recursive elaboration",
            "orthogonal operations",
            "melodic elaboration",
            "interval-first representation",
            "one-sided operations",
            "neighbor notes",
            "passing notes",
            "tonal hierarchy",
            "generalized neighbors"
        ]
    },
    {
        "title": "Can We Increase Interand Intra-Rater Agreement in Modeling General Music Similarity?.",
        "author": [
            "Arthur Flexer",
            "Taric Lallai"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527852",
        "url": "https://doi.org/10.5281/zenodo.3527852",
        "ee": "http://archives.ismir.net/ismir2019/paper/000059.pdf",
        "abstract": "We present a pilot study on ways to increase interand intra-rater agreement in quantification of general similarity between pieces of music. By using a more controlled group of human subjects and carefully curating song material, we try to increase overall agreement between raters concerning the perceived general similarity of songs. Repeated conduction of the experiment with a two week lag shows that intra-rater agreement is higher than inter-rater agreement. Analysis of the results and interviews with test subjects suggests that the genre of songs was a major factor in judging similarity between songs. We discuss the impacts of our results on evaluation of respective machine learning models and question the validity of experiments on general music similarity.",
        "zenodo_id": 3527852,
        "dblp_key": "conf/ismir/FlexerL19",
        "keywords": [
            "quantification",
            "interand intra-rater agreement",
            "controlled group",
            "song material",
            "overall agreement",
            "perceived general similarity",
            "repeated conduction",
            "two week lag",
            "genre of songs",
            "machine learning models"
        ]
    },
    {
        "title": "Tracking Beats and Microtiming in Afro-Latin American Music Using Conditional Random Fields and Deep Learning.",
        "author": [
            "Magdalena Fuentes",
            "Lucas Maia",
            "Mart\u00edn Rocamora",
            "Luiz W. P. Biscainho",
            "H\u00e9l\u00e8ne Camille Crayencour",
            "Slim Essid",
            "Juan Pablo Bello"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527792",
        "url": "https://doi.org/10.5281/zenodo.3527792",
        "ee": "http://archives.ismir.net/ismir2019/paper/000029.pdf",
        "abstract": "Events in music frequently exhibit small-scale temporal deviations (microtiming), with respect to the underlying regular metrical grid. In some cases, as in music from the Afro-Latin American tradition, such deviations appear systematically, disclosing their structural importance in rhythmic and stylistic configuration. In this work we explore the idea of automatically and jointly tracking beats and microtiming in timekeeper instruments of Afro-Latin American music, in particular Brazilian samba and Uruguayan candombe. To that end, we propose a language model based on conditional random fields that integrates beat and onset likelihoods as observations. We derive those activations using deep neural networks and evaluate its performance on manually annotated data using a scheme adapted to this task. We assess our approach in controlled conditions suitable for these timekeeper instruments, and study the microtiming profiles' dependency on genre and performer, illustrating promising aspects of this technique towards a more comprehensive understanding of these music traditions.",
        "zenodo_id": 3527792,
        "dblp_key": "conf/ismir/FuentesMRBCEB19",
        "keywords": [
            "microtiming",
            "underlying regular metrical grid",
            "systematically",
            "disclosing their structural importance",
            "rhythmic and stylistic configuration",
            "language model",
            "conditional random fields",
            "beat and onset likelihoods",
            "deep neural networks",
            "controlled conditions"
        ]
    },
    {
        "title": "A Study of Annotation and Alignment Accuracy for Performance Comparison in Complex Orchestral Music.",
        "author": [
            "Thassilo Gadermaier",
            "Gerhard Widmer"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527924",
        "url": "https://doi.org/10.5281/zenodo.3527924",
        "ee": "http://archives.ismir.net/ismir2019/paper/000094.pdf",
        "abstract": "Quantitative analysis of commonalities and differences between recorded music performances is an increasingly common task in computational musicology. A typical scenario involves manual annotation of different recordings of the same piece along the time dimension, for comparative analysis of, e.g., the musical tempo, or for mapping other performance-related information between performances. This can be done by manually annotating one reference performance, and then automatically synchronizing other performances, using audio-to-audio alignment algorithms. In this paper we address several questions related to those tasks. First, we analyze different annotations of the same musical piece, quantifying timing deviations between the respective human annotators. A statistical evaluation of the marker time stamps will provide (a) an estimate of the expected timing precision of human annotations and (b) a ground truth for subsequent automatic alignment experiments. We then carry out a systematic evaluation of different audio features for audio-to-audio alignment, quantifying the degree of alignment accuracy that can be achieved, and relate this to the results from the annotation study.",
        "zenodo_id": 3527924,
        "dblp_key": "conf/ismir/GadermaierW19",
        "keywords": [
            "Quantitative analysis",
            "commonalities and differences",
            "recorded music performances",
            "computational musicology",
            "manual annotation",
            "performance-related information",
            "audio-to-audio alignment",
            "marker time stamps",
            "audio features",
            "alignment accuracy"
        ]
    },
    {
        "title": "Estimating Unobserved Audio Features for Target-Based Orchestration.",
        "author": [
            "Jon Gillick",
            "Carmine-Emanuele Cella",
            "David Bamman"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527776",
        "url": "https://doi.org/10.5281/zenodo.3527776",
        "ee": "http://archives.ismir.net/ismir2019/paper/000021.pdf",
        "abstract": "Target-based assisted orchestration can be thought of as the process of searching for optimal combinations of sounds to match a target sound, given a database of samples, a similarity metric, and a set of constraints.  A typical solution to this problem is a proposed orchestral score where candidate scores are ranked by similarity in some feature space between the target sound and the mixture of audio samples in the database corresponding to the notes in the score; in the orchestral setting, valid scores may contain dozens of instruments sounding simultaneously.  Generally, target-based assisted orchestration systems consist of a combinatorial optimization algorithm and a constraint solver that are jointly optimized to find valid solutions.  A key step in the optimization involves generating a large number of combinations of sounds from the database and then comparing the features of each mixture of sounds with the target sound.  Because of the high computational cost required to synthesize a new audio file and then compute features for every combination of sounds, in practice, existing systems instead estimate the features of each new mixture using precomputed features of the individual source files making up the combination.  Currently, state of the art systems use a simple linear combination to make these predictions, even if the features in use are not themselves linear.  In this work, we explore neural models for estimating the features of a mixture of sounds from the features of the component sounds, finding that standard features can be estimated with accuracy significantly better than that of the methods currently used in assisted orchestration systems.  We present quantitative comparisons and discuss the implications of our findings for target-based orchestration problems.",
        "zenodo_id": 3527776,
        "dblp_key": "conf/ismir/GillickCB19",
        "keywords": [
            "combinatorial optimization algorithm",
            "constraint solver",
            "target-based assisted orchestration",
            "similarity metric",
            "database of samples",
            "orchestral score",
            "valid solutions",
            "synthetic audio files",
            "precomputed features",
            "linear combination"
        ]
    },
    {
        "title": "Taking Form: A Representation Standard, Conversion Code, and Example Corpora for Recording, Visualizing, and Studying Analyses of Musical Form.",
        "author": [
            "Mark Gotham",
            "Matthew Ireland"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527904",
        "url": "https://doi.org/10.5281/zenodo.3527904",
        "ee": "http://archives.ismir.net/ismir2019/paper/000084.pdf",
        "abstract": "We report on new specification standards for representing human analyses of musical form which enable musicians to represent their analytical view of a piece either on the score (where an encoded version is available) or on a spreadsheet. Both of these representations are simple, intuitive, and highly human-readable. Further, we provide code for converting between these formats, as well as a nested bracket representation adopted from computational linguistics which, in turn, can be visualised in familiar tree diagrams to provide 'at a glance' introductions to works. Finally, we provide an initial corpus of analyses/annotations in these formats, report on the practicalities of amassing them, and offer tools for automatic comparison of the works in the corpus based on the content and structure of the annotations. We intend for this resource to be useful to computational musicologists, enabling study of form at scale, and also useful pedagogically to all teachers, students, and appreciators of music from whom projects of this kind can be rather disconnected. The code and corpus can be found at https://github.com/MarkGotham/Taking-Form.",
        "zenodo_id": 3527904,
        "dblp_key": "conf/ismir/GothamI19",
        "keywords": [
            "specification standards",
            "musical form",
            "score",
            "spreadsheet",
            "human-readable",
            "code conversion",
            "tree diagrams",
            "corpus of analyses",
            "computational musicologists",
            "pedagogical tools"
        ]
    },
    {
        "title": "The RomanText Format: A Flexible and Standard Method for Representing Roman Numerial Analyses.",
        "author": [
            "Mark Gotham",
            "Dmitri Tymoczko",
            "Michael Scott Cuthbert"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527756",
        "url": "https://doi.org/10.5281/zenodo.3527756",
        "ee": "http://archives.ismir.net/ismir2019/paper/000012.pdf",
        "abstract": "Roman numeral analysis has been central to the Western musician's toolkit since its emergence in the early nineteenth century: it is an extremely popular method for recording subjective analytical decisions about the chords and keys implied by a passage of music. Disagreements about these judgments have led to extensive theoretical debates and ongoing controversies. Such debates are exacerbated by the absence of a pubic corpus of expert Roman numeral analyses, and by the more fundamental lack of an agreed-upon, computer-readable syntax in which those analyses might be expressed. This paper specifies such a standard, along with an associated code library in music21, and a preliminary set of example corpora. To frame the project, we review some of the motivations for doing harmonic analysis, some reasons why it resists automation, and some prospective uses for our tools.",
        "zenodo_id": 3527756,
        "dblp_key": "conf/ismir/GothamTC19",
        "keywords": [
            "Roman numeral analysis",
            "Western musicians toolkit",
            "subjective analytical decisions",
            "chords and keys",
            "music theory debates",
            "computer-readable syntax",
            "expert analyses",
            "public corpus",
            "agreed-upon syntax",
            "harmonic analysis motivations"
        ]
    },
    {
        "title": "Auto-adaptive Resonance Equalization using Dilated Residual Networks.",
        "author": [
            "Maarten Grachten",
            "Emmanuel Deruty",
            "Alexandre Tanguy"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527830",
        "url": "https://doi.org/10.5281/zenodo.3527830",
        "ee": "http://archives.ismir.net/ismir2019/paper/000048.pdf",
        "abstract": "In music and audio production, attenuation of spectral resonances is an important step towards a technically correct result. In this paper we present a two-component system to automate the task of resonance equalization. The first component is a dynamic equalizer that automatically detects resonances and offers to attenuate them by a user-specified factor. The second component is a deep neural network that predicts the optimal attenuation factor based on the windowed audio. The network is trained and validated on empirical data gathered from an experiment in which sound engineers choose their preferred attenuation factors for a set of tracks. We test two distinct network architectures for the predictive model and find that a dilated residual network operating directly on the audio signal is on a par with a network architecture that requires a prior audio feature extraction stage. Both architectures predict human-preferred resonance attenuation factors significantly better than a baseline approach.",
        "zenodo_id": 3527830,
        "dblp_key": "conf/ismir/GrachtenDT19",
        "keywords": [
            "attenuation",
            "spectral resonances",
            "automate",
            "resonance equalization",
            "dynamic equalizer",
            "deep neural network",
            "empirical data",
            "sound engineers",
            "resonance attenuation factors",
            "dilated residual network"
        ]
    },
    {
        "title": "An Attention Mechanism for Musical Instrument Recognition.",
        "author": [
            "Siddharth Gururani",
            "Mohit Sharma",
            "Alexander Lerch 0001"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527746",
        "url": "https://doi.org/10.5281/zenodo.3527746",
        "ee": "http://archives.ismir.net/ismir2019/paper/000007.pdf",
        "abstract": "While the automatic recognition of musical instruments has seen significant progress, the task is still considered hard for music featuring multiple instruments as opposed to single instrument recordings. Datasets for polyphonic instrument recognition can be categorized into roughly two categories. Some, such as MedleyDB, have strong per-frame instrument activity annotations but are usually small in size. Other, larger datasets such as OpenMIC only have weak labels, i.e., instrument presence or absence is annotated only for long snippets of a song. We explore an attention mechanism for handling weakly labeled data for multi-label instrument recognition. Attention has been found to perform well for other tasks with weakly labeled data. We compare the proposed attention model to multiple models which include a baseline binary relevance random forest, recurrent neural network, and fully connected neural networks. Our results show that incorporating attention leads to an overall improvement in classification accuracy metrics across all 20 instruments in the OpenMIC dataset. We find that attention enables models to focus on (or 'attend to') specific time segments in the audio relevant to each instrument label leading to interpretable results.",
        "zenodo_id": 3527746,
        "dblp_key": "conf/ismir/GururaniSL19",
        "keywords": [
            "automatic recognition",
            "musical instruments",
            "single instrument recordings",
            "polyphonic instrument recognition",
            "attention mechanism",
            "weakly labeled data",
            "multi-label instrument recognition",
            "OpenMIC dataset",
            "recognition accuracy metrics",
            "interpretable results"
        ]
    },
    {
        "title": "Fast and Flexible Neural Audio Synthesis.",
        "author": [
            "Lamtharn Hantrakul",
            "Jesse H. Engel",
            "Adam Roberts",
            "Chenjie Gu"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527860",
        "url": "https://doi.org/10.5281/zenodo.3527860",
        "ee": "http://archives.ismir.net/ismir2019/paper/000063.pdf",
        "abstract": "Autoregressive neural networks, such as WaveNet, have opened up new avenues for expressive audio synthesis. High-quality speech synthesis utilizes detailed linguistic features for conditioning, but comparable levels of control have yet to be realized for neural synthesis of musical instruments. Here, we demonstrate an autoregressive model capable of synthesizing realistic audio that closely follows fine-scale temporal conditioning for loudness and fundamental frequency. We find the appropriate choice of conditioning features and architectures improves both the quantitative accuracy of audio resynthesis and qualitative responsiveness to creative manipulation of conditioning. While large autoregressive models generate audio much slower than real-time, we achieve these results with a more efficient WaveRNN model, opening the door for exploring real-time interactive audio synthesis with neural networks.",
        "zenodo_id": 3527860,
        "dblp_key": "conf/ismir/HantrakulERGH19",
        "keywords": [
            "Autoregressive neural networks",
            "WaveNet",
            "Expressive audio synthesis",
            "High-quality speech synthesis",
            "Loudness and fundamental frequency conditioning",
            "Neural synthesis of musical instruments",
            "Quantitative accuracy of audio resynthesis",
            "Qualitative responsiveness to creative manipulation",
            "Efficient WaveRNN model",
            "Real-time interactive audio synthesis"
        ]
    },
    {
        "title": "Harmonic Syntax in Time: Rhythm Improves Grammatical Models of Harmony.",
        "author": [
            "Daniel Harasim",
            "Timothy J. O&apos;Donnell",
            "Martin Rohrmeier"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527812",
        "url": "https://doi.org/10.5281/zenodo.3527812",
        "ee": "http://archives.ismir.net/ismir2019/paper/000039.pdf",
        "abstract": "Music is hierarchically structured, both in how it is perceived by listeners and how it is composed. Such structure can be captured elegantly using probabilistic grammatical models similar to those used to study natural language. They address the complexity of the structure using abstract categories in a recursive formalism. Most existing grammatical models of musical structure focus on one single dimension of music--such as melody, harmony, or rhythm. While these grammar models often work well on short musical excerpts, accurate analysis of longer pieces requires taking into account the constraints from multiple domains of structure. The present paper proposes abstract product grammars--a formalism which integrates multiple dimensions of musical structure into a single grammatical model--along with efficient parsing and inference algorithms for this formalism. We use this model to study the combination of hierarchically-structured harmonic syntax and hierarchically-structured rhythmic information. The latter is modeled by a novel grammar of rhythm that is capable of expressing temporal regularities in musical phrases. It integrates grouping structure and meter. The combined model of harmony and rhythm outperforms both single-dimension models in computational experiments. All models are trained and evaluated on a treebank of hand-annotated Jazz standards.",
        "zenodo_id": 3527812,
        "dblp_key": "conf/ismir/HarasimOR19",
        "keywords": [
            "Hierarchical structure",
            "Probabilistic grammatical models",
            "Music perception",
            "Recursive formalism",
            "Harmony and rhythm",
            "Combination of syntax",
            "Rhythm modeling",
            "Temporal regularities",
            "Meter integration",
            "Harmony and rhythm model"
        ]
    },
    {
        "title": "Automatic Music Transcription and Ethnomusicology: a User Study.",
        "author": [
            "Andre Holzapfel",
            "Emmanouil Benetos"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527900",
        "url": "https://doi.org/10.5281/zenodo.3527900",
        "ee": "http://archives.ismir.net/ismir2019/paper/000082.pdf",
        "abstract": "Converting an acoustic music signal into music notation using a computer program has been at the forefront of music information research for several decades, as a task referred to as automatic music transcription (AMT). However, current AMT research is still constrained to system development followed by quantitative evaluations; it is still unclear whether the performance of AMT methods is considered sufficient to be used in the everyday practice of music scholars. In this paper, we propose and carry out a user study on evaluating the usefulness of automatic music transcription in the context of ethnomusicology. As part of the study, we recruited 16 participants who were asked to transcribe short musical excerpts either from scratch or using the output of an AMT system as a basis. We collect and analyze quantitative measures such as transcription time and effort, and a range of qualitative feedback from study participants, which includes user needs, criticisms of AMT technologies, and links between perceptual and quantitative evaluations on AMT outputs. The results show no quantitative advantage of using AMT, but important indications regarding appropriate user groups and evaluation measures are provided.",
        "zenodo_id": 3527900,
        "dblp_key": "conf/ismir/HolzapfelB19",
        "keywords": [
            "acoustic music signal",
            "automatic music transcription",
            "quantitative evaluations",
            "ethnomusicology",
            "user study",
            "transcription time",
            "effort",
            "qualitative feedback",
            "quantitative measures",
            "perceptual evaluations"
        ]
    },
    {
        "title": "Identifying Expressive Semantics in Orchestral Conducting Kinematics.",
        "author": [
            "Yu-Fen Huang",
            "Tsung-Ping Chen",
            "Nikki Moran",
            "Simon Coleman",
            "Li Su 0004"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527754",
        "url": "https://doi.org/10.5281/zenodo.3527754",
        "ee": "http://archives.ismir.net/ismir2019/paper/000011.pdf",
        "abstract": "Existing kinematic research on orchestral conducting movement contributes to beat-tracking and the delivery of performance dynamics. Methodologically, such movement cues have been treated as distinct, isolated events. Yet as practicing musicians and music pedagogues know, conductors' expressive instructions are highly flexible and dependent on the musical context. We seek to demonstrate an approach to search for effective descriptors to express musical features in conducting movement in a valid music context, and to extract complex expressive semantics from elementary conducting kinematic variations. This study therefore proposes a multi-task learning model to jointly identify dynamic, articulation, and phrasing cues from conducting kinematics. A professional conducting movement dataset is compiled using a high-resolution motion capture system. The ReliefF algorithm is applied to select significant features from conducting movement, and recurrent neural network (RNN) is implemented to identify multiple movement cues. The experimental results disclose key elements in conducting movement which communicate musical expressiveness; the results also highlight the advantage of multi-task learning in the complete musical context over single-task learning. To the best of our knowledge, this is the first attempt to use recurrent neural network to explore multiple semantic expressive cuing in conducting movement kinematics.",
        "zenodo_id": 3527754,
        "dblp_key": "conf/ismir/HuangCMCS19",
        "keywords": [
            "kinematic research",
            "beat-tracking",
            "performance dynamics",
            "musical context",
            "flexible conductors",
            "expressive instructions",
            "musical features",
            "conducting kinematics",
            "multi-task learning",
            "recurrent neural network"
        ]
    },
    {
        "title": "Approachable Music Composition with Machine Learning at Scale.",
        "author": [
            "Cheng-Zhi Anna Huang",
            "Curtis Hawthorne",
            "Adam Roberts",
            "Monica Dinculescu",
            "James Wexler",
            "Leon Hong",
            "Jacob Howcroft"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527930",
        "url": "https://doi.org/10.5281/zenodo.3527930",
        "ee": "http://archives.ismir.net/ismir2019/paper/000097.pdf",
        "abstract": "To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model Coconet (Huang et al., 2017) in the style of Bach.  For users to input melodies, we designed a simplified sheet-music based interface.  To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise separable convolutions and fusing operations.  We also reduced the model download size to approximately 400KB through post-training weight quantization.  We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers.  In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries.  Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper.  We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models.",
        "zenodo_id": 3527930,
        "dblp_key": "conf/ismir/HuangHRDWHH19",
        "keywords": [
            "AI-powered Google Doodle",
            "Bach Doodle",
            "melody creation",
            "machine learning model",
            "sheet-music interface",
            "TensorFlow.js",
            "dilated depth-wise separable convolutions",
            "post-training weight quantization",
            "speed test",
            "public dataset"
        ]
    },
    {
        "title": "Automatic Assessment of Sight-reading Exercises.",
        "author": [
            "Jiawen Huang",
            "Alexander Lerch 0001"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527876",
        "url": "https://doi.org/10.5281/zenodo.3527876",
        "ee": "http://archives.ismir.net/ismir2019/paper/000070.pdf",
        "abstract": "Sight-reading requires a musician to decode, process, and perform a musical score quasi-instantaneously and without rehearsal. Due to the complexity of this task, it is difficult to assess the proficiency of a sight-reading performance, and it is even more challenging to model its human assessment. This study aims at evaluating and identifying effective features for automatic assessment of sight-reading performance. The evaluated set of features comprises task-specific, hand-crafted, and interpretable features designed to represent various aspect of sight-reading performance covering parameters such as intonation, timing, dynamics, and score continuity. The most relevant features are identified by Principal Component Analysis and forward feature selection. For context, the same features are also applied to the assessment of rehearsed student music performances and compared across different assessment categories. The results show potential of automatic assessment models for sight-reading and the relevancy of different features as well as the contribution of different feature groups to different assessment categories.",
        "zenodo_id": 3527876,
        "dblp_key": "conf/ismir/HuangL19",
        "keywords": [
            "Sight-reading",
            "musical score",
            "instantaneously",
            "without rehearsal",
            "proficiency assessment",
            "modeling",
            "task-specific",
            "hand-crafted",
            "interpretable",
            "automatic assessment"
        ]
    },
    {
        "title": "Algorithmic Ability to Predict the Musical Future: Datasets and Evaluation.",
        "author": [
            "Berit Janssen",
            "Tom Collins",
            "Iris Yuping Ren"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527780",
        "url": "https://doi.org/10.5281/zenodo.3527780",
        "ee": "http://archives.ismir.net/ismir2019/paper/000023.pdf",
        "abstract": "Music prediction and generation have been of recurring interest in the field of music informatics: many models that emulate listeners' musical expectancies, or that produce novel musical content have been introduced over the past few decades. So far, these models have mostly been evaluated in isolation, following diverse evaluation strategies. Our paper provides an overview of the new MIREX task Patterns for Prediction. We introduce a dataset, which contains monophonic and polyphonic data, both in symbolic and audio representations. We suggest a standardized evaluation procedure to compare algorithmic musical predictions. We compare two neural network models to a baseline model and show that algorithmic approaches can correctly predict about a third of a monophonic segment, and around half of a polyphonic segment, with one of the neural network models achieving best results. However, other approaches to algorithmic music prediction are needed to achieve a more rounded picture of the potential of state-of-the-art methods of music prediction.",
        "zenodo_id": 3527780,
        "dblp_key": "conf/ismir/JanssenCR19",
        "keywords": [
            "Music prediction",
            "evaluation strategies",
            "MIREX task",
            "dataset",
            "monophonic",
            "polyphonic",
            "symbolic",
            "audio representations",
            "algorithmic musical predictions",
            "neural network models"
        ]
    },
    {
        "title": "VirtuosoNet: A Hierarchical RNN-based System for Modeling Expressive Piano Performance.",
        "author": [
            "Dasaem Jeong",
            "Taegyun Kwon",
            "Yoojin Kim",
            "Kyogu Lee",
            "Juhan Nam"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527962",
        "url": "https://doi.org/10.5281/zenodo.3527962",
        "ee": "http://archives.ismir.net/ismir2019/paper/000112.pdf",
        "abstract": "In this paper, we present our application of deep neural network to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations and pedaling from pianists. Our model consists of recurrent neural networks with hierarchical attention and conditional variational autoencoder. The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes. To render musical expressions consistently over long-term sections, we first predict tempo and dynamics in measure-level and, based on the result, refine them in note-level. The evaluation through listening test shows that our model achieves a more human-like expressiveness compared to previous models. We also share the dataset we used for the experiment.",
        "zenodo_id": 3527962,
        "dblp_key": "conf/ismir/JeongKKLN19",
        "keywords": [
            "deep neural network",
            "piano performance",
            "tempo",
            "dynamics",
            "articulations",
            "pedaling",
            "MusicXML",
            "recurrent neural networks",
            "hierarchical attention",
            "conditional variational autoencoder"
        ]
    },
    {
        "title": "Large-vocabulary Chord Transcription Via Chord Structure Decomposition.",
        "author": [
            "Junyan Jiang",
            "Ke Chen 0021",
            "Wei Li 0012",
            "Gus Xia"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527892",
        "url": "https://doi.org/10.5281/zenodo.3527892",
        "ee": "http://archives.ismir.net/ismir2019/paper/000078.pdf",
        "abstract": "While audio chord recognition systems have acquired considerable accuracy on small vocabularies (e.g., major/minor chords), the large-vocabulary chord recognition problem still remains unsolved. This problem hinders the practical usages of audio recognition systems. The difficulty mainly lies in the intrinsic long-tail distribution of chord qualities, and most chord qualities have too few samples for model training.  In this paper, we propose a new model for audio chord recognition under a huge chord vocabulary. The core concept is to decompose any chord label into a set of musically meaningful components (e.g., triad, bass, seventh), each with a much smaller vocabulary compared to the size of the overall chord vocabulary. A multitask classifier is then trained to recognize all the components given the audio feature, and then labels of individual components are reassembled to form the final chord label. Experiments show that the proposed system not only achieves state-of-the-art results on traditional evaluation metrics but also performs well on a large vocabulary.",
        "zenodo_id": 3527892,
        "dblp_key": "conf/ismir/JiangCLX19",
        "keywords": [
            "audio chord recognition",
            "large-vocabulary chord recognition",
            "intrinsic long-tail distribution",
            "musically meaningful components",
            "multitask classifier",
            "traditional evaluation metrics",
            "large vocabulary",
            "state-of-the-art results",
            "large vocabulary",
            "traditional evaluation metrics"
        ]
    },
    {
        "title": "An Interactive Workflow for Generating Chord Labels for Homorhythmic Music in Symbolic Formats.",
        "author": [
            "Yaolong Ju",
            "Samuel Howes",
            "Cory McKay",
            "Nathaniel Condit-Schultz",
            "Jorge Calvo-Zaragoza",
            "Ichiro Fujinaga"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527950",
        "url": "https://doi.org/10.5281/zenodo.3527950",
        "ee": "http://archives.ismir.net/ismir2019/paper/000106.pdf",
        "abstract": "Automatic harmonic analysis is challenging: rule-based models cannot account for every possible edge case, and manual annotation is expensive and sometimes inconsistent, undermining the training and evaluation of machine learning models. We present an interactive workflow to address these problems, and test it on Bach chorales. First, a rule-based model was used to generate preliminary, consistent chord labels in order to pre-train three machine learning models. These four models were grouped into an ensemble that generated chord labels by voting, achieving 91.4% accuracy on a reserved test set. A domain expert then corrected only those chords that the ensemble did not agree on unanimously (20.9% of the generated labels). Finally, we used these corrected annotations to re-train the machine learning models, and the resulting ensemble attained an accuracy of 93.5% on the reserved test set, a 24.4% reduction in the number of errors. This versatile interactive workflow can either work in a fully automatic way, or can capitalize on relatively minimal human involvement to generate higher-quality chord labels. It combines the consistency of rule-based models with the nuance of manual analysis to generate relatively inexpensive high-quality ground truth for training effective machine learning models.",
        "zenodo_id": 3527950,
        "dblp_key": "conf/ismir/JuHMCCF19",
        "keywords": [
            "interactive workflow",
            "rule-based models",
            "manual annotation",
            "machine learning models",
            "chorales",
            "accuracy",
            "domain expert",
            "corrected annotations",
            "residual test set",
            "ensembled models"
        ]
    },
    {
        "title": "Learning Similarity Metrics for Melody Retrieval.",
        "author": [
            "Folgert Karsdorp",
            "Peter van Kranenburg",
            "Enrique Manjavacas"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527848",
        "url": "https://doi.org/10.5281/zenodo.3527848",
        "ee": "http://archives.ismir.net/ismir2019/paper/000057.pdf",
        "abstract": "Similarity measures are indispensable in music information retrieval. In recent years, various proposals have been made for measuring melodic similarity in symbolically encoded scores. Many of these approaches are ultimately based on a dynamic programming approach such as sequence alignment or edit distance, which has various drawbacks. First, the similarity scores are not necessarily metrics and are not directly comparable. Second, the algorithms are mostly first-order and of quadratic time-complexity, and finally, the features and weights need to be defined precisely. We propose an alternative approach which employs deep neural networks for end-to-end similarity metric learning. We contrast and compare different recurrent neural architectures (LSTM and GRU) for representing symbolic melodies as continuous vectors, and demonstrate how duplet and triplet loss functions can be employed to learn compact distributional representations of symbolic music in an induced melody space. This approach is contrasted with an alignment-based approach. We present results for the Meertens Tune Collections, which consists of a large number of vocal and instrumental monophonic pieces from Dutch musical sources, spanning five centuries, and demonstrate the robustness of the learned similarity metrics.",
        "zenodo_id": 3527848,
        "dblp_key": "conf/ismir/KarsdorpKM19",
        "keywords": [
            "deep neural networks",
            "end-to-end similarity metric learning",
            "recurrent neural architectures",
            "LSTM",
            "GRU",
            "symbolic melodies",
            "continuous vectors",
            "duplet and triplet loss functions",
            "induced melody space",
            "Meertens Tune Collections"
        ]
    },
    {
        "title": "An Initial Computational Model for Musical Schemata Theory.",
        "author": [
            "Andreas Katsiavalos",
            "Tom Collins",
            "Bret Battey"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527768",
        "url": "https://doi.org/10.5281/zenodo.3527768",
        "ee": "http://archives.ismir.net/ismir2019/paper/000018.pdf",
        "abstract": "Musical schemata theory entails the classification of subphrase-length progressions in melodic, harmonic and metric feature-sets as named entities (e.g., `Romanesca', `Meyer', `Cadence', etc.), where a musical schema is characterized by factors such as music content and form, position and tonal function within phrase structure, and interrelation with other schemata. To examine and automate the task of musical schemata classification, we developed a novel musical schemata classifier. First, we tested methods for exact and approximate matching of user-defined schemata prototypes, to establish the notions of identity and similarity between composite music patterns. Next, we examined methods for schemata prototype extraction from collections of same-labelled annotated examples, performing training and testing sessions similar to supervised learning approaches. The performance of the above tasks was verified using the same annotated dataset of 40 keyboard sonata excerpts from pre-Classical and Classical periods. Our evaluation of the classifier sheds light on: (a)~ability to parse and interpret music information, (b)~similarity methods for composite music patterns, (c)~categorization methods for polyphonic music.",
        "zenodo_id": 3527768,
        "dblp_key": "conf/ismir/KatsiavalosCB19",
        "keywords": [
            "Musical schemata theory",
            "subphrase-length progressions",
            "named entities",
            "music content",
            "form",
            "position",
            "tonal function",
            "phrase structure",
            "interrelation",
            "schemata classifier"
        ]
    },
    {
        "title": "Towards Interpretable Polyphonic Transcription with Invertible Neural Networks.",
        "author": [
            "Rainer Kelz",
            "Gerhard Widmer"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527822",
        "url": "https://doi.org/10.5281/zenodo.3527822",
        "ee": "http://archives.ismir.net/ismir2019/paper/000044.pdf",
        "abstract": "We explore a novel way of conceptualising the task of polyphonic music transcription, using so-called invertible neural networks. Invertible models unify both discriminative and generative aspects in one function, sharing one set of parameters. Introducing invertibility enables the practitioner to directly inspect what the discriminative model has learned, and exactly determine which inputs lead to which outputs. For the task of transcribing polyphonic audio into symbolic form, these models may be especially useful as they allow us to observe, for instance, to what extent the concept of single notes could be learned from a corpus of polyphonic music alone (which has been identified as a serious problem in recent research). This is an entirely new approach to audio transcription, which first of all necessitates some groundwork. In this paper, we begin by looking at the simplest possible invertible transcription model, and then thoroughly investigate its properties. Finally, we will take first steps towards a more sophisticated and capable version. We use the task of piano transcription, and specifically the MAPS dataset, as a basis for these investigations.",
        "zenodo_id": 3527822,
        "dblp_key": "conf/ismir/KelzW19",
        "keywords": [
            "polyphonic music transcription",
            "invertible neural networks",
            "discriminative and generative aspects",
            "inspect what the discriminative model has learned",
            "directly inspect",
            "concept of single notes",
            "corpus of polyphonic music",
            "serious problem in recent research",
            "sophisticated and capable version",
            "first steps towards"
        ]
    },
    {
        "title": "Adversarial Learning for Improved Onsets and Frames Music Transcription.",
        "author": [
            "Jong Wook Kim",
            "Juan Pablo Bello"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527898",
        "url": "https://doi.org/10.5281/zenodo.3527898",
        "ee": "http://archives.ismir.net/ismir2019/paper/000081.pdf",
        "abstract": "Automatic music transcription is considered to be one of the hardest problems in music information retrieval, yet recent deep learning approaches have achieved substantial improvements in transcription performance. These approaches commonly employ supervised learning models that predict various time-frequency representations, by minimizing element-wise losses such as the cross entropy function. However, applying the loss in this manner assumes conditional independence of each label given the input, and thus cannot accurately express inter-label dependencies. To address this issue, we introduce an adversarial training scheme that operates directly on the time-frequency representations and makes the output distribution closer to the ground-truth. Through adversarial learning, we achieve a consistent improvement in both frame-level and note-level metrics over Onsets and Frames, a state-of-the-art music transcription model. Our results show that adversarial learning can significantly reduce the error rate while increasing the confidence of the model estimations. Our approach is generic and applicable to any transcription model based on multi-label predictions, which are very common in music signal analysis.",
        "zenodo_id": 3527898,
        "dblp_key": "conf/ismir/KimB19",
        "keywords": [
            "automatic music transcription",
            "supervised learning models",
            "cross entropy function",
            "inter-label dependencies",
            "adversarial learning",
            "time-frequency representations",
            "ground-truth",
            "frame-level metrics",
            "note-level metrics",
            "adversarial training scheme"
        ]
    },
    {
        "title": "Intelligent User Interfaces for Music Discovery: The Past 20 Years and What&apos;s to Come.",
        "author": [
            "Peter Knees",
            "Markus Schedl",
            "Masataka Goto"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527737",
        "url": "https://doi.org/10.5281/zenodo.3527737",
        "ee": "http://archives.ismir.net/ismir2019/paper/000003.pdf",
        "abstract": "Providing means to assist the user in finding music is one of the original motivations underlying the research field known as Music Information Retrieval (MIR). Therefore, already the first edition of ISMIR in the year 2000 called for papers addressing the topic of \"User interfaces for music IR\". Since then, the way humans interact with technology to access and listen to music has substantially changed, not least driven by the advances of MIR and related research fields such as machine learning and recommender systems.   In this paper, we reflect on the evolution of MIR-driven user interfaces for music browsing and discovery over the past two decades. We argue that three major developments have transformed and shaped user interfaces during this period, each connected to a phase of new listening practices: first, connected to personal music collections, intelligent audio processing and content description algorithms that facilitate the automatic organization of repositories and finding music according to sound qualities; second, connected to collective web platforms, the exploitation of user-generated metadata pertaining to semantic descriptions; and third, connected to streaming services, the collection of online music interaction traces on a large scale and their exploitation in recommender systems.  We review and contextualize work from ISMIR and related venues from all three phases and extrapolate current developments to outline possible scenarios of music recommendation and listening interfaces of the future.",
        "zenodo_id": 3527737,
        "dblp_key": "conf/ismir/KneesSG19",
        "keywords": [
            "Music Information Retrieval (MIR)",
            "User interfaces for music IR",
            "User interactions with technology",
            "Evolution of MIR-driven user interfaces",
            "Personal music collections",
            "Automatic organization of repositories",
            "User-generated metadata",
            "Collective web platforms",
            "Streaming services",
            "Recommendation systems"
        ]
    },
    {
        "title": "Data-Driven Song Recognition Estimation Using Collective Memory Dynamics Models.",
        "author": [
            "Christos Koutlis",
            "Manos Schinas",
            "Vasiliki Gkatziaki",
            "Symeon Papadopoulos",
            "Yiannis Kompatsiaris"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527820",
        "url": "https://doi.org/10.5281/zenodo.3527820",
        "ee": "http://archives.ismir.net/ismir2019/paper/000043.pdf",
        "abstract": "Cultural products such as music tracks intend to be appreciated and recognized by a portion of the audience. However, no matter how highly recognized a song might be at the beginning of its life, its recognition will inevitably and progressively decay. The mechanism that governs this decreasing trajectory could be modelled as a forgetting curve or a collective memory decay process. Here, we propose a composite model, termed T-REC, that involves chart data, YouTube views, Spotify popularity of tracks and forgetting curve dynamics with the purpose of estimating song recognition levels. We also present a comparative study, involving state-of-the-art and baseline models based on ground truth data from a survey that we conducted regarding the recognition level of 100 songs in Sweden. Our method is found to perform best among this ensemble of models. A remarkable finding of our study pertains to the role of the number of weeks a song remains in the charts, which is found to be a major factor for the accurate estimation of the song recognition level.",
        "zenodo_id": 3527820,
        "dblp_key": "conf/ismir/KoutlisSGPK19",
        "keywords": [
            "Cultural products",
            "recognition",
            "forgetting curve",
            "collective memory",
            "decreasing trajectory",
            "chart data",
            "YouTube views",
            "Spotify popularity",
            "song recognition levels",
            "ensemble of models"
        ]
    },
    {
        "title": "Learning Complex Basis Functions for Invariant Representations of Audio.",
        "author": [
            "Stefan Lattner",
            "Monika D\u00f6rfler",
            "Andreas Arzt"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527906",
        "url": "https://doi.org/10.5281/zenodo.3527906",
        "ee": "http://archives.ismir.net/ismir2019/paper/000085.pdf",
        "abstract": "Learning features from data has shown to be more successful than using hand-crafted features for many machine learning tasks. In music information retrieval (MIR), features learned from windowed spectrograms are highly variant to transformations like transposition or time-shift. Such variances are undesirable when they are irrelevant for the respective MIR task. We propose an architecture called Complex Autoencoder (CAE) which learns features invariant to orthogonal transformations. Mapping signals onto complex basis functions learned by the CAE results in a transformation-invariant \"magnitude space\" and a transformation-variant \"phase space\". The phase space is useful to infer transformations between data pairs. When exploiting the invariance-property of the magnitude space, we achieve state-of-the-art results in audio-to-score alignment and in repeated section discovery for audio. A PyTorch implementation of the CAE, including the repeated section discovery method is available online.",
        "zenodo_id": 3527906,
        "dblp_key": "conf/ismir/LattnerDA19",
        "keywords": [
            "Learning features from data",
            "hand-crafted features",
            "machine learning tasks",
            "windowed spectrograms",
            "orthogonal transformations",
            "magnitude space",
            "phase space",
            "audio-to-score alignment",
            "repeated section discovery",
            "PyTorch implementation"
        ]
    },
    {
        "title": "Audio Query-based Music Source Separation.",
        "author": [
            "Jie Hwan Lee",
            "Hyeong-Seok Choi",
            "Kyogu Lee"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527954",
        "url": "https://doi.org/10.5281/zenodo.3527954",
        "ee": "http://archives.ismir.net/ismir2019/paper/000108.pdf",
        "abstract": "In recent years, music source separation has been one of the most intensively studied research areas in music information retrieval. Improvements in deep learning lead to a big progress in music source separation performance. However, most of the previous studies are restricted to separating a few limited number of sources, such as vocals, drums, bass, and other. In this study, we propose a network for audio query-based music source separation that can explicitly encode the source information from a query signal regardless of the number and/or kind of target signals. The proposed method consists of a Query-net and a Separator: given a query and a mixture, the Query-net encodes the query into the latent space, and the Separator estimates masks conditioned by the latent vector, which is then applied to the mixture for separation. The Separator can also generate masks using the latent vector from the training samples, allowing separation in the absence of a query. We evaluate our method on the MUSDB18 dataset, and experimental results show that the proposed method can separate multiple sources with a single network. In addition, through further investigation of the latent space we demonstrate that our method can generate continuous outputs via latent vector interpolation.",
        "zenodo_id": 3527954,
        "dblp_key": "conf/ismir/LeeCL19",
        "keywords": [
            "music source separation",
            "deep learning",
            "audio query-based",
            "explicit source information",
            "multiple sources",
            "latent space",
            "query-net",
            "Separator",
            "latent vector",
            "experimental results"
        ]
    },
    {
        "title": "Automatic Choreography Generation with Convolutional Encoder-decoder Network.",
        "author": [
            "Juheon Lee",
            "Seohyun Kim",
            "Kyogu Lee"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527958",
        "url": "https://doi.org/10.5281/zenodo.3527958",
        "ee": "http://archives.ismir.net/ismir2019/paper/000110.pdf",
        "abstract": "Automatic choreography generation is a challenging task because it often requires an understanding of two abstract concepts music and dance which are realized in the two different modalities, namely audio and video, respectively. In this paper, we propose a music-driven choreography generation system using an auto-regressive encoder-decoder network. To this end, we first collect a set of multimedia clips that include both music and corresponding dance motion. We then extract the joint coordinates of the dancer from video and the mel-spectrogram of music from audio and train our network using music-choreography pairs as input. Finally, a novel dance motion is generated at the inference time when only music is given as an input. We performed a user study for a qualitative evaluation of the proposed method, and the results show that the proposed model is able to generate musically meaningful and natural dance movements given an unheard song.",
        "zenodo_id": 3527958,
        "dblp_key": "conf/ismir/LeeKL19",
        "keywords": [
            "Automatic choreography generation",
            "Understanding music and dance",
            "Two different modalities",
            "Audio and video",
            "Auto-regressive encoder-decoder network",
            "Multimedia clips",
            "Joint coordinates of the dancer",
            "Mel-spectrogram of music",
            "Music-choreography pairs",
            "Novel dance motion"
        ]
    },
    {
        "title": "Learning a Joint Embedding Space of Monophonic and Mixed Music Signals for Singing Voice.",
        "author": [
            "Kyungyun Lee",
            "Juhan Nam"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527802",
        "url": "https://doi.org/10.5281/zenodo.3527802",
        "ee": "http://archives.ismir.net/ismir2019/paper/000034.pdf",
        "abstract": "Previous approaches in singer identification have used one of monophonic vocal tracks or mixed tracks containing multiple instruments, leaving a semantic gap between these two domains of audio. In this paper, we present a system to learn a joint embedding space of monophonic and mixed tracks for singing voice. We use a metric learning method, which ensures that tracks from both domains of the same singer are mapped closer to each other than those of different singers. We train the system on a large synthetic dataset generated by music mashup to reflect real-world music recordings. Our approach opens up new possibilities for cross-domain tasks, e.g., given a monophonic track of a singer as a query, retrieving mixed tracks sung by the same singer from the database. Also, it requires no additional vocal enhancement steps such as source separation. We show the effectiveness of our system for singer identification and singer-based music retrieval in both the in-domain and cross-domain tasks.",
        "zenodo_id": 3527802,
        "dblp_key": "conf/ismir/LeeN19",
        "keywords": [
            "monophonic",
            "mixed tracks",
            "semantic gap",
            "joint embedding space",
            "metric learning",
            "synthetic dataset",
            "music mashup",
            "cross-domain tasks",
            "singer identification",
            "singer-based music retrieval"
        ]
    },
    {
        "title": "Can We Listen To It Together?: Factors Influencing Reception of Music Recommendations and Post-Recommendation Behavior .",
        "author": [
            "Jin Ha Lee 0001",
            "Liz Pritchard",
            "Chris Hubbles"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527896",
        "url": "https://doi.org/10.5281/zenodo.3527896",
        "ee": "http://archives.ismir.net/ismir2019/paper/000080.pdf",
        "abstract": "Few prior studies on music recommendations investigate the context in which users receive the recommendations, and what impact the recommendation has on the user. In this paper, we aim to better understand the factors that affect people's decisions as to whether they choose to listen to music recommendations and how the recommendations impact their music-listening behaviors. We conducted an online survey asking about people's past experiences on giving and receiving music recommendations. We found that in addition to the aesthetic qualities of music and the respondent's taste, expectations regarding the delivery (e.g., timing, persistence) of the recommendations, familiarity, trust in the recommender's abilities, and the rationale for suggestions were important factors. We discuss the implications for the design of music recommenders based on the findings, including better rationale for and accessibility of recommended music, improved saving options, and more targeted delivery at specific times. The data also suggests disparities in how people wish to receive music recommendations and what will influence them to listen to recommendations, versus how they would like to offer recommendations to others. In addition, the findings highlight the importance of music recommendations in people's existing social relationships and their role in building/improving new relationships.",
        "zenodo_id": 3527896,
        "dblp_key": "conf/ismir/LeePH19",
        "keywords": [
            "context",
            "user experience",
            "recommendation impact",
            "factors affecting decisions",
            "music listening behaviors",
            "past experiences",
            "delivery expectations",
            "familiarity",
            "trust",
            "rationale for suggestions"
        ]
    },
    {
        "title": "Temporal Convolutional Networks for Speech and Music Detection in Radio Broadcast.",
        "author": [
            "Quentin Lemaire",
            "Andre Holzapfel"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527786",
        "url": "https://doi.org/10.5281/zenodo.3527786",
        "ee": "http://archives.ismir.net/ismir2019/paper/000026.pdf",
        "abstract": "The task of speech and music detection aims at the automatic annotation of potentially overlapping speech and music segments in audio recordings. This meta-data extraction process has important applications in royalty collection in broadcast audio. This study focuses on  deep neural network architectures made to process sequential data, and a series of recent architectures that have not yet been applied for this task are evaluated, extended and compared with a state-of-the-art architecture. Moreover, different training strategies are evaluated, and we demonstrate the advantages of a step-wise procedure that facilitates the combination heterogeneous datasets. The study shows that Temporal Convolution Network (TCN) architectures can outperform state-of-the-art architectures, and that especially the novel extension of non-causal TCN introduced in this paper leads to a significant improvement in the accuracy.",
        "zenodo_id": 3527786,
        "dblp_key": "conf/ismir/LemaireH19",
        "keywords": [
            "speech",
            "music",
            "automatic",
            "annotation",
            "audio",
            "recordings",
            "meta-data",
            "extraction",
            "royalty",
            "collection"
        ]
    },
    {
        "title": "Music Performance Analysis: A Survey.",
        "author": [
            "Alexander Lerch 0001",
            "Claire Arthur",
            "Ashis Pati",
            "Siddharth Gururani"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3932439",
        "url": "https://doi.org/10.5281/zenodo.3932439",
        "ee": "http://archives.ismir.net/ismir2019/paper/000002.pdf",
        "abstract": "This thesis aims to develop an automatic singing evaluation system specially suited to evaluate notes singing exercises. We build Hindustani singing datasets with a combined collection of audio samples of 349 reference and performance pairs. The samples are annotated with an overall grade based on pitch accuracy. For this purpose, we developed a GUI grading tool which provides a visual feedback for the performance. This tool helps reduce human biases in the grade annotations. The existing baseline system (BMCS) for singing assessment developed using Turkish Conservatory dataset (MAST dataset) is extensively studied to identify audio alignment as one of the possible areas of improvement. A methodology and appropriate metrics are devised to test the audio alignment performance. Using this methodology, different features are tested to demonstrate that an improved audio-to-audio alignment system can be achieved using a 120-dimensional HPCP feature. The second part of this study is focused on finding suitable features to assess a singing performance, given a good alignment of reference and student audio. A novel &#39;pitch-histogram cosine distance&#39; feature is devised to measure note-level accuracy of singing. The effectiveness of these features with respect to the baseline features is shown by linear regression models trained and tested using the Hindustani and MAST datasets. The effectiveness of &#39;pitch-histogram cosine distance&#39; is indicated by the low mean absolute errors and the interpretability of the linear models developed. These features are also used to provide a note-level accuracy visualization of student performance.",
        "zenodo_id": 3932439,
        "dblp_key": "conf/ismir/LerchAPG19",
        "keywords": [
            "Automatic singing evaluation system",
            "Hindustani singing datasets",
            "GUI grading tool",
            "audio alignment",
            "baseline system",
            "MAST dataset",
            "audio-to-audio alignment",
            "HPCP feature",
            "pitch-histogram cosine distance",
            "note-level accuracy"
        ]
    },
    {
        "title": "Query by Video: Cross-modal Music Retrieval.",
        "author": [
            "Bochen Li",
            "Aparna Kumar"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527882",
        "url": "https://doi.org/10.5281/zenodo.3527882",
        "ee": "http://archives.ismir.net/ismir2019/paper/000073.pdf",
        "abstract": "Cross-modal retrieval learns the relationship between the two types of data in a common space so that an input from one modality can retrieve data from a different modality. We focus on modeling the relationship between two highly diverse data, music and real-world videos. We learn cross-modal embeddings using a two-stream network trained with music-video pairs. Each branch takes one modality as the input and it is constrained with emotion tags. Then the constraints allow the cross-modal embeddings to be learned with significantly fewer music-video pairs. To retrieve music for an input video, the trained model ranks tracks in the music database by cross-modal distances to the query video. Quantitative evaluations show high accuracy of audio/video emotion tagging when evaluated on each branch independently and high performance for cross-modal music retrieval. We also present cross-modal music retrieval experiments on Spotify music using user-generated videos from Instagram and Youtube as queries, and subjective evaluations show that the proposed model can retrieve relevant music. We present the music retrieval results at: http://www.ece.rochester.edu/~bli23/projects/query.html.",
        "zenodo_id": 3527882,
        "dblp_key": "conf/ismir/LiK19",
        "keywords": [
            "Cross-modal retrieval",
            "relationship between data",
            "two types of data",
            "common space",
            "input from one modality",
            "different modality",
            "emotion tags",
            "cross-modal embeddings",
            "two-stream network",
            "music-video pairs"
        ]
    },
    {
        "title": "Improving Singing Aid System for Laryngectomees With Statistical Voice Conversion and VAE-SPACE.",
        "author": [
            "Li Li 0063",
            "Tomoki Toda",
            "Kazuho Morikawa",
            "Kazuhiro Kobayashi",
            "Shoji Makino"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527928",
        "url": "https://doi.org/10.5281/zenodo.3527928",
        "ee": "http://archives.ismir.net/ismir2019/paper/000096.pdf",
        "abstract": "This paper proposes an improved singing aid system for laryngectomees that converts electrolaryngeal (EL) speech produced using an electrolarynx to a more naturally sounding singing voice. Although the previously proposed system employing a noise suppression process and a rule-based pitch control approach has achieved preliminary success in converting EL speech into a singing voice, there are still two major limitations. First, the converted singing voice still sounds mechanical and unnatural owing to the adverse impacts of spectrograms extracted from EL speeches, also making the effect of pitch control limited. Second, the capability and flexibility of the rule-based pitch control in modeling various singing styles are insufficient, causing the converted singing voices to lack variety. To address these limitations, this paper proposes an improved system that uses 1) a statistical voice conversion approach to convert spectrograms extracted from EL speeches into those of natural speeches and 2) a deep generative model-based approach called VAE-SPACE for pitch modification, which generates pitch patterns in a data-driven manner instead of following manually designed rules. The experimental results revealed that 1) the conversion of spectrograms was effective in improving the naturalness of singing voices, and 2) the statistical pitch control approach was able to achieve comparable results with the rule-based approach, which was very carefully designed to be specialized in singing.",
        "zenodo_id": 3527928,
        "dblp_key": "conf/ismir/LiTMKM19",
        "keywords": [
            "electrolarynx",
            "electrolaryngeal speech",
            "singing aid system",
            "noise suppression",
            "rule-based pitch control",
            "spectrograms",
            "naturalness",
            "variety",
            "VAE-SPACE",
            "data-driven pitch modification"
        ]
    },
    {
        "title": "The ISMIR Explorer A Visual Interface for Exploring 20 Years of ISMIR Publications.",
        "author": [
            "Thomas Low",
            "Christian Hentschel",
            "Sayantan Polley",
            "Anustup Das",
            "Harald Sack",
            "Andreas N\u00fcrnberger",
            "Sebastian Stober"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527920",
        "url": "https://doi.org/10.5281/zenodo.3527920",
        "ee": "http://archives.ismir.net/ismir2019/paper/000092.pdf",
        "abstract": "Ever since the first International Symposium on Music Information Retrieval in 2000, the proceedings have been made publicly available to interested researchers. After 20 years of annual conferences and workshops, this number has grown to an impressive amount of almost 2,000 papers. When restricted to linear search and retrieval in a document collection of this size, it becomes inherently hard to identify topics, related work and trends in scientific research. Therefore, this paper presents and evaluates a map-based user interface for exploring 20 years of ISMIR publications. The interface visualizes k-nearest neighbor subsets of semantically similar papers. Users may jump from one neighborhood to the next by selecting another paper from the current subset. Through animated transitions between local k-nn maps, the interface creates the impression of panning a large global map. Evaluations results of a small user study suggest that users are able to discover interesting links between papers. Due to its generic approach, the interface is easily applicable to other document collections as well. The search interface and its source code will be made publicly available.",
        "zenodo_id": 3527920,
        "dblp_key": "conf/ismir/LowHPDSNS19",
        "keywords": [
            "International Symposium on Music Information Retrieval",
            "publicly available",
            "20 years",
            "document collection",
            "k-nearest neighbor subsets",
            "semantically similar papers",
            "user interface",
            "global map",
            "generic approach",
            "source code"
        ]
    },
    {
        "title": "Learning Disentangled Representations of Timbre and Pitch for Musical Instrument Sounds Using Gaussian Mixture Variational Autoencoders.",
        "author": [
            "Yin-Jyun Luo",
            "Kat Agres",
            "Dorien Herremans"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527918",
        "url": "https://doi.org/10.5281/zenodo.3527918",
        "ee": "http://archives.ismir.net/ismir2019/paper/000091.pdf",
        "abstract": "In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model's efficacy using latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high F-scores when tested on our synthesized sounds, which verifies the model's performance of controllable realistic timbre/pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single encoder-decoder architecture, which is evaluated by measuring the shift in the posterior of instrument classification. Our in-depth evaluation confirms the model's ability to successfully disentangle timbre and pitch.",
        "zenodo_id": 3527918,
        "dblp_key": "conf/ismir/LuoAH19",
        "keywords": [
            "disentangled representations",
            "timbre",
            "pitch",
            "musical instrument sounds",
            "variational autoencoders",
            "Gaussian mixture latent distributions",
            "latent space visualization",
            "quantitative analysis",
            "instrument identity",
            "pitch"
        ]
    },
    {
        "title": "Mosaic Style Transfer Using Sparse Autocorrelograms.",
        "author": [
            "Daniel MacKinlay",
            "Zdravko I. Botev"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527956",
        "url": "https://doi.org/10.5281/zenodo.3527956",
        "ee": "http://archives.ismir.net/ismir2019/paper/000109.pdf",
        "abstract": "We introduce a novel mosaic synthesis algorithm for musical style transfer using the autocorrelogram as a feature map. We decompose the autocorrelogram feature map sparsely in a decaying sinusoid basis, using that decomposition as an interpolation scheme in feature space. This efficiently provides gradient information in the mosaicing optimization, including of the challenging time-scale parameters which are usually intractable for discretely sampled signals. The required calculations are straightforward to parallelize on vector-processing hardware. Our implementation of the method provides good quality output and novel musical effects in example tasks by itself and can also be integrated into alternative mosaicing methods.",
        "zenodo_id": 3527956,
        "dblp_key": "conf/ismir/MacKinlayB19",
        "keywords": [
            "mosaic synthesis",
            "musical style transfer",
            "autocorrelogram",
            "feature map",
            "sparsely decomposed",
            "decaying sinusoid basis",
            "interpolation scheme",
            "feature space",
            "gradient information",
            "time-scale parameters"
        ]
    },
    {
        "title": "DeepSRGM Sequence Classification and Ranking in Indian Classical Music Via Deep Learning.",
        "author": [
            "Sathwik Tejaswi Madhusudhan",
            "Girish Chowdhary 0001"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527862",
        "url": "https://doi.org/10.5281/zenodo.3527862",
        "ee": "http://archives.ismir.net/ismir2019/paper/000064.pdf",
        "abstract": "A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre-possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on theComp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music database that are closely related to the presented query sequence.",
        "zenodo_id": 3527862,
        "dblp_key": "conf/ismir/Madhusudhan019",
        "keywords": [
            "Raga",
            "melodic framework",
            "music information retrieval",
            "downstream applications",
            "music recommendations",
            "organizing music collections",
            "deep learning",
            "Long Short Term Memory",
            "Recurrent Neural Networks",
            "state-of-the-art"
        ]
    },
    {
        "title": "Rendering Music Performance With Interpretation Variations Using Conditional Variational RNN.",
        "author": [
            "Akira Maezawa",
            "Kazuhiko Yamamoto",
            "Takuya Fujishima"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527948",
        "url": "https://doi.org/10.5281/zenodo.3527948",
        "ee": "http://archives.ismir.net/ismir2019/paper/000105.pdf",
        "abstract": "Capturing and generating a wide variety of musical expression is important in music performance rendering, but current methods fail to model such a variation.  This paper presents a music performance rendering method that could explicitly model differences in interpretations for a given piece of music.  Conditional variational auto-encoder is used to jointly train, conditioned on the music score, an encoder from performance to a latent code and a decoder from the latent code to music performance.  Evaluation demonstrates the method is capable of generating a wide variety of human-like expressive music performances as the latent code is varied.",
        "zenodo_id": 3527948,
        "dblp_key": "conf/ismir/MaezawaYF19",
        "keywords": [
            "explicitly model differences",
            "given piece of music",
            "explicitly model differences",
            "music performance rendering",
            "jointly train",
            "encoder from performance",
            "decoder from latent code",
            "latent code",
            "wide variety of human-like expressive music performances",
            "evaluation demonstrates"
        ]
    },
    {
        "title": "SAMBASET: A Dataset of Historical Samba de Enredo Recordings for Computational Music Analysis.",
        "author": [
            "Lucas Maia",
            "Magdalena Fuentes",
            "Luiz W. P. Biscainho",
            "Mart\u00edn Rocamora",
            "Slim Essid"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527888",
        "url": "https://doi.org/10.5281/zenodo.3527888",
        "ee": "http://archives.ismir.net/ismir2019/paper/000076.pdf",
        "abstract": "In the last few years, several datasets have been released to meet the requirements of \"hungry\" yet promising data-driven approaches in modern music technology research. Since, for historical reasons, most investigations conducted in the field still revolve around music of the so-called \"Western\" tradition, the corresponding data, methodology and conclusions carry a strong cultural bias. Music of non-\"Western\" background, whenever present, is usually underrepresented, poorly labeled, or even mislabeled, the exception being projects that aim at specifically describing such music. In this paper we present SAMBASET, a dataset of Brazilian samba music that contains over 40 hours of historical and modern samba de enredo commercial recordings. To the best of our knowledge, this is the first dataset of this genre. We describe the collection of metadata (e.g. artist, composer, release date) and outline our semiautomatic approach to the challenging task of annotating beats in this large dataset, which includes the assessment of the performance of state-of-the-art beat tracking algorithms for this specific case. Finally, we present a study on tempo and beat tracking that illustrates SAMBASET's value, and we comment on other tasks for which it could be used.",
        "zenodo_id": 3527888,
        "dblp_key": "conf/ismir/MaiaFBRE19",
        "keywords": [
            "Brazilian samba music",
            "SAMBASET dataset",
            "over 40 hours of recordings",
            "first dataset of its kind",
            "metadata collection",
            "semiautomatic approach",
            "state-of-the-art beat tracking",
            "tempo and beat tracking study",
            "value of the dataset",
            "other potential uses"
        ]
    },
    {
        "title": "Improving Structure Evaluation Through Automatic Hierarchy Expansion.",
        "author": [
            "Brian McFee",
            "Katherine M. Kinnaird"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527764",
        "url": "https://doi.org/10.5281/zenodo.3527764",
        "ee": "http://archives.ismir.net/ismir2019/paper/000016.pdf",
        "abstract": "Structural segmentation is the task of partitioning a recording into non-overlapping time intervals, and labeling each segment with an identifying marker such as A, B, or verse. Hierarchical structure annotation expands this idea to allow an annotator to segment a song with multiple levels of granularity. While there has been recent progress in developing evaluation criteria for comparing two hierarchical annotations of the same recording, the existing methods have known deficiencies when dealing with inexact label matchings and sequential label repetition.  In this article, we investigate methods for automatically enhancing structural annotations by inferring (and expanding) hierarchical information from the segment labels. The proposed method complements existing techniques for comparing hierarchical structural annotations by coarsening or refining labels with variation markers to either collapse similarly labeled segments together, or separate identically labeled segments from each other. Using the multi-level structure annotations provided in the SALAMI dataset, we demonstrate that automatic hierarchy expansion allows structure comparison methods to more accurately assess similarity between annotations.",
        "zenodo_id": 3527764,
        "dblp_key": "conf/ismir/McFeeK19",
        "keywords": [
            "Structural segmentation",
            "partitioning recording",
            "labeling segments",
            "identifying markers",
            "Hierarchical structure annotation",
            "multiple levels of granularity",
            "evaluation criteria",
            "inexact label matchings",
            "sequential label repetition",
            "automatically enhancing annotations"
        ]
    },
    {
        "title": "Conditioned-U-Net: Introducing a Control Mechanism in the U-Net for Multiple Source Separations.",
        "author": [
            "Gabriel Meseguer-Brocal",
            "Geoffroy Peeters"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527766",
        "url": "https://doi.org/10.5281/zenodo.3527766",
        "ee": "http://archives.ismir.net/ismir2019/paper/000017.pdf",
        "abstract": "Data-driven models for audio source separation such as U-Net or Wave-U-Net are usually models dedicated to and specifically trained for a single task, e.g. a particular instrument isolation. Training them for various tasks at once commonly results in worse performances than training them for a single specialized task. In this work, we introduce the Conditioned-U-Net (C-U-Net) which adds a control mechanism to the standard U-Net. The control mechanism allows us to train a unique and generic U-Net to perform the separation of various instruments. The C-U-Net decides the instrument to isolate according to a one-hot-encoding input vector. The input vector is embedded to obtain the parameters that control Feature-wise Linear Modulation (FiLM) layers. FiLM layers modify the U-Net feature maps in order to separate the desired instrument via affine transformations. The C-U-Net performs different instrument separations, all with a single model achieving the same performances as the dedicated ones at a lower cost.",
        "zenodo_id": 3527766,
        "dblp_key": "conf/ismir/Meseguer-Brocal19",
        "keywords": [
            "Data-driven models",
            "audio source separation",
            "U-Net",
            "Wave-U-Net",
            "training for various tasks",
            "worse performances",
            "Conditioned-U-Net (C-U-Net)",
            "control mechanism",
            "Feature-wise Linear Modulation (FiLM)",
            "instrument isolation"
        ]
    },
    {
        "title": "FMP Notebooks: Educational Material for Teaching and Learning Fundamentals of Music Processing.",
        "author": [
            "Meinard M\u00fcller",
            "Frank Zalkow"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.10663901",
        "url": "https://doi.org/10.5281/zenodo.10663901",
        "ee": "http://archives.ismir.net/ismir2019/paper/000069.pdf",
        "abstract": "libfmp Python package for teaching and learning Fundamentals of Music Processing (FMP)",
        "zenodo_id": 10663901,
        "dblp_key": "conf/ismir/MullerZ19"
    },
    {
        "title": "The Harmonix Set: Beats, Downbeats, and Functional Segment Annotations of Western Popular Music.",
        "author": [
            "Oriol Nieto",
            "Matthew C. McCallum",
            "Matthew E. P. Davies",
            "Andrew Robertson",
            "Adam M. Stark",
            "Eran Egozy"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527870",
        "url": "https://doi.org/10.5281/zenodo.3527870",
        "ee": "http://archives.ismir.net/ismir2019/paper/000068.pdf",
        "abstract": "We introduce the Harmonix set: a collection of annotations of beats, downbeats, and functional segmentation for over 900 full tracks that covers a wide range of western popular music. Given the variety of annotated music information types in this set, and how strongly these three types of data are typically intertwined, we seek to foster research that focuses on multiple retrieval tasks at once. The dataset includes additional metadata such as MusicBrainz identifiers to support the linking of the dataset to third-party information or audio data when available. We describe the methodology employed in acquiring this set, including the annotation process and song selection. In addition, an initial data exploration of the annotations and actual dataset content is conducted. Finally, we provide a series of baselines of the Harmonix set with reference beat-trackers, downbeat estimation, and structural segmentation algorithms.",
        "zenodo_id": 3527870,
        "dblp_key": "conf/ismir/NietoMDRSE19",
        "keywords": [
            "Harmonix set",
            "annotations of beats",
            "downbeats",
            "functional segmentation",
            "western popular music",
            "multiple retrieval tasks",
            "MusicBrainz identifiers",
            "acquisition methodology",
            "data exploration",
            "baselines"
        ]
    },
    {
        "title": "Contributing to New Musicological Theories with Computational Methods: The Case of Centonization in Arab-Andalusian Music.",
        "author": [
            "Thomas Nuttall",
            "Miguel Garc\u00eda-Casado",
            "V\u00edctor N\u00fa\u00f1ez-Tarifa",
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527784",
        "url": "https://doi.org/10.5281/zenodo.3527784",
        "ee": "http://archives.ismir.net/ismir2019/paper/000025.pdf",
        "abstract": "Arab-Andalusian music was formed in the medieval Islamic territories of Iberian Peninsula, drawing on local traditions and assuming Arabic influences. The expert performer and researcher of the Moroccan tradition of this music, Amin Chaachoo, is developing a theory, whose last formulation was recently published in La Mu-sique Hispano-Arabe, al-Ala (2016), which argues that centonization, a melodic composition technique used in Gregorian chant, was also utilized for the creation of this repertoire. In this paper we aim to contribute to Chaachoo's theory by means of tf-idf analysis. A highorder n-gram model is applied to a corpus of 149 prescriptive transcriptions of heterophonic recordings, representing each as an unordered multiset of patterns. Computing the tf-idf statistic of each pattern in this corpus provides a means by which we can rank and compare motivic content across nawab\u0101t, distinct musical forms of the tradition. For each nawba, an empirical comparison is made between patterns identified as significant via our approach and those proposed by Chaachoo. Ultimately we observe considerable agreement between the two pattern sets and go further in proposing new, unique and as yet undocumented patterns that occur at least as frequently and with at least as much importance as those in Chaachoo's proposals.",
        "zenodo_id": 3527784,
        "dblp_key": "conf/ismir/NuttallGNRS19",
        "keywords": [
            "Arab-Andalusian music",
            "medieval Islamic territories",
            "Iberian Peninsula",
            "local traditions",
            "Arabic influences",
            "expert performer",
            "researcher",
            "Moroccan tradition",
            "centonization",
            "Gregorian chant"
        ]
    },
    {
        "title": "Learning Notation Graph Construction for Full-Pipeline Optical Music Recognition.",
        "author": [
            "Alexander Pacha",
            "Jorge Calvo-Zaragoza",
            "Jan Hajic Jr."
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527744",
        "url": "https://doi.org/10.5281/zenodo.3527744",
        "ee": "http://archives.ismir.net/ismir2019/paper/000006.pdf",
        "abstract": "Optical Music Recognition (OMR) promises great benefits to Music Information Retrieval by reducing the costs of making sheet music available in a symbolic format. Recent advances in deep learning have turned typical OMR obstacles into clearly solvable problems, especially the stages that visually process the input image, such as staff line removal or detection of music-notation objects. However, merely detecting objects is not enough for retrieving the actual content, as music notation is a configurational writing system where the semantic of a primitive is defined by its relationship to other primitives. Thus, OMR systems must employ a notation assembly stage to infer such relationships among the detected objects. So far, this stage has been addressed by devising a set of predefined rules or grammars, which hardly generalize well. In this work, we formulate the notation assembly stage from a set of detected primitives as a machine learning problem. Our notation assembly is modeled as a graph that stores syntactic relationships among primitives, which allows us to capture the configuration of symbols in a music-notation document. Our results over the handwritten sheet music corpus MUSCIMA++ show 95.2% precision, 96.0% recall, and an F-score of 95.6% in establishing the correct syntactic relationships. When inferring relationships on data from a music object detector, the model achieves 93.2% precision, 91.5% recall and an F-score of 92.3%.",
        "zenodo_id": 3527744,
        "dblp_key": "conf/ismir/PachaCH19",
        "keywords": [
            "Optical Music Recognition",
            "Music Information Retrieval",
            "Sheet music",
            "Symbolic format",
            "Deep learning",
            "Staff line removal",
            "Music-notation objects",
            "Notation assembly stage",
            "Machine learning problem",
            "Graph model"
        ]
    },
    {
        "title": "A Diplomatic Edition of Il Lauro Secco: Ground Truth for OMR of White Mensural Notation.",
        "author": [
            "Emilia Parada-Cabaleiro",
            "Anton Batliner",
            "Bj\u00f6rn W. Schuller"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527868",
        "url": "https://doi.org/10.5281/zenodo.3527868",
        "ee": "http://archives.ismir.net/ismir2019/paper/000067.pdf",
        "abstract": "Early musical sources in white mensural notation\u2014the most common notation in European printed music during the Renaissance\u2014are nowadays preserved by libraries worldwide trough digitalisation. Still, the application of music information retrieval to this repertoire is restricted by the use of digitalisation techniques which produce an uncodified output. Optical Music Recognition (OMR) automatically generates a symbolic representation of image-based musical content, thus making this repertoire reachable from the computational point of view; yet, further improvements are often constricted by the limited ground truth available. We address this lacuna by presenting a symbolic representation in original notation of Il Lauro Secco, an anthology of Italian madrigals in white mensural notation. For musicological analytic purposes, we encoded the repertoire in **mens and MEI formats; for OMR ground truth, we automatically codified the repertoire in agnostic and semantic formats, via conversion from the **mens files.",
        "zenodo_id": 3527868,
        "dblp_key": "conf/ismir/Parada-Cabaleiro19",
        "keywords": [
            "digitalisation",
            "music information retrieval",
            "white mensural notation",
            "library preservation",
            "Renaissance music",
            "computational reach",
            "optical music recognition",
            "symbolic representation",
            "mens and MEI formats",
            "agnostic and semantic formats"
        ]
    },
    {
        "title": "A Bi-Directional Transformer for Musical Chord Recognition.",
        "author": [
            "Jonggwon Park",
            "Kyoyun Choi",
            "Sungwook Jeon",
            "Dokyun Kim",
            "Jonghun Park"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527886",
        "url": "https://doi.org/10.5281/zenodo.3527886",
        "ee": "http://archives.ismir.net/ismir2019/paper/000075.pdf",
        "abstract": "Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model.  In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance.",
        "zenodo_id": 3527886,
        "dblp_key": "conf/ismir/ParkCJKP19",
        "keywords": [
            "Chord recognition",
            "Machine learning models",
            "Convolutional neural networks",
            "Recurrent neural networks",
            "Self-attention mechanism",
            "Bi-directional Transformer",
            "Attention map analysis",
            "Adaptive receptive field",
            "Long-term dependencies",
            "Essential information"
        ]
    },
    {
        "title": "A Cross-Scape Plot Representation for Visualizing Symbolic Melodic Similarity.",
        "author": [
            "Saebyul Park",
            "Taegyun Kwon",
            "Jongpil Lee",
            "Jeounghoon Kim",
            "Juhan Nam"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527834",
        "url": "https://doi.org/10.5281/zenodo.3527834",
        "ee": "http://archives.ismir.net/ismir2019/paper/000050.pdf",
        "abstract": "Symbolic melodic similarity is based on measuring a pairwise distance between two songs from diverse perspectives. The distance is usually summarized as a single value for song retrieval. This obscures observing the details of similarity patterns within the two songs. In this paper, we propose a cross-scape plot representation to visualize multi-scaled melody similarity between two symbolic music. The cross-scape plot is computed by stacking up a minimum local distance between two segments from each of the two songs. As the layer goes up, the segment size increases and it computes incrementally more long-term distances. This hierarchical representation allows for capturing the location and length of similar segments between two songs in a visually intuitive manner. We show the effectiveness of the cross-scape plot by evaluating it on examples from folk music collections with similarity-based categories and plagiarism cases.",
        "zenodo_id": 3527834,
        "dblp_key": "conf/ismir/ParkKLKN19",
        "keywords": [
            "symbolic",
            "melodic",
            "similarity",
            "pairwise",
            "distance",
            "song",
            "retrieval",
            "cross-scape",
            "plot",
            "multi-scaled"
        ]
    },
    {
        "title": "Tunes Together: Perception and Experience of Collaborative Playlists.",
        "author": [
            "So Yeon Park",
            "Audrey Laplante",
            "Jin Ha Lee 0001",
            "Blair Kaneshiro"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527912",
        "url": "https://doi.org/10.5281/zenodo.3527912",
        "ee": "http://archives.ismir.net/ismir2019/paper/000088.pdf",
        "abstract": "Music is well established as a means of social connection. In the age of streaming platforms, personalized playlists and recommendations are popular topics in music information retrieval. We bring the focus of music enjoyment back to social connection and examine how technologies can enhance interpersonal relationships, specifically through the context of the collaborative playlist (CP). We conducted an exploratory study of CP users and non-users (N=65) and examined speculative and experienced purposes and outcomes of CPs, as well as general perspectives on music and social connectedness. We derived a CP Framework with three purposes Practical, Cognitive, and Social and two connotations Utility and Orientation. Both users and non-users shared similar perspectives on music-related activities and CP user outcomes. Projected and actual CP purposes differed between groups, however, as did perception of music's role in connectedness in recent years. These results highlight the importance of music-based social interactions for both groups.",
        "zenodo_id": 3527912,
        "dblp_key": "conf/ismir/ParkLLK19",
        "keywords": [
            "Music",
            "social connection",
            "streaming platforms",
            "personalized playlists",
            "recommendations",
            "collaborative playlist",
            "CP users",
            "CP non-users",
            "exploratory study",
            "CP Framework"
        ]
    },
    {
        "title": "Evolution of the Informational Complexity of Contemporary Western Music.",
        "author": [
            "Thomas Parmer",
            "Yong-Yeol Ahn"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527772",
        "url": "https://doi.org/10.5281/zenodo.3527772",
        "ee": "http://archives.ismir.net/ismir2019/paper/000019.pdf",
        "abstract": "We measure the complexity of songs in the Million Song Dataset (MSD) in terms of pitch, timbre, loudness, and rhythm to investigate their evolution from 1960 to 2010. By comparing the Billboard Hot 100 with random samples, we find that the complexity of popular songs tends to be more narrowly distributed around the mean, supporting the idea of an inverted U-shaped relationship between complexity and hedonistic value. We then examine the temporal evolution of complexity, reporting consistent changes across decades, such as a decrease in average loudness complexity since the 1960s, and an increase in timbre complexity overall but not for popular songs. We also show, in contrast to claims that popular songs sound more alike over time, that they are not more similar than they were 50 years ago in terms of pitch or rhythm, although similarity in timbre shows distinctive patterns across eras and similarity in loudness has been increasing. Finally, we show that musical genres can be differentiated by their distinctive complexity profiles.",
        "zenodo_id": 3527772,
        "dblp_key": "conf/ismir/ParmerA19",
        "keywords": [
            "complexity",
            "pitch",
            "timbre",
            "loudness",
            "rhythm",
            "Billboard Hot 100",
            "inverted U-shaped relationship",
            "decades",
            "average loudness complexity",
            "timbre complexity"
        ]
    },
    {
        "title": "Learning to Traverse Latent Spaces for Musical Score Inpainting.",
        "author": [
            "Ashis Pati",
            "Alexander Lerch 0001",
            "Ga\u00ebtan Hadjeres"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527814",
        "url": "https://doi.org/10.5281/zenodo.3527814",
        "ee": "http://archives.ismir.net/ismir2019/paper/000040.pdf",
        "abstract": "Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful inpaintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models.",
        "zenodo_id": 3527814,
        "dblp_key": "conf/ismir/PatiLH19",
        "keywords": [
            "Music Inpainting",
            "interactive music creation",
            "deep learning-based approach",
            "musical score inpainting",
            "latent space",
            "Recurrent Neural Network",
            "musically meaningful manner",
            "latent spaces",
            "deep generative models",
            "interactive music creation applications"
        ]
    },
    {
        "title": "20 Years of Automatic Chord Recognition from Audio.",
        "author": [
            "Johan Pauwels",
            "Ken O&apos;Hanlon",
            "Emilia G\u00f3mez",
            "Mark B. Sandler"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527739",
        "url": "https://doi.org/10.5281/zenodo.3527739",
        "ee": "http://archives.ismir.net/ismir2019/paper/000004.pdf",
        "abstract": "In 1999, Fujishima published \"Realtime Chord Recognition of Musical Sound: a System using Common Lisp Music\". This paper kickstarted an active research topic that has been popular in and around the ISMIR community. The field of Automatic Chord Recognition (ACR) has evolved considerably from early knowledge-based systems towards data-driven methods, with neural network approaches arguably being central to current ACR research. Nonetheless, many of its core issues were already addressed or referred to in the Fujishima paper. In this paper, we review those twenty years of ACR according to these issues. We furthermore attempt to frame current directions in the field in order to establish some perspective for future research.",
        "zenodo_id": 3527739,
        "dblp_key": "conf/ismir/PauwelsOGS19",
        "keywords": [
            "Fujishimas 1999 paper",
            "Realtime Chord Recognition",
            "ISMIR community",
            "Automatic Chord Recognition",
            "Data-driven methods",
            "Neural network approaches",
            "Core issues addressed",
            "Twenty years of evolution",
            "Current directions",
            "Perspective for future research"
        ]
    },
    {
        "title": "20 Years of Playlists: A Statistical Analysis on Popularity and Diversity.",
        "author": [
            "Lorenzo Porcaro",
            "Emilia G\u00f3mez"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527758",
        "url": "https://doi.org/10.5281/zenodo.3527758",
        "ee": "http://archives.ismir.net/ismir2019/paper/000013.pdf",
        "abstract": "Grouping songs together, according to music preferences, mood or other characteristics, is an activity which reflects personal listening behaviours and tastes. In the last two decades, due to the increasing size of music catalogues accessible and to improvements of recommendation algorithms, people have been exposed to new ways for creating playlists. In this work, through the statistical analysis of more than 400K playlists from four datasets, created in different temporal and technological contexts, we aim to understand if it is possible to extract information about the evolution of humans strategies for playlist creation. We focus our analysis on two driving concepts of the Music Information Retrieval literature: popularity and diversity.",
        "zenodo_id": 3527758,
        "dblp_key": "conf/ismir/PorcaroG19",
        "keywords": [
            "grouping",
            "songs",
            "according",
            "music",
            "preferences",
            "mood",
            "characteristics",
            "personal",
            "listening",
            "behaviours"
        ]
    },
    {
        "title": "Pattern Clustering in Monophonic Music by Learning a Non-Linear Embedding From Human Annotations.",
        "author": [
            "Timothy de Reuse",
            "Ichiro Fujinaga"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527922",
        "url": "https://doi.org/10.5281/zenodo.3527922",
        "ee": "http://archives.ismir.net/ismir2019/paper/000093.pdf",
        "abstract": "Musical pattern discovery algorithms find instances of repetition in symbolic music, allowing for some user-specifiable amount of variation between identified repetitions; however, they can yield an intractably large number of discovered patterns when allowing for even small amounts of variation. This is commonly addressed by defining some heuristic notion of pattern significance, and returning only the most significant patterns. This paper develops a method of pattern discovery that models human judgement of what constitutes a significant pattern by incorporating annotations of repeated patterns, avoiding the need to design heuristics.  We take pattern discovery as a clustering task, where the input is a set of passages of monophonic music, represented as vectors of extracted features, and the output clusters correspond to discovered patterns. The human annotations are used to train a neural network to learn a low-dimensional embedding of the feature space that maps passages of music close together when they are occurrences of the same ground-truth pattern. The results of this approach match up with the annotations significantly better than the results of an approach using clustering without subspace learning. We provide examples of the types of patterns that this method tends to discover and discuss its feasibility and practicality as a tool for extracting useful information about repetitive structure in music.",
        "zenodo_id": 3527922,
        "dblp_key": "conf/ismir/ReuseF19",
        "keywords": [
            "Musical pattern discovery",
            "repetition identification",
            "symbolic music",
            "user-specified variation",
            "intractable number of patterns",
            "heuristic notion of significance",
            "pattern clustering task",
            "low-dimensional embedding",
            "passages of monophonic music",
            "feature space mapping"
        ]
    },
    {
        "title": "Scalable Searching and Ranking for Melodic Pattern Queries.",
        "author": [
            "Philippe Rigaux",
            "Nicolas Travers"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527932",
        "url": "https://doi.org/10.5281/zenodo.3527932",
        "ee": "http://archives.ismir.net/ismir2019/paper/000098.pdf",
        "abstract": "We present the design and implementation of a scalable search engine for large Digital Score Libraries. It covers the core features expected from an information retrieval system. Music representation is pre-processed, simplified and normalized. Collections are searched for scores that match a melodic pattern, results are ranked on their similarity with the pattern, and matching fragments are finally identified on the fly. Moreover, all these features are designed to be integrated in a standard search engine and thus benefit from the horizontal scalability of such systems. Our method is fully implemented, and relies on Elasticsearch for collection indexing. We describe its main components, report and study its performances.",
        "zenodo_id": 3527932,
        "dblp_key": "conf/ismir/RigauxT19",
        "keywords": [
            "Digital Score Libraries",
            "Scalable Search Engine",
            "Information Retrieval System",
            "Music Representation",
            "Melodic Pattern Matching",
            "Search Engine Scalability",
            "Elasticsearch",
            "Performance Study",
            "Scoring Engine",
            "Fragment Identification"
        ]
    },
    {
        "title": "A Holistic Approach to Polyphonic Music Transcription with Neural Networks.",
        "author": [
            "Miguel A. Rom\u00e1n",
            "Antonio Pertusa",
            "Jorge Calvo-Zaragoza"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527914",
        "url": "https://doi.org/10.5281/zenodo.3527914",
        "ee": "http://archives.ismir.net/ismir2019/paper/000089.pdf",
        "abstract": "We present a framework based on neural networks to extract music scores directly from polyphonic audio in an end-to-end fashion. Most previous Automatic Music Transcription (AMT) methods seek a piano-roll representation of the pitches, that can be further transformed into a score by incorporating tempo estimation, beat tracking, key estimation or rhythm quantization. Unlike these methods, our approach generates music notation directly from the input audio in a single stage. For this, we use a Convolutional Recurrent Neural Network (CRNN) with Connectionist Temporal Classification (CTC) loss function which does not require annotated alignments of audio frames with the score rhythmic information. We trained our model using as input Haydn, Mozart, and Beethoven string quartets and Bach chorales synthesized with different tempos and expressive performances. The output is a textual representation of four-voice music scores based on **kern format. Although the proposed approach is evaluated in a simplified scenario, results show that this model can learn to transcribe scores directly from audio signals, opening a promising avenue towards complete AMT.",
        "zenodo_id": 3527914,
        "dblp_key": "conf/ismir/RomanPC19",
        "keywords": [
            "neural networks",
            "directly from polyphonic audio",
            "end-to-end fashion",
            "Automatic Music Transcription",
            "piano-roll representation",
            "score by incorporating",
            "tempo estimation",
            "beat tracking",
            "key estimation",
            "rhythm quantization"
        ]
    },
    {
        "title": "Detecting Stable Regions in Frequency Trajectories for Tonal Analysis of Traditional Georgian Vocal Music.",
        "author": [
            "Sebastian Rosenzweig",
            "Frank Scherbaum",
            "Meinard M\u00fcller"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527816",
        "url": "https://doi.org/10.5281/zenodo.3527816",
        "ee": "http://archives.ismir.net/ismir2019/paper/000041.pdf",
        "abstract": "While Georgia has a long history of orally transmitted polyphonic singing, there is still an ongoing controversial discussion among ethnomusicologists on the tuning system underlying this type of music. First attempts have been made to analyze tonal properties (e. g., harmonic and melodic intervals) based on fundamental frequency (F0) trajectories. One major challenge in F0-based tonal analysis is introduced by unstable regions in the trajectories due to pitch slides and other frequency fluctuations. In this paper, we describe two approaches for detecting stable regions in frequency trajectories: the first algorithm uses morphological operations inspired by image processing, and the second one is based on suitably defined binary time\u2013frequency masks. To avoid undesired distortions in subsequent analysis steps, both approaches keep the original F0-values unmodified, while only removing F0-values in unstable trajectory regions. We evaluate both approaches against manually annotated stable regions and discuss their potential in the context of interval analysis for traditional three-part Georgian singing.",
        "zenodo_id": 3527816,
        "dblp_key": "conf/ismir/RosenzweigSM19",
        "keywords": [
            "oral transmission",
            "polyphonic singing",
            "controversial discussion",
            "tuning system",
            "tonal properties",
            "fundamental frequency",
            "pitch slides",
            "frequency fluctuations",
            "stable regions",
            "morphological operations"
        ]
    },
    {
        "title": "SUPRA: Digitizing the Stanford University Piano Roll Archive.",
        "author": [
            "Zhengshan Shi",
            "Craig Sapp",
            "Kumaran Arul",
            "Jerry McBride",
            "Julius O. Smith III"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527858",
        "url": "https://doi.org/10.5281/zenodo.3527858",
        "ee": "http://archives.ismir.net/ismir2019/paper/000062.pdf",
        "abstract": "This paper describes the digitization process of a large collection of historical piano roll recordings held in the Stanford University Piano Roll Archive (SUPRA), which has resulted in an initial dataset of 478 performances of pianists from the early twentieth century transcribed to MIDI format. The process includes scanning paper rolls, digitizing the hole punches, and translating the pneumatic expression codings into MIDI format to create expressive performance files. We offer derivative files from each step of this process, including a high resolution image of the roll, a \"raw\" MIDI file of hole data, an \"expressive\" MIDI file that translates hole data into dynamics, and an audio file rendering of the expressive MIDI file on a digital piano sample. This provides digital access to the rolls for researchers in a flexible, searchable online database. We currently offer an initial dataset, \"SUPRA-RW\" from a selection of \"red Welte\"-type rolls in the SUPRA. This dataset provides roll scans and MIDI transcriptions of important historical piano performances, many being made available widely for the first time.",
        "zenodo_id": 3527858,
        "dblp_key": "conf/ismir/ShiSAMS19",
        "keywords": [
            "digitization",
            "historical piano roll recordings",
            "Stanford University Piano Roll Archive",
            "initial dataset",
            "478 performances",
            "pianists from the early twentieth century",
            "MIDI format",
            "scanning paper rolls",
            "digitizing hole punches",
            "translating pneumatic expression codings"
        ]
    },
    {
        "title": "Statistical Music Structure Analysis Based on a Homogeneity-, Repetitiveness-, and Regularity-Aware Hierarchical Hidden Semi-Markov Model.",
        "author": [
            "Go Shibata",
            "Ryo Nishikimi",
            "Eita Nakamura",
            "Kazuyoshi Yoshii"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527796",
        "url": "https://doi.org/10.5281/zenodo.3527796",
        "ee": "http://archives.ismir.net/ismir2019/paper/000031.pdf",
        "abstract": "This paper describes a music structure analysis method that splits music audio signals into meaningful segments such as musical sections and clusters them. In this task, how to model the four fundamental aspects of musical sections, i.e., homogeneity, repetitiveness, novelty, and regularity, in a unified way is still an open problem. Here we propose a solid statistical approach based on a homogeneity-, repetitiveness-, and regularity-aware hierarchical hidden semi-Markov model. The higher-level semi-Markov chain represents a sequence of sections that tend to have regularly spaced boundaries. The timbral features in each section are assumed to follow emission distributions that are homogeneous over time. The lower-level left-to-right Markov chain in each section represents a chord sequence whose sequential order is constrained to be a repetition of a chord sequence in another section of the same cluster. The whole model can be trained unsupervisedly based on Bayesian sparse learning where unnecessary sections automatically degenerate. The proposed method outperformed representative methods in segmentation and clustering accuracies with estimated sections having similar statistical properties as the ground truth data.",
        "zenodo_id": 3527796,
        "dblp_key": "conf/ismir/ShibataNNY19",
        "keywords": [
            "music structure analysis",
            "musical sections",
            "clusters",
            "homogeneity",
            "repetitiveness",
            "novelty",
            "regularity",
            "hierarchical hidden semi-Markov model",
            "timbral features",
            "emission distributions"
        ]
    },
    {
        "title": "A Convolutional Approach to Melody Line Identification in Symbolic Scores.",
        "author": [
            "Federico Simonetta",
            "Carlos Eduardo Cancino Chac\u00f3n",
            "Stavros Ntalampiras",
            "Gerhard Widmer"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527966",
        "url": "https://doi.org/10.5281/zenodo.3527966",
        "ee": "http://archives.ismir.net/ismir2019/paper/000114.pdf",
        "abstract": "In many musical traditions, the melody line is of primary significance in a piece. Human listeners can readily distinguish melodies from accompaniment; however, making this distinction given only the written score \u2013 i.e. without listening to the music performed \u2013 can be a difficult task. Solving this task is of great importance for both Music Information Retrieval and musicological applications.  In this paper, we propose an automated approach to identifying the most salient melody line in a symbolic score. The backbone of the method consists of a convolutional neural network (CNN) estimating the probability that each note in the score (more precisely: each pixel in a piano roll encoding of the score) belongs to the melody line. We train and evaluate the method on various datasets, using manual annotations where available and solo instrument parts where not. We also propose a method to inspect the CNN and to analyze the influence exerted by notes on the prediction of other notes; this method can be applied whenever the output of a neural network has the same size as the input.",
        "zenodo_id": 3527966,
        "dblp_key": "conf/ismir/SimonettaCNW19",
        "keywords": [
            "melody line",
            "primary significance",
            "music information retrieval",
            "musicological applications",
            "automated approach",
            "symbolic score",
            "convolutional neural network",
            "piano roll encoding",
            "melody line prediction",
            "notes influence"
        ]
    },
    {
        "title": "Mapping Timing Strategies in Drum Performance.",
        "author": [
            "George Sioros",
            "Guilherme C\u00e2mara",
            "Anne Danielsen"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527926",
        "url": "https://doi.org/10.5281/zenodo.3527926",
        "ee": "http://archives.ismir.net/ismir2019/paper/000095.pdf",
        "abstract": "How do drummers express different timing styles? We conducted an experiment in which we asked twenty-two professional drummers to perform a simple rhythmic pattern while listening to a metronome. Here, we investigate the strategies they employed to express three different instructed timing profiles for the same pattern: \"on\", \"pushed\" and \"laidback\". Our analysis of the recordings follows three stages. First, we compute sixteen binary features that capture the micro-timing relations of the kick, snare and hi-hat drum on-sets, between each other and with regards to the metrical grid. Second, we construct a micro-timing profile (mtP) for every performance by averaging the binary features across the recording. An mtP codifies the frequency with which the various features were found in a performance. Third, through a \"similarity profiles\" hierarchical clustering analysis, we identify groups of recordings with significant similarities in their mtPs. We found distinct strategies to express each intended timing profile that employ specific combinations of relations between the instruments and with regards to the meter. Finally, we created a map that summarizes the main characteristics of the strategies and their relations using a phylogenetic tree visualization.",
        "zenodo_id": 3527926,
        "dblp_key": "conf/ismir/SiorosCD19",
        "keywords": [
            "drummers",
            "express",
            "timing",
            "styles",
            "experiment",
            "professional",
            "rhythmic",
            "pattern",
            "micro-timing",
            "features"
        ]
    },
    {
        "title": "Unmixer: An Interface for Extracting and Remixing Loops.",
        "author": [
            "Jordan B. L. Smith",
            "Yuta Kawasaki",
            "Masataka Goto"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527938",
        "url": "https://doi.org/10.5281/zenodo.3527938",
        "ee": "http://archives.ismir.net/ismir2019/paper/000101.pdf",
        "abstract": "To create their art, remix artists would like to have segmented stem tracks at their disposal; that is, isolated instances of the loops and sounds that the original composer used to create a track. We present Unmixer, a web service that will analyze and extract loops from any audio uploaded by a user. The loops are presented in an interface that allows users to immediately remix the loops; if users upload multiple tracks, they can easily create mash-ups with the loops, which are automatically matched in tempo. To analyze the audio, we use a recently-proposed method of source separation based on the nonnegative Tucker decomposition of the spectrum. To reduce interference among the extracted loops, we propose an extra factorization step with a sparseness constraint and demonstrate that it improves the source separation result. We also propose a method for selecting the best instances of the extracted loops and demonstrate its effectiveness in an evaluation. Both of these improvements are incorporated into the backend of the interface. Finally, we discuss the feedback collected in a set of user evaluations.",
        "zenodo_id": 3527938,
        "dblp_key": "conf/ismir/SmithKG19",
        "keywords": [
            "remix artists",
            "segmented stem tracks",
            "audio analysis",
            "source separation",
            "nonnegative Tucker decomposition",
            "tempo matching",
            "sparseness constraint",
            "loop extraction",
            "user interface",
            "evaluation"
        ]
    },
    {
        "title": "Investigating CNN-based Instrument Family Recognition for Western Classical Music Recordings.",
        "author": [
            "Michael Taenzer",
            "Jakob Abe\u00dfer",
            "Stylianos I. Mimilakis",
            "Christof Weiss",
            "Meinard M\u00fcller"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527884",
        "url": "https://doi.org/10.5281/zenodo.3527884",
        "ee": "http://archives.ismir.net/ismir2019/paper/000074.pdf",
        "abstract": "Western classical music comprises a rich repertoire composed for different ensembles. Often, these ensembles consist of instruments from one or two of the families woodwinds, brass, piano, vocals, and strings. In this paper, we consider the task of automatically recognizing instrument families from music recordings. As one main contribution, we investigate the influence of data normalization, pre-processing, and augmentation techniques on the generalization capability of the models. We report on experiments using three datasets of monotimbral recordings covering different levels of timbral complexity: isolated notes, isolated melodies, and polyphonic pieces. While data augmentation and the normalization of spectral patches turned out to be beneficial, pre-processing strategies such as logarithmic compression and channel-energy normalization did not lead to substantial improvements. Furthermore, our cross-dataset experiments indicate the necessity of further optimization routines such as domain adaptation.",
        "zenodo_id": 3527884,
        "dblp_key": "conf/ismir/TaenzerAMWM19",
        "keywords": [
            "Western classical music",
            "ensembles",
            "instruments",
            "woodwinds",
            "brass",
            "piano",
            "vocals",
            "strings",
            "recording",
            "instrument families"
        ]
    },
    {
        "title": "MIDI-Sheet Music Alignment Using Bootleg Score Synthesis.",
        "author": [
            "Thitaree Tanprasert",
            "Teerapat Jenrungrot",
            "Meinard M\u00fcller",
            "Timothy Tsai 0001"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527748",
        "url": "https://doi.org/10.5281/zenodo.3527748",
        "ee": "http://archives.ismir.net/ismir2019/paper/000008.pdf",
        "abstract": "MIDI-sheet music alignment is the task of finding correspondences between a MIDI representation of a piece and its corresponding sheet music images.  Rather than using optical music recognition to bridge the gap between sheet music and MIDI, we explore an alternative approach: projecting the MIDI data into pixel space and performing alignment in the image domain.  Our method converts the MIDI data into a crude representation of the score that only contains rectangular floating notehead blobs, a process we call bootleg score synthesis.  Furthermore, we project sheet music images into the same bootleg space by applying a deep watershed notehead detector and filling in the bounding boxes around each detected notehead.  Finally, we align the bootleg representations using a simple variant of dynamic time warping.  On a dataset of 68 real scanned piano scores from IMSLP and corresponding MIDI performances, our method achieves a 97.3% accuracy at an error tolerance of one second, outperforming several baseline systems that employ optical music recognition.",
        "zenodo_id": 3527748,
        "dblp_key": "conf/ismir/TanprasertJMT19",
        "keywords": [
            "MIDI-sheet music alignment",
            "optical music recognition",
            "alternative approach",
            "bootleg score synthesis",
            "deep watershed notehead detector",
            "dynamic time warping",
            "accuracy",
            "error tolerance",
            "one second",
            "baseline systems"
        ]
    },
    {
        "title": "Coupled Recurrent Models for Polyphonic Music Composition.",
        "author": [
            "John Thickstun",
            "Za\u00efd Harchaoui",
            "Dean P. Foster",
            "Sham M. Kakade"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527806",
        "url": "https://doi.org/10.5281/zenodo.3527806",
        "ee": "http://archives.ismir.net/ismir2019/paper/000036.pdf",
        "abstract": "This paper introduces a novel recurrent model for music composition that is tailored to the structure of polyphonic music. We propose an efficient new conditional probabilistic factorization of musical scores, viewing a score as a collection of concurrent, coupled sequences: i.e. voices. To model the conditional distributions, we borrow ideas from both convolutional and recurrent neural models; we argue that these ideas are natural for capturing music's pitch invariances, temporal structure, and polyphony. We train models for single-voice and multi-voice composition on 2,300 scores from the KernScores dataset.",
        "zenodo_id": 3527806,
        "dblp_key": "conf/ismir/ThickstunHFK19",
        "keywords": [
            "recurrent model",
            "polyphonic music",
            "novel conditional probabilistic factorization",
            "musical scores",
            "coupled sequences",
            "pitch invariances",
            "temporal structure",
            "polyphony",
            "KernScores dataset",
            "single-voice and multi-voice composition"
        ]
    },
    {
        "title": "AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information Processing.",
        "author": [
            "Shuhei Tsuchida",
            "Satoru Fukayama",
            "Masahiro Hamasaki",
            "Masataka Goto"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527854",
        "url": "https://doi.org/10.5281/zenodo.3527854",
        "ee": "http://archives.ismir.net/ismir2019/paper/000060.pdf",
        "abstract": "We describe the AIST Dance Video Database (AIST Dance DB), a shared database containing original street dance videos with copyright-cleared dance music. Although dancing is highly related to dance music and dance information can be considered an important aspect of music information, research on dance information processing has not yet received much attention in the Music Information Retrieval (MIR) community. We therefore developed the AIST Dance DB as the first large-scale shared database focusing on street dances to facilitate research on a variety of tasks related to dancing to music. It consists of 13,939 dance videos covering 10 major dance genres as well as 60 pieces of dance music composed for those genres. The videos were recorded by having 40 professional dancers (25 male and 15 female) dance to those pieces. We carefully designed this database so that it can cover both solo dancing and group dancing as well as both basic choreography moves and advanced moves originally choreographed by each dancer. Moreover, we used multiple cameras surrounding a dancer to simultaneously shoot from various directions. The AIST Dance DB will foster new MIR tasks such as dance-motion genre classification, dancer identification, and dance-technique estimation. We propose a dance-motion genre-classification task and developed four baseline methods of identifying dance genres of videos in this database. We evaluated these methods by extracting dancer body motions and training their classifiers on the basis of long short-term memory (LSTM) recurrent neural network models and support-vector machine (SVM) models.",
        "zenodo_id": 3527854,
        "dblp_key": "conf/ismir/TsuchidaFHG19",
        "keywords": [
            "AIST Dance Video Database",
            "shared database",
            "original street dance videos",
            "copyright-cleared dance music",
            "Music Information Retrieval community",
            "dancing to music",
            "street dances",
            "dance-motion genre classification",
            "dancer identification",
            "dance-technique estimation"
        ]
    },
    {
        "title": "JosquIntab: A Dataset for Content-based Computational Analysis of Music in Lute Tablature.",
        "author": [
            "Reinier de Valk",
            "Ryaan Ahmed",
            "Tim Crawford"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527836",
        "url": "https://doi.org/10.5281/zenodo.3527836",
        "ee": "http://archives.ismir.net/ismir2019/paper/000051.pdf",
        "abstract": "An enormous corpus of music for the lute, spanning some two and half centuries, survives today. Unlike other musical corpora from the same period, this corpus has undergone only limited musicological study. The main reason for this is that it is written down exclusively in lute tablature, a prescriptive form of notation that is difficult to understand for non-specialists as it reveals little structural information. In this paper we present JosquIntab, a dataset of automatically created enriched diplomatic transcriptions in MIDI and MEI format of 64 sixteenth-century lute intabulations, instrumental arrangements of vocal compositions. Such a dataset enables large-scale content-based computational analysis of music in lute tablature hitherto impossible. We describe the dataset, the mapping algorithm used to create it, as well as a method to quantitatively evaluate the degree of arrangement (goodness of fit) of an intabulation. Furthermore, we present two use cases, demonstrating the usefulness of the dataset for both music information retrieval and musicological research. We make the dataset, the source code, and an implementation of the mapping algorithm, runnable as a command line tool, publicly available.",
        "zenodo_id": 3527836,
        "dblp_key": "conf/ismir/ValkAC19",
        "keywords": [
            "dataset",
            "lute tablature",
            "computational analysis",
            "intabulations",
            "MIDI",
            "MEI format",
            "sixteenth-century",
            "musicological research",
            "mapping algorithm",
            "quantitative evaluation"
        ]
    },
    {
        "title": "Convolutional Composer Classification.",
        "author": [
            "Harsh Verma",
            "John Thickstun"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527866",
        "url": "https://doi.org/10.5281/zenodo.3527866",
        "ee": "http://archives.ismir.net/ismir2019/paper/000066.pdf",
        "abstract": "This paper investigates end-to-end learnable models for attributing composers to musical scores. We introduce several pooled, convolutional architectures for this task and draw connections between our approach and classical learning approaches based on global and n-gram features. We evaluate models on a corpus of 2,500 scores from the KernScores collection, authored by a variety of composers spanning the Renaissance era to the early 20th century. This corpus has substantial overlap with the corpora used in several previous, smaller studies; we compare our results on subsets of the corpus to these previous works.",
        "zenodo_id": 3527866,
        "dblp_key": "conf/ismir/VermaT19",
        "keywords": [
            "end-to-end",
            "learnable",
            "models",
            "attributing",
            "composers",
            "musical",
            "scores",
            "corpus",
            "Renaissance",
            "20th century"
        ]
    },
    {
        "title": "Identification and Cross-Document Alignment of Measures in Music Score Images.",
        "author": [
            "Simon Waloschek",
            "Aristotelis Hadjakos",
            "Alexander Pacha"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527760",
        "url": "https://doi.org/10.5281/zenodo.3527760",
        "ee": "http://archives.ismir.net/ismir2019/paper/000014.pdf",
        "abstract": "In the course of editing musical works, musicologists regularly compare multiple sources of the same musical piece, such as composers' autographs, handwritten copies, and various prints. For efficient comparison, cross-source navigation is essential, enabling to quickly jump back and forth between multiple sources without losing the current musical position. In practice, measures are first annotated by hand in the individual source images and then related to each other. Our approach automates this time-consuming and error-prone process with the help of deep learning. For this purpose, we train a neural network that automatically finds bounding boxes of all measures in images. A second network is trained to compute the similarity between two measures to determine if they have the same musical content and should, therefore, be linked for navigation. Sequences of outputs from the second network are matched using Dynamic Time Warping to provide the final proposal of measure relationships, so-called concordances. In addition to cross-source navigation, the results can be used to spot structural differences across the sources which are essential for editorial work, so that musicologists can focus more on analytical tasks.",
        "zenodo_id": 3527760,
        "dblp_key": "conf/ismir/WaloschekHP19",
        "keywords": [
            "Cross-source navigation",
            "Efficient comparison",
            "Deep learning",
            "Bounding boxes",
            "Measure similarity",
            "Dynamic Time Warping",
            "Concordances",
            "Structural differences",
            "Editorial work",
            "Analytical tasks"
        ]
    },
    {
        "title": "Adaptive Time-Frequency Scattering for Periodic Modulation Recognition in Music Signals.",
        "author": [
            "Changhong Wang",
            "Emmanouil Benetos",
            "Vincent Lostanlen",
            "Elaine Chew"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.5744336",
        "url": "https://doi.org/10.5281/zenodo.5744336",
        "ee": "http://archives.ismir.net/ismir2019/paper/000099.pdf",
        "abstract": "CBFdatasetisa dataset of Chinese bamboo flute (CBF) performances, created for ecologically valid analysis of music playing techniques in context.\n\nThe dataset comprises monophonic recordings of classic CBF pieces and isolated playing techniques, recorded by 10 professional CBF performers; and expert annotations of seven playing techniques: vibrato, tremolo, trill, flutter-tongue (FT), acciaccatura, portamento, and glissando. The recorded pieces includeBusy Delivering Harvest (BH) \u626c\u97ad\u50ac\u9a6c\u8fd0\u7cae\u5fd9, Jolly Meeting (JM) \u559c\u76f8\u9022, Morning (Mo) \u65e9\u6668, and Flying Partridge (FP) \u9e67\u9e2a\u98de.All data was recorded in a professional recording studio using a Zoom H6 recorder at 44.1kHz/24-bits. The difference between different Versions 1.2, 1.1, and 1.0:\n\n\n\tV1.2 is the complete CBFdatasetwith a total duration of 2.6 hours.\n\tV1.1 splits the CBFdataset into two subsets according to playing technique types: CBF-periDBand CBF-petsDB. The former contains all thefull-length pieces, isolated playing techniques, and annotations of four periodic modulations: vibrato, tremolo, trill, and flutter-tongue. The latter comprises the same full-length recordings, isolated playing techniques, and annotations of three pitch evolution-based techniques: acciaccatura, portamento, and glissando.\n\tV1.0 includes only the CBF-periDB.\n\n\nRelatedupdates, demos, and code for reproducibility are available athttp://c4dm.eecs.qmul.ac.uk/CBFdataset.html.Any queries, please feel free to contact Changhong atchanghong.wang@telecom-paris.fr. Please cite the following paper when using this dataset:\n\nChanghong Wang, Emmanouil Benetos, Vincent Lostanlen, and Elaine Chew,Adaptive Scattering Transforms for Playing Technique Recognition,IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP), 30 (2022): 1407-1421.",
        "zenodo_id": 5744336,
        "dblp_key": "conf/ismir/WangBLC19",
        "keywords": [
            "CBFdataset",
            "Chinese bamboo flute performances",
            "ecologically valid analysis",
            "music playing techniques",
            "monophonic recordings",
            "classic CBF pieces",
            "isolated playing techniques",
            "expert annotations",
            "seven playing techniques",
            "professional CBF performers"
        ]
    },
    {
        "title": "Query-by-Blending: A Music Exploration System Blending Latent Vector Representations of Lyric Word, Song Audio, and Artist.",
        "author": [
            "Kento Watanabe",
            "Masataka Goto"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527762",
        "url": "https://doi.org/10.5281/zenodo.3527762",
        "ee": "http://archives.ismir.net/ismir2019/paper/000015.pdf",
        "abstract": "This paper presents Query-by-Blending, a novel music exploration system that enables users to find unfamiliar music content by flexibly combining three musical aspects: lyric word, song audio, and artist. Although there are various systems for music retrieval based on the similarity between songs or artists and for music browsing based on visualized songs, it is still difficult to explore unfamiliar content by flexibly combining multiple musical aspects. Query-by-Blending overcomes this difficulty by representing each of the aspects as a latent vector representation (called a \"flavor\" in this paper) that is a distinctive quality felt to be characteristic of a given word/song/artist. By giving a lyric word as a query, for example, a user can find songs and artists whose flavors are similar to the flavor of the query word.  Moreover, by giving a query combining (blending) lyric-word and song-audio flavors, the user can interactively explore unfamiliar content containing the blended flavor. This multi-aspect blending was achieved by constructing a novel vector space model into which all of the lyric words, song audio tracks, and artist IDs of a collection can be embedded. In our experiments, we embedded 14,505 lyric words, 433,936 songs, and 44,696 artists into the same shared vector space and found that the system can appropriately calculate similarities between different aspects and blend flavors to find related lyric words, songs, and artists.",
        "zenodo_id": 3527762,
        "dblp_key": "conf/ismir/WatanabeG19",
        "keywords": [
            "Query-by-Blending",
            "novel music exploration system",
            "flexibly combining three musical aspects",
            "music retrieval",
            "music browsing",
            "similarities between songs or artists",
            "visualized songs",
            "unfamiliar content",
            "multi-aspect blending",
            "vector space model"
        ]
    },
    {
        "title": "Generating Structured Drum Pattern Using Variational Autoencoder and Self-similarity Matrix.",
        "author": [
            "I-Chieh Wei",
            "Chih-Wei Wu",
            "Li Su 0004"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527946",
        "url": "https://doi.org/10.5281/zenodo.3527946",
        "ee": "http://archives.ismir.net/ismir2019/paper/000104.pdf",
        "abstract": "Drum pattern generation is a task that focuses on the rhythmic aspect of music and aims at generating percussive sequences. With the advancement of machine learning techniques, several models have been proven useful in producing compelling results. However, one of the main challenges is to generate structurally cohesive sequences. In this study, a drum pattern generation model based on Variational Autoencoders (VAEs) is presented; Specifically, the proposed model is built to generate symbolic drum patterns given an accompaniment that consists of melodic sequences. A self-similarity matrix (SSM) is incorporated in the process for encapsulating structural information. Both the objective evaluation and the subjective listening test highlight the model's capability of creating musically meaningful transitions on structural boundaries.",
        "zenodo_id": 3527946,
        "dblp_key": "conf/ismir/WeiWS19",
        "keywords": [
            "Drum pattern generation",
            "Rhythmic aspect of music",
            "Machine learning techniques",
            "Structurally cohesive sequences",
            "Variational Autoencoders (VAEs)",
            "Symbolic drum patterns",
            "Melodic sequences",
            "Self-similarity matrix (SSM)",
            "Musically meaningful transitions",
            "Structural boundaries"
        ]
    },
    {
        "title": "Towards Measuring Intonation Quality of Choir Recordings: A Case Study on Bruckner&apos;s Locus Iste.",
        "author": [
            "Christof Weiss",
            "Sebastian J. Schlecht",
            "Sebastian Rosenzweig",
            "Meinard M\u00fcller"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527798",
        "url": "https://doi.org/10.5281/zenodo.3527798",
        "ee": "http://archives.ismir.net/ismir2019/paper/000032.pdf",
        "abstract": "Unaccompanied vocal music is a central part of Western art music, yet it requires excellent skills for singers to achieve proper intonation. In this paper, we analyze intonation deficiencies by introducing an intonation cost measure that can be computed from choir recordings and may help to assess the singers' intonation quality. With our approach, we measure the deviation between the recording's local salient frequency content and an adaptive reference grid based on the equal-tempered scale. The adaptivity introduces invariance of the local intonation measure to global intonation drifts. In our experiments, we compute this measure for several recordings of Anton Bruckner's choir piece Locus Iste. We demonstrate the robustness of the proposed measure by comparing scenarios of different complexity regarding the availability of aligned scores and multi-track recordings, as well as the number of singers per part. Even without using score information, our cost measure shows interesting trends, thus indicating the potential of our method for real-world applications.",
        "zenodo_id": 3527798,
        "dblp_key": "conf/ismir/WeissSRM19",
        "keywords": [
            "unaccompanied vocal music",
            "intonation deficiencies",
            "intonation cost measure",
            "choir recordings",
            "equal-tempered scale",
            "local salient frequency content",
            "adaptive reference grid",
            "global intonation drifts",
            "real-world applications",
            "Locus Iste"
        ]
    },
    {
        "title": "Guitar Tablature Estimation with a Convolutional Neural Network.",
        "author": [
            "Andrew Wiggins",
            "Youngmoo E. Kim"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527800",
        "url": "https://doi.org/10.5281/zenodo.3527800",
        "ee": "http://archives.ismir.net/ismir2019/paper/000033.pdf",
        "abstract": "Guitar tablature is a popular notation guitarists use to learn and share music. As it stands, most tablatures are created by an experienced guitarist taking the time and effort to annotate a song. As the process is time consuming and requires expertise, we are interested in automating this task. Previous approaches to automatic tablature transcription break the problem into two steps: 1) polyphonic pitch estimation, followed by 2) tablature fingering arrangement. Using a convolutional neural network (CNN) model, we can jointly solve both steps by learning a mapping directly from audio data to tablature. The model can simultaneously leverage physical playability constraints and differences in string timbres implicit in the data to determine the actual fingerings being used by the guitarist. We propose TabCNN, a CNN for estimating guitar tablature from audio of a solo acoustic guitar performance. We train and test our network using microphone recordings from the GuitarSet dataset, and TabCNN outperforms a state-of-the-art multipitch estimation algorithm. We also introduce a set of metrics to evaluate guitar tablature estimation.",
        "zenodo_id": 3527800,
        "dblp_key": "conf/ismir/WigginsK19",
        "keywords": [
            "Guitar tablature",
            "notation",
            "automating",
            "polyphonic pitch estimation",
            "tablature fingering arrangement",
            "convolutional neural network",
            "jointly solving",
            "physical playability constraints",
            "string timbres",
            "TabCNN"
        ]
    },
    {
        "title": "MIDI Passage Retrieval Using Cell Phone Pictures of Sheet Music.",
        "author": [
            "Daniel Yang",
            "Thitaree Tanprasert",
            "Teerapat Jenrungrot",
            "Mengyi Shan",
            "Timothy Tsai 0001"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527964",
        "url": "https://doi.org/10.5281/zenodo.3527964",
        "ee": "http://archives.ismir.net/ismir2019/paper/000113.pdf",
        "abstract": "This paper investigates a cross-modal retrieval problem in which a user would like to retrieve a passage of music from a MIDI file by taking a cell phone picture of a physical page of sheet music.  While audio-sheet music retrieval has been explored by a number of works, this scenario is novel in that the query is a cell phone picture rather than a digital scan.  To solve this problem, we introduce a mid-level feature representation called a bootleg score which explicitly encodes the rules of Western musical notation.  We convert both the MIDI and the sheet music into bootleg scores using deterministic rules of music and classical computer vision techniques for detecting simple geometric shapes.  Once the MIDI and cell phone image have been converted into bootleg scores, we estimate the alignment using dynamic programming.  The most notable characteristic of our system is that it does test-time adaptation and has no trainable weights at all--only a set of about 30 hyperparameters.  On a dataset containing 1000 cell phone pictures taken of 100 scores of classical piano music, our system achieves an F measure score of .869 and outperforms baseline systems based on commercial optical music recognition software.",
        "zenodo_id": 3527964,
        "dblp_key": "conf/ismir/YangTJST19",
        "keywords": [
            "cross-modal retrieval",
            "cell phone picture",
            "sheet music",
            "bootleg score",
            "Western musical notation",
            "deterministic rules",
            "dynamic programming",
            "F measure score",
            "1000 cell phone pictures",
            "100 scores of classical piano music"
        ]
    },
    {
        "title": "Deep Music Analogy Via Latent Representation Disentanglement.",
        "author": [
            "Ruihan Yang",
            "Dingsu Wang",
            "Ziyu Wang 0008",
            "Tianyao Chen",
            "Junyan Jiang",
            "Gus Xia"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527880",
        "url": "https://doi.org/10.5281/zenodo.3527880",
        "ee": "http://archives.ismir.net/ismir2019/paper/000072.pdf",
        "abstract": "Analogy-making is a key method for computer algorithms to generate both natural and creative music pieces. In general, an analogy is made by partially transferring the music abstractions, i.e., high-level representations and their relationships, from one piece to another; however, this procedure requires disentangling music representations, which usually takes little effort for musicians but is non-trivial for computers. Three sub-problems arise: extracting latent representations from the observation, disentangling the representations so that each part has a unique semantic interpretation, and mapping the latent representations back to actual music. In this paper, we contribute an explicitly-constrained variational autoencoder (EC^2-VAE) as a unified solution to all three sub-problems. We focus on disentangling the pitch and rhythm representations of 8-beat music clips conditioned on chords. In producing music analogies, this model helps us to realize the imaginary situation of \"what if\" a piece is composed using a different pitch contour, rhythm pattern, or chord progression by borrowing the representations from other pieces. Finally, we validate the proposed disentanglement method using objective measurements and evaluate the analogy examples by a subjective study.",
        "zenodo_id": 3527880,
        "dblp_key": "conf/ismir/YangWWCJX19",
        "keywords": [
            "analogy-making",
            "computer algorithms",
            "natural music pieces",
            "latent representations",
            "disentangling representations",
            "pitch and rhythm",
            "8-beat music clips",
            "chord progression",
            "imaginary situation",
            "objective measurements"
        ]
    },
    {
        "title": "Blending Acoustic and Language Model Predictions for Automatic Music Transcription.",
        "author": [
            "Adrien Ycart",
            "Andrew McLeod",
            "Emmanouil Benetos",
            "Kazuyoshi Yoshii"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527842",
        "url": "https://doi.org/10.5281/zenodo.3527842",
        "ee": "http://archives.ismir.net/ismir2019/paper/000054.pdf",
        "abstract": "In this paper, we introduce a method for converting an input probabilistic piano roll (the output of a typical multi-pitch detection model) into a binary piano roll. The task is an important step for many automatic music transcription systems with the goal of converting an audio recording into some symbolic format. Our model has two components: an LSTM-based music language model (MLM) which can be trained on any MIDI data, not just that aligned with audio; and a blending model used to combine the probabilities of the MLM with those of the input probabilistic piano roll given by an acoustic multi-pitch detection model, which must be trained on (a comparably small amount of) aligned data. We use scheduled sampling to make the MLM robust to noisy sequences during testing. We analyze the performance of our model on the MAPS dataset using two different timesteps (40ms and 16th-note), comparing it against a strong baseline hidden Markov model with a training method not used before for the task to our knowledge. We report a statistically significant improvement over HMM decoding in terms of notewise F-measure with both timesteps, with 16th note timesteps improving further compared to 40ms timesteps.",
        "zenodo_id": 3527842,
        "dblp_key": "conf/ismir/YcartMBY19",
        "keywords": [
            "probabilistic piano roll",
            "binary piano roll",
            "automatic music transcription",
            "MIDI data",
            "scheduled sampling",
            "MLM",
            "blending model",
            "timesteps",
            "MAPS dataset",
            "HMM decoding"
        ]
    },
    {
        "title": "A Comparative Study of Neural Models for Polyphonic Music Sequence Transduction.",
        "author": [
            "Adrien Ycart",
            "Daniel Stoller",
            "Emmanouil Benetos"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527846",
        "url": "https://doi.org/10.5281/zenodo.3527846",
        "ee": "http://archives.ismir.net/ismir2019/paper/000056.pdf",
        "abstract": "Automatic transcription of polyphonic music remains a challenging task in the field of Music Information Retrieval. One under-investigated point is the post-processing of time-pitch posteriograms into binary piano rolls. In this study, we investigate this task using a variety of neural network models and training procedures.  We introduce an adversarial framework, that we compare against more traditional training losses. We also propose the use of binary neuron outputs and compare them to the usual real-valued outputs in both training frameworks. This allows us to train networks directly using the F-measure as training objective. We evaluate these methods using two kinds of transduction networks and two different multi-pitch detection systems, and compare the results against baseline note-tracking methods on a dataset of classical piano music. Analysis of results indicates that (1) convolutional models improve results over baseline models, but no improvement is reported for recurrent models; (2) supervised losses are superior to adversarial ones; (3) binary neurons do not improve results; (4) cross-entropy loss results in better or equal performance compared to the F-measure loss.",
        "zenodo_id": 3527846,
        "dblp_key": "conf/ismir/YcartSB19",
        "keywords": [
            "Automatic transcription",
            "polyphonic music",
            "Music Information Retrieval",
            "post-processing",
            "time-pitch posteriograms",
            "binary piano rolls",
            "neural network models",
            "training procedures",
            "adversarial framework",
            "binary neuron outputs"
        ]
    },
    {
        "title": "Da-TACOS: A Dataset for Cover Song Identification and Understanding.",
        "author": [
            "Furkan Yesiler",
            "Chris Tralie",
            "Albin Andrew Correya",
            "Diego Furtado Silva",
            "Philip Tovstogan",
            "Emilia G\u00f3mez",
            "Xavier Serra"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.4717628",
        "url": "https://doi.org/10.5281/zenodo.4717628",
        "ee": "http://archives.ismir.net/ismir2019/paper/000038.pdf",
        "abstract": "We present Da-TACOS: a dataset for cover song identification and understanding. It contains two subsets, namelythe benchmark subset(for benchmarking cover song identification systems) andthe cover analysis subset(for analyzing the links among cover songs), withpre-extracted featuresandmetadatafor15,000and10,000 songs, respectively. The annotations included in the metadata are obtained with the API ofSecondHandSongs.com. All audio files we use to extract features are encoded in MP3 format and their sample rate is 44.1 kHz. Da-TACOS does not contain any audio files. Forthe resultsofour analyses on modifiable musical characteristicsusing the cover analysis subset andour initial benchmarking of 7 state-of-the-art cover song identification algorithmson the benchmark subset, you can look at ourpublication.\n\nFor organizing the data, we use the structure of SecondHandSongs where each song is called aperformance, and each clique (cover group) is called awork. Based on this, the file names of the songs are their unique performance IDs (PID, e.g.P_22), and their labels with respect to their cliques are their work IDs (WID, e.g.W_14).\n\nMetadata for each song includes\n\n\n\tperformance title,\n\tperformance artist,\n\twork title,\n\twork artist,\n\trelease year,\n\tSecondHandSongs.com performance ID,\n\tSecondHandSongs.com work ID,\n\twhether the song is instrumental or not.\n\n\nIn addition, we matched the original metadata with MusicBrainz to obtain MusicBrainz ID (MBID), song length and genre/style tags. We would like to note that MusicBrainz related information is not available for all the songs in Da-TACOS, and since we used just our metadata for matching, we include all possible MBIDs for a particular songs.\n\nFor facilitatingreproducibilityin cover song identification (CSI) research, we proposea framework for feature extraction and benchmarkingin our supplementary repository:acoss.The feature extraction componentis designed to help CSI researchers to findthe most commonly used features for CSI in a single address. The parameter values we used to extract the features in Da-TACOS are shared in the same repository. Moreover,the benchmarking componentincludes our implementations of7 state-of-the-art CSI systems. We provide the performance results ofan initial benchmarkingof those7 systemson the benchmark subset of Da-TACOS. We encourage other CSI researchers to contribute to acoss with implementing their favorite feature extraction algorithms and their CSI systems to build up a knowledge base where CSI research can reach larger audiences.\n\nThe instructions for how to download and use the dataset are shared below. Please contact us if you have any questions or requests.\n\n1. Structure\n\n1.1. Metadata\n\nWe provide two metadata files that contain information about the benchmark subset and the cover analysis subset. Both metadata files are stored as python dictionaries in.jsonformat, and have the same hierarchical structure.\n\nAn example to load the metadata files in python:\n\nimport json\n\nwith open('./da-tacos_metadata/da-tacos_benchmark_subset_metadata.json') as f:\n\tbenchmark_metadata = json.load(f)\n\n\nThe python dictionary obtained with the code above will have the respective WIDs as keys. Each key will provide the song dictionaries that contain the metadata regarding the songs that belong to their WIDs. An example can be seen below:\n\n\"W_163992\": { # work id\n\t\"P_547131\": { # performance id of the first song belonging to the clique 'W_163992'\n\t\t\"work_title\": \"Trade Winds, Trade Winds\",\n\t\t\"work_artist\": \"Aki Aleong\",\n\t\t\"perf_title\": \"Trade Winds, Trade Winds\",\n\t\t\"perf_artist\": \"Aki Aleong\",\n\t\t\"release_year\": \"1961\",\n\t\t\"work_id\": \"W_163992\",\n\t\t\"perf_id\": \"P_547131\",\n\t\t\"instrumental\": \"No\",\n\t\t\"perf_artist_mbid\": \"9bfa011f-8331-4c9a-b49b-d05bc7916605\",\n\t\t\"mb_performances\": {\n\t\t\t\"4ce274b3-0979-4b39-b8a3-5ae1de388c4a\": {\n\t\t\t\t\"length\": \"175000\"\n\t\t\t},\n\t\t\t\"7c10ba3b-6f1d-41ab-8b20-14b2567d384a\": {\n\t\t\t\t\"length\": \"177653\"\n\t\t\t}\n\t\t}\n\t},\n\t\"P_547140\": { # performance id of the second song belonging to the clique 'W_163992'\n\t\t\"work_title\": \"Trade Winds, Trade Winds\",\n\t\t\"work_artist\": \"Aki Aleong\",\n\t\t\"perf_title\": \"Trade Winds, Trade Winds\",\n\t\t\"perf_artist\": \"Dodie Stevens\",\n\t\t\"release_year\": \"1961\",\n\t\t\"work_id\": \"W_163992\",\n\t\t\"perf_id\": \"P_547140\",\n\t\t\"instrumental\": \"No\"\n\t}\n}\n\n\n1.2. Pre-extracted features\n\nThe list of features included in Da-TACOS can be seen below. All the features are extracted withacossrepository that uses open-source feature extraction libraries such asEssentia,LibROSA, andMadmom.\n\nTo facilitate the use of the dataset, we provide two options regarding the file structure.\n\n1Inda-tacos_benchmark_subset_single_filesandda-tacos_coveranalysis_subset_single_filesfolders, we organize the data based on their respective cliques, and one file contains all the features for that particular song.\n\n{\n\t\"chroma_cens\": numpy.ndarray,\n\t\"crema\": numpy.ndarray,\n\t\"hpcp\": numpy.ndarray,\n\t\"key_extractor\": {\n\t\t\"key\": numpy.str_,\n\t\t\"scale\": numpy.str_,_\n\t\t\"strength\": numpy.float64\n\t},\n\t\"madmom_features\": {\n\t\t\"novfn\": numpy.ndarray, \n\t\t\"onsets\": numpy.ndarray,\n\t\t\"snovfn\": numpy.ndarray,\n\t\t\"tempos\": numpy.ndarray\n\t}\n\t\"mfcc_htk\": numpy.ndarray,\n\t\"tags\": list of (numpy.str_, numpy.str_)\n\t\"label\": numpy.str_,\n\t\"track_id\": numpy.str_\n}\n\n\n\n\n2Inda-tacos_benchmark_subset_FEATUREandda-tacos_coveranalysis_subset_FEATUREfolders, the data is organized based on their cliques as well, but each of these folders contain only one feature per song. For instance, if you want to test your system that uses HPCP features, you can downloadda-tacos_benchmark_subset_hpcpto access the pre-computed HPCP features. An example for the contents in those files can be seen below:\n\n{\n\t\"hpcp\": numpy.ndarray,\n\t\"label\": numpy.str_,\n\t\"track_id\": numpy.str_\n}\n\n\n\n2. Using the dataset\n\n2.1. Requirements\n\n\n\tPython 3.6+\n\tCreate virtual environment and install requirements\n\tgit clone https://github.com/MTG/da-tacos.git\ncd da-tacos\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n\t\n\n\n2.2. Downloading the data\n\nThe dataset is currently stored in only in Google Drive (it will be uploaded to Zenodo soon), and can be downloaded from thislink. We also provide a python script that automatically downloads the folders you specify. Basic usage of this script can be seen below:\n\npython download_da-tacos.py -h\n\n\nusage: download_da-tacos.py [-h]\n                            [--dataset {benchmark,coveranalysis,da-tacos}]                                                                          \n                            [--type {single_files,cens,crema,hpcp,key,madmom,mfcc,tags}]   \n                            [--source {gdrive,zenodo}]                                                         \n                            [--outputdir OUTPUTDIR]\n                            [--unpack]\n                            [--remove]\n\nDownload script for Da-TACOS \n\noptional arguments:                                                                                                       \n  -h, --help            show this help message and exit                                                                   \n  --dataset {metadata,benchmark,coveranalysis,da-tacos}                                                                      \n                        which subset to download. 'da-tacos' option downloads\n                        both subsets. the options other than 'metadata' will\n                        download the metadata as well. (default: metadata)                                                                     \n  --type {single_files,cens,crema,hpcp,key,madmom,mfcc,tags} [{single_files,cens,crema,hpcp,key,madmom,mfcc,tags} ...]                                     \n                        which folder to download. for downloading multiple\n                        folders, you can enter multiple arguments (e.g. '--\n                        type cens crema'). for detailed explanation, please\n                        check https://mtg.github.io/da-tacos/ (default:\n                        single_files)                  \n  --source {gdrive,zenodo}\n                        from which source to download the files. you can\n                        either download from Google Drive (gdrive) or from\n                        Zenodo (zenodo) (default: gdrive)                                           \n  --outputdir OUTPUTDIR                                                               \n                        directory to store the dataset (default: ./)                   \n  --unpack              unpack the zip files (default: False)                        \n  --remove              remove zip files after unpacking (default: False) \n\n\n2.3. Loading the data in Python\n\nAll files (except the metadata) are stored in.h5format. We recommend usingdeepdishlibrary for python to load the files. An example of how to load the data is shown below:\n\nimport deepdish as dd\n\nfile_path = './da-tacos_coveranalysis_subset_single_files/W_14/P_15.h5'\nP_15_data = dd.io.load(file_path)\n\n\n3. Citing the dataset\n\nPlease cite the followingpublicationwhen using the dataset:\n\n\nFurkan Yesiler, Chris Tralie, Albin Correya, Diego F. Silva, Philip Tovstogan, Emilia Gmez, and Xavier Serra. Da-TACOS: A Dataset for Cover Song Identification and Understanding. In Proc. of the 20th Int. Soc. for Music Information Retrieval Conf. (ISMIR), pages 327-334, Delft, The Netherlands, 2019.\n\n\nBibtex version:\n\n@inproceedings{yesiler2019,\n    author = \"Furkan Yesiler and Chris Tralie and Albin Correya and Diego F. Silva and Philip Tovstogan and Emilia G{\\'{o}}mez and Xavier Serra\",\n    title = \"{Da-TACOS}: {A} Dataset for Cover Song Identification and Understanding\",\n    booktitle = \"Proc. of the 20th Int. Soc. for Music Information Retrieval Conf. (ISMIR)\",\n    year = \"2019\",\n    pages = \"327--334\",\n    address = \"Delft, The Netherlands\"\n}\n\n\n4. License\n\n\n\tThe metadata and the pre-extracted features are licensed underCC BY-NC-SA 4.0\n\n\nCopyright 2019 Music Technology Group\n\n5. Acknowledgments\n\nThis work has received funding from the European Unions Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No. 765068 (MIP-Frontiers).\n\nThis work has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No. 770376 (TROMPA).",
        "zenodo_id": 4717628,
        "dblp_key": "conf/ismir/YesilerTCSTGS19"
    },
    {
        "title": "Hit Song Prediction: Leveraging Lowand High-Level Audio Features.",
        "author": [
            "Eva Zangerle",
            "Michael V\u00f6tter",
            "Ramona Huber",
            "Yi-Hsuan Yang"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527808",
        "url": "https://doi.org/10.5281/zenodo.3527808",
        "ee": "http://archives.ismir.net/ismir2019/paper/000037.pdf",
        "abstract": "Assessing the potential success of a given song based on its acoustic characteristics is an important task in the music industry. This task has mostly been approached from an internal perspective, utilizing audio descriptors to predict the success of a given song, where either lowor high-level audio features have been utilized separately. In this work, we aim to jointly exploit lowand high-level audio features and model the prediction as a regression task. Particularly, we make use of a wide and deep neural network architecture that allows for jointly exploiting lowand high-level features. Furthermore, we enrich the set of features with information about the release year of tracks. We evaluate our approach based on the Million Song Dataset and characterize a song as a hit if it is contained in the Billboard Hot 100 at any point in time. Our findings suggest that the proposed approach is able to outperform baseline approaches as well as approaches utilizing lowor high-level features individually. Furthermore, we find that incorporating the release year as well as features describing mood and vocals of a song improve prediction results.",
        "zenodo_id": 3527808,
        "dblp_key": "conf/ismir/ZangerleVHY19",
        "keywords": [
            "audio characteristics",
            "music industry",
            "jointly exploiting",
            "low-level features",
            "high-level features",
            "regression task",
            "Million Song Dataset",
            "Billboard Hot 100",
            "release year",
            "features describing mood and vocals"
        ]
    },
    {
        "title": "BandNet: A Neural Network-based, Multi-Instrument Beatles-Style MIDI Music Composition Machine.",
        "author": [
            "Yichao Zhou",
            "Wei Chu",
            "Sam Young",
            "Xin Chen"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527894",
        "url": "https://doi.org/10.5281/zenodo.3527894",
        "ee": "http://archives.ismir.net/ismir2019/paper/000079.pdf",
        "abstract": "In this paper, we propose a recurrent neural network (RNN)-based MIDI music composition machine that is able to learn musical knowledge from existing Beatles' music and generate full songs in the style of the Beatles with little human intervention. In the learning stage, a sequence of stylistically uniform, multiple-channel music samples was modeled by an RNN. In the composition stage, a short clip of randomly-generated music was used as a seed for the RNN to start music score prediction. To form structured music, segments of generated music from different seeds were concatenated together. To improve the quality and structure of the generated music, we integrated music theory knowledge into the model, such as controlling the spacing of gaps in the vocal melody, normalizing the timing of chord changes, and requiring notes to be related to the song's key (C major, for example). This integration improved the quality of the generated music as verified by a professional composer. We also conducted a subjective listening test that showed our generated music was close to original music by the Beatles in terms of style similarity, professional quality, and interestingness.",
        "zenodo_id": 3527894,
        "dblp_key": "conf/ismir/ZhouCYC19",
        "keywords": [
            "recurrent neural network",
            "MIDI music composition",
            "learning from Beatles music",
            "generate full songs",
            "little human intervention",
            "stylistically uniform music samples",
            "sequence modeling",
            "music score prediction",
            "structured music formation",
            "music theory knowledge integration"
        ]
    },
    {
        "title": "Hierarchical Classification Networks for Singing Voice Segmentation and Transcription.",
        "author": [
            "Zih-Sing Fu",
            "Li Su 0004"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3527960",
        "url": "https://doi.org/10.5281/zenodo.3527960",
        "ee": "http://archives.ismir.net/ismir2019/paper/000111.pdf",
        "abstract": "Identifying the onset and offset time of a musical note is a challenging step for singing voice transcription, as the soft onset/offset, portamento, and vibrato phenomena are rich in singing voice signals. In this work, we investigate how to utilize local data representation with pattern recognition for onset and offset detection of singing voice. We consider onset and offset detection as a hierarchical classification problem, where every local data representation as input is classified into one of all the possible event states in monophonic singing, namely the silence, activation, and transition states, and the transition state is further classified into the onset and offset states. An objective function based on this hierarchical taxonomy nicely guides the model to capture the complicated temporal dynamics of note sequences. Multi-channel data representations containing spectral differences and pitch saliency are employed to reflect the patterns of note transition in singing voice signals. The proposed method implemented with residual networks provides improved performance over prior art in onset and offset detection. Moreover, by integrating with a pitch detection framework, the proposed method also outperforms previous singing voice transcription methods. This result emphasizes the importance of note segmentation in singing voice transcription.",
        "zenodo_id": 3527960,
        "dblp_key": "conf/ismir/Zih-SingS19",
        "keywords": [
            "challenging step",
            "singing voice transcription",
            "local data representation",
            "pattern recognition",
            "onset and offset detection",
            "hierarchical classification",
            "monophonic singing",
            "note sequences",
            "objective function",
            "residual networks"
        ]
    },
    {
        "title": "Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands, November 4-8, 2019",
        "author": [
            "Arthur Flexer",
            "Geoffroy Peeters",
            "Juli\u00e1n Urbano",
            "Anja Volk"
        ],
        "year": "2019",
        "doi": "10.5281/zenodo.3258042",
        "url": "https://doi.org/10.5281/zenodo.3258042",
        "ee": null,
        "abstract": "Hit Song Prediction Dataset\n\nThis dataset is based on the Million Song Dataset (MSD), which contains one million songs that are representative for western commercial music released between 1922 and 2011. The dataset contains release year information for 515,576 of the MSD songs. Please refer to http://millionsongdataset.com/ for further information on the million song dataset.\n\nFor our hit song prediction experiments, we extract highand low-level audio features using the Essentia toolkit (cf. https://essentia.upf.edu/). For the high-level features, we make use of the pre-trained classifiers as provided by Essentia. For a detailed description of the features, please visit the Essentia documentation.\n\n\nThe dataset hence contains:\n\n\n\tAudio features: the compressed msd_audio_features.tar.gz file contains the lowand high-level features for each track, stored as json files. Please note that we organize all MSD audio feature files based on the track&#39;s identifier with one folder holding all tracks with the same first letter of the track identifier to keep the files manageable. For each track, we provide two files: one containing the high-level and one containing the low-level features extracted by Essentia.\n\tBillboard data: the folder billboard_data contains two files: msd_bb_matches.csvcontains information about the MSD tracks that were also featured in the Billboard Hot 100 charts. Here, we provide the MSD id, Echo Nest id, artist name, track title, release year, peak position in Billboard charts and the number of weeks in the charts. The second file, msd_bb_non_matches.csvcontains meta-information about the tracks of the MSD that were not featured in the Billboard Hot 100 and hence were used as negative samples. Here, we provide the MSD id, Echo Nest id, artist name, track title and the release year.\n\n\n\nIf you make use of the dataset, please kindly cite the following paper:\n\nEva Zangerle, Michael Vtter, Ramona Huber, and Yi-Hsuan Yang. Hit Song Prediction: Leveraging Lowand High-Level Audio Features. In Proceedings of the 20th International Society for Music Information Retrieval Conference 2019 (ISMIR 2019), 2019.\n\n\n@inproceedings{zangerle_ismir19,\ntitle = {{Hit Song Prediction: Leveraging Lowand High-Level Audio Features}},\nauthor = {Eva Zangerle and Ramona Huber and Michael V\\{o}tter and Yi-Hsuan Yang},\nyear = {2019},\nbooktitle = {{Proceedings of the 20th International Society for Music Information Retrieval Conference 2019 (ISMIR 2019)}},\n}",
        "zenodo_id": 3258042,
        "dblp_key": "conf/ismir/2019"
    }
]