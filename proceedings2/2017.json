[
    {
        "title": "Song2Guitar: A Difficulty-Aware Arrangement System for Generating Guitar Solo Covers from Polyphonic Audio of Popular Music.",
        "author": [
            "Shunya Ariga",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417501",
        "url": "https://doi.org/10.5281/zenodo.1417501",
        "ee": "https://zenodo.org/records/1417501/files/ArigaFG17.pdf",
        "abstract": "This paper describes Song2Guitar which automatically generates difficulty-aware guitar solo cover of popular music from its acoustic signals. Previous research has utilized hidden Markov models (HMMs) to generate playable guitar piece from music scores. Our Song2Guitar extends the framework by leveraging MIR technologies so that it can handle beats, chords and melodies extracted from polyphonic audio. Furthermore, since it is important to generate a guitar piece to meet the skill of a player, Song2Guitar generates guitar solo covers in consideration of playing difficulty. We conducted a data-driven investigation to find what factor makes a guitar piece difficult to play, and restricted Song2Guitar to use certain hand forms adaptively so that the player can play the piece without experiencing too much difficulty. The user interface of Song2Guitar is also implemented and is used to conduct user tests. The results indicated that Song2Guitar succeeded in generating guitar solo covers from polyphonic audio with various playing difficulties.",
        "zenodo_id": 1417501,
        "dblp_key": "conf/ismir/ArigaFG17",
        "keywords": [
            "Song2Guitar",
            "difficulty-aware",
            "guitar solo",
            "popular music",
            "acoustic signals",
            "hidden Markov models",
            "MIR technologies",
            "beats",
            "chords",
            "melodies"
        ],
        "content": "SONG2GUITAR: A DIFFICULTY-A WARE ARRANGEMENT SYSTEM FOR\nGENERATING GUITAR SOLO COVERS FROM POLYPHONIC AUDIO OF\nPOPULAR MUSIC\nShunya Ariga\nThe University of Tokyo\nariga@iis-lab.orgSatoru Fukayama\nAIST\ns.fukayama@aist.go.jpMasataka Goto\nAIST\nm.goto@aist.go.jp\nABSTRACT\nThis paper describes Song2Guitar which automatically\ngenerates difﬁculty-aware guitar solo cover of popular\nmusic from its acoustic signals. Previous research has\nutilized hidden Markov models (HMMs) to generate\nplayable guitar piece from music scores. Our Song2Guitar\nextends the framework by leveraging MIR technologies\nso that it can handle beats, chords and melodies extracted\nfrom polyphonic audio. Furthermore, since it is important\nto generate a guitar piece to meet the skill of a player,\nSong2Guitar generates guitar solo covers in consideration\nof playing difﬁculty. We conducted a data-driven\ninvestigation to ﬁnd what factor makes a guitar piece\ndifﬁcult to play, and restricted Song2Guitar to use certain\nhand forms adaptively so that the player can play the\npiece without experiencing too much difﬁculty. The\nuser interface of Song2Guitar is also implemented and\nis used to conduct user tests. The results indicated that\nSong2Guitar succeeded in generating guitar solo covers\nfrom polyphonic audio with various playing difﬁculties.\n1. INTRODUCTION\nA guitar solo cover version of an original song adds new\npleasure to the music experience of the song. Various\nmusical elements such as beats, melodies, and harmonies\nin an original song are represented in a uniform but\nexpressive timbre of a guitar. However, a guitar solo\ncover of one’s favorite song is not always available, and\ncreating guitar arrangements requires advanced skills and\nknowledge and takes a lot of time. If such a guitar solo\ncover of any song can be generated from music audio\nsignals, music listeners can enjoy their favorite songs in\na different way, and guitarists who do not have skills for\nplaying by ear can also enjoy performing any songs on\ntheir guitars.\nThe goal of this research is to develop a system that\ncan automatically generate a guitar solo cover version from\nc\rShunya Ariga, Satoru Fukayama, Masataka Goto.\nLicensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Shunya Ariga, Satoru Fukayama,\nMasataka Goto. “Song2Guitar: A Difﬁculty-Aware Arrangement System\nfor Generating Guitar Solo Covers from Polyphonic Audio of Popular\nMusic”, 18th International Society for Music Information Retrieval\nConference, Suzhou, China, 2017.\nFigure 1: Overview of the Song2Guitar system.\naudio signals. By leveraging Music Information Retrieval\n(MIR) technologies, we propose a guitar arrangement\nsystem, Song2Guitar , that generates guitar solo covers\nfrom polyphonic audio signals of popular music, which\ncontain sounds of various instruments. We also aim\nat creating difﬁcult-aware guitar arrangements — i.e.,\ngenerating guitar tablatures having different levels of\nplaying difﬁculty for guitarists. There are three issues that\nshould be considered:\n(1) Generate from polyphonic audio of popular music\n(2) Difﬁculty-aware arrangement\n(3) Interface to perform the arrangement result\nAn overview of our solutions to address these issues\nis shown in Fig. 1. As for issue (1), even if we use\nthe state-of-the-art MIR technologies, we cannot obtain\ncompletely-transcribed musical scores from such complex\naudio signals. We therefore directly extract important\nmusical elements, such as melody lines represented as\nF0 (fundamental frequency) contour, beats, and chords,\nfrom polyphonic audio. We then reﬂect the extracted\nelements in generating guitar solo covers by using a novel\nextension of a hidden Markov model. As for issue (2), we\nconducted a data-driven survey to ﬁnd what factors make\na guitar tablature difﬁcult to play. Based on the survey,\nSong2Guitar controls the movement of an index ﬁnger and\nthe number of ﬁngers to press the strings. Finally, as for\nissue (3), we designed and implemented an interface that\nenables a guitarist to change the degree of difﬁculty to\nperform the result. In this paper, we will also discuss\na desirable interface for generating various arrangement\nresults and providing training materials for guitarist. The\ndesign of the interface and the results generated by our\nsystem are available on the web1.\n1https://youtu.be/fN4-ibh7ZDI5682. RELATED WORK\n2.1 Creative MIR\nOur research is addressed in a Creative MIR approach.\nMIR researchers have recently explored creativity-oriented\nmusic technologies by applying technologies developed\nin the MIR community. This emerging ﬁeld is\nnamed Creative MIR [11] where music analysis and\ntransformation technologies are used in various creative\napplications. For example, AutoMashUpper [4] is\nan interactive system that creates music mashups by\nautomatically selecting and mixing chosen songs. They\nachieved automatic mashup by estimating mashability ,\nwhich is calculated by using MIR technologies to estimate\nvarious musical elements such as beats, downbeats, and\nchromagram. Song2Quartet [18] generates a cover song\nin the style of string quartet by combining probabilistic\nmodels estimated from a corpus of symbolic classical\nmusic with the target audio ﬁle of a song.\n2.2 Generating playable guitar Solo\nIn order to generate a playable guitar covers, Hori et\nal.[7–9] used a hidden Markov model (HMM) to generate\nguitar arrangements from a symbolic musical score while\nconsidering natural ﬁngerings. Audio signals, however,\nwere not used as the input. By taking audio signals of\nan individual separated guitar part as the input, Yazawa\net al. [25] developed an automatic transcription system\nspecialized for a guitar performance and generated a guitar\ntablature by using multi-pitch analysis and playability\nconstraints. Yazawa et al. [24] then extended their previous\nwork to transcribe a guitar tablature while considering\nacoustical reproducibility and ﬁngering easiness. Even\nthough guitarist’s proﬁciency was considered, creating\nguitar arrangements from polyphonic music including\nmultiple instruments was not tackled so far.\nResearch of automatic ﬁngering decision can also be\nregarded as related work of ours. This is because ﬁngering\ndecision is a sub-problem to generate playable guitar solo,\nand the existence of a ﬁngering for a song is a necessary\ncondition for the song to be playable. Radicioni discussed\nin his thesis how to computationally model the ﬁngering in\nmusic performance [19]. The ﬁngering is often determined\nby searching the ﬁngering sequence as an optimal path\nsearch problem [20, 21].\nFingerings are represented in a tablature score or\ntabs, and they are often utilized to analyze and generate\nplayable scores. A method to analyze and search\nvaluable information in the tablature database has been\nproposed [14]. AutoGuitarTab [15–17] generates guitar\nmusic according to different styles of various guitarists by\ntraining individual probabilistic model using a tablature\ndatabase. Genetic algorithms have been used to search\nthe ﬁngering sequence efﬁciently to generate an guitar solo\narrangement [22].\nTablature transcription from music audio are also the\nrelated work. MIR technologies such as multi-pitch\nanalysis and chord recognition have been used to capture\nFigure 2: Average movement of an index ﬁnger and difﬁculty\nrating of 50tablatures. The correlation coefﬁcient was 0:55. The\nline indicates the linear regression result and the R-squared value\nwas0:30.\nFigure 3: Average number of ﬁnger pressuring strings and\ndifﬁculty rating of 50tablatures. The correlation coefﬁcient\nwas0:51. The line indicates the linear regression result and the\nR-squared value was 0:26.\nnotes and chords in the audio signal. Given those music\nelements, dynamic programming or Viterbi decoding\nwith HMM has been leveraged to output reasonable\nﬁngerings [1, 10, 12].\n3. PLAYING DIFFICULTY OF A GUITAR SOLO\n3.1 Analysis of guitar tablatures\nWe ﬁrst investigated what are the factors that affect the\nplaying difﬁculty. We collected tablatures from a web\nsite distributing classical guitar music2. These tabs were\nwritten in a plain text format and did not have uniformity\nin data structure. We therefore implemented a parser to\nretrieve structured tab information. Since Song2Guitar\nassumes only the standard tuning (E-A-D-G-B-E), we\nexcluded tabs that were instructed to play in another\ntuning. Furthermore, we also excluded scores for guitar\nduo as we focus on guitar solo covers.\n2http://www.classtab.orgProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 569For each tablature, we considered and calculated two\nfactors: the average movements of the index ﬁnger of\na hand to hold the guitar, and the average number of\nﬁngers to press the strings. The position of index ﬁnger\nis determined as follows: if the ﬁngering contains a barre,\nwe use the fret position of the barre, otherwise the position\nof the index ﬁnger is set to be the minimum fret number\namong the frets being pressed. We hypothesized that these\ntwo factors affect the playing difﬁculty of a guitar solo.\n3.2 Subjective test to evaluate the playing difﬁculty\nWe veriﬁed our hypothesis by asking proﬁcient guitarists\nto rate the difﬁculty of the tabs. Five independent\nraters subjectively evaluated the difﬁculty with a 7-point\nLikert scale (1: easiest – 7: most difﬁcult). The raters\nwere instructed to consider only the complexity of the\nﬁngerings of the left hand. As they respectively rated\nrandomly-selected 10 tabs, we consequently obtained\n50 ratings. Fig. 2 and Fig. 3 show the plots of our\nhypothesized features (average movement of an index\nﬁnger, average number of ﬁngers to press strings) and the\nresult of difﬁculty ratings. The correlation coefﬁcients\ncalculated with these two features and the ratings were\n0:55and0:51, respectively. We also conducted a linear\nregression on data. The regression results are also shown\nin Fig. 2 and Fig. 3. R-squared values for these two\nregressions were 0:30and0:26, respectively. These results\nindicated that the tabs were evaluated to be more difﬁcult\nwhen the values of both features get larger.\n4. CREATING DIFFICULTY-A WARE GUITAR\nARRANGEMENTS FROM MUSIC SIGNALS\n4.1 Our problem setting\nIn solving the problem of generating guitar solo covers\nfrom audio signals of popular music, we want to maintain\nand reproduce major characteristics of an original song in a\ngenerated guitar solo cover. The followings are the major\ncharacteristics that most songs in popular music have in\ncommon.\n\u000fIt contains a clear melody line that is performed by\na vocal part.\n\u000fIt contains a bass line corresponding to a chord\nsequence.\n\u000fIt gives a rhythmic groove emerged from sounds of\nrhythmic instruments.\nAlthough previous work of generating guitar arrangements\nfrom symbolic musical scores [7] formulates the problem\nby using HMM, it have not tested with audio input. To\ngenerate from polyphonic music audios of popular music,\nwe formulate the problem by a novel extension of HMM.\nWe also propose how we can generate various results with\ndifferent levels of playing difﬁculty for guitarists.4.2 Guitar Arrangement by using HMM\nWe start from reviewing how HMM can be applied to\nthe ﬁngering decision problem. Suppose we have a\ncollection of guitar music scores, and we want to model\nthis collection statistically. This means that we need to\nobtain a function that returns high probability if the music\nseems to be included in the guitar music collection, and\nlow probability when the music is obviously not a guitar\nmusic. Designing a generative model is one method to\nachieve this.\nThe generative process of a guitar music is apparently\nthe process of performing a guitar instrument. When the\nguitar is played, one hand holds the neck and its ﬁngers\npress strings on the frets. Fingers of the other hand pluck\nthe strings, and eventually a sound is generated. We can\nsee that the output sound is determined when the states of\nboth hands are determined.\nIn terms of the hand to press the strings, it is less likely\nto observe a drastic change of the hand form in a very\nshort duration because of physical constraints of the human\nbody. It is also unlikely to observe a long distance move of\nposition of a hand to hold the neck of a guitar. Since these\ntwo aspects are relationships between the current and the\nprevious state of holding the neck, we can model them by\nthe ﬁrst-order Markov chain. Let Xtbe the ﬁngering at\ntimet. We can deﬁne a probability for observing ﬁngering\nXt+1asP(Xt+1jXt).Xtcontains four components each\nof which corresponds the state of each ﬁnger of the left\nhand. Each component has two values: one indicates the\nstring index of a guitar to put pressure on, and the other\nindicates the fret number to put the ﬁnger on. Fret number\n0 indicates that the ﬁnger does not touch any string.\nThe output sound is audible when strings are plucked.\nThe sounding notes are biased by the ﬁngering. Let Yt\nbe the set of notes played at time t, such as set consisting\nof C3, E4 and G4. Ytfollows a probability distribution\nP(YtjXt)which models the playing notes biased from the\nﬁngering.\nA guitar performance can be realized as a time sequence\nof both the ﬁngering ( XT\n1=X1\u0001\u0001\u0001XT) and the plucking\nof strings at each ﬁngering ( YT\n1=Y1\u0001\u0001\u0001YT). Note\nthatTindicates the length of a sequence, not indicating\ntransposition. The probability of generating notes from\nthe given ﬁngering is calculated by the product of these\nprobabilities as:\nP\u0000\nYT\n1jXT\n1\u0001\n=TY\nt=1P(YtjXt)P(XtjXt\u00001):(1)\nSince the ﬁngering cannot be observed from the\nguitar music afterwards, XT\n1is hidden and therefore\nthis probabilistic model is called as hidden Markov\nmodel.P(YtjXt)is called as emission probability, and\nP(XtjXt\u00001)is called as transition probability. By using\nViterbi decoding, we can efﬁciently estimate the most\nlikely ﬁngerings which maximize the likelihood in terms\nofXT\n1[23].\nNow we can extend the generative model discussed\nabove to let the model generate music that is not necessary570 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017to be guitar music, but could be arranged into guitar music.\nIn particular, the emission probability is revised so that the\nmodel can output notes which are octave higher or lower\nthan the notes of the actually played pitches by plucking\nthe string. As Hori et al. formulated in their works [7–9],\nthe emission probability is set to allow the number of notes\nmore than the guitar can perform simultaneously.\nBy executing the Viterbi algorithm with this extension,\nwe can obtain a sequence of ﬁngering from not only\nguitar music but also from any music which is not\noriginally composed for a guitar. Since the existence of\nproper ﬁngering is the necessary condition for a guitar\narrangement, we can generate a guitar solo cover by the\nabove extension of ﬁngering decision formulation.\n4.3 Creative MIR Approach to Guitar Arrangement\n4.3.1 Leveraging MIR technologies\nOne of the novelties of this research is the further extension\nof the generative model so that it can generate guitar\nsolo covers from polyphonic music audios by leveraging\nMIR technologies. Since the melody, beats, and chords\nare main elements that can be reﬂected in a guitar solo\narrangement, it is not necessary to try to obtain all notes\nby using multi-pitch analysis. We therefore use methods\nthat can estimate the melody (F0 contour), beats, and\nchords in polyphonic audio including drums. We show that\nthese methods developed in the MIR community largely\ncontribute to generating a guitar solo cover.\nThe melody estimation here assumes that the melody\nis sung by a singer. We ﬁrst extract a singing voice track\nby using an existing singing voice extraction method. We\nthen applied a melody estimation method proposed by\nGoto [5] to obtain the F0 contour. We also smooth the F0\ncontour by using an FIR low-pass ﬁlter with 5 Hz cutoff\nfrequency in order to remove the vibrato. To discretize\nthe F0 contour into musical notes, we used beat estimation\nresults to approximately obtain what musical note is played\nas the melody line at every 16th note.\nChord estimation provides chord labels (chord names).\nSince the label contains “on-chords” such as “C/E” or “C\non E”, we literally use the bass note described in the chord\nlabel. We used a chord estimation method developed by\nKorezeniowski et al. [13], which is available in Madmom\nor an audio signal processing library written in python [2].\nFinally, the beat estimation plays an important role in\ngenerating a guitar solo cover. The beat estimation results\ngive us a set of segments corresponding to quarter notes.\nBy dividing every quarter note into four parts, we obtain\na ﬁner set of segments with the resolution of 16th notes.\nThese 16th-note segments can be used to quantize the\nF0 contour of the melody line as explained above, and\nalso quantize chord estimation results. In other words, all\nnote lengths extracted from the audio are quantized into\ninteger multiples of the 16th-note duration. We used a beat\nestimation method proposed by B ¨ocket al. [3] which is\nalso available in Madmom [2].4.3.2 Emission probability\nBased on the results of these estimation methods, we set\nthe emission probability as follows so that the HMM can\nhandle music audio to generate a guitar solo cover:\nP(YtjXt)/Pchord (CtjXt) +Pmelody (MtjXt)\n+Pbass(BtjXt) (2)\nThe subscript tdenotes the index of the onset. Since the\nonsets are not apparent in audio signals, we regard the\ntiming of a sudden increase of power in singing voice and\nthe timing of every chord change as onset timings. The\nonset timings are discretized by using the beat estimation\nresult.Ydenotes the audio segment with 16th-note\nduration.C,M, andBare the chord label, melody pitch,\nand bass pitch, respectively. Xdenotes the ﬁngering to\npress the fret, which is a set of the pressing position of each\nﬁnger including open strings. Open strings are represented\nas pressing the imaginary 0thposition of the fret.\nProbability Pchord is set based on how the current\nﬁngering achieves the chord observed at the time. For\nexample, when the ﬁngering is given to play “C, E, G”,\nthe probability for observing “C maj7” would be high, but\nthe probability for observing “F# maj” would be low. This\ncan be measured by the number of elements in intersection\nbetween the set of notes derived from the ﬁngering and the\nchord label. In this example, the set of notes for “C maj7”\nis “C, E, G, B”, and the set of notes for “F# maj” is “F#,\nA#, C#”. The probability for observing “C maj7” is higher\nsince the intersection has “C, E, G” (3 elements) whereas\n“F# maj” has no elements as intersection. We implemented\nthis as follows:\nPchord (CtjXt)/exp (\u0000\u000b\u0001# (N(Ct)\\N(Xt)))(3)\nwhereN(\u0001)denotes the set of consisting notes of chord\nlabel or guitar ﬁngering. We adjusted the parameter to be\n\u000b= 3:0in our experiments.\nProbability Pmelody is designed by considering how\nthe highest note of the playing notes with the ﬁngering\nis relevant to the melody pitch observed in the acoustic\nsignal. Let M(Xt)be the highest note could be played\nfrom the current ﬁngering. The probability can be designed\nas:\nPmelody (MtjXt)/(1:0 (Mt=M(Xt))\n\"1(Mt=M(Xt) + 12n(n6= 0))\n\"2otherwise\n(4)\nwhere the parameters are set as \"1= 0:3and\"2= 10\u00005.\nThese parameters were set heuristically by iteratively\ngenerating and subjectively evaluating the results.\nFinally, probability Pbassis set similarly to Pmelody . Let\nB(Xt)be the lowest note that could be played from the\ncurrent ﬁngering Xt. The probability is designed as:\nPbass(BtjXt)/(1:0 (Bt=B(Xt))\n\"1(Bt=B(Xt) + 12n(n6= 0))\n\"3otherwise\n(5)\nwhere the parameter \"1shared the same value as in\nPmelody , and\"3was set as\"3= 0:0027 in our experiment.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 571Figure 4: User interface of the Song2Guitar system.\n4.3.3 Transition probability\nFor setting transition probability P(Xt+1jXt), we\nbasically followed the formulation by Hori et al. [7]. We\ndeﬁned the transition probability given the time interval dt\nbetween onsets as:\nP(Xt+1jXt)/1\n2dtexp\u0012\n\u0000\u0015mjI(Xt+1)\u0000I(Xt)j\ndt\u0013\n\u00021\n1 +I(Xt+1)\u00021\n1 +W(Xt+1)\n\u00021\n1 +N(Xt+1)\n(6)\nwhereI(X)denotes the position of an index ﬁnger when\nholding the fret of a guitar with a ﬁngering X.W(X)\ndenotes the length between the leftmost fret used and the\nrightmost fret used under the ﬁngering X.N(X)denotes\nthe number of ﬁngers used to achieve the ﬁngering X.\n4.4 Controlling the Degree of Difﬁculty\nBased on the survey described in section 3, we determined\nthe following two parameters to control the playing\ndifﬁculty for generating a guitar solo cover: the average\nmovement of the index ﬁnger of a hand to hold the guitar,\nwhich is denoted as amove, and the average number of\nﬁngers to press the strings, which is denoted as astring .\nSong2Guitar supports three different levels of playing\ndifﬁculty: EASY , NORMAL, and HARD. To create these\nlevels by changing amove andastring , we adaptively\nrestricted the use of ﬁngering Xaccording to the following\nconstraints:\nEASY :amove\u00142:0 &&astring\u00142:0\nNORMAL : amove\u00144:0 &&astring\u00143:0\nHARD : use all available \fngerings :(7)\n5. INTERFACE DESIGN OF SONG2GUITAR\nSong2Guitar aims at not only generating a guitar solo\ncover automatically but also enabling a guitarist to easily\npractice and perform the generated result. Fig. 4 shows themain interface of Song2Guitar. The design of the interface\nand the results generated by our system are available on the\nweb3.\nBecause the tablature form is more intuitive than the\nmusic score, Song2Guitar visualizes the tablature score\nof the generated cover song. This tablature score scrolls\nautomatically while playing since a guitarist uses both\nhands to perform the guitar and no hands are left to control\nthe system.\nWe also implemented an interface to control the playing\ndifﬁculty of the results. When we aim at creating playable\narrangements for human guitarists, it is important to\ncontrol how difﬁcult the generated score is. Guitarists\nwould be discouraged if the score is too difﬁcult or too\neasy for them.\nThe tablature shown in the interface contains additional\nnotations to make the practice and performance easier.\nNumbers in colored circles on the strings indicate the fret\nthat the guitarist should press on the string. The indicator\nwith a purple vertical line (in the left of Fig. 4) shows the\ntiming to pluck the string.\nThe interface of Song2Guitar also supports\nnon-proﬁcient guitarists to ﬁnd the position to press\nthe indicated frets. Usually a guitarist needs to prepare\nthe hand form to press the fret in advance of plucking the\nstrings. Even though the tablature score is shown, a novice\nguitarist often gets stuck in keeping ﬁnding where to put\ntheir left hand to hold the neck of a guitar. This is because\nthe tablature usually indicates only the ﬁngerings on the\nfrets, but does not indicate the position of the left hand to\nhold the neck of the guitar. Therefore we implemented\nto show small diagrams (shown below of the tablature in\nFig. 4) representing how to place the ﬁngers in a similar\nfashion to a guitar ﬁngerboard. The diagram is shown\nwhen there is a position change in a hand to hold the frets.\nThe Song2Guitar interface also supports a\ndemonstration mode which playbacks the generated\ntablature by using synthesized guitar sounds so that music\nlisteners can simply enjoy the system output.\n3https://youtu.be/fN4-ibh7ZDI572 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. EV ALUATION\nTo evaluate how the performing difﬁculty varied among the\ngenerated guitar solo covers, we conducted an experiment\nin a qualitative evaluation approach.\n6.1 Experimental setting\nWe asked a guitarist who is proﬁcient in playing the\nclassical guitar to participate in the evaluation. The\nguitarist was male and 24 years old, and had an experience\nin playing the acoustic guitar (both steel and nylon strings)\nfor around six years.\nWe used RWC-MDB-P-2001 No.7 from the RWC\nMusic Database [6] to generate a guitar solo cover.\nWe generated three different covers with different levels\nof difﬁculty (EASY , NORMAL, and HARD) by using\nSong2Guitar. To focus on evaluating varying difﬁculty,\nwe manually corrected estimated beats and chords before\ngenerating them.\nThe guitarist was ﬁrst asked to practice each score for\n15 minutes. Since the duration of generated pieces were\nabout ﬁve minutes long and it was too long to practice\nthe entire song, we asked the guitarist to practice only\nthe intro, the ﬁrst verse, and the chorus section. After\nthe practice, we asked the guitarist to play all designated\nsections of each cover. Finally, we conducted a short\ninterview to obtain comments on Song2Guitar. The\nobtained comments were originally in Japanese, and they\nwere translated into English as shown in this paper.\n6.2 Evaluation results\nWe obtained a comment indicating that the participant\nenjoyed using the system:\nI think this is a really great app.. I can play a song\nendlessly, and it was like some kind of a game.\nWe also found a comment to indicate that our system\ngenerated covers in three different levels of playing\ndifﬁculty (EASY , NORMAL, and HARD):\nWell, playing difﬁculties were appropriate, difference\nbetween NORMAL and HARD makes sense.\nAlthough we intended to make three covers as getting\ngradually difﬁcult, the participant commented that the\nplaying difﬁculty of EASY and NORMAL were reversed:\nEASY score was not easy, it was more difﬁcult than\nNORMAL one, for me. The HARD score was like in\nthe middle of NORMAL and EASY.\nThe participant reported why “EASY score was not easy”\nas follows:\nI guess that it’s easier when it consists of chords\n(multiple notes) moderately than full of simple notes.\nChords are the basic form, and I can ﬁgure out how to\ndo ﬁngering in my mind. When only two notes appear\nin the tab, of course, I can ﬁgure out the ﬁngerings,\nhowever, it didn’t go well [...]\nThis comment indicated that smaller number of notes are\nnot always easy to play. The ﬁngering of chords provides a\nbasic form, and a guitarist is more familiar with it than the\nother irregular ﬁngerings for fewer notes.\nThe participant also pointed out the playing difﬁculty\ncomes from the note value of the generated results.The difﬁculty is, I think it’s easy if all notes were\neighth note. Sixteenth note is difﬁcult to ﬁgure out the\ntiming.\nHe also indicated the issue in the interface design:\nIt’s hard to understand beats and timings of notes with\nthe interface. I appreciate if every half beat were\nhighlighted, somehow.\n7. DISCUSSION\nWe conﬁrmed that Song2Guitar was able to generate\nguitar solo covers from polyphonic audio of popular music\nby leveraging MIR technologies. We found that the\nHMM formulation to generate guitar solo combined with\nestimation of melody (F0), beats, and chords was effective\neven from music audio which multi-pitch analysis cannot\nbe sufﬁciently applied to.\nWe also found that Song2Guitar was able to generate\noutput with different playing difﬁculties. We introduced\ntwo parameters: the average movement of an index ﬁnger\nand the average number of ﬁngers to press the strings,\nto control the playing difﬁculty. The evaluation results,\nhowever, suggested that there would be more factors that\naffect the playing difﬁculty. One possible factor for\ndetermining the difﬁculty is the familiarity of particular\nﬁngerings such as chords.\nThe interface of Song2Guitar enabled the player to\npractice and perform the generated result. The comments\nobtained in the experiment revealed that the rhythms of the\ngenerated covers were sometimes hard to recognize. The\ninterface did not visualize the timing except for showing\nthe indicator bar. Highlighting half beats would help the\nplayer recognize the rhythm much easier.\nThe future work of this research is to enable\nSong2Guitar to generate cover songs in real time\nconsidering the player’s proﬁciency. Conducting an\nobjective evaluation is also included in future work.\nSince the generative model is designed as a probabilistic\nmodel, we can verify the ﬁngering model by calculating\ncross-entropy.\n8. CONCLUSION\nWe proposed Song2Guitar that generates guitar solo\ncovers from polyphonic music signals of popular songs.\nThe formulation using HMM was combined with MIR\ntechnologies so that it can generate covers considering\nthe melody, bass and rhythm of the songs. Furthermore,\nSong2Guitar generated covers with different levels of\nplaying difﬁculty. The interface was implemented and a\nguitarist succeeded in playing different guitar solo covers.\nIn the future, cover song generation from music audio\nsignals will be further improved by leveraging other MIR\ntechnologies.\n9. ACKNOWLEDGEMENT\nThis work was supported in part by JST ACCEL Grant\nNumber JPMJAC1602, Japan.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 57310. REFERENCES\n[1] Ana M. Barbancho, Anssi Klapuri, Lorenzo J. Tardon,\nand Isabel Barbancho. Automatic transcription of\nguitar chords and ﬁngering from audio. IEEE/ACM\nTASLP , 20(3):915–921, 2011.\n[2] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter,\nFlorian Krebs, and Gerhard Widmer. madmom: a new\nPython Audio and Music Signal Processing Library. In\nProceedings of the 24th ACM International Conference\non Multimedia , pages 1174–1178, Amsterdam, The\nNetherlands, 10 2016.\n[3] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nJoint beat and downbeat tracking with recurrent neural\nnetworks. In Proc. ISMIR , ISMIR ’16, 2016.\n[4] Matthew E. P. Davies, Philippe Hamel, Kazuyoshi\nYoshii, and Masataka Goto. Automashupper:\nAutomatic creation of multi-song music mashups.\nIEEE/ACM TASLP , 22(12):1726–1737, Dec 2014.\n[5] Masataka Goto. A real-time music-scene-description\nsystem: predominant-f0 estimation for detecting\nmelody and bass lines in real-world audio signals.\nSpeech Communication , 43(4):311 – 329, 2004.\nSpecial Issue on the Recognition and Organization of\nReal-World Sound.\n[6] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nPopular, classical and jazz music databases. In Proc.\nISMIR , volume 2 of ISMIR ’02 , pages 287–288, 2002.\n[7] Gen Hori, Hirokazu Kameoka, and Shigeki Sagayama.\nInput-output HMM applied to automatic arrangement\nfor guitars. Journal of Information Processing ,\n21(2):264–271, 2013.\n[8] Gen Hori and Shigeki Sagayama. HMM-based\nautomatic arrangement for guitars with transposition\nand its implementation. In Proc. ICMC/SMC , 2014.\n[9] Gen Hori and Shigeki Sagayama. Minimax viterbi\nalgorithm for HMM-based guitar ﬁngering decision. In\nProc. ISMIR , 2016.\n[10] Eric J. Humphrey and Juan P. Bello. From music\naudio to chord tablature: Teaching deep convolutional\nnetworkds to play guitar. In Proc. IEEE ICASSP , pages\n7024–7028, 2014.\n[11] Eric J. Humphrey, Douglas Turnbull, and Tom Collins.\nA brief review of Creative MIR. Proc. ISMIR (Late\nBreaking/Demo Session) , 2013.\n[12] Christian Kehling, Jakob Abesser, Chirstian Dittmar,\nand Gerald Schuller. Automatic tablature transcription\nof electric guitar recordings by estimation of score- and\ninstrument-related parameters. In Proc. DAFx , 2014.[13] Filip Korzeniowski and Gerhard Widmer. Feature\nlearning for chord recognition: the deep chroma\nextractor. In Proc. ISMIR , ISMIR ’16, pages 37–43,\n2016.\n[14] Robert Macrae and Simon Dixon. Guitar tab mining,\nanalysis and ranking. In Proc. ISMIR , pages 453–458,\n2011.\n[15] Matt McVicar, Satoru Fukayama, and Masataka Goto.\nAutoleadguitar: Automatic generation of guitar solo\nphrases in the tablature space. In Proc. IEEE ICSP ,\npages 599–604, Oct 2014.\n[16] Matt McVicar, Satoru Fukayama, and Masataka Goto.\nAutorhythmguitar: Computer-aided composition for\nrhythm guitar in the tab space. In Proc. ICMC/SMC ,\n2014.\n[17] Matt McVicar, Satoru Fukayama, and Masataka Goto.\nAutoguitartab: Computer-aided composition of rhythm\nand lead guitar parts in the tablature space. IEEE/ACM\nTASLP , 23(7):1105–1117, 2015.\n[18] Graham Percival, Satoru Fukayama, and Masataka\nGoto. Song2quartet: A system for generating string\nquartet cover songs from polyphonic audio of popular\nmusic. In Proc. ISMIR , pages 114–120. Citeseer, 2015.\n[19] D.P. Radicioni. Computational Modeling of Fingering\nin Music Performance . PhD thesis, Universit `a di\nTorino, Centro di Scienza Cognitiva, 2005.\n[20] Aleksander Radisavljevic and Peter Driessen. Path\ndifference learning for guitar ﬁngering problem. In\nProc. ICMC , volume 28, 2004.\n[21] Samir I. Sayegh. Fingering for string instruments with\nthe optimum path paradigm. Computer Music Journal ,\n13(3):76–84, 1989.\n[22] Daniel R. Tuohy and W. D. Potter. GA-based music\narranging for guitar. In Proc. IEEE Congress on\nEvolutional Computation , pages 1065–1070, 2006.\n[23] Andrew Viterbi. Error bounds for convolutional codes\nand an asymptotically optimum decoding algorithm.\nIEEE Trans. Inf. Theor. , 13(2):260–269, September\n2006.\n[24] Kazuaki Yazawa, Katsutoshi Itoyama, and Hiroshi G.\nOkuno. Automatic transcription of guitar tablature\nfrom audio signals in accordance with player’s\nproﬁciency. In Proc. IEEE ICASSP , pages 3146–3150,\n2014.\n[25] Kazuaki Yazawa, Daichi Sakaue, Kohei Nagira,\nKatsutoshi Itoyama, and Hiroshi G. Okuno.\nAudio-based guitar tablature transcription using\nmultipitch analysis and playability constraints. In\nProc. IEEE ICASSP , pages 196–200, 2013.574 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Piece Identification in Classical Piano Music Without Reference Scores.",
        "author": [
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417673",
        "url": "https://doi.org/10.5281/zenodo.1417673",
        "ee": "https://zenodo.org/records/1417673/files/ArztW17.pdf",
        "abstract": "In this paper we describe an approach to identify the name of a piece of piano music, based on a short audio ex- cerpt of a performance. Given only a description of the pieces in text format (i.e. no score information is pro- vided), a reference database is automatically compiled by acquiring a number of audio representations (performances of the pieces) from internet sources. These are transcribed, preprocessed, and used to build a reference database via a robust symbolic fingerprinting algorithm, which in turn is used to identify new, incoming queries. The main chal- lenge is the amount of noise that is introduced into the identification process by the music transcription algorithm and the automatic (but possibly suboptimal) choice of per- formances to represent a piece in the reference database. In a number of experiments we show how to improve the identification performance by increasing redundancy in the reference database and by using a preprocessing step to rate the reference performances regarding their suitability as a representation of the pieces in question. As the results show this approach leads to a robust system that is able to identify piano music with high accuracy – without any need for data annotation or manual data preparation.",
        "zenodo_id": 1417673,
        "dblp_key": "conf/ismir/ArztW17",
        "keywords": [
            "audio excerpt",
            "piece of piano music",
            "reference database",
            "symbolic fingerprinting",
            "internet sources",
            "transcription",
            "preprocessing",
            "robust system",
            "high accuracy",
            "manual data preparation"
        ],
        "content": "PIECE IDENTIFICATION IN CLASSICAL PIANO MUSIC WITHOUT\nREFERENCE SCORES\nAndreas Arzt, Gerhard Widmer\nDepartment of Computational Perception, Johannes Kepler University, Linz, Austria\nAustrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria\nandreas.arzt@jku.at\nABSTRACT\nIn this paper we describe an approach to identify the\nname of a piece of piano music, based on a short audio ex-\ncerpt of a performance. Given only a description of the\npieces in text format (i.e. no score information is pro-\nvided), a reference database is automatically compiled by\nacquiring a number of audio representations (performances\nof the pieces) from internet sources. These are transcribed,\npreprocessed, and used to build a reference database via a\nrobust symbolic ﬁngerprinting algorithm, which in turn is\nused to identify new, incoming queries. The main chal-\nlenge is the amount of noise that is introduced into the\nidentiﬁcation process by the music transcription algorithm\nand the automatic (but possibly suboptimal) choice of per-\nformances to represent a piece in the reference database.\nIn a number of experiments we show how to improve the\nidentiﬁcation performance by increasing redundancy in the\nreference database and by using a preprocessing step to\nrate the reference performances regarding their suitability\nas a representation of the pieces in question. As the results\nshow this approach leads to a robust system that is able\nto identify piano music with high accuracy – without any\nneed for data annotation or manual data preparation.\n1. INTRODUCTION\nEfﬁcient algorithms for content-based audio retrieval en-\nable systems that allow users to browse and explore music\ncollections (see e.g. [10] for an overview). In this con-\ntextaudio ﬁngerprinting algorithms which permit the fast\nidentiﬁcation of an unknown recording (as long as an al-\nmost exact replica is contained in the reference database)\nplay an important role. For this task there exist highly efﬁ-\ncient algorithms that are in everyday commercial use (see\ne.g., [3, 6, 13, 15–17]).\nHowever, these algorithms are not able to identify dif-\nferent performances of the same piece of music, as they\nare not designed to work in the face of musical variations\nsuch as different tempi, expressive timing, differences in\nc\rAndreas Arzt, Gerhard Widmer. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Andreas Arzt, Gerhard Widmer. “Piece Identiﬁcation in Clas-\nsical Piano Music Without Reference Scores”, 18th International Society\nfor Music Information Retrieval Conference, Suzhou, China, 2017.instrumentation, ornamentation and other performance as-\npects. Regarding classical music, the identiﬁcation of per-\nformances that derive from a common musical score is of\nspecial interest, as in general there exists a large number\nof performances of the same piece (and new renditions are\nperformed every day).\nThis task is generally called audio matching (or, mostly\nin the context of popular music, cover version identiﬁca-\ntion, see e.g. [14]). A common approach to solve this prob-\nlem is to use an audio alignment algorithm. This is com-\nputationally expensive, as it basically involves aligning the\nquery snippet with every position within every audio ﬁle in\nthe database (see [12], and [11] for a indexing method that\nmakes the problem more tractable). Furthermore, due to\nthe coarse feature resolution of these algorithms, relatively\nlarge query sizes are needed.\nAs there exist efﬁcient ﬁngerprinting algorithms, it\nseems natural to try to adapt them to the problem of cover\nversion identiﬁcation. A ﬁrst study towards this is pre-\nsented in [9], where the authors focused on the suitabil-\nity of different low-level features as a basis for ﬁngerprint-\ning algorithms, but neglected the problem of tempo dif-\nferences between performances. In [1] an extension to a\nwell-known ﬁngerprinting algorithm [17] is proposed that\nmakes it invariant to the global tempo. With the help of\nanaudio transcription algorithm for piano music (see [5])\na system was built that, given a short audio query, almost\ninstantly returns the corresponding (symbolic) score from\na reference database – despite the fact that audio transcrip-\ntion is a very hard problem and thus introduces a lot of\nnoise in the process.\nIn this paper we show how to use this algorithm in the\nabsence of symbolic scores to identify unknown perfor-\nmances, using a reference database based on other perfor-\nmances of the pieces in question. As symbolic scores are\noften not readily available, this increases the applicability\nof this algorithm in real life systems. The downside of this\napproach is that now audio transcription is used for both\nthe data contained in the reference database and for the\nqueries, which introduces even more noise. Furthermore,\nthe transcription algorithm we are using is optimised on pi-\nano sounds, which for now limits the proposed system to\npiano music only.\nWe are going to describe this approach in the context of\na system geared towards fully automatic identiﬁcation of\nclassical piano music, in the sense that even the creation354of the collection of audio recordings, which is needed to\nperform the identiﬁcation task, is automated. The moti-\nvation for this is to reduce the amount of costly manual\nannotation to a minimum, and instead facilitate available,\nalbeit noisy, web sources like YouTube1orSoundcloud2.\nThe main challenge in this setting is the noise introduced\ninto the identiﬁcation process via multiple processes (auto-\nmatic retrieval of reference performances, audio transcrip-\ntion of reference performances, and audio transcription of\nthe query). In the paper we will show how to deal with\nthis amount of noise by increasing redundancy in the ref-\nerence database and by an automatic selection strategy for\nthe reference performances.\nThe paper is structured as follows. Section 2 gives an\noverview of the proposed system. Then, in Section 3 the\ndata we are using for our experiments is described. Sec-\ntions 4, 5, 6 and 7 describe the core experiments of the\npaper, showing that our approach is robust enough to cope\nwith the multiple sources of noise and performs well in our\nexperiments. A brief outlook on possible improvements\nand applications is given in Section 8.\n2. SYSTEM OVERVIEW\nIn this section we are going to describe the piece identiﬁ-\ncation system that will be used throughout the paper. The\nmain goals of the system are 1) to automate the process of\ncompiling a reference database, thus making manual anno-\ntations obsolete, and 2) based on this reference database,\nallow for robust and fast piece identiﬁcation. Figure 1 de-\npicts how the components interact with each other.\nThe system is based on a Database Deﬁnition ﬁle,\nwhich is a list of pieces that are to be included in the\ndatabase. On this list each piece is represented by an ID,\nthe name of the composer and the name of the piece, in-\ncluding identiﬁers like the opus number (see Figure 2 for\nan excerpt of the list). We would like to emphasise once\nmore that this is the only input our system needs (in ad-\ndition to a source from which the recordings can be re-\ntrieved). All the data necessary to perform the identiﬁca-\ntion task is then prepared automatically. This also means\nthat extending the database is as easy as adding a new line\nto the text ﬁle, describing the new piece. The data in this\nﬁle also deﬁnes the granularity of the database. For ex-\nample, movements of a sonata could be represented as in-\ndividual pieces or combined as single piece – for our ex-\nperiments we took the latter approach. For our proof-of-\nconcept implementation we settled for 339 piano pieces of\nwell-known composers (Mozart, Beethoven, Chopin, Scri-\nabin, and Debussy), which already represents a substantial\nshare of the classical piano music repertoire.\nAWeb Crawler takes this list of pieces and retrieves\naudio recordings of performances of the pieces. In our\ncase we use a simple crawler for YouTube (an alternative\nwould be to use Soundcloud , amongst others). The queries\nare constructed by concatenating the name of the composer\n1https://www.youtube.com\n2https://soundcloud.comand the piece, and adding the word “piano”, to ensure that\nmainly piano performances are returned.\nNext, the collected recordings are fed into a Music\nTranscription Algorithm that takes the audio ﬁles and\ntranscribes them into series of symbolic events. For this\nstep we rely on a well known neural network based method\npresented in [5], more speciﬁcally the version that is avail-\nable as part of the Madmom library [4]. As input it takes a\nseries of preprocessed and ﬁltered STFT frames with two\ndifferent window lengths. The neural network consists of\na linear input layer with 324 units, three bidirectional fully\nconnected recurrent hidden layers with 88 units, and a re-\ngression output layer with 88 units, which directly repre-\nsent the MIDI pitches. The output of the transcription al-\ngorithm is a list of detected musical events, represented by\ntheir pitches and start times. For details we refer the reader\nto [5]. This algorithm exhibits state of the art results for\nthe task of piano transcription, as was demonstrated at the\nMIREX 20143. Still, polyphonic music transcription is a\nvery hard problem, and thus the output of this transcription\nalgorithm contains a relatively large amount of noise, of\nwhich the following components need to be robust to.\nThe Automatic Preprocessing step is concerned with\nthe question of which of the downloaded recordings for\neach piece should be used in our ﬁngerprint database. In\nthis paper we discuss three setups: take the top match re-\nturned by the web crawler (see Section 4), take the top ﬁve\n/ ﬁfteen matches returned by the web crawler (see Section\n5), and download 30 recordings for each piece, rank them\nautomatically via comparing them to each other and use\nthe top recordings identiﬁed via this approach (see Section\n6). This means that in the latter two experiments a single\npiece is represented by multiple recordings, adding redun-\ndancy to the reference database.\nThe transcribed sequences of symbolic event informa-\ntion, i.e. sequences of pairs (pitch;onset time ), are fed to\ntheTempo-invariant Symbolic Fingerprinter , to build a\ndatabase of ﬁngerprints that later on can be used to iden-\ntify queries. The algorithm is used as described in [1],\nthus it will be summarised here very brieﬂy. The princi-\nple idea of the ﬁngerprinting algorithm is to represent an\ninstance (in this case a transcribed performance, represent-\ning a piece) via a large number of local, tempo-invariant\nﬁngerprint tokens. These tokens are created based on the\npitches of three temporally local note events, together with\nthe ratio of their distances in time. Due to the way they are\ncreated, the tokens are invariant to the global tempo, and\ncan be stored in a hash table and efﬁciently queried for.\nAn incoming Query is processed in the same way as\nabove by the Music Transcription Algorithm . The re-\nsulting sequence of symbolic events is used to query the\nTempo-invariant Symbolic Fingerprinter for matches.\nTo do so, from the query the same kind of ﬁngerprint to-\nkens are computed, and matching tokens are retrieved from\nthe ﬁngerprint database. Finally, in this result set continu-\nous sequences of matching tokens, which are a strong in-\n3http://www.music-ir.org/mirex/wiki/2014:\nMIREX2014_ResultsProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 355Database DeﬁnitionList of Pieces (Text)Web CrawlerCrawl Web Source for Audio Recordings (e.g. YouTube)Music Transcription AlgorithmTranscribe Recordings (Performances of Pieces)Tempo-invariant Symbolic FingerprinterQueryAudio Snippet of an Unseen Performance of a PieceMusic Transcription AlgorithmTranscribe QueryAutomatic PreprocessingAutomatically Identify Suitable PerformancesQuery ResultsName of the Piece, corresponding to the QueryFigure 1 . System Overview\nID ; Composer ; P i e c e\n. . .\n1 7 ; Mozart ; Piano S o n a t a No . 17 i n B \u0000f l a t major K 570\n1 8 ; Mozart ; Piano S o n a t a No . 18 i n D major K 576\n1 9 ; Mozart ; F a n t a s y No . 1 wi th Fugue i n C major K 394\n2 0 ; Mozart ; F a n t a s y No . 2 i n C minor , K 396\n. . .\n4 1 ; Beethoven ; Piano S o n a t a No . 14 , Op . 27 , No . 2 ” Moonlight ”\n4 2 ; Beethoven ; Piano S o n a t a No . 15 , Op . 28 ” P a s t o r a l ”\n. . .\n16 8; Chopin ; Mazurka Op . 7 No . 5 i n C major\n16 9; Chopin ; Nocturne Op . 15 No . 1 i n F major\n17 0; Chopin ; Nocturne Op . 15 No . 2 i n F \u0000s h a r p major\n17 1; Chopin ; Nocturne Op . 15 No . 3 i n G minor\n. . .\n28 1; Debussy ; L 113 , C h i l d r e n ’ s Corner , Doctor Gradus ad Parnassum\n28 2; Debussy ; L 113 , C h i l d r e n ’ s Corner , Jimbo ’ s L u l l a b y\n. . .\n33 2; S c r i a b i n ; Piano S o n a t a No . 3 , Op . 23\n33 3; S c r i a b i n ; Piano S o n a t a No . 4 , Op . 30\n. . .\nFigure 2 . An excerpt of the ﬁle used for collecting the\ndatabase.\ndication that the query matches a speciﬁc part of a piece\nstored in the ﬁngerprint database, are identiﬁed (via a fast,\nhistogram based approach).\nTheQuery Result is a list of positions within the refer-\nence performances that were inserted into the database (see\nTable 1). The positions in the result set are ordered by their\nnumber of tokens matching the query. As can be seen, the\nresult set is actually more detailed than necessary for our\napplications scenario, as we are only interested in identify-\ning the respective piece, and not a speciﬁc reference perfor-\nmance (or even a position within reference performance).\nThus for the experiments in this paper we summarise all\noccurrences of a piece into one score by summing up the\nmatching scores of all its occurrences in the results set.\n3. GROUNDTRUTH DATA AND EXPERIMENTAL\nSETUP\nFor the experiments presented in this paper, ground truth\ndata, i.e. performances for which the composer and the\nname of the piece is known, is needed. We are using com-\nmercial recordings of a large part of the pieces contained\nin our database. This includes e.g. Uchida’s recordings of\nthe Mozart Sonatas, Brendel’s recordings of the Beethoven\nSonatas, Chopin recordings by Arrau, Pires and Pollini,\nand Debussy recordings by Pollini, Thibaudet, Zimerman.\nWe would like to emphasise that to get realistic results,\nin our experiments we made sure manually that no exact\nreplicas of these performances are contained in the auto-Piece ID Performance ID Time in Ref. Score\n1 0 99 351\n1 0 21 292\n1 4 16 109\n1 4 15 36\n1 4 148 36\n1 4 150 32\n10 48 368 7\n1 0 239 7\nTable 1 . An example of a result returned by the ﬁngerprint-\ning algorithm. This query was performed on a database in\nwhich multiple reference performances represent a piece of\nmusic, hence for the piece with ID 1 results for two perfor-\nmances are returned. The score is the number of matching\nﬁngerprint tokens for the given query at the speciﬁc time\nin the reference recording. For our purposes we summarise\nthe results per piece, i.e. the matching score for the piece\nwith ID 1 is 863, and for the piece with ID 10 it is 7.\nmatically downloaded data that is used to build the refer-\nence database later on. In total 370 tracks were selected\nand assigned manually to the respective pieces (roughly\n30 hours of music, or 665 000 transcribed events). Some\nof the tracks were assigned to the same piece, as e.g. the\nmovements of the sonatas are typically represented as dif-\nferent audio tracks, but are represented as a single piece in\nour database.\nThe experimental setup is as follows. We are going to\nuse the same set of randomly extracted queries for each ex-\nperiment. We are using three query lengths of 2, 5 and 10\nseconds (we only took queries though which had at least 10\ntranscribed notes, avoiding to e.g. query for silence), and\nextract for each length ten queries for each ground truth\nperformance (giving a total of 3 700 queries for each query\nlength). The experiments are based on different strategies\nto automatically compile the reference database. We start\nwith a simple baseline approach (Section 4) and then grad-\nually improve on it by introducing redundancy and a selec-\ntion strategy (Sections 5 to 7).\nAs evaluation measure we use the Recall at Rank k4.\n4We would like to note that the related measure Precision at Rank k\nis not useful in our experimental setup, as there will only be at most one\ncorrect result in the result set.356 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Query Length\n2 s 5 s 10 s\nRecall at Rank 1 0.28 0.38 0.46\nRecall at Rank 5 0.34 0.45 0.54\nRecall at Rank 10 0.35 0.47 0.55\nMean Reciprocal Rank 0.30 0.41 0.48\nMean Query Time 0.13 s 0.41 s 0.92 s\nTable 2 . Results of the baseline approach. The results are\nbased on 3 700 queries for each query length.\nThis is the percentage of queries which have the correct\ncorresponding piece in the ﬁrst kretrieval results. In our\nexperiments we look at the recall at ranks 1, 5 and 10. In\naddition, we also report the Mean Reciprocal Rank (MRR).\nMRR =1\njQjjQjX\ni=11\nrank i(1)\nHere, rank irefers to the rank position of the correct re-\nsult for the ithquery.\nThe mean query times (i.e. the mean time it takes to\nprocess a single query) given in the tables are based on a\ndesktop computer on a single core5. If needed, the compu-\ntation could easily be sped up by multi-threading the query\nprocess.\n4. BASELINE APPROACH\nThe baseline approach is very straightforward. The web\ncrawler is used to download the top result from the web\nsource for each piece on the list. The downloaded audio\nﬁles are transcribed and then processed by the ﬁngerprint-\ning algorithm to build the reference database, i.e. in the\nreference database each piece is represented by one perfor-\nmance. Note that due to the automatic process the database\ncan be quite noisy, as some of the pieces might be incom-\nplete (e.g. only a single movement of a piece), represented\nby more than the actual piece (if e.g. the performance\ndownloaded for the piece also contains other pieces, like a\nrecording of a full concert), or the representation is wrong\n(if the top result of the web crawler is actually a perfor-\nmance of some other piece).\nThe generated ﬁngerprint database is queried via the\nprepared excerpts of the collected ground truth data (see\nSection 3). The results of this ﬁrst experiment can be seen\nin Table 2. As can be seen, already in this scenario and\ndespite the small query sizes the method gives reasonable\nresults. For queries of length ten seconds the algorithm re-\nturns the correct name of the piece in close to 50% of the\ncases. A closer look at the results though showed that the\nmain problem with this simplistic approach is that, as ex-\npected, for many pieces the representation in the database\nis not correct or incomplete. This problem is tackled in the\nfollowing sections.\n5Intel Core i7 6700K 4 GHz with 32 GB RAM.Query Length\n2 s 5 s 10 s\nRecall at Rank 1 0.58 0.69 0.74\nRecall at Rank 5 0.72 0.84 0.90\nRecall at Rank 10 0.74 0.86 0.92\nMean Reciprocal Rank 0.64 0.77 0.84\nMean Query Time 0.34 s 0.81 s 2.49 s\nTable 3 . Results on the reference database based on mul-\ntiple recordings (the top ﬁve results according to the web\nsource) to represent each piece. The results are based on\n3 700 queries for each query length.\nQuery Length\n2 s 5 s 10 s\nRecall at Rank 1 0.76 0.87 0.91\nRecall at Rank 5 0.84 0.94 0.97\nRecall at Rank 10 0.86 0.95 0.98\nMean Reciprocal Rank 0.80 0.90 0.94\nMean Query Time 0.82 s 2.85 s 6.08 s\nTable 4 . Results on the reference database based on multi-\nple recordings (the top ﬁfteen results according to the web\nsource) to represent each piece. The results are based on\n3 700 queries for each query length.\n5. USING MULTIPLE INSTANCES PER PIECE\nA simple way to improve the performance of the system is\nto increase the redundancy within the reference database.\nInstead of relying on a single instance (recording) for each\npiece in the reference base, each piece is represented by\nmultiple recordings. For the ﬁrst experiment ﬁve perfor-\nmances per piece were downloaded using the web crawler.\nThe performances were processed in the same way as for\nthe baseline approach in Section 4 above and inserted into\nthe ﬁngerprint database. Then, on this database the same\nset of queries were performed. As described in Section 2,\nthe match score of a piece is computed by summing up the\nscores of the performances representing the piece in ques-\ntion (also see Table 1).\nTable 3 shows the results of this experiment. As can\nbe seen, the increased redundancy leads to a substantial\nincrease in identiﬁcation results, compared to the baseline\n(see Table 2). The added redundancy increases the chances\nthat for each piece at least one “good” performance (in the\nsense of corresponding to the piece and relatively easy to\ntranscribe) is contained in the reference database, and thus\nmitigates the problems caused by noise, at least to some\nextent.\nFor an additional experiment we increased the number\nof performances to ﬁfteen per piece. These results are\nshown in Table 4. This improved the results even further.\nThe downside of adding more instances to the ﬁngerprint\ndatabase is a signiﬁcant increase in computation time.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 357Query Length\n2 s 5 s 10 s\nRecall at Rank 1 0.54 0.68 0.74\nRecall at Rank 5 0.63 0.76 0.83\nRecall at Rank 10 0.64 0.78 0.85\nMean Reciprocal Rank 0.58 0.72 0.78\nMean Query Runtime 0.14 s 0.47 s 0.97 s\nTable 5 . Results on the reference database based on the top\nrecording selected via the proposed strategy to represent\neach piece. The results are based on 3 700 queries for each\nquery length.\nQuery Length\n2 s 5 s 10 s\nRecall at Rank 1 0.72 0.85 0.89\nRecall at Rank 5 0.82 0.92 0.96\nRecall at Rank 10 0.84 0.93 0.97\nMean Reciprocal Rank 0.77 0.88 0.92\nMean Query Time 0.49 s 1.71 s 3.83 s\nTable 6 . Results on the reference database based on mul-\ntiple recordings (top ﬁve recordings selected via the pro-\nposed strategy) to represent each piece. The results are\nbased on 3 700 queries for each query length.\n6. AUTOMATICALLY SELECTING SUITABLE\nREPRESENTATIONS\nA closer look at the results so far shows that increasing the\nredundancy in the reference database indeed leads to bet-\nter results, but also increases the computation time. The\nmain problem with our approach is that in addition to use-\nful data, the process also adds a lot of extra noise to the\nﬁngerprint database. The web crawler returns a consid-\nerable number of performances of the wrong piece, per-\nformances played on a different instrument, and perfor-\nmances recorded in very bad quality. This kind of data\nincreases the runtime and decreases the identiﬁcation ac-\ncuracy. In this section we present a method for identifying\nperformances in a given a set of candidates for a piece that\nmost probably are related to the piece in question, which\nalso enables us to discard performances that most proba-\nbly are noise. In this way we try to reduce the number\nof stored ﬁngerprint tokens, which generally decreases the\ncomputation time, while still achieving good identiﬁcation\nperformance.\nThus, for each piece we perform the following process\nto select appropriate representations. First, 30 recordings\nare downloaded via the web crawler. With a high probabil-\nity at least some of these are actually piano performances\nof the piece we are looking for, while the others might have\nnothing in common. The idea now is to ﬁnd a homoge-\nnous group within this set of candidates. To identify per-\nformances which are part of this group, we again employ\nthe symbolic ﬁngerprinting process, but limited to the setof candidate performances. To do so, the performances are\ntranscribed and inserted into a new ﬁngerprint database.\nThe intuition is that for a query extracted from the same\nset of candidate performances (that actually matches the\npiece), the ﬁngerprinter will likely return three kinds of re-\nsults. Firstly, the top result will be the performance the\nquery was taken from. This is a perfect ﬁt for all tokens,\nwhich results in the maximum score. Secondly, a number\nof other performances will probably also have a high score,\nidentifying them as being based on the same piece and\nas being transcribed in sufﬁcient quality. Thirdly, perfor-\nmances that actually belong to a different piece, or which\nare transcribed poorly, will score very low.\nBased on these observations, we designed the process\nof ranking the performances regarding their suitability to\nrepresent the piece in question as follows. For each of\nthe performances ten queries are randomly extracted (for\nour experiments we used a query length of ten seconds)\nand processed by the ﬁngerprinting algorithm. As in all\nother experiments, the results are summarised on the per-\nformance level (i.e. match scores of positions within the\nsame performance are summed up). Then, for each result\nthe score of the top match (i.e. of the performance the\nquery stems from) is stored, this performance is removed\nfrom the result set, and the remaining matching scores are\nnormalised by dividing by the top match score. The rea-\nsoning behind this is that the absolute scores depend on the\nparticulars of the query (foremost the length in the sense of\nthe number of notes, but also e.g. if the part in question is\nnormally played in a steady tempo or is subject to expres-\nsive tempo changes, which makes it harder to detect and\nleads to a lower score).\nThis results in 300 preprocessed and normalised result\nsets. The suitability of a performance to represent the piece\nin question is computed by summing up all the scores of\nall its occurrences in the result sets. The higher this value\nis for a performance, the more it has in common with the\nother performances assigned to the piece in question.\nBased on this ranking we repeat experiments from Sec-\ntions 4 and 5, but this time for each piece we select the top\none or top ﬁve performances, respectively, according to the\ncomputed rank within the candidate set for each piece. The\nresults are shown in Tables 5 and 6, which should be com-\npared to Tables 2 and 3, respectively. As can be seen the\nselection strategy increases the identiﬁcation performance\nfor both scenarios and for all query lengths.\nA comparison of Tables 6 and 4 shows that by using\nthe proposed selection strategy a lower number of perfor-\nmances (5 versus 15) is sufﬁcient to achieve comparable\nidentiﬁcation accuracy. The decreased number of tokens\nalso results in roughly half the computation time.\nThe runtime actually depends on a number of factors,\nmost importantly the size of the ﬁngerprint database. But\nof similar inﬂuence is the actual number of tokens that are\nreturned by the ﬁngerprint database for a speciﬁc query.\nThe reason is that each of these tokens has to be processed\nindividually to come up with the matching score. This also\nmeans that queries for pieces which are represented in the358 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017database by a large number of performances will actually\ntake longer to compute – a further argument in favour of\nthe selection strategy presented in this section.\n7. USING MULTIPLE QUERIES PER\nPERFORMANCE\nSo far the assumption was that we only have access to a\nsingle short query of two to ten seconds. If instead we have\naccess to a full recording, just querying for one short query\nwould be a suboptimal approach. Thus, we tried an addi-\ntional query strategy on the reference database based on\nthe performance selection strategy from Section 6 above.\nA standard approach for processing long queries (in\nthis case a whole performance) would be to apply shin-\ngling [2,7,8], i.e. splitting longer queries into shorter, over-\nlapping ones and track the results of these sub-queries over\ntime. Here, as proof of concept we use an even simpler\nmethod: we select ten random queries from the piece we\nwant to identify, process them individually and sum up the\nresults. This can be seen as adding redundancy (relying\non multiple queries instead of a single one) on the query\nside. We perform this experiment on the reference database\nbased on the top ﬁve selected recordings via the proposed\nstrategy. The results are shown in Table 7. As can be seen\nthis again considerably improves the results, and we are\ngetting very close to 100%. The main cause for this is that\nthe retrieval precision heavily depends on the quality of\nthe transcription. Some parts of a performance are much\nharder to transcribe than others (e.g. heavily polyphonic\nparts with a lot of sustain pedal, which are difﬁcult to tran-\nscribe correctly). Using multiple queries, randomly dis-\ntributed over the whole performance, increases the chances\nthat at least some parts are transcribed in good quality, and\nthat together these queries enable high retrieval accuracy.\nFinally, we had a closer look at the few performances\nthat were still misclassiﬁed and identiﬁed two problems.\nOur approach does not take care of the problem of record-\nings of full concerts. If included in the reference database\nfor multiple pieces, these will lead to misclassiﬁcations.\nFurthermore, for some pieces only a small number of per-\nformances exists, which causes the crawler to return “sim-\nilar” but wrong performances (e.g. performances of other\npieces of the same composer). We sketch a possible solu-\ntion to these problems in Section 8 below.\n8. CONCLUSIONS AND FUTURE WORK\nIn this paper we presented an approach towards piece\nidentiﬁcation for performances of piano music, based on\nan automatically compiled reference database using web\nsources. It is shown that the symbolic ﬁngerprinting\nmethod is robust enough to deal with the noise introduced\nby the transcription algorithms and allows for fast query-\ning in the symbolic domain. Furthermore, increasing the\nredundancy by using multiple performances to represent a\nsingle piece, especially using the proposed selection strat-\negy, largely alleviates the problem of noise introduced byQuerylength\n2 s 5 s 10 s\nRecall at Rank 1 0.92 0.95 0.95\nRecall at Rank 5 0.98 0.99 0.99\nRecall at Rank 10 0.99 1 1\nMean Reciprocal Rank 0.94 0.97 0.97\nMean Query Time 0.49 s 1.71 s 3.83 s\nTable 7 . Results for querying for a whole performance\nvia ten random small queries with ten seconds each. The\nresults are based on 3 700 queries for each query length.\nthe automatic compilation of the reference database. Addi-\ntionally, this increases the robustness of the identiﬁcation\nprocess via the ﬁngerprinting algorithm, as ’problematic’\nsections (e.g. regarding the transcription process) are rep-\nresented multiple times, thus increasing the chances that\nthe parts in question are well covered by the reference\ndatabase.\nThere exist a number of possible improvements regard-\ning the automatic selection of performances for a piece. In\nour implementation the focus is on increasing the homo-\ngeneity within the group of performances for a piece by\ncomparing them to each other. An additional option is to\nanalyse matches on the full reference database and try to\nﬁnd out which performances match well to multiple pieces\nand exclude them (as they cover multiple songs or were\nmistakenly assigned to multiple pieces by the crawler).\nWe are currently in the process of collecting a much\nlarger collection of classical piano music. This dataset will\ncontain a few thousand pieces, covering a large part of the\nclassical piano repertoire6. On this dataset we are going\nto conduct experiments regarding the scalability of our ap-\nproach in terms of runtime and retrieval accuracy.\nIn the future, we will also investigate the usefulness of\nthe presented approach for non-classical piano music. Pre-\nliminary experiments have shown that this is a much harder\ntask, as compared to classical piano music the pieces are\nnot as strictly deﬁned via a detailed score (e.g. popu-\nlar songs and jazz standards are mostly described via lead\nsheets). Thus, performances of the same piece differ more\nheavily than in classical music. Of course we would also\nlike to lift the restriction to piano music and try our method\non other genres, but thus far general music transcription is\nnot robust enough to be used with our approach. Hopefully\nthis will change in the future.\nFinally, regarding real-world applications, an automatic\nmethod to determine which pieces are well covered by\nthe database, and which ones would beneﬁt from man-\nual intervention, would be desirable. This would help to\nquickly build a reference database which already covers\nmost pieces well, and then to manually add additional ref-\nerences (based on performances, or even on symbolic score\ndata) for pieces the identiﬁcation algorithm struggles with.\n6The reference database is of course compiled automatically (based\non the list of pieces), but the preparation of the ground truth for the ex-\nperiments is a time consuming, manual process.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 3599. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Council\n(ERC Grant Agreement 670035, project CON ESPRES-\nSIONE).\n10. REFERENCES\n[1] Andreas Arzt, Sebastian B ¨ock, and Gerhard Widmer.\nFast identiﬁcation of piece and score position via sym-\nbolic ﬁngerprinting. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 433–438, Porto, Portugal, 2012.\n[2] Andreas Arzt, Gerhard Widmer, and Reinhard\nSonnleitner. Tempo- and transposition-invariant iden-\ntiﬁcation of piece and score position. In Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 549–554, Taipeh,\nTaiwan, 2014.\n[3] Shumeet Baluja and Michele Covell. Waveprint: Ef-\nﬁcient wavelet-based audio ﬁngerprinting. Pattern\nRecognition , 41(11):3467–3480, 2008.\n[4] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: a new\nPython Audio and Music Signal Processing Library.\nInProceedings of the 24th ACM International Con-\nference on Multimedia , pages 1174–1178, Amsterdam,\nThe Netherlands, 10 2016.\n[5] Sebastian B ¨ock and Markus Schedl. Polyphonic piano\nnote transcription with recurrent neural networks. In\nProceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\npages 121–124, Kyoto, Japan, 2012.\n[6] Pedro Cano, Eloi Batlle, Ton Kalker, and Jaap Haitsma.\nA review of algorithms for audio ﬁngerprinting. In Pro-\nceedings of the IEEE International Workshop on Multi-\nmedia Signal Processing (MMSP) , pages 169–173, St.\nThomas, Virgin Islands, USA, 2002.\n[7] Michael A. Casey and Malcolm Slaney. Song intersec-\ntion by approximate nearest neighbor search. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 144–149,\nVictoria, Canada, 2006.\n[8] Peter Grosche and Meinard M ¨uller. Toward character-\nistic audio shingles for efﬁcient cross-version music re-\ntrieval. In Proceedings of the IEEE International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , Kyoto, Japan, 2012.\n[9] Peter Grosche and Meinard M ¨uller. Toward musically-\nmotivated audio ﬁngerprints. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP) , pages 93–96, Kyoto,\nJapan, 2012.[10] Peter Grosche, Meinard M ¨uller, and Joan Serr `a. Au-\ndio content-based music retrieval. In Meinard M ¨uller,\nMasataka Goto, and Markus Schedl, editors, Mul-\ntimodal Music Processing , volume 3 of Dagstuhl\nFollow-Ups , pages 157–174. Schloss Dagstuhl–\nLeibniz-Zentrum f ¨ur Informatik, Dagstuhl, Germany,\n2012.\n[11] Frank Kurth and Meinard M ¨uller. Efﬁcient index-based\naudio matching. IEEE Transactions on Audio, Speech,\nand Language Processing , 16(2):382–395, 2008.\n[12] Meinard M ¨uller, Frank Kurth, and Michael Clausen.\nAudio matching via chroma-based statistical features.\nInProceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , pages 288–\n295, London, UK, 2005.\n[13] Mathieu Ramona and Geoffroy Peeters. Audioprint:\nan efﬁcient audio ﬁngerprint system based on a\nnovel cost-less synchronization scheme. In Proceed-\nings of the IEEE International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) , pages\n818–822, Vancouver, Canada, 2013.\n[14] Joan Serr `a, Emilia G ´omez, and Perfecto Herrera. Au-\ndio cover song identiﬁcation and similarity: back-\nground, approaches, evaluation and beyond. In Z. W.\nRas and A. A. Wieczorkowska, editors, Advances in\nMusic Information Retrieval , volume 274 of Studies\nin Computational Intelligence , chapter 14, pages 307–\n332. Springer, Berlin, Germany, 2010.\n[15] Joren Six and Marc Leman. Panako - a scalable acous-\ntic ﬁngerprinting system handling time-scale and pitch\nmodiﬁcation. In Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 259–264, Taipei, Taiwan, 2014.\n[16] Reinhard Sonnleitner and Gerhard Widmer. Robust\nquad-based audio ﬁngerprinting. IEEE/ACM Trans-\nactions on Audio, Speech and Language Processing ,\n24(3):409–421, 2016.\n[17] Avery Wang. An industrial strength audio search al-\ngorithm. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 7–13, Baltimore, Maryland, USA, 2003.360 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Sketching Sonata Form Structure in Selected Classical String Quartets.",
        "author": [
            "Louis Bigo",
            "Mathieu Giraud",
            "Richard Groult",
            "Nicolas Guiomard-Kagan",
            "Florence Levé"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415020",
        "url": "https://doi.org/10.5281/zenodo.1415020",
        "ee": "https://zenodo.org/records/1415020/files/BigoGGGL17.pdf",
        "abstract": "Many classical works from 18th and 19th centuries are sonata forms, exhibiting a piece-level tonal path through an exposition, a development and a recapitulation and in- volving two thematic zones as well as other elements. The computational music analysis of scores with such a large- scale structure is a challenge for the MIR community and should gather different analysis techniques. We propose first steps in that direction, combining analysis features on symbolic scores on patterns, harmony, and other elements into a structure estimated by a Viterbi algorithm on a Hid- den Markov Model. We test this strategy on a set of first movements of Haydn and Mozart string quartets. The pro- posed computational analysis strategy finds some pertinent features and sketches the sonata form structure in some pieces that have a simple sonata form.",
        "zenodo_id": 1415020,
        "dblp_key": "conf/ismir/BigoGGGL17",
        "keywords": [
            "sonata forms",
            "classical works",
            "piece-level tonal path",
            "exposition",
            "development",
            "recapitulation",
            "two thematic zones",
            "Hid-dden Markov Model",
            "Viterbi algorithm",
            "Haydn and Mozart string quartets"
        ],
        "content": "SKETCHING SONATA FORM STRUCTURE\nIN SELECTED CLASSICAL STRING QUARTETS\nLouis Bigo1Mathieu Giraud1Richard Groult2Nicolas Guiomard-Kagan2Florence Lev ´e2;1\n1CRIStAL, UMR 9189, CNRS, Universit ´e de Lille, France\n2MIS, Universit ´e de Picardie Jules Verne, Amiens, France\nflouis,mathieu,richard,nicolas,florence g@algomus.fr\nABSTRACT\nMany classical works from 18th and 19th centuries are\nsonata forms , exhibiting a piece-level tonal path through\nan exposition, a development and a recapitulation and in-\nvolving two thematic zones as well as other elements. The\ncomputational music analysis of scores with such a large-\nscale structure is a challenge for the MIR community and\nshould gather different analysis techniques. We propose\nﬁrst steps in that direction, combining analysis features on\nsymbolic scores on patterns, harmony, and other elements\ninto a structure estimated by a Viterbi algorithm on a Hid-\nden Markov Model. We test this strategy on a set of ﬁrst\nmovements of Haydn and Mozart string quartets. The pro-\nposed computational analysis strategy ﬁnds some pertinent\nfeatures and sketches the sonata form structure in some\npieces that have a simple sonata form.\n1. INTRODUCTION\n1.1 Sonata Forms\nSonata form is a large-scale structure that can be found\nin many works from early Classical (18th century) to late\nRomantic period (end of 19th century). Sonata forms can\nbe found in almost all ﬁrst movements (and, often, in other\nmovements) on Haydn, Mozart and Beethoven works.\nFigure 1 shows an example of a very reduced sonata\nform in a piano sonatina by Kuhlau. Basically, a sonata\nform is built on a piece-level tonal path involving a pri-\nmary thematic zone (P) and a contrasting secondary the-\nmatic zone (S) . It contains the following parts [15]:\n\u000fanexposition , often repeated, containing the the-\nmatic zone P in the main tonality (denoted by I), and\nthe thematic zone S in an auxiliary tonality (usually,\nbut not always, the tonality of the dominant of I, de-\nnoted by V);\n\u000fadevelopment (D) characterized by tonal instabil-\nity, in which the existing themes are transformed and\nc\rLouis Bigo, Mathieu Giraud, Richard Groult, Nicolas\nGuiomard-Kagan, Florence Lev ´e. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: Louis\nBigo, Mathieu Giraud, Richard Groult, Nicolas Guiomard-Kagan, Flo-\nrence Lev ´e. “Sketching Sonata Form Structure in Selected Classical\nString Quartets”, 18th International Society for Music Information Re-\ntrieval Conference, Suzhou, China, 2017.possibly new themes are introduced, ﬁnished by a re-\ntransition (R), that focus back to the main tonality;\n\u000farecapitulation of the themes P and S, both in the\ntonality of the tonic, possibly including elements\nthat were added throughout the development.\nSeveral striking events are found between these sec-\ntions, in particular cadences . The transition (TR) between\nthe P and S zones often ends on a Medial Caesura (MC),\nthat is often a Half Cadence (HC) with additional break\nfeatures [14]. The S zone generally concludes with a Per-\nfect Authentic Cadence (PAC), and is followed by conclud-\ning patterns (C) without thematic content.\nThere are many possible variations on this basic struc-\nture. Somehow, the “regular” sonata form does not exist,\nand is merely a reconstruction. Some forms even do not\nhave two contrasting themes but rather a “continuous ex-\nposition”, such as in several Haydn string quartets or in the\nﬁrst movement of Mozart’s “The Hunt” K 458 [15].\nMore than a rigid framework between sections, what\nconstitutes the essence of a sonata form is a high-level bal-\nance in the whole piece: the tonal tension (the auxiliary\ntonality) and the rhetorical tension (textures, themes) cre-\nated by the exposition and the development are resolved\nduring the recapitulation. The development of sonata form\nwas consubtantial to the emergence of instrumental mu-\nsic, this high-level balance enabling the design of musical\nworks at a larger scale than before.\n1.2 Sonata Forms, Musicology and Pedagogy\nThe term “Sonata form” was ﬁrst coined in mid-1820s,\nin the A. B. Marx’s Berliner allgemeine musikalische\nZeitung , and later formalized in [22] and [6], even if some\nunderlying principles were already known before [19, 29].\nSomes authors conducted in-depth analyses of corpora\nwith sonata forms, such as [12, 27] for Beethoven’s String\nquartets or [32] for Beethoven’s Piano sonatas.\nIn the last decades, authors proposed systematic theo-\nries on those forms [3, 4, 11, 21, 23, 28, 30]. The work\nof Hepokoski and colleagues [13, 14], culminating in the\nbook [15], will be used here as a reference. These works\nformalized the notion of rotations organizing musical tech-\nniques throughout the piece.\nMusic research on sonata forms is thus still active today,\ntwo centuries after the climax of compositions in sonata752Figure 1 . Allegretto of the piano sonatina Op. 55, no. 2 by Kuhlau. HC/PAC/EEC/ESC describe cadences and structure\nendings using the notations from [15]. This movement has very short sections, a tiny development (Dev) that is almost\nonly a retransition, and almost no transition between themes P and S. It nevertheless features the characteristic tonal path :\nS (and C) is in the dominant tonality (V , D major) during the exposition and comes back to the main tonality (I, G major)\nduring the recapitulation. Theme S and conclusion C are exactly transposed between the exposition and the recapitulation.\nThe theme P is both times in the main tonality, but lasts 8 measures in the exposition and 11 in the recapitulation.\nform. Such studies help to understand some principles of\ncompositions and to have a new look on the history of mu-\nsic. Finally, sonata forms are one of the focus of lectures\nin music history, analysis or in composition.\n1.3 Sonata Forms and MIR\nSeveral works in the MIR community target in sonata\nforms, for example to test pattern extraction [25], tonal-\nity estimation [34], classiﬁcation on n-grams, interval and\nmetrical analyses [18]. However, there are very few works\nfocusing on the sonata form structure . There will never\nbe a “deﬁnitive analysis” of some piece in sonata form\n– even between musicians, one may choose to focus on\nsome aspects. Anyway, some analytical viewpoints on\nthe sonata forms make consensus and can be the focus of\nMIR research. On audio signals, Jiang and M ¨uller com-\nputed correlations to detect exposition/recapitulation on\nthe ﬁrst movements of 28 Beethoven piano sonatas with\nself-similarity matrices [17]. They also trace transpositions\nand harmonic changes during the different parts. Weiß and\nM¨uller propose a model of the “tonal complexity” and map\nit on sections of sonata forms [35].\nOn symbolic data, we proposed in [7] a ﬁrst approach\nto detect the exposition/recapitulation based on pattern\nmatching. Barat `e and al. proposed a model of the sonata\nform strucure trough Petri Nets, but without any algo-\nrithm [2].\nWe argue that sonata forms are very stimulating exam-\nples for MIR research, going from simple cases (repeated\npattern with a tonal path in a sonatina, as in Figure 1) to\nvery elaborated constructions (such as Beethoven piano\nsonatas) with many deviations from the norm [36]. The\nkey point of an analysis of sonata forms – a large-scale\ntonal path – combines local-level features (themes, har-\nmony) with a piece-level analysis.\nAn example of the complexity is the detection of Medial\nCaesura (MC) that marks the break between the two the-matic zones. The MC is often marked by a half-cadence,\nbut also by a long preparation, a “triple hammer blow” and\nthen a silence on all voices [14]. However these events are\nnot always found – and such events can also appear outside\nof a MC. To our knowledge, not any study in MIR tried to\ndetect MC in sonata forms.\nMore generally, sketching an analysis of large-scale\nstructures such as sonata forms is challenging for any an-\nalyst. A student in music analysis or a music theorist con-\nsiders different elements and, through diligent analytical\nchoices , summarize them into a coherent analysis. Our\ncomputational strategy to analyze sonata forms in sym-\nbolic scores takes inspiration from this approach. We pro-\npose to detect several analysis “features” using or extend-\ning MIR techniques (Section 2) and then to combine them\nto sketch the large-scale structure (Section 3). We test this\nstrategy on a corpus of ten Haydn and Mozart string quar-\ntets and discuss the results (Sections 4 and 5).\n2. ANALYSIS FEATURES\nThe following paragraphs list analysis features on which\nwe will build to sketch the structure of the sonata form.\nFigures 2b and Figure 3 show these features on a ﬁrst\nmovement of a string quartet by Mozart. Such musical\nfeatures are common in textbook or lecture descriptions of\nthe sonata form. Their selection was done according to\nwhether their presence or absence could be characteristic\nof one (or several) section(s) in a sonata form .\nThe following paragraphs lists these features, noted\nwith boxes such as P. The detection of some of these fea-\ntures is taken from previous works [8]. Note that these\nfeatures are already relatively mid-level or high-level MIR\nfeatures, and their detection is often a challenge by it-\nself that will not be detailed and evaluated here. Al-\nthough not perfect, these methods detect features that can\nbe combined as observable symbols produced by a Hidden\nMarkov Model (Section 3).Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 753P S P' S' P/S\nI V IIV I vi iiI VI iiviI VI Tonality\ng, g, d'' g,, a' g' g, g, g' cz,, Pedals\n# # #\n7Harmony\nr r r r Breaks\nCadences\nL Harm.seq.\nUnissonb) Analysis features++EEC ++MC' ++ESCI V V vi ii I I I\nP TR S C D R P' TR' S' C'\n++MCTonality\nStructurea) Reference analysis\nc) Structure estimation\nStructureS\n++MC' ++MCP TR S D R P' TR' S' C' CFigure 2 . First movement of the String Quartet no. 4 in C major by W. A. Mozart (K157). (a.) Reference analysis indicating\nthe main sections, following notations of [15], with the cadential structure endings, in particular the MC (Medial Caesura)\nbetween the P/TR and S zones. (b.) Analysis features as described in Section 2. (c.) Structure estimation by the HMM\ndescribed in Section 3.2. The section with a dotted frame, around the ﬁrst MC, is detailed on Figure 3.\n2.1 Thematic Features\nIn a “regular” sonata form, the two thematic zones are\nstrong markers of the form. We detect here repeated\nthemes by computing a similarity score [8], which is based\non equations similar to the Mongeau-Sankoff algorithm\nthat uses dynamic programming [24]. The score function\nfavors diatonic similarities, and only allows here alignment\nof two notes having the same duration.\n\u000fP theme. PThe P theme is searched using the score\nfunction, forbidding any transposition, and by com-\nparing the start of the piece with other parts. The\npattern is extracted only from the highest voice (ﬁrst\nviolin), but successive occurrences may be found in\nother voices. The ﬁrst searched pattern begins at\nthe start of the piece and ends at most at 1=3of the\nlength of the piece. If no repeated pattern is found,\nthe search is done again, starting from 2 measures\nafter. The P theme must start in the ﬁrst 10 measures\nand its length has to be more than 1 measure.\n\u000fS theme. SThe S theme is searched after the ﬁrst P\ntheme, again for at least one more occurrence. The\nS theme must start before 1=3of the length of the\npiece, end before 1=2of the length of the piece, and\nits length has to be between at least 4 measures. This\ntime, the cost forces to ﬁnd some pattern with a dom-\ninant transposition between the ﬁrst occurrence and\nthe following one. Once again, if no repeated pat-\ntern is found, the search is done by starting from a\nfurther position.\nThese features were introduced in [7] and may be re-\nlated to the approach taken by [17] on audio signals. The\nselected ratios ( 1=3,1=2) reﬂect a generic balance of the\nstructure of the sonata form. The score function could beimproved by further research, in particular to allow more\nvariations between the statements of the themes.\n2.2 Harmonic Features\nAs tonal path is the most striking element of a sonata form,\nsome features speciﬁcally focus on the harmony. Indeed,\neven without detection of full P/S themes, the harmony\nalone should give hints on analyzing sonata forms.\n\u000fTonality. IA OWe detect local tonalities on 2-\nmeasures windows with a Krumhansl-Schmukler al-\ngorithm [20] used with the pitch proﬁles improved\nby Temperley [31]. Tonalities are then output rela-\ntively against the main (most present) tonality of the\npiece: main I, auxiliary Aand other Otonalities.\nAs our goal is not to infer precisely the tonality but\nto give a hint of the tonal context that will be used\nnext in a probabilistic model, we do not use any al-\ngorithm improving this detection such as the full al-\ngorithm of [31].\n\u000fAuthentic cadences. ACCadences are markers be-\ntween sections. Moreover, cadences appear more\nlikely in conclusive sections (C). We detect candi-\ndates of simple Perfect Authentic Cadences (PAC)\nand rooted Imperfect Authentic Cadences (rIAC) by\nchecking harmonies over any V-I bass movement on\nstrong beats using the algorithm of [8]. To take for-\neign notes into account, the V chord, characterized\nby the leading tone and possibly the seventh, has to\nbe found somewhere while the bass holds the dom-\ninant. As this detection is here solely based on the\nharmony, it may induce some false positives. This is\nthe case on Figure 3, where two successive V-I bass\nmovements are interpreted as PACs even if they do\nnot correspond to any phrase ending.754 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017r#S\nAOAtransition (TR)medialcaesura(MC)secondarytheme(S)\nTR@\nSACAChaFigure 3 . First medial caesura (MC) in the ﬁrst movement\nof the String Quartet no. 4 in C major by W. A. Mozart\n(K157), measures 29 to 32. See Figure 2 for an overview\nof this movement. The MC ends the transition (TR) and is\nbefore the beginning of the secondary theme (S). The com-\nputed analysis retrieves several features within this region:\nthe thematic pattern S S– falsely detected even before\nthe MC, see discussion in the text – tonality regions corre-\nsponding to auxiliary Aand – falsely detected – other O\ntonalities, a chromatic upward bass movement #and a full\nrest r. Two spurious cadences ACare also detected at the\nbeginning of the secondary theme. Although not taken into\naccount in the present work, the extract includes a triple\nhammer blow hacharacteristic of the medial caesura.\n\u000fPreparation of half-cadences. We detect both chro-\nmatic upward bass movements #(one chromatic\nsemitone followed by one diatonic semitone, con-\ntiguous notes with the same pitch being taken as one\nnote, see Figure 3) and putative diminished seven\nchords 7(any diminished seventh or augmented\nsecond interval between two notes sounding at the\nsame time). These feature are often found in the\npreparation of half-cadences, and especially for the\npreparation of the Medial Caesura.\n\u000fPedals. pedWe detect pedals during more than 1\nmeasure in one voice. Pedals are often found during\ndevelopment and conclusion sections. On the con-\ntrary, they are often not found in thematic P/S zones,\nexcept at the very beginning of the piece.\n2.3 Other Features\nThese features combine melody and harmony and/or other\nmusic elements.\n\u000fFull rests. rWe look for rests that occur in all\nvoices simultaneously. Such rests are often found\nat key places: after the MC, after the exposition, and\njust before the recapitulation.\n\u000fUnisons. uniWe detect unisons between all\nthe voices using the algorithm presented in [10].\nUnisons are strong markers that often also break the\nmusical ﬂow: They are also likely to be found in\nstructural breaks.\u000fLong harmonic sequences. LWe detect harmonic\nsequences by at least three successive occurrences of\nmelodic patterns in all four voices, using the algo-\nrithm presented in [9] and reporting sequences dur-\ning at least 5 measures. Such long harmonic se-\nquences, often modulating, can be found in the de-\nvelopment.\n3. STRUCTURE ESTIMATION THROUGH\nFEATURE COMBINING\nAnalysis features are sampled at regular intervals to give\nsequences of symbols (Figure 2b). We propose to gather\nthese features into a sonata form structure (Figure 2c). The\nfollowing paragraphs present the Hidden Markov Models\n(HMM) framework we use and then the particular HMM\ndesigned for sonata forms.\n3.1 Hidden Markov Models with Multiple Outputs\nMarkov model. We consider a ﬁnite alphabet of sym-\nbolsA=f\u000b1;\u000b2:::gthat will be here the analysis fea-\ntures. The Markov model M= (Q;\u0019;\u001c;T;E )onAis\ndeﬁned by a set of nstatesQ=fq1;:::qngcorresponding\nhere to sections of the sonata form, the initial state prob-\nabilities\u0019= (\u00191;:::\u0019n), and the ﬁnal state probabilities\n\u001c= (\u001c1;:::\u001cn).T(i!j)is the transition probability –\nstateqigoes to state qj– andE(i \u000b)is the emission\nprobability – stateqiemits feature \u000b. All probabilities are\nbetween 0 and 1, and the probabilities arrays sum to 1.\nGiven an integer t, we call at-tupleP= (p1;:::pt)2\n[1;n]tapath inM. This path goes through the tstates\nqp1:::qpt. We also consider a sequence of symbols w=\n\u000b1:::\u000bt\u000012At\u00001. The probability that the model Mfol-\nlows a path Pwhile outputting the sequence w, one state\noutputting one character at each step, is given by:\np(P;w) =\u0019p1\u0001\u0005t\u00001\ni=1(E(pi \u000bi)\u0001T(pi!pi+1))\u0001\u001cpt\nOutputting mutiple symbols. Several features can be\npredicted at the same step. We thus now consider that a\nstate may output simultaneously a set of symbols A=\nf\u000b1:::\u000bag\u001aA . If these emissions are independent events,\nthe probability that the state qioutputs the set Ais\nE(i A) = \u0005\u000b2AE(i \u000b)\u0001\u0005\u000b2AnA(1\u0000E(i \u000b))\nWe now consider a path Pas before and a sequence\nof sets of symbols W=A1:::At\u00001. The probability\np(P;W )that the modelMfollows a path Pwhile out-\nputting the sequence Wis given by the same equation, re-\nplacingE(pi \u000bi)byE(pi Ai).\nHMM. Now we considerMas a Hidden Markov Model\n(HMM). The path Pis unknown but we observe a se-\nquence of sets of symbols W.\nFinding the most probable path Pthat maximizes\np(P;W )is done by the classical Viterbi algorithm [26,33]\nthat ﬁrst uses a forward stage to compute the probability\nof being in a state while outputting A1:::Aj, and then that\nﬁnds back the optimal path in a backward pass.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 755P TR MC S C d D R p P’ TR’ MC’ S’ C’Exposition Development Recapitulation\ntonalities\nthemes\notherI\nPIA\nPS\npedIA\nS\n#7\nAC runiA\nSA\nS\nped\nACA\n7\nruniIAO\nLA\npedI\n#7\nAC runi\nFigure 4 . Hidden Markov Model sketching a regular sonata form structure from analysis features. The initial state is P,\nthe ﬁnal states are S’ and C’. The square states (MC/MC’ “medial caesura”, d “transition to development”, p “retransition\nto primary theme”) are transient states intended to last one or a few quarters, and are characterized by break features ( #,\n7,AC,r,uni). Each state has a (not shown) loop transistion over itself. The horizontal straight transitions have the\nsecond highest probabilities, and the curved dashed transitions enable to skip some states with a low probability. Only\nthe main emissions are shown here: the states may also emit other symbols with a low probability. For clarity, auxiliary\ntransitions and emissions are not shown in the recapitulation. They are the same than in the exposition, except that the\ntonality emissions focus on the main tonality I.\n3.2 A HMM to Sketch Sonata Form Structure\nFigure 4 depicts the HMM created to sketch the\nsonata form structure. The HMM uses the alphabet\nfP;S;I;A;O;#;7;ped;r;uni;L;ACg containing\nthe analysis features presented in the previous section. The\nfeatures are sampled at every quarter note. The 14 states\nQ=fP;TR;MC;S;C;d;D;R;p;P0;TR0;MC0;S0;C0g\nwere selected to match the various sections of the “regular”\nsonata form as well as some transitions fMC;d;p;MC0g\nbetween these sections.\nAs discussed in the introduction, even if such a “reg-\nular” sonata form is a ﬁction, some pieces do follow\nthis structure: The proposed states intend to match these\npieces. As the model is very simple – and as the detec-\ntion of the features is far from perfect – the goal is not to\nperfectly match these 14 stages to actual sections of sonata\nforms, but rather to sketch the structure. We deﬁned so\nmany states to try to follow actual structures – for example,\nTR and MC states deﬁnitely imply different music events:\nin TR, focalization on the auxiliary tonality A, possibly\nwith some ped, and, in MC, conclusion with a cadence,\npossibly AC, possibly with additional events: #,7,r.\nTransitions and emission probabilities of the selected\nsymbols were choosen manually by a trial-and-error pro-\ncess (see discussion in Section 5). Each state has a loop\ntransistion over itself with a very high probability (0.8).\nThe emission probabilities were drafted according to what\nwas described in the previous section. Some adjustments\nwere made to take into account limits of some feature de-\ntection. For example, the feature Sis often detected out-\nside of S, as on Figure 3. Indeed, sometimes some few\nmeasures before the MC in the recapitulation are exactly a\ntransposition of the same passage in the exposition. Thus,\nin the HMM, Scan also be emitted by the states TR and\nMC. The model with all probabilities can be downloaded\natalgomus.fr/sonata .4. CORPUS AND RESULTS\n4.1 Corpus and Reference Analysis\nExperiments were done in python3, within the music21\nframework [5] extended with analytic labels [1]. Pieces\nwere given as .krn Humdrum ﬁles [16] downloaded from\nkern.humdrum.org . The corpusC10contains 10 ﬁrst\nmovements of classical string quartets composed by Haydn\nand Mozart (see Table 1). All these are in major mode. The\nselected Mozart quartets are mostly early works (K80 and\nthe three Milanese quartets K155, K156, K157) that have\na simple sonata form, even if the ﬁrst movement of K80\nis adagio. Although they have the typical tonal path, the\ntwo Haydn quartets 54-3 and 64-4 do not exhibit a clear S\n(or S’) theme. We denote by C8the set of the 8 remaining\npieces. These pieces were analyzed following principles\nof [15] to determine P/TR/S/C sections as well as MCs. At\nleast two curators checked every reference analysis. These\nanalyses are available under an open-source license from\nalgomus.fr/datasets .\n4.2 Results of the Proposed Strategy\nTable 1 shows the structure estimation of the HMM on all\npieces inC10. As written above, the 14-state HMM was\nnot intended to ﬁt perfectly with the structure, but rather\nto give hints on the sonata form structure. Moreover, some\nsections are difﬁcult to predict, or even to deﬁne: for ex-\nample, the start of the transition (TR) is often “blurred”\nin the end of P. Note also that the features and model we\nproposed do not separate well the S themes from the con-\nclusions C. We thus propose here to focus the evaluation\non four key events of the sonata form (start of S, D, P’, S’):\n\u000fMC+S. In the exposition, the MC followed by the\nstart of S is perfectly or approximately found in 4\npieces inC8. In non-regular structures ( ?), a S may\nbe falsely detected, because the feature Smay re-\nport transposed sections of the theme P.756 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Reference (top) and computed (bottom) analyses MC+S D P’ MC’+S’\nHaydn\nop. 33 no. 2P S C P’ S’ C’\nP MC S C D R P’ MC’ S’ C’\u0000 + + \u0000\nHaydn\nop. 33 no. 3P S C P’ S’ C’\nP TR MCSCd D R P’ TR’ MC’ S’ C’\u0000 = + +\nHaydn\nop. 33 no. 5P S C P’ S’C’ O\nP TR MCS C D R pP’TR’ S’ C’+ + + \u0000\nHaydn ?\nop. 54 no. 3P S C P’ C’ O\nP TR C d D RP’ C’\u0001 \u0000 = \u0001\nHaydn ?\nop. 64 no. 4P TR S C P’ P’ C’\nP TR C D RpP’ TR’ C’\u0001 = ? \u0001\nMozart\nK80 no. 1P S C P’ S’ C’\nP TR S C d D P’ MC’ S’ C’\u0000 = + =\nMozart\nK155 no. 2P TR S C P’ TR’ S’ C’ O\nP TR SCD R pP’MC’ S’ C’\u0000 \u0000 + \u0000\nMozart\nK156 no. 3P S C P’ S’ C’\nP S d D Rp P’ MC’ S’ C’= \u0000 + +\nMozart\nK157 no. 4P TR S C D R P’ TR’ S’ C’\nP TRMC S CD R p P’ TR’MC’ S’ C’+ +\u0000 +\nMozart\nK387 no. 14P TR S C P’ TR’ S’ C’\nP TR MC S C d D R P’ TR’ MC’ S’ C’+ \u0000 + +\nTable 1 . Structure detection on ten ﬁrst movements of Haydn and Mozart string quartets. The top lines are the reference\nanalyses and the bottom line the structure found by the HMM. The four columns MC+S, D, P’ and MC’+S’ evaluate the\nprediction of the start of these events or sections: +(perfect or almost, that is at most 1 measure shifted from the reference),\n=(approximate match, between 2 and 3 measures), \u0000(not found, or too far from the reference, at least 4 measures). We\ndo not evaluate S positions ( \u0001) for pieces marked with ?, as they do not follow a “regular” bithematic sonata form structure\nwith a clear secondary theme.\n\u000fD. The start of the development is perfectly or ap-\nproximately found in 6 pieces in C10. This detection\nis usually grounded by the feature r.\n\u000fP’. The start of the recapitulation is perfectly or ap-\nproximately found in 8 pieces in C10, mainly driven\nby the feature P. Haydn op. 64 no. 4 has partial\nrepeats of the P theme during the recapitulation, and\nMozart K157 has a long retransition that is falsely\ndetected as a P theme due to the feature I.\n\u000fMC’+S’. In the recapitulation, the start of S’ is ap-\nproximately found in 5 pieces in C8. It is again often\ngrounded on the break features.\nSonata structure sketch. Back on the motivation of this\nstudy, the predicted sonata form structure seems quite good\nfor Mozart K157 and K387: starts and durations of sec-\ntions are quite precisely detected. For Mozart K80, K156\nand Haydn 33-3 and 33-5, the structure is coarsely de-\ntected, but bad lengths or shifts in some predicted sections\nare not satisfying. Note that, on K80, even if the thematic\nfeatures are not detected (data not shown), the path esti-\nmated by the HMM is still sensible, mainly due to tonali-\nties as well as break events.\nThe bad results on the other pieces mostly come from\na wrong detection of the start of S/S’. This suggest that\nfeatures helping the prediction of the MC as well as the\nHMM should be improved.5. DISCUSSION\nThe music analysis of large-scale structures, such as the\nsonata forms, requires to gather different analytical ele-\nments into some coherent analysis. Taking inspiration\nfrom what the analysts do, we proposed a strategy to sketch\nsuch sonata structures, designing a HMM modeling music\nknowledge over analysis features. The proposed strategy\nmanages to sketch the structure of some “regular” sonata\nforms in string quartets, ﬁnding the most important sec-\ntions (P/S, D, P’/S’) and sometimes detecting the location\nof the Medial Caesura (MC).\nThis strategy should now be evaluated on a larger cor-\npus. More general perspectives include both the improve-\nment of individual feature detection – conceiving or using\nMIR techniques that may be used to analyze any tonal mu-\nsic, in classical music but also in jazz or pop repertoires –\nand also improvement of the HMM. Other HMM topolo-\ngies could analyze more elaborated variations of sonata\nforms – especially continuous exposition. Analyzing late\nMozart quartets or some romantic quartets will also be very\nchallenging.\nIn the present work, we manually designed transition\nand emission probabilities. These probabilities could also\nbe learned on larger corpora, but the number of parame-\nters to learn makes such a learning difﬁcult. A solution\nto beneﬁt both from human expertise and machine learn-\ning could be also to learn the weights of only manually\nselected emissions and transitions.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 757Acknowledgements. We thank Elise Janvresse, Marc\nRigaudi `ere, and the anonymous reviewers for their insight-\nful comments. This work was supported by a grant from\nthe French Research Agency (ANR-11-EQPX-0023 IR-\nDIVE).\n6. REFERENCES\n[1] Guillaume Bagan, Mathieu Giraud, Richard Groult,\nand Emmanuel Leguy. Mod ´elisation et visualisation de\nsch´emas d’analyse musicale avec music21. In Journ ´ees\nd’Informatique Musicale (JIM 2015) , 2015.\n[2] Adriano Barat `e, Goffredo Haus, and Luca A Ludovico.\nMusic analysis and modeling through Petri nets. In In-\nternational Symposium on Computer Music Modeling\nand Retrieval (CMMR 2005) , pages 201–218, 2005.\n[3] William E. Caplin. Classical Form: A Theory of For-\nmal Functions for the Instrumental Music of Haydn,\nMozart, and Beethoven . Oxford University Press,\n2000.\n[4] William E. Caplin. The classical sonata exposition:\nCadential goals and form-functional plans. Tijdschrift\nvoor Muziektheorie , 6(3):195–209, 2001.\n[5] Michael Scott Cuthbert and Christopher Ariza. mu-\nsic21: A toolkit for computer-aided musicology and\nsymbolic music data. In International Society for Mu-\nsic Information Retrieval Conference (ISMIR 2010) ,\npages 637–642, 2010.\n[6] Carl Czerny. School of Practical Composition . R.\nCocks, London, 1848.\n[7] Laurent David, Mathieu Giraud, Richard Groult,\nCorentin Louboutin, and Florence Lev ´e. Vers une\nanalyse automatique des formes sonates. In Journ ´ees\nd’Informatique Musicale (JIM 2014) , 2014.\n[8] Mathieu Giraud, Richard Groult, Emmanuel Leguy,\nand Florence Lev ´e. Computational fugue analysis.\nComputer Music Journal , 39(2), 2015.\n[9] Mathieu Giraud, Richard Groult, and Florence Lev ´e.\nDetecting episodes with harmonic sequences for fugue\nanalysis. In International Society for Music Informa-\ntion Retrieval Conference (ISMIR 2012) , 2012.\n[10] Mathieu Giraud, Florence Lev ´e, Florent Mercier, Marc\nRigaudi `ere, and Donatien Thorez. Modeling texture in\nsymbolic data. In International Society for Music In-\nformation Retrieval Conference (ISMIR 2014) , 2014.\n[11] Robert O. Gjerdingen. Music in the Galant Style . Ox-\nford University Press, 2007.\n[12] Theodor Helm. Beethovens Streichquartette: Versuch\neiner technischen Analyse dieser Werke im Zusammen-\nhange mit ihren geistigen Gehalt . Leipzig, 1885.\n[13] James Hepokoski. Beyond the sonata principle. J. of\nthe American Musicological Society , 55(2):91, 2002.[14] James Hepokoski and Warren Darcy. The medial\ncaesura and its role in the eighteenth-century sonata\nexposition. Music Theory Spectrum , 19(2):115–154,\n1997.\n[15] James Hepokoski and Warren Darcy. Elements of\nSonata Theory: Norms, Types, and Deformations in\nthe Late-Eighteenth-Century Sonata . Oxford Univer-\nsity Press, 2006.\n[16] David Huron. Music information processing using the\nHumdrum toolkit: Concepts, examples, and lessons.\nComputer Music Journal , 26(2):11–26, 2002.\n[17] Nanzhu Jiang and Meinard M ¨uller. Automated meth-\nods for analyzing music recordings in sonata form. In\nInternational Society for Music Information Retrieval\nConference (ISMIR 2013) , pages 595–600, 2013.\n[18] Stephanie Klauk and Frank Zalkow. Das italienis-\nche Streichquartett im 18. Jahrhundert. M ¨oglichkeiten\nder semiautomatisierten Stilanalyse. urn:nbn:de:101:1-\n201609071562, 2016.\n[19] Heinrich Christoph Koch. Versuch einer Einleitung zur\nComposition (volume 3) . Leipzig, 1793.\n[20] Carol L. Krumhansl and Edward J. Kessler. Tracing the\ndynamic changes in perceived tonal organisation in a\nspatial representation of musical keys. Psychological\nReview , 89(2):334–368, 1982.\n[21] Steve Larson. Recapitulation recomposition in the\nsonata-form ﬁrst movements of Haydn’s string quar-\ntets: Style change and compositional technique. Music\nAnalysis , 22(1-2):139–177, 2003.\n[22] Adolph Bernhard Marx. Die Lehre von der musikalis-\nchen Komposition (volumes 2 et 3) . Breitkopf & H ¨artel,\nLeipzig, 1838, 1845.\n[23] Jan Miyake. The Role of Multiple New-key Themes in\nSelected Sonata-form Exposition . PhD thesis, Univ. of\nNew York, 2004.\n[24] Marcel Mongeau and David Sankoff. Comparison of\nmusical sequences. Computers and the Humanities ,\n24(3):161–175, 1990.\n[25] Oriol Nieto and Morwaread Mary Farbood. Perceptual\nevaluation of automatically extracted musical motives.\nInInternational Conference on Music Perception and\nCognition (ICMPC 2012) , pages 723–727, 2012.\n[26] Lawrence R Rabiner. A tutorial on hidden Markov\nmodels and selected applications in speech recognition.\nProceedings of the IEEE , 77(2):257–286, 1989.\n[27] Phillip Radcliffe. Beethoven’s String Quartets . E. P.\nDutton, 1965.\n[28] Leonard Ratner. Classical Music: Expression, Form,\nand Style . Schirmer, 1980.758 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[29] Anton Reicha. Trait ´e de haute composition musicale\n(volume 2) . Paris, 1826.\n[30] Charles Rosen. Sonata Forms . W. W. Norton, 1980.\n[31] David Temperley. What’s key for key ? the Krumhansl-\nSchmuckler key-ﬁnding algorithm reconsidered. Music\nPerception , 17(1):65–100, 1999.\n[32] Donald Francis Tovey. A Companion to Beethoven’s\nPianoforte Sonatas: Complete Analyses . AMS Press\n(reedited in 1998), 1931.\n[33] Andrew Viterbi. Error bounds for convolutional codes\nand an asymptotically optimum decoding algorithm.\nIEEE Transactions on Information Theory , 13(2):260–\n269, 1967.\n[34] Christof Weiß and Julian Habryka. Chroma-based\nscale matching for audio tonality analysis. In Con-\nference on Interdisciplinary Musicology (CIM 2014) ,\n2014.\n[35] Christof Weiß and Meinard M ¨uller. Quantifying and vi-\nsualizing tonal complexity. In Conference on Interdis-\nciplinary Musicology (CIM 2014) , 2014.\n[36] Frans Wiering and Emmanouil Benetos. Digital musi-\ncology and MIR: Papers, projects and challenges. In\nInternational Society for Music Information Retrieval\nConference (ISMIR 2013) , 2013. Late-breaking ses-\nsion.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 759"
    },
    {
        "title": "Automatic Playlist Sequencing and Transitions.",
        "author": [
            "Rachel M. Bittner",
            "Minwei Gu",
            "Gandalf Hernandez",
            "Eric J. Humphrey",
            "Tristan Jehan",
            "Hunter McCurry",
            "Nicola Montecchio"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417028",
        "url": "https://doi.org/10.5281/zenodo.1417028",
        "ee": "https://zenodo.org/records/1417028/files/BittnerGHHJMM17.pdf",
        "abstract": "Professional music curators and DJs artfully arrange and mix recordings together to create engaging, seamless, and cohesive listening experiences, a craft enjoyed by audi- ences around the world. The average listener, however, lacks both the time and the skill necessary to create compa- rable experiences, despite access to same source material. As a result, user-generated listening sessions often lack the sophistication popularized by modern artists, e.g. tracks are played in their entirety with little or no thought given to their ordering. To these ends, this paper presents methods for automatically sequencing existing playlists and adding DJ-style crossfade transitions between tracks: the former is modeled as a graph traversal problem, and the latter as an optimization problem. Our approach is motivated by an analysis of listener data on a large music catalog, and subjectively evaluated by professional curators.",
        "zenodo_id": 1417028,
        "dblp_key": "conf/ismir/BittnerGHHJMM17",
        "keywords": [
            "music curators",
            "DJ mixing",
            "engaging listening",
            "audience worldwide",
            "compared experiences",
            "user-generated sessions",
            "sophistication",
            "modern artists",
            "graph traversal problem",
            "optimization problem"
        ],
        "content": "AUTOMATIC PLAYLIST SEQUENCING AND TRANSITIONS\nRachel M. Bittner, Minwei Gu, Gandalf Hernandez, Eric J. Humphrey,\nTristan Jehan, P. Hunter McCurry, Nicola Montecchio\nSpotify Inc., USA\nABSTRACT\nProfessional music curators and DJs artfully arrange and\nmix recordings together to create engaging, seamless, and\ncohesive listening experiences, a craft enjoyed by audi-\nences around the world. The average listener, however,\nlacks both the time and the skill necessary to create compa-\nrable experiences, despite access to same source material.\nAs a result, user-generated listening sessions often lack the\nsophistication popularized by modern artists, e.g. tracks\nare played in their entirety with little or no thought given to\ntheir ordering. To these ends, this paper presents methods\nfor automatically sequencing existing playlists and adding\nDJ-style crossfade transitions between tracks: the former\nis modeled as a graph traversal problem, and the latter as\nan optimization problem. Our approach is motivated by\nan analysis of listener data on a large music catalog, and\nsubjectively evaluated by professional curators.\n1. INTRODUCTION\nDJs are modern artists that carefully select, sort, and com-\nbine recordings in order to enhance the music listening ex-\nperience over simpler forms, such as albums or playlists.\nThey traditionally create mixes orsetsthat ﬂow seamlessly\nfrom one song to the next by sequencing styles, matching\nkeys and tempos, and smoothly transitioning between mu-\nsical ideas. Importantly, the ordering of tracks or samples\nand the quality of the transitions between them are funda-\nmentally linked: it can be very difﬁcult to create an enjoy-\nable transition between songs that signiﬁcantly differs in\nstyle, tempo, or key. Transitioning between a slow, smooth\njazz piece and a high energy, fast electronic track, for ex-\nample, will likely feel awkward or unnatural and create an\nabrupt change in the listening experience.\nThough listening to DJ mixes is not a new phenomenon,\nmodern music streaming services indicate that there is sig-\nniﬁcant appetite among users for curating their own sets,\nhaving produced over 2 billion playlists in the last decade\non Spotify alone.1To develop a vague sense of how many\n1https://press.spotify.com/us/about/\nc\rRachel M. Bittner, Minwei Gu, Gandalf Hernandez, Eric\nJ. Humphrey,. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Rachel M. Bittner, Min-\nwei Gu, Gandalf Hernandez, Eric J. Humphrey,. “Automatic Playlist Se-\nquencing and Transitions”, 17th International Society for Music Informa-\ntion Retrieval Conference, 2016.users might be aspiring “DJs” in the home or car, we ﬁnd\nthat roughly 1% of the public playlists available through\nSpotify’s Web API contain “party” in the title.2Even\nthrough coarse extrapolation, this suggests that some 20M\nplaylists are candidates for DJ-style production.\nTherefore, given that so many users are actively exer-\ncising their curatorial skills, the steady advance of machine\nlistening technology offers promise that the more technical\nchallenges of creating a DJ mix could be achieved compu-\ntationally. In this paper we focus speciﬁcally on the two\nhurdles faced in creating a DJ set from a given playlist:\ncompute an optimal sequencing, and create song-to-song\ntransitions between sequenced tracks. One of the chal-\nlenges of building a model for these two tasks is deﬁning\nhow to evaluate performance. Because quality of a song\nsequence and of a song-to-song transition is highly subjec-\ntive, we rely on user studies to evaluate the performance of\nour systems.\n2. RELATED WORK\nSeveral commercial products (e.g., Serato DJ3and Na-\ntive Instruments’ Traktor line4) are designed to assist DJs\nwith digital mixing on a laptop. These are mainly tools\nfor enthusiasts and professionals who already have experi-\nence in mixing, and as such these tools tend to replicate\nwith software their original analog counterparts. Auto-\nmatic audio analysis techniques are sometimes exploited\nto let the user sort playlists by tempo and key, however\nby design it is up to the DJ to make a ﬁnal selection and\ndecide on where to transition: the software’s role is to as-\nsist with time-stretching and facilitating the execution of\nbeat-aligned transitions. This paper is concerned with the\nautomation of the entire experience, demanding less in-\nvolvement by the users; examples of commercial software\nin this category include Algoriddim DJay5, Pacemaker6,\nand Serato Pyro7.\nSequential ordering is the primary concern of [6], that\nuses an audio similarity metric built on Gaussian models\nof MFCCs. However, the approach does not constrain the\nproblem to a pre-selected set of songs and instead gener-\nates playlists from a large pool. In analyzing the order-\n2https://developer.spotify.com/web-api/\nplaylist-endpoints/\n3https://serato.com/dj\n4https://www.native-instruments.com/en/\nproducts/traktor\n5https://www.algoriddim.com/\n6https://pacemaker.net/\n7https://seratopyro.com/442ing of songs in professionally-made DJ sets, [11] presents\nevidence that timbral factors play an important role in se-\nquencing. In [3], consideration is given to “tempo trajec-\ntories” over time as a way of modeling human DJs’ ability\nto structure the rise and fall of energy levels in the music\nas the sequence of songs progresses. Ishizaki, et al. [9] fo-\ncus on making smooth tempo adjustments to songs with\nthe goal of minimizing abrupt changes that could cause\nlistener discomfort. In choosing optimal mixing regions\nbetween two songs, [8] employs section similarity met-\nrics derived from chroma information, along with beat and\ntempo features. Similarly, [7] proposes a model for mea-\nsuring the perceptual consonance for different transition\nregions given two tracks. In [15], a more complete DJ\nsimulation method is proposed, which performs song se-\nlection, ordering and cross-fading for electronic music. A\nclosely related problem is the automatic creation of mu-\nsical “mash-ups”, for which a number of algorithms have\nrecently been proposed [5, 12].\n3. SEQUENCING\nGiven a playlist, the goal of a sequencing algorithm is to\norder the tracks it contains in a way as to make the music\n“ﬂow smoothly” from each song to the next. Cunningham\net al. performed an in-depth study of how individual users\nsequence tracks, and concluded that the task is “more of an\nart than a science” [4]. Thus, the notion of ﬂow and its at-\ntainment is ultimately an aesthetic phenomenon; a DJ may\nwant the tempo to stay relatively constant or neighboring\nsongs to be acoustically similar as a function of creative\nintent, as illustrated in Figure 1. If songs are to be cross-\nfaded, proper sequencing can ensure that consecutive pairs\nof songs have similar keys and tempos, allowing for less\nabrupt transitions. Understandably, the scope of this work\nentails a more calculated approach than that of an expert\nDJ, and we identify artist-quality sequencing as a broader\naim of this research area. It is important to note that this\nproblem is related to, but different from the task of gener-\nating playlists, for example as in [2] – in this task we are\ngiven a list of tracks and the task is to reorder them, rather\nthan to ﬁnd a list of coherent tracks from a large corpus.\nExamples of playlists sequenced using the proposed ap-\nproach can be found online.8 9\n3.1 Method\nThe problem of sequencing tracks lends itself well to be\nformulated in a graph theory setting. The central step con-\nsists in mapping acoustic features into a Euclidean space so\nthat songs that are ﬁt to be sequenced next to each other are\nalso close together in the feature space. Finding a good se-\nquencing involves ﬁnding the shortest non-repeating path\nbetween all the songs.\n8https://open.spotify.com/user/rabitt3/\nplaylist/6a4lxKlqWZwKQgV3VhRMjX\n9https://open.spotify.com/user/rabitt3/\nplaylist/0Cl1BNwnWxmLkfUn8YQZVS\nOriginal Playlist \nTitle Artist Tempo \nAll Star Smash Mouth 104\n...Baby One \nMore Time Britney Spears 92\nBills, Bills, Bills Destiny’s Child 127\nEvery Morning Sugar Ray 109\nGenie In A \nBottle Christina \nAguilera 175\nI Want It That \nWayBackstreet \nBoys 99\nLivin’ la Vida \nLoca Ricky Martin 178\nMiami Will Smith 108\nNo Scrubs TLC 92\nSmooth Santana, Rob \nThomas 115Sequenced by Timbre \nTitle Artist Tempo \n...Baby One \nMore Time Britney Spears 92\nI Want It That \nWayBackstreet \nBoys 99\nGenie In A \nBottle Christina \nAguilera 175\nNo Scrubs TLC 92\nBills, Bills, Bills Destiny’s Child 127\nMiami Will Smith 108\nAll Star Smash Mouth 104\nEvery Morning Sugar Ray 109\nSmooth Santana, Rob \nThomas 115\nLivin’ la Vida \nLoca Ricky Martin 178Sequenced by Tempo & Timbre \nTitle Artist Tempo \n...Baby One \nMore Time Britney Spears 92\nI Want It That \nWayBackstreet \nBoys 99\nAll Star Smash Mouth 104\nEvery Morning Sugar Ray 109\nSmooth Santana, Rob \nThomas 115\nLivin’ la Vida \nLoca Ricky Martin 178\nGenie In A \nBottle Christina \nAguilera 175\nBills, Bills, Bills Destiny’s Child 127\nNo Scrubs TLC 92\nMiami Will Smith 108Sequenced by Tempo \nTitle Artist Tempo \n...Baby One \nMore Time Britney Spears 92\nNo Scrubs TLC 92\nI Want It That \nWayBackstreet \nBoys 99\nAll Star Smash Mouth 104\nMiami Will Smith 108\nEvery Morning Sugar Ray 109\nSmooth Santana, Rob \nThomas 115\nBills, Bills, Bills Destiny’s Child 127\nGenie In A \nBottle Christina \nAguilera 175\nLivin’ la Vida \nLoca Ricky Martin 178\nTitle Artist Tempo Key\nAll Star Smash Mouth 104 F# major \n...Baby One \nMore Time Britney Spears 92 C minor \nBills, Bills, Bills Destiny’s Child 127 B minor \nEvery Morning Sugar Ray 109 Ab major \nGenie In A \nBottle Christina \nAguilera 175 F minor \nI Want It That \nWayBackstreet \nBoys 99 F# minor \nLivin’ la Vida \nLoca Ricky Martin 178 C# minor \nMiami Will Smith 108 Bb minor \nNo Scrubs TLC 92 Ab minor \nSmooth Santana, Rob \nThomas 115 A minor Figure 1 : Example playlist sequencing by tempo and tim-\nbre.\n3.1.1 Constructing the Feature Space\nSeveral acoustic aspects of a song are exposed so that they\ncan be combined differently:\n\u000facoustic vectors are created by ﬁrst using a con-\nvolutional neural network [16] trained to repro-\nduce collaborative-ﬁltering vectors in ( R2048). The\nacoustic vectors are low dimensional embeddings\n(R2048)R8) of the output of the convolutional\nneural network, where the embedding was trained\nto minimize the Euclidean distance between artists.\nThese features mostly capture the timbral character\nof a song.\n\u000fkey and mode information from the Echonest ana-\nlyzer is mapped into points in R3so that adjacent\nkeys in the circle of ﬁfths and relative major/minor\nkeys are equidistant, as pictured in Figure 2: Left.\n\u000ftempo (originally in beats per minute estimated from\nthe Echonest analyzer) is represented in a base-2\nlogarithmic scale. In certain applications it is de-\nsirable to preserve tempo-octave invariance: in that\ncase tempo is represented as a unit vector whose po-\nlar angle is mapped into a tempo octave, as in Fig-\nure 2: Right.\n34, 68, 136 bpm44, 88, 176 bpm\nCGDAEBF#C#AbEbBbFCmFmBbmEbmG#mC#mF#mBmEmAmDmGmhh1\nFigure 2 : Left: key/mode mapping to Euclidean space.\nRight: octave-invariant tempo mapping to Euclidean\nspace.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 443A feature vector is ﬁnally constructed by concatenating\nthe above individual feature vectors, each feature is option-\nally weighted according to the application (e.g., in dance\nplaylist, tempo coherence is important, thus tempo would\nhave a large weight).\n3.1.2 Solution as a Graph Problem\nLet us consider the complete symmetric graph in which\neach song is associated to a vertex, and edges are weighted\nby the Euclidean distance between the corresponding\nsongs’ features.\nAHamiltonian Path is a path that visits each vertex\nin a graph exactly once. The optimal sequencing of a\nplaylist corresponds to the Shortest Hamiltonian Path in\nthe (complete) graph, a problem which is unfortunately\nNP-Complete (the total cost of an ordering is the sum of all\nthe weights of the edges in the path). Several approxima-\ntion strategies, shown below, have been considered; their\ncomputational cost is dominated by the construction of the\nweight matrix, quadratic in the length of the playlist.\nA straightforward greedy approximation (which we de-\nnote by HAM-1 ) consists of iteratively selecting the closest\nnon-visited vertex, starting from a given seed vertex. An\nimprovement ( HAM-2 ) can be made by selecting the clos-\nest non-visited vertex from either the tail or the head of the\npartial sequencing.\nEmpirically, both methods give satisfying results; the\ntotal cost of a HAM-2 sequencing is virtually always lower\n(better) than its HAM-1 counterpart, although the seed\ntrack does not end up as the head of the sequencing any-\nmore (which could be itself a desirable feature). An un-\ndesirable artifact is the presence of poor track pairings at\nthe tail of the sequencing for HAM-1 and at both ends for\nHAM-2 , due to the greedy nature of the algorithm.\nA different solution is given by the Shortest Hamil-\ntonian Cycle , an NP-complete problem (also known as\ntheTraveling Salesman Problem ) which however admits\na polynomial 2-approximation [13]. The cost is usually\nhigher than either of the greedy Hamiltonian Path solu-\ntions, but the resulting playlist will have smooth transi-\ntions, even when repeated in a loop, and is free from the\nhead and tail artifacts described above.\n3.2 Evaluation\nTo measure the effectiveness of the sequencing algorithm,\nwe ran a pilot study in which professional curators blindly\ncompared six sequenced vs. randomly sequenced playlists.\nEach of the six playlists contained 30 “Discover Weekly”\nplaylists. The sequenced version of the playlist was created\nusing HAM-2 with acoustic vectors as features. For each\nof the six playlist pairs, the curators were instructed to (1)\nchoose which playlist was sequenced better, and (2) list the\npairs of tracks in each playlist that were deemed “abrupt”\nwhen played sequentially.\nIn the ﬁrst task, for playlists 1, 2, and 5 the curators\nunanimously chose the sequenced playlist over the random\nplaylist. For playlists 3 and 4, the curators were evenlysplit showing no preference, and for playlist 6, half pre-\nferred the sequenced, and half had either no preference or\npreferred the random playlist. Table 1 shows the average\nnumber of “abrupt” pairs of tracks across curators for each\nplaylist. As expected there were more abrupt pairs in the\nrandom versions than in the sequenced versions. This is\nparticularly drastic for playlist 5, probably due to the wider\nrange of genres.\nPlaylist Genres Random Sequenced\n1Folk Pop ,\nCountry2.8 (1.8) 1.2 (1.3)\n2Underground Hip-Hop ,\nFunk3.8 (4.3) 1.2 (1.3)\n3Abstract Hip-Hop ,\nIndietronica2.7 (2.0) 3.3 (2.2)\n4Indietronica ,\nIndie Rock2.8 (1.9) 2.8 (3.7)\n5Jazz, Classical,\nHouse9.3 (1.5) 2.7 (1.2)\n6Folk Metal ,\nDeath Metal4.00 (3.6) 3.50 (2.3)\nAverage 4.2 (2.5) 2.4 (1.0)\nTable 1 : Average number of song pairs (out of a total of\n29 pairs) marked as “abrupt” across curators. The standard\ndeviation is indicated in parentheses.\n4. TRANSITIONS\nVarious streaming services provide, as a toggleable fea-\nture, a simple ﬁxed-length crossfading between tracks; this\nhowever does not take content into account. About 95% of\nthe users of Spotify forgo the option, and use standard end-\nto-end playback. To motivate the inclusion of transitions in\na playlist, an A-B test was run on 10% of users of Spotify,\nwhere DJ-curated transitions were added to several popu-\nlar playlists for the test group. The results showed that the\npercentage of people who returned to the playlists per day\nwas 1.4 percentage points higher for the test group than\ncontrol, suggesting that the listeners enjoyed the playlists\nwith DJ curated transitions more and were thus more likely\nto listen again.\nThe goal of the algorithm we present is to create inter-\nesting DJ-like transitions between pairs of songs, which\ncould be offered as an enhanced alternative to the existing\ncrossfade. This involves choosing where in each track the\ntransition will occur given a ﬁxed transition length (in units\nof number of beats).\n4.1 Method\nGiven a pair of tracks and a target transition length, our\nmethod selects transition start and end points in both songs,\nand uses this information to render the transition. Transi-\ntion locations are restricted to downbeats, and are heavily\nweighted to occur on section boundaries, such as at the\nintersaection of a verse and a chorus. Additionally, we as-\nsume that regions of tracks that have similar timbre and\npitch distributions will yield the smoothest transition. In\nthis work, we only consider music in quadruple meter.444 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017A symmetric crossfade, depicted in Figure 3, is ar-\nguably the most basic kind of transition: t(A)\n1andt(B)\n1\ndenote the fade out start and end points in track 1, and\nt(A)\n2andt(B)\n2denote the fade in start and end points in\ntrack 2; the duration of the transition region , the interval\n[t(A)\ni; t(B)\ni], for track 1 and 2 need not be equal.\nFigure 3 : Sample crossfade transition. t(A)\n1andt(B)\n1mark\nthe start and end points of the fade out for track 1. Sim-\nilarly, t(A)\n2andt(B)\n2mark the start and end points of the\nfade in for track 2.\n4.1.1 Features\nUnless otherwise stated, each of the following features are\ncomputed for each track using the Echo Nest Analyzer,\nwhich is largely based on [10]. Let bbe a list of es-\ntimated beat positions in seconds. Given the beat posi-\ntions of each track, we compute several different types of\nevent locations, each on the same time grid as the estimated\nbeats. Let Mbe the set of indices of bwhich are down-\nbeats. Similarly, let Sbe the set of indices of bwhich\nare section boundaries, and Dbe indices which are “drop”\npoints. Section boundaries are computed using the method\ndescribed in [14], and the “drop” point estimation is de-\nscribed in Section 4.1.2.\nWhen choosing transition points, not all beats are cre-\nated equal: the best transition points occur at strong struc-\ntural boundaries. Each type of event location has a dif-\nferent level of structural signiﬁcance in the track. The\nstrongest structural boundaries, if they exist, are at the drop\npoints. The next strongest points are section boundaries,\nfollowed by downbeats. Ideally, all drop points are section\nboundaries, and all section boundaries are downbeats, but\nthis may not be the case.\nIn addition to these event locations, we compute several\nbeat-synchronous features. Let Nbe the number of beats.\nTimbre featuresTare a ( 12xN) matrix describing the\nspectral shape of each beat, and the chroma features Care\na (12xN) matrix giving the pitch class distribution for\neach beat. Loudness features `and “vocalness” features v\ngive the loudness and probability of vocals for each beat,\nand are each size ( 1xN). Intuitively, transition regions\nwith low loudness can often sound awkward and abrupt,\nand when vocals are present we risk overlapping vocals\nwith the other track, or cutting over mid-sentence.4.1.2 Drop Point Estimation\nThe goal in drop point estimation is to ﬁnd the points in a\ntrack where the “drop” happens. The term “drop” is typ-\nically used in the context of speciﬁc types of electronic\ndance music, and refers to the point(s) in time where a\ndrastic change in the song occurs following a large build.\nIn our context, we are looking for points in a song where\nan exceptionally interesting event occurs. Rather than take\na content-based solution similar to [17], we use a crowd-\nsourced approach following from the work described in\nP. Lamere’s blog10. Lamere computes the points where\nusers moved (scrubbed) the playhead while listening to a\ntrack. Typically users tend to move the playhead towards\nthe most interesting points in the track. Figure 4 (top)\nshows an example of the aggregated playhead scrubbing\ndata (blue) for Skrillex: “First of the Year” . The large\npeak occurring around 66 seconds accurately marks the\nﬁrst big drop, and the second smaller peak around 145 sec-\nonds marks the second big drop.\nTo identify these peak locations, we use a standard peak\npicking approach from the onset detection literature [1]:\nan adaptive threshold (shown in green) is computing us-\ning a median ﬁlter, then a detection function subtracts the\nadaptive threshold from the normalized scrub ratio and se-\nlects its peaks, as shown in Figure 4 (bottom). Choosing\nthe closest downbeat that occurs before each resulting peak\ngives us our ﬁnal drop index D. Note that in Figure 4 there\nis a small peak near the start of the track which is not a\nsigniﬁcant musical point. We correct for this by removing\npeaks that occur within the ﬁrst 15 seconds of the track.\nFigure 4 : Drop point estimation intermediate steps for\nSkrillex: “First of the Year (Equinox)” .Top: Normal-\nized scrub ratio and adaptive threshold. Bottom : Detec-\ntion function and detected drop points. The ﬁrst peak in\nthe detection function is not a drop point because it occurs\nwithin the ﬁrst 15 seconds of the track.\n10http://musicmachinery.com/2015/06/16/\nthe-drop-machine/Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4454.1.3 Selecting Transition Points\nThe procedure for selecting transition points between track\n1 (T1) and track 2 ( T2) of length nbeats is outlined in\nAlgorithm 1. The functions beats andfeatures are\ndescribed in Section 4.1.1.\nLett1andt2be the set of transition point candidates\nfrom which t(A)\n1andt(A)\n2will be selected. Since we are\ngiven a transition duration (in units of number of beats),\nt(B)\n1andt(B)\n2can be determined from the values of t(A)\n1\nandt(A)\n2. Initially, we set t1=M1andt2=M2.\nWe prune these sets to ensure that the transition points\nhappen in reasonable portions of the track, removing ob-\nvious “bad” regions. The pruning is performed using the\nfollowing rules:\n\u000ft(B)\n1occurs before the fade out, t(A)\n2is after the fade in\n\u000ft(B)\n1occurs within the last 25% of the track, t(A)\n2occurs\nwithin the ﬁrst 20% of the track.\nAfter pruning, the remaining points in t1andt2are con-\nsidered valid candidates. These pruned sets are the output\nof the candidates function.\nFor each pair of points in t1andt2, we compute pair-\nwise comparisons along a series of different features over\nthe entire overlapping region. For a transition of length\nnbeats, the overlapping region begins at beats iandj,\nand ends at beats i+nandj+n. In Algorithm 1 be-\nginning at line 9, we use the notation T1[i:in]to denote\nfeatures within the region beginning at beat iand ending at\nbeatin. Let \u0003be the combined transition point cost ma-\ntrix, where one axis represents the beat indices of track 1\nand the second of track 2. Let \u0003xbe the transition cost\nmatrix for a particular feature comparison x. For timbre\nand chroma features, we compute \u0003Tand\u0003Cas the Eu-\nclidean distance between the features directly (Algorithm 1\nlines 9, 10). \u0003`(line 11) is computed as the sum of the av-\nerage inverse loudness for each track, giving regions that\nare loud in both tracks a low transition cost. Similarly, \u0003v\nis the sum of the average “vocalness” probability, to assign\ntransitions that both have vocals a high cost. Finally, we\npenalize transitions that do not end on a drop or a second\nboundary in both tracks (lines 13, 14), with a score of 2 if\nneither track’s region ends on a boundary, and a score of 1\nif only one track’s region ends on a boundary.\nEach feature’s individual cost matrix \u0003xis standardized\nso that the minimum cost is 0 and the maximum cost is 1.\nThe ﬁnal cost matrix \u0003is computed as a weighted sum of\neach feature’s cost matrix after standardization. An exam-\nple of each of feature’s standardized matrix is shown in\nFigure 5, and the weighed combination is shown in Fig-\nure 6. The ﬁnal transition points t(A)\n1andt(B)\n2are chosen\nas the times corresponding to the minimum cost entry in\n\u0003.\n4.2 Rendering Transitions\nTransitions are rendered such that the beats in the two\ntracks occur at the same time. In virtually every case, theAlgorithm 1 Picking Transition Points\n1:procedure TRANSITION-POINTS( T1; T2; n)\n2: b1 beats (T1),b2 beats (T2)\n3:T1; C1; `1; v1; M 1; D1; S1 features (T1;b1)\n4:T2; C2; `2; v2; M 2; D2; S2 features (T2;b2)\n5: t1 candidates (T1; M 1; S1; D1; `1)\n6: t2 candidates (T2; M 2; S2; D2; `2)\n7: fori2t1; j2t2do\n8: in i+n jn j+n\n9: \u0003T[i; j] norm (T1[i:in]\u0000S2[j:jn])\n10: \u0003C[i; j] norm (C1[i:in]\u0000C2[j:jn])\n11: \u0003`[i; j] avg(2\u0000(`1[i:in] +`2[j:jn]))\n12: \u0003v[i; j] avg(v1[i:in]) +avg(v2[j:jn])\n13: \u0003D[i; j] 1in=2D1+ 1jn=2D2\n14: \u0003S[i; j] 1in=2S1+ 1jn=2S2\n15: end for\n16: \u0003 [\u0003T;\u0003C;\u0003`;\u0003v;\u0003D;\u0003S]\n17: fork2\u0003do\n18: k standardize(k)\n19: end for\n20: \u0003 weightedAvg (\u0003T;\u0003C;\u0003`;\u0003v;\u0003D;\u0003S)\n21: i\u0003; j\u0003 argmin (\u0003)\n22: t(A)\n1; t(A)\n2 b1[i\u0003];b2[j\u0003]\n23: return t(A)\n1; t(A)\n2\n24:end procedure\ntempos are not perfectly in sync, each beat is timestretched\nsuch that the tempo slowly changes from the tempo of track\n1 to the tempo of track 2. For an Nbeat transition, if the\nnth beat in track 1 has duration d1and the beat in track 2\nhas duration d2, the total duration of the new nth beat is\ndout=N\u0000n\nNd1+n\nNd2. To achieve this, the nth beat in\ntrack 1 is time stretched by a factor of d1=dout, and the nth\nbeat in track 2 by d2=dout.\n4.3 Evaluation\nA selection of rendered transitions were evaluated by sub-\njective human review. We randomly picked 48 pairs of\ntracks from a selection of popular music across multiple\ndance genres, using tempo constraints when picking the\ntracks to make sure the tempo difference between pairs was\nno more than 5 bpm.\nFor each of the pairs, we asked four professional cura-\ntors to listen to the transition all the way through at least\nonce and rate the quality. For subjective measurement, the\noverall quality is described as Good (3), OK (2) and Bad\n(1). Additionally, curators were asked to describe any po-\ntential problems they noticed within the transitions, such as\n“beats do not align” or “key clash”. The results are shown\nin Tables 2 and 3, respectively.\nA fairly large number (15%) of transitions were marked\nas “Bad” because the “beats do not align”. Since we con-\nstrain transitions to align along estimated beats, we con-\nclude that the “beats do not align” transitions occur as a re-\nsult of errors in the beat estimation algorithm. Transitions\nlabeled as “transitioning mid-vocals” are also likely a re-\nsult of errors in our vocal activity detection algorithm. In446 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 5 : Transition matrices for each feature for a pair of songs. The x-axis show beat indices in Track 1, and the y-axis\nfor Track 2. Many index pairs have no score because they are not part of the set of candidates. Dark blue points indicate\ngood transition pairs for the feautre, while red indicates a poor pair. In this example, no drops were detected, so \u0003Dis a\nuniform matrix.\nFigure 6 : Weighted combination \u0003of the individual fea-\nture matrices in Figure 5.The x-axis show beat indices in\nTrack 1, and the y-axis for Track 2. The point with the\nlowest cost is circled in green.\nRating Percentage\n3 - Good 64.13%\n2 - OK 28.26%\n1 - Bad 7.61%\nAverage (Std) Rating 2.56 (0.38)\nTable 2 : Average percentage of quality rating for all track\npairs and average rating of song pairs in rendered transition\ntest set. The standard deviation is indicated in parentheses.\nReason Percentage\nBeats do not align 15.22%\nNot on downbeat 2.17%\nKey clash 0%\nAwkward transition points 2.17%\nTransitions mid-vocals 6.52%\nContrasting Songs 4.34%\nTable 3 : Average percentage of song pairs marked as the\nstated reason for bad quality transitions by curators.both of these cases, as beat tracking and vocal activity de-\ntection algorithms improve, these transition quality issues\nshould be mitigated. An interesting ﬁnding is that “key\nclash” is not marked as problematic by any of the curators\nfor a single transition in either transition types.\n5. CONCLUSIONS\nThis paper has presented systems for automatically se-\nquencing and generating DJ-style transitions for a playlist\nof songs. Both systems were evaluated with the help of\nprofessional curators. Beat and downbeat tracking errors\nwere found to be the primary bottleneck in the subjective\nperformance of automated transitions.\nA possible alternative approach for tackling the se-\nquencing and transitioning problems entails the usage of\nMachine Learning approaches. Given a large number of\n(carefully curated) playlists and transition points between\nthem, one might attempt to directly learn the mapping of\nlow-level audio representation of recordings into their op-\ntimal sequencing and transitions. Such an methodology\nis certainly fascinating, and represents in fact a future re-\nsearch direction. However, the experiments above prove\nhow just a few interpretable features are suitable for this\nproblem to a remarkable extent. We chose then to inves-\ntigate an approach that is heuristic in nature, but whose\nparticular behavior can be customized by the user in an\nextremely intuitive manner (e.g., weighting acoustic sim-\nilarity more than key and tempo might be preferred when\nconstructing a playlist for a radio show, while the reverse\nis true in the case of a dancing playlist).\nFinally, this work has focused on speciﬁc genres of mu-\nsic – namely “party” music. The constraints we imposed\nmay not be necessary or sufﬁcient for other genres of mu-\nsic, for example rap. However, the same framework could\nbe applied substituting different features in the optimiza-\ntion problem. The exploration of how to apply this model\nto other genres is left as future work.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4476. REFERENCES\n[1] Juan Pablo Bello, Laurent Daudet, Samer Abdallah,\nChris Duxbury, Mike Davies, and Mark B Sandler. A\ntutorial on onset detection in music signals. Speech and\nAudio Processing, IEEE Transactions on , 13(5):1035–\n1047, 2005.\n[2] Shuo Chen, Josh L Moore, Douglas Turnbull, and\nThorsten Joachims. Playlist prediction via metric em-\nbedding. In Proceedings of the 18th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining , pages 714–722. ACM, 2012.\n[3] Dave Cliff. Hang the DJ: Automatic sequencing and\nseamless mixing of dance-music tracks. HP LABORA-\nTORIES TECHNICAL REPORT HPL , 104, 2000.\n[4] Sally Jo Cunningham, David Bainbridge, and Annette\nFalconer. ’more of an art than a science’: Supporting\nthe creation of playlists and mixes. In ISMIR , pages\n240–245, 2006.\n[5] Matthew EP Davies, Philippe Hamel, Kazuyoshi\nYoshii, and Masataka Goto. Automashupper: Au-\ntomatic creation of multi-song music mashups.\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing (TASLP) , 22(12):1726–1737, 2014.\n[6] Arthur Flexer, Dominik Schnitzer, Martin Gasser, and\nGerhard Widmer. Playlist generation using start and\nend songs. In ISMIR , pages 173–178, 2008.\n[7] Roman B Gebhardt, Matthew EP Davies, and Bern-\nhard U Seeber. Psychoacoustic approaches for har-\nmonic music mixing. Applied Sciences , 6(5):123,\n2016.\n[8] Tatsunori Hirai, Hironori Doi, and Shigeo Morishima.\nMusicmixer: Computer-aided dj system based on an\nautomatic song mixing.\n[9] Hiromi Ishizaki, Keiichiro Hoashi, and Yasuhiro Tak-\nishima. Full-automatic dj mixing system with optimal\ntempo adjustment based on measurement function of\nuser discomfort. In ISMIR , pages 135–140, 2009.\n[10] Tristan Jehan. Creating music by listening . PhD thesis,\nMassachusetts Institute of Technology, 2005.\n[11] Thor Kell and George Tzanetakis. Empirical analy-\nsis of track selection and ordering in electronic dance\nmusic using audio feature extraction. In ISMIR , pages\n505–510, 2013.\n[12] Chuan-Lung Lee, Yin-Tzu Lin, Zun-Ren Yao, Feng-Yi\nLee, and Ja-Ling Wu. Automatic mashup creation by\nconsidering both vertical and horizontal mashabilities.\nInISMIR , pages 399–405, 2015.\n[13] Shen Lin and Brian W Kernighan. An effective heuris-\ntic algorithm for the traveling-salesman problem. Op-\nerations research , 21(2):498–516, 1973.[14] B. McFee and D. P. W. Ellis. Analyzing song structure\nwith spectral clustering. In ISMIR , 2014.\n[15] Jaume Parera. Dj codo nudo: a novel method for\nseamless transition between songs for electronic music.\nMaster’s thesis, Universitat Pompeu Fabra, Barcelona,\n2016.\n[16] Aaron Van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommenda-\ntion. In Advances in Neural Information Processing\nSystems , pages 2643–2651, 2013.\n[17] Karthik Yadati, Martha Larson, Cynthia CS Liem, and\nAlan Hanjalic. Detecting drops in electronic dance mu-\nsic: Content based approaches to a socially signiﬁcant\nmusic event. In ISMIR , pages 143–148, 2014.448 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Deep Salience Representations for F0 Estimation in Polyphonic Music.",
        "author": [
            "Rachel M. Bittner",
            "Brian McFee",
            "Justin Salamon",
            "Peter Li",
            "Juan Pablo Bello"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417937",
        "url": "https://doi.org/10.5281/zenodo.1417937",
        "ee": "https://zenodo.org/records/1417937/files/BittnerMSLB17.pdf",
        "abstract": "Estimating fundamental frequencies in polyphonic music remains a notoriously difficult task in Music Information Retrieval. While other tasks, such as beat tracking and chord recognition have seen improvement with the appli- cation of deep learning models, little work has been done to apply deep learning methods to fundamental frequency related tasks including multi-f0 and melody tracking, pri- marily due to the scarce availability of labeled data. In this work, we describe a fully convolutional neural network for learning salience representations for estimating fundamen- tal frequencies, trained using a large, semi-automatically generated f0 dataset. We demonstrate the effectiveness of our model for learning salience representations for both multi-f0 and melody tracking in polyphonic audio, and show that our models achieve state-of-the-art performance on several multi-f0 and melody datasets. We conclude with directions for future research.",
        "zenodo_id": 1417937,
        "dblp_key": "conf/ismir/BittnerMSLB17",
        "keywords": [
            "fundamental frequencies",
            "Music Information Retrieval",
            "deep learning models",
            "scarce availability of labeled data",
            "fully convolutional neural network",
            "salience representations",
            "multi-f0",
            "melody tracking",
            "polyphonic audio",
            "state-of-the-art performance"
        ],
        "content": "DEEP SALIENCE REPRESENTATIONS FOR F0ESTIMATION IN\nPOLYPHONIC MUSIC\nRachel M. Bittner1\u0003, Brian McFee1;2, Justin Salamon1, Peter Li1, Juan P. Bello1\n1Music and Audio Research Laboratory, New York University, USA\n2Center for Data Science, New York University, USA\n\u0003Please direct correspondence to: rachel.bittner@nyu.edu\nABSTRACT\nEstimating fundamental frequencies in polyphonic music\nremains a notoriously difﬁcult task in Music Information\nRetrieval. While other tasks, such as beat tracking and\nchord recognition have seen improvement with the appli-\ncation of deep learning models, little work has been done\nto apply deep learning methods to fundamental frequency\nrelated tasks including multi- f0and melody tracking, pri-\nmarily due to the scarce availability of labeled data. In this\nwork, we describe a fully convolutional neural network for\nlearning salience representations for estimating fundamen-\ntal frequencies, trained using a large, semi-automatically\ngenerated f0dataset. We demonstrate the effectiveness of\nour model for learning salience representations for both\nmulti- f0and melody tracking in polyphonic audio, and\nshow that our models achieve state-of-the-art performance\non several multi- f0and melody datasets. We conclude\nwith directions for future research.\n1. INTRODUCTION\nEstimating fundamental frequencies in polyphonic music\nremains an unsolved problem in Music Information Re-\ntrieval (MIR). Speciﬁc cases of this problem include multi-\nf0tracking, melody extraction, bass tracking, and piano\ntranscription among others. Percussion, overlapping har-\nmonics, high degrees of polyphony, and masking make\nthese tasks notoriously difﬁcult. Furthermore, training and\nbenchmarking is difﬁcult due to the limited amount of\nhuman-labeled f0data available.\nHistorically, most algorithms for estimating fundamen-\ntal frequencies in polyphonic music have been built on\nheuristics. In melody extraction, two algorithms that have\nretained the best performance are based on pitch contour\ntracking and characterization [8,27]. Algorithms for multi-\nf0tracking and transcription have been based on heuris-\ntics such as enforcing spectral smoothness and emphasiz-\ning harmonic content [17], comparing properties of co-\nc\rRachel M. Bittner1\u0003, Brian McFee1;2, Justin Salamon1,\nPeter Li1, Juan P. Bello1. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: Rachel M.\nBittner1\u0003, Brian McFee1;2, Justin Salamon1, Peter Li1, Juan P. Bello1.\n“Deep Salience Representations for F0Estimation in Polyphonic Music”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.occurring spectral peaks/non-peaks [11], and combining\ntime and frequency-domain periodicities [29]. Other ap-\nproaches to multi- f0tracking are data-driven and require\nlabeled training data, e.g. methods based on supervised\nNMF [32], PLCA [3], and multi-label discriminative clas-\nsiﬁcation [23]. For melody extraction, machine learning\nhas been used to predict the frequency bin of an STFT\ncontaining the melody [22], and to predict the likelihood\nan extracted frequency trajectory is part of the melody [4].\nThere are a handful of datasets with fully-annotated\ncontinuous- f0labels. The Bach10 dataset [11] contains\nten 30-second recordings of a quartet performing Bach\nchorales. The Su dataset [30] contains piano roll annota-\ntions for 10 excerpts of real-world classical recordings, in-\ncluding examples of piano solos, piano quintets, and violin\nsonatas. For melody tracking, the MedleyDB dataset [5]\ncontains melody annotations for 108 full length tracks that\nare varied in musical style.\nMore recently, deep learning approaches have been ap-\nplied to melody and bass tracking in speciﬁc musical sce-\nnarios, including a BLSTM model for singing voice track-\ning [25] and fully connected networks for melody [2] and\nbass tracking [1] in jazz music. In multi- f0tracking, deep\nlearning has also been applied to solo piano transcription\n[7,28], but nothing has been proposed that uses deep learn-\ning for multi- f0tracking in a more general musical con-\ntext. In speech, deep learning has been applied to both\npitch tracking [14] and multiple pitch tracking [18], how-\never there is much more labeled data for spoken voice, and\nthe space of pitch and spectrum variations is quite different\nthan what is found in music.\nThe primary contribution of this work is a model for\nlearning pitch salience representations using a fully convo-\nlutional neural network architecture, which is trained using\na large, semi-automatically annotated dataset. Addition-\nally, we present experiments that demonstrate the useful-\nness of the learned salience representations for both multi-\nf0and melody extraction, outperforming state-of-the-art\napproaches in both tasks. All code used in this paper, in-\ncluding trained models, is made publicly available.1\n2. SALIENCE REPRESENTATIONS\nPitch salience representations are time-frequency represen-\ntations that aim to measure the saliency (i.e. perceived am-\n1github.com/rabitt/ismir2017-deepsalience63plitude/energy) of frequencies over time. They typically\nrely on the assumption that sounds humans perceive as\nhaving a pitch have some kind of harmonic structure. The\nideal salience function is zero everywhere where there is\nno perceptible pitch, and a positive value that reﬂects the\npitches’ perceived loudness at the fundamental frequency.\nSalience representations are core components of a number\nof algorithms for melody [8, 12, 27] and multi- f0track-\ning [17,26]. Computations of salience representations usu-\nally perform two functions: (1) de-emphasize un-pitched\nor noise content (2) emphasize content that has harmonic\nstructure.\nThe de-emphasis stage can be performed in a variety\nof ways, including harmonic-percussive source separation\n(HPSS), re-weighting frequency bands (e.g. using an equal\nloudness ﬁlter or a high pass ﬁlter), peak picking, or sup-\npressing low amplitude or noise content [8, 12, 17, 26, 27].\nIn practice most salience functions also end up emphasiz-\ning harmonics and subharmonics because they are difﬁcult\nto untangle from the fundamental, especially in complex\npolyphonies. The many parameters of these ﬁltering and\nsmoothing steps are typically set manually.\nHarmonic content is most commonly emphasized via\nharmonic summation, which re-weights the input repre-\nsentation across frequency, where frequency bins in the\nsalience representation are a weighted sum of harmoni-\ncally related bins in the input representation [17, 27]. The\nweights in this summation vary from method to method,\nand are usually chosen heuristically based on assumptions\nabout the data. In another variant, the input represen-\ntation is modeled using non-negative least squares to a\nmanually constructed set of ideal harmonic templates [19].\nThe Fan Chirp transform [9] uses harmonic information in\nthe transform itself, thus directly performing the harmonic\n“weighting”.\nIn melody extraction, the salience representation has\nbeen found to be a bottleneck in algorithmic perfor-\nmance [4], often because large portions of the melody are\nnot emphasized. In particular, the salience representation\nused in Melodia [27] was found to emphasize vocal content\nwell, but often miss instrumental content.\nThe combination of HPSS, equalization, and harmonic\nsummation to emphasize pitched content and suppress the\nrest can be naturally extended in the context of deep learn-\ning architectures. For example, a simple version of HPSS\nperforms median ﬁltering with one kernel in time and fre-\nquency, and assigns bins to the harmonic or percussive\ncomponent by a max ﬁltering operation [13]. The har-\nmonic and percussive decompositions can be cascaded to\ncompute, for example, the harmonic component of the per-\ncussive signal as in [10, 25] to recover content that is not\nstrongly activated by vertical or horizontal median ﬁlters\nsuch as singing voice. This cascade of median ﬁltering\ncan be naturally extended to a convolutional neural net-\nwork setting, where instead of using only two manually set\nkernels, any number of kernels can be learned and their\noutputs combined in order to generalize to many types of\nmusical sounds. Similarly, the parameters of harmonicsummation can be implicitly learned by using an input\nrepresentation that aligns harmonically related content—\nnamely we introduce the harmonic CQT which we de-\nscribe in Section 3.1. Furthermore, with a convolutional\narchitecture, the parameters of the de-noising stage and the\nharmonic emphasis stage can be learned jointly.\n3. METHOD\nWe frame our approach as a de-noising problem as de-\npicted in Figure 1: given a time-frequency representation\n(e.g. a CQT), learn a series of convolutional ﬁlters that will\nproduce a salience representation with the same shape in\ntime and frequency. We constrain the target salience rep-\nresentation to have values between 0 and 1, where large\nvalues should occur in time-frequency bins where funda-\nmental frequencies are present.\n3.1 Input Representation\nIn order to better capture harmonic relationships, we use a\nharmonic constant-Q transform (HCQT) as our input rep-\nresentation. The HCQT is a 3-dimensional array indexed\nby harmonic, frequency, and time: H[h; t; f ], measures\nthehth harmonic of frequency fat time t. The harmonic\nh= 1refers to the fundamental, and we introduce the no-\ntationH[h]to denote harmonic hof the “base” CQT H[1].\nFor any harmonic h > 0,H[h]is computed as a standard\nCQT where the minimum frequency is scaled by the har-\nmonic: h\u0001fmin, and the same frequency resolution and\nnumber of octaves is shared across all harmonics. The re-\nsulting representation His similar to a color image, where\nthehdimension is the depth .\nIn a standard CQT representation, the kth frequency\nbin measures frequency fk=fmin\u00012k=BforBbins per\noctave. As a result, harmonics h\u0001fkcan only be di-\nrectly measured for h= 2n(for integer n), making it\ndifﬁcult to capture odd harmonics. The HCQT represen-\ntation, however, conveniently aligns harmonics across the\nﬁrst dimension, so that the kth bin ofH[h]has frequency\nfk=h\u0001fmin\u00012k=B, which is exactly the hth harmonic\nof the kth bin ofH[1]. By aligning harmonics in this way,\nthe HCQT is amenable to modeling with two-dimensional\nconvolutional neural networks, which can now efﬁciently\nexploit locality in time, frequency, and harmonic.\nIn this work, we compute HCQTs with h2\nf0:5;1;2;3;4;5g: one subharmonic below the fundamen-\ntal (0.5), the fundamental (1), and up to 4 harmonics above\nthe fundamental. Our hop size is \u001911 ms in time, and\nwe compute 6 octaves in frequency at 60 bins per octave\n(20 cents per bin) with minimum frequency at h= 1 of\nfmin= 32:7Hz (i.e. C1). We include a subharmonic in ad-\ndition to harmonics to help disambiguate between the fun-\ndamental frequency and the ﬁrst harmonic, whose patterns\nof upper harmonics are often similar – for the fundamen-\ntal, the ﬁrst subharmonic should have low energy, where\nfor the ﬁrst harmonic, a subharmonic below it will have\nenergy. Our implementation is based on the CQT imple-\nmentation in librosa [21].64 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017h \nt f f \nt Figure 1 . Input HCQT (left) and target salience function\n(right).\n3.2 Output Representation\nThe target outputs we use to train the model are time-\nfrequency representations with the same shape as H[1].\nGround truth fundamental frequency values are quantized\nto the nearest time-frequency bin, and given magnitude\n= 1 in the target representation. The targets are Gaus-\nsian blurred in frequency such that the energy surrounding\na ground truth frequency decays to zero within a quarter-\ntone, in order to soften the penalty for near-correct pre-\ndictions during training. Additionally, since the data is\nhuman labeled it may not be accurate to 20 cents, so\nwe do not necessarily want to label nearby frequencies\nas “wrong”. Similar training label “blurring” techniques\nhave been shown to help the performance of models for\nbeat/downbeat tracking [6] and structural boundary detec-\ntion [31].\n3.3 Model\nOur model uses a fully convolutional architecture, with 5\nconvolutional layers of varying dimensionality, as illus-\ntrated in Figure 2. The ﬁrst two layers have 128 and 64\n(5 x 5) ﬁlters respectively, which cover approximately 1\nsemitone in frequency and 50 ms in time. The following\ntwo layers each have 64 (3 x 3) ﬁlters, and the ﬁnal layer\nhas 8 (70 x 3) ﬁlters, covering 14 semitones in frequency\nto capture relationships between frequency content within\nan octave. At each layer, the convolutions are zero padded\nsuch that the input shape is equal to the output shape in\nthe time-frequency dimension. The input to each layer is\nbatch normalized [15], and the outputs are passed through\nrectiﬁed linear units. The ﬁnal layer uses logistic activa-\ntion, mapping each bin’s output to the range [0;1]. The\npredicted saliency map can be interpreted as a likelihood\nscore of each time-frequency bin belonging to an f0con-\ntour. Note that we do not include pooling layers, since\nwe do notwant to be invariant to small shifts in time fre-\nquency.\nThe model is trained to minimize cross entropy:\nL(y;^y) =\u0000ylog(^y)\u0000(1\u0000y) log(1\u0000^y) (1)\nwhere both yand^yare continuous values between 0 and 1.\nWe ﬁt our model using the Adam [16] optimizer.\n360\n50\n6 128 64 64 64 8360\n50\n15\n5 55 3\n3 3370\n31\n1Figure 2 . CNN architecture. The input to each layer\nis batch-normalized. The output of each layer is passed\nthrough a rectiﬁed linear unit activation function except the\nlast layer which is passed through a sigmoid.\n4. MULTIPLE- F0TRACKING EXPERIMENTS\nWe ﬁrst explore the usefulness of our model when trained\nto produce a multi- f0salience representation.\n4.1 Data Generation\nBecause there is no large human-labeled dataset to use for\ntraining, we generate a dataset from a combination of hu-\nman and machine generated f0annotations by leveraging\nmultitrack data. Our total dataset contains 240 tracks from\na combination of the 108 MedleyDB multitrack dataset [5]\nand a set of 132 pop music multitracks. The pop multi-\ntrack set consists of western popular music from the 1980s\nthrough today, and were obtained from a variety of sources\nand are not available for redistribution—because of this we\nonly use these examples during training. The tracks are\nsplit into train, validate, and test groups using an artist-\nconditional randomized split (i.e. tracks belonging to the\nsame artist must all belong to the same group). The test\nset is constrained to contain only tracks from MedleyDB,\nand contains 28 full-length tracks. The training and valida-\ntion sets contain 184 and 28 full-length tracks respectively,\ntotaling to about 10 hours of training data and 2 hours of\nvalidation data.\nEach multitrack in the dataset contains mixes and iso-\nlated stems, and a subset of these stems contain human-\nlabeled f0annotations. To have a mix where all pitched\ncontent is annotated, we re-create partial mixes by com-\nbining any stems with human annotations, all stems with\nmonophonic instruments (e.g. electric bass), and all per-\ncussive stems, effectively creating mixes that are similar\nto the originals, but with all “unknown” pitch content re-\nmoved. The stems are linearly mixed with weights esti-\nmated from the original mixes using a least squares ﬁt.\nThe human-labeled f0annotations are directly added to\nthe ground truth labels. Annotations for monophonic in-\nstrument stems without human labels are created by run-\nningpYIN [20] and using the output as a proxy for ground\ntruth.\n4.2 Results\nTo generate multi- f0output, we need to explicitly select a\nset of fundamental frequency values for each time frame\nfrom our salience representation. A natural way to do thisProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 65would be to threshold the representation at 0.5, however\nsince the model is trained to reproduce Gaussian-blurred\nfrequencies, the values surrounding a high energy bin are\nusually above 0.5 as well, creating multiple estimates very\nclose to one another. Instead, we perform peak picking\non the learned representation and select a minimum ampli-\ntude threshold by choosing the threshold that maximizes\nthe multi- f0accuracy on the validation set.\nWe evaluate the model on three datasets: the Bach10\nand Su datasets, and the test split of the MedleyDB\ndata described in Section 4.1, and compare to well-\nperforming baseline multi- f0algorithms by Benetos [3]\nand Duan [11].\nFigure 3 shows the results for each algorithm on the\nthree datasets. We see that our CNN model under-performs\non Bach10 compared to Benetos’ and Duan’s models by\nabout 10 percentage points, but outperforms both algo-\nrithms on the Su and MedleyDB datasets. We attribute the\ndifference in performance across these datasets to the way\neach model was trained. Both Benetos’ and Duan’s meth-\nods were in some sense developed with the Bach10 dataset\nin mind simply because it has been one of the few avail-\nable test sets when the algorithms were developed. On the\nother hand, our model was trained on data most similar to\nthe MedleyDB test set, so it is unsurprising that it performs\nbetter on this set. The Bach10 dataset is homogeneous (as\ncan be seen by the small variance in performance across\nall methods), and while our model performs obtains higher\nscores on the Bach10 dataset than the other two used for\nevaluation, this dataset only measures how well an algo-\nrithm performs on simple 4-part harmony classical record-\nings. Indeed, we found that on the MedleyDB test set, both\nBenetos’ and Duan’s models perform best (50% and 48%\naccuracy respectively) on the example that is most similar\nto the Bach10 data (a string quartet), and our approach per-\nforms similarly on that track to the overall performance on\nthe Bach10 set with 59% accuracy.\nTo get a better sense of the track level performance, Fig-\nure 4 displays the difference between the CNN accuracy\nand the best accuracy of Benetos and Duan’s model per\ntrack. In addition to having a better score on average for\nMedleyDB (from Figure 3), we see that the CNN model\noutperforms the other two models on every track on Med-\nleyDB by quite a large margin. We see a similar result for\nthe Su dataset, though on one track (Beethoven’s Moon-\nlight sonata) we have a lower score than Benetos. A qual-\nitative analysis of this track showed that our algorithm re-\ntrieves the melody and the bass line, but fails to emphasize\nseveral notes that are part of the harmony line. Unsurpris-\ningly, on the Bach10 dataset, the other two algorithms out-\nperform our approach for every track.\nTo further explain this negative result, we explore how\nour model will perform in an oracle scenario by constrain-\ning the maximum polyphony to 4 (the maximum for the\nBach10 dataset) and look at the accuracy when we vary the\ndetection threshold. Figure 5 shows the CNN’s average ac-\ncuracy on the Bach10 dataset as a function of the detection\nthresholds. The solid dotted line shows the threshold auto-matically estimated from the validation set. For the Bach10\ndataset, the optimal threshold is much lower (0.05 vs. 0.3),\nand the best performance (63% accuracy) gets closer to\nthat of the other two datasets (68% for Duan and 76% for\nBenetos). Even in this ideal scenario, the difference in per-\nformance is due to recall – similarly to the Su example, our\nalgorithm is good at retrieving the melody and bass lines\nin the Bach10 dataset, but often misses notes that occur\nin between. This is likely a result of the characteristics of\nthe artiﬁcial mixes in our training set: the majority of au-\ntomatically annotated (monophonic) stems are either bass\nor vocals, and there are few examples with simultaneous\nharmonically related pitch content.\nOverall, our model has good precision, even on the\nBach10 dataset (where the scores are hurt by recall), which\nsuggests that the learned salience function does a good job\nof de-emphasizing non-pitched content. However, the low\nrecall on the Bach10 and Su datasets suggests that there is\nstill room for the model to improve on emphasizing har-\nmonic content. Compared to the other two algorithms,\nthe CNN makes fewer octave mistakes (3% of mistakes\non MedleyDB compared with 5% and 7% of mistakes for\nBenetos and Duan respectively), reﬂected in the difference\nbetween the accuracy and chroma accuracy.\nWhile the algorithm improves on the state of the art on\ntwo datasets, the overall performance still has a lot of room\nto improve, with the highest score on the Su dataset reach-\ning only 41% accuracy on average. To explore this further,\nin Figure 6 we plot the outputs on excerpts of tracks from\neach of the three datasets. In each of the excerpts, the out-\nputs look reasonably accurate. The top row shows an ex-\ncerpt from Bach10, and while our model sometimes misses\nportions of notes, the salient content (e.g. melody and bass)\nis emphasized. Overall, we observe that the CNN model is\ngood at identifying bass and melody patterns even when\nhigher polyphonies are present, while the other two mod-\nels try to identify chords, even when only melody and bass\nare present.\n4.3 Model Analysis\nThe output of the CNN for an unseen track from the Su\ndataset is shown in Figure 7. H[1]is plotted in the left\nplot, and we can see that it contains a complex polyphonic\nmixture with many overlapping harmonics. Qualitatively,\nwe see that the CNN was able to de-noise the input repre-\nsentation and successfully emphasize harmonic content.\nTo better understand what the model learned, we plot\nthe 8 feature maps from the penultimate layer in Figure 8.\nThe red-colored activations have positive weights and the\nblue-colored have negative weights in the output ﬁlter. Ac-\ntivations (a) and (b) seem to emphasize harmonic content,\nincluding some upper harmonics. Interestingly, activation\n(e) deemphasizes the octave mistake from activation (a),\nas does activation (d). Similarly, activations (f) and (g) act\nas a “cut out” for activations (a) and (b), deemphasizing\nthe broadband noise component. Activation (h) appears to\ndeemphasize low-frequency noise.66 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170.25 0.50 0.75 1.00\nScoreRecallPrecisionChroma AccuracyAccuracy\nBach10\n0.25 0.50 0.75 1.00\nScoreSu\nCNN\nDuan\nBenetos\n0.25 0.50 0.75 1.00\nScore\nMedleyDBFigure 3 . A subset of the standard multiple-f0 metrics on the Bach10, Su, and MedleyDB test sets for the proposed\nCNN-based method, Duan [11], and Benetos [3].\n0.2\n0.00.2Max./uni00A0accuracy/uni00A0diff.Bach10\n0.2\n0.00.2Su\n0.2\n0.00.2MedleyDB\nFigure 4 . The per-track difference in accuracy between the\nCNN model and the maximum score achieved by Duan or\nBenetos’ algorithm on each dataset. Each bar corresponds\nto CNN - max( Duan, Benetos )on a single track.\n0.0 0.2 0.4 0.6 0.8 1.0\nThreshold0.000.250.50Accuracy\nFigure 5 . CNN accuracy on the Bach10 dataset as a func-\ntion of the detection threshold, and when constraining the\nmaximum polyphony to 4. The vertical dotted line shows\nthe value of the threshold chosen on the validation set.\n5. MELODY ESTIMATION EXPERIMENTS\nTo further explore the usefulness of the proposed model\nfor melody extraction, we train a CNN with identical an\narchitecture on melody data.\n5.1 Data Generation\nInstead of training on HCQTs computed from partial mixes\nand semi-automatic targets (as described in Section 4.1),\nwe use HCQTs from the original full mixes from Med-\nleyDB, as well as targets generated from the human-\nlabeled melody annotations. The ground truth salience\nfunctions contain only melody labels, using the “Melody\n2” deﬁnition from MedleyDB (i.e. one melody pitch per\nunit time coming from multiple instrumental sources). We\nestimate the melody line from the learned salience repre-\n0 2.5 5 7.5 10\nTime/uni00A0(sec)128256512Frequency/uni00A0(Hz)\nGT\nCNN\n0 2.5 5 7.5 10\nTime/uni00A0(sec)\nGT\nBenetos\n0 2.5 5 7.5 10\nTime/uni00A0(sec)\nGT\nDuan\n0 12 24 36\nTime/uni00A0(sec)641282565121024Frequency/uni00A0(Hz)\nGT\nCNN\n0 12 24 36\nTime/uni00A0(sec)\nGT\nBenetos\n0 12 24 36\nTime/uni00A0(sec)\nGT\nDuan\n60 70 80 90\nTime/uni00A0(sec)641282565121024Frequency/uni00A0(Hz)\nGT\nCNN\n60 70 80 90\nTime/uni00A0(sec)\nGT\nBenetos\n60 70 80 90\nTime/uni00A0(sec)\nGT\nDuanFigure 6 . Multi-f0 output for each of the 3 algorithms for\nan example track from the Bach10 dataset (top), the Su\ndataset (middle), and the MedleyDB test set (bottom)\nsentation by choosing the frequency with the maximum\nsalience at every time frame. The voicing decision is deter-\nmined by a ﬁxed threshold chosen on the validation set. In\nthis work we did not explore more sophisticated decoding\nmethods.\n5.2 Results\nWe compare the output of our CNN-based melody track-\ning system with two strong, salience-based baseline al-\ngorithms: “Salamon” [27] and “Bosch” [8]. The for-\nmer is a heuristic algorithm that long held the state of\nthe art in melody extraction. The latter recently reached\nstate-of-the-art performance by combining a source-ﬁlter\nbased salience function and heuristic rules for contour\nselection—this model is the current best performing base-\nline. Figure 9 shows the results of the three methods on the\nMedleyDB test split described in Section 4.1.\nOn average, the CNN-based melody extraction outper-\nforms both Bosch and Salamon in terms of Overall (+ 5 andProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 6710 14 18 22\nTime/uni00A0(sec)652621046Frequency/uni00A0(Hz)\n10 14 18 22\nTime/uni00A0(sec)10 14 18 22\nTime/uni00A0(sec)\nFigure 7 . (left) InputH[1], (middle) predicted output,\n(right) ground truth annotation for an unseen track in the\nSu dataset.\n(a)(b)(c)(d)(e)(f)(g)(h)\nFigure 8 . Activations from the ﬁnal convolutional layer\nwith octave height ﬁlters for the example given in Figure 7.\nActivations (a)–(c) have positive coefﬁcients in the output\nlayer, while the others have negative coefﬁcients.\n0.0 0.2 0.4 0.6 0.8 1.0\nScoreVFAVRRCARPAOA\nCNN\nBosch\nSalamon\nFigure 9 . Melody metrics – Overall Accuracy (OA), Raw\nPitch Accuracy (RPA), Raw Chroma Accuracy (RCA),\nV oicing Recall (VR) and V oicing False Alarm (VFA) –\non the MedleyDB test set for the proposed CNN-based\nmethod, Salamon [27], and Bosch [8].\n13 percentage points), Raw Pitch (+15 and 22 percentage\npoints), and Raw Chroma Accuracy (+6 and 14 percentage\npoints). The CNN approach is also considerably more var-\nied in performance than the other two algorithms, with a\nwide range in performance across tracks.\nBecause we choose the frequency with maximum am-\nplitude in our approach, the Raw Pitch Accuracy measures\neffectiveness of the salience representation: in an ideal\nsalience representation for melody, the melody should have\nthe highest amplitude in the salience function over time.\nIn our learned salience function, \u001962% of the time the\nmelody has the largest amplitude. A qualitative analysis\n0 10 20\nTime/uni00A0(sec)12825651210242048Frequency/uni00A0(Hz)\n0 510 15 20 25\nTime/uni00A0(sec)Figure 10 . CNN output on a track beginning with a pi-\nano melody (0 - 10 seconds) and continuing with a clarinet\nmelody (10 - 25 seconds). (left) CNN model melody out-\nput in red against the ground truth in back. (right) CNN\nmelody salience output.\nof the mistakes made by the CNN method revealed that\nthe vast majority incorrect melody estimates occurred for\nmelodies played by under-represented melody instrument\nclasses in the training set, such as piano and guitar. For\nexample, Figure 10 shows the output of the CNN model\nfor an excerpt beginning with a piano melody and contin-\nuing with a clarinet melody. Clarinet is well represented\nin our training set and the model is able to retrieve most\nof the clarinet melody, while virtually none of the piano\nmelody is retrieved. Looking at the salience output (Fig-\nure 10 right), there is very little energy in the early region\nwhere the piano melody is active. This could be a result\nof the model not being exposed to enough examples of the\npiano timbre to activate in those regions. Alternatively, in\nmelody salience scenario, the model is trained to suppress\n“accompaniment” and emphasize melody. Piano is often\nplaying accompaniment in the training set, and the model\nmay not have enough information to untangle when a pi-\nano timbre should be emphasized as part of the melody and\nwhen it should be suppressed as accompaniment. We note\nthat while in this qualitative example the errors could be\nattributed to the pitch height, we observed that this was not\na consistent factor in other examples.\n6. CONCLUSIONS\nIn this paper we presented a model for learning a salience\nrepresentation for multi- f0tracking and melody extraction\nusing a fully convolutional neural network. We demon-\nstrated that simple decoding of both of these salience repre-\nsentations yields state-of-the art results for multi- f0track-\ning and melody extraction. Given a sufﬁcient amount of\ntraining data, this architecture would also be useful for re-\nlated tasks including bass, piano, and guitar transcription.\nIn order to further improve the performance of our sys-\ntem, data augmentation can be used to both diversify our\ntraining set and to balance the class distribution (e.g. in-\nclude more piano and guitar). The training set could fur-\nther be augmented by training on a large set of weakly-\nlabeled data such as the Lakh-midi dataset [24]. In addition\nto augmentation, there is a wide space of model architec-\ntures that can be explored to add more temporal informa-\ntion, such as recurrent neural networks.68 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20177. REFERENCES\n[1] Jakob Abeßer, Stefan Balke, Klaus Frieler, Martin\nPﬂeiderer, and Meinard M ¨uller. Deep learning for jazz\nwalking bass transcription. In AES International Con-\nference on Semantic Audio , 2017.\n[2] Stefan Balke, Christian Dittmar, Jakob Abeßer, and\nMeinard M ¨uller. Data-driven solo voice enhancement\nfor jazz music retrieval. In ICASSP , Mar. 2017.\n[3] Emmanouil Benetos and Tillman Weyde. An efﬁ-\ncient temporally-constrained probabilistic model for\nmultiple-instrument music transcription. In ISMIR ,\npages 701–707, 2015.\n[4] Rachel M Bittner, Justin Salamon, Slim Essid, and\nJuan P Bello. Melody extraction by contour classiﬁ-\ncation. In ISMIR , October 2015.\n[5] Rachel M Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan P. Bello.\nMedleyDB: A multitrack dataset for annotation-\nintensive MIR research. In ISMIR , October 2014.\n[6] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nJoint beat and downbeat tracking with recurrent neural\nnetworks. In Proc. of the 17th Int. Society for Music\nInformation Retrieval Conf.(ISMIR) , 2016.\n[7] Sebastian B ¨ock and Markus Schedl. Polyphonic pi-\nano note transcription with recurrent neural networks.\nInAcoustics, speech and signal processing (ICASSP),\n2012 ieee international conference on , pages 121–124.\nIEEE, 2012.\n[8] Juan Jos ´e Bosch, Rachel M Bittner, Justin Salamon,\nand Emilia G ´omez. A comparison of melody extrac-\ntion methods based on source-ﬁlter modeling. In IS-\nMIR, pages 571–577, New York, August 2016.\n[9] Pablo Cancela, Ernesto L ´opez, and Mart ´ın Rocamora.\nFan chirp transform for music representation. In DAFx ,\n2010.\n[10] Jonathan Driedger and Meinard M ¨uller. Extracting\nsinging voice from music recordings by cascading au-\ndio decomposition techniques. In Acoustics, Speech\nand Signal Processing (ICASSP), 2015 IEEE Interna-\ntional Conference on , pages 126–130. IEEE, 2015.\n[11] Zhiyao Duan, Bryan Pardo, and Changshui Zhang.\nMultiple fundamental frequency estimation by model-\ning spectral peaks and non-peak regions. IEEE TASLP ,\n18(8):2121–2133, 2010.\n[12] Jean-Louis Durrieu, Bertran David, and Gael Richard.\nA musically motivated mid-level representation for\npitch estimation and musical audio source separation.\nIEEE J. on Selected Topics on Signal Processing ,\n5(6):1180–1191, Oct. 2011.\n[13] Derry Fitzgerald. Harmonic/percussive separation us-\ning median ﬁltering. 2010.[14] Kun Han and DeLiang Wang. Neural network based\npitch tracking in very noisy speech. IEEE/ACM Trans-\nactions on Audio, Speech and Language Processing\n(TASLP) , 22(12):2158–2168, 2014.\n[15] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[16] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[17] Anssi Klapuri. Multiple fundamental frequency esti-\nmation based on harmonicity and spectral smoothness.\nIEEE TASLP , 11(6):804–816, Nov. 2003.\n[18] Yuzhou Liu and DeLiang Wang. Speaker-dependent\nmultipitch tracking using deep neural networks.\nThe Journal of the Acoustical Society of America ,\n141(2):710–721, 2017.\n[19] Matthias Mauch and Simon Dixon. Approximate note\ntranscription for the improved identiﬁcation of difﬁcult\nchords. In ISMIR , pages 135–140, 2010.\n[20] Matthias Mauch and Simon Dixon. PYIN: a Fun-\ndamental Frequency Estimator Using Probabilistic\nThreshold Distributions. In ICASSP , pages 659–663.\nIEEE, 2014.\n[21] Brian McFee, Matt McVicar, Oriol Nieto, Stefan\nBalke, Carl Thome, Dawen Liang, Eric Battenberg,\nJosh Moore, Rachel Bittner, Ryuichi Yamamoto, and\net al. librosa 0.5.0, Feb 2017.\n[22] Graham E. Poliner and Dan PW Ellis. A classiﬁca-\ntion approach to melody transcription. In ISMIR , pages\n161–166, London, Sep. 2005.\n[23] Graham E Poliner and Daniel PW Ellis. A dis-\ncriminative model for polyphonic piano transcrip-\ntion. EURASIP Journal on Applied Signal Processing ,\n2007(1):154–154, 2007.\n[24] Colin Raffel. Learning-Based Methods for Comparing\nSequences, with Applications to Audio-to-MIDI Align-\nment and Matching . PhD thesis, COLUMBIA UNI-\nVERSITY , 2016.\n[25] Franc ¸ois Rigaud and Mathieu Radenen. Singing voice\nmelody transcription using deep neural networks. In\nISMIR , pages 737–743, 2016.\n[26] Matti Ryyn ¨anen and Anssi Klapuri. Automatic tran-\nscription of melody, bass line, and chords in poly-\nphonic music. Computer Music J. , 32(3):72–86, 2008.\n[27] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. IEEE TASLP , 20(6):1759–1770, Aug.\n2012.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 69[28] Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\npiano music transcription. IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) ,\n24(5):927–939, 2016.\n[29] Li Su and Yi-Hsuan Yang. Combining spectral and\ntemporal representations for multipitch estimation\nof polyphonic music. IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) ,\n23(10):1600–1612, 2015.\n[30] Li Su and Yi-Hsuan Yang. Escaping from the abyss\nof manual annotation: New methodology of building\npolyphonic datasets for automatic music transcription.\nInInternational Symposium on Computer Music Multi-\ndisciplinary Research , pages 309–321. Springer, 2015.\n[31] Karen Ullrich, Jan Schl ¨uter, and Thomas Grill. Bound-\nary detection in music structure analysis using con-\nvolutional neural networks. In ISMIR , pages 417–422,\n2014.\n[32] Emmanuel Vincent, Nancy Bertin, and Roland Badeau.\nAdaptive harmonic spectral decomposition for mul-\ntiple pitch estimation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 18(3):528–537,\n2010.70 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Quantifying Music Trends and Facts Using Editorial Metadata from the Discogs Database.",
        "author": [
            "Dmitry Bogdanov",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416376",
        "url": "https://doi.org/10.5281/zenodo.1416376",
        "ee": "https://zenodo.org/records/1416376/files/BogdanovS17.pdf",
        "abstract": "While a vast amount of editorial metadata is being actively gathered and used by music collectors and enthusiasts, it is often neglected by music information retrieval and mu- sicology researchers. In this paper we propose to explore Discogs, one of the largest databases of such data available in the public domain. Our main goal is to show how large- scale analysis of its editorial metadata can raise questions and serve as a tool for musicological research on a number of example studies. The metadata that we use describes music releases, such as albums or EPs. It includes infor- mation about artists, tracks and their durations, genre and style, format (such as vinyl, CD, or digital files), year and country of each release. Using this data we study correla- tions between different genre and style labels, assess their specificity and analyze typical track durations. We esti- mate trends in prevalence of different genres, styles, and formats across different time periods. In our analysis of styles we use electronic music as an example. Our contri- bution also includes the tools we developed for our analy- sis and the generated datasets that can be re-used by MIR researchers and musicologists.",
        "zenodo_id": 1416376,
        "dblp_key": "conf/ismir/BogdanovS17",
        "keywords": [
            "editorial metadata",
            "Discogs",
            "public domain",
            "musicological research",
            "example studies",
            "genre labels",
            "track durations",
            "trends",
            "electronic music",
            "MIR researchers"
        ],
        "content": "QUANTIFYING MUSIC TRENDS AND FACTS USING EDITORIAL\nMETADATA FROM THE DISCOGS DATABASE\nDmitry Bogdanov, Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra\ndmitry.bogdanov@upf.edu, xavier.serra@upf.edu\nABSTRACT\nWhile a vast amount of editorial metadata is being actively\ngathered and used by music collectors and enthusiasts, it\nis often neglected by music information retrieval and mu-\nsicology researchers. In this paper we propose to explore\nDiscogs, one of the largest databases of such data available\nin the public domain. Our main goal is to show how large-\nscale analysis of its editorial metadata can raise questions\nand serve as a tool for musicological research on a number\nof example studies. The metadata that we use describes\nmusic releases, such as albums or EPs. It includes infor-\nmation about artists, tracks and their durations, genre and\nstyle, format (such as vinyl, CD, or digital ﬁles), year and\ncountry of each release. Using this data we study correla-\ntions between different genre and style labels, assess their\nspeciﬁcity and analyze typical track durations. We esti-\nmate trends in prevalence of different genres, styles, and\nformats across different time periods. In our analysis of\nstyles we use electronic music as an example. Our contri-\nbution also includes the tools we developed for our analy-\nsis and the generated datasets that can be re-used by MIR\nresearchers and musicologists.\n1. INTRODUCTION\nIn this paper we propose to explore the editorial meta-\ndata available in the Discogs1database and show how its\nanalysis can be used as a potential tool to support musico-\nlogical studies and research in music information retrieval\n(MIR). Discogs is one of the largest online databases of\neditorial metadata2used by music collectors and enthusi-\nasts. It hosts all metadata under Public Domain license and\nprovides complete monthly data dumps3which makes it\nvery easy to access and re-use the data.\n1https://discogs.com\n2Discogs mission statement is “to build the biggest and most compre-\nhensive music database and marketplace”.\n3https://data.discogs.com/\nc\rDmitry Bogdanov, Xavier Serra. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Dmitry Bogdanov, Xavier Serra. “Quantifying music trends\nand facts using editorial metadata from the Discogs database”, 18th In-\nternational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.Discogs metadata contains information about music re-\nleases (such as albums or EPs) including artists name, track\nlist including track durations, genre and style, format (e.g.,\nvinyl or CD), year and country of release. It also con-\ntains information about artist roles and relations as well\nas recording companies and labels. The quality of the data\nin Discogs is considered to be high among music collec-\ntors because of its strict guidelines, moderation system and\na large community of involved enthusiasts. The database\ncontains contributions by more than 347,000 people. It\ncontains 8.4 million releases by 5 million artists covering a\nwide range of genres and styles (although the database was\ninitially focused on Electronic music).\nRemarkably, it is the largest open database containing\nexplicit crowd-sourced genre annotations. Discogs im-\nplements a two-level genre hierarchy including top-level\nbroad genres and more speciﬁc sub-genres called styles .4\nThis taxonomy is determined by the community modera-\ntors and there are guidelines for annotation. Genre/style\nlabels are non-exclusive, meaning that a release can be an-\nnotated by multiple genres and styles. Releases almost\nalways contain both genre- and style-level annotations as\nboth are required by the submission system of the database.\nTo our surprise, little is known about Discogs metadata\namong the MIR community and there is a lack of musi-\ncological studies using this data. We identiﬁed two MIR\nstudies, using this data in the past. One study used it for\nmusic recommendation by building and comparing artist\ntag-cloud proﬁles including genres, styles, record labels,\nyears, and countries associated with each artist [3]. An-\nother study analyzes graphs of artist collaborations in or-\nder to identify music clusters that can then be associated\nwith genres [5].\nThere has been done some MIR research in the context\nof mining genre annotations using other music databases,\nincluding AllMusic [14], Wikipedia [16] and Last.fm [15],\nhowever, all these studies are limited in their scope and\ntheir use-cases of the data. Furthermore, the employed\ndata sources are either ill-structured (Wikipedia, Last.fm),\nmiss explicit genre information (Last.fm), or contain pro-\nprietary data (AllMusic) which cannot be reused on the\nlarge scale. In order to address these issues we are cur-\nrently creating a new genre metadata corpus for MIR tasks,\nincluding Discogs metadata [10].\n4https://reference.discogslabs.com/wiki/\nStyle-Guide89In this paper we propose new ways of how editorial\nmetadata can be used in MIR, speciﬁcally in the context\nof studies directed to assist musicological research. To\nthis end, we present a number of example studies using the\ndata available in Discogs in which we identify and quantify\nsome music trends and facts, some of which were previ-\nously documented by musicologists and music journalists.\nIn these examples we consider overall trends across broad\ngenres together with a more speciﬁc analysis for the case of\nelectronic music which commonly lacks attention of the re-\nsearch community.5Our analysis is exploratory and is not\naimed at concrete musicological conclusions. Instead we\npresent how data can be used to identify interesting facts\nand raise questions for further research. Importantly, our\nstudy is focused exclusively on music recorded and pub-\nlished on physical or digital media in form of collectable\nartifacts. We share the tools we developed for the analysis,\npreprocessed datasets and the complete results for future\nuse by the MIR community and musicologists.5\n2. DISCOGS DATASET\nWe downloaded the data dump of music releases dated\nApril 2017. Each release contains the information about\na speciﬁc album, single, EP, or compilation.6For our ex-\nploratory study we are interested in artist name, tracklist\nincluding track durations, genre and style, format, year and\ncountry of each release.\nAs we wanted to perform per-year analysis of the data,\nwe decided to discard all releases from the ongoing year\n2017 to avoid any bias due to incompleteness of data.7\nWe extracted the desired data from the dump. The result-\ning dataset includes 7,954,870 releases by 1,290,943 artists\nwith the total of 67,895,500 released tracks. All releases\nare annotated by 11 genres and 442 styles.8Approxi-\nmately half of the releases (52%) are annotated with track\ndurations, the 11% of releases are annotated as compila-\ntions, and around 1% are marked as mixed.\nWe estimated the overall genre coverage in terms of\npercentage of releases, and tracks and artists associated\nwith those releases, out of their total number in the dataset\n(Figure 1). The rationale behind counting track numbers is\nthat many releases in the databases are vinyl EPs and sin-\ngles with a smaller number of tracks than albums or compi-\nlations. The number of artists provides an alternative use-\nful estimate, as artists may vary in their “release produc-\ntivity”. Our inspection revealed the predominance of the\nRock, Electronic, and Pop genre categories in the database,\n5We refer the reader to the additional materials online including\ncode and the detailed analysis results: https://github.com/\ndbogdanov/ismir2017-discogs\n6See an example of a release page: https://www.discogs.\ncom/LFO-Frequencies/release/3649\n7We can also suspect that the database is still missing some releases\nfor 2016 as there was possibly not enough time to gather contributions\nfrom the Discogs community. Still, we decided to keep those releases.\n8For simplicity, we ignored Brass & Military ,Children’s ,Non-Music ,\nandStage & Screen genre categories present in the Discogs taxonomy due\nto being less represented and/or not being strictly related to music. See the\ncomplete genre taxonomy at https://github.com/dbogdanov/\nismir2017-discogs/tree/master/taxonomy .\nFigure 1 : Genre coverage (%).\nFigure 2 : Number of releases per country by year.\nwith the largest styles being Pop Rock and House. Still,\neven the least represented genres (such as Blues) have al-\nmost 200,000 releases. Around 90% and 74% of styles\nhave more than 1000 and 10,000 releases, respectively.\nInspecting country distribution for releases, we can see\nan overall predominance of music from western countries\nin the dataset. Top countries included US, UK, Germany,\nFrance, Italy, Japan and Netherlands. Figure 2 presents an\nexample of total number of releases published in various\ncountries by year. Our observations lead us to an open\nquestion of whether the disbalance in the distribution rep-\nresents the actual evolution of the recording industry in\neach country, or that the Discogs database has insufﬁcient\ncoverage of music from some countries/cultures. While\nboth reasons are plausible, we suppose that our data is still\nvalid for research focused on western countries and music.\n3. EXAMPLE STUDIES\n3.1 Average Track Duration\nIn our ﬁrst example study we analyzed the distribution of\nthe duration of tracks9across different genres and styles\nand its evolution in time. To this end, we discarded all\nmixed compilations that contain only fragments of tracks\ninstead of full recordings. As there are music releases an-\nnotated by multiple genres, we computed tracks duration\nstatistics twice: ﬁrst, including all releases annotated by a\nparticular genre, and second, excluding releases that were\nalso annotated by other genres.\nFigure 3 presents box plots with the obtained results.\nIn both cases, Classical, Electronic, and Jazz music has\nthe largest median durations and the largest variability, ac-\n9Note that we are considering duration of tracks on a recording\nmedium, not of the original music pieces. The former can be seen as\na proxy for the latter, at least for some of the music styles.90 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017(a) all releases\n(b) releases annotated by a single genre\nFigure 3 : Boxplot of track durations (mins.) for each\ngenre. Number of computed tracks is given in brackets.\nWhiskers are set at 5% and 95%.\ncording to the interquartile range. We observed similar re-\nsults when including and excluding releases with multiple\ngenre annotations. Still there are some variations in me-\ndian values, quartiles and whiskers positions, most notably\nfor Classical, Electronic and Jazz. This suggests that music\nannotated by these genres on the crossover with other gen-\nres tends to be shorter. The duration of some music tracks\nby these genres reached over 8 minutes in contrast to other\ngenres that were far shorter.10\nSimilarly, we analyzed all styles present in the Discogs\ntaxonomy.11Overall comparison suggests that the median\nduration for the majority of styles is below 10 minutes, the\nshortest starting at 2 to 2.5 minutes (e.g., Grindcore, Crust,\nSurf, Doo Wop, Beat and Rockabilly). We can observe\nthat some styles can be associated with higher variability\nin the duration of tracks, while others have durations con-\ndensed around a common value. For example, in the case\nof Electronic music, we can identify a cluster of styles with\na large variability (including Harsh Noise Wall, Drone,\nNoise, Musique-Concrete, Berlin-School, and Dark Am-\nbient) with the duration of tracks surpassing 15 minutes.\nAll these styles share a unconventional, or experimental,\napproach to sound and music composition. In contrast, the\nElectronic styles with the lowest variability (such as Euro-\ndance, Jumpstyle, or Grime, among many others) are com-\nmonly related to dance/club music.\nAs a next step, we checked whether there is a global\ntrend in change of average track duration across years. We\ncomputed per-genre distributions of track durations across\nyears, some of them shown in Figure 4.12We observe ex-\nistence of a time period with a clear tendency of increase in\nalmost all genre categories: Blues (early 60s to early 70s),\nFolk, World & Country (mid-60s to 00s), Funk/Soul (early\n60s to late 70s), Jazz (late 60s to late 70s), Latin (mid-60s\nto mid-00s), Pop (mid-60s to mid-80s), Rock (dramatic in-\ncrease in mid-60s to early 70s) and Electronic (early 70s\nto 2010 with a consequent decrease). It appears that all\n10Of course, this only suggests a general trend excluding outliers.\n11See results for all styles grouped per genre in additional materials.\n12In the case of Electronic and Classical, large value jumps in early\nyears may be associated to small amount of tracks annotated by duration.\n(a) Pop\n (b) Electronic\n(c) Hip Hop\n (d) Classical\nFigure 4 : Evolution of duration of Rock, Electronic, Hip\nHop, and Classical tracks (mins.) by year.\nthese genres have reached a plateau in median track dura-\ntion after a period of stable increase. We can also see an\nincrement in variability of durations with time.\nThe increase in duration may be associated with the\nchange of record formats, which we propose to assess in\nfuture studies. Interestingly no such tendency was found\nfor Classical music. In contrast, while median duration re-\nmains constant, we can observe a decrease in variation with\nthe longer half of the tracks getting shorter. Furthermore,\nHip Hop tracks are steadily decreasing in duration since\nthe origin of the genre in the late 70s. All these ﬁndings\ncan raise questions for further musicological research.\n3.2 Release Formats\nThere is a large number of release formats registered in the\nDiscogs database.13In our next example study we quan-\ntify the evolution of the most common formats of the past\nhalf-century: vinyl, cassettes, CD/CDr, and ﬁles.14We\ncompare the amount of music released on each of these for-\nmats from the 1960s to nowadays. To this end, we count\nthe overall number of releases and tracks recorded on a\nparticular medium. We can then compute the percentage\nof music released on each format each year. Track and re-\nlease percentages can vary signiﬁcantly due to the capac-\nity of each medium: typically CD releases contain a larger\nnumber of tracks then vinyl. Figures 5a, 5b and 5c present\ntrack statistics for all music in the dataset, and for Blues\nand Electronic genres in particular.15\nOverall, our analysis corroborates existing RIAA sales\nreports [12]. We can evidence the commonly known rapid\ngrowth of CD format from the mid-80s to the late 90s, fol-\nlowed by a plateau period till the mid-00s. The following\ndecline of CD can be clearly associated with the growth\nof digital ﬁle formats. Remarkably, vinyl, following its de-\n13https://www.discogs.com/help/formatslist\n14Note that we cannot be conﬁdent in estimations for the ﬁle format as\ndigital releases may be under-represented in the Discogs database.\n15See additional materials for complete results.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 91(a) All music\n (b) Blues\n(c) Electronic\n (d) Synth-pop (Electronic)\n(e) Experimental (Electronic)\n (f) Deep House (Electronic)\nFigure 5 : Percentage of released tracks per format by year\n(%).\ncline since the mid-80s, is now growing since the early 10s,\nwhich corroborates recent observations of the new “vinyl\nboom” [2].\nCassette releases appear to be always below vinyl or\nCD releases through the history of the format, descend-\ning to its supposed death in the mid-00s. However, since\nthen, we can observe the second growth of cassettes, which\nconﬁrms observations of the growing “cassette culture” by\nsome music journalists and musicologists [8,7,6]. Interest-\ningly, there is a considerable amount of music released on\na CDr format, which appeared in the late 90s and achieved\nits maximum at the time of the death of cassettes. We\ncan suppose that the observed CDr and cassette trends are\nlinked to the DIY culture of independent music distribu-\ntion [18].\nAnalyzing particular genres, we identiﬁed Blues, Rock,\nReggae, and Funk/Soul to have the highest percentage of\ntracks released on vinyl in the recent years, surpassing\n30%. Remarkably, these genres can be nowadays consid-\nered somewhat “old-school”, and therefore of a potentially\nhigher interest among vinyl music collectors. We hypoth-\nesize that many of new vinyl releases are reissues.\nFinally, we ran per-style analysis of data, on the exam-\nple of Electronic music, in order to identify peculiarities\nof music distribution within certain styles. Figures 5d, 5e\nand 5f demonstrate differences in formats on the example\nof Synth-pop, Ambient and Deep House. From our results\nwe can evidence a transition to the predominance of CDs\nfor all considered styles with the turning points16start-\n16The year of an equal number of tracks released on both formats.ing since 1988. The styles that moved to CD ﬁrst were\nAmbient, Synth-pop and Experimental, with their turning\npoints in transition between 1988 and 1991. We specu-\nlate that early transition to CD was at least partially moti-\nvated by the demand of home consumers, meanwhile other\nstyles, supported by DJs, had a technical demand for vinyl.\nSuch styles had their transition point to CD later between\n1993 (e.g. House, Techno) to 1996-97 (Drum n Bass, Deep\nHouse). Remarkably, we identiﬁed the existence of styles\nwith the absolute predominance of digital formats (CD,\nCDr and ﬁle). In the case of Glitch, this fact may be linked\nto musical characteristics of the style, which make releases\non vinyl/cassettes aesthetically or technically unfeasible.\nIn 2016 the digital ﬁle format is leading in almost all\nstyles. The turning point towards its predominance ap-\npeared between 2007 and 2011. Still, we can observe a\ntrend in growth of vinyl in the recent 6 years. Moreover we\ncan also see the growth of cassettes since 2005, with the\nmost signiﬁcant example being Experimental electronic\nmusic (reaching almost 20% of tracks being released on\ncassettes in 2016). Interestingly, almost 30% of released\nAmbient tracks in the early-to-late 80s, and similarly over\n40% of Experimental from the early 80s to the early 90s,\nwere released on cassettes (which again supports the exis-\ntence of the DIY cassette scene in experimental music of\nthe 80s) [17].\n3.3 Genre and Style Trends\nIn this section we present another use-case example: an\nanalysis of genre and style trends across time periods. We\nconsider overall trends in genres and exemplify style anal-\nysis on Electronic music. Again, we quantiﬁed the amount\nof music in terms of number of releases, tracks and artists\nassociated with those releases. Their absolute values (the\namount of music in year Nby genre G) and proportions\n(the percentage of music from year Nby genre G) allow us\nto suggest possible trends. Figure 6 presents results of the\nanalysis.17Below we summarize our observations on the\ntracks level.\nRock appears to be the major genre since the late 60s\ncovering more then 40% of released tracks since the late\n70s and reaching 50% nowadays. It is currently followed\nby Electronic music that reached its peak at 38% in 2011.\nPop music is in a steady decline since its short dominance\nin the mid-60s, falling below Electronic since the early 90s.\nWe can also see the decline of Jazz after its huge 50% peak\nin the mid-50s, being a predominant genre at that time, the\nrise and fall of Funk/Soul, and the growth of Hip Hop being\nthe 4th leading genre in the 2000s. A valley in the Rock\nplot in the mid-70s corresponds to the peak of Funk/Soul\n(including “the disco boom” [9]).18\nWe then repeated the analysis for styles of Elec-\ntronic music. Again, we used absolute and relative re-\nlease/track/artist counts. The relative values represent the\n17Note that the percentage values do not sum up to 100 because releases\ncan be annotated by multiple genres.\n18This is especially well seen on the release and artist-level plots, with\nthe number of Funk/Soul artists being temporarily higher than for Rock.92 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017(a) Tracks (all genres)\n (b) Releases (all genres)\n (c) Tracks (Electronic)\nFigure 6 : Percentage of released music per genre and style by year (%).\npercentage of music within a style out of the total amount\nof Electronic music released each year. Out of 110 Elec-\ntronic styles, we identiﬁed the most important ones in\nterms of their overall presence: we computed their rela-\ntive share in all Electronic music across years from 1970\nto 2016. We then summed these values for each style and\nidentiﬁed the styles with the highest values. Figure 6c\npresents track-level results for those styles.19\nAccording to our data, we can see how electronic Disco\npeaked in the late 70s followed by the Synth-pop peak in\nthe mid-80s, both being the predominant styles of their\ntime. The decline of Synth-pop in the late 80s/early 90s\nmet the peak of House (1990) and the ﬁrst peak of Techno\n(1992). Later, growing styles included Trance (peaked in\n2000), and Electro (having its second peak in the late 00s,\nthe ﬁrst one in 1984). We can also guess the period of birth\nof each genre using our data (e.g., house in the mid-80s,\ntechno in the late 80s, and trance in the early 90s). All of\nthese observations seem to be well-aligned with the exist-\ning literature on the history of electronic music [11].\nInterestingly, after the year 2010 we observe lower per-\ncentage values for all styles. This suggests the diversiﬁca-\ntion of electronic music: more styles are taking a share in\nthe amount of music released each year. Finally, it is worth\nnoting that similar analyses can be run on per-country ba-\nsis. This can be useful for identifying potential regional\ntrends or analyzing the following of a particular style in\nvarious countries.20\n3.4 Genre and Style Co-occurrences\nIn this section, we consider another use-case for edito-\nrial metadata with genre/style annotations and study co-\noccurrences between different genre and style labels. We\nalso attempt to assess the speciﬁcity of labels: while top-\nlevel genre categories are very broad, styles may vary a lot\nin their speciﬁcity and coverage.\nGiven that releases can be annotated by multiple\ngenre/style labels within the Discogs taxonomy, we com-\nputed a genre (and style) co-occurrences matrix in order\nto identify possible relations. For each pair of genres (or\nstyles) X and Y we counted the number times both appear\non the same release across all releases in the dataset. The\n19See additional materials for full results.\n20For example, we could observe that Hardcore, Breakbeat and Drum\nn Bass styles, well-represented in UK in the early-to-mid 90s, were never\nprevailing in Germany.\nFigure 7 : Genre co-occurrences (%).\nFigure 8 : Electronic styles co-occurrences (%).\nresulting matrix is asymmetric and its values represent the\npercentage of music by genre (style) X (on the x-axis) also\nbeing annotated by genre (style) Y (on the y-axis).\nFigure 7 presents the resulting genre co-occurrences\nmatrix computed using all our data. We can conclude from\nit that Classical, Electronic, and Reggae seem to be the\ngenres that are well isolated from others, that is, the mu-\nsic under these genre labels is self-contained and all co-\noccurrences with other genres are relatively small (below\n11%). On the contrary, the most interconnected genres are\nBlues (46% of it is also Rock), Pop (33% is Rock) and\nHip-Hop (24% is Electronic). We can also observe how\nPop and Rock, and Electronic appear to be commonly co-\noccurring genres for many genres, probably, due to being\nthe most popular ones.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 93Figure 9 : Percentage of House releases also annotated by\nother styles by year (%).\nWe proceeded with style co-occurrences in a similar\nmanner. The resulting matrix is huge and here we analyze\na portion, again, for the case of Electronic music.21To\ngive an idea, Figure 8 provides an overview of this matrix\ncontaining 110 Electronic styles. As we can see, there are\na number of traceable vertical lines corresponding to par-\nticular styles that often co-occur with many other styles.\nThose include Techno, House, Experimental, Synth-pop\nand Ambient — all being among the most frequent styles\nwithin Electronic music in our dataset. Less predominant,\nbut still traceable vertical lines include Electro and Down-\ntempo. We can suppose that these style labels are wide in\ncoverage or generic enough to embrace some other styles.\nIn contrast, we can also see styles that typically do not co-\noccur with others (e.g., Speed Garage or New Beat), which\nmight indicate their high speciﬁcity or “nicheness”.\nSumming all row values for a column we can get a\n“genericness-vs-nicheness” score. According to it, some\nexamples of niche styles are Beatdown, Neo Trance,\nSkweee, New Disco, and Italo House, while the most\ngeneric styles include Downtempo, Synth-pop, Electro,\nAmbient, Techno, Experimental, and House. On the other\nside, co-occurrence values above 50% might indicate a\nsubgenre-to-genre relation and give us a degree of potential\n“sub-styleness”, or “derivativeness”. For example, 86% of\nGabber is also annotated as Hardcore, 75% of Hardbeat as\nNew Beat, and 68% of Power Electronics as Noise. After\nidentifying all such examples in the matrix, we were able\nto corroborate this hypothesis.\nInterestingly, it is also possible to compute co-\noccurrences for particular time epochs. Figure 9 illustrates\nthe idea of evolution of style co-occurrences in time on\nthe example of House. We can see how Disco, Synth-pop,\nFunk and Soul potentially had an inﬂuence on the style at\nthe time of its origin (indeed these styles are often cited\nas such [11]), followed by a peak of Acid-House and then,\nlater, Electro and Tech-House.\nThe co-occurrence matrices demonstrate the intrinsic\nvariability in genre annotations and we believe that such\ndata can be very useful for the MIR community in the\ncontext of evaluating music genre classiﬁers and for other\ntasks. Indeed, some studies on audio-based genre classi-\nﬁcation (such as [13, 1, 4]) reveal similar patterns in mis-\nclassiﬁcations, and they can be supported by our data.\n21See additional materials for full results.4. DISCUSSION\nNaturally, the presented data analysis is limited by the cov-\nerage of the Discogs database, with a possible bias towards\nWestern music and collectable music items, and other so-\nciocultural factors. Digital releases are possibly underrep-\nresented since the new online distribution models allow\nartists to instantly share their work and the concept of “re-\nlease” might be changing. We are far more conﬁdent in the\ndata for the former time period of predominance of phys-\nical releases. Assessment of coverage of editorial music\ndatabases is an open question for future research.\nOur analysis is essentially grounded on the statistics of\nmusic production, not consumption. No analysis of music\ntrends in terms of popularity among listeners is addressed.\nInstead, we deliberately focused on another aspect: what\nmusic artists tend to produce, including in the long tail.\nA release of 100 copies is treated equally to a release of\nthousands in our analysis.\nWe are also aware of the problem of release-level\ngenre/style annotations: labels do not necessarily apply to\nall tracks. Still, we suppose a certain congruency between\ntracks on a release. Interestingly, our data reveals that even\nreleases with a single track have multiple labels (1.2 genres\nand styles on average). This suggests that a genre annota-\ntion problem is inherently multi-label. Finally, in our anal-\nysis we are limited to the Discogs’ genre taxonomy. Their\nbroad genre categories might not be appropriate for some\nresearch tasks, but we can be much more conﬁdent in style\nannotations, at least for some genres.\nMany music releases actually correspond to the same\nconceptual items (e.g., album CD version, CD version in\nanother country, vinyl version, reissue). Discogs provides\ninformation about their groupings, and it should be con-\nsidered depending on a task at hand. For simplicity, in our\nexample studies we treat such releases as if they were in-\ndependent. Such releases are often released in various for-\nmats, countries, years, and can have different track lengths,\nbringing additional information to our analysis.\n5. CONCLUSIONS\nIn this paper we propose to take a closer look on the ed-\nitorial metadata in the Discogs database. We believe that\nanalysis of this data can be a valuable tool for researchers.\nIt can help to identify and analyze various musical phe-\nnomena and raise different musicological questions. Im-\nportantly, Discogs is one of the largest sources for such\ndata in the public domain which allows to address potential\nresearch questions on a very large scale. We demonstrated\nthe use of this data in a number of example studies in which\nwe attempted to quantify a number of music trends and\nfacts, some previously documented by musicologists and\nmusic journalists. Our examples are far from being com-\nplete and of course there are more potential questions to be\nraised and addressed using this data. We share the analy-\nsis tools we developed, our preprocessed datasets and the\ncomplete results for our example studies for further re-use\nby MIR and musicology researchers.94 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. ACKNOWLEDGEMENTS\nThis research has received funding from the European\nUnions Horizon 2020 research and innovation programme\nunder grant agreement No 688382 (AudioCommons). We\nalso thank Alastair Porter for proofreading.\n7. REFERENCES\n[1] B. K. Baniya and J. Lee. Importance of audio fea-\nture reduction in automatic music genre classiﬁcation.\nMultimedia Tools and Applications , 75(6):3013–3026,\n2016.\n[2] D. Bartmanski and I. Woodward. The vinyl: The ana-\nlogue medium in the age of digital reproduction. Jour-\nnal of Consumer Culture , 15(1):3–27, 2015.\n[3] D. Bogdanov and P. Herrera. Taking Advantage of\nEditorial Metadata to Recommend Music. In Interna-\ntional Symposium on Computer Music Modeling and\nRetrieval (CMMR’12) , 2012.\n[4] D. Bogdanov, A. Porter, P. Herrera, and X. Serra.\nCross-collection evaluation for music classiﬁcation\ntasks. In International Society for Music Information\nRetrieval Conference (ISMIR’16) , 2016.\n[5] J. Burke, R. Rygaard, and Z. Yellin-Flaherty. Clam!:\nInferring genres in the Discogs collaboration network.\n2014.\n[6] J. Demers. Cassette tape revival as creative anachro-\nnism. Twentieth-Century Music , 14(1):109–117, 2017.\n[7] C. Eley. Technostalgia and the resurgence of cas-\nsette culture. The Politics of Post-9/11 Music: Sound,\nTrauma, and the Music Industry in the Time of Terror ,\npages 43–55, 2011.\n[8] M. Hogan. This is not a mixtape. http:\n//pitchfork.com/features/article/\n7764-this-is-not-a-mixtape/ , 2010.\nAccessed on 13.07.2017.\n[9] T. Lawrence. Love saves the day: A history of Ameri-\ncan dance music culture, 1970–1979 . Duke University\nPress, 2004.\n[10] A. Porter, D. Bogdanov, and X. Serra. Mining meta-\ndata from the web for AcousticBrainz. In Interna-\ntional workshop on Digital Libraries for Musicology\n(DLfM’16) , pages 53–56. ACM, 2016.\n[11] S. Reynolds. Energy ﬂash: A journey through rave mu-\nsic and dance culture . Soft Skull Press, 2012.\n[12] RIAA. U.S. sales database. https://www.riaa.\ncom/u-s-sales-database , 2017. Accessed on\n13.07.2017.\n[13] N. Scaringella, G. Zoia, and D. Mlynek. Automatic\ngenre classiﬁcation of music content: a survey. IEEE\nSignal Processing Magazine , 23(2):133–141, 2006.[14] A. Schindler, R. Mayer, and A. Rauber. Facilitating\ncomprehensive benchmarking experiments on the mil-\nlion song dataset. In International Society for Music\nInformation Retrieval Conference (ISMIR’12) , 2012.\n[15] H. Schreiber. Improving genre annotations for the Mil-\nlion Song Dataset. In International Society for Music\nInformation Retrieval Conference (ISMIR’15) , 2015.\n[16] H. Schreiber. Genre ontology learning: Comparing cu-\nrated with crowd-sourced ontologies. In International\nSociety for Music Information Retrieval Conference\n(ISMIR’16) , 2016.\n[17] J. Scott. The Noise-Arch archive. https:\n//archive.org/details/noise-arch ,\n2015. Accessed on 13.07.2017.\n[18] R. Strachan. Do-It-Yourself: Industry, ideology, aes-\nthetics and micro independent record labels in the UK.\nUnpublished masters thesis, University of Liverpool,\nLiverpool, England , 2003.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 95"
    },
    {
        "title": "End-to-End Optical Music Recognition Using Neural Networks.",
        "author": [
            "Jorge Calvo-Zaragoza",
            "Jose J. Valero-Mas",
            "Antonio Pertusa"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1418333",
        "url": "https://doi.org/10.5281/zenodo.1418333",
        "ee": "https://zenodo.org/records/1418333/files/Calvo-ZaragozaV17.pdf",
        "abstract": "This work addresses the Optical Music Recognition (OMR) task in an end-to-end fashion using neural net- works. The proposed architecture is based on a Recurrent Convolutional Neural Network topology that takes as input an image of a monophonic score and retrieves a sequence of music symbols as output. In the first stage, a series of convolutional filters are trained to extract meaningful fea- tures of the input image, and then a recurrent block models the sequential nature of music. The system is trained us- ing a Connectionist Temporal Classification loss function, which avoids the need for a frame-by-frame alignment be- tween the image and the ground-truth music symbols. Ex- perimentation has been carried on a set of 90,000 synthetic monophonic music scores with more than 50 different pos- sible labels. Results obtained depict classification error rates around 2 % at symbol level, thus proving the po- tential of the proposed end-to-end architecture for OMR. The source code, dataset, and trained models are publicly released for reproducible research and future comparison purposes.",
        "zenodo_id": 1418333,
        "dblp_key": "conf/ismir/Calvo-ZaragozaV17",
        "keywords": [
            "Optical Music Recognition",
            "end-to-end fashion",
            "neural networks",
            "Recurrent Convolutional Neural Network",
            "monophonic score",
            "music symbols",
            "Connectionist Temporal Classification",
            "synthetic monophonic music scores",
            "classification error rates",
            "potential"
        ],
        "content": "END-TO-END OPTICAL MUSIC RECOGNITION\nUSING NEURAL NETWORKS\nJorge Calvo-Zaragoza\nCentre for Interdisciplinary Research in Music\nMedia and Technology, McGill University\nMontreal, QC, CanadaJose J. Valero-Mas, Antonio Pertusa\nSoftware and Computing Systems\nUniversity of Alicante\nAlicante, Spain\nABSTRACT\nThis work addresses the Optical Music Recognition\n(OMR) task in an end-to-end fashion using neural net-\nworks. The proposed architecture is based on a Recurrent\nConvolutional Neural Network topology that takes as input\nan image of a monophonic score and retrieves a sequence\nof music symbols as output. In the ﬁrst stage, a series of\nconvolutional ﬁlters are trained to extract meaningful fea-\ntures of the input image, and then a recurrent block models\nthe sequential nature of music. The system is trained us-\ning a Connectionist Temporal Classiﬁcation loss function,\nwhich avoids the need for a frame-by-frame alignment be-\ntween the image and the ground-truth music symbols. Ex-\nperimentation has been carried on a set of 90,000 synthetic\nmonophonic music scores with more than 50 different pos-\nsible labels. Results obtained depict classiﬁcation error\nrates around 2 % at symbol level, thus proving the po-\ntential of the proposed end-to-end architecture for OMR.\nThe source code, dataset, and trained models are publicly\nreleased for reproducible research and future comparison\npurposes.\n1. INTRODUCTION\nLarge-scale analysis of music is of great interest, and so\nmany computational tools have been developed for such\npurpose. Quite often, the bottleneck for exploiting these\nideas is the lack of large corpora of symbolic music.\nThe transcription of sheet music into some machine-\nreadable format can be carried out manually. However, the\ncomplexity of music notation inevitably leads to burden-\nsome software for music score editing, which makes the\nwhole process very time-consuming and prone to errors.\nAs a consequence, the development of automatic transcrip-\ntion systems for musical documents is gaining importance\nover the last years.\nThe ﬁeld devoted to address this task is known as Opti-\ncal Music Recognition (OMR) [1]. Typically, an OMR tool\ntakes an image of a music score and provides its symbolic\nc\rJorge Calvo-Zaragoza, Jose J. Valero-Mas, Antonio Per-\ntusa. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Jorge Calvo-Zaragoza, Jose J.\nValero-Mas, Antonio Pertusa. “End-to-end Optical Music Recognition\nusing Neural Networks”, 18th International Society for Music Informa-\ntion Retrieval Conference, Suzhou, China, 2017.content encoded in some structured digital format such as\nMEI or MusicXML. Unfortunately, OMR is a challeng-\ning problem, and results have not been very promising so\nfar [18].\nThe process of automatically recognizing the content of\na music score is complex, and therefore the workﬂow of an\nOMR system is very extensive. Previous proposals related\nto this task focus on speciﬁc aspects of the pipeline, such\nas the binarization of the image [14], the detection of the\nstaves [2], the separation between lyrics and music [3], the\nstaff-line removal [8]—which may be even considered as\na task by itself [7]—or the classiﬁcation of isolated sym-\nbols [17]. It therefore comes as no surprise that no work\nhave directly addressed the whole OMR process for mod-\nern western notation. We only ﬁnd full recognition propos-\nals for old music [5,15,16] that, in spite of involving music\nnotation, entails a very different challenge.\nOne of the practical aspects that constrains end-to-end\nOMR research is the difﬁculty of obtaining an aligned\ndataset containing the labeled music symbols along with\ntheir exact position in the image of the score. Note that,\nfrom a musical perspective, it is not necessary to retrieve\nthe exact position of each music symbol in the image since\nthe important information is the succession of the music\nﬁgures. Thus, it seems interesting to tackle the OMR task\nin an holistic fashion, in which the output is directly the se-\nquence of symbols present in the score image disregarding\ntheir exact position in pixels.\nOur work aims at setting the basis towards the develop-\nment of systems that can directly work with a greater part\nof the OMR workﬂow. For that, we propose the use of\nrecurrent neural networks, which have been applied with\ngreat success to many sequential recognition task such as\nspeech recognition [11], handwriting recognition [12], or\nautomatic music transcription [20]. The premise is that\nthe network works on a single staff section, much in the\nsame way as most Optical Character Recognition systems\nfocuses on recognizing words appearing in a given line im-\nage [21, 23].\nThe traditional limitation of such type of networks is\nthat they require a strongly-aligned training set, i.e., the\nnetwork has to be provided with the desired output of the\nrecurring block for every single input frame of the im-\nage. This constraint has typically led to consider other\nsequential models such as hidden Markov Models, which\ncan be trained with just pairs of input images and tran-472script sequences. Nonetheless, Graves et al. [10] proposed\na method to train recurrent networks with unaligned data\nknown as Connectionist Temporal Classiﬁcation (CTC).\nThe CTC is actually a loss function that focuses on the de-\nsired output sequence, regardless of which frames output\neach symbol.\nFor the precise case of this work, we rely on the Convo-\nlutional Recurrent Neural Network (CRNN) architecture\nfor scene text recognition proposed by Shi et al. [19]. A\nCRNN is a deep neural network that comprises a series\nof convolutional layers, which focus on learning a suitable\nrepresentation of the input image, followed by recurrent\nlayers, which deal with the sequential nature of the task.\nIn order to jointly train the network in an end-to-end fash-\nion, the CTC loss function is considered.\nBesides text recognition, Shi et al. also evaluated\nCRNN with a small number of music scores, just to as-\nsess its capabilities for any sequence-based task. Taking\nthis work as a starting point, we further study the poten-\ntial of the mentioned end-to-end CRNN model for the case\nof OMR. More precisely, our contributions are: (i) the re-\ndesign and optimization of the original CRNN architecture\nfor this particular task; (ii) a thorough and quantitative as-\nsessment of the proposed architecture in terms of a large\ncollection of more than 90,000 monophonic music scores.\nThe rest of the paper is structured as follows: Section 2\ndescribes the details of the corpus created for this work;\nSection 3 describes the end-to-end model proposed; the\nevaluation procedure as well as the results obtained are\nshown and discussed in Section 4; ﬁnally, Section 5 con-\ncludes the work and proposes future lines to address.\n2. CORPUS GENERATION\nFor assessing the proposed scheme we generated a set of\nmonophonic score images together with their ground-truth\nannotations disregarding any frame-level alignment for the\ncase of end-to-end training. This set contains 94,984 ran-\ndom sequences from a vocabulary of 52 Common Western\nMusic Notation symbols: music notes from C4 to E5 (10\npitches), four possible note durations (half, quarter, eighth,\nand sixteenth) and their four respective silences, three time\nsignatures (3/4, 4/4, and 6/8), accidentals (sharp, ﬂat, and\nnatural), the treble clef, and the bar line.\nAll the scores follow this structure: an initial clef; a\nset of alterations for the key of the piece; the time signa-\nture; the music content, being always the bar line annotated\nas it constitutes a symbol to be recognized. Note that bar\nlines are not randomly placed in the score but in their cor-\nresponding positions at the end of each complete bar.\nThe length of the generated sequences is random, with\na minimum length of 4 symbols and a maximum of 37.\nFigure 1 shows a histogram of the length of the produced\nsequences.\nThe generation of the music content is random, i.e., no\nrestriction is imposed about the pitch interval between two\nconsecutive notes or their respective duration. Similarly,\naccidentals are randomly applied to further increase the\nvariability in the scores. Given a sequence of music sym-\n 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000\n 5  10  15  20  25  30  35Frequency\nLengthFigure 1 . Histogram of the length of the sequences of the\ncorpus.\nbols we generated the image scores with the music engrav-\ning software Lilypond1. Figure 2 shows two examples of\nmusic scores along with their ground-truth annotations.\n(a) Simple score (8 symbols)\n(b) Challenging score (34 symbols)\nFigure 2 . Example of scores depicting different levels\nof difﬁculty from our collection, along with its associated\nground-truth.\n3. FRAMEWORK\nOur OMR approach is based on a Convolutional Recur-\nrent Neural Network (CRNN) which takes as input an im-\nage of a monophonic staff section and directly outputs\nthe sequence of music symbols, with no previous symbol\nsegmentation or staff-line removal process. A conceptual\nscheme is illustrated in Figure 3.\nBefore the actual CRNN, we assume that a preprocess-\ning step identiﬁes and segments the different monophonic\nstaff sections from the initial image for processing them in-\ndependently. While this may be seen as a strong assump-\ntion, there exist algorithms in the literature that success-\nfully address this task [6]. Once this monophonic staff sec-\ntion is segmented, the resulting image is normalized (pixel\nvalues between 0 and 1), rescaled to an aspect ratio of 1:4\n1http://lilypond.org/Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 473Figure 3 . Conceptual scheme of the proposed approach. The input score is processed with a series of convolutional ﬁlters;\nthe resulting features are then processed by the recurrent layers to model the temporal context of the piece; a frame-wise\ntranscription using CTC is performed to obtain the estimation in an end-to-end fashion.\n(i.e., the width is four times the height), and used as input\nto the CRNN. We established that this ratio is adequate for\nthe task at issue by means of informal testing.\nTable 1 shows the speciﬁc details of the proposed\nCRNN architecture, whose conﬁguration and parameter-\nization were determined experimentally. First, the im-\nage is processed with a series of convolutional layers\nwhich use Rectiﬁer Linear Unit (ReLU) activation func-\ntions, followed by max pooling layers. Then, the out-\nput of the convolutional block is reshaped to serve as in-\nput to a recurrent neural network block, which is com-\nposed of three Bidirectional Long-Short Term Memory\n(BLSTM) networks [9, 13] with 256 hidden units. Finally,\na fully-connected layer with a SoftMax activation function\nis added to retrieve the most likely class of each frame.\nThe CRNN model is trained using a batch size of 32\nsamples (i.e., 32 monophonic staff sections), RMSprop as\nthe gradient descent method, and the aforementioned CTC\nas the loss function. We set 20 epochs for the training of\nthe model and selecting the conﬁguration that minimizes\nthe validation error.\nNote that the output of the CRNN is a framewise pre-\ndiction that must be processed to obtain the actual output\nsymbol sequence. However, this process is very straight-\nforward because the CTC loss function forces the network\nto predict a blank symbol to indicate the separation be-\ntween consecutive symbols [10].\n4. EVALUATION\n4.1 Partitions\nWe split the generated corpus in three ﬁxed partitions:\ntraining and validation, which are meant to train the model\nand select the most appropriate hyper-parameters of the\nnetwork, and a test partition to eventually assess the per-\nformance of the system. These sets represent the 60 % ,\n20 % , and 20 % out of the total set of available scores,\nrespectively.\nTable 2 describes these partitions in terms of the number\nof scores, measures, and running symbols in each of them.\nIt must be noted that at least one element of the vocabulary\nappears in all the partitions, and so there are no out-of-\nvocabulary elements.4.2 Metrics\nIn order to assess the performance of the proposed method\nwe consider three metrics which allow the evaluation at\ndifferent levels:\n\u000fScore-level error rate (Se): ratio of scores that are\nnot correctly recognized in their entirely (i.e., con-\ntain at least one error amongst the estimated ones).\n\u000fEdit distance (Ed): average number of edit oper-\nations to convert the predicted sequence into the\nground-truth one.\n\u000fNormalized edit distance (EdN): same as the Edit\ndistance metric but normalizing each sequence by\nits length.\nNote that the relevance of each metric depends on the ﬁ-\nnal scenario. If a totally autonomous system is pursued, it\nis important to pay attention to the score-level error. How-\never, quite often it is assumed that an expert user will su-\npervise the output of the system because guaranteeing a\nerror-free model is not feasible [4]. In this case, therefore,\nit is more interesting to measure the errors at the symbol\nlevel, which is more related to the number of corrections\nto be made.\n4.3 Results\nInput images must be resized to ﬁxed dimensions for the\ninput of the network. As mentioned earlier, an aspect ra-\ntio of 1:4 was chosen. Thus, we have experimented with\nvalues involving 40\u0002160,50\u0002200, and 60\u0002240.\nFor each case, network parameters are optimized by\nmeans of the training set, while the validation set is used\nto ﬁnd the most appropriate epoch to stop. The metric cho-\nsen to determine the performance after each epoch during\ntraining is the normalized edit distance ( EdN).\nOnce a model is trained, predictions are made on the\nsamples of test set. Table 3 shows the results of our se-\nries of experiments in terms of the three ﬁgures of merit\npreviously described.\nAn initial remark to begin is that all input sizes behave\nsimilarly. In all the cases, a remarkable performance at\nsymbol level is attained, with ﬁgures lower than 0:6and\n4 %forEdandEdN, respectively. It is true, however, that474 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Block Conﬁguration\nConvolutionalConv(64,3,3)\nMaxPool (2,2)Conv(128,3,3)\nMaxPool (2,2)Conv(256,3,3)\nConv(256,3,3)\nMaxPool(2,1)Conv(512,3,3)\nConv(512,3,3)\nMaxPool(2,1)\nRecurrent BLSTM (256) BLSTM (256) BLSTM (256) FC (52)\nTable 1 . Description of the CRNN architecture considered. Notation Conv(f,w,h) stands for a layer with fconvolution\noperators of size w\u0002hpixels followed by a ReLU activation function. MaxPool(w,h) stands for the max-pooling operator\nof dimensions w\u0002hpixels, BLSTM(n) represents a Bidirectional Long-Short Term Memory unit with nhidden layers,\nand FC(n) is a fully-connected layer of nneurons followed by a SoftMax activation function.\nTraining Validation Test\nScores 56 991 18 996 18 997\nMeasures 125 971 41 883 41 986\nSymbols 989 744 329 802 330 092\nTable 2 . Statistics of the partitions used in this work, re-\nporting the number of scores, the number of measures, and\nthe number of running symbols.\nMetric\nInput image Se(%)EdEdN(%)\n40\u0002160 27.30 0.52 3.01\n50\u0002200 29.79 0.54 3.12\n60\u0002240 22.37 0.37 2.16\nTable 3 . Performance achieved on the test partition with\nrespect to the shape of the input image.\nthe score-level error rates are much higher. That is, quite\noften there is at least one incorrectly recognized symbol in\neach score sequence.\nBest results are obtained using images of 60\u0002240.\nIn that case, a symbol-level error rate of 22:37 % is at-\ntained, with an average of 0:37symbol-level errors per\nscore ( 2:16 % of the symbols if lengths are taken into ac-\ncount). This means that less than one symbol has to be\ncorrected to obtain the actual score, on average. An exam-\nple of prediction results depicting representative transcrip-\ntion errors is illustrated in Figure 4. Note how some of\nthese error change the arrangement of the beamed groups,\nas the predicted sequence does not fulﬁll time signature\nconstraints.\nClearly, these results reﬂect that the proposed frame-\nwork allows recognizing accurately the symbols of mono-\nphonic scores in an end-to-end manner. In turn, the ap-\nproach is not so reliable to optimize the number of per-\nfectly recognized images, regardless of the number of er-\nrors. However, it has to be considered that some music\nsymbols of the generated scores have vertical overlapping,\n(a) Input score\n(b) Prediction of the CRNN\nFigure 4 . An example of prediction with errors ( Ed= 3,\nEdN= 11:53) obtained in our experiments.\nas can be seen in the ﬁrst note C from Figure 2. When\nthis happens, the order of the symbolic sequence might not\nperfectly align with the order of the symbols in the image,\nthereby introducing noise in the samples.\nAs discussed in Sect. 1, there are no previous ap-\nproaches dealing with the OMR task in an end-to-end way\nand, therefore, there is no feasible comparison in this work.\nNevertheless, it is our hope that these results will establish\na new way of approaching OMR.\n4.4 Further Analysis\nIn this section we further analyze some details of the ex-\nperiments carried out.\nFirst we intend to measure the performance of the mod-\nels with respect to the size of the input sequence. Clearly,\nthe size of the sequence has a direct impact on the abil-\nity of the models to recognize all their symbols. It is ex-\npected that the greater the number of symbols in the score,\nthe worse performance the models attain. Figure 5 shows\nthe performance curves as a function of the size of the se-\nquences. On the one hand, Figure 5(a) reports the error rate\ncurve, which depicts that the performance gets dramati-\ncally worsen from sequences of 10-15symbols, depending\non the model. On the other hand, Figure 5(b) shows the\ncurve of the edit distance, for which it is observed that the\naverage number of editing operations to correct a sequence\npredicted by the model is less than 1up to 25symbols. The\ninteresting remark about these curves is that they allow us\nto conclude that in relatively short sequences, the modelsProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 475can obtain an almost optimal performance. Fortunately,\nthe scores can be further subdivided into bars, which have\na limited number of symbols. Therefore, it might be in-\nteresting to address the problem by ﬁrst performing a seg-\nmentation of measures, for which there already exists suc-\ncessful algorithms [22] as previously mentioned.\n 0 0.2 0.4 0.6 0.8 1\n 5  10  15  20  25  30  35Error rate\nLength of the sequence40x160 50x200 60x240\n(a) Score-level error rate ( Se)\n 0 1 2 3 4 5 6 7 8\n 5  10  15  20  25  30  35Avg. edit distance\nLength of the sequence40x160 50x200 60x240\n(b) Edit distance ( Ed)\nFigure 5 . Performance attained by the models with respect\nto the length of the input sequences.\nFinally, it is interesting to analyze the convergence for\neach considered model, since that is an indicator of the\nrepresentation capacity and the difﬁculty with which they\nlearn the task. Figure 6 shows the normalized edit distance\non the validation set as a function of the number of train-\ning epochs. It is observed that all models follow a simi-\nlar trend, in which there is a drastic decrease in the ﬁrst 6\nepochs. After that, models begin to need more epochs to\nimprove their results, reaching convergence (except for mi-\nnor ﬂuctuations) around 12epochs. We can therefore say\nthat all models have a similar representation capabilities,\nalthough it has been demonstrated in the previous section\nthat the model that accepts 60\u0002240images has a greater\ngeneralization ability. In addition, the low number of re-\nquired epochs indicate that the models are able to learn the\ntask quickly.\n5. CONCLUSIONS\nThis work addresses the Optical Music Recognition task in\nan end-to-end fashion with the use of a Convolutional Re-\ncurrent Neural Network (CRNN). We have redesigned the\narchitecture from Shi et al. [19] for OMR using a large col-\nlection of over 90,000 synthetic scores generated through\n 0 0.2 0.4 0.6 0.8 1\n 2  4  6  8  10  12  14  16  18  20Normalized edit distance\nTraining epoch40x160 50x200 60x240Figure 6 . Validation performance (normalized edit dis-\ntance) with respect to the number of training epochs.\nLilypond, a music engraving system. As the network is\ntrained using a Connectionist Temporal Classiﬁcation loss\nfunction, the music symbols do not need to be aligned with\nthe pixels of the original images.\nThe CRNN topology and hyper-parameters were exper-\nimentally adjusted for the task at hand, obtaining remark-\nably low error rates with the evaluated corpus. The net-\nwork converges quickly and an average edit distance of\n0:37is obtained using as input 60\u0002240images.\nIn order to increase the accuracy of the proposed\nmethod, those scores containing temporal overlappings\ncould be removed from the corpus. However, the ultimate\ngoal of OMR is to detect music symbols in polyphonic\nscores. This is a challenging task using CRNN as it im-\nplies to extend CTC for multi-label classiﬁcation, which\nstands as future work to explore and study.\nAnother evident future work line is to train the network\nwith real scores. Synthetic data could be used as a basis\nby adding noise and transformations such as rotation or\nscaling for a preliminary experimentation as in [19], but\nideally a large real corpus should be used instead. Cur-\nrently there are no large datasets containing labeled images\nof real scores, but an end-to-end annotation of the data is\nstraightforward as it does not requires the symbols to be\naligned with the image pixels.\nFinally, note that one of the main advantages of the pro-\nposed neural-based approach is that alternative notations\ncould be recognized by just changing the corpus and re-\ntraining the model. This opens a path for research in re-\nsearch of ancient music recognition written in, for instance,\nmensural or neume notation, among others.\n6. REPRODUCIBILITY\nFor reproducibility purposes, the source code, trained\nmodels, and considered data have been publicly re-\nleased at http://grfia.dlsi.ua.es/gen.php?\nid=software .\n7. ACKNOWLEDGEMENT\nThis work was partially supported by the Social Sciences\nand Humanities Research Council of Canada, and Univer-\nsidad de Alicante through FPU program (UAFPU2014–\n5883) and grant GRE-16-04.476 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20178. REFERENCES\n[1] D. Bainbridge and T. Bell. The challenge of opti-\ncal music recognition. Computers and the Humanities ,\n35(2):95–121, 2001.\n[2] V . Bosch, J. Calvo-Zaragoza, A. H. Toselli, and E.\nVidal. Sheet music statistical layout analysis. Interna-\ntional Conference on Frontiers in Handwriting Recog-\nnition 2016 , 2016.\n[3] J. A. Burgoyne, Y . Ouyang, T. Himmelman, J. De-\nvaney, L. Pugin, and I. Fujinaga. Lyric extraction and\nrecognition on digital images of early music sources. In\nProceedings of the 10th International Society for Mu-\nsic Information Retrieval Conference , pages 723–727,\n2009.\n[4] J. Calvo-Zaragoza and J. Oncina. An efﬁcient approach\nfor interactive sequential pattern recognition. Pattern\nRecognition , 64:295–304, 2017.\n[5] J. Calvo-Zaragoza, A. H. Toselli, and E. Vidal. Early\nhandwritten music recognition with hidden markov\nmodels. In 15th International Conference on Frontiers\nin Handwriting Recognition, ICFHR 2016, Shenzhen,\nChina, October 23-26, 2016 , pages 319–324, 2016.\n[6] V . B. Campos, J. Calvo-Zaragoza, A. H. Toselli, and E.\nVidal-Ruiz. Sheet music statistical layout analysis. In\n15th International Conference on Frontiers in Hand-\nwriting Recognition, ICFHR 2016, Shenzhen, China,\nOctober 23-26, 2016 , pages 313–318, 2016.\n[7] C. Dalitz, M. Droettboom, B. Pranzas, and I. Fujinaga.\nA comparative study of staff removal algorithms. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 30(5):753–766, 2008.\n[8] T. G ´eraud. A morphological method for music score\nstaff removal. In Proceedings of the 21st International\nConference on Image Processing (ICIP) , pages 2599–\n2603, Paris, France, 2014.\n[9] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber.\nLearning precise timing with lstm recurrent networks.\nJ. Mach. Learn. Res. , 3:115–143, March 2003.\n[10] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmid-\nhuber. Connectionist temporal classiﬁcation: Labelling\nunsegmented sequence data with recurrent neural net-\nworks. In Proceedings of the 23rd International Con-\nference on Machine Learning , ICML ’06, pages 369–\n376, New York, NY , USA, 2006. ACM.\n[11] A. Graves, A.-r. Mohamed, and G. Hinton. Speech\nrecognition with deep recurrent neural networks. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing , pages 6645–6649. IEEE, 2013.\n[12] A. Graves and J. Schmidhuber. Ofﬂine handwriting\nrecognition with multidimensional recurrent neural\nnetworks. In Advances in neural information process-\ning systems , pages 545–552, 2009.[13] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural Comput. , 9(8):1735–1780, November\n1997.\n[14] T. Pinto, A. Rebelo, G. A. Giraldi, and J. S. Cardoso.\nMusic score binarization based on domain knowl-\nedge. In Pattern Recognition and Image Analysis - 5th\nIberian Conference, IbPRIA 2011, Las Palmas de Gran\nCanaria, Spain, June 8-10, 2011. Proceedings , pages\n700–708, 2011.\n[15] L. Pugin. Optical music recognitoin of early typo-\ngraphic prints using hidden markov models. In ISMIR\n2006, 7th International Conference on Music Informa-\ntion Retrieval, Victoria, Canada, 8-12 October 2006,\nProceedings , pages 53–56, 2006.\n[16] C. Ramirez and J. Ohya. Automatic recognition\nof square notation symbols in western plainchant\nmanuscripts. Journal of New Music Research ,\n43(4):390–399, 2014.\n[17] A. Rebelo, G. Capela, and J. S. Cardoso. Optical recog-\nnition of music symbols - A comparative study. IJDAR ,\n13(1):19–31, 2010.\n[18] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S.\nMarc ¸al, C. Guedes, and J. S. Cardoso. Optical mu-\nsic recognition: state-of-the-art and open issues. Inter-\nnational Journal of Multimedia Information Retrieval ,\n1(3):173–190, 2012.\n[19] B. Shi, X. Bai, and C. Yao. An end-to-end trainable\nneural network for image-based sequence recognition\nand its application to scene text recognition. Comput-\ning Research Repository , abs/1507.05717, 2015.\n[20] S. Sigtia, E. Benetos, and S. Dixon. An end-to-end neu-\nral network for polyphonic piano music transcription.\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing (TASLP) , 24(5):927–939, 2016.\n[21] A. H. Toselli, V . Romero, M. Pastor, and E. Vidal. Mul-\ntimodal interactive transcription of text images. Pattern\nRecognition , 43(5):1814–1825, 2010.\n[22] G. Vigliensoni, G. Burlet, and I. Fujinaga. Optical mea-\nsure recognition in common music notation. In Pro-\nceedings of the 14th International Society for Music\nInformation Retrieval Conference, ISMIR 2013, Cu-\nritiba, Brazil, November 4-8, 2013 , pages 125–130,\n2013.\n[23] P. V oigtlaender, P. Doetsch, and H. Ney. Handwriting\nrecognition with large multidimensional long short-\nterm memory recurrent neural networks. In 15th In-\nternational Conference on Frontiers in Handwriting\nRecognition, ICFHR 2016, Shenzhen, China, October\n23-26, 2016 , pages 228–233, 2016.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 477"
    },
    {
        "title": "One-Step Detection of Background, Staff Lines, and Symbols in Medieval Music Manuscripts with Convolutional Neural Networks.",
        "author": [
            "Jorge Calvo-Zaragoza",
            "Gabriel Vigliensoni",
            "Ichiro Fujinaga"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417493",
        "url": "https://doi.org/10.5281/zenodo.1417493",
        "ee": "https://zenodo.org/records/1417493/files/Calvo-ZaragozaV17a.pdf",
        "abstract": "One of the most complex stages of optical music recog- nition workflows is the detection and isolation of musical symbols. Traditionally, this goal is achieved by performing preprocesses of binarization and staff-line removal. How- ever, these are commonly performed using heuristics that do not generalize widely when applied to different types of documents such as medieval scores. In this paper we propose an effective and generalizable approach to address this problem in one step. Our proposal classifies each pixel of the image among background, staff lines, and sym- bols using supervised learning techniques, namely convo- lutional neural networks. Experiments on a set of medieval music pages proved that the proposed approach is very ac- curate, achieving a performance upwards of 90% and out- performing common ensembles of binarization and staff- line removal algorithms.",
        "zenodo_id": 1417493,
        "dblp_key": "conf/ismir/Calvo-ZaragozaV17a",
        "keywords": [
            "binarization",
            "staff-line removal",
            "supervised learning",
            "convolutional neural networks",
            "background",
            "staff lines",
            "musical symbols",
            "medieval music pages",
            "performance",
            "90%"
        ],
        "content": "ONE-STEP DETECTION OF BACKGROUND, STAFF LINES, AND\nSYMBOLS IN MEDIEVAL MUSIC MANUSCRIPTS WITH\nCONVOLUTIONAL NEURAL NETWORKS\nJorge Calvo-Zaragoza, Gabriel Vigliensoni, and Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Technology (CIRMMT)\nMcGill University, Montreal, QC, Canada\nABSTRACT\nOne of the most complex stages of optical music recog-\nnition workﬂows is the detection and isolation of musical\nsymbols. Traditionally, this goal is achieved by performing\npreprocesses of binarization and staff-line removal. How-\never, these are commonly performed using heuristics that\ndo not generalize widely when applied to different types\nof documents such as medieval scores. In this paper we\npropose an effective and generalizable approach to address\nthis problem in one step. Our proposal classiﬁes each\npixel of the image among background, staff lines, and sym-\nbols using supervised learning techniques, namely convo-\nlutional neural networks. Experiments on a set of medieval\nmusic pages proved that the proposed approach is very ac-\ncurate, achieving a performance upwards of 90% and out-\nperforming common ensembles of binarization and staff-\nline removal algorithms.\n1. INTRODUCTION\nOptical music recognition (OMR) is the ﬁeld of computer\nscience devoted to providing computers with the ability to\nextract the musical content of a score from the optical scan-\nning of its source image [1]. This problem represents a\ncomplex challenge for which there are no completely sat-\nisfactory solutions yet [20]. The task can be further di-\nvided into two different stages [6]: document image pro-\ncessing, in which the objective is to detect and recognize\neach meaningful symbol appearing in the image; and re-\nconstruction of musical notation, in which musical mean-\ning is assigned to each of these symbols in order to encode\nthe content in a structured symbolic music format such as\nMEI (Music Encoding Initiative) or MusicXML.\nDue to the arrangement of the elements on the staff, the\nimage-processing stage is usually approached following a\nstrategy of segmentation and classiﬁcation. That is, ele-\nments within the score are ﬁrst detected independently and\nc\rJorge Calvo-Zaragoza, Gabriel Vigliensoni, and Ichiro\nFujinaga. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Jorge Calvo-Zaragoza,\nGabriel Vigliensoni, and Ichiro Fujinaga. “One-step detection of back-\nground, staff lines, and symbols in medieval music manuscripts with con-\nvolutional neural networks”, 18th International Society for Music Infor-\nmation Retrieval Conference, Suzhou, China, 2017.then a classiﬁcation algorithm assigns a category to each\nof them. Our approach focuses on the segmentation stage.\nThe ﬁnal objective of segmentation is to detect the re-\ngions of the image that correspond to music symbols. To\nachieve this, traditional segmentation workﬂows incorpo-\nrate the steps of binarization of the document and detection\nand removal of staff lines. Staff-line detection and removal\nalgorithms usually use a binarized image as input, which\nfacilitates certain procedures such as morphological oper-\nations or histogram analysis—core processes of many of\nthese algorithms. In addition, the segmentation workﬂow\nalso allows for the detection of staff line positions. If sym-\nbol isolation were done from a color image, it would not\nknow which parts belong to the background of the docu-\nment and which to staff lines. Note that the position of\nstaff lines is crucial for determining the pitch of the sym-\nbols.\nThe traditional segmentation workﬂow, however, has a\nnumber of drawbacks. First, the staff-line detection and re-\nmoval becomes heavily dependent on the accuracy of bina-\nrization, as errors are propagated between the two stages.\nIn addition, the traditional methods follow heuristic tech-\nniques that assume speciﬁc conditions in the images to be\ntreated. While this may be useful if the context of their\nuse is limited to a particular style of documents, it is dif-\nﬁcult to generalize these methods so that they can be used\nin various cases. This is especially true when dealing with\nmedieval manuscripts, which present a greater heterogene-\nity in this regard.\nFor all of the above cases, we propose a framework with\nthe goals of isolating the symbols depicted in the image of\na music score and keeping the staff-line information. In\nour approach we perform a document analysis procedure\nthat allows for categorical discrimination of each pixel, ac-\ncording to the class it belongs to (e.g., background ,staff\nlines , orsymbols ) in a single step. In order to make this\napproach generalizable we address the task using the su-\npervised learning paradigm. That is, we assume that a ref-\nerence set is available that can be used to train a model to\nperform such task. In particular, we make use of convolu-\ntional networks for this purpose. These networks are pow-\nerful models that are capable of learning a suitable repre-\nsentation for a given task, thus avoiding the necessity of de-\nveloping a feature extraction strategy speciﬁcally designed\nfor each type of document to be processed. Our experi-\nments on two sets of medieval documents report excellent724results, outperforming different combinations of binariza-\ntion algorithms and staff-line removal algorithms.\nThe rest of the paper is structured as follows: related\nwork and the context of our proposal is presented in Sec-\ntion 2; the proposed method to solve the problem is de-\ntailed in Section 3; the experimental setup to validate our\napproach is described in Section 4; comparative and qual-\nitative results are reported in Section 5; and conclusions\nand promising avenues for future work are summarized in\nSection 6.\n2. RELATED WORK\nOMR has to deal with many aspects of musical notation,\none of which is the presence of the staff. Since most sym-\nbols in the score are connected through these lines, it has\nbeen traditionally necessary to remove them in order to de-\ntect musical symbols.\nThe staff-line removal stage is usually performed af-\nter the binarization of the document in the OMR work-\nﬂow [20] because this step helps to reduce the complex-\nity of the problem and is required to apply certain tech-\nniques such as morphological operators, histogram anal-\nysis, or connected components. In addition, starting from\nthe color image, the processes of binarization and staff-line\nremoval, one after the other, allow the separation of back-\nground, staff lines, and musical symbols regions.\nA comprehensive review and comparison of the early\nattempts for the staff-line removal can be found in the work\nof Dalitz et al. [7]. Given the interest in this challeng-\ning task, many other methods have been proposed more\nrecently. Cardoso et al. [9] proposed a method that con-\nsiders the staff lines as connecting paths between the two\nmargins of the score. The score was modeled as a graph so\nthat staff detection was solved as a maximization problem.\nDutta et al. [10] developed a method for printed scores that\nconsidered the staff-line segment as a horizontal connec-\ntion of vertical black runs with uniform height. Piatkowska\net al. [18] designed a method that used a Swarm Intelli-\ngence algorithm. Their approach can apparently deal with\nany type of image, but only results on binary images were\nreported. Su et al. [23] ﬁtted an approximate staff con-\nsidering properties such as height and space. Geraud [11]\ndeveloped a method that entails a series of morphological\noperators: ﬁrst, a permissive hit-or-miss with a horizontal\nline pattern, followed by a horizontal median ﬁlter and a\ndilation operation. A binary mask is then obtained with a\nmorphological closing. Finally, a vertical median ﬁlter is\napplied to the largest components of the mask. The proce-\ndure is directly applied to the image, which eventually re-\nmoves staff lines. Montagner et al. [15] proposed to learn\nimage operators, whose combination remove staff lines.\nThe problem with these methods is that they focus on\nparticular aspects of the style of the speciﬁc scores toward\nwhich they are oriented and it is, therefore, very difﬁcult\nto adapt them to other types of documents (for example,\nfrom different eras or with different notations or styles).\nIn addition, most of these methods assume already bina-\nrized images as input. The binary nature of modern musi-cal scores (black ink on white paper) has, to some extent,\njustiﬁed this assumption. Of course, document binariza-\ntion is not a trivial problem—especially when dealing with\nancient documents [16]. Furthermore, it turns out that tra-\nditional document binarization methods, which were de-\nsigned mainly for text documents, are often not suitable\nfor musical scores [4].\nHere we introduce a more generalized framework to\nsolve the whole segmentation problem directly. The frame-\nwork is based on machine learning so that it can be applied\nto a wide variety of musical notation styles and musical\ndocuments, as long as training data is available. Our strat-\negy is inspired by the work of Calvo-Zaragoza et al. [5],\nin which a Support Vector Machine classiﬁer was trained\nto discriminate if a foreground pixel of a binary image be-\nlongs to a symbol or to a staff line . Our approach is similar\nin formulation, but we do not assume that the documents\nare binarized or that they contain only symbols or staff\nlines. Furthermore, we also extend the procedure by using\na more advanced classiﬁcation scheme based on Convolu-\ntional Neural Networks (CNN).\n3. METHOD\nAlthough rarely formulated in this way, the problems re-\nlated to image processing for musical documents are con-\ncerned with pixel-level classiﬁcation processes. That is,\nfor each pixel of the image we want to know whether it\nbelongs to a musical symbol or not. In the latter case, we\nwant to know whether the pixel belongs to a staff line or\nnot, as this information is valuable for determining the ver-\ntical position of the notes (pitches), among others.\nTherefore, the process can be formulated as a classiﬁca-\ntion problem in which a model is trained to distinguish the\ncategory a given pixel belongs to. Formally, our approach\nconsiders a model that categorizes a given pixel into three\npossible classes: background ,staff, and symbol . The re-\nquirement to carry out this idea consists of a reference set\nthat allows providing examples of each category to the su-\npervised learning algorithm.\nIn our framework, this classiﬁcation process is carried\nout by means of Deep Learning. Recently, Deep Neural\nNetworks have shown a remarkable leap of performance in\npattern recognition. Speciﬁcally, CNN have been applied\nwith great success for the detection, segmentation, and\nrecognition of objects and regions in images, approaching\nhuman performance on some of these tasks [13].\nCNN are composed of a series of ﬁlters (i.e., convolu-\ntions) that obtain several representations of the input im-\nage. These ﬁlters are applied in a hierarchy of layers, each\nof which represent different levels of abstraction; while ﬁl-\nters of the ﬁrst layers may enhance details of the image,\nﬁlters of the last layers may detect high-level entities [12].\nThe key to this approach is that, instead of being ﬁxed,\nthese ﬁlters are modiﬁed through a gradient descent opti-\nmization algorithm called back-propagation [14].\nOne of the main advantages of CNN is their ability to\nlearn a suitable representation of the training data with-\nout any human intervention, affording greater general-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 725ization to documents of different style. In other words,\nthese networks learn a suitable representation of the data\nfrom raw data, without the need of feature extraction.\nSince collections of music documents are a rich source of\nhighly complex information—often more heterogeneous\nthan other types of documents—a framework based on\nCNN is promising.\n3.1 Input Feature Set\nAs mentioned above, our intention is to train a CNN to dif-\nferentiate pixels belonging to the different categories. An-\nalyzing the organization of musical documents, we hypoth-\nesize that a pixel can be correctly categorized by using its\nlocal, neighboring information. In other words, we assume\nthat the surrounding region of a pixel contains enough dis-\ncriminative information to classify it into its correct cate-\ngory. As a result, the input set to the classiﬁer in our frame-\nwork is a portion of the input image, centered at the pixel\nof interest. Figure 1 illustrates some examples of input fea-\nture set for each of the considered categories, in which the\npixel to be classiﬁed is located in the center of the patch.\nFigure 1 . Example of input feature sets for pixels of in-\nterest of the three classes: symbol ,background , and staff.\nNote that the pixel to be classiﬁed is located at the center of\neach window (highlighted in red for a better illustration).\nNote that the method can work directly with color im-\nages and that the size of the neighborhood (i.e., the size of\nthe window) is a parameter to be tuned according to the\nscale of the images to be processed.\n3.2 Convolutional Neural Networks\nSince there are no previously proposed CNN models to\nsolve a task of this kind, we designed a new network con-\nﬁguration. Note, however, that the ultimate goal of this pa-\nper is not to ﬁnd the best network topology—which would\ninvolve a comprehensive set of experiments to ﬁnd the best\nset of parameters—but to demonstrate that the proposed\ncategorization of music documents based on pixel-wise\nclassiﬁcation with CNN is feasible.\nOur design is inspired by the VGG network [22], a\ntopology widely used in the computer vision communityfor object recognition. This network contains several lay-\ners of convolution plus 2\u00022max-pooling ( 16or19, de-\npending on its version). By means of informal testing we\nsimpliﬁed this network to up to 3layers, adjusting the num-\nber of convolutional ﬁlters per layer to 64, and the size of\nthe convolution kernels to 7.\nLearning of the network weights is performed by means\nof stochastic gradient descent [2] with a batch size of 32,\nconsidering the adaptive learning rate proposed by Zeiler\n[26] (default parameterization) and a cross-entropy loss\nfunction. Once the CNN has learned how to distinguish\namong the considered categories it can be used to perform\nthe layout analysis of a document. To do so, each pixel of\nthe image is queried, and its feature set is forwarded and\nprocessed by the network in order to obtain its most likely\ncategory.\n4. EXPERIMENTAL SETUP\n4.1 Corpora\nWe trained and tested our approach on a set of high-\nresolution image scans of two different old music docu-\nments. The ﬁrst corpus is a subset of 10 pages of the\nEinsiedeln, Stiftsbibliothek, Codex 611(89), from 1314.1\nThe second corpus consists of 10 pages of the Salzinnes\nAntiphonal manuscript (CDM-Hsmu M2149.14), music\nscore dated 1554–5.2Pages from the two manuscripts are\nshown in Figure 2. As a reference measure for scale, both\npages depict a separation between staff lines of approxi-\nmately 50pixels.\nNote that the image scans of these two manuscripts have\nzones with different lighting conditions that may affect the\nperformance of the proposal we evaluate. The Einsiedeln\nmanuscript images, in particular, present areas with severe\nbleed-through that may mislead the automatic recognition.\nThe ground-truth data from the corpora was created by\nmanually labeling pixels into the three categories consid-\nered, as illustrated in Figure 3. Note that the class symbol\nincludes both musical symbols and other types of symbols\n(such as lyrics). This should not be an issue as there exist\nsuccessful algorithms to separate music and lyrics [3].\nTaking into account the scale of the images of our cor-\npora, an input window size of 41\u000241pixels was empir-\nically chosen, which corresponds to more than half of the\nspace between the staff lines.\n4.2 Comparative Assessment\nTo the best of our knowledge, there are no other algorithms\nthat perform a direct detection of staff lines and symbols\nfrom music document images, and so we decided to com-\npare our approach with combinations of standard binariza-\ntion and staff-line removal algorithms. In order to select\nthese algorithms, we took into account the results of the IC-\nDAR / GREC 2013 Competition on Music Scores: Staff Re-\nmoval [24]. In this contest, the two strategies that obtained\nthe best performance were LRDE [11] and INESC [9].\n1http://www.e-codices.unifr.ch/en/sbe/0611/\n2https://cantus.simssa.ca/manuscript/133/726 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017(a) Einsiedeln\n (b) Salzinnes\nFigure 2 . Pages from the corpora used in this work.\n(a) Source image\n(b) Ground-truth\nFigure 3 . Example of ground-truth created. Background\npixels are labeled in white, staff-line pixels are labeled in\nred, and symbol pixels are labeled in blue.\nThese methods were based on published approaches (de-\nscribed in Section 2).\nAs mentioned before, these methods require that the in-put image only contains binary values. Therefore, the fol-\nlowing binarization strategies are considered:\nSauvola method [21] is perhaps the most widely consid-\nered binarization algorithm for document images. It\nis based on the assumption that foreground pixels are\ncloser to black than background pixels. It computes\na threshold at each pixel considering the mean and\nstandard deviation of a square window centered at\nthe pixel under consideration.\nWolf & Jolion method [25] is an extension of Sauvola’s,\nwith a change in threshold formula to normalize\ncontrast and the mean gray-level of the considered\nsquare window.\nBLIST method [19] (Binarization based in LIne Spacing\nand Thickness) is specially designed for binarizing\nmusic scores. It consists of an adaptive local thresh-\nolding algorithm based on the estimation of the fea-\ntures of the staff lines depicted in the score.\nTo obtain the three categories mentioned above, we as-\nsume that background are those pixels removed by the\nbinarization algorithm, while staff are those removed by\nthe staff-line removal algorithm from the binarized image.\nThe remaining pixels are thus classiﬁed as belonging to the\nsymbol category.\nEach combination of staff-line removal and binarization\nmethods was evaluated experimentally. To assure a fairProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 727comparison, the parameters for each method (if any) were\ntuned to obtain the best results in the training set.\n4.3 Evaluation\nTo evaluate our proposal, we used a scheme of 10-fold\ncross-validation on each corpus. That is, at each iteration,\none of the pages was used as a test set, and the other nine\nwere used to train the network and optimize its conﬁgu-\nration. Speciﬁcally, 30 000 samples of each of the three\nclasses were randomly selected for training (total: 90 000 ),\nwhile 600 000 of each class (total: 1 800 000 ) were used as\na validation set. Note that these partitions represent a tiny\nportion of the available data, as each page contains about\n2\u0001107pixels. However, these values were considered ad-\nequate to successfully train the networks (both in accuracy\nand computational load) on the machines that were used\nfor that purpose. A more clever use of all available data\nwill be discussed when addressing future work. As a re-\nsult, the training set was used to optimize the CNN through\ngradient descent, whereas the validation set was used to se-\nlect the most appropriate epoch to stop the learning process\nand prevent over-ﬁtting. The complete testing pages were\nﬁnally used to measure the performance of the model cre-\nated by the network during training.\nGiven that the number of pixels of each class is not\nevenly balanced in the documents, we consider the F-\nmeasure (F 1) class-wise ﬁgure of merit for quantitatively\nassessing the classiﬁcation accuracy of the system. Taking\none class at a time as reference, this metric summarizes the\ncorrectly classiﬁed elements ( True Positive , TP), elements\nfalsely classiﬁed as belonging to the reference set ( False\nPositive , FP), and elements of the reference set misclassi-\nﬁed as belonging to another category ( False Negative , FP)\nin a single value. Then, the F1is formalized as:\nF1=2\u0001TP\n2\u0001TP+FP+FN:\nFinally, in order to minimize the possibility that the dif-\nferences in model performance were due to chance vari-\nation, we will perform a pairwise, non-parametric test\n(Wilcoxon signed rank [8]).\n5. RESULTS\nWe show in Table 1 the average F1results obtained in each\ncorpus, as well as the overall performance when the whole\nset of documents is taken into consideration.\nAs can be seen in the table, the staff-line removal algo-\nrithm is the most relevant element in the considered con-\nﬁgurations, because the differences are smaller when vary-\ning the binarization algorithm. In particular, the LRDE\napproach reports poor results in both sets of documents,\ndespite having obtained the best results in the aforemen-\ntioned competition. This directly demonstrates the lack of\ngeneralization of this approach. The INESC algorithm ex-\nhibits a fair performance, especially in the Salzinnes cor-\npus. In regard to binarization algorithms, no conclusion\ncan be drawn since the results seem too similar and depend\non the corpus.StrategyDataset\nEinsiedeln Salzinnes Whole\nLRDESauvola 58.5 78.6 68.6\nWolf 58.7 70.6 64.6\nBLIST 59.2 74.0 66.6\nINESCSauvola 80.3 91.6 86.0\nWolf 83.0 90.7 86.9\nBLIST 83.8 88.0 85.9\nCNN 88.0 92.6 90.3\nTable 1 . Average F1obtained in the 10-fold cross-\nvalidation scheme for each corpus and the whole set.\nThe approach based on CNN, which performs the pro-\ncess in a single step, yields the best results in all cases\nconsidered. Since these results only reﬂect the average\nperformance, we used the 20independent results ( 10for\neach corpus) to perform statistical tests. It resulted in p-\nvalues below 0:01in all pairwise comparisons, and so our\napproach is signiﬁcantly better than the rest of the conﬁg-\nurations with an alpha signiﬁcance level of 99%.\nIn order to have a qualitative reference, Table 2 shows\nan example of the categorization obtained by LRDE and\nINESC methods on a piece of Einsiedeln documents, con-\nsidering BLIST binarization (best case), as well as the cat-\negorization of the approach based on CNN. It is observed\nthat LRDE is only able to partially detect one of the lines\nof staff. INESC achieves an acceptable performance but it\nmislabels some sections of the staff. CNN shows a predic-\ntion that is very similar to the reference one. In addition,\nit completes one of the staff lines that is not perfectly seen\nin the original document (which, in turn, may be detrimen-\ntal when computing its accuracy). Also, the CNN tends to\nmislabel pixel close to boundaries of elements, in which is\nnot clear the actual category of the pixel. It is expected,\nhowever, that these errors will not cause inconveniences in\nsubsequent procedures of the recognition workﬂow.\nAll in all, we can state that a trained CNN can success-\nfully detect the selected categories at the pixel level in im-\nages of music scores. Our approach reports the best per-\nformance among the evaluated methods although it is fair\nto say that it is not by a wide margin. Nevertheless, its\nstrength can be observed in the improvements achieved in\neach corpus. On the Salzinnes corpus, which seems to be\nless degraded and simpler, the margin was narrower. How-\never, in the Einsiedeln manuscript the improvement over\nthe compared methods was higher. This could mean that,\nas the difﬁculty increases, our approach could be more gen-\neralizable and adaptable.\nIt should be emphasized that the intention of this work\nwas not to ﬁnd the most suitable combination of input\nfeature size and network topology, but to show that this\napproach allows dealing with the analysis of music doc-728 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Source Ground-truth\nBLIST+LRDE BLIST+INESC CNN\nTable 2 . Qualitative examples of categorization from Einsiedeln document, depicting the original piece along with the man-\nually created ground-truth, and the labeling predicted by BLIST+LRDE, BLIST+INESC, and CNN. Coloring: background\nin white, staff lines in red, and symbols in blue.\numents successfully. Therefore, a more comprehensive\nsearch of the optimal parameters could be carried out to\nobtain an even better performance.\n6. CONCLUSIONS\nIn this paper we presented a framework for detecting back-\nground, staff lines, and symbols in medieval manuscripts.\nOur approach was based on the classiﬁcation of the dif-\nferent elements of the image at pixel level using machine\nlearning. We use a CNN along with a training dataset of\nreasonable size that contained examples for each category.\nOur results showed that the accuracy obtained is high,\nachieving around 90% ofF1in the evaluated corpus. It\nhas also been shown that our proposal is able to outper-\nform state-of-the-art strategies based on heuristic image\nprocessing, demonstrating that CNN is a robust and gen-\neralizable alternative to those traditional approaches.\nIn future work, efforts should be devoted to overcom-\ning the problem of getting enough data to train the CNN. It\ncould be interesting to consider an incremental interactive\nframework in which the user does not have to label every\nsingle pixel of the image but only those erroneously la-\nbeled by a base classiﬁer. The use of transfer learning [17]\nis another way to reduce the initial effort when dealing with\na new type of document.\nMoreover, there are several ways to improve the accu-\nracy of the model in the future. Of course, ﬁnding a more\nsuitable network conﬁguration for this problem is a way\nof improving the results presented here. Also, since the\navailable data is very large (i.e., a single page of ground-\ntruth provides millions of examples of pixels labeled by\nhumans), it would be more beneﬁcial to train the networkfollowing a smarter strategy than choosing a random sub-\nset of the available data. For example, a random training\nset can be initially chosen to perform a ﬁrst training itera-\ntion (as in the case of this work). After that, training doc-\numents can be evaluated so that the network is re-trained\nonly with those pixels that would be misclassiﬁed by the\ncurrent model. In this way, the network would pay special\nattention to the most difﬁcult cases.\n7. ACKNOWLEDGEMENT\nThis work was partially supported by the Social Sciences\nand Humanities Research Council of Canada.\nSpecial thanks to Vi-An Tran for manually labeling the\ndifferent layers in all the manuscripts used for this re-\nsearch.\n8. REFERENCES\n[1] D. Bainbridge and T. Bell. The challenge of opti-\ncal music recognition. Computers and the Humanities ,\n35(2):95–121, 2001.\n[2] L. Bottou. Large-scale machine learning with stochas-\ntic gradient descent. In Proceedings of COMP-\nSTAT’2010 , pages 177–186. Springer, 2010.\n[3] J. A. Burgoyne, Y . Ouyang, T. Himmelman, J. De-\nvaney, L. Pugin, and I. Fujinaga. Lyric extraction and\nrecognition on digital images of early music sources. In\nProceedings of the 10th International Society for Mu-\nsic Information Retrieval Conference , pages 723–727,\n2009.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 729[4] J. A. Burgoyne, L. Pugin, G. Eustace, and I. Fujinaga.\nA comparative survey of image binarisation algorithms\nfor optical recognition on degraded musical sources.\nInProceedings of the 8th International Conference on\nMusic Information Retrieval , pages 509–512, 2007.\n[5] J. Calvo-Zaragoza, L. Mic ´o, and J. Oncina. Music staff\nremoval with supervised pixel classiﬁcation. Interna-\ntional Journal on Document Analysis and Recognition ,\n19(3):211–219, 2016.\n[6] J. Calvo-Zaragoza, G. Vigliensoni, and I. Fujinaga.\nDocument analysis for music scores via machine learn-\ning. In Proceedings of the 3rd International Work-\nshop on Digital Libraries for Musicology , pages 37–\n40. ACM, 2016.\n[7] C. Dalitz, M. Droettboom, B. Pranzas, and I. Fujinaga.\nA comparative study of staff removal algorithms. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 30(5):753–766, 2008.\n[8] J. Demsar. Statistical comparisons of classiﬁers over\nmultiple data sets. Journal of Machine Learning Re-\nsearch , 7:1–30, 2006.\n[9] J. Dos Santos Cardoso, A. Capela, A. Rebelo, C.\nGuedes, and J. Pinto da Costa. Staff detection with sta-\nble paths. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 31(6):1134–1139, June 2009.\n[10] A. Dutta, U. Pal, A. Forn ´es, and J. Llad ´os. An efﬁ-\ncient staff removal approach from printed musical doc-\numents. In 20th International Conference on Pattern\nRecognition, ICPR 2010, Istanbul, Turkey, 23-26 Au-\ngust 2010 , pages 1965–1968, 2010.\n[11] T. G ´eraud. A morphological method for music score\nstaff removal. In Proceedings of the 21st International\nConference on Image Processing (ICIP) , pages 2599–\n2603, Paris, France, 2014.\n[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima-\ngenet classiﬁcation with deep convolutional neural net-\nworks. In 26th Annual Conference on Neural Informa-\ntion Processing Systems , pages 1106–1114, 2012.\n[13] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning.\nNature , 521(7553):436–444, 2015.\n[14] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278–2324,\n1998.\n[15] I. d. S. Montagner, R. Hirata, and N. S. T. Hirata. A\nmachine learning based method for staff removal. In\nProceedings of the 22nd International Conference on\nPattern Recognition (ICPR) , pages 3162–3127, 2014.\n[16] K. Ntirogiannis, B. Gatos, and I. Pratikakis.\nICFHR2014 competition on handwritten docu-\nment image binarization (H-DIBCO 2014). In 14thInternational Conference on Frontiers in Handwriting\nRecognition , pages 809–813, 2014.\n[17] S. J. Pan and Q. Yang. A survey on transfer learning.\nIEEE Transactions on Knowledge and Data Engineer-\ning, 22(10):1345–1359, 2010.\n[18] W. Piatkowska, L. Nowak, M. Pawlowski, and M.\nOgorzalek. Stafﬂines pattern detection using the swarm\nintelligence algorithm. In L. Bolc, R. Tadeusiewicz,\nL. J. Chmielewski, and K. Wojciechowski, editors,\nComputer Vision and Graphics , volume 7594 of Lec-\nture Notes in Computer Science , pages 557–564.\nSpringer Berlin Heidelberg, 2012.\n[19] T. Pinto, A. Rebelo, G. A. Giraldi, and J. S. Cardoso.\nMusic score binarization based on domain knowledge.\nInProceedings of the 5th Iberian Conference on Pat-\ntern Recognition and Image Analysis , pages 700–708,\nLas Palmas de Gran Canaria, Spain, 2011.\n[20] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S.\nMarc ¸al, C. Guedes, and J. S. Cardoso. Optical mu-\nsic recognition: State-of-the-art and open issues. Inter-\nnational Journal of Multimedia Information Retrieval ,\n1(3):173–190, 2012.\n[21] J. Sauvola and M. Pietik ¨ainen. Adaptive document im-\nage binarization. Pattern Recognition , 33(2):225–236,\n2000.\n[22] K. Simonyan and A. Zisserman. Very deep con-\nvolutional networks for large-scale image recogni-\ntion. Computing Research Repository , abs/1409.1556,\n2014.\n[23] B. Su, S. Lu, U. Pal, and C. L. Tan. An effective\nstaff detection and removal technique for musical doc-\numents. In Proceedings of the 10th IAPR International\nWorkshop on Document Analysis Systems (DAS) , pages\n160–164, 2012.\n[24] M. Visaniy, V . Kieu, A. Forn ´es, and N. Journet. IC-\nDAR/GREC 2013 music scores competition: Staff re-\nmoval. In Proceedings of the 12th International Con-\nference on Document Analysis and Recognition (IC-\nDAR) , pages 1407–1411, 2013.\n[25] C. Wolf, J.-M. Jolion, and F. Chassaing. Text local-\nization, enhancement and binarization in multimedia\ndocuments. In Proceedings of the International Con-\nference on Pattern Recognition , volume 2, pages 1037–\n1040, 2002.\n[26] M. D. Zeiler. ADADELTA: An adaptive learn-\ning rate method. Computing Research Repository ,\nabs/1212.5701, 2012.730 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Improving Note Segmentation in Automatic Piano Music Transcription Systems with a Two-State Pitch-Wise HMM Method.",
        "author": [
            "Dorian Cazau",
            "Yuancheng Wang",
            "Olivier Adam",
            "Qiao Wang",
            "Grégory Nuel"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417929",
        "url": "https://doi.org/10.5281/zenodo.1417929",
        "ee": "https://zenodo.org/records/1417929/files/CazauWAWN17.pdf",
        "abstract": "Many methods for automatic piano music transcription in- volve a multi-pitch estimation method that estimates an activity score for each pitch. A second processing step, called note segmentation, has to be performed for each pitch in order to identify the time intervals when the notes are played. In this study, a pitch-wise two-state on/off first-order Hidden Markov Model (HMM) is developed for note segmentation. A complete parametrization of the HMM sigmoid function is proposed, based on its orig- inal regression formulation, including a parameter α of slope smoothing and β of thresholding contrast. A com- parative evaluation of different note segmentation strate- gies was performed, differentiated according to whether they use a fixed threshold, called “Hard Thresholding” (HT), or a HMM-based thresholding method, called “Soft Thresholding” (ST). This evaluation was done following MIREX standards and using the MAPS dataset. Also, dif- ferent transcription and recording scenarios were tested us- ing three units of the Audio Degradation toolbox. Results show that note segmentation through a HMM soft thresh- olding with a data-based optimization of the {α, β} pa- rameter couple significantly enhances transcription perfor- mance.",
        "zenodo_id": 1417929,
        "dblp_key": "conf/ismir/CazauWAWN17",
        "keywords": [
            "Hidden Markov Model",
            "note segmentation",
            "pitch-wise",
            "two-state",
            "on/off",
            "HMM sigmoid function",
            "Hard Thresholding",
            "Soft Thresholding",
            "MIREX standards",
            "Audio Degradation toolbox"
        ],
        "content": "IMPROVING NOTE SEGMENTATION IN AUTOMATIC PIANO MUSIC\nTRANSCRIPTION SYSTEMS WITH A TWO-STATE PITCH-WISE HMM\nMETHOD\nDorian Cazau1Yuancheng Wang2Olivier Adam3\nQiao Wang2Gr´egory Nuel4\n1Lab-STICC, ENSTA-Bretagne\n2School of Information Science and Engineering, Southeast China\n3Institut d’Alembert, UPMC\n4LPMA, UPMC\ndorian.cazau@ensta-bretagne.fr\nABSTRACT\nMany methods for automatic piano music transcription in-\nvolve a multi-pitch estimation method that estimates an\nactivity score for each pitch. A second processing step,\ncalled note segmentation, has to be performed for each\npitch in order to identify the time intervals when the notes\nare played. In this study, a pitch-wise two-state on/off\nﬁrst-order Hidden Markov Model (HMM) is developed\nfor note segmentation. A complete parametrization of the\nHMM sigmoid function is proposed, based on its orig-\ninal regression formulation, including a parameter \u000bof\nslope smoothing and \fof thresholding contrast. A com-\nparative evaluation of different note segmentation strate-\ngies was performed, differentiated according to whether\nthey use a ﬁxed threshold, called “Hard Thresholding”\n(HT), or a HMM-based thresholding method, called “Soft\nThresholding” (ST). This evaluation was done following\nMIREX standards and using the MAPS dataset. Also, dif-\nferent transcription and recording scenarios were tested us-\ning three units of the Audio Degradation toolbox. Results\nshow that note segmentation through a HMM soft thresh-\nolding with a data-based optimization of the f\u000b;\fgpa-\nrameter couple signiﬁcantly enhances transcription perfor-\nmance.\n1. INTRODUCTION\nWork on Automatic Music Transcription (AMT) dates\nback more than 30 years [21], and has known numerous\napplications in the ﬁelds of music information retrieval, in-\nteractive computer systems, and automated musicological\nanalysis [16]. Due to the difﬁculty in producing all the in-\nc\rDorian Cazau, Yuancheng Wang, Olivier Adam, Qiao\nWang, Gr ´egory Nuel. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: Dorian\nCazau, Yuancheng Wang, Olivier Adam, Qiao Wang, Gr ´egory Nuel. “IM-\nPROVING NOTE SEGMENTATION IN AUTOMATIC PIANO MUSIC\nTRANSCRIPTION SYSTEMS WITH A TWO-STATE PITCH-WISE\nHMM METHOD”, 18th International Society for Music Information Re-\ntrieval Conference, Suzhou, China, 2017.formation required for a complete musical score, AMT is\ncommonly deﬁned as the computer-assisted process of an-\nalyzing an acoustic musical signal so as to write down the\nmusical parameters of the sounds that occur in it, which\nare basically the pitch, onset time, and duration of each\nsound to be played. In this study, we will restrict our-\nselves to this task of “low-level” transcription. Despite this\nlarge enthusiasm for AMT challenges, and several audio-\nto-MIDI converters available commercially, perfect poly-\nphonic AMT systems are out of reach of today’s technol-\nogy.\nThe diversity of music practice, as well as supports\nof recording and diffusion, makes the task of AMT very\nchallenging. These variability sources can be partitioned\nbased on three broad classes: 1) instrument based, 2) mu-\nsic language model based and 3) technology based. The\nﬁrst class covers variability from tonal instrument tim-\nbre. All instruments possess a speciﬁc acoustic signa-\nture, that makes them recognizable among different instru-\nments playing a same pitch. This timbre is deﬁned by\nacoustic properties, both spectral and temporal, speciﬁc\nto each instrument. The second class includes variability\nfrom the different ways an instrument can be played, that\nvary with the musical genre (e.g. tonality, tuning, rhythm),\nthe playing techniques (e.g. dynamics, plucking modes),\nand the personal interpretations of a same piece. These\nﬁrst two classes induce a high complexity of note spec-\ntra over time, whose non-stationarity is determined both\nby the instrument and the musician playing characteris-\ntics. The third class includes variability from electrome-\nchanics (e.g. transmission channel, microphone), environ-\nment (e.g. background noise, room acoustics, distant mi-\ncrophone), data quality (e.g. sampling rate, recording qual-\nity, audio codec/compression). For example, in ethnomu-\nsicological research, extensive sound datasets currently ex-\nist, with generally poor quality recordings made on the\nﬁeld, while a growing need for automatic analysis appears\n[9, 18, 20, 25].\nConcerning AMT methods, many studies have used\nrank reduction and source separation methods, exploiting\nboth the additive and oscillatory properties of audio sig-523nals. Among them, spectrogram factorization methods\nhave become very popular, from the original Non-negative\nMatrix Factorization (NMF) to the recent developments\nof the Probabilistic Latent Component Analysis (PLCA)\n[2, 5]. PLCA is a powerful method for Multi-Pitch Esti-\nmation (MPE), representing the spectra as a linear com-\nbination of vectors from a dictionary. Such models take\nadvantage of the inherent low-rank nature of magnitude\nspectrograms to provide compact and informative descrip-\ntions. Their output generally takes the form of a pianoroll-\nlike matrix showing the “activity” of each spectral basis\nagainst time, that is itself discretized into successive time\nframe of analysis (of the order of magnitude of 11 ms).\nFrom this activity matrix, the next processing step in view\nof AMT is note segmentation, that aims to identify for each\npitch the time intervals when the notes are played. To per-\nform this operation, most spectrogram factorization-based\ntranscription methods [11, 15, 22] use a simple threshold-\nbased detection of the note activations from the pitch activ-\nity matrix, followed by a minimum duration pruning. One\nof the main drawback of this PLCA method with a simple\nthreshold is that all successive frame are processed inde-\npendently from one another, and thus temporal correlation\nbetween successive frames is not modeled. One solution\nthat has been proposed is to jointly learn spectral dictionar-\nies as well as a Markov chain that describes the structure\nof changes between these dictionaries [5, 22, 23].\nIn this paper, we will focus on the note segmentation\nstage, using a pitch-wise two-state on/off ﬁrst-order HMM,\ninitially proposed by Poliner et al. [24] for AMT. This\nHMM allows taking into account the dependence of pitch\nactivation across time frames. We review the formalism of\nthis model, including a full parametrization of the sigmoid\nfunction used to map HMM observation probabilities into\nthe[0;1]interval, with a term \u000bof slope smoothing and\n\fof thresholding contrast. After demonstrating the rel-\nevance of an optimal adjustment of these parameters for\nnote segmentation, a supervised approach to estimate the\nsigmoid parameters from a learning corpus is proposed.\nNote that Cheng et al. [8] explicitly modeled the different\nstages of a piano sound for note tracking, while we rather\nfocus on more general musical features such as dynam-\nics. Also, the Audio Degradation toolbox [19] was used\nto build three “degraded” sound datasets that have allowed\nto evaluate transcription performance on real life types of\naudio recordings, such as radio broadcast and MP3 com-\npressed audio, that are almost never dealt with in transcrip-\ntion studies.\n2. METHODS\n2.1 Background on PLCA\nPLCA is a probabilistic factorization method [26] based\non the assumption that a suitably normalized magnitude\nspectrogram, V, can be modeled as a joint distribution over\ntime and frequency, P(f;t), withfis the log-frequency\nindex andt= 1;:::;T the time index with Tthe number\nof time frames. This quantity can be factored into a frameprobabilityP(t), which can be computed directly from the\nobserved data (i.e. energy spectrogram), and a conditional\ndistribution over frequency bins P(fjt), as follows [7]\nP(fjt) =X\np;mP(fjp;m)P(mjp;t)P(pjt) (1)\nwhereP(fjp;m)are the spectral templates for pitch p=\n1;:::;Np(withNpthe number of pitches) and playing\nmodem,P(mjp;t)is the playing mode activation, and\nP(pjt)is the pitch activation (i.e. the transcription). In\nthis paper, the playing mode mwill refer to different play-\ning dynamics (i.e. note loudness). To estimate the model\nparametersP(mjp;t)andP(pjt), since there is usually\nno closed-form solution for the maximization of the log-\nlikelihood or the posterior distributions, iterative update\nrules based on the Expectation-Maximization (EM) algo-\nrithm [10] are employed (see [4] for details). The pitch\nactivity matrix P(p;t)is deduced from P(pjt)with the\nBayes’ rule\nP(p;t) =P(t)P(pjt) (2)\nPLCA note templates are learned with pre-recorded iso-\nlated notes, using a one component PLCA model (i.e. m=\n1 in Eq. (1). Three different note templates per pitch are\nused during MPE. In this paper, we use the PLCA-based\nMPE system developed by Benetos and Weyde [6]1.\nIn the following, for p= 1;:::;Npandt= 1;:::;T ,\nwe deﬁne the logarithmic pitch activity matrix as\nXp;t= log\u0000\nP(p;t)\u0001\n(3)\n2.2 Note Segmentation Strategies\n2.2.1 HT: Hard Thresholding\nThe note segmentation strategy HT consists of a sim-\nple thresholding \fHTof the logarithmic pitch activity ma-\ntrixX(p;t), as it is most commonly done in spectrogram\nfactorization-based transcription or pitch tracking systems,\ne.g. in [11, 15, 22]. This HT is sometimes combined with\na minimum duration constraint with typical post ﬁltering\nlike “all runs of active pitch of length smaller than k are set\nto 0”.\n2.2.2 ST: Soft Thresholding\nIn this note segmentation strategy, initially proposed by\nPoliner and Ellis [24], each pitch pis modelled as a two-\nstate on/off HMM, i.e. with underlying states qt2f0;1g\nthat denote pitch activity/inactivity. The state dynamics,\ntransition matrix, and state priors are estimated from our\n“directly observed” state sequences, i.e. the training MIDI\ndata, that are sampled at the precise times corresponding\nto the analysis frames of the activation matrix.\nFor each pitch p, we consider an independent HMM\nwith observations Xp;t, that are actually observed, and hid-\nden binary Markov sequence Q=q1;:::;qT, illustrated in\nﬁgure 1. The Markov model then follows the law:\n1Codes are available at https://code.soundsoftware.ac.\nuk/projects/amt_mssiplca_fast .524 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Xp;1Xp;2Xp;3 Xp;T\nq1q2q3 qT....\nFigure 1 . Graphical representation of the two-state on/off\nHMM.qt2f0;1gare the underlying states label at time t,\nandotthe the probability observations.\nP(Q;X )/P(q1)TY\nt=2P(qtjqt\u00001)TY\nt=1P(qtjXp;t)(4)\nwhere/means “proportional to”, as the probabilities do\nnot sum to 1. For t= 1;:::;T , we assume that:\nP(qt= 0jqt= 0) = 1\u0000\u001c0P(qt= 1jqt= 0) =\u001c0(5)\nP(qt= 0jqt= 1) =\u001c1P(qt= 1jqt= 1) = 1\u0000\u001c1(6)\nwith\u001c0;\u001c12[0;1]the transition probabilities, and the con-\nvention that q0= 0 because all notes are inactive at the\nbeginning of a recording. The transition probabilities \u001c\ncorrespond to the state transitions: on/on, on/off, off/on,\noff/off. Parameter \u001c0(resp.\u001c1) is directly related to the\nprior duration of inactivity (resp. activity) of pitch p. With-\nout observation, the length of an inactivity run (resp. activ-\nity run) would be geometric with parameter \u001c0(resp.\u001c1)\nwith average length 1=\u001c0(resp. 1=\u001c0).\nThe observation probabilities are deﬁned as follows, us-\ning a sigmoid curve with the PLCA pitch activity matrix\nXp;tas input,\nP(qt= 0jXp;t)/1=Z (7)\nP(qt= 1jXp;t)/e[e\u000b(Xp;t\u0000\f)]=Z(8)\nwith\u000b;\f2R, andZdeﬁned such asP\nqtP(qtjXp;t) =\nZ. The parameter of the model is denoted \u0012= (\u001c;\u000b;\f )\nwhich includes the speciﬁc value for all pitches. The HMM\nmodel is solved using classical forward-backward recur-\nsions for all t= 1;:::;T , i.e.P\u0012(qt=sjXp;t) =\u0011s(t)/\nFt(s)Bt(s).\nNote that the HMM deﬁnition combines both the spa-\ntial pitch dependence (the Markov model) with a PLCA\ngenerative model. As a result of this combination, the re-\nsulting model is deﬁned up to a constant factor, but this is\nnot a problem since we will exploit this model to compute\nposterior distribution. In contrast, in the initial model [24],\none should note that a similar model is suggested where the\nPLCA generative part is associated with the so-called “vir-\ntual observation”. We here preferred the fully generative\nformulation presented above, but both models are totally\nequivalent.\nUsing logarithmic values, the parameters f\u000b;\fg, ex-\npressed in dB, are directly interpretable by physics. \fis anoffset thresholding parameter, which allows separating sig-\nnal from noise (or in other words, i.e. the higher its value,\nthe more pitch candidates with low probability will be dis-\ncarded.), while \u000bis a contrast parameter, a value superior\nto 0 is used for a fast switch from noise to signal (i.e. low\ndegree of tolerance from threshold), and a value inferior\nto 0 for a smoother switch. Figure 2 shows a sigmoid\ncurve with different values of \fand\u000b. This suggested\nparametrizationf\u000b;\fgcan therefore be seen as a general-\nization of the initial [24]’s model.\nFigure 2 . Effects of the parameters \f(top) and\u000b(bottom)\non the theoretical sigmoid given by Eq. (8). On top, a ﬁxed\nvalue of 0 is set to \u000b, and on bottom, a ﬁxed value of -5 is\nset to\f.\nFor this note segmentation strategy ST, we use the set of\nparametersf\u000b;\fg=f0;\fHTg, as used in previous studies\n[5, 24].\n2.2.3 OST: Optimized Soft Thresholding\nThe note segmentation strategy OST is based on the same\nHMM model as the ST strategy, although the parameters\nf\u000b;\fgare now optimized for each pitch. Given the ground\ntruth of a musical sequence test, we use the Nelder-Mead\noptimizer of the R software to iteratively ﬁnd the optimal\nf\u000b;\fgparameters that provide the best transcription per-\nformance measure. The Nelder-Mead method is a simplex-\nbased multivariate optimizer known to be slow and impre-\ncise but generally robust and suitable for irregular and dif-\nﬁcult problems. For optimization, we use the Least Mean\nSquare Error (LMSE) metric between ground truths and\nmarginal posterior probabilities of pitch activation, as it al-\nlows to take into account the precise shape of activation\nproﬁles. Figure 3 provides an example of this optimization\nthrough the contour graph of the log10(LMSE )function.\nHowever, classical AMT error metrics (see Sec. 2.3.3) will\nbe used as display variables for graphics as they allow di-\nrect interpretation and comparison in terms of transcription\nperformance.\nIn real world scenarios of AMT, the ground truth of a\nmusical piece is never known in advance. A common strat-\negy to estimate model or prior knowledge parameters is to\ntrain them on a learning dataset that is somewhat similar to\nthe musical piece to be transcribed. This was done in this\nstudy for thef\u000b;\fgparameters, through a cross-validation\nprocedure with the LMSE-optimization (see Sec. 2.3.2).Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 525Figure 3 . Example of a data-based optimization of\nthef\u000b;\fgparameters through the contour graph of\nthelog10(LMSE )function, using the musical piece\nMAPS MUS-alb esp2 AkPnCGdD . The dashed white\nlines point to the local minimum.\n2.3 Evaluation Procedure\n2.3.1 Sound Dataset\nTo test and train the AMT systems, three different sound\ncorpus are required: audio musical pieces of an instrument\nrepertoire, the corresponding scores in the form of MIDI\nﬁles, and a complete dataset of isolated notes for this in-\nstrument. Audio musical pieces and corresponding MIDI\nscores were extracted from the MAPS database [12], be-\nlonging to the solo classical piano repertoire. The 56 musi-\ncal pieces of the two pianos labelled AkPnCGdD and EN-\nSTDkCl were used, and constituted our evaluation sound\ndataset called Baseline. The ﬁrst piano model is the virtual\ninstrument Akoustik Piano (concert grand D piano) devel-\noped by the software Native Instruments. The second one\nis the real upright piano model Yamaha Disklavier Mark\nIII. Three other sound datasets of musical pieces have then\nbeen deﬁned as follows:\n\u000fMP3 dataset. It corresponds to the same musical\npieces of the dataset Baseline, but modiﬁed with\nthe Strong MP3 Compression degradation from the\nAudio Degradation toolbox [19]. This degradation\ncompresses the audio data to an MP3 ﬁle at a con-\nstant bit rate of 64 kbps using the Lame encoder ;\n\u000fSmartphone dataset. It corresponds to the same mu-\nsical pieces of the dataset Baseline, but modiﬁed\nwith the Smartphone Recording degradation from\nthe Audio Degradation toolbox [19]. This degrada-\ntion simulates a user holding a phone in front of a\nspeaker: 1. Apply Impulse Response, using the IR\nof a smartphone microphone (“Google Nexus One”),\n2. Dynamic Range Compression, to simulate the\nphone’s auto-gain, 3. Clipping, 3 % of samples, 4.\nAdd Noise, adding medium pink noise ;\n\u000fVinyl dataset. It corresponds to the same musical\npieces of the dataset Baseline, but modiﬁed with\nthe Vinyl degradation from the Audio Degradation\ntoolbox [19]. This degradation applies an Impulse\nResponse, using a typical record player impulse re-\nsponse, adds Sound and record player crackle, aWow Resample, imitating wow-and-ﬂutter, with the\nwow-frequency set to 33 rpm (speed of Long Play\nrecords), and adds Noise and light pink noise.\nFor all datasets, isolated note samples were extracted\nfrom the RWC database (ref. 011, CD 1) [14].\n2.3.2 Cross-validation\nDuring a cross-validation procedure, the model is ﬁt to a\ntraining dataset, and predictive accuracy is assessed using\na test dataset. Two cross-validation procedures were used\nfor training thef\u000b;\fgparameters of the OST strategy, and\ntesting separately the three thresholding strategies. The\nﬁrst one is the “leave-one-out” cross-validation procedure,\nusing only one musical piece for parameter training and\ntesting all others. This process is iterated for each musical\npiece. The second one is a repeated random sub-sampling\nvalidation, also known as Monte Carlo cross-validation. At\neach iteration, the complete dataset of musical pieces is\nrandomly split into training and test data accordingly to a\ngiven training/test ratio. The results are then averaged over\nthe splits. The advantage of this method (over k-fold cross\nvalidation) is that the proportion of the training/test split is\nnot dependent on the number of iterations (folds). A num-\nber of 20 iterations was used during our simulations. We\nalso tested different training/test ratio, ranging from 10/90\n% to 80/20 % in order to evaluate the inﬂuence of the train-\ning dataset on transcription performance.\n2.3.3 Evaluation Metrics\nFor assessing the performance of our proposed transcrip-\ntion system, frame-based evaluations are made by com-\nparing the transcribed output and the MIDI ground-truth\nframe by frame using a 10 ms scale as in the MIREX\nmultiple-F0estimation task [1]. We used the frame-based\nrecall (TPR), precision (PPV), the F-measure (FMeas) and\nthe overall accuracy (Acc)\nTPR =PT\nt=1TP[t]\nPT\nt=1TP[t] +FN[t](9)\nPPV =PT\nt=1TP[t]\nPT\nt=1TP[t] +FP[t](10)\nFMeas =2:PPV:TPR\nPPV+TPR(11)\nAcc=PT\nt=1TP[t]\nPT\nt=1TP[t] +FP[t] +FN[t](12)\nwhereTis the total number of time frames, and TP [t],\nTN[t], FN[t]and FP [t]are the numbers of true positive, true\nnegative, false negative and false positive pitches at frame\nt. The recall is the ratio between the number of relevant\nand original items; the precision is the ratio between the\nnumber of relevant and detected items; and the F-measure\nis the harmonic mean between precision and recall. For all\nthese evaluation metrics, a value of 1 represents a perfect\nmatch between the estimated transcription and the refer-\nence one.526 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20172.3.4 MPE Algorithms on the Benchmark\nIn this study, we tested the four following MPE algorithms:\n\u000fTolonen2000, this algorithm2[27] is an efﬁcient\nmodel for multipitch and periodicity analysis of\ncomplex audio signals. The model essentially di-\nvides the signal into two channels, below and above\n1000 Hz, computes a “generalized” autocorrelation\nof the low-channel signal and of the envelope of the\nhigh-channel signal, and sums the autocorrelation\nfunctions ;\n\u000fEmiya2010, this algorithm3[12] models the spec-\ntral envelope of the overtones of each note with a\nsmooth autoregressive model. For the background\nnoise, a moving-average model is used and the com-\nbination of both tends to eliminate harmonic and\nsub-harmonic erroneous pitch estimations. This\nleads to a complete generative spectral model for\nsimultaneous piano notes, which also explicitly in-\ncludes the typical deviation from exact harmonicity\nin a piano overtone series. The pitch set which max-\nimizes an approximate likelihood is selected from\namong a restricted number of possible pitch combi-\nnations as the one ;\n\u000fHALCA, the Harmonic Adaptive Latent Compo-\nnent Analysis algorithm4[13] models each note in\na constant-Q transform as a weighted sum of ﬁxed\nnarrowband harmonic spectra, spectrally convolved\nwith some impulse that deﬁnes the pitch. All param-\neters are estimated by means of the EM algorithm,\nin the PLCA framework. This algorithm was evalu-\nated by MIREX and obtained the 2ndbest score in\nthe Multiple Fundamental Frequency Estimation &\nTracking task, 2009-2012 [1] ;\n\u000fBenetos2013, this PLCA-based MPE system5[3]\nuses pre-ﬁxed templates deﬁned with real note sam-\nples, without updating them in the maximization\nstep of the EM algorithm. It has been ranked ﬁrst\nin the MIREX transcription tasks [1].\n2.4 Setting the HT Threshold Value\nWe need to deﬁne the threshold value \fHTused in the note\nsegmentation strategies HT and ST. Although most studies\nin AMT literature [11, 15, 22] use this note segmentation\nstrategy, threshold values are barely reported and proce-\ndures to deﬁne them have not yet been standardize. Most\nof the time, one threshold value is computed across each\nevaluation dataset, which is dependent on various parame-\nters of the experimental set-up, such as the used evaluation\nmetric, input time-frequency representation, normalization\n2We used the source code implemented in the MIR toolbox [17],\ncalled mirpitch(..., ’Tolonen’).\n3Source code courtesy of the primary author.\n4Source codes are available at http://www.benoit-fuentes.\nfr/publications.html .\n5Source codes are available at https://code.\nsoundsoftware.ac.uk/projects/amt_mssiplca_fast .of input waveform. In this paper, we will use a similar\nempirical dataset-based approach to deﬁne the HT thresh-\nold value. ROC curves (True Positives against False Pos-\nitives) are computed over the threshold range [0 ; -5] dB\nso as to choose the value that maximizes True Positive and\nminimizes False Positives, i.e. that increases transcription\nperformance at best over each dataset.\n3. RESULTS AND DISCUSSION\nAll following results on transcription performance have\nbeen obtained using the Benetos2013 MPE system, ex-\ncept for ﬁgure 6 where all MPE systems are comparatively\nevaluated. Figure 4 represents the boxplots of the optimal\nf\u000b;\fgvalues obtained for each pitch. The “leave-one-out”\ncross-validation procedure has been applied to the different\ndatasets, from top to bottom. For each dataset, we can see\nthat the data-based pitch-wise optimization leads to \fval-\nues drastically different from the threshold value \fHTused\nin the ST and HT thresholding strategies (represented by\nthe horizontal red lines). Differences range from 0.5 to 2\ndB, that have an important impact for note segmentation.\nSlighter differences are observed in values of \u000b, although\nslightly positive values of \u000b(around + 1 dB) tend to con-\ntribute to reduce the LMSE metric used in optimization.\nAlso, note that optimal \fHTvalues are also dependent on\nthe datasets, varying from -1.8 to -2.8 dB.\nNow, let’s see how this optimization of f\u000b;\fgin the\nmethod OST impacts real transcription performance. Ta-\nble 1 shows transcription results obtained with the “leave-\none-out” cross-validation procedure, applied to the differ-\nent thresholding strategies. In comparison to the meth-\nods HT and ST, important gains in transcription perfor-\nmance are brought by the proposed method OST. These\ngains are the highest for the baseline dataset D1, in the or-\nder of magnitude of 5 to 8 % for the two metrics Acc and\nFMeas. They remain systematically positive for the other\ndatasets, with a minimum gain of 4 % whatever the dataset,\nerror metric and compared thresholding strategy. Alto-\ngether, these gains are very signiﬁcant in regards to com-\nmon gains in transcription performance reported in litera-\nture, and demonstrate the validity of our proposed method.\nIn Figure 5, we evaluated the dependency of transcrip-\ntion performance on the training dataset size, through\na Monte Carlo cross-validation procedure with different\ntraining/test ratios, ranging from 10 to 60 % of the com-\nplete dataset of musical pieces, plus the “leave-one-out”\n(labelled LOM) ratio. This ﬁgure shows that increasing\nthe size of the training set directly induces average tran-\nscription gains from 0.5 to 6 % of the metric FMeas with\nthe OST method, in comparison to the HT method. We\nnote that once the curves reach the 60/40 % training/test\nratio, all systems ﬁnd a quick convergence to the gain ceil-\ning achieved with the LOM ratio.\nEventually, we studied the dependency of OST tran-\nscription performance on the MPE system used, in com-\nparison to the method HT. Figure 6 shows the differences\nbetween the FMeas obtained with the methods OST and\nHT. We can observe that these differences are relativelyProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 527Figure 4 . Boxplots of the optimal f\u000b;\fgvalues obtained\nfor each pitch, and for each evaluation dataset. The hor-\nizontal red lines in each boxplot represents the parameter\nvalues used in the ST and HT thresholding strategies.\nFigure 5 . Difference between the F-measures obtained\nwith the OST and HT note segmentation methods, using\n20 iterations of the repeated random sub-sampling valida-\ntion method with training/test ratio ranging from 10/90 %\nto60/40 %, plus the “leave-one-out” (labelled LOM) ratio.DatasetsNote segmentation\nstrategiesAcc (%) Fmeas (%)\nBaselineHT 54.9 53.3\nST 57.6 55.3\nOST 62.3 59.2\nMP3HT 51.9 52.6\nST 52.2 50.1\nOST 55.6 56.7\nSmartphoneHT 52.2 51.9\nST 53.1 51.3\nOST 58.4 56.5\nVinylHT 50.8 48.8\nST 51.1 49.2\nOST 57.8 54.1\nTable 1 . Averages of error metrics FMeas and Acc ob-\ntained with the different thresholding strategies, i.e. ST,\nOST and HT, using a leave-one-out cross-validation pro-\ncedure.\nsmall, i.e. inferior to 2 %. This demonstrates that the pro-\nposed OST method improves transcription performance in\na rather universal way, as independent from the character-\nistics of activation matrices as long as MPE system spe-\nciﬁc training datasets are used. Only MPE system Tolo-\nnen2000 shows higher transcription gains (especially for\nthe datasets D3andD4) brought by the OST method as\nthis system outputs the worst activation matrices.\nFigure 6 . Difference between the F-measures obtained\nwith the OST and HT note segmentation methods, using\ndifferent MPE systems.\n4. CONCLUSION\nIn this study, an original method for the task of note seg-\nmentation was presented. This task is a crucial process-\ning step in most systems of automatic music transcription.\nThe presented method is based on a two-state pitch-wise\nHidden Markov Model method, augmented with two sig-\nmoid parameters on contrast and slope smoothing that are\ntrained with a learning dataset. This rather simple method\nhas brought signiﬁcant results in transcription performance\non piano music datasets with different characteristics. It\ncan also be used as a universal post-processing block after\nany pitch-wise activation matrix, showing great promise\nfor future use, although it remains to be tested on different\ninstrument repertoires.528 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20175. REFERENCES\n[1] MIREX (2007). Music information retrieval evalua-\ntion exchange (mirex). 2011. available at http://music-\nir.org/mirexwiki/ (date last viewed January 9, 2015).\n[2] V . Arora and L. Behera. Instrument identiﬁcation us-\ning PLCA over stretched manifolds. In Communica-\ntions (NCC), 2014 Twentieth National Conference on ,\npages 1–5, Feb 2014.\n[3] E. Benetos, S. Cherla, and T. Weyde. An efﬁcient shift-\ninvariant model for polyphonic music transcription. In\n6th Int. Workshop on Machine Learning and Music,\nPrague, Czech Republic , 2013.\n[4] E. Benetos and S. Dixon. A shift-invariant latent vari-\nable model for automatic music transcription. Com-\nputer Music Journal , 36:81–84, 2012.\n[5] E. Benetos and S. Dixon. Multiple-instrument poly-\nphonic music transcription using a temporally con-\nstrained shift-invariant model. J. Acoust. Soc. Am. ,\n133:1727–1741, 2013.\n[6] E. Benetos and T. Weyde. An efﬁcient temporally-\nconstrained probabilistic model for multiple-\ninstrument music transcription. In 16th International\nSociety for Music Information Retrieval Conference,\nMalaga , Spain , pages 355–360, 2015.\n[7] D. Cazau, O. Adam, J. T. Laitman, and J. S. Reiden-\nberg. Understanding the intentional acoustic behavior\nof humpback whales: a production-based approach. J.\nAcoust. Soc. Am. , 134:2268–2273, 2013.\n[8] T. Cheng, S. Dixon, and M. Mauch. Improving piano\nnote tracking by HMM smoothing. In 23th EUSIPCO\nconference , pages 2054–2058, 2015.\n[9] O. Cornelis, M. Lesaffre, D. Moelants, and M. Le-\nman. Access to ethnic music: Advances and perspec-\ntives in content-based music information retrieval. Sig-\nnal Proc. , 90:1008–1031, 2010.\n[10] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-\nmum likelihood from incomplete data via the em algo-\nrithm. Journal of the Royal Statistical Society, Series\nB, 39:1–38, 1977.\n[11] A. Dessein, A. Cont, and G. Lemaitre. Real-time\npolyphonic music transcription with nonnegative ma-\ntrix factorization and beta-divergence. In 11th Interna-\ntional Society for Music Information Retrieval Confer-\nence, Utretcht, Netherlands , pages 489–494, 2010.\n[12] V . Emiya, R. Badeau, and G. Richard. Multipitch es-\ntimation of piano sounds using a new probabilistic\nspectral smoothness principle. IEEE Trans. on Audio,\nSpeech, Lang. Proc. , 18:1643–1654, 2010.\n[13] B. Fuentes, R. Badeau, and G. Richard. Harmonic\nadaptive latent component analysis of audio and ap-\nplication to music transcription. IEEE Trans. on Audio\nSpeech Lang. Processing , 21:1854–1866, 2013.[14] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRwc music database: Popular, classical, and jazz music\ndatabases. In 3rd International Conference on Music\nInformation Retrieval, Baltimore,MD. , pages 287–288,\n2003.\n[15] G. Grindlay and D. P. W. Ellis. Transcribing\nmulti-instrument polyphonic music with hierarchical\neigeninstruments. IEEE J. Sel. Topics Signal Proc. ,\n5:1159–1169, 2011.\n[16] A. Klapuri. Automatic music transcription as we know\nit today. J. of New Music Research , 33:269–282, 2004.\n[17] O. Lartillot and P. Toiviainen. A matlab toolbox for\nmusical feature extraction from audio. In Proc. of the\n10th Int. Conference on Digital Audio Effects (DAFx-\n07), Bordeaux, France, September 10-15, 2007 , 2007.\n[18] T. Lidy, C. N. Silla, O. Cornelis, F. Gouyon, A. Rauber,\nC. A. A. Kaestner, and A. L. Koerich. On the suitability\nof state-of-the-art music information retrieval methods\nfor analyzing, categorizing and accessing non-western\nand ethnic music collections. Signal Proc. , 90:1032–\n1048, 2010.\n[19] M. Mauch and S. Ewert. The Audio Degradation tool-\nbox and its application to robustness evaluation. In 14th\nInternational Society for Music Information Retrieval\nConference, Curitiba, PR, Brazil , pages 83–88, 2013.\n[20] D. Moelants, O. Cornelis, M. Leman, J. Gansemans,\nR. T. Caluwe, G. D. Tr ´e, T. Matth ´e, and A. Hallez. The\nproblems and opportunities of content-based analysis\nand description of ethnic music. International J. of In-\ntangible Heritage , 2:59–67, 2007.\n[21] J. A. Moorer. On the transcription of musical sound by\ncomputer. Computer Music Journal , 1:32–38, 1977.\n[22] G. J. Mysore and P. Smaragdis. Relative pitch esti-\nmation of multiple instruments. In International Con-\nference on Acoustical Speech and Signal Processing,\nTaipei, Taiwan , pages 313–316, 2009.\n[23] M. Nakano, J. Le Roux, H. Kameoka, O. Kitano,\nN. Ono, and S. Sagayama. Nonnegative matrix factor-\nization with markov-chained bases for modeling time-\nvarying patterns in music spectrograms. In LVA/ICA\n2010, LNCS 6365, V . Vigneron et al. (Eds.) , pages 149–\n156, 2010.\n[24] G. Poliner and D. Ellis. A discriminative model for\npolyphonic piano transcription. J. on Advances in Sig-\nnal Proc. , 8:1–9, 2007.\n[25] J. Six and O. Cornelis. Computer-assisted transcription\nof ethnic music. In 3th International Workshop on Folk\nMusic Analysis, Amsterdam, Netherlands , pages 71–\n72, 2013.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 529[26] P. Smaragdis, B. Raj, and M. Shanshanka. A proba-\nbilistic latent variable model for acoustic modeling. In\nNeural Information Proc. Systems Workshop, Whistler,\nBC, Canada , 2006.\n[27] T. Tolonen and M. Karjalainen. A computationally efﬁ-\ncient multipitch analysis model. IEEE Trans. on speech\nand audio processing , 8:708–716, 2000.530 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "From Bach to the Beatles: The Simulation of Human Tonal Expectation Using Ecologically-Trained Predictive Models.",
        "author": [
            "Carlos Eduardo Cancino Chacón",
            "Maarten Grachten",
            "Kat Agres"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416886",
        "url": "https://doi.org/10.5281/zenodo.1416886",
        "ee": "https://zenodo.org/records/1416886/files/ChaconGA17.pdf",
        "abstract": "Tonal structure is in part conveyed by statistical regularities between musical events, and research has shown that com- putational models reflect tonal structure in music by cap- turing these regularities in schematic constructs like pitch histograms. Of the few studies that model the acquisi- tion of perceptual learning from musical data, most have employed self-organizing models that learn a topology of static descriptions of musical contexts. Also, the stimuli used to train these models are often symbolic rather than acoustically faithful representations of musical material. In this work we investigate whether sequential predictive models of musical memory (specifically, recurrent neural networks), trained on audio from commercial CD record- ings, induce tonal knowledge in a similar manner to listen- ers (as shown in behavioral studies in music perception). Our experiments indicate that various types of recurrent neural networks produce musical expectations that clearly convey tonal structure. Furthermore, the results imply that although implicit knowledge of tonal structure is a neces- sary condition for accurate musical expectation, the most accurate predictive models also use other cues beyond the tonal structure of the musical context.",
        "zenodo_id": 1416886,
        "dblp_key": "conf/ismir/ChaconGA17",
        "keywords": [
            "tonal structure",
            "statistical regularities",
            "computational models",
            "pitch histograms",
            "acquisition of perceptual learning",
            "self-organizing models",
            "topology of static descriptions",
            "musical contexts",
            "recurrent neural networks",
            "audio from commercial CD recordings"
        ],
        "content": "FROM BACH TO THE BEATLES: THE SIMULATION OF HUMAN TONAL\nEXPECTATION USING ECOLOGICALLY-TRAINED PREDICTIVE\nMODELS\nCarlos Cancino-Chac ´on1;2Maarten Grachten2Kat Agres3\n1Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria\n2Department of Computational Perception, Johannes Kepler University, Linz, Austria\n3Institute of High Performance Computing, A*STAR, Singapore\ncarlos.cancino@ofai.at, maarten.gracthen@ofai.at, kat agres@ihpc.a-star.edu.sg\nABSTRACT\nTonal structure is in part conveyed by statistical regularities\nbetween musical events, and research has shown that com-\nputational models reﬂect tonal structure in music by cap-\nturing these regularities in schematic constructs like pitch\nhistograms. Of the few studies that model the acquisi-\ntion of perceptual learning from musical data, most have\nemployed self-organizing models that learn a topology of\nstatic descriptions of musical contexts. Also, the stimuli\nused to train these models are often symbolic rather than\nacoustically faithful representations of musical material.\nIn this work we investigate whether sequential predictive\nmodels of musical memory (speciﬁcally, recurrent neural\nnetworks), trained on audio from commercial CD record-\nings, induce tonal knowledge in a similar manner to listen-\ners (as shown in behavioral studies in music perception).\nOur experiments indicate that various types of recurrent\nneural networks produce musical expectations that clearly\nconvey tonal structure. Furthermore, the results imply that\nalthough implicit knowledge of tonal structure is a neces-\nsary condition for accurate musical expectation, the most\naccurate predictive models also use other cues beyond the\ntonal structure of the musical context.\n1. INTRODUCTION AND RELATED WORK\nComputers are increasingly being used to perform music-\nrelated tasks (automated music analysis, music recommen-\ndation, composition, etc). To perform such tasks reliably,\nthere is a need for computers to grasp concepts that are rel-\nevant to our perception and understanding of music [37].\nEmpirical ﬁndings from music psychology are valuable in\nthis respect, since they shed light on the process of human\nmusic perception and cognition.\nc\rCarlos Cancino-Chac ´on, Maarten Grachten, Kat Agres.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Carlos Cancino-Chac ´on, Maarten\nGrachten, Kat Agres. “From Bach to the Beatles: The simulation of\nhuman tonal expectation using ecologically-trained predictive models”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.We know from extensive research in music psychology\nthat listeners implicitly extract statistical properties gov-\nerning tonal structure through exposure to music [3,19,29].\nThe tonal stability , or relative importance, of notes in a\nkey may be largely due to the frequency of occurrence\nof pitches in a piece of music. The more foundational\npitches (e.g., C, E, and G in the key of C major) will tend\nto be anchor points in the music, and will often occur on\nmetrically-important positions [21, 26].\nThrough exposure to these kinds of melodic (and\nharmonic) statistical properties, listeners form an im-\nplicit mental model of tonality. Evidence for this has\nbeen provided, for example, through the seminal work\nof Krumhansl and colleagues employing a ‘probe-tone\nparadigm’, in which listeners rate how well the last pitch,\nor probe-tone, of a musical sequence ﬁts in with the previ-\nous context. When provided with a tonal context, such as\nan ascending or descending musical scale, listeners per-\nceive certain pitches as sounding more appropriate than\nothers [19, 21, 22]. The proﬁle of listeners’ ratings of\nprobe-tones reﬂects a tonal hierarchy, and it is this hier-\narchy of pitch stabilities that plays a large role in govern-\ning tonal perception. The extent to which different music\nlistening behaviors and one’s musical ‘culture’ inﬂuence\ntonal perception is an open question, although evidence ex-\nists that Western classical music training results in differ-\nentiated, and often more nuanced, pitch expectations and\nprobe-tone proﬁles [4, 10, 20, 34].\nTo model these types of ﬁndings, computational mod-\nels of tonal perception typically aim to provide methods\nthat, given a musical context, compute a response that can\nbe judged to be more or less appropriate for the implicit\ntonality of that context. Given the predominance of the\nprobe-tone paradigm for studies of human tonal percep-\ntion, a common practice is to elicit a quasi -goodness-of-ﬁt\nresponse from the model for a probe-tone given a musical\nstimulus, such that the responses can be compared to hu-\nman probe-tone ratings (e.g. [6, 23, 25, 35]). Another way\nto judge the responses is to deﬁne a metric over the re-\nsponses and compare the resulting topology to geometric\nconstructs from music theory, such as the Tonnetz [36], a\ntoroidal representation of key distance [18], or the circle of494ﬁfths [6].\nThe computational models proposed in the literature\ntend to emphasize one of various different factors that are\nthought to play a role in tonal perception. Whereas some\nworks seek to explain empirical results mainly by a com-\nputational account of the lower levels of the auditory sys-\ntem [23,25], others focus more strongly on the role of long-\nterm memory in tonal perception [6, 24, 35].\nOf the models that involve some representation of long-\nterm memory, most do not account for that representation\nin an ecologically plausible manner, meaning that there is\nno plausible simulation of how the long-term memory rep-\nresentations come about as a result of long-term exposure\nto music. First, long-term memory is usually modeled by\nsome form of self-organization of static representations of\nmusical contexts or events, producing a low-dimensional\nmap of musical stimuli, in which the neighborhood re-\nlationship captures semantic information (such as tonal\nafﬁnity) [6, 24, 35, 36]. Although the principle of self-\norganization has been used to account for the structure of\ncortical maps such as those in the visual cortex [12], there\nis no evidence that this principle also underpins long-term\nmemory. Moreover, the fact that musical contexts are be-\ning mapped as static entities is at odds with the fundamen-\ntally temporal nature of the music listening process. As\nformalized in the predictive coding framework [13], an in-\ncreasingly prominent idea in cognitive science is that of an-\nticipation as a universal driving force for cognition [8, 11].\nMusic researchers have also focused on the temporal\ndynamics of tonal and harmonic expectations (e.g., [32]\nand [30]), and some models based on self-organizing maps\n(SOMs) [17] do account for effects of temporal order in\nmusical listening [25, 35]. A limitation, however, is that\nthese effects are not taken into account in the training of\nthe maps, representing the learning process that forms long\nterm memory of music. Toiviainen and Krumhansl [36]\nalso employ a SOM, but, as they state, use it for visualiza-\ntion purposes, and not “to simulate any kind of perceptual\nlearning that would occur in listeners through, for instance,\nthe extraction of regularities present in Western music.”\nAlthough the model offered by [9] does learn from musi-\ncal sequences to predict tonal expectation in listeners, the\nmodel itself does not use sequential tonal information to\nlearn and drive its predictions.\nAnother limitation of most long-term memory models\nfor tonal learning is that they work with stimuli that are re-\nduced in one or more ways. For example, the input may\nconsist of discrete representations of tones such as MIDI\nnote numbers [6], pitch classes [35], artiﬁcial harmonic\nrepresentations [2], or of artiﬁcial harmonic sounds such\nas Shepard tones [25]. Furthermore, the musical mate-\nrial that a model is exposed to may be limited to mono-\nphonic melodic lines [6], sets of chords or harmonic ca-\ndences [25], or even a set of probe-tone proﬁles [36]. A no-\ntable exception to this is [24], which uses an audio record-\ning of Bach’s Well Tempered Clavier (WTC), performed\non a harpsichord, to train a SOM by converting the acous-\ntic signal to auditory images . The work of [9] also takes anecological approach by using real audio and plausible psy-\nchological representations, with multiple representations\nalong the sensory-cognitive spectrum, to better account for\nhuman tonal expectation.\nThe central question of this work is whether sequen-\ntial predictive models of musical memory induce memory\nrepresentations that convey tonal structure, similar to the\nstatic self-organizing models that are predominant in com-\nputational modeling of tonal learning. To answer this ques-\ntion, we employ Recurrent Neural Networks (RNNs) and\nvariants such as Long Short Term Memory (LSTM) [16],\nwhich provide a common and effective modeling approach\nto the task of predicting future input from a history of past\ninputs. A further objective is to see whether tonal expec-\ntations can also be elicited in the models by training on\necologically valid musical data rather than artiﬁcial data.\nThe present work approaches ecological validity in four\nways: 1) using commercial audio recordings rather than\nsymbolic or reduced music, 2) employing a psychoacous-\ntically plausible input representation (the Constant-Q rep-\nresentation), 3) training corpora that span more than one\ngenre (Bach and the Beatles) to better reﬂect a lister’s mu-\nsical experience, and 4) using more than one key to train\nthe model (much related research transposes the training\ndataset to one key, e.g. [1, 6]). We test the effect of the\ntraining data on the strength and character of the tonal ex-\npectations of the model. Furthermore we measure the im-\npact of shufﬂing the training data to gauge the importance\nof the sequential order of the music. Finally, we investigate\nthe relationship between the training objective of the mod-\nels (to predict the immediate future based on the present\nand past), and the strength of tonal hierarchy in the model\nexpectations.\nThe paper is structured as follows. In Section 2, we\nprovide a brief description of both the audio representa-\ntion and of the predictive RNN models used in our experi-\nments. Section 3 brieﬂy reviews the datasets used to train\nthe RNN models, and presents and discusses a comparison\nof model predictions to the results of probe-tone experi-\nments. Finally, conclusions and future work are presented\nin Section 4.\n2. METHOD\nIn this Section we describe the predictive models we use\nfor our experiment (Section 2.2), and the audio representa-\ntion used to present the data to the models (Section 2.1).\n2.1 Constant-Q Transform\nThe Constant-Q Transform (CQT) [5] is a discrete fre-\nquency domain representation of audio. Although the\nCQT was not conceived explicitly as a model of the hu-\nman auditory periphery, it shares an important character-\nistic with such models in that it samples the frequency\naxis logarithmically—a psychoacoustically plausible fea-\nture, since human listeners tend to perceive pairs of tones\nas equidistant when their respective frequency ratios are\nequal. The CQT is widely used in applications involvingProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 495musical audio, since its frequency bins can be conﬁgured\nto match the 12 tone octave division of Western music. To\nobtain a CQT spectrogram, conveying the change in fre-\nquency content of audio over time, the CQT can be com-\nputed over series of consecutive short, windowed segments\nof the audio, analogous to the Short-Time Fourier Trans-\nform.\n2.2 Recurrent Neural Networks\nAn RNN is a neural architecture that allows for modeling\ndynamical systems [15]. Let x1;:::;xtbe a sequence of\nN-dimensional (normalized) input vectors and y1;:::;yt\nbe its corresponding sequence of outputs. An RNN pro-\nvides a natural way to model xt+1, the next event in the se-\nquence, by using the outputs of the network to parametrize\na predictive distribution given by\np(xt+1;ijxt;:::;x1) =yt;i (1)\nwherext+1;iandyt;iare thei-th component of xt+1and\nytrespectively.\nThe basic component of an RNN is the recurrent layer ,\nwhose activation at time tdepends on both the input at\ntimetand its activation at time t\u00001. Although theo-\nretically very powerful, in practice RNNs with vanilla re-\ncurrent layers are known to have problems learning long\nterm dependencies due to a number of problems, includ-\ning vanishing and exploding gradients [27]. Other recur-\nrent layers such as LSTM layers [16] and gated recurrent\nunits (GRUs) [7] try to address some of these problems\nby introducing special structures within the layer, such as\npurpose-built memory cells and gates to better store infor-\nmation. More recently, recurrent layers with multiplicative\nintegration (MI-RNNs) [38] have been shown to extend the\nexpressivity of traditional additive RNNs by changing the\nway the information from different sources is aggregated\nwithin the layer while introducing just a small number of\nextra parameters.\nGiven a training set consisting of inputs and targets, the\nparameters of an RNN can be learned in a supervised fash-\nion by minimizing the cross entropy ( CE) between its pre-\ndictions and the targets.\nA more thorough description of RNNs lies outside of\nthe scope of this paper. For a more mathematical formula-\ntion of LSTMs and GRUs, we refer the reader to [7,15]. A\nmore detailed description of MI-RNNs can be found in the\nAppendix of [38].\n3. EXPERIMENTS\nIn this Section we describe the two datasets used for the\nexperiments in this paper (Section 3.2) and brieﬂy review\nthe theoretical framework of probe-tone experiments (Sec-\ntion 3.1), as well as a description of the training procedure\n(Section 3.3). In Section 3.4 the results of the probe-tone\nexperiments are presented and discussed.3.1 Probe-Tone Experiments\nA probe-tone test is an experimental framework to quan-\ntitatively assess the hierarchy of tonal stability [19]. This\nexperimental framework consists of a set of musical stim-\nuli like scales or cadences that unambiguously instantiate\na speciﬁc musical context, such as a key. After presenting\nthe stimulus, a participant hears a set of probe-tones, usu-\nally the set of 12 pitch classes, and the participant, either\na human participant or a computer model, is asked to rate\non quantitatively how well the probe-tones ﬁt the musical\nstimulus.\nLetX=fx1;\u0001\u0001\u0001xTgbe an input musical stimulus,\nandT=f\u001c1;:::;\u001c12gthe set of probe-tones each cor-\nresponding to one of the 12 pitch classes. In order to\nquantitatively assess how well a probe-tone \u001cﬁts the mu-\nsical stimulus, we compare y\u0003, the predictions of the RNN\ngiven the input stimulus, and the probe-tone using the\nKullback-Leibler (KL) divergence.\nIn this paper, we use the above described model to re-\nproduce the classic Krumhansl and Kessler (KK) probe-\ntone experiment [18]. This study is interesting for us\nmainly because 1) the probe tone contexts are polyphonic,\nfeaturing scales, chords, and cadences, thus highlighting\ncapability of the proposed model to process polyphonic\ndata, and 2) only expert listeners were tested (the partic-\nipants of this experiment had an average of 11 years of\nformal music education), allowing us to directly compare\nthe expectations of the model to those of an expert listener.\nThe setup for this experiment requires a set of 14 tonal\ncontexts1: ascending major and (harmonic) minor scales,\nthree chord cadences (II-V-I, IV-V-I, VI-V-I) in both ma-\njor and minor and individual chords (major triad, minor\ntriad, dominant seventh chord and diminished chord). In\nour experiments, we transpose each context to every key,\nyielding 12 variants of each context. In order to aggregate\nthe results over all keys, we average the KL divergence for\neach context.\nFollowing the original experimental setup, both stimuli\nand probe-tones are generated using Shepard-tones, which\nconsists of ﬁve sine wave components in a ﬁve-octave\nrange from 77.8 Hz to 2349 Hz, with an amplitude en-\nvelope such that the low and high ends of the range ap-\nproached hearing threshold [19].\nWe use Pearson’s correlation coefﬁcient to compare the\ngoodness-of-ﬁt of the probe-tones learned by the models\nwith the KK probe-tone ratings.\n3.2 Datasets\nThe WTC is a collection of 96 pieces for solo keyboard,\nconsisting of two sets of 24 Preludes and Fugues in each\nkey. Composed by Johann Sebastian Bach, the WTC is\nwidely recognized as one of the most important works in\nWestern music. We use a performance of the WTC by\nrenowned Canadian pianist Angela Hewitt2. The total\nduration of this recording is 4.5 hours. We perform data\n1See Table 1 in [18].\n2Hyperion CDS44291/4 1998496 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017augmentation on the WTC dataset by pitch shifting each\nrecording between \u00006and+5semitones using pyrubber-\nband3. We thus obtain 1152 pieces for the WTC, equiva-\nlent to nearly 53 hours of music.\nAdditionally, we use a second dataset consisting of 12\nAlbums by The Beatles, with a total of 179 songs with an\napproximate duration of 7.5 hours. We do not perform data\naugmentation on the Beatles data4.\nTo facilitate the exposure of the models to regularities\nin the change of pitch content over time, we do not com-\npute the CQT spectrograms by taking equidistant frames\nin absolute time, but instead link the spectrogram frame\nrate to the musical time, such that the instantaneous frame\nrate is always an integer multiple or submultiple of the\nbeat rate. For the Beatles data, we do so by using pub-\nlicly available beat annotations5. For the WTC record-\ning by Hewitt no such annotations were available, but ver-\nsions in Humdrum format of the pieces were obtained from\nKernScores6. The Humdrum ﬁles were converted into\nMIDI ﬁles, which were manually edited using MuseScore\nto match the repetitions as performed by Hewitt. By align-\ning piano-synthesized audio renderings of the MIDI ﬁles to\nthe Hewitt recordings using the method described in [14],\nbeat times were automatically inferred for the recordings.\nBased on the typical temporal densities of musical\nevents in the two datasets, we chose a temporal resolution\nof a quarter beat for the CQT spectrogram in the case of\nthe Beatles, and a sixteenth beat in the case of WTC. We\nwill return to this issue in Section 3.3.1.\nEach slice of the CQT spectrogram is a 334-\ndimensional vector that represents frequencies between\n27.5 and 16744.04 Hz with a resolution of 36 frequency\nbins per octave. This conﬁguration was chosen to avoid\nspectral leakage between adjacent frequency bins, and is\nsimilar to the one used by Purwins et. al. [28]. Addition-\nally, this conﬁguration is also able to accommodate at least\nthe fundamental frequency plus at least three harmonics to\nthe highest note of a piano. We normalize each slice of the\nCQT to lie between 0 and 1.\n3.3 Training\nFor the experiments in this paper we use RNNs as de-\nscribed in Section 2.2 as a sequential alternative to the\nstatic models typically used for tonal learning, such as\nSOMs and RBMs. To get an impression of the perfor-\nmance of sequential models in general for this task, we\ntest ﬁve different variants of the recurrent layer, namely\na vanilla RNN (vRNN), an LSTM, a GRU, and two mod-\nels with multiplicative integration: a vanilla recurrent layer\n(vRNN/MI) and an LSTM/MI7. In all variants, the model\n3https://github.com/bmcfee/pyrubberband Accessed April 2017.\n4Exploratory experiments showed that using pitch shifting on the\nBeatles songs worsened the predictions of the RNNs. This worsening\nmight be due to the fact that most of these recordings include several in-\nstruments and voices, including unpitched percussion instruments.\n5http://isophonics.net/content/reference-annotations-beatles .\nAccessed April 2017.\n6http://kern.ccarh.org . Accessed April 2017.\n7In the current experiments the GRU/MI yielded pathological results,\npossibly due to an implementation problem.has a single hidden recurrent layer with 75 tanh units and\nan output layer with sigmoid units. The use of different\nmodel variants also allows us to investigate the relationship\nbetween the prediction error and the similarity of model\nexpectations to human goodness-of-ﬁt ratings of probe-\ntones.\nIn order to investigate the kind of statistical regularities\nin music that produce human-like probe-tone results, we\ntrain each model on two different versions of each dataset,\nnamely training the model using the original data, and\ntraining the model shufﬂing the spectrograms in a piece-\nwise fashion. Randomizing inputs per piece preserves the\nglobal pitch distribution of the piece but disrupts temporal\ncues to musical expectations, like harmonic progressions\nand voice-leading.\nWe split each dataset into 5 equally sized non-\noverlapping folds, resulting in 4RNN architectures \u00022\norderings of the CQT spectrograms (original vs. random-\nized spectrograms) \u00025folds\u00022datasets = 80 trained\nmodels. For each fold, 80% of the pieces (ca. 184 pieces\nfor the WTC and 29 for the Beatles) are randomly se-\nlected to be used for training and 20% for testing (ca. 46\npieces for the WTC and 7 for the Beatles). The predictive\naccuracy of each model is measured by the mean cross-\nentropy (MCE) on the test set. The models are trained us-\ning RMSProp [33], a variant of stochastic gradient descent\nthat adaptively updates the step-size using a moving av-\nerage of the of the magnitude of the gradients. The initial\nlearning rate is set to 10\u00003. The gradients are computed us-\ning truncated back propagation through time, where com-\nputation of the gradients is truncated after 100 steps and are\nclipped at 1. Each training batch consists of 20 sequences\nof 100 CQT slices. Each sequence is selected randomly\nout of the training data. Thus, an epoch of training cor-\nresponds to the model seeing roughly the same number of\ntime steps as in the whole fold. Early stopping is used af-\nter 100 epochs without any improvement in the test set.\nAll RNNs are implemented using Lasagne8. We provide\nonline supplementary materials describing all of the tech-\nnical details for performing the probe-tone experiments in\nthis paper9.\n3.3.1 Biasing Learning Towards Predicting Change\nA crucial question when applying discrete time recurrent\nmodels to a continuous stream of data such as audio is how\nto choose the rate of discrete time steps with respect to the\nabsolute time of the data. This choice depends on the ap-\nproximate rate or temporal density of relevant events in the\ndata—in our case the notes that make up the musical ma-\nterial. Ideally, we would like the discrete time steps to be\nsmall enough to capture the occurrence of even the short-\nest notes individually, but if the discrete time step is cho-\nsen much smaller than the median event rate, this leads to\nstrong correlations between data at consecutive time steps.\nA result of this is that training models to predict the data\nat time step t+ 1teaches them to strongly expect the data\n8https://github.com/Lasagne/Lasagne . Accessed April 2017.\n9http://carloscancinochacon.com/documents/online_extras/\nismir2017/sup_materials.html .Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 497att+ 1to be approximately equal to the data at t. Choos-\ning a larger discrete step size for the model alleviates this\nproblem, but has the disadvantage that the data the model\nsees at a particular time may actually be an average over\nconsecutive events that happened within that larger step.\nWe slightly revise the training objective of the models as\na remedy to this unfortunate trade-off. This revised objec-\ntive biases the models to care more about correctly predict-\ning the data at t+1when the change from ttot+1is large\n(e.g. the start of a new note) than when it is small (e.g a\ntransition without any starting or ending note events). This\nallows us to use a relatively small step size without caus-\ning the models to trivially learn to expect the data to stay\nconstant between consecutive time steps.\nMore speciﬁcally, we modify the original cross-entropy\nobjective CEtby multiplying it with a time-varying weight\nwtas follows:\n~CEt wtCEt; (2)\nwherewtis given by\nwt=\u001a\n1ifPN\nijxt+1;i\u0000xt;ij>\"\n\fotherwise(3)\nwhere\"2Racts as a threshold distinguishing small and\nlarge change transitions, and \f2Rcontrols the relative\ninﬂuence of prediction errors on the training in the case of\nsmall change transitions10.Based on an informal inspec-\ntion of the model predictions in a grid search on \fand\",\nwe choose\f= 10\u00003, and\"such that\nPtraining (NX\nijxt+1;i\u0000xt;ij \u0014\") = 0:505 (4)\nwherePtraining (X)denotes the empirical probability\nof eventXunder the training data.\n3.4 Results and Discussion\nFigure 1 compares the aggregation of the probe-tone rat-\nings (see Section 3.1) for both major and minor contexts\nwith the expectations of the best predictive models (as in\nlowest MCE in the test set) for each dataset, which in both\ncases is the GRU trained without shufﬂing the data. Table\n1 shows the correlation between the KK proﬁles and the\nmodel expectations. All of the correlations are statistically\nsigniﬁcant (p<0:0002 ). Although the values obtained for\nthe models trained on the Beatles data are slightly lower,\nthe strength of the correlations between the empirical data\nand the model simulation is on a par with those reported in\nthe literature [24, 35]. Pairwise two-sample Kolmogorov–\nSmirnov tests (KK vs. Hewitt/WTC, KK vs. Beatles and\nWTC/Hewitt vs. Beatles) reveal that the three proﬁles are\nnot signiﬁcantly different from one another ( p\u00150:19).\nThe above result shows that the expectations of the pro-\nposed models reﬂect the tonal characteristics of the mu-\nsical context that evoked those expectations. This is ex-\npected but not trivial, since the training objective of the\n10We empirically found a binary distinction between small and large\nchange transitions to be more effective than a gradual weighting scheme\nCC# DD# E FF# GG# AA# B0.00.20.40.60.81.0goodness-of-fit rating (normalized)KK profile Major\nWTC/Hewitt\nBeatles\nExpert listeners\nCC# DD# E FF# GG# AA# B0.00.20.40.60.81.0goodness-of-fit rating (normalized)KK profile Minor\nWTC/Hewitt\nBeatles\nExpert listenersFigure 1 . Expectations of the models trained on WTC and\nBeatles datasets compared to average probe-tone ratings by\nexpert listeners for major and minor contexts [18]\nKK major KK minor\nWTC/Hewitt 0.915 0.940\nBeatles 0.900 0.885\nTable 1 . Pearson’s correlation between normalized predic-\ntions of the model with the lowest mean cross-entropy for\neach dataset and KK major and minor proﬁles\nmodels is solely to predict how a given sequence of musi-\ncal information (in the form of CQT spectrograms) will\ncontinue. An interesting question is therefore whether\nthere is any relation between the predictive accuracy of a\nmodel (that is, how successfully it predicts future musical\nevents based on the music up to now), and the correlation\nof its probe-tone response to that of human subjects. In\nthe plots of Figure 2, the vertical axis measures the Pear-\nson correlation coefﬁcient of the probe-tone responses of\ndifferent models with the KK proﬁles, and the horizontal\naxis measures predictive accuracy of the models, in terms\nof their MCE over the test data. For each model type in\nthe legend, there are ﬁve different scatter points, represent-\ning models trained on each of ﬁve non-overlapping folds of498 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017the data (see Section 3.3). The vertical coordinate of each\nscatter point is the result of averaging the correlation coef-\nﬁcients of responses to all transpositions of the probe-tone\nstimuli (see Section 3.1).\nThe scatterplots for the WTC and Beatles show that on\naverage, MCE is higher for models trained the Beatles data\nthan for those trained on WTC. This is likely due to the fact\nthat the WTC data are single instrument recordings (piano)\nwith relatively homogeneous CQT spectrograms, whereas\nthe Beatles recordings are multi-instrumental, leading to\nmore dense and complex CQT spectrograms.\nFor the WTC data, training models on shufﬂed CQT\ndata has a noticeable negative impact on both predictive\naccuracy and tonal expectations. For the Beatles data this\neffect is less pronounced. There may be multiple explana-\ntions for this. First, even if the WTC data, being solo piano\nrecordings, are spectrally simpler, they are probably more\ncomplex both harmonically and melodically than the Bea-\ntles data. As such, shufﬂing the data temporally is more\nof a disruption to the WTC data than to the Beatles data.\nSecondly, the WTC pieces tend to include brief modula-\ntions away from the main key of the piece. This means\nthat shufﬂing the data within a piece may mix data from\ndifferent keys, making prediction more diﬁccult.\nDespite these differences, both WTC- and Beatles-\ntrained models roughly show the same overall pattern:\nmodels with low predictive error have high KK correla-\ntions, whereas models with high predictive error may or\nmay not have high KK correlations. This suggests that\nin order to form accurate musical expectations, it is in-\ndispensable to have a notion of tonal structure. But con-\nversely, having a notion of tonal structure by itself is not\na sufﬁcient condition for accurate musical expectations.\nThis implies that there are other factors beyond tonality,\nsuch as voice leading, rhythm, and cadential structure, that\nhelp predict how a given musical context will continue\n(see [31] and [32] for behavioral evidence to this effect).\n4. CONCLUSION\nIn this paper we showed that the expectations of eco-\nlogically trained predictive models of music exhibit tonal\nstructure very similar to that observed in humans through\nprobe-tone experiments. We believe this ﬁnding is rel-\nevant, since most computational modeling approaches to\ntonal perception that involve a representation of statistical\nregularities in musical data do not account for the percep-\ntual learning of such regularities in a plausible way. The\nmusical expectations of the models used here are formed\nby training the model to reduce the prediction error for fu-\nture musical events based on the musical context up to the\npresent—a cognitively plausible task according to the pre-\ndictive coding theory of the brain [8]. Furthermore, we\ndemonstrate that tonal learning within such models is not\nonly possible based on training data known to exhibit rich\ntonal qualities (Bach’s WTC, artiﬁcial cadences), but also\noccurs as an effect of exposure to audio representations of\n“real-world” popular and harmonically simpler music (The\nBeatles). This more accurately mirrors the kind of musical\n0.115 0.120 0.125 0.130 0.135 0.140 0.145\nMean Cross Entropy0.00.20.40.60.81.0Correlation with KK profilesWTC/Hewitt\nvRNN\nvRNN/MI\nGRU\nLSTM\nLSTM/MI\nshuf vRNN\nshuf vRNN/MI\nshuf GRU\nshuf LSTM\nshuf LSTM/MI\n0.27 0.28 0.29 0.30 0.31 0.32 0.33\nMean Cross Entropy0.00.20.40.60.81.0Correlation with KK profilesBeatlesFigure 2 . Similarity of model expectations to human\nprobe-tone ratings (Pearson correlation coefﬁcient) plotted\nversus the mean cross-entropy of the models over a test set;\nshuf denotes models trained on shufﬂed data\nexposure people have, even if real-world musical encultur-\nation would typically involve a wider range of music.\nAn analysis of the relation between the predictive ac-\ncuracy of the model and the degree of tonal structure ex-\nhibited by model expectations shows that tonal expecta-\ntions are a necessary but not a sufﬁcient condition for ac-\ncurate musical expectations. This suggests that there are\nother—presumably temporal—cues to musical expectation\nbeyond tonal structure. Evidence for this is the fact that\nmodels trained on temporally shufﬂed WTC data form less\naccurate expectations than models trained on the ordered\ndata. This effect is not observed for the Beatles data, possi-\nbly because of its simpler melodic and harmonic structure.\nThe empirical validation of the models we presented\nhere offers various further avenues of research that we have\nnot yet pursued. For example, a qualitative analysis of the\nlearned representations of the models may provide further\ninsights into the cues that inﬂuence musical expectations.\nIn models with multiple hidden layers, an interesting ques-\ntion is where the different learned representations lie along\nthe sensory-cognitive spectrum of tonal representations, as\nhypothesized by [9].Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4995. ACKNOWLEDGMENTS\nThis work has been partly funded by the European Re-\nsearch Council (ERC) under the EUs Horizon 2020\nFramework Programme (ERC Grant Agreement No.\n670035, project CON ESPRESSIONE). We thank Carol\nL. Krumhansl for providing the probe-tone data.\n6. REFERENCES\n[1] K. Agres, C. Cancino, M. Grachten, and S. Lattner.\nHarmonics co-occurrences bootstrap pitch and tonality\nperception in music: Evidence from a statistical unsu-\npervised learning model. In CogSci 2015: The annual\nmeeting of the Cognitive Science Society , 2015.\n[2] K. Agres, C. E. Cancino Chac ´on, M. Grachten, and\nS. Lattner. Harmonics co-occurrences bootstrap pitch\nand tonality perception in music: Evidence from a sta-\ntistical unsupervised learning model. In CogSci 2015:\nThe annual meeting of the Cognitive Science Society ,\nPasadena, CA, USA, 2015.\n[3] K. Agres, S. Abdallah, and M. Pearce. Information-\ntheoretic properties of auditory sequences dynamically\ninﬂuence expectation and memory. Cognitive Science ,\n2017.\n[4] G. M. Bidelman, S. Hutka, and S. Moreno. Tone lan-\nguage speakers and musicians share enhanced percep-\ntual and cognitive abilities for musical pitch: evidence\nfor bidirectionality between the domains of language\nand music. PloS one , 8(4):e60676, 2013.\n[5] J. C. Brown. Calculation of a constant Q spectral trans-\nform. The Journal of the Acoustical Society of America ,\n89(1):425–434, January 1991.\n[6] C. E. Cancino Chac ´on, S. Lattner, and M. Grachten.\nDeveloping tonal perception through unsupervised\nlearning. In Proceedings of the 15th International Con-\nference on Music Information Retrieval , Taipei, Tai-\nwan, October 2014.\n[7] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empir-\nical evaluation of gated recurrent neural networks on\nsequence modeling. arXiv preprint arXiv:1412.3555 ,\n2014.\n[8] A. Clark. Whatever next? predictive brains, situated\nagents, and the future of cognitive science. Behavioral\nand Brain Sciences , 36(3):181–204, 2013.\n[9] T. Collins, B. Tillmann, F. S. Barrett, C. Delbe, and\nP. Janata. A combined model of sensory and cognitive\nrepresentations underlying tonal expectations in music:\nfrom audio signals to behavior. Psychological review ,\n121(1):33, 2014.\n[10] L. L. Cuddy and B. Badertscher. Recovery of the tonal\nhierarchy: Some comparisons across age and levels\nof musical experience. Perception & Psychophysics ,\n41(6):609–620, 1987.[11] D. C. Dennett. Consciousness Explained . Penguin\nBooks, 1991.\n[12] R. Durbin and G. Mitchison. A dimension reduction\nframework for understanding cortical maps. Nature ,\n343:644–647, 1990.\n[13] K. Friston. A theory of cortical responses. Philosophi-\ncal Transactions of the Royal Society B: Biological Sci-\nences , 360(1456):815–836, 2005.\n[14] M. Grachten, M. Gasser, A. Arzt, and G. Widmer. Au-\ntomatic alignment of music performances with struc-\ntural differences. In Proceedings of the 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence, Curitiba, Brazil, November 2013.\n[15] A. Graves. Generating Sequences With Recurrent Neu-\nral Networks. arXiv , 1308:850, 2013.\n[16] S. Hochreiter and J. Schmidhuber. Long Short-Term\nMemory. Neural Computation , 9(8):1735–1780, 1997.\n[17] T. Kohonen. Self-organized formation of topologi-\ncally correct feature maps. Biol. Cybernetics , 43:59–\n69, 1982.\n[18] C. L. Krumhansl and E. J. Kessler. Tracing the dynamic\nchanges in perceived tonal organization in a spatial\nrepresentation of musical keys. Psychological review ,\n89(4):334–368, July 1982.\n[19] C. L. Krumhansl. Cognitive foundations of musical\npitch . Cognitive foundations of musical pitch. Oxford\nUniversity Press, New York, 1990.\n[20] C. L. Krumhansl. Music psychology: Tonal structures\nin perception and memory. Annual review of psychol-\nogy, 42(1):277–303, 1991.\n[21] C. L. Krumhansl and L. L. Cuddy. A Theory of Tonal\nHierarchies in Music. In Music Perception , pages 51–\n87. Springer New York, New York, NY , June 2010.\n[22] C. L. Krumhansl and F. C. Keil. Acquisition of the hi-\nerarchy of tonal functions in music. Memory & Cogni-\ntion, 10(3):243–251, 1982.\n[23] E. W. Large, J. C. Kim, N. K. Flaig, J. J. Bharucha,\nand C. L. Krumhansl. A neurodynamic account of mu-\nsical tonality. Music Perception: An Interdisciplinary\nJournal , 33(3):319–331, 2016.\n[24] M. Leman and F. Carreras. The self-organization of sta-\nble perceptual maps in a realistic musical environment.\nIn G. Assayah, editor, Proceedings of the Journ ´ees\nd’Informatique Musicale 1996 , pages 156–169, Caen,\n1996. Univ. de Caen – IRCAM, Les Cahiers du GR-\nEYC No. 4.\n[25] M. Leman. A model of retroactive tone-center percep-\ntion. Music Perception , 12(4):439–471, 1995.\n[26] F. Lerdahl and R. Jackendoff. A Generative Theory of\nTonal Music . The MIT Press, 1983.500 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[27] R. Pascanu, T. Mikolov, and Y . Bengio. On the difﬁ-\nculty of training recurrent neural networks. In Proceed-\nings of the 30th International Conference on Machine\nLearning , pages 1–9, Atlanta, Georgia, USA, 2013.\n[28] H. Purwins, B. Blankertz, and K. Obermayer. A new\nmethod for tracking modulations in tonal music in au-\ndio data format. In Proceedings of the International\nJoint Conference on Neural Networks , volume 6, pages\n270–275. IEEE, 2000.\n[29] J. R. Saffran, E. K. Johnson, R. N. Aslin, and E. L.\nNewport. Statistical learning of tone sequences by hu-\nman infants and adults. Cognition , 70(1):27–52, 1999.\n[30] M. A. Schmuckler and M. G. Boltz. Harmonic and\nrhythmic inﬂuences on musical expectancy. Attention,\nPerception, & Psychophysics , 56(3):313–325, 1994.\n[31] M. A. Schmuckler and M. G. Boltz. Harmonic and\nrhythmic inﬂuences on musical expectancy. Attention,\nPerception, & Psychophysics , 56(3):313–325, 1994.\n[32] D. Sears, W. E. Caplin, and S. McAdams. Perceiving\nthe classical cadence. Music Perception: An Interdisci-\nplinary Journal , 31(5):397–417, 2014.\n[33] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Di-\nvide the gradient by a running average of its recent\nmagnitude. In COURSERA Neural Networks for Ma-\nchine Learning , 2012.\n[34] B. Tillmann. Music cognition: Learning, perception,\nexpectations. Computer music modeling and retrieval.\nSense of sounds , pages 11–33, 2008.\n[35] B. Tillmann, J. J. Bharucha, and E. Bigand. Implicit\nlearning of tonality: A self-organizing approach. Psy-\nchological Review , 107(4):885–913, 2000.\n[36] P. Toiviainen and C. L. Krumhansl. Measuring and\nmodeling real-time responses to music: The dynam-\nics of tonality induction. Perception , 32(6):741–766,\n2003.\n[37] G. Widmer. Getting closer to the essence of music: The\nCon Espressione manifesto. ACM TIST , 8(2):19:1–\n19:13, 2017.\n[38] Y . Wu, S. Zhang, Y . Zhang, Y . Bengio, and\nR. Salakhutdinov. On Multiplicative Integration with\nRecurrent Neural Networks. In 30th Conference on\nNeural Information Processing Systems (NIPS 2016) ,\nBarcelona, Spain, 2016.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 501"
    },
    {
        "title": "High-Level Music Descriptor Extraction Algorithm Based on Combination of Multi-Channel CNNs and LSTM.",
        "author": [
            "Ning Chen 0007",
            "Shijun Wang"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417901",
        "url": "https://doi.org/10.5281/zenodo.1417901",
        "ee": "https://zenodo.org/records/1417901/files/ChenW17.pdf",
        "abstract": "Although Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTM) have yielded impressive performances in a variety of Music Information Retrieval (MIR) tasks, the complementarity among the CNNs of different architectures and that between CNNs and LSTM are seldom considered. In this paper, multi- channel CNNs with different architectures and LSTM are combined into one unified architecture (Multi-Channel Convolutional LSTM, MCCLSTM) to extract high-level music descriptors. First, three channels of CNNs with different shapes of filter are applied on each spectrogram image chunk to extract the pitch-, tempo-, and bass- relevant descriptors, respectively. Then, the outputs of each CNNs channel are concatenated and then passed through a fully connected layer to obtain the fused descriptor. Finally, LSTM is applied on the fused descriptor sequence of the whole track to extract its long-term structure property to obtain the high-level descriptor. To prove the efficiency of the MCCLSTM model, the obtained high-level music descriptor is applied to the music genre classification and emotion prediction task. Experimental results demonstrate that, when compared with the hand-crafted schemes or conventional deep learning (Multi Layer Perceptrons (MLP), CNNs, and LSTM) based ones, MCCLSTM achieves higher prediction accuracy on three music collections with different kinds of semantic tags.",
        "zenodo_id": 1417901,
        "dblp_key": "conf/ismir/ChenW17",
        "keywords": [
            "Convolutional Neural Networks",
            "Long Short Term Memory",
            "Music Information Retrieval",
            "Music genre classification",
            "Emotion prediction",
            "High-level music descriptors",
            "Multi-Channel Convolutional LSTM",
            "Fused descriptor",
            "LSTM",
            "Music collections"
        ],
        "content": "HIGH-LEVEL MUSIC DESCRIPTOR EXTRACTION ALGORITHM\nBASED ON COMBINATION OF MULTI-CHANNEL CNNS AND LSTM\nNing Chen\nEast China University of\nScience and Technology\nnchen@ecust.edu.cnShijun Wang\nEast China University of\nScience and Technology\nsqwsj@hotmail.com\nABSTRACT\nAlthough Convolutional Neural Networks (CNNs) and\nLong Short Term Memory (LSTM) have yielded\nimpressive performances in a variety of Music Information\nRetrieval (MIR) tasks, the complementarity among the\nCNNs of different architectures and that between CNNs\nand LSTM are seldom considered. In this paper, multi-\nchannel CNNs with different architectures and LSTM are\ncombined into one uniﬁed architecture (Multi-Channel\nConvolutional LSTM, MCCLSTM) to extract high-level\nmusic descriptors. First, three channels of CNNs with\ndifferent shapes of ﬁlter are applied on each spectrogram\nimage chunk to extract the pitch-, tempo-, and bass-\nrelevant descriptors, respectively. Then, the outputs of\neach CNNs channel are concatenated and then passed\nthrough a fully connected layer to obtain the fused\ndescriptor. Finally, LSTM is applied on the fused\ndescriptor sequence of the whole track to extract its\nlong-term structure property to obtain the high-level\ndescriptor. To prove the efﬁciency of the MCCLSTM\nmodel, the obtained high-level music descriptor is applied\nto the music genre classiﬁcation and emotion prediction\ntask. Experimental results demonstrate that, when\ncompared with the hand-crafted schemes or conventional\ndeep learning (Multi Layer Perceptrons (MLP), CNNs,\nand LSTM) based ones, MCCLSTM achieves higher\nprediction accuracy on three music collections with\ndifferent kinds of semantic tags.\n1. INTRODUCTION\nThe amount of online music tracks is constantly growing,\nwhich makes it difﬁcult to tag them manually. Without\naccurate labels, most of the tracks cannot be accessed.\nSo, auto-tagging technique has become a hot topic in the\nﬁeld of Music Information Retrieval (MIR) for the past\ntwo decades. It can be used in music classiﬁcation, music\nretrieval, and music recommendation systems.\nc\rNing Chen, Shijun Wang. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Ning Chen, Shijun Wang. “HIGH-LEVEL\nMUSIC DESCRIPTOR EXTRACTION ALGORITHM BASED ON\nCOMBINATION OF MULTI-CHANNEL CNNS AND LSTM”, 18th\nInternational Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.In the past ten years, deep learning models, such as\nDeep Neural Networks (DNNs), Convolutional Neural\nNetworks (CNNs), and Long Short Term Memory (LSTM)\n[9] have achieved tremendous success for a variety of MIR\ntasks, such as onset detection [20], emotion recognition\n[13], chord estimation [5], rhythm stimuli recognition\n[22], auto-tagging [3,16], source separation [10], or music\nrecommendation [25], etc. It has been proved that deep\nlearning based models are superior to hand-crafted ones\nin music content analysis because [16]: i) The nonlinear\nmapping in deep learning model (e.g. CNNs) is suitable\nfor describing the time-varying nonlinear property of\nmusic signal. ii) The hierarchical architecture of deep\nlearning model is ﬁt for representing the hierarchical\nnature of music in both time domain (onset, rhythm)\nand frequency domain (note, chord) [16]. iii) Long-\nterm dependencies property of music (music structure or\nrecurrent harmonies), which is important for human music\nperception and understanding, can be modeled by deep\nlearning model (e.g. LSTM) very well [11].\nDespite the rich potential of CNNs and LSTM in\ndescribing music properties, they are individually limited\nin their modeling capability [19]. In [16], the CNNs\nwere adopted to learn high-level descriptor from the\nspectrogram image of the music signal, and the ﬁlter\nshape of CNNs was studied to make it suitable for\nrepresenting different music relevant descriptors. It was\nveriﬁed that wider ﬁlters and higher ﬁlters may be capable\nof learning longer temporal dependencies and more spread\ntimbral features, respectively. This scheme achieved\ncompetitive results in auto-tagging on the Ballroom dataset\n[8]. However, as shown in [16, 19], CNNs may only\nmodel the local context, such as instrument’s timbre or\nmusical units, well, but not the long-term dependencies,\nsuch as music structure or recurrent harmonies, of the\nmusic. As for the LSTMs based schemes, their main issues\nare two aspects. On the one hand, the temporal modeling\nis usually done on the low-level descriptor, which makes\nit difﬁcult to disentangle underlying factors of variation\nwithin the input [12]. On the other hand, as shown in [15],\nthere is no intermediate nonlinear hidden layer in LSTM,\nso the history of previous inputs cannot be summarized\nefﬁciently.\nTo take advantage of the complementarity between\nCNNs and LSTM, some researchers proposed to combine\nthem in a uniﬁed architecture [2, 4]. In [2], LSTM509and CNNs are combined in parallel to exploit sequential\ncorrelation and local spectro-temporal information. Then,\nthe outputs of the CNNs and LSTM are combined\nby the fully connected layers to obtain the fused\ndescriptor. Experimental results demonstrated that this\nscheme outperformed the conventional DNNs, CNNs, or\nLSTM based ones in acoustic scene classiﬁcation task.\nIn [19], considering that: ”CNNs are good at reducing\nfrequency variations, LSTMs are good at temporal\nmodeling, and DNNs are appropriate for mapping features\nto a more separable space”, the three models are combined\ninto one uniﬁed architecture to take advantage of the\ncomplementarity among them. This scheme achieved\nbetter performances than the LSTM based one in the voice\nsearch task.\nIn this paper, a new deep learning based architecture\n(called Multi-Channel Convolutional LSTM, MCCLSTM)\nis proposed for high-level music descriptor extraction.\nMCCLSTM is different from the methods discussed above\nas it takes advantage of the complementarity among CNNs\nwith different architectures and that between CNNs and\nLSTM in modeling music properties. Considering that\ndifferent musical properties correspond to different time-\nfrequency resolutions [16], the descriptor extracted by one\nCNN with a speciﬁc ﬁlter may not characterize the music\nproperty comprehensively. So, in the proposed scheme,\nthree channels of CNNs with different shapes of ﬁlters\nare adopted to extract pitch-, tempo-, and bass-relevant\ndescriptors from the spectrogram image, respectively.\nThen, the outputs of each CNNs are concatenated and\nthen passed trough a fully connected layer to map to the\nfused descriptor. Finally, since it has been veriﬁed in [15]\nthat the performance of LSTM can be improved greatly\nwhen provided with high-level descriptor, the LSTM is\nput as a higher level of the fully connected layer in the\nproposed scheme to learn the time dependency in the fused\ndescriptor sequence to extract the long-term structure of\nthe whole track. Since the obtained high-level descriptor\ncontains both local context based and long-term structure\nbased information of the music, it may describe the music\nproperty more comprehensively. Experimental results\ndemonstrate that the proposed model is superior to the\nhand-crafted schemes [14, 18] and the conventional deep\nlearning (Multi Layer Perceptrons (MLP) [21], CNNs [16],\nand LSTM) based ones in music auto-tagging task on three\nmusic collections with different kinds of semantic tags.\nThe rest of this paper is organized as follows. The\nproposed scheme is described in detail in Section 2.\nThe performances of the proposed scheme in music\nauto-tagging task in comparison with other state-of-the-\nart schemes are evaluated and discussed in Section 3.\nConclusions and prospects on future work are given in\nSection 4.\n2. MCCLSTM MODEL\nThe MCCLSTM architecture is shown in Figure 1.\nFigure 1 : Multi-Channel Convolutional LSTM (MCCLSTM)\narchitecture.\n2.1 Preprocessing\nThe same preprocessing procedure shown in [16] is\nadopted in the proposed scheme. First, the Short-Time\nFourier Transform (STFT) is applied to the input music\naudio signal, whose sampling rate is 44100 Hz, to obtain\nthe spectrum of it. In STFT, a Blackman-Harris window\nof 2048 samples is chosen, and the hop size is 1024\nsamples. Next, the 40-band Mel ﬁlter-bank is applied\non the obtained spectrum to generate the corresponding\nspectrogram image of it. Then, the whole spectrogram\nimage is split into Lchunks without overlapping. The size\nof each spectrogram chunk is M\u0002N, where MandN\nstand for the number of frequency bins and that of frames,\nrespectively.\n2.2 Multiple Musical Descriptors Extraction and\nFusion\nThere is no one universal deep learning architecture\nor hand-crafted scheme that performs well in modeling\nmultiple music properties at the same time. To solve\nthis problem and describe the music content more\ncomprehensively, three channels of CNNs with different\nshape of ﬁlters are adopted and combined in the proposed\nscheme to obtain fused descriptor, which contains tempo-\n, pitch-, and bass-relevant information of the input music.\nThis idea was ﬁrst proposed in [16] to combine the tempo-\nand pitch-relevant information and was modiﬁed in this\npaper by adding another bass-relevant information.\n\u000fPitch-channel CNNs: in this channel, a m\u0002\n1(m\u001cM)frequency ﬁlter is chosen. This type\nof ﬁlter is designed for modeling frequency features.\nThe upper layer can represent some temporal\ndependencies from the resulting activations as well\n[16]. In the proposed scheme, this channel of\nCNNs are responsible for extracting pitch, timbre, or510 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017equalization setups relevant descriptor of the music.\n\u000fTempo-channel CNNs: in this channel, a 1\u0002n(n\u001c\nN)temporal ﬁlter is adopted. This kind of ﬁlter\nwill be suitable for learning temporal dependencies\nbut not frequency dependencies. Also, the upper\nlayer may exploit the frequency relations [16]. In\nthe proposed scheme, this channel of CNNs try to\nlearn rhythmic/tempo relevant patterns of the music.\n\u000fBass-channel CNNs: in this channel, a m\u0002n(m\u001c\nM; n\u001cN)ﬁlter is taken. This type of ﬁlter is\ncapable of learning time and frequency features at\nthe same time. Different musical aspects can be\nlearned by such ﬁlters with different combination of\nmandn. Considering that the task of this channel of\nCNNs is to model the bass- or kick-relevant feature,\nwhich most entails ﬁnding changes over time, a ﬁlter\nthat is wide in time and narrow in frequency (i.e.\nm < n ) is adopted [16].\nTo take advantage of the complementarity among the\nobtained pitch-, tempo-, and bass-relevant descriptors, they\nare concatenated on the fusion layer and then passed\nthrough a fully connected layer to generate the fused\ndescriptor.\n2.3 Long-Term Structure Analysis\nAlthough CNNs may model the local context in the\nspectrogram chunk well, it may not model the long-term\nstructure (music structure or recurrent harmonies) of the\nwhole track, which is quite important for human music\nperception and understanding [11]. To solve this problem,\na LSTM layer is added on top of the fully connected\nlayer. It will help to learn the time dependence in the\nfused descriptor sequence of the whole track. The number\nof the nodes in LSTM is equal to that of the chunks\nincluded in the whole track. Since the obtained high-\nlevel descriptor contains the information of different music\nproperties (pitch, tempo, and bass) and that of long-term\nstructure of the music as well, it may be more suitable for\nauto tagging of music with different kinds of semantic tags.\n3. EXPERIMENTS\nIn the experiment, the input of the architecture is 40-\ndimensional log-mel ﬁlterbank based spectrogram images,\ncomputed every 3.712s (i.e., M= 40 ; N = 80 ). As\nshown in Figure 1, all three CNNs channels are composed\nof 1 convolutional layer and 1 max-pooling layer. The size\nof each kernel in the multi-channel convolutional neural\nnetwork is listed in Table 1. When 2 (or 3) CNNs channels\nare fused, the fully connected layer and the LSTM layer\ncontain 200 (or 400) and 100 (or 200) units, respectively.\nThe number of nodes in LSTM is equal to that of the\nchunks included in the whole spectrogram. The weights\nfor all CNN and LSTM layers are randomly initialized to\nbe Gaussian, with a variance of 1. And a softmax layer\nis added upon the LSTM layer to discriminate the tag ofthe overall input music. Six kinds of architectures based\non different combinations of CNNs and LSTM (see Table\n2) are studied in the experiment. To verify the efﬁciency\nof the proposed high-level musical descriptor extraction\nscheme, its performances in music auto-tagging task are\ntested on three music collections with different kinds of\nsemantic tags, in comparison with those obtained by the\nhand-crafted schemes or conventional deep learning-based\nones.\nThe whole architecture shown in Figure 1 is trained\ntogether with the categorical cross-entropy criterion, using\nthe asynchronous stochastic gradient descent optimization\nstrategy. The weights for all CNN and LSTM layers are\nrandomly initialized to be Gaussian, with a variance of 1.\nThe prediction accuracies obtained by deep learning-based\nschemes are computed using 10-fold cross validation with\na randomly generated train-validation-test split of 80%-\n10%-10%.\nLayer name P-channel T-channel B-channel\nConvolutional layer (32,1) (1,60) (13,9)\nMax-pooling layer (1,80) (40,1) (4,4)\nTable 1 : Size of each kernel in the multi-channel convolutional\nneural network.\nID Architecture\nR0 CNNs (T)\nR1 LSTM only\nR2 [16] CNNs (T+P)\nR3 CNNs (T+P+B)\nR4 CNNs (T+P)+LSTM\nMCCLSTM CNNs (T+P+B)+LSTM\nTable 2 : Six architectures studied in the experiments.\n3.1 Datasets\nThe following three music collections with different kinds\nof semantic tags are adopted to test the performances of the\nproposed scheme in auto-tagging task.\n\u000fGTZAN genre collection [24]: Although GTZAN\ndataset suffers from some repetitions, mislabelings\nand distortions problems [17], it is often adopted to\nevaluate genre classiﬁcation accuracy. This dataset\nis composed of 1000 audio tracks, each 30 seconds\nlong. It contains 10 genres (blues, classical, country,\ndisco, hiphop, jazz, metal, pop, reggae, and rock),\neach of which is represented by 100 tracks.\n\u000fBallroom dataset: This dataset comprises 698 audio\ntracks, each around 30 seconds long. It contains\n8 ballroom dancing genres (cha-cha-cha 111, jive\n60, quickstep 82, rumba 98, samba 86, tango 86,\nviennese valtz 65, and slow waltz 110).\n\u000fSoundtracks dataset [7] for music and emotion: This\ndataset contains 470 ﬁlm music excerpts, each 15-30\nseconds long. The tag of each excerpts is one of the\nﬁve discrete emotions: anger 61, fear 116, sadness\n108, happiness 89, and tenderness 96 [6].Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 5113.2 Experimental Results\nTo verify the efﬁciency of the MCCLSTM model in\nmusic auto-tagging task, its performance, in terms of\nprediction accuracy, is compared with those obtained by\nthe conventional hand-crafted schemes and other deep\nlearning-based ones (MLP, CNNs, and LSTM) on each of\nabove datasets.\n3.2.1 Baselines\nIn the experiment, as shown in Table 3-5, two hand-crafted\nschemes [14, 18] and four deep learning-based ones (MLP\n[21], R0, R1, and CNNs-3) are included as baselines.\nIn R0, the output of the tempo-channel CNNs is used\nfor tag prediction, directly. While, in R1, the LSTM\nis learned directly on the spectrogram chunk sequence.\nThe CNNs-3 scheme is composed of two convolutional\nlevels, two max-pooling levels, and one fully connected\nlevel (200 units). The sizes of the two convolutional\nlayers are (5,5) and (3,3), respectively. The sizes of the\ntwo max-pooling layers are both (2,2). The two max-\npooling layers are alternated with convolutional layers.\nIt should be noted that, since the codes of the schemes\nin [14], [18], and [21] are not available, we just include\nthe prediction accuracies obtained by them on speciﬁc\nmusic collections. As shown in Table 3-5, when compared\nwith the hand-crafted schemes [14, 18], the MCCLSTM\nscheme can enhance the prediction accuracy from 3.50% to\n16.95%. As for the deep learning-based ones (MLP [21],\nR0, R1, and CNNs-3), MCCLSTM scheme can achieve\na higher prediction accuracy of 1.69%-32.99%, 4.90%-\n27.90%, and 9.85%-22.35%, on GTZAN, Ballroom, and\nSoundtracks datasets, respectively. So, it is veriﬁed that the\nproposed scheme is superior to the hand-crafted schemes\nand conventional deep learning-based ones in auto tagging\ntask across the datasets included.\nSchemes Accuracy: mean% \u0006std\n[14] 72:80\n[21] (3 layers) 83:00\u00061:10\nCNNs-3 79:80\u00061:70\nR0 51:70\u00062:60\nR1 59:30\u00062:20\nR2 [16] 78:40\u00061:90\nR3 82:90\u00062:11\nR4 83:70\u00061:10\nMCCLSTM 84:69\u00061:76\nTable 3 : Performance comparison on GTZAN dataset.\nSchemes Accuracy: mean% \u0006std\n[14] 88:40\nCNNs-3 87:00\u00061:32\nR0 81:79\u00064:72\nR1 64:00\u00061:50\nR2 [16] 87:68\u00064:44\nR3 89:45\u00062:18\nR4 90:32\u00061:12\nMCCLSTM 91:90\u00062:33\nTable 4 : Performance comparison on Ballroom dataset.Schemes Accuracy: mean% \u0006std\n[18] 57:40\u00065:50\nCNNs-3 60:87\u00063:76\nR0 52:00\u00062:20\nR1 64:50\u00062:00\nR2 [16] 57:28\u00062:85\nR3 63:04\u00062:16\nR4 73:70\u00062:47\nMCCLSTM 74:35\u00061:63\nTable 5 : Performance comparison on Soundtracks dataset.\n3.2.2 Multi-Channel CNNs Based Schemes\nIn [16] (denoted as R2 in this paper), the outputs of\nthe pitch-channel CNNs and the tempo-channel CNNs in\nFigure 1 were concatenated to obtain the fused musical\ndescriptor, which then contains both pitch- and tempo-\nrelevant information. To make the fused descriptor contain\nbass relevant information also, a bass-channel CNNs is\nadded in R2 to obtain the three-channel CNNs based one\n(denoted as R3). As shown in Table 3-5, R3 achieves\nhigher prediction accuracy than [16] on all three datasets.\nEspecially, for the music mood auto-tagging (see Table\n5), R3 enhances the prediction accuracy by 5.76% when\ncompared with [16]. The latent reason may be that the\nbass relevant information plays a crucial role in mood\nclassiﬁcation [1].\n3.2.3 Multi-Channel CNNs +LSTM Based Schemes\nMusic can be described as sequences of events that are\nstructured in pitch and time [23]. So, how to learn\nand represent such complex event sequences (or long-\nterm structure) is very important for music perception\nand cognition. However, CNNs may only model the\nlocal context well but not the long-term dependencies\ncontained in the whole track [16], which will affect the\naccurate describing of the music properties. Considering\nthat LSTM is good at extracting the sequential information\nfrom the consecutive features, a LSTM layer is added\non the top of the fully connected layer (as shown in\nFigure 1) to model the long-term structure property\nof the music. To show the beneﬁts of LSTM, it is\napplied on the two-channel and three-channel CNNs fused\ndescriptor, respectively, to construct R4 and MCCLSTM\nschemes. The experimental results shown in Table 3-\n5 indicate that for the two-channel CNNs based scheme\n(R2), the introducing of LSTM layer can help to enhance\nthe prediction accuracy of 5.30%, 2.64%, and 16.42%\non GTZAN, Ballroom, and Soundtracks, respectively.\nFor the three-channel CNNs based scheme (R3), the\nadding of LSTM layer can help to enhance the prediction\naccuracy of 1.79%, 2.45%, and 11.31% on GTZAN,\nBallroom, and Soundtracks, respectively. So, it is veriﬁed\nthat adopting LSTM to analyze the time dependencies\ncontained in the fused descriptor sequence further may\nhelp to describe the musical characteristic more accurately\nand comprehensively.512 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20174. CONCLUSIONS AND FUTURE WORK\nIn this paper, we present a uniﬁed deep learning\narchitecture (MCCLSTM) for high-level musical\ndescriptor extraction. First, the CNNs with different\nresolutions are utilized to analyze each spectrogram\nchunk of the input music to extract different music\nproperty-relevant descriptors, respectively. Then, the\noutputs of each CNNs channels are concatenated and\nthen passed through a fully connected layer to obtain\nthe fused descriptor. Finally, the LSTM is performed\non the fused descriptor sequence to model the long-term\nstructure of the input music. To verify the efﬁciency of\nthe MCCLSTM scheme, its performance in music auto\ntagging are compared with those obtained by the hand-\ncrafted schemes and the conventional deep learning (MLP,\nCNNs, and LSTM) based ones on three music collections\nwith different kinds of semantic tags. Experimental results\ndemonstrate that the proposed scheme is superior to hand-\ncrafted schemes in [14, 18] and other deep learning-based\nones ( [16, 21], R0, R1, R3, R4, and CNNs-3). However,\nsince the fused descriptor is obtained by concatenating\nthe outputs of each CNNs channel, the complementarity\namong these three descriptors cannot be fused efﬁciently.\nSo, our future work is to study new fusion mechanism,\nwhich can utilize the common as well as complementary\naspects of each musical descriptors more efﬁciently.\n5. ACKNOWLEDGEMENT\nThis work is supported by the National Natural Science\nFoundation of China (No. 61271349).\n6. REFERENCES\n[1] Jakob Abeßer, Hanna Lukashevich, Christian Dittmar,\nand Gerald Schuller. Genre classiﬁcation using bass-\nrelated high-level features and playing styles. In\nProceedings of 10th International Conference on\nMusic Information Retrieval (ISMIR 2009) , pages 453–\n458, 2009.\n[2] Soo Hyun Bae, Inkyu Choi, and Nam Soo\nKim. Acoustic scene classiﬁcation using parallel\ncombination of lstm and cnn. In 2016 Detection and\nClassiﬁcation of Acoustic Scenes and Events (DCASE\n2016) , pages 1–5, 2016.\n[3] Keunwoo Choi, George Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional\nneural networks. Proceedings of 17th International\nConference on Music Information Retrieval (ISMIR\n2016), 2016.\n[4] Keunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler,\nand Kyunghyun Cho. Convolutional recurrent neural\nnetworks for music classiﬁcation. In 2017 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 2392–2396. IEEE,\n2017.[5] Junqi Deng and Yu-Kwong Kwok. A hybrid gaussian-\nhmm-deep-learning approach for automatic chord\nestimation with very large vocabulary. Proceedings of\n17th International Conference on Music Information\nRetrieval (ISMIR 2016), 2016.\n[6] Tuomas Eerola and Jonna K Vuoskoski. A comparison\nof the discrete and dimensional models of emotion in\nmusic. Psychology of Music , 2010.\n[7] Tuomas Eerola and Jonna K Vuoskoski. A comparison\nof the discrete and dimensional models of emotion in\nmusic. Psychology of Music , 39(1):18–49, 2011.\n[8] Fabien Gouyon, Anssi Klapuri, Simon Dixon, Miguel\nAlonso, George Tzanetakis, Christian Uhle, and Pedro\nCano. An experimental comparison of audio tempo\ninduction algorithms. IEEE Transactions on Audio,\nSpeech, and Language Processing , 14(5):1832–1844,\n2006.\n[9] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\n[10] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis. Joint optimization of masks and\ndeep recurrent neural networks for monaural source\nseparation. IEEE/ACM Transactions on Audio, Speech\nand Language Processing (TASLP) , 23(12):2136–\n2147, 2015.\n[11] Eric J Humphrey, Juan P Bello, and Yann LeCun.\nFeature learning and deep architectures: new\ndirections for music informatics. Journal of Intelligent\nInformation Systems , 41(3):461–481, 2013.\n[12] Bernhard Lehner, Gerhard Widmer, and Sebastian\nBock. A low-latency, real-time-capable singing voice\ndetection method with lstm recurrent neural networks.\nIn2015 23rd European Signal Processing Conference\n(EUSIPCO) , pages 21–25. IEEE, 2015.\n[13] Xinxing Li, Haishu Xianyu, Jiashen Tian, Wenxiao\nChen, Fanhang Meng, Mingxing Xu, and Lianhong\nCai. A deep bidirectional long short-term memory\nbased multi-scale approach for music dynamic emotion\nprediction. In 2016 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 544–548. IEEE, 2016.\n[14] Athanasios Lykartsis and Alexander Lerch. Beat\nhistogram features for rhythm-based musical genre\nclassiﬁcation using multiple novelty functions. In\nProceedings of 16th International Conference on\nMusic Information Retrieval (ISMIR 2015) , pages 434–\n440, 2015.\n[15] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho,\nand Yoshua Bengio. How to construct deep recurrent\nneural networks. In Proceedings of the Second\nInternational Conference on Learning Representations\n(ICLR 2014) , 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 513[16] Jordi Pons, Thomas Lidy, and Xavier Serra.\nExperimenting with musically motivated convolutional\nneural networks. In 2016 14th International Workshop\non Content-Based Multimedia Indexing (CBMI) , pages\n1–6. IEEE, 2016.\n[17] Francisco Rodrıguez-Algarra, Bob L Sturm, and Hugo\nMaruri-Aguilar. Analysing scattering-based music\ncontent analysis systems: Wheres the music? In\nProceedings of 17th International Conference on\nMusic Information Retrieval (ISMIR 2016) , 2016.\n[18] Pasi Saari, Tuomas Eerola, and Olivier Lartillot.\nGeneralizability and simplicity as criteria in feature\nselection: Application to mood classiﬁcation in music.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 19(6):1802–1812, 2011.\n[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and\nHas ¸im Sak. Convolutional, long short-term memory,\nfully connected deep neural networks. In 2015 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 4580–4584. IEEE,\n2015.\n[20] Jan Schl ¨uter and Sebastian B ¨ock. Improved musical\nonset detection with convolutional neural networks.\nIn2014 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 6979–\n6983. IEEE, 2014.\n[21] Siddharth Sigtia and Simon Dixon. Improved music\nfeature learning with deep neural networks. In\nProceedings of 2014 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\npages 6959–6963. IEEE, 2014.\n[22] Sebastian Stober, Daniel J Cameron, and Jessica A\nGrahn. Using convolutional neural networks to\nrecognize rhythm stimuli from electroencephalography\nrecordings. In Advances in neural information\nprocessing systems , pages 1449–1457, 2014.\n[23] Barbara Tillmann. Music and language perception:\nexpectations, structural integration, and cognitive\nsequencing. Topics in cognitive science , 4(4):568–584,\n2012.\n[24] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nspeech and audio processing , 10(5):293–302, 2002.\n[25] Aaron Van den Oord, Sander Dieleman, and\nBenjamin Schrauwen. Deep content-based music\nrecommendation. In Advances in neural information\nprocessing systems , pages 2643–2651, 2013.514 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Transfer Learning for Music Classification and Regression Tasks.",
        "author": [
            "Keunwoo Choi",
            "György Fazekas",
            "Mark B. Sandler",
            "Kyunghyun Cho"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1418015",
        "url": "https://doi.org/10.5281/zenodo.1418015",
        "ee": "https://zenodo.org/records/1418015/files/ChoiFSC17.pdf",
        "abstract": "In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple lay- ers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music repre- sentation. In the experiments, a convnet is trained for mu- sic tagging and then transferred to other music-related clas- sification and regression tasks. The convnet feature out- performs the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as low- and high-level music features.",
        "zenodo_id": 1418015,
        "dblp_key": "conf/ismir/ChoiFSC17",
        "keywords": [
            "transfer learning",
            "music classification",
            "regression tasks",
            "pre-trained convnet feature",
            "concatenated feature vector",
            "general-purpose music representation",
            "convnet feature",
            "music tagging",
            "aggregating MFCCs",
            "low-level music features"
        ],
        "content": "TRANSFER LEARNING FOR\nMUSIC CLASSIFICATION AND REGRESSION TASKS\nKeunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler\nCentre for Digital Music\nQueen Mary University of London, London, UK\nkeunwoo.choi@qmul.ac.ukKyunghyun Cho\nCenter for Data Science\nNew York University, New York, NY , USA\nkyunghyun.cho@nyu.edu\nABSTRACT\nIn this paper, we present a transfer learning approach for\nmusic classiﬁcation and regression tasks. We propose to\nuse a pre-trained convnet feature , a concatenated feature\nvector using the activations of feature maps of multiple lay-\ners in a trained convolutional network. We show how this\nconvnet feature can serve as general-purpose music repre-\nsentation. In the experiments, a convnet is trained for mu-\nsic tagging and then transferred to other music-related clas-\nsiﬁcation and regression tasks. The convnet feature out-\nperforms the baseline MFCC feature in all the considered\ntasks and several previous approaches that are aggregating\nMFCCs as well as low- and high-level music features.\n1. INTRODUCTION\nIn the ﬁeld of machine learning, transfer learning is of-\nten deﬁned as re-using parameters that are trained on a\nsource task for a target task, aiming to transfer knowledge\nbetween the domains. A common motivation for transfer\nlearning is the lack of sufﬁcient training data in the target\ntask. When using a neural network, by transferring pre-\ntrained weights, the number of trainable parameters in the\ntarget-task model can be signiﬁcantly reduced, enabling ef-\nfective learning with a smaller dataset.\nA popular example of transfer learning is semantic im-\nage segmentation in computer vision, where the network\nutilises rich information, such as basic shapes or prototyp-\nical templates of objects, that were captured when trained\nfor image classiﬁcation [37]. Another example is pre-\ntrained word embeddings in natural language processing.\nWord embedding, a vector representation of a word, can\nbe trained on large datasets such as Wikipedia [35] and\nadopted to other tasks such as sentiment analysis [27].\nc\rKeunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler,\nKyunghyun Cho. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Keunwoo Choi, Gy ¨orgy\nFazekas, Mark Sandler, Kyunghyun Cho. “TRANSFER LEARN-\nING FOR MUSIC CLASSIFICATION AND REGRESSION TASKS”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017. This work has been part funded by FAST IMPACt\nEPSRC Grant EP/L019981/1 and the European Commission H2020 re-\nsearch and innovation grant AudioCommons (688382). Mark Sandler\nacknowledges the support of the Royal Society as a recipient of a Wolf-\nson Research Merit Award. Kyunghyun Cho thanks the support by eBay,\nTenCent, Facebook, Google and NVIDIA.There have been several works on transfer learn-\ning in Music Information Retrieval (MIR). Hamel et al.\nproposed to directly learn music features using linear\nembedding [57] of mel-spectrogram representations and\ngenre/similarity/tag labels [20]. Oord et al. outlines a\nlarge-scale transfer learning approach, where a multi-layer\nperceptron is combined with the spherical K-means algo-\nrithm [16] trained on tags and play-count data [54]. After\ntraining, the weights are transferred to perform genre clas-\nsiﬁcation and auto-tagging with smaller datasets. In music\nrecommendation, Choi et al. used the weights of a con-\nvolutional neural network for feature extraction in playlist\ngeneration [10], while Liang et al. used a multi-layer per-\nceptron for feature extraction of content-aware collabora-\ntive ﬁltering [29].\n2. TRANSFER LEARNING FOR MUSIC\nIn this section, our proposed transfer learning approach is\ndescribed. A convolutional neural network (convnet) is de-\nsigned and trained for a source task, and then, the network\nwith trained weights is used as a feature extractor for tar-\nget tasks. The schematic of the proposed approach is illus-\ntrated in Figure 1.\n2.1 Convolutional Neural Networks for Music Tagging\nWe choose music tagging as a source task because i)large\ntraining data is available and ii)its rich label set covers\nvarious aspects of music, e.g., genre ,mood ,era, and in-\nstrumentations . In the source task, a mel-spectrogram ( X),\na two-dimensional representation of music signal, is used\nas the input to the convnet. The mel-spectrogram is se-\nlected since it is psychologically relevant and computation-\nally efﬁcient. It provides a mel-scaled frequency represen-\ntation which is an effective approximation of human au-\nditory perception [36] and typically involves compressing\nthe frequency axis of short-time Fourier transform repre-\nsentation (e.g., 257/513/1025 frequency bins to 64/96/128\nMel-frequency bins). In our study, the number of mel-\nbins is set to 96 and the magnitude of mel-spectrogram is\nmapped to decibel scale ( log10X), following [8] since it is\nalso shown to be crucial in [7].\nIn the proposed system, there are ﬁve layers of convolu-\ntional and sub-sampling in the convnet as shown in Figure\n1. This convnet structure with 2-dimensional 3\u00023kernels\nand 2-dimensional convolution, which is often called Vg-\ngnet [44], is expected to learn hierarchical time-frequency141Figure 1 :A block diagram of the training and feature extraction\nprocedures. Exponential linear unit (ELU) is used as an activation\nfunction in all convolutional layers [15]. Max-pooling of (2, 4),\n(4, 4), (4, 5), (2, 4), (4, 4) is applied after every convolutional\nlayer respectively. In all the convolutional layers, the kernel sizes\nare (3, 3), numbers of channels Nis 32, and Batch normalisation\nis used [24]. The input has a single channel, 96-mel bins, and\n1360 temporal frames. After training, the feature maps from 1st–\n4th layers are subsampled using average pooling while the feature\nmap of 5th layer is used as it is, since it is already scalar (size\n1\u00021). Those 32-dimensional features are concatenated to form\naconvnet feature .\npatterns. This structure was originally proposed for visual\nimage classiﬁcation and has been found to be effective and\nefﬁcient in music classiﬁcation1[11].\n2.2 Representation Transfer\nIn this section, we explain how features are extracted from\na pre-trained convolutional network. In the remainder of\nthe paper, this feature is referred to as pre-trained convnet\nfeature , or simply convnet feature .\nIt is already well understood how deep convnets learn\nhierarchical features in visual image classiﬁcation [58].\nBy convolution operations in the forward path, lower-level\nfeatures are used to construct higher-level features. Sub-\nsampling layers reduce the size of the feature maps while\nadding local invariance. In a deeper layer, as a result, the\nfeatures become more invariant to (scaling/location) dis-\ntortions and more relevant to the target task.\nThis type of hierarchy also exists when a convnet is\ntrained for a music-related task. Visualisation and soni-\nﬁcation of convnet features for music genre classiﬁcation\nhas shown the different levels of hierarchy in convolutional\nlayers [13], [9].\nSuch a hierarchy serves as a motivation for the pro-\nposed transfer learning. Relying solely on the last hidden\nlayer may not maximally extract the knowledge from a pre-\ntrained network. For example, low-level information such\nas tempo, pitch, (local) harmony or envelop can be cap-\ntured in early layers, but may not be preserved in deeper\nlayers due to the constraints that are introduced by the net-\nwork structure: aggregating local information by discard-\ning less-relevant information in subsampling. For the same\nreason, deep scattering networks [6] and a convnet for mu-\n1For more recent information on kernel shapes for music classiﬁca-\ntion, please see [40].sic tagging introduced in [28] use multi-layer representa-\ntions.\nBased on this insight, we propose to use not only the\nactivations of the ﬁnal hidden layer but also the activations\nof (up to) allintermediate layers to ﬁnd the most effective\nrepresentation for each task. The ﬁnal feature is generated\nby concatenating these features as demonstrated in Figure\n1, where all the ﬁve layers are concatenated to serve as an\nexample.\nGiven ﬁve layers, there areP5\nn=15Cn= 31 strate-\ngies of layer-wise combination. In our experiment, we\nperform a nearly exhaustive search and report all results.\nWe designate each strategy by the indices of layers em-\nployed. For example, a strategy named ‘ 135’ refers to\nusing a 32\u00023 = 96 -dimensional feature vector that con-\ncatenates the ﬁrst, third, and ﬁfth layer convnet features.\nDuring the transfer, average-pooling is used for the 1st–\n4th layers to reduce the size of feature maps to 1\u00021as\nillustrated in Figure 1. Averaging is chosen instead of max\npooling because it is more suitable for summarising the\nglobal statistics of large regions, as done in the last layer\nin [30]. Max-pooling is often more suitable for capturing\nthe existence of certain patterns, usually in small and local\nregions2.\nLastly, there have been works suggesting random-\nweights (deep) neural networks including deep convnet can\nwork well as a feature extractor [22] [59] (Not identical,\nbut a similar approach is transferring knowledge from an\nirrelevant domain, e.g., visual image recognition, to mu-\nsic task [19].) We report these results from random con-\nvnet features and denote it as random convnet feature . As-\nsessing performances of random convnet feature will help\nto clarify the contributions of the pre-trained knowledge\ntransfer versus the contributions of the convnet structure\nand nonlinear high-dimensional transformation.\n2.3 Classiﬁers and Regressors of Target Tasks\nVariants of support vector machines (SVMs) [45, 50] are\nused as a classiﬁer and regressor. SVMs work efﬁciently\nin target tasks with small training sets, and outperformed\nK-nearest neighbours in our work for all the tasks in a pre-\nliminary experiment. Since there are many works that use\nhand-written features and SVMs, using SVMs enables us\nto focus on comparing the performances of features.\n3. PREPARATION\n3.1 Source Task: Music Tagging\nIn the source task, 244,224 preview clips of the Mil-\nlion Song Dataset [5] are used (201,680/12,605/25,940\nfor training/validation/test sets respectively) with top-50\nlast.fm tags including genres, eras, instrumentations, and\nmoods. Mel-spectrograms are extracted from music sig-\nnals in real-time on the GPU using Kapre [12]. Binary\ncross-entropy is used as the loss function during training.\n2Since the average is affected by zero-padding which is applied to sig-\nnals that are shorter than 29 seconds, those signals are repeated to create\n29-second signals. This only happens in Task 5 and 6 in the experiment.142 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Task Dataset name #clips Metric #classes\nT1.Ballroom dance genre classiﬁcation Extended ballroom [32] 4,180 Accuracy 13\nT2.Genre classiﬁcation Gtzan genre [53] 1,000 Accuracy 10\nT3.Speech/music classiﬁcation Gtzan speech/music [52] 128 Accuracy 2\nT4.Emotion prediction EmoMusic (45-second) [46] 744Coefﬁcient of\ndetermination ( r2)N/A\n(2-dimensional)\nT5.V ocal/non-vocal classiﬁcation Jamendo [41] 4,086 Accuracy 2\nT6.Audio event classiﬁcation Urbansound8K [42] 8,732 Accuracy 10\nTable 1 : The details of the six tasks and datasets used in our transfer learning evaluation.\nThe ADAM optimisation algorithm [25] is used for accel-\nerating stochastic gradient descent. The convnet achieves\n0.849 AUC-ROC score (Area Under Curve - Receiver Op-\nerating Characteristic) on the test set. We use the Keras\n[14] and Theano [51] frameworks in our implementation.\n3.2 Target Tasks\nSix datasets are selected to be used in six target tasks. They\nare summarised in Table 1.\n\u000fTask 1: The Extended ballroom dataset consists of spe-\nciﬁc Ballroom dance sub-genres.\n\u000fTask 2: The Gtzan genre dataset has been extremely\npopular, although some ﬂaws have been found [48].\n\u000fTask 3: The dataset size is smaller than the others by an\norder of magnitude.\n\u000fTask 4: Emotion predition on the arousal-valence plane.\nWe evaluate arousal and valence separately. We trim and\nuse the ﬁrst 29-second from the 45-second signals.\n\u000fTask 5. Excerpts are subsegments from tracks with bi-\nnary labels ( ‘vocal’ and‘non-vocal’ ). Many of them are\nshorter than 29s. This dataset is provided for bench-\nmarking frame-based vocal detection while we use it as\na pre-segmented classiﬁcation task, which may be easier\nthan the original task.\n\u000fTask 6: This is a non-musical task. For example, the\nclasses include air conditioner ,car horn , and dog bark .\nAll excerpts are shorter than 4 seconds.\n3.3 Baseline Feature and Random Convnet Feature\nAs a baseline feature, the means and standard deviations\nof 20 Mel-Frequency Cepstral Coefﬁcients (MFCCs), and\ntheir ﬁrst and second-order derivatives are used. In this pa-\nper, this baseline feature is called MFCCs orMFCC vec-\ntors. MFCC is chosen since it has been adopted in many\nmusic information retrieval tasks and is known to provide\na robust representation. Librosa [34] is used for MFCC\nextraction and audio processing.\nThe random convnet feature is extracted using the iden-\ntical convnet structure of the source task and after random\nweights initialisation with a normal distribution [21] but\nwithout a training.\n4. EXPERIMENTS\n4.1 Conﬁgurations\nFor Tasks 1-4, the experiments are done with 10-fold cross-\nvalidation using stratiﬁed splits. For Task 5, pre-deﬁned\n/uni00000017/uni00000268\n/uni00000084/uni00000083/uni0000008e/uni0000008e/uni00000094/uni00000091/uni00000091/uni0000008f\n/uni00000089/uni00000087/uni00000090/uni00000094/uni00000087\n/uni00000183/uni00000083/uni00000085/uni00000085/uni00000184/uni00000017/uni00000269\n/uni00000089/uni00000096/uni0000009c/uni00000083/uni00000090\n/uni00000089/uni00000087/uni00000090/uni00000094/uni00000087\n/uni00000183/uni00000083/uni00000085/uni00000085/uni00000184/uni00000017/uni0000026a\n/uni00000089/uni00000096/uni0000009c/uni00000083/uni00000090\n/uni00000095/uni00000092/uni00000087/uni00000087/uni00000085/uni0000008a/uni00000175/uni0000008f/uni00000097/uni00000095/uni0000008b/uni00000085\n/uni00000183/uni00000083/uni00000085/uni00000085/uni00000184/uni00000017/uni0000026b/uni00000083\n/uni00000087/uni0000008f/uni00000091/uni00000096/uni0000008b/uni00000091/uni00000090\n/uni0000017f/uni00000083/uni00000094/uni00000091/uni00000097/uni00000095/uni00000083/uni0000008e/uni00000180\n/uni00000183/uni00000094/uni0000024b/uni00000269/uni00000184/uni00000017/uni0000026b/uni00000098\n/uni00000087/uni0000008f/uni00000091/uni00000096/uni0000008b/uni00000091/uni00000090\n/uni0000017f/uni00000098/uni00000083/uni0000008e/uni00000087/uni00000090/uni00000085/uni00000087/uni00000180\n/uni00000183/uni00000094/uni0000024b/uni00000269/uni00000184/uni00000017/uni0000026c\n/uni00000098/uni00000091/uni00000085/uni00000083/uni0000008e\n/uni00000090/uni00000091/uni00000090/uni0000015e/uni00000098/uni00000091/uni00000085/uni00000083/uni0000008e\n/uni00000183/uni00000083/uni00000085/uni00000085/uni00000184/uni00000017/uni0000026d\n/uni00000083/uni00000097/uni00000086/uni0000008b/uni00000091\n/uni00000087/uni00000098/uni00000087/uni00000090/uni00000096\n/uni00000183/uni00000083/uni00000085/uni00000085/uni00000184/uni0000015c/uni00000265/uni0000015c/uni00000269/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015c\n/uni0000015c/uni0000026f/uni0000026d/uni0000026e/uni0000015c/uni0000026f/uni00000268/uni00000270\n/uni0000015c/uni0000026d/uni00000269/uni0000026a/uni0000015c/uni00000270/uni0000026b/uni00000270/uni0000015c/uni0000026f/uni00000270/uni0000026f\n/uni0000015c/uni0000026e/uni0000026f/uni00000268\n/uni0000015c/uni0000026d/uni0000026d/uni00000265/uni0000015c/uni00000270/uni0000026b/uni0000026c/uni00000268/uni0000015c\n/uni0000015c/uni00000270/uni00000270/uni00000269/uni0000015c/uni00000270/uni00000270/uni00000269\n/uni00000268/uni0000015c\n/uni0000015c/uni0000026d/uni0000026c/uni0000026d\n/uni0000015c/uni0000026c/uni0000026e/uni00000265/uni0000015c/uni0000026c/uni0000026c/uni00000268/uni0000015c/uni0000026e/uni00000265/uni0000026b\n/uni0000015c/uni0000026b/uni0000026d/uni00000269\n/uni0000015c/uni0000026a/uni0000026c/uni0000026b\n/uni0000015c/uni00000269/uni00000265/uni00000268/uni0000015c/uni0000026c/uni00000265/uni00000265/uni0000015c/uni00000270/uni0000026e/uni00000269\n/uni0000015c/uni00000270/uni0000026c/uni00000269/uni0000015c/uni00000270/uni00000268/uni0000026e\n/uni0000015c/uni0000026d/uni00000270/uni00000268/uni0000015c/uni0000026e/uni00000268/uni0000026b/uni0000015c/uni0000026d/uni0000026c/uni00000268/uni0000015c/uni0000026e/uni00000270/uni00000265/uni00000005/uni00000087/uni00000095/uni00000096/uni00000003/uni00000091/uni00000088\n/uni00000085/uni00000091/uni00000090/uni00000098/uni00000090/uni00000087/uni00000096/uni00000006/uni00000091/uni00000090/uni00000085/uni00000083/uni00000096/uni0000015c\n/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000003/uni000002ab/uni00000003/uni00000010/uni00000009/uni00000006/uni00000006/uni00000095/uni00000010/uni00000009/uni00000006/uni00000006/uni00000095\n/uni00000084/uni00000083/uni00000095/uni00000087/uni0000008e/uni0000008b/uni00000090/uni00000087/uni00000016/uni00000091/uni00000017/uni00000004Figure 2 :Summary of performances of the convnet feature\n(blue), MFCCs (purple), and state-of-the-art (red) for Task 1-6\n(State-of-the-art of Task 5 does not exist).\ntraining/validation/test sets are used. The experiment on\nTask 6 is done with 10-fold cross-validation without re-\nplacement to prevent using the sub-segments from the\nsame recordings in training and validation. The SVM pa-\nrameters are optimised using grid-search based on the vali-\ndation results. Kernel type/bandwidth of radial basis func-\ntion and the penalty parameter are selected from the ranges\nbelow:\n\u000fKernel type: [linear ,radial ]\n–Bandwidth\rin radial basis function :\n[1=23;1=25;1=27;1=29;1=211;1=213;1=Nf]\n\u000fPenalty parameter C:[0:1;2:0;8:0;32:0]\nA radial basis function is exp(\u0000\rjx\u0000x0j2), and\randNf\nrefer to the radial kernel bandwidth and the dimensionality\nof feature vector respectively. With larger C, the penalty\nparameter or regularisation parameter, the loss function\ngives more penalty to misclassiﬁed items and vice versa.\nWe use Scikit-learn [38] for these target tasks. The code\nfor the data preparation, experiment, and visualisation are\navailable on GitHub3.\n4.2 Results and Discussion\nFigure 2 shows a summary of the results. The scores of\nthei)best performing convnet feature, ii)concatenating\n‘12345 ’4convnet feature and MFCCs, iii)MFCC fea-\nture, and iv)state-of-the-art algorithms for all the tasks.\nIn all the six tasks, the majority of convnet features\noutperforms the baseline feature. Concatenating MFCCs\n3https://github.com/keunwoochoi/transfer_\nlearning_music\n4Again, ‘ 12345 ’ refers to the convnet feature that is concatenated\nfrom 1st–5th layers. For another example, ‘ 135’ means concatenating\nthe features from ﬁrst, third, and ﬁfth layers.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 143with ‘ 12345 ’ convnet feature usually does not show im-\nprovement over a pure convnet feature except in Task 6,\naudio event classiﬁcation. Although the reported state-of-\nthe art is typically better, almost all methods rely on musi-\ncal knowledge and hand-crafted features, yet our features\nperform competitively. An in-depth look at each task is\ntherefore useful to provide insight.\nIn the following subsections, the details of each task are\ndiscussed with more results presented from (almost) ex-\nhaustive combinations of convnet features as well as ran-\ndom convnet features at all layers. For example, in Fig-\nure 3, the scores of 28 different convnet feature combina-\ntions are shown with blue bars. The narrow, grey bars next\nto the blue bars indicate the scores of random convnet fea-\ntures. The other three bars on the right represent the scores\nof the concatenation of ‘ 12345 ’ + MFCC feature, MFCC\nfeature, and the reported state-of-the-art methods respec-\ntively. The rankings within the convnet feature combina-\ntions are also shown inthe bars where top-7 and lower-7\nare highlighted.\nWe only brieﬂy discuss the results of random convnet\nfeatures here. The best performing random convnet fea-\ntures do not outperform the best-performing convnet fea-\ntures in any task. In most of the combinations, convnet\nfeatures outperformed the corresponding random convnet\nfeatures, although there are few exceptions. However, ran-\ndom convnet features also achieved comparable or even\nbetter scores than MFCCs, indicating i)a signiﬁcant part\nof the strength of convnet features comes from the network\nstructure itself, and ii)random convnet features can be use-\nful especially if there is not a suitable source task.\n4.2.1 Task 1. Ballroom Genre Classiﬁcation\nFigure 3 shows the performances of different features\nfor Ballroom dance classiﬁcation. The highest score is\nachieved using the convnet feature ‘ 123’ with 86.7% of\naccuracy. The convnet feature shows good performances,\neven outperforming some previous works that explicitly\nuse rhythmic features.\nThe result clearly shows that low-level features are cru-\ncial in this task. All of the top-7 strategies of convnet fea-\nture include the second layer, and 6=7of them include the\nﬁrst layer. On the other hand, the lower-7 are [‘ 5’, ‘4’,\n‘3’, ‘45’, ‘35’, ‘2’, ‘25’], none of which includes the\nﬁrst layer. Even ‘ 1’ achieves a reasonable performance\n(73.8%).\nThe importance of low-level features is also supported\nby known properties of this task. The ballroom genre la-\nbels are closely related to rhythmic patterns and tempo [32]\n[49]. However, there is no label directly related to tempo\nin the source task. Moreover, deep layers in the proposed\nstructure are conjectured to be mostly invariant to tempo.\nAs a result, high-level features from the fourth and ﬁfth\nlayers poorly contribute to the task relative to those from\nthe ﬁrst, second, and third layers.\nThe state-of-the-art algorithm which is also the only al-\ngorithm that used the same dataset due to its recent re-\nlease uses 2D scale transform , an alternative representa-\ntion of music signals for rhythm-related tasks [33], and\n/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni00000268/uni00000269/uni0000026b/uni00000268/uni00000269/uni0000026c/uni00000268/uni0000026a/uni0000026b/uni00000268/uni0000026a/uni0000026c/uni00000268/uni0000026b/uni0000026c/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026c/uni00000269/uni0000026b/uni0000026c/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c/uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c/uni0000026a/uni0000026b/uni0000026a/uni0000026c/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c\n/uni00000085/uni00000091/uni00000090/uni00000085/uni00000083/uni00000096/uni0000008f/uni00000088/uni00000085/uni00000085/uni00000016/uni00000091/uni00000017/uni00000004/uni0000015c/uni00000265/uni0000015c/uni00000269/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015c/uni00000004/uni00000085/uni00000085/uni00000097/uni00000094/uni00000083/uni00000085/uni0000009b/uni0000015c/uni0000026f/uni0000026d/uni0000026e/uni0000015c/uni0000026f/uni00000268/uni00000270\n/uni0000015c/uni0000026d/uni00000269/uni0000026a/uni0000015c/uni00000270/uni0000026b/uni00000270\n/uni00000268/uni00000269 /uni0000026a /uni0000026b /uni0000026c/uni0000026d/uni0000026e /uni0000026f /uni00000270/uni00000268/uni00000265 /uni00000268/uni00000268 /uni00000268/uni00000269/uni00000268/uni0000026a /uni00000268/uni0000026b /uni00000268/uni0000026c /uni00000268/uni0000026d /uni00000268/uni0000026e /uni00000268/uni0000026f/uni00000268/uni00000270 /uni00000269/uni00000265/uni00000269/uni00000268/uni00000269/uni00000269 /uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c/uni00000269/uni0000026d/uni00000269/uni0000026e/uni00000269/uni0000026f/uni00000017/uni00000268/uni0000015c/uni00000003/uni00000008/uni0000009a/uni00000096/uni00000087/uni00000090/uni00000086/uni00000087/uni00000086/uni00000003/uni00000005/uni00000083/uni0000008e/uni0000008e/uni00000094/uni00000091/uni00000091/uni0000008fFigure 3 :Performances of Task 1 - Ballroom dance genre clas-\nsiﬁcation of convnet features (with random convnet features in\ngrey), MFCCs, and the reported state-of-the-art method. (Note\nthe exception that the SoTA is reported in weighted average re-\ncall.)\n/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni00000268/uni00000269/uni0000026b/uni00000268/uni00000269/uni0000026c/uni00000268/uni0000026a/uni0000026b/uni00000268/uni0000026a/uni0000026c/uni00000268/uni0000026b/uni0000026c/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026c/uni00000269/uni0000026b/uni0000026c/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c/uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c/uni0000026a/uni0000026b/uni0000026a/uni0000026c/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c\n/uni00000085/uni00000091/uni00000090/uni00000085/uni00000083/uni00000096/uni0000008f/uni00000088/uni00000085/uni00000085/uni00000016/uni00000091/uni00000017/uni00000004/uni0000015c/uni00000265/uni0000015c/uni00000269/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015c/uni00000004/uni00000085/uni00000085/uni00000097/uni00000094/uni00000083/uni00000085/uni0000009b/uni0000015c/uni0000026f/uni00000270/uni0000026f\n/uni0000015c/uni0000026e/uni0000026f/uni00000268\n/uni0000015c/uni0000026d/uni0000026d/uni00000265/uni0000015c/uni00000270/uni0000026b/uni0000026c\n/uni00000268/uni00000269/uni0000026a /uni0000026b /uni0000026c/uni0000026d/uni0000026e/uni0000026f /uni00000270/uni00000268/uni00000265 /uni00000268/uni00000268 /uni00000268/uni00000269/uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c /uni00000268/uni0000026d/uni00000268/uni0000026e /uni00000268/uni0000026f /uni00000268/uni00000270 /uni00000269/uni00000265/uni00000269/uni00000268 /uni00000269/uni00000269/uni00000269/uni0000026a /uni00000269/uni0000026b/uni00000269/uni0000026c/uni00000269/uni0000026d/uni00000269/uni0000026e /uni00000269/uni0000026f/uni00000017/uni00000269/uni0000015c/uni00000003/uni0000000a/uni00000096/uni0000009c/uni00000083/uni00000090/uni00000003/uni0000008f/uni00000097/uni00000095/uni0000008b/uni00000085/uni00000003/uni00000089/uni00000087/uni00000090/uni00000094/uni00000087\nFigure 4 :Performances of Task 2 - Gtzan music genre classiﬁ-\ncation of convnet features (with random convnet features in grey),\nMFCCs, and the reported state-of-the-art method.\nreports 94.9% of weighted average recall. For additional\ncomparisons, there are several works that use the Ball-\nroom dataset [18]. This has 8 classes and it is smaller\nin size than the Extended Ballroom dataset (13 classes).\nLaykartsis and Lerch [31] combines beat histogram and\ntimbre features to achieve 76.7%. Periodicity analysis with\nSVM classiﬁer in Gkiokas et al. [17] respectively shows\n88.9%/85.6 - 90.7%, before and after feature selection.\n4.2.2 Task 2. Gtzan Music Genre Classiﬁcation\nFigure 4 shows the performances on Gtzan music genre\nclassiﬁcation. The convnet feature shows 89.8% while the\nconcatenated feature and MFCCs respectively show only\n78.1% and 66.0% of accuracy. Although there are meth-\nods that report accuracies higher than 94.5%, we set 94.5%\nas the state-of-the-art score following the dataset analysis\nin [48], which shows that the perfect score cannot surpass\n94.5% considering the noise in the Gtzan dataset.\nAmong a signiﬁcant number of works that use the Gtzan\nmusic genre dataset, we describe four methods in more\ndetail. Three of them use an SVM classiﬁer, which en-\nables us to focus on the comparison with our feature.\nArabi and Lu [1] is most similar to the proposed convnet\nfeatures in a way that it combines low-level and high-level\nfeatures and shows a similar performance. Beniya et al. [4]\nand Huang et al. [23] report the performances with many\nlow-level features before and after applying feature selec-\ntion algorithms. Only the latter outperforms the proposed\nmethod and only after feature selection.\n\u000fArabi and Lu [1] uses not only low-level features such as\nfspectral centroid/ﬂatness/roll-off/ﬂux g, but also high-\nlevel musical features such as fbeat, chord distribution\nand chord progressions g. The best combination of the\nfeatures shows 90.79% of accuracy.\n\u000fBeniya et al. [4] uses a particularly rich set of statistics\nsuch asfmean, standard deviation, skewness, kurtosis,144 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017/uni00000084/uni0000008e/uni00000097/uni00000087/uni00000095\n/uni00000085/uni0000008e/uni00000083/uni00000095/uni00000095/uni0000008b/uni00000085/uni00000083/uni0000008e/uni00000085/uni00000091/uni00000097/uni00000090/uni00000096/uni00000094/uni0000009b/uni00000086/uni0000008b/uni00000095/uni00000085/uni00000091/uni0000008a/uni0000008b/uni00000092/uni0000008a/uni00000091/uni00000092/uni0000008c/uni00000083/uni0000009c/uni0000009c/uni0000008f/uni00000087/uni00000096/uni00000083/uni0000008e/uni00000092/uni00000091/uni00000092\n/uni00000094/uni00000087/uni00000089/uni00000089/uni00000083/uni00000087/uni00000094/uni00000091/uni00000085/uni0000008d/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015c/uni00000004/uni00000085/uni00000085/uni00000097/uni00000094/uni00000083/uni00000085/uni0000009b/uni0000015c/uni00000268/uni0000026e/uni0000015c/uni00000265/uni0000026c/uni0000015c/uni00000268/uni00000269/uni0000015c/uni00000268/uni0000026c/uni0000015c/uni00000268/uni0000026f/uni0000015c/uni00000268/uni0000026a/uni0000015c/uni00000268/uni00000269/uni0000015c/uni00000268/uni00000265/uni0000015c/uni00000265/uni0000026f/uni0000015c/uni00000268/uni00000265 /uni00000085/uni00000091/uni00000090/uni00000098/uni00000090/uni00000087/uni00000096\n/uni00000250/uni0000026c/uni00000250\n/uni00000085/uni00000091/uni00000090/uni00000098/uni00000090/uni00000087/uni00000096\n/uni00000250/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000250Figure 5 :Comparison of per-label results of two convnet fea-\nture strategies, ‘ 12345 ’ and ‘ 5’ for Gtzan music genre classiﬁ-\ncation. Numbers denote the differences of scores.\n/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni00000268/uni00000269/uni0000026b/uni00000268/uni00000269/uni0000026c/uni00000268/uni0000026a/uni0000026b/uni00000268/uni0000026a/uni0000026c/uni00000268/uni0000026b/uni0000026c/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026c/uni00000269/uni0000026b/uni0000026c/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c/uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c/uni0000026a/uni0000026b/uni0000026a/uni0000026c/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c\n/uni00000085/uni00000091/uni00000090/uni00000085/uni00000083/uni00000096/uni0000008f/uni00000088/uni00000085/uni00000085/uni00000016/uni00000091/uni00000017/uni00000004/uni0000015c/uni00000265/uni0000015c/uni00000269/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015c/uni00000004/uni00000085/uni00000085/uni00000097/uni00000094/uni00000083/uni00000085/uni0000009b/uni0000015c/uni00000270/uni00000270/uni00000269/uni0000015c/uni00000270/uni00000270/uni00000269/uni00000017/uni0000026a/uni0000015c/uni00000003/uni0000000a/uni00000096/uni0000009c/uni00000083/uni00000090/uni00000003/uni00000095/uni00000092/uni00000087/uni00000087/uni00000085/uni0000008a/uni00000175/uni0000008f/uni00000097/uni00000095/uni0000008b/uni00000085\nFigure 6 :Performances of Task 3 - Speech/music classiﬁca-\ntion of convnet features (with random convnet features in grey),\nMFCCs, and the reported state-of-the-art method. All scores of\nconvnet features and SoTA are 1.0 and omitted in the plot.\ncovariancegof many low-level features including fRMS\nenergy, attack, tempo, spectral features, zero-crossing,\nMFCC, dMFCC, ddMFCC, chromagram peak and cen-\ntroidg. The feature vector dimensionality is reduced\nby MRMR (max-relevance and min-redundancy) [39] to\nobtain the highest classiﬁcation accuracy of 87.9%.\n\u000fHuang et al. [23] adopts another feature selection algo-\nrithm, self-adaptive harmony search [55]. The method\nuses statistics such as fmean, standard deviation gof\nmany features including fenergy , pitch, and timbral\nfeaturesgand their derivatives. The original 256-\ndimensional feature achieved 84.3% of accuracy which\nincreases to 92.2% and 97.2% after feature selection.\n\u000fReusing AlexNet [26], a pre-trained convnet for visual\nimage recognition achieved 78% of accuracy [19].\nIn summary, the convnet feature achieves better perfor-\nmance than many approaches which use extensive music\nfeature sets without feature selection as well as some of the\napproaches with feature selection. For this task, it turns out\nthat combining features from all layers is the best strategy.\nIn the results, ‘ 12345 ’, ‘2345 ’, and ‘ 1234 ’ are three best\nconﬁgurations, and all of the top-7 scores are from those\nstrategies that use more than three layers. On the contrary,\nall lower-7 scores are from those with only 1 or 2 layers.\nThis is interesting since the majority (7/10) of the target\nlabels already exists in source task labels, by which it is\nreasonable to assume that the necessary information can\nbe provided only with the last layer for those labels. Even\nin such a situation, however, low-level features contribute\nto improving the genre classiﬁcation performance5.\nAmong the classes of target task, classical anddisco ,\nreggae do not exist in the source task classes. Based on\nthis, we consider two hypotheses, i)the performances of\nthose three classes may be lower than the others, ii)low-\nlevel features may play an important role to classify them\nsince high-level feature from the last layer may be biased\nto the other 7 classes which exist in the source task. How-\never, both hypotheses are rebutted by comparing the per-\nformances for each genres with convnet feature ‘ 5’ and\n5On the contrary, in Task 5 - music emotion classiﬁcation, high-level\nfeature plays a dominant role (see Section 4.2.4).\n/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni00000268/uni00000269/uni0000026b/uni00000268/uni00000269/uni0000026c/uni00000268/uni0000026a/uni0000026b/uni00000268/uni0000026a/uni0000026c/uni00000268/uni0000026b/uni0000026c/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026c/uni00000269/uni0000026b/uni0000026c/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c/uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c/uni0000026a/uni0000026b/uni0000026a/uni0000026c/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c\n/uni00000085/uni00000091/uni00000090/uni00000085/uni00000083/uni00000096/uni0000008f/uni00000088/uni00000085/uni00000085/uni00000016/uni00000091/uni00000017/uni00000004/uni0000015c/uni00000265/uni0000015c/uni00000269/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015cr2/uni00000003/uni00000095/uni00000085/uni00000091/uni00000094/uni00000087 /uni0000015c/uni0000026d/uni0000026c/uni0000026d/uni0000015c/uni0000026c/uni0000026e/uni00000265\n/uni0000015c/uni0000026c/uni0000026c/uni00000268/uni0000015c/uni0000026e/uni00000265/uni0000026b\n/uni00000268 /uni00000269 /uni0000026a/uni0000026b/uni0000026c /uni0000026d /uni0000026e /uni0000026f /uni00000270 /uni00000268/uni00000265 /uni00000268/uni00000268 /uni00000268/uni00000269/uni00000268/uni0000026a /uni00000268/uni0000026b /uni00000268/uni0000026c /uni00000268/uni0000026d /uni00000268/uni0000026e /uni00000268/uni0000026f /uni00000268/uni00000270 /uni00000269/uni00000265 /uni00000269/uni00000268/uni00000269/uni00000269/uni00000269/uni0000026a /uni00000269/uni0000026b/uni00000269/uni0000026c/uni00000269/uni0000026d/uni00000269/uni0000026e/uni00000269/uni0000026f/uni00000017/uni0000026b/uni00000083/uni0000015c/uni00000003/uni00000010/uni00000097/uni00000095/uni0000008b/uni00000085/uni00000003/uni00000087/uni0000008f/uni00000091/uni00000096/uni0000008b/uni00000091/uni00000090/uni00000003/uni0000017f/uni00000083/uni00000094/uni00000091/uni00000097/uni00000095/uni00000083/uni0000008e/uni00000180\n/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni00000268/uni00000269/uni0000026b/uni00000268/uni00000269/uni0000026c/uni00000268/uni0000026a/uni0000026b/uni00000268/uni0000026a/uni0000026c/uni00000268/uni0000026b/uni0000026c/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026c/uni00000269/uni0000026b/uni0000026c/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c/uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c/uni0000026a/uni0000026b/uni0000026a/uni0000026c/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c\n/uni00000085/uni00000091/uni00000090/uni00000085/uni00000083/uni00000096/uni0000008f/uni00000088/uni00000085/uni00000085/uni00000016/uni00000091/uni00000017/uni00000004/uni0000015c/uni00000265/uni0000015c/uni00000269/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015cr2/uni00000003/uni00000095/uni00000085/uni00000091/uni00000094/uni00000087\n/uni0000015c/uni0000026b/uni0000026d/uni00000269\n/uni0000015c/uni0000026a/uni0000026c/uni0000026b\n/uni0000015c/uni00000269/uni0000026d/uni0000026e/uni0000015c/uni0000026c/uni00000265/uni00000265\n/uni00000268 /uni00000269 /uni0000026a/uni0000026b /uni0000026c /uni0000026d /uni0000026e/uni0000026f/uni00000270 /uni00000268/uni00000265 /uni00000268/uni00000268 /uni00000268/uni00000269/uni00000268/uni0000026a /uni00000268/uni0000026b /uni00000268/uni0000026c /uni00000268/uni0000026d/uni00000268/uni0000026e /uni00000268/uni0000026f /uni00000268/uni00000270 /uni00000269/uni00000265 /uni00000269/uni00000268/uni00000269/uni00000269 /uni00000269/uni0000026a/uni00000269/uni0000026b /uni00000269/uni0000026c/uni00000269/uni0000026d/uni00000269/uni0000026e /uni00000269/uni0000026f/uni00000017/uni0000026b/uni00000098/uni0000015c/uni00000003/uni00000010/uni00000097/uni00000095/uni0000008b/uni00000085/uni00000003/uni00000087/uni0000008f/uni00000091/uni00000096/uni0000008b/uni00000091/uni00000090/uni00000003/uni0000017f/uni00000098/uni00000083/uni0000008e/uni00000087/uni00000090/uni00000085/uni00000087/uni00000180Figure 7 :Performances of Task 4a (arousal) and 4v (valence) -\nMusic emotion prediction of convnet features (with random con-\nvnet features in grey), MFCCs, and the reported state-of-the-art\nmethod.\n‘12345 ’ as in Figure 5. First, with ‘ 5’ convnet feature,\nclassical shows the highest accuracy while both disco and\nreggae show accuracies around the average accuracy re-\nported over the classes. Second, aggregating early-layer\nfeatures affects all the classes rather than the three omit-\nted classes. This suggests that the convnet features are not\nstrongly biased towards the genres that are included in the\nsource task and can be used generally for target tasks with\nmusic different from those genres.\n4.2.3 Task 3. Gtzan Speech/music Classiﬁcation\nFigure 6 shows the accuracies of convnet features, base-\nline feature, and state-of-the-art [47] with low-level fea-\ntures including MFCCs and sparse dictionary learning for\nGtzan music/speech classiﬁcation. A majority of the con-\nvnet feature combinations achieve 100% accuracy. MFCC\nfeatures achieve 99.2%, but the error rate is trivial (0.8% is\none sample out of 128 excerpts).\nAlthough the source task is only about music tags, the\npre-trained feature in any layer easily solved the task, sug-\ngesting that the nature of music and speech signals in the\ndataset is highly distinctive.\n4.2.4 Task 4. Music Emotion Prediction\nFigure 7 shows the results for music emotion prediction\n(Task 4). The best performing convnet features achieve\n0.633 and 0.415 r2scores on arousal and valence axes re-\nspectively.\nOn the other hand, the state-of-the-art algorithm reports\n0.704 and 0.500 r2scores using music features with a re-\ncurrent neural network as a classiﬁer [56] that uses 4,777\naudio features including many functionals (such as quan-\ntiles, standard deviation, mean, inter peak distances ) of\n12 chroma features ,loudness ,RMS Energy ,zero crossing\nrate,14 MFCCs ,spectral energy ,spectral roll-off , etc.\nFor the prediction of arousal, there is a strong depen-\ndency on the last layer feature. All top-7 performances are\nfrom the feature vectors that include the ﬁfth layer. The\nﬁrst layer feature also seems important, since all of the top-\n5 strategies include the ﬁrst and ﬁfth layer features. ForProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 145/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni00000268/uni00000269/uni0000026b/uni00000268/uni00000269/uni0000026c/uni00000268/uni0000026a/uni0000026b/uni00000268/uni0000026a/uni0000026c/uni00000268/uni0000026b/uni0000026c/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026c/uni00000269/uni0000026b/uni0000026c/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c/uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c/uni0000026a/uni0000026b/uni0000026a/uni0000026c/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c\n/uni00000085/uni00000091/uni00000090/uni00000085/uni00000083/uni00000096/uni0000008f/uni00000088/uni00000085/uni00000085/uni00000016/uni00000091/uni00000017/uni00000004/uni0000015c/uni00000265/uni0000015c/uni00000269/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015c/uni00000004/uni00000085/uni00000085/uni00000097/uni00000094/uni00000083/uni00000085/uni0000009b/uni0000015c/uni00000270/uni0000026e/uni00000269 /uni0000015c/uni00000270/uni0000026c/uni0000026a\n/uni0000015c/uni00000270/uni00000268/uni0000026e/uni00000268/uni00000269 /uni0000026a/uni0000026b /uni0000026c/uni0000026d /uni0000026e /uni0000026f /uni00000270 /uni00000268/uni00000265 /uni00000268/uni00000268 /uni00000268/uni00000269 /uni00000268/uni0000026a /uni00000268/uni0000026b/uni00000268/uni0000026c /uni00000268/uni0000026d/uni00000268/uni0000026e /uni00000268/uni0000026f /uni00000268/uni00000270/uni00000269/uni00000265 /uni00000269/uni00000268 /uni00000269/uni00000269 /uni00000269/uni0000026a /uni00000269/uni0000026b /uni00000269/uni0000026c /uni00000269/uni0000026d /uni00000269/uni0000026e /uni00000269/uni0000026f/uni00000017/uni0000026c/uni0000015c/uni00000003/uni00000019/uni00000091/uni00000085/uni00000083/uni0000008e/uni00000175/uni00000090/uni00000091/uni00000090/uni0000015e/uni00000098/uni00000091/uni00000085/uni00000083/uni0000008eFigure 8 :Performances of Task 5 - V ocal detection of convnet\nfeatures (with random convnet features in grey) and MFCCs.\n/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni00000268/uni00000269/uni0000026b/uni00000268/uni00000269/uni0000026c/uni00000268/uni0000026a/uni0000026b/uni00000268/uni0000026a/uni0000026c/uni00000268/uni0000026b/uni0000026c/uni00000269/uni0000026a/uni0000026b/uni00000269/uni0000026a/uni0000026c/uni00000269/uni0000026b/uni0000026c/uni0000026a/uni0000026b/uni0000026c/uni00000268/uni00000269/uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c/uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c/uni0000026a/uni0000026b/uni0000026a/uni0000026c/uni0000026b/uni0000026c/uni00000268/uni00000269/uni0000026a/uni0000026b/uni0000026c\n/uni00000085/uni00000091/uni00000090/uni00000085/uni00000083/uni00000096/uni0000008f/uni00000088/uni00000085/uni00000085/uni00000016/uni00000091/uni00000017/uni00000004/uni0000015c/uni00000265/uni0000015c/uni00000269/uni0000015c/uni0000026b/uni0000015c/uni0000026d/uni0000015c/uni0000026f/uni00000268/uni0000015c/uni00000004/uni00000085/uni00000085/uni00000097/uni00000094/uni00000083/uni00000085/uni0000009b/uni0000015c/uni0000026d/uni00000270/uni00000268 /uni0000015c/uni0000026e/uni00000268/uni0000026b\n/uni0000015c/uni0000026d/uni0000026c/uni00000268/uni0000015c/uni0000026e/uni00000270/uni00000265\n/uni00000268/uni00000269/uni0000026a /uni0000026b/uni0000026c/uni0000026d /uni0000026e /uni0000026f/uni00000270 /uni00000268/uni00000265 /uni00000268/uni00000268 /uni00000268/uni00000269 /uni00000268/uni0000026a/uni00000268/uni0000026b/uni00000268/uni0000026c /uni00000268/uni0000026d /uni00000268/uni0000026e /uni00000268/uni0000026f /uni00000268/uni00000270 /uni00000269/uni00000265/uni00000269/uni00000268/uni00000269/uni00000269 /uni00000269/uni0000026a/uni00000269/uni0000026b/uni00000269/uni0000026c /uni00000269/uni0000026d /uni00000269/uni0000026e /uni00000269/uni0000026f/uni00000017/uni0000026d/uni0000015c/uni00000003/uni00000004/uni00000085/uni00000091/uni00000097/uni00000095/uni00000096/uni0000008b/uni00000085/uni00000003/uni00000087/uni00000098/uni00000087/uni00000090/uni00000096\nFigure 9 :Performances of Task 6 - Acoustic event detection of\nconvnet features (with random convnet features in grey), MFCCs,\nand the reported state-of-the-art method.\nvalence prediction, the third layer feature seems to be the\nmost important one. The third layer is included in all of\nthe top-6 strategies. Moreover, ‘ 3’ strategy was found to\nbe best performing among strategies with single layer fea-\nture.\nTo summarise the results, the predictions of arousal and\nvalence rely on different layers, for which they should be\noptimised separately. In order to remove the effect of the\nchoice of a classiﬁer and assess solely the effect of fea-\ntures, we compare our approach to the baseline method\nof [56] which is based on the same 4,777 features with\nSVM, not a recurrent neural network. The baseline method\nachieves .541 and .320 r2scores respectively on arousal\nand valence, both of which are lower than those achieved\nby using the proposed convnet feature. This further con-\nﬁrms the effectiveness of the proposed convnet features.\n4.2.5 Task 5. Vocal/non-vocal Classiﬁcation\nFigure 8 presents the performances on vocal/non-vocal\nclassiﬁcation using the Jamendo dataset [41]. There is\nno known state-of-the-art result, as the dataset is usually\nused for frame-based vocal detection/segmentation . Pre-\nsegmented Excerpt classiﬁcation is the task we formulate\nin this paper. For this dataset, the fourth layer plays the\nmost important role. All the 14 combinations that include\nthe fourth layer outperformed the other 14 strategies with-\nout the fourth layer.\n4.2.6 Task 6. Acoustic Event Detection\nFigure 9 shows the results on acoustic event classiﬁca-\ntion using Urbansound8K dataset [42]. Since this is not\na music-related task, there are no common tags between\nthe source and target tasks, and therefore the ﬁnal-layer\nfeature is not expected to be useful for the target task.\nThe strategy of concatenating ‘ 12345 ’ convnet fea-\ntures and MFCCs yields the best performance. Among\nconvnet features, ‘ 2345 ’, ‘12345 ’, ‘123’, and ‘ 234’\nachieve good accuracies. In contrast, those with only oneor two layers do not perform well. We were not able to\nobserve any particular dependency on a certain layer.\nSince the convnet features are trained on music, they\ndo not outperform a dedicated convnet trained for the tar-\nget task. The state-of-the-art method is based on a deep\nconvolutional neural network with data augmentation [43].\nWithout augmenting the training data, the accuracy of con-\nvnet in the same work is reported to be 74%, which is still\nhigher than our best result (71.4%).6\nThe convnet feature still shows better results than con-\nventional audio features, demonstrating its versatility even\nfor non-musical tasks. The method in [42] with fminimum,\nmaximum, median, mean, variance, skewness, kurtosis gof\n25 MFCCs andfmean and variance gof the ﬁrst and sec-\nond MFCC derivatives (225-dimensional feature) achieved\nonly 68% accuracy using the SVM classiﬁer. This is worse\nthan the performance of the best performing convnet fea-\nture.\nIt is notable again that unlike in the other tasks, concate-\nnating convnet feature and MFCCs results in an improve-\nment over either a convnet feature or MFCCs (71.4%).\nThis suggests that they are complementary to each other\nin this task.\n5. CONCLUSIONS\nWe proposed a transfer learning approach using deep learn-\ning and evaluated it on six music information retrieval\nand audio-related tasks. The pre-trained convnet was ﬁrst\ntrained to predict music tags and then aggregated features\nfrom the layers were transferred to solve genre classiﬁ-\ncation, vocal/non-vocal classiﬁcation, emotion prediction,\nspeech/music classiﬁcation, and acoustic event classiﬁca-\ntion problems. Unlike the common approach in transfer\nlearning, we proposed to use the features from every con-\nvolutional layers after applying an average-pooling to re-\nduce their feature map sizes.\nIn the experiments, the pre-trained convnet feature\nshowed good performance overall. It outperformed the\nbaseline MFCC feature for all the six tasks, a feature that\nis very popular in music information retrieval tasks be-\ncause it gives reasonable baseline performance in many\ntasks. It also outperformed the random-weights convnet\nfeatures for all the six tasks, demonstrating the improve-\nment by pre-training on a source task. Somewhat surpris-\ningly, the performance of the convnet feature is also very\ncompetitive with state-of-the-art methods designed specif-\nically for each task. The most important layer turns out\nto differ from task to task, but concatenating features from\nall the layers generally worked well. For all the ﬁve music\ntasks, concatenating MFCC feature onto convnet features\ndid not improve the performance, indicating the music in-\nformation in MFCC feature is already included in the con-\nvnet feature. We believe that transfer learning can alleviate\nthe data sparsity problem in MIR and can be used for a\nlarge number of different tasks.\n6Transfer learning targeting audio event classiﬁcation was recently in-\ntroduced in [2, 3] and achieved a state-of-the-art performance.146 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. REFERENCES\n[1] Arash Foroughmand Arabi and Guojun Lu. Enhanced\npolyphonic music genre classiﬁcation using high level\nfeatures. In Signal and Image Processing Applica-\ntions (ICSIPA), 2009 IEEE International Conference\non, pages 101–106. IEEE, 2009.\n[2] Relja Arandjelovi ´c and Andrew Zisserman. Look, lis-\nten and learn. arXiv preprint arXiv:1705.08168 , 2017.\n[3] Yusuf Aytar, Carl V ondrick, and Antonio Torralba.\nSoundnet: Learning sound representations from unla-\nbeled video. In Advances in Neural Information Pro-\ncessing Systems , pages 892–900, 2016.\n[4] Babu Kaji Baniya, Joonwhoan Lee, and Ze-Nian Li.\nAudio feature reduction and analysis for automatic mu-\nsic genre classiﬁcation. In Systems, Man and Cyber-\nnetics (SMC), 2014 IEEE International Conference on ,\npages 457–462. IEEE, 2014.\n[5] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whit-\nman, and Paul Lamere. The million song dataset. In\nProceedings of the 12th International Society for Mu-\nsic Information Retrieval Conference, October 24-28,\n2011, Miami, Florida , pages 591–596. University of\nMiami, 2011.\n[6] Joan Bruna and St ´ephane Mallat. Invariant scattering\nconvolution networks. IEEE transactions on pattern\nanalysis and machine intelligence , 35(8):1872–1886,\n2013.\n[7] Keunwoo Choi, Gy ¨orgy Fazekas, Kyunghyun Cho, and\nMark Sandler. The effects of noisy labels on deep\nconvolutional neural networks for music classiﬁcation.\narXiv:1706.02361 , 2017.\n[8] Keunwoo Choi, Gy ¨orgy Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional neural\nnetworks. In The 17th International Society of Mu-\nsic Information Retrieval Conference, New York, USA .\nInternational Society of Music Information Retrieval,\n2016.\n[9] Keunwoo Choi, Gy ¨orgy Fazekas, and Mark Sandler.\nExplaining deep convolutional neural networks on mu-\nsic classiﬁcation. arXiv preprint arXiv:1607.02444 ,\n2016.\n[10] Keunwoo Choi, Gy ¨orgy Fazekas, and Mark San-\ndler. Towards playlist generation algorithms using rnns\ntrained on within-track transitions. In Workshop on\nSurprise, Opposition, and Obstruction in Adaptive and\nPersonalized Systems (SOAP), Halifax, Canada, 2016 ,\n2016.\n[11] Keunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler, and\nKyunghyun Cho. Convolutional recurrent neural net-\nworks for music classiﬁcation. In 2017 IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing , 2017.[12] Keunwoo Choi, Deokjin Joo, and Juho Kim. Kapre:\nOn-gpu audio preprocessing layers for a quick imple-\nmentation of deep neural network models with keras.\nInMachine Learning for Music Discovery Workshop\nat 34th International Conference on Machine Learn-\ning. ICML, 2017.\n[13] Keunwoo Choi, Jeonghee Kim, Gy ¨orgy Fazekas, and\nMark Sandler. Auralisation of deep convolutional neu-\nral networks: Listening to learned features. In Interna-\ntional Society of Music Information Retrieval (ISMIR),\nLate-Breaking/Demo Session, Malaga, Spain . Interna-\ntional Society of Music Information Retrieval, 2015.\n[14] Franc ¸ois Chollet. Keras: Deep learn-\ning library for theano and tensorﬂow.\nhttps://github.com/fchollet/keras, 2015.\n[15] Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learn-\ning by exponential linear units (elus). arXiv preprint\narXiv:1511.07289 , 2015.\n[16] Adam Coates and Andrew Y Ng. Learning feature rep-\nresentations with k-means. In Neural Networks: Tricks\nof the Trade , pages 561–580. Springer, 2012.\n[17] Aggelos Gkiokas, Vassilis Katsouros, and Gy ¨orgy\nCarayannis. Towards multi-purpose spectral rhythm\nfeatures: An application to dance style, meter and\ntempo estimation. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , 24(11):1885–1896,\n2016.\n[18] Fabien Gouyon, Simon Dixon, Elias Pampalk, and\nGerhard Widmer. Evaluating rhythmic descriptors for\nmusical genre classiﬁcation. In 25th AES International\nConference, London, UK , 2004.\n[19] Grzegorz Gwardys and Daniel Grzywczak. Deep im-\nage features in music information retrieval. Interna-\ntional Journal of Electronics and Telecommunications ,\n60(4):321–326, 2014.\n[20] Philippe Hamel, Matthew EP Davies, Kazuyoshi\nYoshii, and Masataka Goto. Transfer learning in mir:\nSharing learned latent representations for music audio\nclassiﬁcation and similarity. Curitiba, Brazil, 2013.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Delving deep into rectiﬁers: Surpassing human-\nlevel performance on imagenet classiﬁcation. In Pro-\nceedings of the IEEE international conference on com-\nputer vision , pages 1026–1034, 2015.\n[22] Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong\nSiew. Extreme learning machine: a new learning\nscheme of feedforward neural networks. In IEEE In-\nternational Joint Conference on Neural Networks , vol-\nume 2, pages 985–990. IEEE, 2004.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 147[23] Yin-Fu Huang, Sheng-Min Lin, Huan-Yu Wu, and Yu-\nSiou Li. Music genre classiﬁcation based on local fea-\nture selection using a self-adaptive harmony search al-\ngorithm. Data & Knowledge Engineering , 92:60–76,\n2014.\n[24] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[25] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in neural information\nprocessing systems , pages 1097–1105, 2012.\n[27] Quoc V Le and Tomas Mikolov. Distributed represen-\ntations of sentences and documents. In International\nConference on Machine Learning , volume 14, pages\n1188–1196, 2014.\n[28] Jongpil Lee and Juhan Nam. Multi-level and multi-\nscale feature aggregation using pre-trained convolu-\ntional neural networks for music auto-tagging. arXiv\npreprint arXiv:1703.01793 , 2017.\n[29] Dawen Liang, Minshu Zhan, and Daniel PW Ellis.\nContent-aware collaborative music recommendation\nusing pre-trained neural networks. In Conference of the\nInternational Society for Music Information Retrieval\n(ISMIR 2015) , pages 295–301. Malaga, Spain, 2015.\n[30] Min Lin, Qiang Chen, and Shuicheng Yan. Network in\nnetwork. arXiv preprint arXiv:1312.4400 , 2013.\n[31] Athanasios Lykartsis and Alexander Lerch. Beat his-\ntogram features for rhythm-based musical genre classi-\nﬁcation using multiple novelty functions. In Proceed-\nings of the 16th ISMIR Conference , pages 434–440,\n2015.\n[32] Ugo Marchand and Geoffroy Peeters. The extended\nballroom dataset. Conference of the International Soci-\nety for Music Information Retrieval (ISMIR 2016) late-\nbreaking session , 2016.\n[33] Ugo Marchand and Geoffroy Peeters. Scale and shift\ninvariant time/frequency representation using auditory\nstatistics: Application to rhythm description. In Ma-\nchine Learning for Signal Processing (MLSP), 2016\nIEEE 26th International Workshop on , pages 1–6.\nIEEE, 2016.\n[34] Brian McFee, Matt McVicar, Colin Raffel, Dawen\nLiang, Oriol Nieto, Eric Battenberg, Josh Moore, Dan\nEllis, Ryuichi YAMAMOTO, Rachel Bittner, Douglas\nRepetto, Petr Viktorin, Jo ˜ao Felipe Santos, and Adrian\nHolovaty. librosa: 0.4.1, October 2015.[35] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781 , 2013.\n[36] Brian CJ Moore. An introduction to the psychology of\nhearing . Brill, 2012.\n[37] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef\nSivic. Learning and transferring mid-level image repre-\nsentations using convolutional neural networks. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition , pages 1717–1724, 2014.\n[38] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay. Scikit-learn: Machine learning in Python. Journal\nof Machine Learning Research , 12:2825–2830, 2011.\n[39] Hanchuan Peng, Fuhui Long, and Chris Ding. Fea-\nture selection based on mutual information crite-\nria of max-dependency, max-relevance, and min-\nredundancy. IEEE Transactions on pattern analysis\nand machine intelligence , 27(8):1226–1238, 2005.\n[40] Jordi Pons and Xavier Serra. Designing efﬁcient archi-\ntectures for modeling temporal features with convolu-\ntional neural networks. IEEE International Conference\non Acoustics, Speech, and Signal Processing , 2017.\n[41] Mathieu Ramona, Ga ¨el Richard, and Bertrand David.\nV ocal detection in music with support vector machines.\nInAcoustics, Speech and Signal Processing (ICASSP),\n2008 IEEE International Conference on , pages 1885–\n1888, March 31 - April 4 2008.\n[42] J. Salamon, C. Jacoby, and J. P. Bello. A dataset and\ntaxonomy for urban sound research. In 22st ACM In-\nternational Conference on Multimedia (ACM-MM’14) ,\nOrlando, FL, USA, Nov. 2014.\n[43] Justin Salamon and Juan Pablo Bello. Deep convolu-\ntional neural networks and data augmentation for envi-\nronmental sound classiﬁcation. IEEE Signal Process-\ning Letters , 2017.\n[44] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recogni-\ntion. arXiv preprint arXiv:1409.1556 , 2014.\n[45] Alex J Smola and Bernhard Sch ¨olkopf. A tutorial on\nsupport vector regression. Statistics and computing ,\n14(3):199–222, 2004.\n[46] Mohammad Soleymani, Micheal N Caro, Erik M\nSchmidt, Cheng-Ya Sha, and Yi-Hsuan Yang. 1000\nsongs for emotional analysis of music. In Proceedings\nof the 2nd ACM international workshop on Crowd-\nsourcing for multimedia , pages 1–6. ACM, 2013.148 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[47] M Srinivas, Debaditya Roy, and C Krishna Mohan.\nLearning sparse dictionaries for music and speech clas-\nsiﬁcation. In Digital Signal Processing (DSP), 2014\n19th International Conference on , pages 673–675.\nIEEE, 2014.\n[48] Bob L Sturm. The gtzan dataset: Its contents, its faults,\ntheir effects on evaluation, and its future use. arXiv\npreprint arXiv:1306.1461 , 2013.\n[49] Bob L Sturm et al. Revisiting priorities: Improving mir\nevaluation practices. In Proc. 17th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR’16), New York, NY, USA , 2016.\n[50] Johan AK Suykens and Joos Vandewalle. Least squares\nsupport vector machine classiﬁers. Neural processing\nletters , 9(3):293–300, 1999.\n[51] The Theano Development Team, Rami Al-Rfou, Guil-\nlaume Alain, Amjad Almahairi, Christof Anger-\nmueller, Dzmitry Bahdanau, Nicolas Ballas, Fr ´ed´eric\nBastien, Justin Bayer, Anatoly Belikov, et al. Theano:\nA python framework for fast computation of mathe-\nmatical expressions. arXiv preprint arXiv:1605.02688 ,\n2016.\n[52] George Tzanetakis. Gtzan musicspeech. availabe on-\nline at http://marsyas.info/download/data sets , 1999.\n[53] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. Speech and Audio Pro-\ncessing, IEEE transactions on , 10(5):293–302, 2002.\n[54] A ¨aron Van Den Oord, Sander Dieleman, and Ben-\njamin Schrauwen. Transfer learning by supervised pre-\ntraining for audio-based music classiﬁcation. In Con-\nference of the International Society for Music Informa-\ntion Retrieval (ISMIR 2014) , 2014.\n[55] Chia-Ming Wang and Yin-Fu Huang. Self-adaptive\nharmony search algorithm for optimization. Expert\nSystems with Applications , 37(4):2826–2837, 2010.\n[56] Felix Weninger, Florian Eyben, and Bjorn Schuller.\nOn-line continuous-time music mood regression with\ndeep recurrent neural networks. In Acoustics, Speech\nand Signal Processing (ICASSP), 2014 IEEE Interna-\ntional Conference on , pages 5412–5416. IEEE, 2014.\n[57] Jason Weston, Samy Bengio, and Nicolas Usunier. Ws-\nabie: Scaling up to large vocabulary image annota-\ntion. International Joint Conference on Artiﬁcial Intel-\nligence , 2011.\n[58] Matthew D Zeiler and Rob Fergus. Visualizing and un-\nderstanding convolutional networks. In European con-\nference on computer vision , pages 818–833. Springer,\n2014.\n[59] Yujun Zeng, Xin Xu, Yuqiang Fang, and Kun Zhao.\nTrafﬁc sign recognition using extreme learning classi-\nﬁer with deep convolutional features. In The 2015 in-\nternational conference on intelligence science and big\ndata engineering (IScIDE 2015), Suzhou, China , 2015.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 149"
    },
    {
        "title": "Exploiting Playlists for Representation of Songs and Words for Text-Based Music Retrieval.",
        "author": [
            "Chia-Hao Chung",
            "Yian Chen",
            "Homer H. Chen"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416436",
        "url": "https://doi.org/10.5281/zenodo.1416436",
        "ee": "https://zenodo.org/records/1416436/files/ChungCC17.pdf",
        "abstract": "As a result of the growth of online music streaming services, a large number of playlists have been created by users and service providers. The title of each playlist provides useful information, such as the theme and listening context, of the songs in the playlist. In this paper, we investigate how to exploit the words extracted from playlist titles for text-based music retrieval. The main idea is to represent songs and words in a common latent space so that the music retrieval is converted to the problem of selecting songs that are the nearest neighbors of the query word in the latent space. Specifically, an unsupervised learning method is proposed to generate a latent representation of songs and words, where the learning objects are the co-occurring songs and words in playlist titles. Five metrics (precision, recall, coherence, diversity, and popularity) are considered for performance evaluation of the proposed method. Qualitative results demonstrate that our method is able to capture the semantic meaning of songs and words, owning to the proximity property of related songs and words in the latent space.",
        "zenodo_id": 1416436,
        "dblp_key": "conf/ismir/ChungCC17",
        "keywords": [
            "online music streaming services",
            "user-created playlists",
            "service providers",
            "playlist titles",
            "song words",
            "latent space",
            "music retrieval",
            "unsupervised learning",
            "coherence",
            "diversity"
        ],
        "content": "EXPLOITING PLAYLISTS FOR  REPRESENTATION OF \nSONGS AND WORDS FOR TEXT -BASED  MUSIC RETRIEV AL \nChia -Hao Chung  Yian Chen  Homer Chen  \nNational Taiwan University  \nb99505003 @ntu.edu.tw  KKBOX I nc. \nannchen@kkbox.com  National Taiwan University  \nhomer@ntu.edu.tw \nABSTRACT  \nAs a result  of the growth of online music streaming  \nservices , a large number of playlists have been  created  by \nusers and service providers . The title of each playlist \nprovides useful information, such as  the theme and \nlistening context , of the songs in the playlist . In this paper, \nwe investigate how to exploit the words extracted from \nplaylist titles for text -based music retrieval. The main \nidea is to represent songs and words in a common latent \nspace so that the music retrieval is converted to the \nproblem of select ing songs that are t he nearest neighbors \nof the query word in the latent space. Specifically, an \nunsupervised learning method is proposed to generate a \nlatent representation of songs and words, where the \nlearning objects are the co -occurring songs and words in \nplaylist titles . Five metrics ( precision, recall, coherence, \ndiversity, and popularity) are considered for performance \nevaluation of the proposed meth od. Qualitative results \ndemonstrate that our method is able to capture the \nsemantic meaning of songs and words, owning to  the \nproximity property of related songs and words in the \nlatent space.  \n1. INTRODUCTION  \nOnline music streaming services , such a s Spotify, Apple \nMusic, and KKBOX,  create  various playlists for the \nconvenience of music listening for users. Meanwhile , \nusers may create their own playlists for replay or music \nsharing with friends  [1–5]. The use of playlist makes \nmusic retrieval and organization simple and easy, largely \nbecause the title of a playlist carries significant thematic  \ninformation of the songs contained in the playlist [1–3]. \nThe theme can be about an artist, genre, mood, or con text \nof the playlist . Therefore, the thematic information is  \nuseful for music retrieval. The goal of this paper is to \nexploit playlist s for text -based music retrieval . \nOne critical issue of text -based music retrieval is \nhow to identify and quantify the relation ship between \nwords and songs  (i.e. which songs and words are relevant  \nto each other  and how much is the relevance ). Most \nprevious approaches to text -based music retrieval rely on \nhuman -labeled dataset s [6, 7] or social  tags [8, 9] which  \nnormally have  a limited size of vocabulary  (word set) . The web-based approach [10, 11]  has been considered a \ngood alternative because web document s have rich text \ninformation . However, its performance may degrade i n \nthe presence of noisy text [12]. In contrast, the playlist-\nbased approach has the following appealing features: 1) \nThe rich text information  conveyed by  the succinct \nplaylist title is highly relevant  to the songs  in the playlist  \nand 2)  Songs wrapped in one playlist must be  related to \neach other  in a certain way . If the relationship can be \ndetermined from the playlist, additional efforts on audio \nsignal analysis [6, 7, 11] can be saved . \nOur main idea is to represent songs and words in a \ncommon latent space so that music retrieval can be  \nconverted  to the problem of selecting songs sufficiently \nnear the query word in the latent space.  Specifically, we \npropose an unsupervised learning method to generate a \nrepresentation of songs and words  extracted from playlist \ntitles , in which t he learning function is optimized based \non the co -occurrence of songs and words  in playlists . As \neach song or word (an object) is represented as a vector in \na latent space , the semantic similarity between two \nobjects can be easily determined by the distanc e between \nthe two corresponding vectors. By exploiting this \nproperty , we can improve the performance of text-based \nmusic retrieval .  \nOur contributions  can be summarized as follows : \n We propose an unsupervised learning method to \nmodel the relevance between s ongs and words of \nplaylists and to represent these two kinds of objects  in \na common latent space . \n We make  text-based music retrieval  easier to solve by \nformulating it as a nearest neighbor  search problem in \nthe latent space . \n Both qualitative and quantitative evaluations  are \nconduct ed to demonstrate the effectiveness of the \nproposed method . \n2. RELATED WORK  \nIn this section,  we review previous work related  to \nplaylist understanding , text-based music retrieval , and  \nrepresentation lea rning . \n2.1 Playlist Understanding  \nTo understand the use of  playlist , Hagen [1] and \nCunningham et al. [2] conducted user interviews to \nanalyze  various themes and contexts of playlists . The \nresults mo tivated Pichl et al. [3] to mine common \nlistening contexts using  playlist  titles for context -aware  © Chia -Hao Chung, Yian Chen, Homer Chen.  \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Chia -Hao Chung, Yian  Chen, \nHomer Chen. “Exploiting Playlists for Representation of Songs and \nWords for Text -Based Music Retrieval” , 18th International Society for \nMusic Information Retrieval Conference, Suzhou, China, 2017.  \n478  \n \nmusic recommendation . In our work, we take a step \nfurther and  investigate how to exploit the words \nextracted from playlist titles for text -based music \nretrieval .  \nA related issue is playlist quality m easurement . \nMotivated by the observation that  the songs in a playlist , \nalthough diverse,  are related  to each other in a certain \nway, Fields [4] introduced coherence  and diversity  as \nmetrics of  playlist quality . It was found that  popularity  \nand freshness  of songs in a playlist  are also important \nmetrics  [5]. Considering  that the response  to a text query \nis in the form of  playlist, we apply  coherence, diversity, \nand popularity  as metrics for performance evaluation .  \n2.2 Text-Based Music Retrieval  \nTo allow  the retriev al of  music pieces by text query , the \nrelevance between words and songs  has to be identified . \nTurnbull et al. [6] and Chechik et al. [ 7] develop ed a \nmulti -class  classification  approach to predict  the \nrelevance of a music piece to a query.  To address  the \nissue that  the perception of relevance is subjective , Hariri \net al. [8] and Cheng et al. [9] used a probabilistic model  \nand listening records to personalize  text-based music \nretrieval . To extend  the coverage of text queries , Knees \net al . [10–12] crawled web documents  relevant to a \nmusic piece  and represented the music piece by the text \nextracted from the web documents . However,  most of  the \nbody  of words  contained in web documents can be \nirrelevant to the theme of the music pieces. To solve the \nproblem , we develop an alternative approach that seeks \nrelevant words from the playlist titles . \n2.3 Representation Learning  \nRepresentation learning ha s been widely applied to \nmusic recommendation [13, 16, 29], playlist recommen -\ndation [17], music annotation a nd retrieval [18], playlist \ngeneration [19, 20], and listening behavior analysis  [21, \n22]. The popularity of representation learning  is due to \nits two appealing features.  First, it can efficient ly handle  \nlarge scale dataset [23, 24] because of low model complexity . Second, it makes information retrieval  or \nrecommendation  an easy task that can be efficiently \naccomplished . However, little attention has been paid to  \nexploit  representation learning for text -based music \nretrieval. In this paper, we extend  the idea of embedding \nlearning  [16–24], which is a typical representation \nlearning approach , to model the relevance between songs \nand words of playlists .  \n3. PROPOSED METHOD  \nWe first introduce  the notation s used in this paper . Then, \nwe describe the proposed method for learning a \nrepresentation  of songs and words  and the detail of the \ntraining processing, including optimization  and data \nsampling. Finally, we describe how the learned \nrepresentation  is applied to  text-based  music retrieva l. \n3.1 Notation s \nLet 𝐿={𝑙1,𝑙2,…,𝑙I} be a set of playlists  and 𝑇=\n{𝑡1,𝑡2,…,𝑡I} be the set of corresponding playlist titles. \nEach playlist  𝑙𝑖, 1≤𝑖≤I, as illustrated in Table 1,  is \nassociated with a set of songs 𝑆𝑖={𝑠1𝑖,𝑠2𝑖,…,𝑠|𝑙𝑖|𝑖} and a \nset of words 𝑊𝑖={𝑤1𝑖,𝑤2𝑖,…,𝑤|𝑡𝑖|𝑖} extracted from 𝑡𝑖. \nLet 𝑆={𝑠1,𝑠2,…,𝑠N} be the  union of all 𝑆𝑖, and 𝑊=\n{𝑤1,𝑤2,…,𝑤M} be the union of all 𝑊𝑖. The goal is to \nlearn a  representation 𝜃(∙) to map each 𝑠𝑛∈𝑆 or 𝑤𝑚∈\n𝑊 to a vector . \n3.2 Representation Learning for Songs and Words  \nWe extend the idea of embedding learning  to songs and \nwords. In its basic form, t he embedding learning  \ngenerates a representation for a set of objects based on \nthe co-occurrence  of the objects [ 23]. It consists of two \nstages. In the first (or initialization) stage, the \nrepresentation 𝜐(∙) assigns  a vector of random values to  \neach object . In second  (or update) stage, t he vector  is \nprogressively  updated  in two steps. In the first step , a \nconditional probability 𝑃(𝑜𝑐|𝜐(𝑜)) for each pair of \nobject s 𝑜 and 𝑜𝑐 is created , where 𝑜𝑐 is the co -occurring Playlist Title Words  Songs (Artists)  \nSummer's Over  summer  The Boys of Summer   (The Ataris ) \nSo Long, So Long  (Dashboard Confessional ) \nLast Days of Summer  (Silverstein ) \nClose To Home  (The Get Up Kids ) \nAlways Summer  (Yellowcard ) \n… \nHappy Morning \nChill  happy  \nmorning  \nchill Snap Out Of It  (Arctic Monkeys)  \nUnbelievers  (Vampire Weekend)  \nDemons  (Imagine Dragons)  \nThe Mother We Share  (Chvrches ) \nEverybody Wants To Rule The World  (Lorde)  \n… \nGeorge Michael - \nFor the Heart  george_michael  \nheart  Don't Let the Sun Go Down on Me  (George Michael)  \nCareless Whisper  (George Michael)  \nHeal The Pain  (George Michael)  \nA Different Corner  (George Michael)  \nI Can't Make You Love Me  (George Michael)  \n… \nTable  1. Illustration of words extracted from playlist s.  Only the first five songs of a playlist are shown.  Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 479  \n \nobject of 𝑜. In the second step , 𝜐(𝑜) is optimized by \nmaximizing the conditional probability.  The two steps  are \nrepeat ed until the maximization  for every pair of 𝑜 and 𝑜𝑐 \nis completed . \nTo extend the basic idea of embedding learning  to \nsongs and words , we need to define the co -occurring \nrelationship between songs and words. If two songs \n(words) belong to the same playlist (playlist title), we say \nthat they are co -occurring in the playlist (playlist title). \nLikewise,  any word of a playlist title and any song in the \nplaylist have a co -occurring relationship . In our method, \nco-occurring pairs of songs, words, or song and word are \nconsidered positive pairs.  \nOnce the songs, words, and positive pairs of all \nplaylists are in place , a learning process that consists of \ntwo stages  is applied . In the first stage, the representation \n𝜃(∙) for each song or word is randomly initialized.  In the \nsecond  stage, 𝜃(∙) is optimized. Specifically, we go \nthrough every positive  pair and randomly select a word \n(or song) , denoted as 𝑠 (or 𝑤) from it. Then, we optimize \nthe representation 𝜃(𝑠) (or 𝜃(𝑤)) in two steps. In the first \nstep, we construct a conditional probability  which  can be \nexpressed in one of the following four formats : \n𝑃(𝑠𝑐|𝜃(𝑠)), 𝑃(𝑤𝑐|𝜃(𝑠)), 𝑃(𝑤𝑐|𝜃(𝑤)), or 𝑃(𝑠𝑐|𝜃(𝑤)), \nwhere 𝑠𝑐or 𝑤𝑐 is the remainder song or word in the \npositive  pair. In the second step, we optimize t he \nrepresentation  𝜃(𝑠) (or 𝜃(𝑤)) by maximizing the \nconditional probabilit y. The two steps , as illustrated in \nFigure 1 , are repeated  until the maximization  for every \npositive  pair is completed ( e.g. an epoch is completed) .  \nWe formulate the entire learning process  by the \nfollowing  object  function : \nℒ=∑ (∑ (∑ log𝑃(𝑠𝑐|𝜃(𝑠))𝑠𝑐∈𝑆𝑖 +𝑠∈𝑆𝑖 𝑙𝑖∈𝐿\n             ∑ log𝑃(𝑤𝑐|𝜃(𝑠))𝑤𝑐∈𝑊𝑖 )+\n∑ (∑ log𝑃(𝑠𝑐|𝜃(𝑤))𝑠𝑐∈𝑆𝑖 +𝑤∈𝑊𝑖\n               ∑ log𝑃(𝑤𝑐|𝜃(𝑤))𝑤𝑐∈𝑊𝑖 )).      (1) \nNote that the natural logarithm converts a conditional \nprobability to a log likelihood for the convenience of \nupdate stage [24]. The conditional probabilit y \n𝑃(𝑠𝑐|𝜃(𝑤)) is modeled  by a softmax function  [23] and \ncan be rewritten as : \n𝑃(𝑠𝑐|𝜃(𝑤))=exp  (𝜑(𝑠𝑐)∙𝜃(𝑤))\n∑ exp  (𝜑(𝑠𝑐′)∙𝜃(𝑤))𝑠𝑐′∈𝑆,             (2) where 𝜑(∙) maps 𝑠𝑐 into a vector space . Likewise, \n𝑃(𝑤𝑐|𝜃(𝑤)), 𝑃(𝑠𝑐|𝜃(𝑠)), and 𝑃(𝑤𝑐|𝜃(𝑠)) are modeled  \nin the same way. Finally, 𝜃(∙) and 𝜑(∙) is op timized by \nmaximizing Equation (1) . \n3.3 Training  \nThere are 2×(N+M)×D parameters , including 𝜃(∙) \nand 𝜑(∙), to be optimized , where N is the number of  \nsongs , M is the number  of words , and D is the dimension \nof the representation . The parameters are optimize d by \nmaximizing Equation (1) using the Adam algorithm [25]. \nHowever, the computation cost of the optimiz ation  is \nproportional to N and M because of the normalization \nterm in the softmax function. As an alternative, we adopt \nthe negative sampling approach [24] to reduce the \ncomputation al cost, where 30 negative pairs are randomly \nsampled for each positive pair.  \nIn our experiments, the dimension of the \nrepresentation was set to 32 , and the hyper -parameters  of \nthe Adam algorithm were  α=0.025, β1=0.9, β2=\n0.999, and ϵ=1𝑒−08. The training  was repeat ed for five \nepoch s. \n3.4 Text -based Music Retrieval  \nThe response to  a text query 𝑞∈𝑊 is the songs that are \nthe nearest neighbor s of 𝑞 in the latent space . Specifically, \nthe cosine similarity between each 𝑠𝑛∈𝑆 and 𝑞 is \ncalculated : \n𝑐𝑜𝑠𝑖𝑛𝑒  𝑠𝑖𝑚𝑖𝑎𝑟𝑖𝑡𝑦 =𝜃(𝑠𝑛)∙𝜃(𝑞)\n‖𝜃(𝑠𝑛)‖2‖𝜃(𝑞)‖2,            (3) \nwhere ‖∙‖2 denotes the Euclidean norm  of a vector. The \nsongs having high cosine similarity  are the response to 𝑞. \n4. EXPERIMENT S \nIn this section, we describe the experiment s conducted to \nevaluate the performance of the proposed method  against \nmatrix factorization, which is a nother  typical approach to \nrepresentation learning . We first describe the dataset  used \nin the experiments and the pre -processing  step applied to  \nthe dataset . Then, we describe  the implementation details \nof matrix factorization . Finally, we describe the results of \nperformance evaluation.  \n4.1 Dataset  and Pre -processing  \nThe dataset was collected by Pichl et al. [3] using Spotify \nAPI1. It contains 21, 485 playlists created by 1,500 users , \nand each playlist contains  a title and a list of songs. \nStandard  natural language processing techniques were  \napplied to process the playlist titles. First, all characters in \nplaylist titles were convert ed to lowercase , and \npunctuations and stop words, such as “the”, “of”, and “a”, \nwere  remove d. Then, each playlist title was segmented \ninto a set of words using the NLTK toolkit2, and single \n                                                           \n1 https://developer.spotify.com/web -api/ \n2 http://www.nltk.org/   \nFigure 1.  Representation learning for songs and words. \nGiven a song or a word, the representation is optimized \nbased on its co-occurring song or word.  \n480 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017  \n \ncharacter s or digit s were removed from the resulting \nword  set. As many playlists titles contain  artist names , \nthe name entity recognition implemented in the NLTK \ntoolkit was applied to  identify  artist name s. Such artist \nnames  were  considered one entity. The space in an artist \nname was replaced with the symbol “_” for the \nconvenience of data processing. As shown in Table 1 , the \nwords  extracted f rom the  playlist title “George Michael - \nFor the Heart” are “george _michael” and “heart”.   \nHowever , like in other popularity studies [26], we \nfound a serious long tail phenomenon : Few songs and \nwords appear frequently while many  other s appear rarely. \nBoth kinds of  songs and words may affect representation \nlearning . Therefore , we remove d songs and word s that \nappear  less than 4 times or more th an 100 times in the \ndataset . Interestingly , similar to stop words , words like \n“radio ”, “liked ”, and “ music ” are not useful for music \nretrieval  but they were automatically removed  because \nthey appear many ti mes in playlist titles . At the end of  \nthis filter ing processing, 33,625  songs and 1,623  words \nwere left  (a playlist was remove d if its songs and words \nwere all removed) . The statistics  of the final dataset is \nlisted  in Table 2 , and the song popularity (the number of \ntimes a song appears  in playlists) and word frequency \n(the number of times a word appear s in playlist titles)  are \nshown in Figure 2. Note that the entire dataset was used \nfor representation learning, and the performance of the \nrepresentation for music retrieval  was evaluated . \n4.2 Matrix Factorization  \nMatrix factorization  (MF) [14, 15]  is compar ed with the \nproposed method . In MF, t he vector 𝒙𝑤 for word  𝑤 and \nthe vector 𝒚𝑠 for song 𝑠 are learned by solving the \noptimization problem  \n    min\n𝑞∗,𝑝∗∑ (𝑐𝑤𝑠−𝒙𝑤𝑇𝒚𝑠)2+𝜆(‖𝒙𝑤‖2+‖𝒚𝑠‖2) 𝑤,𝑠 ,    (4) \nwhere 𝑐𝑤𝑠 is the number of times 𝑤 and 𝑠 co-occur  in the \nplaylists , and λ is a regularization parameter  to avoid overfitting . The inner product of a query vector and each \nsong vector is calculated  to determine which music piece  \nto retrieve.  A song with a higher inner product value  is \nconsidered a better  response to  the query.  \nWe adopt ed the implementation by  MyMediaLite3. \nThe dimension of the vectors learned by MF was set to \n32, and λ was set to 0.015.   \n4.3 Performance Evaluation  \nWe measure  the quality of the  response to a text query by \nthe following five metrics : \nPrecision and recall:  We use t hese two standard \nperformance evaluation metrics to measure the relevance \nof a response  to a query as follows :  \n𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =|𝑆𝑟∩𝑆𝑡|\n|𝑆𝑟|,                         (5) \n𝑟𝑒𝑐𝑎𝑙𝑙 =|𝑆𝑟∩𝑆𝑡|\n|𝑆𝑡|,                             (6) \nwhere 𝑆𝑟 is the set of retrieved songs (the songs in the \nresponse) and 𝑆𝑡 is the set of relevant songs  (the songs in \nthe playlists that have the query in the titles ). A high \nprecision means that most of the retrieved songs are \nrelevant , and a high recall means that most relevant songs \nare retrieved .  \nCoherence: This metric measures the coherence of \nthe songs  in the response to  a query . Specifically, we \nobtain  social tags  of songs from Allmusic4 and calculate \npointwise mutual information (PMI) for every pair of the \nsongs  in a response . The coherence is defined as the \naverage  of the PMIs , \n𝑐𝑜ℎ𝑒𝑟𝑒𝑛𝑐𝑒 =1\nL∑ log𝑃(𝑠𝑖,𝑠𝑗)\n𝑃(𝑠𝑖)𝑃(𝑠𝑗)𝑖<𝑗 ,               (7) \nwhere L is the number of the song pairs , 𝑃(𝑠) denotes the \nprobability of 𝑠 having tag s and 𝑃(𝑠𝑖,𝑠𝑗) denotes the \nprobability of 𝑠𝑖 and 𝑠𝑗 having  the same tag s. The \ncohe rence would be high if the songs in the response  \nhave the same social tags.  \nDiversity: This metric measures how diverse  the \nsongs in a response  are [4, 27] . The diversity is defined as \nthe cross entropy of artists  appearing in the response : \n𝑑𝑖𝑣𝑒𝑟𝑠𝑖𝑡𝑦 =∑ 𝑃(𝑎)log(𝑃(𝑎)) 𝑎∈𝐴 ,            (8) \nwhere 𝐴 represents the set of artists in the response , and \n𝑃(𝑎) denotes the probability of artist a appearing in the \nresponse . The divers ity would be high if various artists  \nappear in the response . \n                                                           \n3 http://www.mymedialite.net/  \n4 http://www.allmusic.com/discover  Number of playlists  18,417  \nNumber of songs  33,625  \nNumber of words  1,623  \nAverage number of songs per playlist  20.37  \nAverage number of words per playlist  1.10 \nTable  2. Data statistics.  \n \nFigure  2. Song popularity and word frequency.  \nProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 481  \n \nPopularity: We calculate  the average popularity of \nthe songs in a response  [27], \n𝑝𝑜𝑝𝑢𝑙𝑎𝑟𝑖𝑡𝑦 =1\nK∑ 𝑃𝑆𝑘 𝑘≤K ,                    (9) \nwhere K is the number of songs  in the response , and 𝑃𝑆𝑘 \nrepresents how many times a song appears in the dataset . \nA low -popularity response  is desired for a music retrieval \nand recommendation system because users may discover \nsongs they have never heard before . \nThe response  quality of the proposed method  is \ncompared with that of MF  by the five metrics . Every \nword in the vocabulary (𝑊) is consider ed a query to \nretrieve k relevant songs using a method. The average \nresults  are shown in Figure 3. We can see from Figures \n3(a) and 3(b) that the precision and recall of the proposed \nmethod are higher  than those of MF. It shows the \neffectiveness of the proposed method for preserv ing the \nrelevance between songs and words. In Figure 3(c), we \ncan see that the proposed  method outperforms MF in \nterms  of coherence. As more songs are ret rieved, our \nmethod provides a stable  coher ence while MF  has a \ndescending coherence . In Figure 3( d), we can see that our \nmethod has a large  variation in terms of diversity . It is \nbecause many responses  of our method contain only the \nsongs of one artist ( zero diversity)  if the query is an artist \nname . In Figure 3( e), we can see that the responses of  the \nproposed method tend to have lower popularity  than the \nresponses of  MF. It implies that MF favors popu lar songs .  \nWe compare the responses of queries with  different \nword frequenc ies. As shown i n Figure 4 , queries are \ndivided into  four groups according to the word frequency . \nWe can see from Figures 4(a) and 4(b) that the proposed \nmethod has higher  precision and recall than MF for each group.  We can also see from Figure 4( c) that our method \nprovides a stable  coherence  regardless of the frequency of \nquery word s while MF  favors queries with high word \nfrequency.  In Figure 4( d), it is interesting to see that our \nmethod yields a low diversity for some queries with low \nword frequency. It is because  part of  the words with low \nfrequency  are artist  name s. Figure 4(e) shows that the \nproposed method provides responses  with low popularity \nregardless of the word frequency of quer ies. \n4.4 Qualitative Study  \nWe show the responses  of the two methods under \ncomparison  to five queries (“christmas”, “punk”, “60s”, \n“coldplay”, and “miles_davis”) in Table 3. The five \nqueries are selected manually to cover various semantic \nmeaning s and word frequenc ies. Additional results and \nvisualization of the learned latent space are provided on \nour website5. \nThe first query  “christmas ” has a high word \nfrequency , which means  this word  is frequently used in \nplaylist titles. We can see that both the proposed method \nand MF can find songs relevant to Christmas. However, \nwe note that the response of MF conta ins only two artists \n(actually, four of the five songs in Table 3 belong to the \nsame artist)  and has  a high popularity . In contract, our \nmethod can find songs with high divers ity and low \npopularity.  The second query “punk” has a lower  word \nfrequency than the first query . We can see that the \nproposed method still provides a good response , while \nthe response of MF is not very relevant to “punk” . It \nimplies that MF may fail when the query has a low word \nfrequency.  \n                                                           \n5 http://mpac.ee.ntu.edu.tw/chiahaochung/textMR.php       \n(a) (b) (c) (d) (e) \nFigure  3. Performance comparison of the proposed method and MF. The results are shown as box plot s [28], where the \nbottom and top of a box are the first and third quartiles, and the band inside the box is the second quartile (the median).  \nPlease refer to [28] for the details of box plot . \n     \n(a) (b) (c) (d) (e) \nFigure  4. Performance comparison of the proposed method and MF for queries with different word frequency. T he \nnumber of retrieved songs is set to 15 . \n482 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017  \n \nThe query “60s” is interesting, as it is related  to the \nsongs or artists  in 1960s . We can see that our method can \nfind the songs of artists who were popular in 1960 s, \nincluding  Bobby Vee, The Tremeloe s, Sam The Sham & \nThe Pharaohs, The Lovin' Spoonful, and Bobby Vinton . \nMF fails in this case because “60s” has a low frequency.  \nThe last two queries “ coldplay ” and “ miles_davis ” \nare both artist s, where  the former has higher word \nfrequency  than the latter. We can see that our method \nprovides good responses to the two queries , while MF \nfails in the case of “miles_davis ”. It can be  expected that \nthe response to  this kind of query  should contain only the \nsongs of the artist  specified in the query . Note that t here \nare many artist names in our vocabulary , and most of \nthem have low word frequency . Because  the proposed  \nmethod  works for these artist quer ies as well as other \nqueries , the diversity of the proposed method has a large \nvariation , as shown Figure s 3(d) and 4( d). \n5. DISC USSION  \nWe first discuss the difference between the proposed \nmethod and MF  in terms of the learning function. As \ndescribed  in Equation (4), MF considers only the co -\noccurrence  of song -word  pairs . In contrast, our method \nexploits  three types of co-occurrence  between songs and \nwords  of playlists . Although an improved MF [29] can be \napplied to  factorize multiple co-occurrence matrices, we \nbelieve that the property of MF (i.e. the favor of popular \nsongs and words) would make MF unsuitable for text -\nbased music retrieval . \nOur method is related to the embedding method \nproposed by Moore et al. [20] for representation  learning  \nof songs and tags for playlist prediction. The difference \nbetween o ur method and their method lies in the function \nused to model t he conditional probability : a softmax function  vs a logistic function that uses  the Euclidean \ndistance between  two vectors as input . Besides, we \napplied two modern approaches, the Adam algorithm [25] \nand the negative sampling [24], to improve the efficiency \nof representation  learning.  \nFinally , we discuss two possible directions to extend \nthe proposed  method . One direction is to enlarge song set \nand word set . As song titles and lyrics also contain rich \ntext information,  they can be  incorporat ed to expand \nword set . Besides, the approach proposed by Oord et al. \n[30] can be applied to map new songs into the latent \nspace learned by our method . This approach  also solves  \nthe cold start problem  [27]. The other direction is to \ndevelop a music retrieval system which allows multiple \nwords as a query , because people may use multiple words  \nor even a sentence to retrieve music. There are simple \nsolutions , for example, combining the responses to \nmultiple single -word queries  [6]. However, such \ncombination may not truly capture the semantic meaning \nof a multiple -words  query. To deal with such query, a \nbetter solution , such as the approach proposed by \nMikolov et al. [24], can be incorpor ated in to the proposed \nmethod.  We can see the potential and high extendability \nof our method . \n6. CONCLUSION  \nIn this paper, we have proposed an unsupervised learning \nmethod to generate the latent representation of songs and \nwords of playlists for text -based music retrieval. Such \nrepresentation captures the relevance  between  songs and \nwords , owning to the proximity property of the latent \nspace . Both qualitative and quantitative evaluation s show  \nthe effectiveness of the proposed method  compare d \nagainst  the matri x factorization  method  for text -based \nmusic retrieval . Query  Matrix factorization  Proposed method  \nchristmas \n(98a) It's Beginning To Look A Lot  Like Christmas (Michael Bubléb, 38c) \nAll I Want For Christmas Is You (Mariah Carey, 40)  \nWhite Christmas (Michael Bublé, 23)  \nSanta Claus Is Coming To Town (Michael Bublé, 15)  \nAll I Want For Christmas Is You (Michael Bublé, 25)  Queen Of The Winter Night (Trans -Siberian Orchestra, 5)  \nO Come All Ye Faithful/ O Holy Night (Trans -Siberian Orchestra, 6)  \nRudolph The Red Nosed Reindeer (Burl Ives, 6)  \nRockin' Around The Christmas Tree (She & Him, 5)  \nChristmas Is Going To The Dogs (Eels, 6)  \npunk   \n(23) Sing (Ed Sheeran, 68)  \nShirtsleeves (Ed Sheeran, 17)  \nDon't Let It Go (Beck, 30)  \nSomewhereinamerica (JAY Z, 24)  \nBloodstream (Ed Sheeran, 29)  I Want To Conq uer The World (Bad Religion, 6)  \nStory of My Life (Social Disto rtion, 19)  \nMonosyllabic Girl (NOFX, 6)  \nGenerator (Bad Religion, 10)  \nLeave It Alone (NOFX, 8)  \n60s \n(13) Together (Calvin Harris, 8)  \nThe Card Cheat (The Clash , 6) \nBowery (Local Natives, 14)  \nYou Make  Loving Fun (Fleetwood Mac, 34)  \nSecond Hand News - Early Take (Fleetwood Mac, 6)  Take Good  Care Of My Baby (B obby Vee, 5)  \nSilen ce Is Golden (The Tremeloes, 5)  \nWooly Bully ( Sam The Sham & The Pharaohs, 8)  \nDaydream (The Lovin' Spoonful, 11)  \nBlue Velvet (Bobby Vinton, 10)  \ncoldplay  \n(40) Charlie Brown (Coldplay, 51)  \nMajor Minus (Coldplay,  17) \nMylo Xyloto  (Coldplay, 19)  \nHurts Like Heaven (Coldplay, 36)  \nEvery Teardrop Is a Waterfall (Coldplay, 42)  U.F.O. (Coldplay, 19)  \nProspekt's M arch/Poppyfields (Coldplay, 12)  \nWhite Shadows (Coldplay, 15)  \nMylo Xyloto - Live (Coldplay, 7)  \n Twisted Logic (Coldplay, 9)  \nmiles_davis  \n(7) Scarborough Fair / C anticle (Simon & Garfunkel, 14)  \nIs She Weird (Pixies, 6)  \nShoes Upon the Table (Blood Brothers - 1995 London Cast, 5)  \nI Would For You (Nine Inch Nails, 13)  \nLove Is The Answer (Aloe Blacc, 13)  Fran-Dance (Miles Davis, 7)  \nOn Green Dolphin Street (Miles Davis, 7)  \nSpanish Key (Miles Davis, 5)  \nFlam enco Sketches (Miles Davis, 13)  \nLove For Sale (Miles Davis, 8)  \nTable  3. Qualitative Study. Only the top five  songs to a query are shown.  (a word frequency, b artist, c song populari ty)  Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 483  \n \n7. REFERENCES  \n[1] A. N. Hagen , “The playlist experience: Personal  \nplaylists in music streaming services ,” Popular \nMusic and Society , vol. 38, no. 5, pp. 625–645, 2015 . \n[2] S. J. Cunningham , D. Bainbridge , and A. Falconer , \n“More of an art than a science:  Supporting the \ncreation of playlists and mixes ,” in Proc. 7th Int. \nConf. Music Inf. Retrieval  (ISMIR) , pp. 240 –245, \n2006 . \n[3] M. Pichl, E. Zangerle , and G . Specht , “Towards a \ncontext -aware music recommendation approach : \nWhat is hidden in the  playlist name ?” in Proc. 15th \nIEEE Int. Conf. Data Mining Workshop  (ICDMW) , \npp. 1360 –1365 , 2015 . \n[4] B. Fields , “Contextualize your listening : The playlist \nas recommendation  engine ,” PhD dissertation , Dept. \nComput ., Goldsmiths, Univ . London , 2011 . \n[5] D. Jannach , I. Kamehkhosh , and G. Bonnin , \n“Analyzing the characteristics of shared playlists for  \nmusic recommendatio n,” in Proc . 6th Workshop \nRecommender Syst . Social Web (RSWeb) , 2014 . \n[6] D. Turnbull, L. Barring ton, D. Torres, and G. \nLanckriet, “ Towards musical query -by-semantic -\ndescription using  the CAL500 data set,” in Proc. \n30th Int. ACM Conf. Res. Develop. Inf. Retrieval  \n(SIGIR) , pp. 23–27, 2007.  \n[7] G. Chechik , E. Ie, M. Rehn , S. Bengio , and D. Lyon , \n“Large -scale content -based audio retrieval  from text \nqueries,” in Proc. 1st ACM Int. Conf. Multimedia Inf. \nRetrieval , pp. 105 –112, 2008.  \n[8] N. Hariri, B. Mobasher, and R. Burke , “Personalized \ntext-based music retrieval ,” in Workshops 27th AAAI \nConf. Artificial Intell. , 2013.  \n[9] Z. Cheng , J. Shen , and S. C.H. Hoi , “On  effective \npersonalized music retrieval by exploring  online \nuser behavio rs,” in Proc. 39th Int. ACM Conf. Res. \nDevelop. Inf. Retrieval  (SIGIR ), pp. 125–134, 2016.  \n[10] P. Knees, T. Pohle, M. Schedl,  D. Schni tzer, and K. \nSeyerlehner, “A document -centered approach to a \nnatural language music search en gine,” in European \nConf. Inf. Retrieval  (ECIR) , Springer Berlin \nHeidelberg , pp. 627 –631, 2008.  \n[11] P. Knees, T. Pohle, M. Schedl, D. Schnitzer, K. \nSeyerlehner, and G. Widmer, “Augmenting text -\nbased music retrieval  with audio similarity,” in Proc. \n10th Int. Soc. Music Inf. Retrieval Conf. (ISM IR), pp. \n579–584, 2009.  \n[12] P. Knees, M. Schedl, T. Pohle, K. Seyerlehner, and \nG. Widmer , “Supervised and unsupervised web \ndocument fi ltering  techniques to improve text -based \nmusic retrieval,” in Proc. 11th Int. Soc. Music Inf. \nRetrieval Conf. ( ISMIR ), pp. 543–548, 2010 . [13] G. Dror , N. Koenigstein , and Y. Koren , “Yahoo! \nmusic recomm endations: Mod eling music ratings  \nwith temporal dynamics and item tax onomy,” in \nProc. 5th ACM Int. Conf. Recommender Syst. \n(RecSys) , pp. 165 –172, 2011.  \n[14] Y. Koren, R. Bell, and C. Volinsky, “Matrix \nfactorization techniques for recommender systems,” \nIEEE Computer , vol. 42, no. 8, pp. 30–37, 2009.  \n[15] Y. Hu, Y. Koren, a nd C. Volinsky, “Collaborative \nfiltering for implicit feedback datasets,” in Proc. 8th \nIEEE Int. Conf. Data Mining  (ICDM) , pp. 263 –272, \n2008.  \n[16] C.-M. Chen, M .-F. Tsai, Y .-C. Lin, and Y .-H. Yang, \n“Query -based music recommendations via \npreference embed ding,” in Proc. 10th ACM Conf. \nRecommender Syst. (RecSys) , pp. 79 –82, 2016.  \n[17] C.-M. Chen,  C.-Y. Yang, C.-C. Hsia, Y. Chen, and \nM.-F. Tsai, “Music playlist recommendation via \npreference embedd ing,” in Poster Proc. 10th ACM \nConf. Recommender Syst. (RecSys) , 2016.  \n[18] J. Weston, S. Bengio, and P. Hamel , “Larg e-scale \nmusic annotation and retrie val: Learn ing to rank in \njoint semantic sp aces,”  arXiv preprint arXiv:  \n1105.5196 , 2011 . \n[19] S. Chen , J. L. Moore , D. Turnbull , and T. Joachims , \n“Playlist  prediction via metric embedd ing,” in Proc. \n18th ACM Int. Conf. Knowledge Discovery Data \nMining (SIGKDD) , pp. 714 –722, 2012 . \n[20] J. L. Moore, S. Chen, T. Joachims , and D. Turnbull , \n“Learning to embed songs and tags for playlist \nprediction ,” in Proc. 13th Int. Soc. Music Inf. \nRetrieval  Conf. (ISMIR) , pp. 349–354, 2012 . \n[21] J. L. Moore, S. Chen, T. Joachims , and D. Turnbull , \n“Taste over time: The temporal dynamics of user  \npreferences,” in Proc. 14th Int. Soc. Music Inf. \nRetrieval Conf. (ISMIR) , pp. 401–406, 201 3. \n[22] C.-H. Chung , J.-K. Lou, and H. Chen , “A latent \nrepresentation of users, sessions, and songs for \nlistening behavior analysis,” in Proc. 17th Int. Soc. \nMusic Inf. Retrieval Conf. (ISMIR) , 2016.  \n[23] T. Mikolov, K. Chen, G. Corrado , and J. Dean, \n“Efficient estimation of word representations in \nvector space,” arXiv preprint arXiv: 1301.3781 , \n2013.  \n[24] T. Mikolov, I. Sutskever, K. Chen,G. Corrado, and J. \nDean, “Distributed representations of words and \nphrases and their compositionality,” in Proc. \nAdvances in Neural Inf. Process. Syst.  (NIPS) , pp. \n3111 –3119, 2013.  \n[25] D. P. Kingma  and J. L. Ba, “Adam: A method for \nstochastic optimization,” arXiv preprint arXiv:  \n1412.6980 , 2014 . 484 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017  \n \n[26] O. Celma, Music Recommendation and Discovery , \nSpringer Berlin Heide lberg, 2010.  \n[27] S.-Y. Chou, Y. -H. Yang, and Y.C. Lin, “Evaluating \nmusic recommendation in a real -world setting: On \ndata splitting and evaluation metrics,” in Proc. IEEE \nInt. Conf. Multimedia Expo  (ICME) , pp. 1 –6, 2015.  \n[28] R. McGill,  J. W. Tukey and W. A Larsen, \n“Variations of box plots ,” American Statistician , vol. \n32, no. 1, pp. 12–16, 1978 . \n[29] A. Vall, M. Skowron, P. Knees, and M.  Schedl, \n“Improving music recommendations with a \nweighted factorization of the tagging activity ,” in \nProc. 16th Int. Soc. Music Inf. Retrieval Conf. \n(ISMIR) , pp. 65 –71, 2015.  \n[30] A. Van de Oord, S. Dieleman, and B. Schrauwen, \n“Deep content -based music recommendation,” in \nProc. Advances Neural Inform. Process. Syst. \n(NIPS) , pp. 2643 –2651 , 2013.  Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 485"
    },
    {
        "title": "A Metric for Music Notation Transcription Accuracy.",
        "author": [
            "Andrea Cogliati",
            "Zhiyao Duan"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415830",
        "url": "https://doi.org/10.5281/zenodo.1415830",
        "ee": "https://zenodo.org/records/1415830/files/CogliatiD17.pdf",
        "abstract": "Automatic music transcription aims at transcribing musical performances into music notation. However, most existing transcription systems only focus on parametric transcrip- tion, i.e., they output a symbolic representation in absolute terms, showing frequency and absolute time (e.g., a piano- roll representation), but not in musical terms, with spelling distinctions (e.g., A♭versus G♯) and quantized meter. Re- cent attempts at producing full music notation output have been hindered by the lack of an objective metric to mea- sure the adherence of the results to the ground truth mu- sic score, and had to rely on time-consuming human eval- uation by music theorists. In this paper, we propose an edit distance, similar to the Levenshtein Distance used for measuring the difference between two sequences, typically strings of characters. The metric treats a music score as a sequence of sets of musical objects, ordered by their on- sets. The metric reports the differences between two music scores based on twelve aspects: barlines, clefs, key signa- tures, time signatures, notes, note spelling, note durations, stem directions, groupings, rests, rest duration, and staff assignment. We also apply a linear regression model to the metric in order to predict human evaluations on a dataset of short music excerpts automatically transcribed into music notation.",
        "zenodo_id": 1415830,
        "dblp_key": "conf/ismir/CogliatiD17",
        "keywords": [
            "Automatic music transcription",
            "parametric transcription",
            "musical terms",
            "objective metric",
            "Levenshtein Distance",
            "music score",
            "edit distance",
            "sequence of musical objects",
            "twelve aspects",
            "linear regression model"
        ],
        "content": "A METRIC FOR MUSIC NOTATION TRANSCRIPTION ACCURACY\nAndrea Cogliati\nUniversity of Rochester\nElectrical and Computer Engineering\nandrea.cogliati@rochester.eduZhiyao Duan\nUniversity of Rochester\nElectrical and Computer Engineering\nzhiyao.duan@rochester.edu\nABSTRACT\nAutomatic music transcription aims at transcribing musical\nperformances into music notation. However, most existing\ntranscription systems only focus on parametric transcrip-\ntion, i.e., they output a symbolic representation in absolute\nterms, showing frequency and absolute time (e.g., a piano-\nroll representation), but not in musical terms, with spelling\ndistinctions (e.g., A [versus G ]) and quantized meter. Re-\ncent attempts at producing full music notation output have\nbeen hindered by the lack of an objective metric to mea-\nsure the adherence of the results to the ground truth mu-\nsic score, and had to rely on time-consuming human eval-\nuation by music theorists. In this paper, we propose an\nedit distance, similar to the Levenshtein Distance used for\nmeasuring the difference between two sequences, typically\nstrings of characters. The metric treats a music score as a\nsequence of sets of musical objects, ordered by their on-\nsets. The metric reports the differences between two music\nscores based on twelve aspects: barlines, clefs, key signa-\ntures, time signatures, notes, note spelling, note durations,\nstem directions, groupings, rests, rest duration, and staff\nassignment. We also apply a linear regression model to the\nmetric in order to predict human evaluations on a dataset of\nshort music excerpts automatically transcribed into music\nnotation.\n1. INTRODUCTION\nAutomatic Music Transcription (AMT) is the process of\ninferring a symbolic representation of a musical perfor-\nmance. Despite four decades of active research, AMT is\nstill an open problem, with humans being able to achieve\nbetter results than machines [2]. AMT systems can be\nbroadly classiﬁed into two categories according to the cho-\nsen symbolic representation: parametric transcription and\nmusic notation transcription. Parametric transcription sys-\ntems output a parametric representation of the musical per-\nformance, such as an unquantized MIDI pianoroll [14].\nThis representation is expressed in physical terms, such as\nseconds for note onset and duration, and hertz or MIDI\nnumbers for pitch [7]. It can faithfully represent the mu-\nc\rAndrea Cogliati, Zhiyao Duan. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Andrea Cogliati, Zhiyao Duan. “A metric for music notation\ntranscription accuracy”, 18th International Society for Music Information\nRetrieval Conference, Suzhou, China, 2017.sical performance, but normally it does not explicitly en-\ncode high-level musical structures, such as key, meter and\nvoicing [21]. Music notation transcription systems, on the\nother hand, output a common music notation that human\nmusicians read. This representation is expressed in musi-\ncally meaningful terms, such as quantized meter for note\nonset and duration, and spelling distinctions (e.g., A [ver-\nsus G]) for pitch. Compared to parametric transcription,\nmusic notation transcription is generally more desirable for\nmany applications connecting humans and machines, such\nas computational musicological analysis and music tutor-\ning systems. The vast majority of existing AMT methods,\nhowever, are parametric transcription systems.\nResearchers have put considerable effort toward build-\ning music notation transcription systems by identifying\nmusical structures from unquantized parametric represen-\ntations, especially MIDI ﬁles, from both MIR and cog-\nnitive perspectives [20]. Cambouropoulos [3] described\nthe key components necessary to convert a MIDI per-\nformance into music notation: identiﬁcation of elemen-\ntary musical objects (i.e., chords, arpeggiated chords, and\ntrills), beat identiﬁcation and tracking, time quantization\nand pitch spelling. Takeda et al. [18] describe a Hid-\nden Markov Model (HMM) for the automatic transcription\nof monophonic MIDI performances. Cemgil [4] presents\na Bayesian framework for music transcription, identify-\ning some issues related to automatic music typesetting\n(i.e., the automatic rendering of a musical score from a\nsymbolic representation), in particular tempo quantization,\nand chord and melody identiﬁcation. Karydis et al. [12]\nproposed a perceptually motivated model for voice sep-\naration capable of grouping polyphonic groups of notes,\nsuch as chords or other forms of accompaniment ﬁgures,\ninto a perceptual stream. A more recent paper by Gro-\nhganz et al. [11] introduced the concepts of score-informed\nMIDI ﬁle (S-MIDI), in which musical tempo and beats are\nproperly represented, and performed MIDI ﬁle (P-MIDI),\nwhich records a performance in absolute time. The paper\nalso presented a procedure to approximate an S-MIDI ﬁle\nfrom a P-MIDI ﬁle – that is, to detect the beats and the me-\nter implied in the P-MIDI ﬁle, starting from a tempogram\nthen analyzing the beat inconsistency with a salience func-\ntion based on autocorrelation.\nResearchers have also attempted to infer musical struc-\ntures directly from audio. Ochiai et al. [16] proposed a\nmodel for the joint estimation of note pitches, onsets, off-\nsets and beats based on Non-negative Matrix Factorization407(NMF) constrained with a rhythmic structure modeled with\na Gaussian mixture model. Collins et al. [8] proposed a\nmodel for multiple fundamental frequency estimation, beat\ntracking, quantization, and pattern discovery. The pitches\nare estimated with a neural network. An HMM is sepa-\nrately used for beat tracking. The results are then com-\nbined to quantize the notes. Note spelling is performed by\nestimating the key of the piece and assigning to MIDI notes\nthe most probable pitch class given the key.\nAn immediate problem arising when building a music\nnotation transcription system by incorporating the above-\nmentioned musical structure inference methods is to ﬁnd\nan appropriate way to evaluate the transcription accuracy\nof the system. In our prior work [7], we asked music\ntheorists to evaluate music notation transcriptions along\nthree different musical aspects, i.e., the pitch notation, the\nrhythm notation, and the note positioning. However, sub-\njective evaluation is time consuming and difﬁcult to scale\nto provide enough feedback to further improve the tran-\nscription system. It would be very helpful to have an ob-\njective metric for music notation transcription, just like the\nstandard metric F-measure for parametric transcription [1].\nConsidering the inherent complexity of music notation,\nsuch a metric would need to take into account all of the\naspects of the high-level musical structures in the notation.\nTo the best of our knowledge, there is no such metric, and\nthe goal of this paper is to propose such a metric.\nSpeciﬁcally, in this paper we propose an edit distance,\nbased on similar metrics used in bioinformatics and lin-\nguistics, to compare a music transcription with the ground-\ntruth score. The design of the metric was guided by a data-\ndriven approach, and by simplicity. The metric is calcu-\nlated in two stages. In the ﬁrst stage, the two scores are\naligned based on the pitch content; in the second stage,\nthe differences between the two scores are accumulated,\ntaking into account twelve different aspects of music nota-\ntion: barlines, clefs, key signatures, time signatures, notes,\nnote spelling, note durations, stem directions, groupings,\nrests, rest duration, and staff assignment. This will serve\nthe same purpose as F-measure in evaluating parametric\ntranscription. To validate the saliency and the usefulness\nof this metric we also apply a linear regression model to\nthe errors measured by the metric to predict human evalu-\nations of transcriptions.\n2. BACKGROUND\nApproximate sequence comparison is a typical problem in\nbioinformatics [13], linguistics, information retrieval, and\ncomputational biology [15]. Its purpose is to ﬁnd simi-\nlarities and differences between two or more sequences of\nelements or characters. The sequences are assumed sufﬁ-\nciently similar but potentially corrupted by errors. Possi-\nble differences include the presence of different elements,\nmissing elements or extra elements. Several metrics have\nbeen proposed to measure the distance between two se-\nquences, including the family of edit metrics [15], and gap-\npenalizing alignment techniques [13].\nA music score in traditional Western notation can beviewed as a sequence of musical characters, such as clefs,\ntime and key signatures, notes and rests, possibly oc-\ncurring concurrently, such as in simultaneous notes or\nchords. Transcription errors include alignment errors due\nto wrong meter estimation or quantization, extra or miss-\ning notes and rests, note and rest duration errors, wrong\nnote spelling, wrong staff assignment, wrong note group-\ning and beaming, and wrong stem direction. All of these\nerrors contribute to a various degree to the quality of the\nresulting transcription. However, the impact of each error\nand error category has not, to the best of our knowledge,\nbeen researched.\nAs an example, Fig. 1 shows two transcriptions of the\nsame piece. Both transcriptions contain similar errors, i.e.,\nwrong meter detection, but the transcription in Fig. 1c is\narguably worse than that in Fig. 1b. A similar problem can\nbe observed with the standard F-measure typically used to\nevaluate parametric transcriptions [1]; while the metric is\nobjective and widely used, the impact of different errors\non the perceptual quality of a transcription has not been\nresearched. Intuitively, certain errors, such as extra notes\noutside of the harmony, should be perceptually more ob-\njectionable than others, such as octave errors. This is the\nreason for both proposing an objective metric and correlat-\ning the metric with human evaluations of transcriptions.\n&?4242rœœœ#œŒpJœ‰rœœœœœœœœœœœJœ‰rœœœ#œœœœœœœœrœœœ#œrœœœ#œœœœœœœ\n(a) Ground truth\n&?4242rœœœ#œJœ‰ŒœœœrœœœœJœ‰œœœœœœœrœœœ#œrœœœ#œœœœœœœœrœœœ#œŒœœœŒ\n(b) Transcription with a wrong pickup measure\n&?4242≈rœœœ#œœ≈≈≈‰≈œœœ≈rœœœœœ≈œœœœœœœœœœœ≈rœœœ#œrœœœ#œœœœœœœœœœœœrœœœ#Rœ≈‰œœœœœRœœ≈‰\n(c) Transcription off by a 16th note\nFigure 1 : Comparison of two transcriptions of the same\npiece containing similar errors but with different readabil-\nity.\n3. PROPOSED METHOD\nThe proposed metric is calculated in two stages: in the\nﬁrst stage, the transcription is aligned with the ground-\ntruth music notation based on its pitch content only, i.e.,\nall of the other objects, such as rests, barlines, and time\nand key signatures are ignored; in the second stage, all of\nthe objects occurring at the aligned portions of the scores408 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017&?##4343Pianoœœœœœ˙˙˙œœœœ.˙œœmœœœ.˙œœœ.˙œmœœœœ.˙œœœœœ.˙&?##Pno.7œœœœœœœœjœ.˙œœœœœœœœœœ˙œœœœœœœœœmœœœ.˙œœœœœœœœ&?##....Pno.13œmœœœœ˙œœœœœœ˙œœœœœœœœœ.˙˙œEthan WeinsteinK-8\n&?##4343Piano˙œœœœœŒ˙œ˙ŒœœœÓ..˙Ó.œœœœœÓ..˙Ó.œœœÓ..˙Ó.œœœœœÓ..˙Ó.œœœœœÓ..˙Ó.œœœœœÓ.œœœÓ.&?##Pno.8.˙Ó.œœœœœÓ.œœœœœÓ.˙œÓ.œœœÓ.œœœÓ.œœœœœœÓ..˙Ó.œœœÓ.œœœœœÓ.œœœœœÓ.˙œÓ.œœœœœÓ.˙œÓ.&?##Pno.15œœœœœÓ.œœœÓ.˙ŒÓ..˙ÓœL-8Ethan WeinsteinFigure 2 : Alignment between the ground-truth (top) and\na transcription (bottom) of Bach’s Minuet in G. Arrows\nindicate aligned beats.\n&?##4343Pianoœœœœœ˙˙˙œœœœ.˙œœmœœœ.˙œœœ.˙œmœœœœ.˙œœœœœ.˙&?##Pno.7œœœœœœœœjœ.˙œœœœœœœœœœ˙œœœœœœœœœmœœœ.˙œœœœœœœœ&?##....Pno.13œmœœœœ˙œœœœœœ˙œœœœœœœœœ.˙˙œEthan WeinsteinK-8\n&?4444Piano‰Jœ.œœœœ.œœ‰Jœœ˙˙œ≈œœœœŒœ≈œœ.˙≈œœœœ..œœœ≈œœœœ#∑œ‰jœ‰.rœœw&?Pno.5≈œœ.œœ≈œœœœ≈œœ.˙≈œœœœœœ≈œœœ#wœœ≈œœ≈œœœœœŒ≈œœ≈œœœœœ‰jœŒ‰JœŒœ≈œœœœ&?Pno.9œœ≈œœ≈œœœ.œœœœœ‰jœ‰.rœœœœ‰Jœ‰.RœœŒ‰œœœ≈œ#œœÓ‰.Rœœœœ≈œœœœœ‰.Rœœœœ.œœœœœ≈œœœœ.œœœ≈œ#œœœœ&?Pno.14≈œœœœ≈œœœœ‰jœœ.œœœÓœœœ#˙3‰.Rœœ˙œŒÓ.œœœÓG-8Ethan Weinstein\nFigure 3 : Alignment between the ground-truth (top) and\nanother transcription (bottom) of Bach’s Minuet in G. Ar-\nrows indicate aligned beats.\nare grouped together and compared. The metric reports the\ndifferences in aligned portions in terms of twelve aspects:\nbarlines, clefs, key signatures, time signatures, notes, note\nspelling, note durations, stem directions, groupings, rests,\nrest duration, and staff assignment.\nSome algorithms to efﬁciently calculate certain edit dis-\ntances, e.g., the Wagner-Fischer algorithm to calculate the\nLevenshtein distance between two strings, are able to align\ntwo sequences and calculate the edit costs in a single stage.\nWe initially tried to apply the same strategy to our problem,\nbut we discovered that the algorithm was not sufﬁciently\nrobust, especially with transcriptions highly corrupted by\nwrong meter estimation. Intuitively, notes are the most\nsalient aspects of music, so it is arguable that the align-\nment of two transcriptions should be based primarily on\nthat aspect, while the overall quality of the transcription\nshould be judged on a variety of other aspects.\nThe ground truth and the transcription are both encoded\nin MusicXML, a standard format to share sheet music ﬁles\nbetween applications [10]. The two scores are aligned us-\ning Dynamic Time Warping [17]. The local distance is\nsimply the number of mismatching pitches, regardless of\nduration, spelling and staff positioning.\nTo illustrate the purpose of the initial alignment, we\nshow two examples in Fig. 2 and Fig. 3. The alignment\nstage outputs a list of pairs of aligned beats. Fig. 2 shows\nthe alignment of a fairly good transcription of Bach’s Min-\nuet in G from the Notebook for Anna Magdalena Bach,\nwith the ground truth, which corresponds to the followingsequence, expressed in beats, numbered as quarter notes\nstarting from 0 (GT is ground truth, T is transcription):\nGT 0.0 1.0 1.5 2.0 2.5 3.0 4.0\nT 0.0 1.0 1.5 2.0 2.5 3.0 4.0\n4.0 5.0 6.0 7.0 7.5 8.0 8.5 9.0\n5.0 5.0 6.0 7.0 7.5 8.0 8.5 9.0\n10.0 10.0 11.0 12.0 13.0 13.5 14.0 14.5\n10.0 11.0 11.0 12.0 13.0 13.5 14.0 14.5\n15.0 16.0 16.5 17.0 17.5\n15.0 16.0 16.5 17.0 17.5\nIn this case, since the transcription is properly aligned\nwith the ground truth, the sequence is just a list of all equal\nnumbers, one for each onset of the notes in the score. How-\never, beat 4.0 in the ground truth is matched with beats 4.0\nand 5.0 in the transcription; the same happens for beats\n10.0 and 11.0, so DTW cannot properly distinguish re-\npeated pitches. Only one alignment is shown in the ﬁgure\nfor clarity.\nFig. 3 shows an example of an alignment for a badly\naligned transcription of the same piece. The corresponding\nsequence is the following:\nGT 0.0 0.0 0.0 1.0 1.0 1.5\nT 0.0 0.5 1.0 1.75 2.0 2.5\n2.0 2.5 3.0 3.0 3.0 4.0 4.0\n3.0 3.75 4.25 4.5 5.0 5.5 7.0\n5.0 6.0 6.0 6.0 7.0 7.5 8.0\n7.0 8.25 8.5 9.0 9.75 10.25 10.75\n8.0 8.5 9.0 10.0 10.0 10.0 11.0\n11.0 11.5 12.0 13.5 14.75 15.0 15.0\nIn this case, multiple beats in the transcription corre-\nspond to the same beat in the ground truth, e.g., beat 1.0 in\nthe ground truth corresponds to beats 1.75 and 2.0 in the\ntranscription, because a single note in the ground truth has\nbeen transcribed as two tied notes. Only one alignment is\nshown in the ﬁgure for clarity.\nTo calculate the distance between the two aligned\nscores, we proceed by ﬁrst grouping all of the musical ob-\njects occurring inside aligned portions of the two scores\ninto sets, thus losing the relative location of the objects\nwithin each set but preserving all of the other aspects, in-\ncluding staff assignment. Then the aligned sets are com-\npared, and the differences between the two sets are re-\nported separately. The following aspects only allow binary\nmatching: barlines, clefs, key signatures, and time signa-\ntures. Rests are matched for duration and staff assignment,\ni.e., a rest with the correct duration but on the wrong staff\nwill be considered a staff assignment error, a rest with the\ncorrect staff assignment but wrong duration will be consid-\nered a rest duration error. A missing or an extra rest will be\nconsidered a rest error. Notes are matched for spelling, du-\nration, stem direction, staff assignment, and grouping into\nchords. For groupings, we only report the absolute value\nof the difference between the number of chords present in\nthe two sets. The metric does not distinguish missing orProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4092 3 4 5 6 7 8 9\nEvaluator score123456789Predicted score(a) Pitch Notation\n2 3 4 5 6 7 8\nEvaluator score2345678Predicted score\n(b) Rhythm Notation\n2 3 4 5 6 7 8\nEvaluator score2.533.544.555.566.57Predicted score\n(c) Note Positioning\nFigure 4 : Correlation between the predicted ratings and\nthe average human evaluator ratings of all of the transcrip-\ntions in the dataset.extra elements. These choices were dictated by simplicity\nof design and implementation.\nAll of the errors are cumulated for all of the matching\nsets. The errors for barlines, notes, note spelling, note du-\nrations, stem directions, groupings, rests, rest duration, and\nstaff assignment are then normalized by dividing the total\nnumber of errors for each aspect by the total number of\nmusical objects taken into account in the score. This step\nis necessary to normalize the number of errors for pieces\nof different lengths. The errors for clefs, key signatures,\nand time signatures are not normalized, as they are typi-\ncally global aspects of the scores, and not inﬂuenced by the\nlength of the piece. This might be a limitation for pieces\nwith frequent changes in key signature or time signature.\nAs an example, the set of objects at the ﬁrst beat of the\nﬁrst measure of Fig. 2 include the initial barlines, clefs,\ntime signature, key signature, and notes starting on the\ndownbeat of the measure. Barlines, clefs, time signature,\nand key signature are all correctly matched. All of the\nnotes are correct in pitch, spelling and duration, however\nthere are two errors in stem direction, one error in group-\ning, and one error in staff assignment. All of the rests are\nconsidered rest errors at each respective onsets.\nFor the ﬁrst beat of the ﬁrst measure of Fig. 3, all of the\nelements of the transcription till the ﬁrst transcribed notes\n(the three notes pointed by the ﬁrst arrow) and the notes\ntied to them will be considered as part of the same set. The\nwrong key signature and time signature will be reported as\nerrors. The two eight rests will be reported as rest errors.\nThe three notes in the transcription are properly spelled,\nbut their duration is wrong, so that will be counted as three\nnote duration errors. The missing D from the chord will\nbe reported as a note error. The extra tied notes will be\nreported as note errors as well.\nIn summary, the following twelve normalized error\ncounts are calculated by the metric: barlines, clefs, key\nsignatures, time signatures, notes, note spelling, note dura-\ntions, stem directions, groupings, rests, rest duration, and\nstaff assignment. In order to translate these error counts\ninto a musically relevant evaluation, we propose to use\nlinear regression of the twelve error counts to ﬁt human\nratings of three musical aspects of automatic transcrip-\ntions, i.e., the pitch notation, the rhythm notation, and the\nnote positioning. For each aspect, the linear regression\nlearns twelve weights, one for each of the normalized error\ncounts, to ﬁt the human ratings. These weights can then be\nused to predict the human ratings of other music notation\ntranscriptions.\n4. EXPERIMENTAL RESULTS\nTo evaluate the proposed approach, we calculate the nor-\nmalized error count and run linear regression to ﬁt human\nratings of 19 short music excerpts collected in our prior\nwork [7]. These music excerpts were from the Kostka-\nPayne music theory book, all of them piano pieces by well-\nknown composers, and were performed on a MIDI key-\nboard by a semi-professional piano player. These excerpts\nwere then transcribed into music notation using four differ-410 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017ent methods: a novel method proposed in the paper (which\nwill be referred to as CDT), MuseScore, GarageBand and\nFinale. For each transcription, the human evaluators were\nasked to assign a numerical rating between 1 and 10 for\nthree musical aspects, i.e., the pitch notation, the rhythm\nnotation, and the note positioning.\nThe proposed method of calculating the error counts\nuses MusicXML [10], the de facto standard for shar-\ning sheet music ﬁles between applications, as the for-\nmat of music notation. Two of the methods evalu-\nated in the paper (Finale and MuseScore) can output\nthe scores into MusicXML. For GarageBand, CDT and\nthe ground truth, however, MusicXML was not avail-\nable or was difﬁcult to output automatically. We had to\nmanually convert the scores into MusicXML. The tran-\nscribed scores are named with the initial of the tran-\nscription method and a number indicating the excerpt.\nSo,M-8.mxl represents the eight excerpt transcribed\nwith MuseScore. The letter K, for Kostka-Payne, in-\ndicates the ground truth scores. This dataset and a\nPython implementation of the proposed approach are\navailable at http://www.ece.rochester.edu/\n˜acogliat/repository.html . The implementa-\ntion uses the music21 toolkit [9] for parsing the Mu-\nsicXML ﬁles and processing the imported scores. The im-\nplementation has been tested with music21 V3.1.0.\nIn order to validate the quality of the prediction we\ncalculated the coefﬁcient of determination R2, which is\nthe square of the Pearson correlation coefﬁcient. The R2\nwas0:558for the pitch notation correlation, 0:534for the\nrhythm notation, and 0:601for note positioning. These re-\nsults are reﬂected in Fig. 4; the proposed metric ﬁts the\ndata adequately, in general, even though the correlation is\nnot perfect. It can also be noted that the prediction of the\nscore for note positioning is the best, while the prediction\nof the score for rhythm notation is the worst.\nTo understand the underlying causes of the covariance\nwe ﬁrstly analyzed the ratings given by the human evalua-\ntors. As we can see from Fig. 5, the human evaluators were\noftentimes in disagreement among themselves. It must also\nbe noted that in our prior work [7], the human annotators\nwere not given exact instructions on what features to con-\nsider for the evaluation, so a considerable amount of sub-\njectivity and judgment calls were likely to be present in the\nratings.\nWe also analyzed two transcriptions with the largest de-\nviation from the predicted ratings, i.e., one transcription\nwith a high predicted rating and a low human rating, and\none transcription with a low predicted rating and a high hu-\nman rating. The largest positive deviation occurred for the\nrhythm notation of transcription M-1, for which the pro-\nposed metric predicted a rating of 2.78, while the average\nhuman rating was 5.98. If we compare the transcription\nwith the ground truth in Fig. 6 we can see that MuseScore\nmisinterpreted the meter, causing the proposed metric to\nreport a large number of note duration errors and barline\nerrors, which resulted in a low rating. Human annotators,\non the other side, likely penalized the meter error only once\nPiece12345678910Score(a) Pitch Notation\nPiece12345678910Score\n(b) Rhythm Notation\nPiece12345678910Score\n(c) Note Positioning\nFigure 5 : Distributions of the human ratings of the 76 tran-\nscriptions contained in the dataset. Each boxplot repre-\nsents the ratings from 5 human evaluators.\nglobally, but still considered the transcription acceptable\noverall.\nThe largest negative deviation occurred for the pitch no-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 411&?Pianoœœ#œœœnœœœœœœœœ#œnœ#œnœŒÓœœœœœ#œnœœbœœ.œ.Óœœœœœœ&œœœ#œUœ.œ.˙˙˙Uœœœbœœœ&&Pno.4jœœœœœUœ.œ.˙˙bUœbœbœœπœbœbœœœb.œ.œœbœœœœœœœbœœbœœœb.œœ.œbœbœœœbœœœbœœ?œbœbœœœœœbb.œœœ.˙bœbœ˙œœœœbœœœœœœ&?Pno.8ŒœœbœbœŒœbœœœbœbœœbœbŒœbœœœbŒlegatoŒœbœbœœbŒœbœœœbœœbœbœŒœbœœœbŒ&?Pno.10ŒœœbœbœŒœœœœœbœœbœbŒœœœnœŒŒœœbœnœŒœbœœœbœœœbœnŒœœbœœŒ&?Pno.12ŒœnœbœœŒœnœbœœnœœbœœŒœœbœœŒŒœœbœœŒœ#œœbœ#œbœœœbŒœnœœbœnŒfK-1(a) Ground Truth\n&?4444œœbœœnœœœœwwœœœœœ#œnœ#œn∑œœœœ#œnœJœbœ3∑&œ.œœœ.œœŒ˙˙&&5œœŒœnÓ˙bwww˙Ó...˙˙˙Œœœ#œ.œœœœŒ˙˙˙˙˙3wwbœœ˙ŒwwŒœœ#ŒÓŒœŒœ#Œœœ#œbœ#œnœŒÓŒœœ#œ&&12œœ#œ#œœ#ŒÓ‰Jœ#œŒœœ#œ#œnœbŒœ#œ#œœ#.œœœnb#Œœœœ.œ#.œ#œ#œ?Œ˙bŒœœ##œœœ#œœœnœ#œœœŒ˙˙#bŒœ#œ#œ#œœœœ#œœŒœœ.&?17∑œœœŒÓœ#œ#œœœ#œœ#&∑œ#œœ#œ#œœb.œœ#?&∑œœbœœ#œ#œœ#œ#?&&&20∑œœ#œbœ#œœbœ#œ#?&∑œœ#œ#œœ#œ#œœ#?&\n(b) M-1\nFigure 6 : Transcription of the ﬁrst excerpt in the dataset\nby MuseScore, which shows the largest positive difference\nbetween the average human rating and the predicted rating,\nthat is a high predicted rating and a low human rating. This\nevaluation difference occurs on the rhythm notation.\ntation of transcription C-13, for which the proposed metric\npredicted a rating of 6.83, while the annotators assigned an\naverage score of of 4.48. If we compare the transcription\nwith the ground truth in Fig. 7, we can notice that CDT\nmakes a single mistake in notating the pitches, i.e., G [[in-\nstead of E ]. It also makes a systematic error notating all Bs\none octave lower. Finally, not grouping the eight notes in\nthe treble staff makes the transcription hard to read. Pos-\nsibly, the human annotators penalized the transcription be-\ncause of its poor readability.\n5. CONCLUSION AND FUTURE WORK\nIn this paper we proposed an objective metric to mea-\nsure the differences between music notation transcriptions\nand the ground truth score. The metric is calculated by\nﬁrst aligning the pitch content of the transcription and the\nground-truth music notation, and then counting the differ-\nences in twelve key musical aspects: barlines, clefs, key\nsignatures, time signatures, notes, note spelling, note dura-\ntions, stem directions, groupings, rests, rest duration, and\nstaff assignment. We then used linear regression to predict\nhuman evaluator ratings along three aspects of music nota-\ntion, namely, pitch notation, rhythm notation, and note po-\nsitioning, from the error counts. Experiments show a clear\ncorrelation between the predicted ratings and the average\nhuman ratings, even though the correlation is not perfect.\nOne issue with the prediction is the high variance of the\nevaluator ratings, which likely originates from the inher-\nent subjectivity of the tasks. Another issue of the proposed\n&&######4242Piano˙#œœ#œœœœœœ‰jœ#œœ˙?œœnœœœœœœœœœœœœœœ#œœœœœœœœœœ#œœœœnœnœ&?######Pno.4œœœœœœœœœœ#œœœœnœœRœ≈‰Œ&œœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœ#œ#?&?######Pno.8œœ#œœœœœœœœ#œœœœnœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœœ&?######Pno.12œœœœœœœœœ#œœœœœ#œœ#œœUŒUœUŒUK-13(a) Ground Truth\n /accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flatflat/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/accidentals.sharp/clefs.G/noteheads.s2/accidentals.sharp/accidentals.sharp/accidentals.sharp/noteheads.s2/rests.3/noteheads.s2/accidentals.flatflat/timesig.C44/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/rests.1o/noteheads.s2/accidentals.natural/clefs.F/noteheads.s2/accidentals.sharp/accidentals.sharp/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/brace237/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/accidentals.flatflat/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/clefs.F/accidentals.sharp/accidentals.sharp/accidentals.sharp3/accidentals.sharp/accidentals.sharp/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flatflat/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flatflat/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/brace222/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s25/clefs.G/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.F/accidentals.sharp/accidentals.sharp/accidentals.sharp/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/dots.dot/rests.1/rests.1/rests.1/rests.3/rests.3/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flatflat/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/brace178/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flatflat/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n(b) C-13\nFigure 7 : Transcription of the thirteenth excerpt in the\ndataset by CDT, which shows the largest negative devia-\ntion between the average human rating and the predicted\nrating on rhythm notation, that is a low predicted rating and\na high human rating. This evaluation difference occurs on\nthe pitch notation.\nmetric is that it does not incorporate music theory knowl-\nedge, such as the method proposed by Temperley to evalu-\nate metrical models [19].\nThe current experiments were conducted on music no-\ntation transcriptions of human performances recorded on\na MIDI keyboard; as a consequence, the transcriptions\ndo not contain the errors commonly observed in audio-to-\nMIDI conversion processes, such as octave errors and extra\nor missing notes [5,6]. More research is necessary to eval-\nuate the performance of the proposed method in the pres-\nence of such errors. In addition, the excerpts in the dataset\nwere very short, compared to real piano pieces, so addi-\ntional research is necessary to assess the robustness of the\nmetric, and its computational complexity on longer pieces.\nA Python implementation of the proposed ap-\nproach, along with the dataset, is available at http:\n//www.ece.rochester.edu/ ˜acogliat/\nrepository.html . This implementation can be used\nto calculate the twelve error counts as well as to predict\nhuman ratings on the three musical aspects of a music\nnotation transcription.\n6. REFERENCES\n[1] Mert Bay, Andreas F Ehmann, and J Stephen Downie.\nEvaluation of Multiple-F0 Estimation and Tracking\nSystems. In Proc. of International Society for Music\nInformation Retrieval (ISMIR) , pages 315–320, 2009.\n[2] Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, 2013.\n[3] Emilios Cambouropoulos. From MIDI to traditional\nmusical notation. In Proc. of the AAAI Workshop on Ar-\ntiﬁcial Intelligence and Music: Towards Formal Mod-412 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017els for Composition, Performance and Analysis , vol-\nume 30, 2000.\n[4] Ali Taylan Cemgil. Bayesian music transcription . PhD\nthesis, Radboud University Nijmegen, 2004.\n[5] Andrea Cogliati and Zhiyao Duan. Piano Music Tran-\nscription Modeling Note Temporal Evolution. In Proc.\nof the IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP) , pages 429–\n433, Brisbane, Australia, 4 2015. IEEE.\n[6] Andrea Cogliati, Zhiyao Duan, and Brendt Wohlberg.\nPiano Transcription with Convolutional Sparse Lateral\nInhibition. IEEE Signal Processing Letters , 24(4):392–\n396, 2017.\n[7] Andrea Cogliati, David Temperley, and Zhiyao Duan.\nTranscribing human piano performances into music\nnotation. In Proc. of International Society for Music\nInformation Retrieval (ISMIR) , 2016.\n[8] Tom Collins, Sebastian B ¨ock, Florian Krebs, and Ger-\nhard Widmer. Bridging the audio-symbolic gap: The\ndiscovery of repeated note content directly from poly-\nphonic music audio. In Audio Engineering Society\nConference: 53rd International Conference: Semantic\nAudio , 2014.\n[9] Michael Scott Cuthbert and Christopher Ariza. mu-\nsic21: A Toolkit for Computer-Aided Musicology and\nSymbolic Music Data. In Proc. of International Society\nfor Music Information Retrieval (ISMIR) , 2010.\n[10] Michael Good. MusicXML for notation and analysis.\nThe virtual score: representation, retrieval, restora-\ntion, 12:113–124, 2001.\n[11] Harald Grohganz, Michael Clausen, and Meinard\nM¨uller. Estimating Musical Time Information from\nPerformed MIDI Files. In Proc. of International So-\nciety for Music Information Retrieval (ISMIR) , pages\n35–40, 2014.\n[12] Ioannis Karydis, Alexandros Nanopoulos, Apostolos\nPapadopoulos, Emilios Cambouropoulos, and Yan-\nnis Manolopoulos. Horizontal and vertical integra-\ntion/segregation in auditory streaming: a voice separa-\ntion algorithm for symbolic musical data. In Proc. 4th\nSound and Music Computing Conference (SMC2007) ,\n2007.\n[13] Jonathan M. Keith, editor. Bioinformatics , volume\n1525 of Methods in Molecular Biology . Springer New\nYork, New York, NY , 2017.\n[14] Meinard M ¨uller. Fundamentals of Music Processing:\nAudio, Analysis, Algorithms, Applications . Springer,\n2015.\n[15] Gonzalo Navarro. A guided tour to approximate string\nmatching. ACM Computing Surveys , 33(1):31–88, 3\n2001.[16] Kazuki Ochiai, Hirokazu Kameoka, and Shigeki\nSagayama. Explicit beat structure modeling for non-\nnegative matrix factorization-based multipitch analy-\nsis. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 133–\n136, 2012.\n[17] H. Sakoe and S. Chiba. Dynamic programming algo-\nrithm optimization for spoken word recognition. IEEE\nTransactions on Acoustics, Speech, and Signal Pro-\ncessing , 26(1):43–49, 2 1978.\n[18] Haruto Takeda, Naoki Saito, Tomoshi Otsuki, Mitsuru\nNakai, Hiroshi Shimodaira, and Shigeki Sagayama.\nHidden Markov model for automatic transcription of\nMIDI signals. In Multimedia Signal Processing, 2002\nIEEE Workshop on , pages 428–431, 2002.\n[19] David Temperley. An Evaluation System for Metrical\nModels. Computer Music Journal , 2004.\n[20] David Temperley. Music and probability . The MIT\nPress, 2007.\n[21] David Temperley. A uniﬁed probabilistic model for\npolyphonic music analysis. Journal of New Music Re-\nsearch , 38(1):3–18, 2009.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 413"
    },
    {
        "title": "A Database Linking Piano and Orchestral MIDI Scores with Application to Automatic Projective Orchestration.",
        "author": [
            "Léopold Crestel",
            "Philippe Esling",
            "Lena Heng",
            "Stephen McAdams"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416204",
        "url": "https://doi.org/10.5281/zenodo.1416204",
        "ee": "https://zenodo.org/records/1416204/files/CrestelEHM17.pdf",
        "abstract": "This article introduces the Projective Orchestral Database (POD), a collection of MIDI scores composed of pairs linking piano scores to their corresponding orchestrations. To the best of our knowledge, this is the first database of its kind, which performs piano or orchestral prediction, but more importantly which tries to learn the correlations be- tween piano and orchestral scores. Hence, we also intro- duce the projective orchestration task, which consists in learning how to perform the automatic orchestration of a piano score. We show how this task can be addressed using learning methods and also provide methodological guide- lines in order to properly use this database.",
        "zenodo_id": 1416204,
        "dblp_key": "conf/ismir/CrestelEHM17",
        "keywords": [
            "Projective Orchestral Database (POD)",
            "MIDI scores",
            "piano scores",
            "orchestration",
            "prediction task",
            "correlations",
            "learning methods",
            "automated orchestration",
            "piano score",
            "orchestral score"
        ],
        "content": "A DATABASE LINKING PIANO AND ORCHESTRAL MIDI SCORES WITH\nAPPLICATION TO AUTOMATIC PROJECTIVE ORCHESTRATION\nL´eopold Crestel1Philippe Esling1\nLena Heng2Stephen McAdams2\n1Music Representations, IRCAM, Paris, France\n2Schulich School of Music, McGill University, Montr ´eal, Canada\nleopold.crestel@ircam.fr\nABSTRACT\nThis article introduces the Projective Orchestral Database\n(POD ), a collection of MIDI scores composed of pairs\nlinking piano scores to their corresponding orchestrations.\nTo the best of our knowledge, this is the ﬁrst database of\nits kind, which performs piano or orchestral prediction, but\nmore importantly which tries to learn the correlations be-\ntween piano and orchestral scores. Hence, we also intro-\nduce the projective orchestration task, which consists in\nlearning how to perform the automatic orchestration of a\npiano score. We show how this task can be addressed using\nlearning methods and also provide methodological guide-\nlines in order to properly use this database.\n1. INTRODUCTION\nOrchestration is the subtle art of writing musical pieces for\nthe orchestra by combining the properties of various instru-\nments in order to achieve a particular musical idea [11,23].\nAmong the variety of writing techniques for orchestra, we\ndeﬁne as projective orchestration [8] the technique which\nconsists in ﬁrst writing a piano score and then orchestrating\nit (akin to a projection operation, as depicted in Figure 1).\nThis technique has been used by classic composers for cen-\nturies. One such example is the orchestration by Maurice\nRavel of Pictures at an Exhibition , a piano work written by\nModest Mussorgsky. This paper introduces the ﬁrst dataset\nof musical scores dedicated to projective orchestrations. It\ncontains pairs of piano pieces associated with their orches-\ntration written by famous composers. Hence, the purpose\nof this database is to offer a solid knowledge for studying\nthe correlations involved in the transformation from a pi-\nano to an orchestral score.\nThe remainder of this paper is organized as follows.\nFirst, the motivations for a scientiﬁc investigation of or-\nchestration are exposed (section 2). By reviewing the\nprevious attempts, we highlight the speciﬁc need for a\nc\rL´eopold Crestel, Philippe Esling, Lena Heng, Stephen\nMcAdams. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: L´eopold Crestel, Philippe\nEsling, Lena Heng, Stephen McAdams. “A database linking piano and\norchestral MIDI scores with application to automatic projective orches-\ntration”, 18th International Society for Music Information Retrieval Con-\nference, Suzhou, China, 2017.\nPiano\nscore\nOrchestr a\nscoreOrchestr ationFigure 1 .Projective orchestration of the ﬁrst three bars\nof Modest Mussorgsky’s piano piece Pictures at an Exhi-\nbition by Maurice Ravel. Piano notes are assigned to one\nor several instruments, possibly with doubling or harmonic\nenhancement.\nsymbolic database of piano and corresponding orchestral\nscores. In an attempt to ﬁll this gap, we built the Projective\nOrchestral Database (POD ) and detail its structure in sec-\ntion 3. In section 4, the automatic projective orchestration\ntask is proposed as an evaluation framework for automatic\norchestration systems. We report our experiment with a\nset of learning-based models derived from the Restricted\nBoltzmann Machine [26] and introduce their performance\nin the previously deﬁned evaluation framework. Finally, in\nsection 5 we provide methodological guidelines and con-\nclusions.\n2. A SCIENTIFIC INVESTIGATION OF\nORCHESTRATION\nOver the past centuries, several treatises have been written\nby renowned composers in an attempt to decipher some\nguiding rules in orchestration [11, 21, 23]. Even though\nthey present a remarkable set of examples, none of them\nbuilds a systemic set of rules towards a comprehensive the-\nory of orchestration. The reason behind this lack lies in\nthe tremendous complexity that emerges from orchestral\nworks. A large number of possible sounds can be created\nby combining the pitch and intensity ranges of each instru-592ments in a symphonic orchestra. Furthermore, during a\nperformance, the sound produced by a mixture of instru-\nments is also the result of highly non-linear acoustic ef-\nfects. Finally, the way we perceive those sounds involves\ncomplex psychoacoustic phenomena [14, 16, 25]. It seems\nalmost impossible for a human mind to grasp in its entirety\nthe intertwined mechanisms of an orchestral rendering.\nHence, we believe that a thorough scientiﬁc investiga-\ntion could help disentangle the multiple factors involved in\norchestral works. This could provide a ﬁrst step towards\na greater understanding of this complex and widely un-\ncharted discipline. Recently, major works have reﬁned our\nunderstanding of the perceptual and cognitive mechanisms\nspeciﬁcally involved when listening to instrumental mix-\ntures [15, 22, 25]. Orchids, an advanced tool for assisting\ncomposers in the search of a particular sonic goal has been\ndeveloped [8]. It relies on the multi-objective optimiza-\ntion of several spectro-temporal features such as those de-\nscribed in [20].\nHowever, few attempts have been made to tackle a sci-\nentiﬁc exploration of orchestration based on the study of\nmusical scores. Yet, symbolic representations implicitly\nconvey high-level information about the spectral knowl-\nedge composers have exploited for timbre manipulations.\nIn [6] a generative system for orchestral music is intro-\nduced. Given a certain style, the system is able to generate\na melodic line and its accompaniment by a full symphonic\norchestra. Their approach relies on a set of templates and\nhand-designed rules characteristic of different styles. [19]\nis a case study of how to automatically transfer the Ode\nto joy to different styles. Unfortunately, very few details\nare provided about the models used, but it is interesting to\nobserve that different models are used for different styles.\nAutomatic arrangement, which consists in reducing an or-\nchestral score to a piano version that is can be played by\na two-hand pianist, has been tackled in [10] and [24]. The\nproposed systems rely on an automatic analysis of the or-\nchestral score in order to split it into structuring elements.\nThen, each element is assigned a role which determines\nwhether it is played or discarded in the reduction. To the\nbest of our knowledge, the inverse problem of automati-\ncally orchestrating a piano score has never been tackled.\nHowever, we believe that unknown mechanisms of orches-\ntration could be revealed by observing how composers per-\nform projective orchestration, which essentially consists in\nhighlighting an existing harmonic, rhythmic and melodic\nstructure of a piano piece through a timbral structure.\nEven though symbolic data are generally regarded as\na more compact representation than a raw signal in the\ncomputer music ﬁeld, the number of pitch combinations\nthat a symphonic orchestra can produce is extremely large.\nHence, the manipulation of symbolic data still remains\ncostly from a computational point of view. Even through\ncomputer analysis, an exhaustive investigation of all the\npossible combinations is not feasible. For that reason, the\napproaches found in the literature rely heavily on heuristics\nand hand-designed rules to limit the number of possible\nsolutions and decrease the complexity. However, the re-cent advents in machine learning have brought techniques\nthat can cope with the dimensionality involved with sym-\nbolic orchestral data. Besides, even if a wide range of\norchestrations exist for a given piano score, all of them\nwill share strong relations with the original piano score.\nTherefore, we make the assumption that projective orches-\ntration might be a relatively simple and well-structured\ntransformation lying in a complex high-dimensional space.\nNeural networks have precisely demonstrated a spectac-\nular ability for extracting a structured lower-dimensional\nmanifold from a high-dimensional entangled representa-\ntion [13]. Hence, we believe that statistical tools are now\npowerful enough to lead a scientiﬁc investigation of pro-\njective orchestration based on symbolic data.\nThese statistical methods require an extensive amount\nof data, but there is no symbolic database dedicated to or-\nchestration. This dataset is a ﬁrst attempt to ﬁll this gap\nby building a freely accessible symbolic database of piano\nscores and corresponding orchestrations.\n3. DATASET\n3.1 Structure of the Database\nThe database can be found on the companion website1\nof this article, along with statistics and Python code for\nreproducibility.\n3.1.1 Organization\nThe Projective Orchestral Database ( POD ) contains 392\nMIDI ﬁles. Those ﬁles are grouped in pairs containing a\npiano score and its orchestral version. Each pair is stored\nin a folder indexed by a number. The ﬁles have been col-\nlected from several free-access databases [1] or created by\nprofessional orchestration teachers.\n3.1.2 Instrumentation\nAs the ﬁles gathered in the database have various origins,\ndifferent instrument names were found under a variety of\naliases and abbreviations. Hence, we provide a comma-\nseparated value ( CSV) ﬁle associated with each MIDI ﬁle\nin order to normalize the corresponding instrumentations.\nIn these ﬁles, the track names of the MIDI ﬁles are linked\nto a normalized instrument name.\n3.1.3 Metadata\nFor each folder, a CSV ﬁle with the name of the folder\ncontains the relative path from the database root directory,\nthe composer name and the piece name for the orches-\ntral and piano works. A list of the composers present in\nthe database can be found in table 1. It is important to\nnote the imbalanced representativeness of composers in the\ndatabase. It can be problematic in the learning context we\ninvestigate, because a kind of stylistic consistency is a pri-\norinecessary in order to extract a coherent set of rules.\nPicking a subset of the database would be one solution,\nbut another possibility would be to add to the database this\nstylistic information and use it in a learning system.\n1https://qsdfo.github.io/LOP/databaseProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 593ComposerNumber of\npiano ﬁlesPercentage\npiano framesNumber of\norchestra ﬁlesPercentage\norchestra frames\nArcadelt. Jacob 1 0.07\nArresti. Floriano 3 0.57\nBach. Anna Magdalena 3 0.43\nBach. Johann Sebastian 9 4.57 4 0.81\nBanchieri. Adriano 1 0.32\nBeethoven. Ludwig Van 1 0.60 38 42.28\nBerlioz. Hector 1 0.14\nBrahms. Johannes 3 0.28\nBuxtehude. Dietrich 1 0.21\nByrd. William 1 0.13\nCharpentier. Marc-Antoine 2 0.38\nChopin. Frederic 2 0.44\nClarke. Jeremiah 1 0.23\nDebussy. Claude 1 0.59 6 0.90\nDvorak. Anton 6 2.42\nErlebach. Philipp Heinrich 1 0.10\nFaure. Gabriel 1 0.60\nFischer. Johann Caspar Ferdinand 1 0.10\nGluck. Christoph Willibald 1 1.61\nGrieg. Edvard 1 2.10\nGuerrero. Francisco 1 0.12\nHandel. George Frideric 4 1.00 1 0.75\nHaydn. Joseph 6 1.01\nKempff. Wilhelm 1 1.58\nLeontovych. Mykola 2 0.22\nLiszt. Franz 34 39.98\nMahler. Gustav 1 0.85\nMendelssohn. Felix 2 1.41\nMoussorgsky. Modest 1 0.04\nMozart. Wolfgang Amadeus 1 0.71 8 1.45\nOkashiro. Chitose 3 1.09\nPachelbel. Johann 1 0.15\nPraetorius. Michael 2 0.14\nPurcell. Henry 1 0.08\nRavel. Maurice 6 6.49 8 6.69\nRondeau. Michel 2 0.25 1 0.14\nSchonberg. Arnold 1 0.21\nSchumann. Robert 1 0.05\nShorter. Steve 1 0.26\nSmetana. Bedrich 1 0.61\nSoler. Antonio 1 0.54\nStrauss. Johann 1 0.04\nStrauss. Richard 1 0.22\nStravinsky. Igor 4 0.94\nTchaikovsky. Piotr Ilyich 36 20.08\nTelemann. Georg Philipp 2 1.04\nUnknown. 107 40.18 28 7.47\nVivaldi. Antonio 4 2.94\nWalther. Johann Gottfried 1 0.14\nWiberg. Steve 1 0.75\nZachow. Friedrich Wilhelm 1 0.32 2 0.23\nTable 1 . This table describes the relative importance of the\ndifferent composers present in the database. For each com-\nposer, the number of piano (respectively orchestral) scores\nin the database are indicated in the second (respectively\nfourth) column. The total number of ﬁles is 184 x 2 = 392.\nAs the length of the ﬁles can vary signiﬁcantly, a more\nsigniﬁcant indicator of a composer’s representativeness in\nthe database is the ratio of the number of frames from its\nscores over the total number of frames in the database.\nFigure 2 highlights the activation ratio of each pitch in\nthe orchestration scores (#fpitch on g\n#fpitch on g+#fpitch off g, where #is\nthe cardinal of an ensemble) over the whole dataset. Note\nthat this activation ratio does not take the duration of notes\ninto consideration, but only their number of occurrences.\nThe pitch range of each instrument can be observed be-\nneath the horizontal axis.\nTwo different kinds of imbalance can be observed in\nﬁgure 2. First, a given pitch is rarely played. Second,\nsome pitches are played more often compared with others.\nClass imbalance is known as being problematic for ma-\nchine learning systems, and these two observations high-\nlight how challenging the projective orchestration task is.\nVln. (40,101)Fl. (38,101)Tba. (21,66)Bsn. (21,77)Org. (35,88)Ob. (54,94)Picc. (59,111)Horn (25,93)Vc. (21,85)Tbn. (25,81)Vla. (40,92)Voice (31,88)Db. (8,68)Tpt. (42,92)Clar. (35,98)Hp. (20,107)pitchFigure 2 . Activation ratio per pitch in the whole orches-\ntral score database. For one bin on the horizontal axis, the\nheight of the bar represents the number of notes played by\nthis instrument divided by the total number of frames in\nthe database. This value is computed for the event-level\naligned representations 4.2. The different instruments are\ncovered by the pitch axis, and one can observe the peaks\nthat their medium ranges form. The maximum value of the\nvertical axis (0.06), which is well below 1, indicates that\neach pitch is rarely played in the whole database.\nMore statistics about the whole database can be found on\nthe companion website.\n3.1.4 Integrity\nBoth the metadata and instrumentation CSV ﬁles have been\nautomatically generated but manually checked. We fol-\nlowed a conservative approach by automatically rejecting\nany score with the slightest ambiguity between a track\nname and a possible instrument (for instance bass can refer\ntodouble-bass orvoice bass ).\n3.1.5 Formats\nTo facilitate the research work, we provide pre-computed\npiano-roll representations such as the one displayed in\nFigure 3. In this case, all the MIDI ﬁles of piano (respec-\ntively orchestra) work have been transformed and concate-\nnated into a unique two-dimensional matrix. The starting\nand ending time of each track is indicated in the meta-\ndata.pkl ﬁle. These matrices can be found in Lua/Torch\n(.t7), Matlab (.m), Python (.npy) and raw (.csv) data for-\nmats.\n3.1.6 Score Alignment\nTwo versions of the database are provided. The ﬁrst\nversion contains unmodiﬁed midi ﬁles. The second\nversion contains MIDI ﬁles automatically aligned us-\ning the Needleman-Wunsch [18] algorithm as detailed in594 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Pitch&&&&???bbbbbbbbbbbbbb45454545454545464646464646464545454545454546464646464646Horns 1.2.Horns 3.4.Trumpet 1 (C)Trumpets 2.3.(C)Trombones 1.2.Bass Trombone(Trb.3)Tubaœ-œ-œ-œ-œœ-fœ-œœ-œ-œ-œ-œ-œœ-œœ-œœ-œœ-œœ-œœ-œœ-œœ-œœ-œœ-œ-œ-œ-œ-œœ-œœ-œœ-œœ-œœ-œœ-œ-œ-œ-œ-œ-œ-œ-œ-œ-œ-fffffœœ-œœ-œœ-œœn-œœ-œœ-œœ-œœ-œœ-œœ-œœn-œœ-œ-œœ-œ-œ-œ-œ-œœ-œœ-œœ-œœn-œœ-œœ-œ-œ-œ-œ-œn-œ-œ-œ-œ-œ-œn-œ-\nTimePitchTrumpetsTrombonesHorns\nTubaOriginal score\nPiano-rollrepresentationFigure 3 .Piano-roll representation of orchestral scores.\nThepiano-rollpris a matrix. A pitch pat timetplayed\nwith an intensity iis represented by pr(p;t) =i, where 0\nis a note off. This deﬁnition is extended to an orchestra by\nsimply concatenating the piano-rolls of every instrument\nalong the pitch dimension.\nSection 3.2.\n3.2 Automatic Alignment\nGiven the diverse origins of the MIDI ﬁles, a piano\nscore and its corresponding orchestration are almost never\naligned temporally. These misalignments are very prob-\nlematic for learning or mining tasks, and in general for any\nprocessing which intends to take advantage of the joint\ninformation provided by the piano and orchestral scores.\nHence, we propose a method to automatically align two\nscores, and released its Python implementation on the\ncompanion website2. More precisely, we consider the\npiano-roll representations (Figure 3) where the scores are\nrepresented as a sequence of vectors. By deﬁning a dis-\ntance between two vectors, the problem of aligning two\nscores can be cast as a univariate sequence-alignment prob-\nlem.\n3.2.1 Needleman-Wunsch\nTheNeedleman-Wunsch (NW) algorithm [18] is a dynamic\nprogramming technique, which ﬁnds the optimal align-\nment between two symbolic sequences by allowing the in-\ntroduction of gaps (empty spaces) in the sequences. An\napplication of the NW algorithm to the automatic align-\nment of musical performances is introduced in [9]. As\npointed out in that article, NW is the most adapted tech-\nnique for aligning two sequences with important structural\ndifferences like skipped parts, for instance.\nThe application of the NW algorithm relies solely on\nthe deﬁnition of a cost function, which allows the pairwise\n2https://qsdfo.github.io/LOP/codecomparison of elements from the two sequences, and the\ncost of opening or extending a gap in one of the two se-\nquences.\n3.2.2 Similarity Function\nTo measure the similarity between two chords, we propose\nthe following process:\n\u000fdiscard intensities by representing notes being\nplayed as one and zero otherwise.\n\u000fcompute the pitch-class representation of the two\nvectors, which ﬂattens all notes to a single octave\nvector (12 notes). In our case, we set the pitch-class\nto one if at least one note of the class is played. For\ninstance, we set the pitch-class of C to one if there is\nany note with pitch C played in the piano-roll vector.\nThis provides an extremely rough approximation of\nthe harmony, which proved to be sufﬁcient for align-\ning two scores. After this step, the dimensions of\neach vector is 12.\n\u000fif one of the vectors is only ﬁlled with zeros, it rep-\nresents a silence, and the similarity is automatically\nset to zero (note that the score function can take neg-\native values).\n\u000ffor two pitch-class vectors AandB, we deﬁne the\nscore as\nS(A;B) =C\u0002P12\ni=1\u000e(Ai+Bi)\nmax(jjA+Bjj1;1)(1)\nwhere\u000eis deﬁned as:\n\u000e(x) =8\n<\n:0if x = 0\n\u00001if x = 1\n1if x = 2\nCis a tunable parameter and jjxjj1=P\nijxijis the\nL1norm.\nBased on the values recommended in [18] and our own\nexperimentations, we set C to 10. The gap-open parameter,\nwhich deﬁnes the cost of introducing a gap in one of the\ntwo sequences, is set to 3 and the gap-extend parameter,\nwhich deﬁnes the cost of extending a gap in one of the two\nsequences, is set to 1.\n4. AN APPLICATION : PROJECTIVE\nAUTOMATIC ORCHESTRATION\nIn this section, we introduce and formalize the automatic\nprojective orchestration task (Figure 1). In particular, we\npropose a system based on statistical learning and deﬁne\nan evaluation framework for using the POD database.\n4.1 Task Deﬁnition\n4.1.1 Orchestral Inference\nFor each orchestral piece, we deﬁne as OandPthe aligned\nsequences of column vectors from the piano-roll of the or-\nchestra and piano parts. We denote as Tthe length of the\naligned sequences OandP.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 595The objective of this task is to infer the present orches-\ntral frame knowing both the recent past of the orchestra\nsequence and the present piano frame. Mathematically, it\nconsists in designing a function fwhere\n^O(t) =f[P(t);O(t\u00001);:::;O (t\u0000N)]8t2[N;:::T ]\n(2)\nandNdeﬁnes the order of the model.\n4.1.2 Evaluation Framework\nWe propose a quantitative evaluation framework based on a\none-step predictive task. As discussed in [5], we make the\nassumption that an accurate predictive model will be able\nto generate original acceptable works. Whereas evaluating\nthe generation of a complete musical score is subjective\nand difﬁcult to quantify, a predictive framework provides\nus with a quantitative evaluation of the performance of a\nmodel. Indeed, many satisfying orchestrations can be cre-\nated from the same piano score. However, the number of\nreasonable inferences of an orchestral frame given its con-\ntext (as described in equation 2) is much more limited.\nAs suggested in [4,12], the accuracy measure [2] can be\nused to compare an inferred frame ^O(t)drawn from (2) to\nthe ground-truth O(t)from the original ﬁle.\nAccuracy (t) = 100:TP(t)\nTP(t) +FP(t) +FN(t)(3)\nwhereTP(t)(true positives) is the number of notes cor-\nrectly predicted (note played in both ^O(t)andO(t)).\nFP(t)(false positive) is the number of notes predicted that\nare not in the original sequence (note played in ^O(t)but\nnot inO(t)).FN(t)(false negative) is the number of un-\nreported notes (note absent in ^O(t), but played in O(t)).\nWhen the quantization gets ﬁner, we observed that a\nmodel which simply repeats the previous frame gradu-\nally obtains the best accuracy as displayed in Table 2.\nTo correct this bias, we recommend using an event-level\nevaluation framework where the comparisons between the\nground truth and the model’s output is only performed for\ntime indices in Tedeﬁned as the set of indexes tesuch that\nO(te)6=O(te\u00001)\nThe deﬁnition of event-level indices can be observed in\nFigure 4.\nIn the context of learning algorithms, splitting the\ndatabase between disjoint train andtestsubsets is highly\nrecommended [3, pg.32-33], and the performance of a\ngiven model is only assessed on the test subset. Finally,\nthe mean accuracy measure over the dataset is given by\n1\nKX\ns2DtestX\nte2Te(s)Accuracy (te) (4)\nwhereDtest deﬁnes the test subset, Te(s)the set of\nevent-time indexes for a given score s, and K=P\ns2DtestjTe(s)j.\n4.2 Proposed Model\nIn this section, we propose a learning-based approach to\ntackle the automatic orchestral inference task.4.2.1 Models\nWe present the results for two models called condi-\ntional Restricted Boltzmann Machine (cRBM ) and Fac-\ntored Gated cRBM (FGcRBM ). The models we explored\nare deﬁned in a probabilistic framework, where the vec-\ntorsO(t)andP(t)are represented as binary random vari-\nables. The orchestral inference function is a neural net-\nwork that expresses the conditional dependencies between\nthe different variables: the present orchestral frame O(t),\nthe present piano frame P(t)and the past orchestral frames\nO(t\u00001;:::;t\u0000N).Hidden units are introduced to model\nthe co-activation of these variables. Their number is a\nhyper-parameter with an order of magnitude of 1000. A\ntheoretical introduction to these models can be found in\n[26], whereas their application to projective orchestration\nis detailed in [7].\n4.2.2 Data Representation\nIn order to process the scores, we import them as piano-\nrollmatrices (see Figure 3). Their extension to orchestral\nscores is obtained by concatenating the piano-rolls of each\ninstrument along the pitch dimension.\nThen, new events te2Teare extracted from both\npiano-rolls as described in Section 4.1. A consequence is\nthat the trained model apprehends the scores as a succes-\nsion of events with no rhythmic structure. This is a sim-\npliﬁcation that considers the rhythmic structure of the pro-\njected orchestral score to be exactly the same as the one of\nthe original piano score. This is false in the general case,\nsince a composer can decide to add nonexistent events in\nan orchestration. However, this provides a reasonable ap-\nproximation that is veriﬁed in a vast majority of cases.\nDuring the generation of an orchestral score given a piano\nscore, the next orchestral frame is predicted in the event-\nlevel framework, but inserted at the temporal location of\nthe corresponding piano frame as depicted in Figure 4.\nAutomatic alignment of the two piano-rolls is per-\nformed on the event-level representations, as described in\nSection 3.2.\nIn order to reduce the input dimensionality, we sys-\ntematically remove any pitch which is never played in the\ntraining database for each instrument. With that simpliﬁ-\ncation the dimension of the orchestral vector typically de-\ncreases from 3584 to 795 and the piano vector dimension\nfrom 128 to 89. Also, we follow the usual orchestral sim-\npliﬁcations used when writing orchestral scores by group-\ning together all the instruments of a same section. For in-\nstance, the violin section, which might be composed by\nseveral instrumentalists, is written as a single part. Finally,\nthe velocity information is discarded, since we use binary\nunits that solely indicate if a note is on or off.\nEventually, we observed that an important proportion of\nthe frames are silences, which mathematically corresponds\nto a column vector ﬁlled with zeros in the piano-roll rep-\nresentation. A consequence of the over-representation of\nsilences is that a model trained on this database will lean\ntowards orchestrating with a silence any piano input, which\nis statistically the most relevant choice. Therefore, orches-596 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Frame levelEvent levelPiano\nOrchestraPitchTimeFigure 4 . From a piano score, the generation of an or-\nchestral score consists in extracting the event-level repre-\nsentation of the piano score, generating the sequence of\norchestral events, and then injecting them at the position\nof the event from the piano score. Note that the silence in\nthe fourth event of the piano score is not orchestrated by\nthe probabilistic model, but is automatically mapped to a\nsilence in the orchestral version.\ntration of silences in the piano score ( P(t) = 0 ) are not\nused as training points. However, it is important to note\nthat they are not removed from the piano-rolls. Hence, si-\nlences could still appear in the past sequence of a training\npoint, since it is a valuable information regarding the struc-\nture of the piece. During generation time, the silences in\nthe piano score are automatically orchestrated with a si-\nlence in the orchestra score. Besides, silences are taken\ninto consideration when computing the accuracy.\n4.2.3 Results\nThe results of the cRBM andFGcRBM on the orchestral\ninference task are compared to two na ¨ıve models. The ﬁrst\nmodel is a random generation of the orchestral frames ob-\ntained by sampling a Bernoulli distribution of parameter\n0:5. The second model predicts an orchestral frame at time\ntby simply repeating the frame at time t\u00001. The results\nare summed up in Table 2.\nModelFrame-level\naccuracy (Q = 4)Frame-level\naccuracy (Q = 8)Event-level\naccuracy\nRandom 0.73 0.73 0.72\nRepeat 61.79 76.41 50.70\ncRBM 5.12 34.25 27.67\nFGcRBM 33.86 43.52 25.80\nTable 2 . Results of the different models for the projective\norchestration task based on frame-level accuracies with a\nquantization of 4 and 8 and event-level accuracies.4.3 Discussion\nAs expected, the random model obtains very poor results.\nThe repeat model outperform all three other models, sur-\nprisingly even in the event-level framework. Indeed, we\nobserved that repeated notes still occur frequently in the\nevent-level framework. For instance, if between two suc-\ncessive events only one note out of ﬁve is modiﬁed, the\naccuracy of the repeat model on this frame will be equal to\n66%.\nIf the FGcRBM model outperforms the cRBM model\nin the frame-level framework, the cRBM is slightly better\nthan the FGcRBM model in the event-level framework.\nGenerations from both models can be listened to on the\ncompanion website3. Even though some fragments are\ncoherent regarding the piano score and the recent past or-\nchestration, the results are mostly unsatisfying. Indeed, we\nobserved that the models learn an extremely high probabil-\nity for every note to be off. Using regularization methods\nsuch as weight decay has not proven efﬁcient. We believe\nthat this is due to the sparsity of the vectors O(t)we try to\ngenerate, and ﬁnding a more adapted data representation\nof the input will be a crucial step.\n5. CONCLUSION AND FUTURE WORK\nWe introduced the Projective Orchestral Database ( POD ),\na collection of MIDI ﬁles dedicated to the study of the re-\nlations between piano scores and corresponding orchestra-\ntions. We believe that the recent advent in machine learn-\ning and data mining has provided the proper tools to take\nadvantage of this important mass of information and in-\nvestigate the correlations between a piano score and its or-\nchestrations. We provide all MIDI ﬁles freely, along with\naligned and non-aligned pre-processed piano-roll repre-\nsentations on the website https://qsdfo.github.\nio/LOP/index.html .\nWe proposed a task called automatic orchestral infer-\nence. Given a piano score and a corresponding orchestra-\ntion, it consists in trying to predict orchestral time frames,\nknowing the corresponding piano frame and the recent past\nof the orchestra. Then, we introduced an evaluation frame-\nwork for this task based on a train and test split of the\ndatabase, and the deﬁnition of an accuracy measure. We\nﬁnally present the results of two models (the cRBM and\nFGcRBM ) in this framework.\nWe hope that the POD will be useful for many re-\nsearchers. Besides the projective orchestration task we de-\nﬁned in this article, the database can be used in several\nother applications, such as generating data for a source-\nseparation model [17]. Even if small errors still persist, we\nthoroughly checked manually the database and guarantee\nits good quality. However, the number of ﬁles collected\nis still small with the aim of leading statistical investiga-\ntions. Hence, we also hope that people will contribute to\nenlarge this database by sharing ﬁles and helping us gather\nthe missing information.\n3https://qsdfo.github.io/LOP/resultsProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 5976. REFERENCES\n[1] Imslp. http://imslp.org/wiki/Main_Page .\nAccessed : 2017-01-23.\n[2] Mert Bay, Andreas F Ehmann, and J Stephen Downie.\nEvaluation of multiple-f0 estimation and tracking sys-\ntems. In ISMIR , pages 315–320, 2009.\n[3] Christopher M Bishop. Pattern recognition and ma-\nchine learning . springer, 2006.\n[4] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. arXiv\npreprint arXiv:1206.6392 , 2012.\n[5] Darrell Conklin and Ian H Witten. Multiple viewpoint\nsystems for music prediction. Journal of New Music\nResearch , 24(1):51–73, 1995.\n[6] J. Cookerly. Complete orchestration system, May 18\n2010. US Patent 7,718,883.\n[7] Leopold Crestel and Philippe Esling. Live orchestral\npiano, a system for real-time orchestral music genera-\ntion. In Proceedings of the 14th Sound and Music Com-\nputing Conference , Aalto, Finland, July 2017.\n[8] Philippe Esling, Gr ´egoire Carpentier, and Carlos Agon.\nDynamic musical orchestration using genetic algo-\nrithms and a spectro-temporal description of musical\ninstruments. Applications of Evolutionary Computa-\ntion, pages 371–380, 2010.\n[9] Maarten Grachten, Martin Gasser, Andreas Arzt, and\nGerhard Widmer. Automatic alignment of music per-\nformances with structural differences. In In Proceed-\nings of 14th International Society for Music Informa-\ntion Retrieval Conference (ISMIR . Citeseer, 2013.\n[10] Jiun-Long Huang, Shih-Chuan Chiu, and Man-Kwan\nShan. Towards an automatic music arrangement frame-\nwork using score reduction. ACM Transactions on\nMultimedia Computing, Communications, and Appli-\ncations (TOMM) , 8(1):8, 2012.\n[11] Charles Koechlin. Trait ´e de l’orchestration .´Editions\nMax Eschig, 1941.\n[12] Victor Lavrenko and Jeremy Pickens. Polyphonic mu-\nsic modeling with random ﬁelds. In Proceedings of the\neleventh ACM international conference on Multimedia ,\npages 120–129. ACM, 2003.\n[13] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep learning. Nature , 521(7553):436–444, 05 2015.\n[14] Sven-Amin Lembke and Stephen McAdams. Timbre\nblending of wind instruments: acoustics and percep-\ntion. 2012.[15] Stephen McAdams. Timbre as a structuring force in\nmusic. In Proceedings of Meetings on Acoustics , vol-\nume 19, page 035050. Acoustical Society of America,\n2013.\n[16] Stephen McAdams and Bruno L Giordano. The per-\nception of musical timbre. The Oxford handbook of\nmusic psychology , pages 72–80, 2009.\n[17] M. Miron, J. Janer, and E. G ´omez. Generating data to\ntrain convolutional neural networks for classical music\nsource separation. In Proceedings of the 14th Sound\nand Music Computing Conference , pages 227–233,\nAalto, Finland, 2017 2017.\n[18] Saul B. Needleman and Christian D. Wunsch. A gen-\neral method applicable to the search for similarities in\nthe amino acid sequence of two proteins. Journal of\nMolecular Biology , 48(3):443 – 453, 1970.\n[19] Franc ¸ois Pachet. A joyful ode to automatic orches-\ntration. ACM Trans. Intell. Syst. Technol. , 8(2):18:1–\n18:13, October 2016.\n[20] Geoffroy Peeters, Bruno L Giordano, Patrick Susini,\nNicolas Misdariis, and Stephen McAdams. The tim-\nbre toolbox: Extracting audio descriptors from musical\nsignals. The Journal of the Acoustical Society of Amer-\nica, 130(5):2902–2916, 2011.\n[21] Walter Piston. Orchestration . New York: Norton,\n1955.\n[22] Daniel Pressnitzer, Stephen McAdams, Suzanne Wins-\nberg, and Joshua Fineberg. Perception of musical\ntension for nontonal orchestral timbres and its rela-\ntion to psychoacoustic roughness. Perception & psy-\nchophysics , 62(1):66–80, 2000.\n[23] Nikolay Rimsky-Korsakov. Principles of Orchestra-\ntion. Russischer Musikverlag, 1873.\n[24] Hirofumi Takamori, Haruki Sato, Takayuki Nakatsuka,\nand Shigeo Morishima. Automatic arranging musical\nscore for piano using important musical elements. In\nProceedings of the 14th Sound and Music Computing\nConference , Aalto, Finland, July 2017.\n[25] Damien Tardieu and Stephen McAdams. Perception of\ndyads of impulsive and sustained instrument sounds.\nMusic Perception , 30(2):117–128, 2012.\n[26] Graham W Taylor and Geoffrey E Hinton. Factored\nconditional restricted boltzmann machines for model-\ning motion style. In Proceedings of the 26th annual\ninternational conference on machine learning , pages\n1025–1032. ACM, 2009.598 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Analysis of Interactive Intonation in Unaccompanied SATB Ensembles.",
        "author": [
            "Jiajie Dai",
            "Simon Dixon"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1418327",
        "url": "https://doi.org/10.5281/zenodo.1418327",
        "ee": "https://zenodo.org/records/1418327/files/DaiD17.pdf",
        "abstract": "Unaccompanied ensemble singing is common in many mu- sical cultures, yet it requires great skill for singers to listen to each other and adjust their pitch to stay in tune. The aim of this research is to investigate interaction in four-part (SATB) singing from the point of view of pitch accuracy (intonation). In particular we compare intonation accuracy of individual singers and collaborative ensembles. 20 par- ticipants (five groups of four) sang two pieces of music in three different listening conditions: solo, with one vocal part missing and with all vocal parts. After semi-automatic pitch extraction and manual correction, we annotated the recordings and calculated the pitch error, melodic interval error, harmonic interval error and note stability. We ob- served significant differences between individual and in- teractional intonation, more specifically: 1) Singing with- out the bass part has less mean absolute pitch error than singing with all vocal parts; 2) Mean absolute melodic in- terval error increases when participants can hear the other parts; 3) Mean absolute harmonic interval error is higher in the one-way interaction condition than the two-way inter- action condition; and 4) Singers produce more stable notes when singing solo than with their partners.",
        "zenodo_id": 1418327,
        "dblp_key": "conf/ismir/DaiD17",
        "keywords": [
            "unaccompanied ensemble singing",
            "pitch accuracy",
            "musical cultures",
            "great skill",
            "interaction in singing",
            "pitch error",
            "melodic interval error",
            "harmonic interval error",
            "note stability",
            "significant differences"
        ],
        "content": "ANALYSIS OF INTERACTIVE INTONATION IN UNACCOMPANIED SATB\nENSEMBLES\nJiajie Dai, Simon Dixon\nCentre for Digital Music, Queen Mary University of London, United Kingdom\nfj.dai, s.e.dixon g@qmul.ac.uk\nABSTRACT\nUnaccompanied ensemble singing is common in many mu-\nsical cultures, yet it requires great skill for singers to listen\nto each other and adjust their pitch to stay in tune. The\naim of this research is to investigate interaction in four-part\n(SATB) singing from the point of view of pitch accuracy\n(intonation). In particular we compare intonation accuracy\nof individual singers and collaborative ensembles. 20 par-\nticipants (ﬁve groups of four) sang two pieces of music\nin three different listening conditions: solo, with one vocal\npart missing and with all vocal parts. After semi-automatic\npitch extraction and manual correction, we annotated the\nrecordings and calculated the pitch error, melodic interval\nerror, harmonic interval error and note stability. We ob-\nserved signiﬁcant differences between individual and in-\nteractional intonation, more speciﬁcally: 1) Singing with-\nout the bass part has less mean absolute pitch error than\nsinging with all vocal parts; 2) Mean absolute melodic in-\nterval error increases when participants can hear the other\nparts; 3) Mean absolute harmonic interval error is higher in\nthe one-way interaction condition than the two-way inter-\naction condition; and 4) Singers produce more stable notes\nwhen singing solo than with their partners.\n1. INTRODUCTION AND BACKGROUND\nV oice is our original instrument [8], even from prehistoric\ntimes [13], and it is one of the deﬁning features of human-\nity [26]. This instrument communicates emotion, express-\ning joy and sadness, hope and despair. Throughout the his-\ntory of vocal performance, various theories have been set\nforth on vocal aesthetics and intonation in both individual\nand ensemble settings. This paper investigates the inﬂu-\nence of interaction between singers on the intonation of\nsinging ensembles.\nIntonation describes how a pitch is played or sung in\ntune [7]. Its extreme importance in Western music arises\nfrom the fact that it relates to both melody and harmony,\ntwo central aspects of tonal music. The accuracy of into-\nnation is determined by culturally speciﬁc tuning systems\nc\rJiajie Dai, Simon Dixon. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Jiajie Dai, Simon Dixon. “Analysis of interactive intonation in\nunaccompanied SATB ensembles”, 18th International Society for Music\nInformation Retrieval Conference, Suzhou, China, 2017.such as the equal tempered tuning system in Western mu-\nsic [25].\nWithout interaction or accompaniment, it is extremely\ndifﬁcult to sing with accurate pitch. Only 0.01% of people\nhave absolute pitch [22], which is the ability to identify\nor reproduce any given note on demand [2]. Others must\nrely on relative pitch for tuning, comparing current audi-\ntory feedback with the memory of recently heard tones. As\nthis memory fades, singers may sing out of tune or exhibit\npitch drift, where intonation moves away from the refer-\nence pitch during a performance [9, 12, 20]. Singers also\nuse their muscle memory, a learnt relationship between\nmuscle strength and pitch, to tune their pitch [1].\nAlthough the intonation of singers in individual and\ngroup settings has been investigated, very little of this re-\nsearch addresses interaction between singers in vocal en-\nsembles. In Western music, one common conﬁguration\nfor singing ensembles and choirs comprises four musical\nvoices or parts: soprano, alto, tenor and bass (SATB); so\nwe chose the SATB ensemble as the research target for this\npaper.\nMusic ensembles are well-characterised examples of in-\nteractive work groups [28]. Every member of a musical en-\nsemble needs to execute his or her own part ﬂawlessly as\nwell as contribute to the overall performance in a manner\nthat produces a cohesive, uniﬁed sound [3]. This means\nthat individual singers have to stay in tune with their own\npart (their previous notes) and with other singers’ parts\n(concurrent and previous notes) [18, p. 151]. This creates\na practical difﬁculty for SATB singers, because they have\nmultiple potentially conﬂicting reference pitches, as well\nas their own tonal reference, on which they could base their\nrelative pitch, and attending to any speciﬁc one of these\nmay be difﬁcult.\nInteraction plays an important role in ensemble perfor-\nmance, but its effects can be negative. Terasawa and Hi-\nroko [23] claimed that the intonation accuracy of choral\nmembers was inﬂuenced by the progression of chord roots.\nBrandler and Peynircioglu [3] observed that participants\nlearned new pieces of music more efﬁciently when learn-\ning it individually than with companions. M ¨urbe et al. [15]\nobserved that singers’ intonation accuracy is reduced in\nthe absence of auditory feedback. When singers cannot\nhear themselves, they have to rely on their muscle mem-\nory to tune which leads to an inaccurate intonation. Dai\nand Dixon [4] noted that even the presence of an in-tune\nstimulus during singing reduced singers’ accuracy.599Although many publications give guidelines to keep\nsingers in tune by training them as excellent soloists [1,2],\nthe interaction in SATB ensemble performance as it un-\nfolds in real-time has not been fully researched. The tar-\nget of this study is to test the inﬂuence of the various vo-\ncal parts and how the singers interact with each other, es-\npecially how hearing other singers inﬂuences the perfor-\nmance of each vocal part. These effects are tested in terms\nof their effect on intonation.\nIn the next section, we describe the research questions,\nhypotheses and experimental design. The methodology\nsection follows, covering musical materials, experimental\nprocedure and intonation metrics. Then in section 4 we\npresent results in terms of pitch error, melodic interval er-\nror, harmonic interval error and note variability in different\nexperimental conditions. This is followed by a discussion\nin section 5, and a conclusion in section 6. The recordings,\nannotated data and software are made freely available for\nresearch; details are given in section 8.\n2. EXPERIMENTAL DESIGN\n2.1 Research Questions and Hypotheses\nThis study of interactive intonation in unaccompanied\nSATB singing is driven by a number of research questions.\nFirstly, we wish to determine whether singers rely on a\nparticular vocal part for intonation, which we test by sys-\ntematically isolating each vocalist so that the other singers\ncannot hear them. We expect that the bass part, which of-\nten contains the root notes of chords, is more important\nas a tonal reference [23], leading to our ﬁrst hypothesis:\npitch error will be higher when the bass part is missing\nthan when other voices are isolated.\nThe second research question involves the effect of\nhearing other voices on intonation. Previous work suggests\nthat singers are distracted by simultaneous sounds when\nthey are singing (see section 1), and they are less able to\nattend to their auditory feedback loop in order to sing ac-\ncurately. This leads to hypothesis 2, that the conditions in\nwhich singers hear no other voice will have less melodic\ninterval error than the conditions in which they hear other\nsingers. This effect might be strengthened by conscious\nadjustment of singers to the other parts in order to improve\nthe harmonic intervals. Thus as a corollary we frame our\nthird hypothesis, that we expect to see less harmonic in-\nterval error when singers can hear each other than when\nthey are isolated. An additional effect of interaction should\nbe that singers adjust their pitch more during notes where\nthey hear other singers (who might also be adjusting). Thus\nour fourth hypothesis is that within-note variability in pitch\nwill be higher (note stability will be lower) when singers\nhear each other than when they do not.\n2.2 Design\nTo test these hypotheses, a novel experiment was designed\nand implemented, by which we investigate the interaction\nbetween the four vocal parts. We deﬁne three different lis-\ntening conditions, based on what the singer can hear as\nListening \nconditions  Closed condition\nPartial \ncondition\nOpen \nconditionsinger \nsinger singer \nsinger X\n(2 trials)(4*2 trials)Soprano isolated\ncondition\nA STBor Alto isolated\ncondition\nTenor isolated\ncondition\nBass isolated\nconditionS ATBor\nT SABor\nB SATorOne‐to‐three conditionThree‐to‐one condition\nIndependent singerDependent singer\n(Test conditions) Figure 1 : Listening and test conditions. The arrows indi-\ncate the direction of acoustic feedback.\nthey sing. In the closed condition , the singer hears no\nother voice than their own, thus they are effectively singing\nsolo. In the partially-open condition (orpartial condition\nfor short), the singer can only hear some, but not all of\nthe other vocal parts. This is achieved by isolating one\nsinger from the other three, and allowing acoustic feedback\n(via microphones and loudspeakers) in one direction only,\neither from the isolated singer to the other three singers\n(one-to-three condition ), or from the three singers to the\nisolated one ( three-to-one condition ). Finally, in the open\ncondition , all singers can hear each other.\nFor testing the partial condition, there are four pairs of\ntest conditions corresponding to the vocal part that is iso-\nlated and the direction of feedback. For example, one test\ncondition is called the soprano isolated one-to-three condi-\ntion, where the soprano sings in a closed condition, but all\nother parts hear each other (the soprano’s voice being pro-\nvided to the others via a loudspeaker). In such a case the\nisolated singer is called the independent singer as they are\nnot able to react to the other vocal parts to choose their tun-\ning. In other cases the singer can hear all (open condition)\nor some (partial condition) of the other voices, and thus is\ncalled a dependent singer . Figure 1 gives an overview of\nthe listening and test conditions.\n3. EXPERIMENTAL METHODS\n3.1 Participants\n20 adult amateur singers (10 male and 10 female) with\nchoir experience volunteered to take part in the study. The\nage range was from 20 to 55 years old (mean: 27.95, me-\ndian: 26.50, std.dev.: 7.84). Participants were compen-\nsated £10 for their participation. The participants were\nable to sing their parts comfortably and they were given\nthe score and sample audio ﬁles at least 2 weeks before\nthe experiment. They came from the music society and a\ncapella society of the university and a local choir.\nTraining is a crucial factor for intonation accuracy. For600 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017testing the effect of training, all the participants were given\na questionnaire based on the Goldsmiths Musical Sophis-\ntication Index [14]. The participants had an average of 3.3\nyears of music lessons and 5.8 years of singing experience.\n3.2 Materials\nTwo contrasting musical pieces were selected for this\nstudy: a Bach chorale, “Oh Thou, of God the Father”\n(BWV 164/6) and Leo Mathisen’s jazz song “To be or not\nto be”. Both pieces were chosen for their wide range of\nharmonic intervals (see section 3.5.2): the ﬁrst piece has\n34 different harmonic intervals between parts and the sec-\nond piece has 30 harmonic intervals. To control the dura-\ntion of the experiment, we shortened the original score by\ndeleting the repeat. We also reduced the tempo from that\nspeciﬁed in the score, in order to make the pieces easier to\nsing and compensate for the limited time that the singers\nhad to learn the pieces. The resulting duration of the ﬁrst\npiece is 76 seconds and the second song is 100 seconds.\nLinks to the score and training materials can be found in\nsection 8.\nThe equipment included an SSL MADI-AX converter,\nﬁve cardioid microphones and four loudspeakers. All the\ntracks were controlled and recorded by the software Logic\nPro 10. The metronome and the four starting reference\npitches were also given by Logic Pro. The total latency of\nthe system is 4.9 ms (3.3 ms due to hardware and 1.6 ms\nfrom the software).\n3.3 Procedure\nA pilot experiment with singers not involved in the study\nwas performed to test the experimental setup and minimise\npotential problems such as bleed between microphones.\nThen the participants in the study were distributed into 5\ngroups according to their voice type, time availability and\ncollaborative experience (the singers from the same music\nsociety were placed in the same group). Each group con-\ntained two female singers (soprano and alto) and two male\nsingers (tenor and bass). Each participant had at least two\nhours practice before the recording, sometimes on sepa-\nrate days. They were informed about the goal of the study,\nto investigate interactive intonation in SATB singing, and\nthey were asked to sing their best in all circumstances.\nFor each trial, the singers were played their starting\nnotes before commencing the trial, and a metronome ac-\ncompanied the singing to ensure that the same tempo was\nused by all groups. Each piece was sung 10 times by each\ngroup. The ﬁrst and the last trial were recorded in the open\ncondition. The partial and closed condition trials, consist-\ning of 8 test conditions, 4 (isolated voice) \u00022 (direction of\nfeedback), were recorded in between. The order of isolated\nconditions was randomly chosen to control for any learn-\ning effect. For each isolated condition, the three-to-one\ncondition always preceded the one-to-three condition. We\nuse the performance of isolated singers in the one-to-three\nconditions as the data for the closed condition.\nThe singers were recorded in two acoustically isolated\nrooms. For the partial and closed conditions, the isolatedsingers were recorded in a separate room from the other\nthree singers. Loudspeakers in each room provided acous-\ntic feedback according to the test condition. There was no\nvisual contact between singers in different rooms. With the\nexception of warm-up and rehearsal, but including all the\ntrials and the questionnaire, the total duration of the exper-\niment for each group was about one hour and a half.\n3.4 Annotation\nThe experimental data comprises 5 (groups) \u00024 (singers)\n\u00022 (pieces)\u000210 (trials) = 400 audio ﬁles, each contain-\ning 65 to 116 notes. The software Tony [10] was chosen\nas the annotation tool. Tony performs pitch detection us-\ning the PYIN algorithm, which outperforms the YIN algo-\nrithm [11], and then automatically segments pitch trajec-\ntories into note objects, and provides a convenient inter-\nface for manual checking and correction of the resulting\nannotations. For each audio ﬁle, we exported two .csv\nﬁles, one containing the note-level information (for calcu-\nlating pitch and interval errors) and the other containing the\npitch trajectories (for calculating pitch variability). All the\nintonations were measured by twelve-tone equal tempera-\nment, expressed in semitones according to MIDI standard\npitch numbering. It took about 67 hours to manually check\nand correct the 400 ﬁles, resulting in 49200 annotated sin-\ngle notes, to which we added information on the singer\n(anonymised), score notes and metrics of accuracy.\n3.5 Intonation Metrics\nTo quantify the effects of interaction on intonation, we\nmeasure pitch accuracy in terms of pitch error, melodic\ninterval error, harmonic interval error and note stability,\ndeﬁned below.\n3.5.1 Pitch Error\nAssuming that a reference pitch has been given, pitch error\ncan be deﬁned as the difference between observed pitch\nand score pitch [12]:\nep\ni=¯pi-ps\ni (1)\nwhere ¯piis the median of the observed pitch trajectory of\nnotei(calculated over the duration of an individual note),\nandps\niis the score pitch of note i.\nTo evaluate the pitch accuracy of a sung part, we use\nmean absolute pitch error (MAPE) as the measurement.\nFor a group of M notes with pitch errors ep\n1, . . . ,ep\nM, the\nMAPE is deﬁned as:\nMAPE =1\nMMX\ni=1jep\nij (2)\n3.5.2 Melodic and Harmonic Interval Error\nA musical interval is the difference between two pitches\n[19], which is proportional to the logarithm of the ratio of\nthe fundamental frequencies of the two pitches. We distin-\nguish two types of interval in this experiment: in a melodic\ninterval , the two notes are sounded in succession; while inProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 601aharmonic interval , both notes are played simultaneously\n(Figure 2).\nFigure 2 : A melodic interval and harmonic interval of a\nmajor third (four semitones).\nWe thus calculate the melodic interval error as the dif-\nference between the observed and score intervals:\nem\ni= (¯pi+1-¯pi) - (ps\ni+1-ps\ni) (3)\nwhere ps\niandps\ni+1are the score pitches of two sequenced\nnotes, and ¯piand¯pi+1are their observed median pitches.\nSimilarly, harmonic interval error is deﬁned as:\neh\ni,A,j,B= ( ¯pi,A-¯pj,B) - (ps\ni,A-ps\nj,B) (4)\nwhere ps\ni,Aandps\nj,Bare the score pitches of two simulta-\nneous notes from singers AandBrespectively, and ¯pi,A\nand¯pj,Bare their observed median pitches.\nThemean absolute melodic interval error (MAMIE) for\nMintervals is calculated as follows:\nMAMIE =1\nMMX\ni=1jem\ni j. (5)\nThemean absolute harmonic interval error (MAHIE)\nis calculated similarly (where we simplify the notation and\nassume Mharmonic intervals in total, indexed by i):\nMAHIE =1\nMMX\ni=1jeh\nij. (6)\nHarmonic intervals were evaluated for all pairs of notes\nwhich overlap in time. If one singer sings two notes while\nthe second singer holds one note in the same time period,\ntwo harmonic intervals are observed. Thus indices iandj\nin Eq. (4) are not assumed to be equal.\n3.5.3 Note Stability\nPitch stability has been deﬁned as the mean square pitch\nerror of the note trajectory [17, 24], annotated using a ﬁne\ntime resolution, in this case Tony’s default hop size of\n5.8ms (section 3.4). We prefer to call this pitch variabil-\nity, as higher values correspond to less stable notes. For\na note trajectory for note iconsisting of Nframes, if the\npitch of frame nispf\ni,nand the median pitch ¯pi, the note\nvariability viis given by:\nvi=1\nNNX\nn=1jpf\ni,n-¯pij2(7)\nThemean note variability (MNV) is the mean variabil-\nity of M notes:MNV =1\nMMX\ni=1vi (8)\n4. RESULTS\nThe primary aim of this study was to test experimentally\nwhether, and under what conditions, interaction is beneﬁ-\ncial or detrimental to SATB intonation accuracy. We tested\nthe intonation accuracy of individuals by pitch error (sec-\ntion 4.1), melodic interval error (section 4.2) and note sta-\nbility (section 4.4); and tested the intonation of pairs of\nsingers by harmonic interval error (section 4.3). In order\nto avoid biasing mean errors by outliers, where a partici-\npant sang a wrong note rather than an out-of-tune attempt\nat the correct pitch, all the tests exclude notes with pitch\nerror or interval error larger in magnitude than one semi-\ntone. 96.4% of observed notes had an absolute pitch error\nless than one semitone.\n4.1 Pitch Error\nThe ﬁrst task is to investigate whether the ensemble de-\npends on a certain vocal part to tune their pitch. After ex-\ncluding the notes which have an absolute pitch error larger\nthan one semitone (3.6%), most of the observed notes are\nrelatively accurate (mean: 0.25 semitones; median: 0.26;\nstd.dev.: 0.07).\nWe compute pitch error for the three non-isolated\nsingers in each three-to-one condition and open condition,\nand analyse results by test condition. The MAPE was com-\nputed as an average across the three non-isolated singers\nand the ﬁve groups. For example, in the soprano isolated\nthree-to-one condition, we average the pitch errors of alto,\ntenor, bass parts from each group and report the resulting\nMAPE. We compare these results with the performance of\nthe same three singers in the open conditions.\nA correlated samples analysis of variance (ANOV A)\nshowed a signiﬁcant difference in MAPE between three-\nto-one and open conditions (F(1,21625)=13, p <.001).\nThe MAPE of the three-to-one condition is less than the\nMAPE of the open condition. We then performed separate\nANOV As for each isolated voice type (Table 1), and found\nthat the results vary across test conditions. The bass and\ntenor isolated three-to-one conditions both showed signif-\nicant differences, while the results for the other two voice\ntypes were not signiﬁcant.\nTest condition Partial vs open condition\nSoprano isolated F(1,9391)=2.86, p=0.09\nAlto isolated F(1,9614)=0.61, p=0.11\nTenor isolated F(1,9742)=5.07, p=0.02*\nBass isolated F(1,10223)=14.39, p <.001***\nTable 1 : Results of correlated samples ANOV As for\nthree-to-one and open listening conditions (***p <.001;\n**p<.01; *p <.05))602 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017These results suggest that the bass part is the most in-\nﬂuential vocal part in all observed groups. However, the\ndirection of inﬂuence is the opposite of that hypothesised:\nremoving the bass vocal part from the ensemble reduces\nthe observed pitch error on average.\nThe next ANOV A shows that the MAPE is signiﬁcantly\ndifferent between the test conditions in the three-to-one\nlistening condition (F(3,12948)=28.67, p <.001). Table 2\nshows the 95% conﬁdence intervals, which demonstrate\nthat the bass and tenor isolated conditions are signiﬁcantly\ndifferent from all other three-to-one conditions. The bass\nisolated condition has 4 cents MAPE less than soprano and\nalto isolated conditions, and 2 cents MAPE smaller than\nthe tenor isolated condition.\nTest condition MAPE Conﬁdence interval\nSoprano isolated 0.2484 [0.2420, 0.2548]\nAlto isolated 0.2483 [0.2422, 0.2545]\nTenor isolated 0.2328 [0.2271, 0.2385]\nBass isolated 0.2082 [0.2028, 0.2135]\nTable 2 : Mean absolute pitch error (MAPE) and 95% con-\nﬁdence intervals for three-to-one test conditions, for all\nnon-isolated singers and all groups.\nThese results contradict hypothesis one: when singers\ndo not hear the bass part, they sing more accurately on av-\nerage, as shown by comparisons within the three-to-one\nconditions and between the three-to-one and open condi-\ntions.\n4.2 Melodic Interval Error\nTo test the inﬂuence of interaction on adjacent notes within\na voice (hypothesis two), melodic interval error was cal-\nculated. 91.9% of the note pairs have a melodic interval\nerror smaller than one semitone (mean:0.21; median:0.21;\nstd.dev.:0.07).\nWe performed a correlated-samples ANOV A to test\nthe effect of listening condition on MAMIE. The\nMAMIE is signiﬁcantly different across listening condi-\ntions (F(2,18333)=27.96, p <.001). The listening condi-\ntion of singing without hearing any partners (closed) has\nsmaller MAMIE than the listening conditions with part-\nners (partial and open). Table 3 shows the mean and con-\nﬁdence intervals for the three listening conditions where\nthe closed listening condition has 3 cents smaller MAMIE\nthan the open listening condition.\nListening condition MAMIE Conﬁdence interval\nClosed condition 0.1874 [0.1828, 0.1919]\nPartial condition 0.2001 [0.1953, 0.2049]\nOpen condition 0.2138 [0.2102, 0.2174]\nTable 3 : Mean absolute melodic interval error (MAMIE)\nand 95% conﬁdence intervals for each listening condition.\nThe acoustic feedback from other vocal parts increasesMAMIE, which concurs with ﬁndings from previous re-\nsearch [15] and supports hypothesis two. The accompa-\nniment from other vocal parts may mask the singer’s own\nvoice or distract the singer’s attention from their own into-\nnation. Alternatively, the increase in melodic interval error\ncould be a side effect of deliberate adjustment of intonation\nto reduce harmonic interval error.\n4.3 Harmonic Interval Error\nBeside the intonation accuracy of individual singers, the\naccuracy of pairs of singers was also tested. There are four\nindividual singers and up to six harmonic intervals simul-\ntaneously present at any point in time. All the harmonic in-\ntervals were observed under two circumstances: one-way\ninteraction and two-way interaction.\nIn the partial conditions, some of the communication is\nonly in one direction, so that any deliberate adjustment in\nharmonic interval must be attributed to the singer who can\nhear their partner. In this case, we have a one-way inter-\naction . In the open conditions, both singers in a pair are\nable to adjust to each other, creating a two-way interac-\ntion. Taking soprano isolated conditions as an example,\nthe harmonic intervals involving soprano are one-way in-\nteractions, and the harmonic intervals between alto, tenor\nand bass are two-way interactions (Figure 3).\nsopranoalto\ntenor\nbassHarmonic interval with\none‐way interaction:\none‐to‐three condition\nthree‐to‐one condition\nHarmonic interval with two‐way interactionIsolated singer\nFigure 3 : Interaction in the soprano isolated conditions\nWe compare the MAHIE for two-way interactions\nwith those for one-way interactions in the three-to-one\ntest conditions. MAHIE is signiﬁcantly smaller for\nthe two-way interactions than for one-way interactions\n(F(1,23659)=10.94, p <.001). This supports the third hy-\npothesis, and indicates that acoustic feedback helps singers\nto interactively tune harmonic intervals.\nHowever, no signiﬁcant difference was found between\nMAHIE for different directions of intonation, that is the\nthree-to-one condition versus the one-to-three condition\n(F(1,23524)=0.39, p=0.53). When one side of interactive\nintonation is without acoustic feedback, the direction of the\nfeedback does not appear to inﬂuence the harmonic inter-\nval.\n4.4 Note Stability\nThe note stability is measured by its converse, note vari-\nability (Eq. 7). The acoustic feedback of other singers not\nonly has an inﬂuence on intonation accuracy (section 4.2)\nbut also has an inﬂuence on note variability.\nThe note variability in the closed condition is signiﬁ-\ncantly different from that in the partial and open condi-\ntions (F(1,23659)=41.23, p <.001), but no signiﬁcant dif-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 603ference was found between the partial and open conditions\n(F(1,22514)=1.37, p=0.24). Note trajectories become less\nstable when singers can hear other singers in addition to\ntheir own voice, which is further evidence of interaction in\nintonation. This agrees with previous studies, which show\nthat singers perform worse when singing with an unstable\nreference pitch [4, 16].\nMoreover, the note variability is weakly positively cor-\nrelated to the MAPE of individual notes (r=0.18, p <.001),\nbut it is not obviously related to the singer (r=0.01, p=0.01)\nor training experience (r=0.08, p <.001).\nThe fourth hypothesis has been tested, and the results\nconﬁrm that there is a relationship between the listening\ncondition and note stability. This complements results\nfrom other research which assert that note stability of in-\ndividual singers depends on emotional expression [5, 21].\nOther possible relationships, such as a connection between\nmusical training and note stability, were not supported by\nthe experimental results.\n5. DISCUSSION AND FUTURE WORK\nThis study tested four hypotheses using various metrics of\nsinging accuracy and statistical tests. In each case, signif-\nicant results were found. In three of the four cases, the\nresults supported the hypotheses, however for the ﬁrst hy-\npothesis, the direction of the observed effect was the oppo-\nsite of what was predicted.\nParticipants noted that the bass part (male singer) is the\nmost difﬁcult vocal part to recruit. It is possible that this\nleads to a lower average standard among bass singers. A\ncomparison of pitch error by vocal type reveals that the\nbass vocal part has a larger MAPE than the other vocal\nparts. This may be the cause of the unexpected result for\nthe bass isolated condition: i.e. because the bass voice had\ngreater pitch error, other parts which tuned to the bass also\nincreased their pitch error.\nThe factor of interaction, that is when singers can hear\neach other, increases the pitch error of the individual\nsingers but decreases the harmonic interval error between\nthe singers. Although these results may appear to be con-\ntradictory, this can occur when interval errors accumulate,\nand the sung pitches drift away from the initial tonal refer-\nence, as has been demonstrated by Howard [6].\nMany factors of inﬂuence have been researched which\nare crucial for singing, such as age and gender (boys are\nmore likely to sing out of tune than girls), and individual\ndifferences [27]. As it is not possible to cover all aspects in\nthis paper, we leave the analysis of results from the ques-\ntionnaire to future work, including the investigation of the\nrelationships between intonation accuracy and active en-\ngagement with music, perceptual abilities, musical training\nand singing ability.\n6. CONCLUSIONS\nFor analysis of the effect of interaction on intonation in un-\naccompanied SATB singing, we designed a novel experi-\nment and tested the intonation accuracy of ﬁve groups ofsingers in a series of test and listening conditions. The re-\nsults conﬁrm that interaction exists between singers and in-\nﬂuences their intonation, and that intonation accuracy de-\npends on which other singers each individual singer can\nhear.\nIn particular, we observed that the three-to-one bass iso-\nlated test condition had a signiﬁcantly lower MAPE com-\npared with other three-to-one conditions, and compared\nwith the open condition. In other words, singers were more\naccurate when they could not hear the bass. This surpris-\ning result might be due to the fact that the bass singers were\nless accurate on average than other singers in this experi-\nment.\nWe observed increases in pitch error and melodic inter-\nval error when singers could hear each other. The closed\ncondition had the smallest MAMIE, while the open condi-\ntion had the largest. At the same time, acoustic feedback\ndecreased the harmonic interval error, while the direction\nof the feedback did not inﬂuence the harmonic interval er-\nror.\nInteraction also has the effect of reducing the note sta-\nbility, or increasing its variability. Pitch within a note\nvaries more when singers hear each other, as one might\nexpect if the singers are adjusting their intonation to be in\ntune with each other.\nIn conclusion, this paper addresses a gap in singing in-\ntonation studies, by investigating the effects of interaction\nbetween singers. We found that interaction signiﬁcantly\ninﬂuences the pitch accuracy, leading to increases in the\npitch error, melodic interval error, and note stability but a\ndecrease in the harmonic interval error. Although many as-\npects of the data remain to be explored, we hope the current\nresults provide useful information and better understand-\ning of interactive intonation.\n7. ACKNOWLEDGEMENT\nMany thanks to all of the participants who contributed\nto this project, including the QMUL A Capella Society,\nQMUL Music Society, London Philharmonic Choir, the\nHi-Fan V ocal Group. We also thank Marcus Pearce, Daniel\nStowell and Christophe Rhodes for their advice on experi-\nmental design. Jiajie Dai is supported by a China Scholar-\nship Council and Queen Mary Joint PhD Scholarship.\n8. DATA A VAILABILITY\nAnnotated data, experimental score and code to re-\nproduce our results are available at: https://code.\nsoundsoftware.ac.uk/projects/analysis-of-\ninteractive-intonation-in-unaccompanied-\nsatb-ensembles/repository .\n9. REFERENCES\n[1] Per-Gunnar Alldahl. Choral Intonation . Gehrmans,\n2008.604 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[2] Jocelei C S Bohrer. Intonational Strategies in Ensem-\nble Singing. Doctoral thesis, City University London ,\n2002.\n[3] Brian J Brandler and Zehra F Peynircioglu. A Com-\nparison of the Efﬁcacy of Individual and Collabora-\ntive Music Learning in Ensemble Rehearsals. Journal\nof Research in Music Education , 63(3):281–297, 2015.\n[4] Jiajie Dai and Simon Dixon. Analysis of V ocal Imi-\ntation of Pitch Trajectories. 17th ISMIR Conference ,\n2016.\n[5] Janina Fyk. Melodic Intonation, Psychoacoustics and\nthe Violin . Organon, 1995.\n[6] David M Howard. Intonation drift in a capella soprano,\nalto, tenor, bass quartet singing with key modulation.\nJournal of Voice , 21(3):300–315, 2007.\n[7] Joyce B Kennedy and Michael Kennedy. The Concise\nOxford Dictionary of Music . Oxford University Press,\n2004.\n[8] Joan La Barbara. V oice is the Original Instrument.\nContemporary Music Review , 21(1):35–48, 2002.\n[9] Peggy A Long. Relationships Between Pitch Memory\nin Short Melodies and Selected Factors. Journal of Re-\nsearch in Music Education , 25(4):272–282, 1977.\n[10] M. Mauch, C. Cannam, R. Bittner, G. Fazekas, J. Bello\nJ. Salamon and, J. Dai, and S. Dixon. Tony: a Tool for\nEfﬁcient Computer-aided Melody Note Transcription.\nInProceedings of the First International Conference\non Technologies for Music Notation and Representa-\ntion (TENOR) , 2015.\n[11] M. Mauch and S. Dixon. PYIN: a fundamental fre-\nquency estimator using probabilistic threshold distribu-\ntions. In IEEE International Conference on Acoustics\nSpeech and Signal Processing (ICASSP) , pages 659–\n663, 2014.\n[12] Matthias Mauch, Klaus Frieler, and Simon Dixon. In-\ntonation in unaccompanied singing: Accuracy, drift,\nand a model of reference pitch memory. The Journal\nof the Acoustical Society of America , 136(1):401–411,\n2014.\n[13] Steven J Mithen. The Singing Neanderthal: A Search\nfor the Origins of Art, Religion, and Science . Harvard\nUniversity Press, Cambridge, MA, 2007. esp. Ch. 16,\npp. 246–265.\n[14] Daniel M ¨ullensiefen, Bruno Gingras, Jason Musil, and\nLauren Stewart. The musicality of non-musicians: An\nindex for assessing musical sophistication in the gen-\neral population. PloS one , 9(2):e89642, 2014.\n[15] Dirk M ¨urbe, Friedemann Pabst, Gert Hofmann, and\nJohan Sundberg. Signiﬁcance of Auditory and Kines-\nthetic Feedback to Singers’ Pitch Control. Journal of\nVoice , 16(1):44–51, 2002.[16] Peter Q Pfordresher and Steven Brown. Poor-pitch\nSinging in The Absence of “Tone Deafness”. Music\nPerception: An Interdisciplinary Journal , 25(2):95–\n115, 2007.\n[17] Peter Q Pfordresher, Steven Brown, Kimberly M\nMeier, Michel Belyk, and Mario Liotti. Imprecise\nSinging is Widespread. The Journal of the Acoustical\nSociety of America , 128(4):2182–2190, 2010.\n[18] John Potter, editor. The Cambridge Companion to\nSinging . Cambridge University Press, 2000.\n[19] E. Prout. Harmony: Its Theory and Practice . Cam-\nbridge University Press, 2011.\n[20] R. Seaton, D. Pim, and D. Sharp. Pitch Drift in a\nCappella Choral Singing-Work in Progress Report. In-\nstitute of Acoustics Annual Spring Conference 2013 ,\n2013.\n[21] Johan Sundberg, Filipa M B L ˜a, and Evangelos Hi-\nmonides. Intonation and Expressivity: A Single Case\nStudy of Classical Western Singing. Journal of Voice ,\n27(3):391.e1–e8, 2013.\n[22] Annie H Takeuchi and Stewart H Hulse. Absolute\npitch. Psychological Bulletin , 113(2):345–361, 1993.\n[23] Hiroko Terasawa. Pitch drift in choral music, 2004.\nMusic 221A ﬁnal paper, Center for Computer Research\nin Music and Acoustics, Stanford University, CA.\n[24] Sten Ternstr ¨om and Johan Sundberg. Intonation Preci-\nsion of Choir Singers. The Journal of the Acoustical\nSociety of America , 84(1):59–69, 1988.\n[25] Richard A Warren and Meagan E Curtis. The Actual\nvs. Predicted Effects of Intonation Accuracy on V ocal\nPerformance Quality. Music Perception: An Interdisci-\nplinary Journal , 33(2):135–146, 2015.\n[26] Graham F Welch. Singing as communication. Musical\nCommunication , pages 239–259, 2005.\n[27] Graham F Welch, Desmond C Sergeant, and Peta J\nWhite. Age, Sex, and V ocal Task as Factors In Singing\n“in Tune” During The First Years of Schooling. Bul-\nletin of the Council for Research in Music Education ,\npages 153–160, 1997.\n[28] Vivienne M Young and Andrew M Colman. Some Psy-\nchological Processes in String Quartets. Psychology of\nMusic , 7(1):12–18, 1979.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 605"
    },
    {
        "title": "FMA: A Dataset for Music Analysis.",
        "author": [
            "Michaël Defferrard",
            "Kirell Benzi",
            "Pierre Vandergheynst",
            "Xavier Bresson"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1283344",
        "url": "https://doi.org/10.5281/zenodo.1283344",
        "ee": "https://ismir2017.smcnus.org/wp-content/uploads/2017/10/75_Paper.pdf",
        "abstract": "Karaosmanoğlu and Bozkurt have studied the problem of usul and makam driven automatic melodic segmentation for Turkish Music.\n\nThere are 899 SymbTr-scores. The scores were manually annotated into melodic segments by 3 experts. In total, there are 31362 phrase annotations in this dataset.\n\nUsing this dataset\n\nPlease refer to the following paper if you use the SymbTr data:\n\n\nB. Bozkurt, M. K. Karaosmanoğlu, B. Karaalı, and E. &Uuml;nal. 2014. Usul and Makam Driven Automatic Melodic Segmentation for Turkish Music. Journal of New Music Research, 43:4, pp. 375-389.\n\n\nhttps://doi.org/10.1080/09298215.2014.924535\n\nPlease refer to the following paper if you use the Matlab code:\n\n\nM. K. Karaosmanoglu, B. Bozkurt, A. Holzapfel, N. D. Disiacik, A symbolic dataset of Turkish makam music phrases, Folk Music Analysis Workshop (FMA), Istanbul, 2014.\n\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\n\n\nhttp://compmusic.upf.edu/node/236",
        "zenodo_id": 1283344,
        "dblp_key": "conf/ismir/DefferrardBVB17",
        "keywords": [
            "Automatic Melodic Segmentation",
            "Turkish Music",
            "SymbTr-scores",
            "Manual Annotations",
            "3 Experts",
            "31362 Phrase Annotations",
            "Journal of New Music Research",
            "Usul and Makam Driven",
            "Matlab Code",
            "Folk Music Analysis Workshop"
        ]
    },
    {
        "title": "Large Vocabulary Automatic Chord Estimation with an Even Chance Training Scheme.",
        "author": [
            "Jun-qi Deng",
            "Yu-Kwong Kwok"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417421",
        "url": "https://doi.org/10.5281/zenodo.1417421",
        "ee": "https://zenodo.org/records/1417421/files/DengK17.pdf",
        "abstract": "This paper presents a large vocabulary automatic chord es- timation system implemented using a bidirectional long short-term memory recurrent neural network trained with a skewed-class-aware scheme. This scheme gives the un- common chord types much more exposure during the train- ing process. The evaluation results indicate that: compared with a normal training scheme, the proposed scheme can boost the weighted chord symbol recalls of some uncom- mon chords and significantly improve the average chord quality accuracy, at the expense of the overall weighted chord symbol recall.",
        "zenodo_id": 1417421,
        "dblp_key": "conf/ismir/DengK17",
        "keywords": [
            "automatic chord estimation",
            "bidirectional long short-term memory",
            "recurrent neural network",
            "skewed-class-aware scheme",
            "uncommon chord types",
            "training process",
            "weighted chord symbol recalls",
            "average chord quality accuracy",
            "overall weighted chord symbol recall",
            "evaluation results"
        ],
        "content": "LARGE VOCABULARY AUTOMATIC CHORD ESTIMATION WITH AN\nEVEN CHANCE TRAINING SCHEME\nJunqi Deng and Yu-Kwong Kwok\nDepartment of Electrical and Electronic Engineering\nThe University of Hong Kong\nfjqdeng,ykwokg@eee.hku.hk\nABSTRACT\nThis paper presents a large vocabulary automatic chord es-\ntimation system implemented using a bidirectional long\nshort-term memory recurrent neural network trained with\na skewed-class-aware scheme. This scheme gives the un-\ncommon chord types much more exposure during the train-\ning process. The evaluation results indicate that: compared\nwith a normal training scheme, the proposed scheme can\nboost the weighted chord symbol recalls of some uncom-\nmon chords and signiﬁcantly improve the average chord\nquality accuracy, at the expense of the overall weighted\nchord symbol recall.\n1. INTRODUCTION\nAutomatic chord estimation (ACE) is one of the central\nproblems in music informatics. It asks for an algorithm\nto extract the harmonic progression within a piece of tonal\nmusic and label each harmony region with a chord sym-\nbol and a time stamp. For any artiﬁcial intelligence that\nis able to perform music analysis, an ACE algorithm will\ndeﬁnitely be an important part of it.\nFor around two decades, ACE researches have been fo-\ncusing around a very small vocabulary such as major and\nminor (ormajmin ) [1, 7, 16, 18, 22, 25, 30, 31]. Larger vo-\ncabularies are mostly only considered in some early works\n[12, 19, 26, 29]. Until recently, the large vocabulary issue\nhas been brought back to the ﬁeld [6, 9, 20], but except for\nthe bass-treble chromagram proposed by Mauch and Dixon\n[21], the pre-segmented large vocabulary chord classiﬁca-\ntion proposed by Deng and Kwok [8], and the Bayesian\nscaled likelihood estimation proposed by Humphrey [15],\nthere is no technique specially designed for large vocabu-\nlary automatic chord estimation (LV ACE).\nRecently there has been a trend of using deep neural\nnets to solve ACE problems. Notable examples are: a con-\nvolutional neural network (CNN) based system [16], a hy-\nbrid fully connected neural network (FCNN) + recurrent\nneural network (RNN) system [3], a hybrid deep belief\nc\rJunqi Deng and Yu-Kwong Kwok. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Junqi Deng and Yu-Kwong Kwok. “Large V ocabulary\nAutomatic Chord Estimation with an Even Chance Training Scheme”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.network (DBN) + RNN system [27], and a hybrid DBN\n+ hidden Markov model (HMM) system [33]. They all\nshow promising results comparable with or better than the\nstate-of-the-art in terms of metrics that are based on ma-\njorandminor triads. While last year there is a hybrid\nDBN + Gaussian-mixture-hidden-Markov-model (GMM-\nHMM) system [9] that tries to address the LV ACE prob-\nlem, it does not really pay special attention to the uncom-\nmon chords during the training process.\nThis paper, on the other hand, proposes a scheme that\nis dedicated to the uncommon and long-tail chords in the\nlarge vocabulary. The LV ACE system is implemented\nwith a standard feature extraction process and a bidirec-\ntional long short-term memory recurrent neural network\n(BLSTM-RNN) sequence decoder. Unlike the scaled like-\nlihood estimation [15] that incorporates the prior distribu-\ntion of chords into the estimation system, our large vo-\ncabulary strategy is to make sure each chord type has an\nuniform probability of being “seen” by the network at the\nstart of each training case. This is called the “even chance”\ntraining scheme. Compared with a normal scheme that\npicks training cases at random, evaluation results show that\nthe even chance training scheme can achieve much bet-\nter uncommon weighted chord symbol recalls and signiﬁ-\ncantly better average chord quality accuracy.\nThis paper is organized as follows: Section 2 elabo-\nrates on the LV ACE system design; Section 3 describes\nthe experimental setup, which contains the details of the\nproposed even chance training scheme; Section 4 reports\nand discusses the evaluation results; and ﬁnally Section 5\nconcludes the paper with the key ﬁndings and gives some\npossible future directions of LV ACE.\n2. THE LVACE SYSTEM\nFigure 1 shows an overview of the LV ACE system,\nwhich mainly contains a feature extraction module and\na BLSTM-RNN sequence segmentation and classiﬁcation\nmodule. In the following we will ﬁrst elaborate on the\nfeature extraction process, and then discuss the working\nmechanisms of the BLSTM-RNN.\n2.1 Feature Extraction\nThe feature extraction process resembles the one described\nby Deng and Kwok [9]. It starts by resampling the raw531raw audio\nBLSTM -RNN\nsegmented chord sequenceFeature Extraction\nnotegramFigure 1 . BLSTM-RNN LV ACE system overview. The\nraw audio is transformed by a feature extraction process\ninto a piece of notegram, and then decoded by a BLSTM-\nRNN into a segmented chord sequence.\naudio input at 11025 Hz, which is followed by a short-\ntime-Fourier-transform (STFT) with 4096-point Hamming\nwindow and 512-point hop size. It then proceeds to trans-\nform the linear-frequency spectrogram (2049-bin) to the\nlog-frequency spectrogram (252-bin, 3 bins per semitone\nranging from MIDI note 21 to 104) using the two cosine\ninterpolation kernels proposed by Mauch [20]. The output\nat this step is a log-spectrogram Xk;m, wherekis the index\nof frequency bins, and mis the index of time frames. The\ntotal number of frames is M, and the total number of bins\nin each spectrum is K(in this context K= 252).\nThe process then estimates the amount of deviation\nfrom standard tuning using the algorithm in [10], where\nthe amount of detuning is estimated as:\n\u000e=wrap (\u0000'\u00002\u0019=3)\n2\u0019; (1)\nwherewrap is a function wrapping its input to [\u0000\u0019;\u0019)\nand'is the phase angle at 2\u0019=3of the discrete-Fourier-\ntransform (DFT) ofP\nmXk;m=M. The tuning frequency\n\u001cis then given by:\n\u001c= 440\u00012\u000e=12; (2)\nand the original tuning is thus updated by interpolating the\noriginal spectrogram Xk;\u0001atXk+p;\u0001, where:\np= (log(\u001c=440)=log(2))\u000236; (3)\nsince there are 36 bins per octave (3 bins per semitone)\ninXk;\u0001. The interpolation results will update the origi-\nnalXk;m, and the new Xk;mspectrogram will be referred\nto as “notegram”, which will be the input feature of the\nBLSTM-RNN sequence decoder.\n2.2 Recurrent Neural Network\nAn RNN is a neural network with cyclical connections, so\nthat the network can be recurrently unrolled into multiple\nframes [11, 17]. It can be used to model the conditional\nprobability of an output sequence Y(Y1;Y2;:::)given an\ninput sequence X(X1;X2;:::), where the superscripts de-\nnote time steps. With a forward hidden layer, it modelsthis relationship in a sequential manner, so that every out-\nput frameYtis conditioned on not only the current input\nframeXtbut also all previous input frames X1:t. Besides,\nif a backward hidden layer is added to the model, Ytwill\nbe conditioned on the whole input sequence X. This mod-\niﬁed network is called bidirectional recurrent neural net-\nwork (BRNN).\nWhen the training sequence is long, the learning signal\nmay die down gradually via the back-propagation-through-\ntime (BPTT) [24]. This gradient vanishing phenomenon\n[2] often makes the training ineffective or unsuccessful.\nUsing LSTM [13] units instead of normal non-linearities\nwithin a (B)RNN is a useful way to circumvent this unde-\nsirable effect.\n2.3 BLSTM-RNN Architecture\nThe proposed LV ACE system uses a BRNN with LSTM\nunits, or a BLSTM-RNN. It has a forward and a backward\nhidden layer both with 800 LSTM units. The input layer\nhas 252 real-value nodes, connected to a notegram spec-\ntrum. The output layer is a #-chord-way softmax layer. In\nthis implementation, we use a typical LSTM conﬁguration,\nthat all LSTM gates employ sigmoid activations, and that\nboth the LSTM cell and the LSTM output use hyperbolic\ntangent activations. Note that this network is different from\nthe one in [9], in that this BLSTM-RNN could take a vari-\nable length input sequence and generate multiple outputs,\nbut the other one is designed to handle a ﬁxed length of\ninput with a single softmax regression output.\n3. EXPERIMENTAL SETUP\nThis section describes the vocabulary, datasets and\ntraining-validation schemes used in the experiments.\n3.1 Vocabulary\nThe large vocabulary supported by the proposed system\nis the SeventhsBass introduced in MIREX ACE 20131. It\ncontains the “ NC” chord2, allmajandmintriads, all maj7 ,\nmin7 , and 7chords, and all of their inversions.\n3.2 Datasets and Data Augmentation\nSix datasets of 546 tracks are used during the experiments.\nThey contain both eastern and western pop/rock songs.\nThey are: 20 tracks from the Chinese pop song dataset\n(CNPop20, or C)3; 29 tracks from the JayChou dataset\n(JayChou29, or J)3; 26 tracks from the Carole King +\nQueen dataset (K) dataset4; 191 songs from the USPop\ndataset (U)5; 100 tracks from the RWC dataset (R)6; and\n180 tracks from the TheBeatles180 (B) dataset [14]. The\ncombination of datasets is notated by concatenating their\n1music information retrieval exchange: http://www.music-ir.\norg/mirex/wiki/MIREX\\_HOME\n2means “not a chord”, or “no chord”\n3http://tangkk.net/label/\n4http://isophonics.net/datasets\n5https://github.com/tmc323/Chord-Annotations\n6https://staff.aist.go.jp/m.goto/RWC-MDB/532 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017letter codes. For example, a combination of all datasets is\ndenoted as “CJKURB”.\nTo generate the training data, all raw audios are trans-\nformed to notegram representations. The original segment-\nwise ground truth annotations are upsampled to become\nframe-wise annotations with 1-to-1 mappings to the note-\ngram frames. Due to the absence of phase information in\nnotegram, all data can be transposed to 12 keys to yield 12\ntimes the original amount of data [16].\n3.3 Training and Cross-validation\nTwo different training schemes are used. The only differ-\nence between them are the way of choosing training cases\nat each iteration:\n\u000fcompletely random (CR): a random training case is\nchosen.\n\u000feven chance (EC): a training case starting with a cer-\ntain chord type is chosen, and each chord type has an\neven chance to be chosen as the start.\nThe EC training scheme is inspired by the skewed class\nsensitive training methods [5]. Considering a skewed dis-\ntribution of chords in the training set [4], a random sam-\npling scheme like CR will inevitably draw samples based\non that same distribution, which causes lack of exposure of\nuncommon chords. The EC scheme, however, gives each\nuncommon chord much more exposure during the training\nprocess. Concretely, the EC scheme is formalized as fol-\nlows in Algorithm 1:\nAlgorithm 1 EvenChanceTraining\nRequire: training data set - (X;y); number of chord\nclasses -nclass ; early stopping ﬂag - es.\nod= BalancedOrderedDict( y,nclass )\niter =0\nwhile not early-stopping do\nifmod(iter,nclass ) is0then\ncoidx = random shufﬂe( 0:nclass -1)\ntclist =od(coidxmod (iter;nclass ))\ndraw a random item efromtclist\nupdate network with (X;y)e\niter++\nThe core of this procedure is the “ BalancedOrdered-\nDict” which generates a dictionary of (track index, chord\nchange position) tuples indexed by chord classes. It is for-\nmalized in Algorithm 2, where each entry of odcontains a\nlist of (track index, chord change position) tuples.\nIt should be pointed out that, besides chord classiﬁca-\ntion, the BLSTM-RNN has to also perform segmentation,\nwhich means the training samples have to contain chord\nsegmentation boundaries for the network to learn from. As\na result, we set the length of each training case to be 500\nframes, which contains multiple chords. Because of this,\nthere is still uneven distribution of common and uncom-\nmon chords during the training process. The EC schemeAlgorithm 2 BalancedOrderedDict\nRequire: labels of training data set - y; number of chord\nclasses -nclass .\nforeach classifrom 0tonclass\u00001do\ninitialize an empty list od[i]\nforeach track index jinydo\nforeach frame poistion kiny[j]do\nifkis a chord change position then\nappend (j,k) tood[y[j][k]]\nreturnod\ncan guarantee a uniform chord distribution at the start of\neach training case, but it does not try to alter the sampling\nof the other chords. In effect, it only boosts the exposure of\nuncommon chords to a certain level, but could not make the\nchances of common and uncommon chords totally even.\nThe following describes the remaining training proce-\ndures that apply throughout the experiments. We try to\nreport the precise settings of every parameter so that the\nreaders may reproduce the results:\n\u000fEach training case contains 500 frames of audio con-\ntent with ground truth labels;\n\u000fThe network update signal is computed by an\nAdadelta optimizer [32];\n\u000fThe training is regularized with dropout [28] and\nearly-stopping [23];\n\u000fAll dropout probabilities are set to 0.5;\n\u000fAll early-stopping criteria are monitored using the\nvalidation error of the CNPop20 dataset, which is\nnot in any cross-validation set; The validation cycle\nis 100 iterations;\n\u000fThe model with the lowest validation loss will be\nsaved; If the current validation loss is smaller than\n0.996 of the best one, the early-stopping patience\nwill increase by 0.3 times the current number of it-\nerations;\n\u000fTraining stops when the early-stopping patience is\nless than the current number of iterations.\nFor evaluation, ﬁve-fold cross-validation (CV) is per-\nformed throughout all experiments. Each fold is a combi-\nnation of approximately 1/5 tracks of each dataset. Every\nmodel is trained on four folds and cross-validated on the re-\nmaining fold, resulting in a total number of ﬁve validation\nscores, the average of which will be the ﬁnal scores to be\nreported in Section 4. For this research to be reproducible,\nall implementation details are made available online7.\n4. RESULTS AND DISCUSSIONS\nThroughout this section, we use the MIREX ACE stan-\ndard evaluation metric, “weighted chord symbol recall”\n7https://github.com/tangkk/tangkk-mirex-aceProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 533(WCSR ), to report system performances. The “chord sym-\nbol recall” ( CSR) is deﬁned as follows:\nCSR =jS\\S\u0003j\njS\u0003j; (4)\nwhereSandS\u0003represents the automatic estimated seg-\nments, and ground truth annotated segments, respectively,\nand the intersection of SandS\u0003is the part where they\noverlap and have equal chord annotations. WCSR is the\nweighted average of all tracks’ CSRs by the lengths of\nthese tracks:\nWCSR =PLength (Tracki)\u0003CSRiPLength (Tracki); (5)\nwhere the subscript idenotes theithtrack. Likewise, the\nWCSR of a speciﬁc chord type is:\nWCSRC=PLength (Ci)\u0003CSRiPLength (Ci); (6)\nwhere the subscript idenotes the ithinstance of chord C\nwithin the data set.\nTo measure the balanced performance of a system, we\nreport “average chord quality accuracy” ( ACQA ) [6]:\nACQA =PWCSRC\n#ofchords: (7)\nwhich sums up the WCSR s of all chord types in the vocab-\nulary. Systems that over-ﬁt a few chord types or neglect\nuncommon chords tend to get lower ACQA s, while those\nwell balanced systems will have higher ACQA s.\nThe original scores in this section are computed using\nthe MusOOEvaluator8.\n4.1 Sevenths, Inversions and ACQA\nTable 1 shows the comparison between CR and EC train-\ning schemes on some uncommon ( non-majmin ) [4] chords’\nWCSR s as well as the ACQA . The six chord types in the ta-\nble are chosen because they have relatively more weights\nin pop/rock songs than the more long-tail ones such as\nmin/5 andmin/b3 . Note that maj/5 andmaj/3 are also in-\ncluded in two other large vocabularies proposed by Mauch\n[20] and Cho [6].\nmaj7 7 min7 maj/5 maj/3 7/b7 ACQA\nCR 7.3 6.6 24.1 4.5 24.5 0.0 10.8\nEC 14.6 9.9 30.9 12.0 32.4 7.8 13.2\nTable 1 . Comparison between CR and EC: seventh chords,\ninversions and ACQA scores; Dataset: JKURB\nThe results show that EC outscores CR in all categories,\nsome of which by very large amount such as maj/5 and\nmaj/3 . Although not all chord types’ results are shown, the\nACQA results suggest that the EC training scheme could\nlead to a much more balanced LV ACE system under a\nskewed class distribution.\n8https://github.com/jpauwels/MusOOEvaluator\n1.4 1.45 1.5 1.55 1.6\nFriedman test with Tukey HSD: on ACQACR-JKURBEC-JKURBFigure 2 . Multiple comparison test on ACQAs\nWe perform a Friedman test on the track-wise ACQA\nresults of both systems. After that we use the Tukey HSD\n(honest signiﬁcant difference) to perform a multiple com-\nparison test on the Friedman test’s statistics with a signiﬁ-\ncance level of 0.05. The results as shown in Figure 2 con-\nﬁrm that EC is signiﬁcantly better than CR in ACQA .\n4.2 Major, Minor and WCSR\nThe EC trained system has a more balanced performance\nthan the CR’s, however, it scariﬁes common chords’\nWCSR s. Table 2 shows the comparison between CR and\nEC on some common ( majmin ) [4] chords’ WCSR s as well\nas on the overall SeventhsBass WCSR .\nmaj min WCSR\nCR 74.2 52.2 52.0\nEC 67.8 51.4 50.6\nTable 2 . Comparison between CR and EC: major, minor\nand WCSR scores; Dataset: JKURB\nAlthough the two schemes have very close scores on\nmin, there is a large difference in maj. Due to the domi-\nnantly large weight of majchords in the JKURB dataset\ncombination, it eventually leads to CR’s much higher\nWCSR , despite EC performs better in most of the other\nchord types. CR’s much higher maj WCSR is not unex-\npected: since it draws each training case at random, the\nprobability that each chord type gets “seen” by the neu-\nral net is subject to the distribution of chord types in the\ntraining dataset, and therefore the majchords are “learned”\nmuch more than the other chords.\n1.351.41.451.51.551.61.65\nFriedman test with Tukey HSD: on WCSRCR-JKURBEC-JKURB\nFigure 3 . Multiple comparison test on WCSRs534 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017We perform a Friedman test on the track-wise WCSR re-\nsults of both systems. After that we use the Tukey HSD to\nperform a multiple comparison test on the Friedman test’s\nstatistics with a signiﬁcance level of 0.05. The results as\nshown in Figure 3 conﬁrm that CR is signiﬁcantly better\nthan EC in WCSR .\n4.3 On Different Datasets\nFor more convincing comparison results, the same exper-\niment is run 4 times using different dataset combinations.\nTable 3 shows the results of JK, JKU, JKUR and JKURB.\nWe only report the WCSR andACQA for brevity.\nCR-WCSR EC-WCSR CR-ACQA EC-ACQA\nJK 46.4 46.4 13.5 15.5\nJKU 50.4 49.1 11.2 13.5\nJKUR 50.1 49.6 12.8 14.5\nJKURB 52.0 50.6 10.8 13.2\nTable 3 . Comparison between CR and EC: WCSR and\nACQA on different datasets.\nIn all these experiments, the EC systems get higher\nACQA s, but lower or equal WCSR s, than the CR systems.\nIt is sufﬁcient to say that EC is better at training a bal-\nanced performing LV ACE system under skewed class dis-\ntribution, while CR is better at training an LV ACE system\nwith higher overall performance.\nFor both training schemes, the increment of training\ndata will lead to the increase of WCSR , but the same thing\ndoes not happen in ACQA . Assuming that every dataset\ncontains a certain amount of noise (i.e., mis-labeled or mis-\nsegmented chord regions), this observation could be tenta-\ntively explained as follows. WCSR is mostly relying on the\nquality of majmin chord labels, which are on average eas-\nier to be labeled. Therefore the increment of data will also\nincrease the WCSR score. ACQA , however, is mostly rely-\ning on the quality of non-majmin chord labels, which are\non average more difﬁcult to be labeled. Therefore the in-\ncrement of data could not guarantee the increase of ACQA\nscore, since it is hard to guarantee the proportion of non-\nmajmin noise in the incremental data is smaller than those\nof the original data.\n5. CONCLUSIONS\nThis paper presents a BLSTM-RNN based LV ACE sys-\ntem, trained using a skewed class oriented “even chance”\nscheme. This scheme is compared with a more intuitive\n“completely random” scheme that chooses training case\nrandomly at each iteration. Evaluation results demonstrate\nthat the EC training scheme is superior in both the uncom-\nmon ( non-majmin ) chords’ WCSR s and the ACQA , at the\nexpense of the common ( majmin ) chords’ WCSR s and the\noverall WCSR .\nA successful LV ACE system is marked by both high\nWCSR and high ACQA , because human chord recognition\nexperts are able to achieve both of them. The EC training\nscheme is a technique to improve a system’s ACQA , thusit is a valuable approach to consider when we design an\nLV ACE system in the future.\nThe fundamental driving force of LV ACE research\nshould be the ground-truth data and their qualities, espe-\ncially the qualities of the uncommon or long-tail chords.\nAs we see in the discussion above, ACQA is very vulner-\nable to uncommon chords’ quality. Therefore, it might\nbe possible that in the future as we gradually increase the\namount of ground-truth data, we could use ACQA in a way\nto perform sanity check on the quality of the incremental\ndata.\n6. REFERENCES\n[1] Juan Pablo Bello and Jeremy Pickens. A robust mid-\nlevel representation for harmonic content in music sig-\nnals. In Proceedings of the 6th International Society for\nMusic Information Retrieval Conference, ISMIR , vol-\nume 5, pages 304–311, 2005.\n[2] Yoshua Bengio. Learning deep architectures for AI.\nFoundations and trends R\rin Machine Learning ,\n2(1):1–127, 2009.\n[3] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Audio chord recognition with recurrent\nneural networks. In Proceedings of the 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ISMIR , pages 335–340, 2013.\n[4] John Ashley Burgoyne, Jonathan Wild, and Ichiro Fuji-\nnaga. An expert ground truth set for audio chord recog-\nnition and music analysis. In Proceedings of the 12th\nInternational Society for Music Information Retrieval\nConference, ISMIR , volume 11, pages 633–638, 2011.\n[5] Nitesh V Chawla, Nathalie Japkowicz, and Aleksander\nKotcz. Editorial: special issue on learning from imbal-\nanced data sets. ACM Sigkdd Explorations Newsletter ,\n6(1):1–6, 2004.\n[6] Taemin Cho. Improved techniques for automatic chord\nrecognition from music audio signals . PhD thesis, New\nYork University, 2014.\n[7] Taemin Cho, Ron J Weiss, and Juan Pablo Bello. Ex-\nploring common variations in state of the art chord\nrecognition systems. In Proceedings of the Sound and\nMusic Computing Conference (SMC) , pages 1–8, 2010.\n[8] Junqi Deng and Yu-Kwong Kwok. Automatic chord\nestimation on SeventhsBass chord vocabulary using\ndeep neural network. In Proceedings of the 41th Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2016.\n[9] Junqi Deng and Yu-Kwong Kwok. A hybrid gaussian-\nHMM-deep-learning approach for automatic chord es-\ntimation with very large vocabulary. In Proceedings of\nthe 17th International Society for Music Information\nRetrieval Conference, ISMIR , 2016.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 535[10] Karin Dressler and Sebastian Streich. Tuning fre-\nquency estimation using circular statistics. In Proceed-\nings of the 8th International Society for Music Infor-\nmation Retrieval Conference, ISMIR 2007 , pages 357–\n360, 2007.\n[11] Jeffrey L Elman. Finding structure in time. Cognitive\nscience , 14(2):179–211, 1990.\n[12] Takuya Fujishima. Realtime chord recognition of mu-\nsical sound: A system using common lisp music. In\nProceedings of the 25th International Computer Music\nConference , volume 1999, pages 464–467, 1999.\n[13] Alex Graves. Supervised sequence labelling. 2012.\n[14] Christopher Harte. Towards automatic extraction of\nharmony information from music signals . PhD thesis,\nDepartment of Electronic Engineering, Queen Mary,\nUniversity of London, 2010.\n[15] Eric Humphrey. An Exploration of Deep Learning in\nContent-based Music Informatics. PhD thesis, New\nYork University, 2015.\n[16] Eric J Humphrey and Juan P Bello. Rethinking au-\ntomatic chord recognition with convolutional neural\nnetworks. In Proceedings of the 11th International\nConference on Machine Learning and Applications\n(ICMLA) , volume 2, pages 357–362. IEEE, 2012.\n[17] Michael I Jordan. Attractor dynamics and parallellism\nin a connectionist sequential machine. In Proceedings\nof the Eighth Annual Meeting of the Cognitive Sci-\nence Society , pages 531–546. Lawrence Erlbaum As-\nsociates, 1986.\n[18] Maksim Khadkevich and Maurizio Omologo. Use of\nhidden Markov models and factored language models\nfor automatic chord recognition. In Proceedings of the\n10th International Society for Music Information Re-\ntrieval Conference, ISMIR 2009 , pages 561–566, 2009.\n[19] Namunu C Maddage, Changsheng Xu, Mohan S\nKankanhalli, and Xi Shao. Content-based music struc-\nture analysis with applications to music semantics un-\nderstanding. In Proceedings of the 12th annual ACM\ninternational conference on Multimedia , pages 112–\n119. ACM, 2004.\n[20] Matthias Mauch. Automatic chord transcription from\naudio using computational models of musical con-\ntext. PhD thesis, School of Electronic Engineering and\nComputer Science Queen Mary, University of London,\n2010.\n[21] Matthias Mauch and Simon Dixon. Approximate note\ntranscription for the improved identiﬁcation of difﬁcult\nchords. In Proceedings of the 11th International So-\nciety for Music Information Retrieval Conference, IS-\nMIR, pages 135–140, 2010.[22] Yizhao Ni, Matt McVicar, Raul Santos-Rodriguez, and\nTijl De Bie. An end-to-end machine learning system\nfor harmonic analysis of music. IEEE Transactions on\nAudio, Speech, and Language Processing , 20(6):1771–\n1783, 2012.\n[23] Lutz Prechelt. Early stopping-but when? In Neural\nNetworks: Tricks of the trade , pages 55–69. Springer,\n1998.\n[24] David E Rumelhart, James L McClelland, PDP Re-\nsearch Group, et al. Parallel distributed processing ,\nvolume 1. IEEE, 1988.\n[25] Matti P Ryyn ¨anen and Anssi P Klapuri. Automatic\ntranscription of melody, bass line, and chords in poly-\nphonic music. Computer Music Journal , 32(3):72–86,\n2008.\n[26] Alexander Sheh and Daniel PW Ellis. Chord segmenta-\ntion and recognition using EM-trained hidden Markov\nmodels. In Proceedings of the 4th International Society\nfor Music Information Retrieval Conference, ISMIR ,\npages 185–191. International Symposium on Music In-\nformation Retrieval, 2003.\n[27] Siddharth Sigtia, Nicolas Boulanger-Lewandowski,\nand Simon Dixon. Audio chord recognition with a hy-\nbrid recurrent neural network. In Proceedings of the\n16th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2015.\n[28] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout:\na simple way to prevent neural networks from\noverﬁtting. Journal of Machine Learning Research ,\n15(1):1929–1958, 2014.\n[29] Borching Su and Shyh-Kang Jeng. Multi-timbre\nchord classiﬁcation using wavelet transform and self-\norganized map neural networks. In Proceedings IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) . Citeseer, 2001.\n[30] Jan Weil and Jean-Louis Durrieu. An HMM-based\naudio chord detection system: Attenuating the main\nmelody. The Music Information Retrieval Exchange ,\n2008.\n[31] Adrian Weller, Daniel Ellis, and Tony Jebara. Struc-\ntured prediction models for chord transcription of mu-\nsic audio. In International Conference on Machine\nLearning and Applications, ICMLA , pages 590–595.\nIEEE, 2009.\n[32] Matthew D Zeiler. ADADELTA: an adaptive learning\nrate method. arXiv preprint arXiv:1212.5701 , 2012.\n[33] Xinquan Zhou and Alexander Lerch. Chored detection\nusing deep learning. In Proceedings of the 16th Inter-\nnational Society for Music Information Retrieval Con-\nference, ISMIR , volume 53, 2015.536 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Learning Audio-Sheet Music Correspondences for Score Identification and Offline Alignment.",
        "author": [
            "Matthias Dorfer",
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417807",
        "url": "https://doi.org/10.5281/zenodo.1417807",
        "ee": "https://zenodo.org/records/1417807/files/DorferAW17.pdf",
        "abstract": "This work addresses the problem of matching short ex- cerpts of audio with their respective counterparts in sheet music images. We show how to employ neural network- based cross-modality embedding spaces for solving the following two sheet music-related tasks: retrieving the cor- rect piece of sheet music from a database when given a mu- sic audio as a search query; and aligning an audio record- ing of a piece with the corresponding images of sheet mu- sic. We demonstrate the feasibility of this in experiments on classical piano music by five different composers (Bach, Haydn, Mozart, Beethoven and Chopin), and additionally provide a discussion on why we expect multi-modal neural networks to be a fruitful paradigm for dealing with sheet music and audio at the same time.",
        "zenodo_id": 1417807,
        "dblp_key": "conf/ismir/DorferAW17",
        "keywords": [
            "audio",
            "sheet music",
            "neural network",
            "cross-modality embedding",
            "database",
            "search query",
            "aligning",
            "audio recording",
            "corresponding images",
            "classical piano music"
        ],
        "content": "LEARNING AUDIO – SHEET MUSIC CORRESPONDENCES\nFOR SCORE IDENTIFICATION AND OFFLINE ALIGNMENT\nMatthias Dorfer\u0003Andreas Arzt\u0003yGerhard Widmer\u0003y\n\u0003Department of Computational Perception, Johannes Kepler University Linz, Austria\nyThe Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Austria\nmatthias.dorfer@jku.at\nABSTRACT\nThis work addresses the problem of matching short ex-\ncerpts of audio with their respective counterparts in sheet\nmusic images. We show how to employ neural network-\nbased cross-modality embedding spaces for solving the\nfollowing two sheet music-related tasks: retrieving the cor-\nrect piece of sheet music from a database when given a mu-\nsic audio as a search query; and aligning an audio record-\ning of a piece with the corresponding images of sheet mu-\nsic. We demonstrate the feasibility of this in experiments\non classical piano music by ﬁve different composers (Bach,\nHaydn, Mozart, Beethoven and Chopin), and additionally\nprovide a discussion on why we expect multi-modal neural\nnetworks to be a fruitful paradigm for dealing with sheet\nmusic and audio at the same time.\n1. INTRODUCTION\nTraditionally, automatic methods for linking audio and\nsheet music data are based on a common mid-level rep-\nresentation that allows for comparison (i.e., computation\nof distances or similarities) of time points in the audio\nand positions in the sheet music. Examples of mid-level\nrepresentations are symbolic descriptions, which involve\nthe error-prone steps of automatic music transcription on\nthe audio side [2, 4, 12, 20] and optical music recognition\n(OMR) on the sheet music side [3, 9, 19, 24], or spectral\nfeatures like pitch class proﬁles (chroma features), which\navoid the explicit audio transcription step but still depend\non variants of OMR. For examples of the latter approach\nsee, e.g., [8, 11, 15].\nIn this paper we present a methodology to directly learn\ncorrespondences between complex audio data and images\nof the sheet music, circumventing the problematic deﬁ-\nnition of a mid-level representation. Given short snip-\npets of audio and their respective sheet music images, a\ncross-modal neural network is trained to learn an embed-\nding space in which both modalities are represented as 32-\ndimensional vectors. which can then be compared, e.g., via\nc\rMatthias Dorfer, Andreas Arzt, Gerhard Widmer. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Matthias Dorfer, Andreas Arzt, Gerhard\nWidmer. “Learning Audio – Sheet Music Correspondences\nfor Score Identiﬁcation and Ofﬂine Alignment”, 18th International Soci-\nety for Music Information Retrieval Conference, Suzhou, China, 2017.their cosine distance. Essentially, the neural network re-\nplaces the complete feature computation process (on both\nsides) by learning a transformation of data from the audio\nand from the sheet music to a common vector space.\nThe idea of matching sheet music and audio with neu-\nral networks was recently proposed in [6]. The approach\npresented here goes beyond that in several respects. First,\nthe network in [6] requires both sheet music and audio\nas input at the same time to predict which location in the\nsheet image best matches the current audio excerpt. We ad-\ndress a more general scenario where both input modalities\nare required only at training time, for learning the relation\nbetween score and audio. This requires a different net-\nwork architecture that can learn two separate projections,\none for embedding the sheet music and one for embedding\nthe audio. These can then be used independently of each\nother. For example, we can ﬁrst embed a reference col-\nlection of sheet music images using the image embedding\npart of the network, then embed a query audio and search\nfor its nearest sheet music neighbours in the joint embed-\nding space. This general scenario is referred to as cross-\nmodality retrieval and supports different applications (two\nof which are demonstrated in this paper). The second as-\npect in which we go beyond [6] is the sheer complexity\nof the musical material: while [6] was restricted to simple\nmonophonic melodies, we will demonstrate the power of\nour method on real, complex pieces of classical music.\nWe demonstrate the utility of our approach via prelim-\ninary results on two real-world tasks. The ﬁrst is piece\nidentiﬁcation: given an audio rendering of a piece, the cor-\nresponding sheet music is identiﬁed via cross-modal re-\ntrieval. (We should note here that for practical reasons, in\nour experiments the audio data is synthesized from MIDI –\nsee below). The second task is audio-to-sheet-music align-\nment . Here, the trained network acts as a complex distance\nfunction for given pairs of audio and sheet music snippets,\nwhich in turn is used by a dynamic time warping algorithm\nto compute an optimal sequence alignment.\nOur main contributions, then, are (1) a methodology for\nlearning cross-modal embedding spaces for relating audio\ndata and sheet music data; (2) data augmentation strate-\ngies which allow for training the neural network for this\ncomplex task even with a limited amount of data; and (3)\nﬁrst results on two important MIR tasks, using this new\napproach.1152. Annotation of individual note heads \n3. Relate note heads and onset timesImage of Sheet Music\n1. Detect systems\nby bounding box\nFigure 1 . Work ﬂow for preparing the training data (correspondences between sheet music images and the respective music\naudio). Given the relation between the note heads in the sheet music image and their corresponding onset times in the audio\nsignal we sample audio-sheet-music pairs for training our networks. Figure 2 shows four examples of such training pairs.\n2. DESCRIPTION OF DATA\nOur approach is built around a neural network designed for\nlearning the relationship between two different data modal-\nities. The network learns its behaviour solely from the ex-\namples shown for training. As the presented data is crucial\nto make this class of models work, we dedicate this section\nto describing the underlying data as well as the necessary\npreparation steps needed to generate training examples for\noptimizing our networks.\n2.1 Sheet-Music-Audio Annotation\nAs already mentioned, we want to address two tasks: (1)\nsheet music (piece) identiﬁcation from audio queries and\n(2) ofﬂine alignment of a given audio with its correspond-\ning sheet music image. Both are multi-modal problems in-\nvolving sheet music images and audio. We therefore start\nby describing the process of producing the ground truth\nfor learning correspondences between a given score and its\nrespective audio. Figure 1 summarizes the process.\nStep one is the localization of staff systems in the sheet\nmusic images. In particular, we annotate bounding boxes\naround the individual systems. Given the bounding boxes\nwe detect the positions of the note heads within each of the\nsystems1. The next step is then to relate the note heads to\ntheir corresponding onset times in the audio.\nOnce these relations are established, we know for each\nnote head its location (in pixel coordinates) in the image,\nand its onset time in the audio. Based on this relationship\nwe cut out corresponding snippets of sheet music images\n(in our case 180\u0002200pixels) and short excerpts of au-\ndio represented by log-frequency spectrograms ( 92bins\u0002\n42frames). Figure 2 shows four examples of such sheet-\nmusic-audio correspondences; these are the pairs presented\nto our multi-modal networks for training.\n1We of course do not annotate all of the systems and note heads by\nhand but use a note head and staff detector to support this tasks (again a\nneural network trained for this purpose).\nFigure 2 . Sheet-music audio correspondences presented to\nthe network for retrieval embedding space learning.\n2.2 Composers, Sheet Music and Audio\nFor our experiments we use classical piano music by\nﬁve different composers: Mozart (14 pieces), Bach (16),\nBeethoven (5), Haydn (4) and Chopin (1). To give an\nimpression of the complexity of the music, we have, for\ninstance, Mozart piano sonatas (K.545 1st mvt., K.331\n3rd) and symphony transcriptions for piano (Symphony\n40 K.550 1st), preludes and fugues from Bach’s WTC,\nBeethoven piano sonata movements and Chopin’s Noc-\nturne Op.9 No.1. In terms of experimental setup we will\nuseonly the 13 pieces of Mozart for training, Mozart’s\nK.545 mvt.1 for validation, and all remaining pieces for\ntesting. This results in 18,432 correspondences for train-\ning, 989 for validating, and 11,821 for testing. Our sheet\nmusic is collected from Musescore2where we selected\nonly scores having a ‘realistic’ layout close to the type-\nsetting of professional publishers3. The reason for using\nMusescore for initial experiments is that along with the\nsheet music (as pdfor image ﬁles) Musescore also pro-\nvides the corresponding midi ﬁles. This allows us to syn-\nthesize the music for each piece of sheet music and to com-\n2https://musescore.com\n3This is an example of a typical score we used for the experi-\nment (Beethoven Sonata Op.2 No.1): https://musescore.com/\nclassicman/scores/55331116 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017pute the exact note onset times from the midis, and thus to\nestablish the required sheet-music audio correspondences.\nIn terms of audio preparation we compute log-\nfrequency spectrograms of the audios, with a sample rate\nof22:05kHz, a FFT window size of 2048 samples, and\na computation rate of 20 frames per second. For dimen-\nsionality reduction we apply a normalized 16-band loga-\nrithmic ﬁlterbank allowing only frequencies from 30Hzto\n16kHz , which results in 92 frequency bins.\n2.3 Data Augmentation\nTo improve the generalization ability of the resulting net-\nworks, we propose several data augmentation strategies\nspecialized to score images and audio. In machine learn-\ning,data augmentation refers to the application of (realis-\ntic) data transformations in order to synthetically increase\nthe effective size of the training set. We already emphasize\nat this point that data augmentation is a crucial component\nfor learning cross-modality representations that generalize\nto unseen music, especially when little data is available.\n200 pxl\n180 pxl\nspectrogram\nFigure 3 . Overview of image augmentation strategies.\nThe size of the sliding image window remains constant\n(180\u0002200pixels) but its content changes depending on\nthe augmentations applied. The spectrogram remains the\nsame for the augmented image versions.\nForsheet image augmentation we apply three differ-\nent transformations, summarized in Figure 3. The ﬁrst is\nimage scaling where we resize the image between 95and\n105% of its original size. This should make the model ro-\nbust to changes in the overall dimension of the scores. Sec-\nondly, in \u0001ysystem translation we slightly shift the system\nin the vertical direction by \u0001y2[\u00005;5]pixels. We do this\nas the system detector will not detect each system in ex-\nactly the same way and we want our model to be invariant\nto such translations. In particular, it should not be the abso-\nlute location of a note head in the image that determines its\nmeaning (pitch) but its relative position with respect to the\nstaff. Finally, we apply \u0001xnote translation , meaning that\nwe slightly shift the corresponding sheet image window by\n\u0001x2[\u00005;5]pixels in the horizontal direction.\nIn terms of audio augmentation we render the train-\ning pieces with three different sound fonts and addition-\nally vary the tempo between 100 and 130 beats per minute\n(bpm). The test pieces are all rendered at a rate of 120 bpm\nusing an additional unseen sound font . The test set is kept\nﬁxed to reveal the impact of the different data augmenta-\ntion strategies.3. AUDIO - SHEET MUSIC CORRESPONDENCE\nLEARNING\nThis section describes the underlying learning methodol-\nogy. As mentioned above, the core of our approach is a\ncross-modality retrieval neural network capable of learning\nrelations between short snippets of audio and sheet music\nimages. In particular, we aim at learning a joint embedding\nspace of the two modalities in which to perform nearest-\nneighbour search. One method for learning such a space,\nwhich has already proven to be effective in other domains\nsuch as text-to-image retrieval, is based on the optimiza-\ntion of a pairwise ranking loss [14, 22]. Before explaining\nthis optimization target, we ﬁrst introduce the general ar-\nchitecture of our correspondence learning network.\nEmbedding LayerRanking Loss\nView 1\n View 2\nFigure 4 . Architecture of correspondence learning net-\nwork. The network is trained to optimize the similarity (in\nembedding space) between corresponding audio and sheet\nimage snippets by minimizing a pair-wise ranking loss.\nAs shown in Figure 4 the network consists of two sepa-\nrate pathways fandgtaking two inputs at the same time.\nInput one is a sheet image snippet iand input two is an\naudio excerpt a. This means in particular that network fis\nresponsible for processing the image part of an input pair\nand network gis responsible for processing the audio. The\noutput of both networks (represented by the Embedding\nLayer in Figure 4) is a k-dimensional vector representa-\ntion encoding the respective inputs. In our case the dimen-\nsionality of this representation is 32. We denote these hid-\nden representations by x=f(i;\u0002f)for the sheet image\nandy=g(a;\u0002g)for the audio spectrogram, respectively,\nwhere \u0002fand\u0002gare the parameters of the two networks.\nGiven this network design, we now explain the pairwise\nranking objective. Following [14] we ﬁrst introduce a scor-\ning function s(x;y)as the cosine similarity x\u0001ybetween\nthe two hidden representations ( xandyare scaled to have\nunit norm). Based on this scoring function we optimize the\nfollowing pairwise ranking objective (‘hinge loss’):\nLrank =X\nxX\nkmaxf0;\u000b\u0000s(x;y) +s(x;yk)g(1)\nIn our application xis an embedded sample of a sheet im-\nage snippet, yis the embedding of the matching audio ex-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 117cerpt and ykare the embeddings of the contrastive (mis-\nmatching) audio excerpts (in practice all remaining sam-\nples of the current training batch). The intuition behind this\nloss function is to encourage an embedding space where\nthe distance between matching samples is lower than the\ndistance between mismatching samples. If this condition\nis roughly satisﬁed, we can then perform cross-modality\nretrieval by simple nearest neighbour search in the embed-\nding space. This will be explained in detail in Section 4.\nThe network itself is implemented as a VGG- style con-\nvolution network [21] consisting of 3\u00023convolutions fol-\nlowed by 2\u00022max-pooling as outlined in detail in Table 1.\nThe ﬁnal convolution layer computes 32 feature maps and\nis subsequently processed with a global average pooling\nlayer [16] that produces a 32-dimensional vector for each\ninput image and spectrogram, respectively. This is exactly\nthe dimension of our retrieval embedding space. At the top\nof the network we put a canonically correlated embedding\nlayer [7] combined with the ranking loss described above.\nIn terms of optimization we use the adam update rule [13]\nwith an initial learning rate of 0:002. We watch the per-\nformance of the network on the validation set and halve\nthe learning rate if there is no improvement for 30 epochs.\nThis procedure is repeated ten times to ﬁnetune the model.\nTable 1 .Audio-sheet-music model. BN: Batch Normaliza-\ntion [10], ELU: Exponential Linear Unit [5], MP: Max Pool-\ning, Conv( 3, pad-1)- 16:3\u00023convolution, 16 feature maps and\npadding 1.\nSheet-Image 180\u0002200 Audio (Spectrogram) 92\u000242\n2\u0002Conv( 3, pad-1)- 12 2\u0002Conv( 3, pad-1)- 12\nBN-ELU + MP( 2) BN-ELU + MP( 2)\n2\u0002Conv( 3, pad-1)- 24 2\u0002Conv( 3, pad-1)- 24\nBN-ELU + MP( 2) BN-ELU + MP( 2)\n2\u0002Conv( 3, pad-1)- 48 2\u0002Conv( 3, pad-1)- 48\nBN-ELU + MP( 2) BN-ELU + MP( 2)\n2\u0002Conv( 3, pad-1)- 48 2\u0002Conv( 3, pad-1)- 48\nBN-ELU + MP( 2) BN-ELU + MP( 2)\nConv( 1, pad-0)- 32-BN-LINEAR Conv( 1, pad-0)- 32-BN-LINEAR\nGlobalAveragePooling GlobalAveragePooling\nEmbedding Layer + Ranking Loss\n4. EVALUATION OF AUDIO - SHEET\nCORRESPONDENCE LEARNING\nIn this section we evaluate the ability of our model to re-\ntrieve the correct counterpart when given an instance of\nthe other modality as a search query. This ﬁrst set of ex-\nperiments is carried out on the lowest possible granularity,\nnamely, on sheet image snippets and spectrogram excerpts\nsuch as shown in Figure 2. For easier explanation we de-\nscribe the retrieval procedure from an audio query point\nof view but stress that the opposite direction works in ex-\nactly the same fashion. Given a spectrogram excerpt aas\na search query we want to retrieve the corresponding sheet\nimage snippet i. For retrieval preparation we ﬁrst embed\nall candidate image snippets ijby computing xj=f(ij)\nas the output of the image network. In the present case,\nthese candidate snippets originate from the 26 unseen test\npieces by Bach, Haydn, Beethoven and Chopin. In a sec-\nond step we embed the given query audio as y=g(a)us-\ning the audio pathway gof the network. Finally, we select\nCross-modality retrieval\nby cosine distance\nAudio\n Sheet\nquery\nresultFigure 5 . Sketch of sheet-music-from-audio retrieval. The\nblue dots represent the embedded candidate sheet music\nsnippets. The red dot is the embedding of an audio query.\nThe larger blue dot highlights the closest sheet music snip-\npet candidate selected as retrieval result.\nthe audio’s nearest neighbour xjfrom the set of embedded\nimage snippets as\nxj= arg min\nxi\u0012\n1:0\u0000xi\u0001y\njjxijjjjyjj\u0013\n(2)\nbased on their pairwise cosine distance. Figure 5 shows a\nsketch of this retrieval procedure.\nIn terms of experimental setup we use the 13 pieces of\nMozart for training the network, and the pieces of the re-\nmaining composers for testing. As evaluation measures we\ncompute the Recall@k (R@k) as well as the Median Rank\n(MR) . The R@k rate (high is better) is the percentage of\nqueries which have the correct corresponding counterpart\nin the ﬁrstkretrieval results. The MR(low is better) is the\nmedian position of the target in a cosine-similarity-ordered\nlist of available candidates.\nTable 2 summarizes the results for the different data\naugmentation strategies described in Section 2.3. The un-\nseen synthesizer and the tempo for the test set remain ﬁxed\nfor all settings. This allows us to directly investigate the\ninﬂuence of the different augmentation strategies. The re-\nsults are grouped into audio augmentation, sheet augmen-\ntation, and applying all or no data augmentation at all.\nOn ﬁrst sight the retrieval performance appears to be very\npoor. In particular the MRseems hopelessly high in view\nof our target applications. However, we must remember\nthat our query length is only 42 spectrogram frames ( \u0019\n2 seconds of audio) per excerpt and we select from a set\nof11;821available candidate snippets. And we will see\nin the following sections that this retrieval performance is\nstill sufﬁcient to perform tasks such as piece identiﬁcation.\nTaking the performance of no augmentation as a baseline\nwe observe that all data augmentation strategies help im-\nprove the retrieval performance. In terms of audio aug-\nmentation we observe that training the model with differ-\nent synthesizers and varying the tempo works best. From\nthe set of image augmentations, the \u0001ysystem translation\nhas the highest impact on retrieval performance. Overall\nwe get the best retrieval model when applying allaugmen-\ntation strategies. Note also the large gap between no aug-\nmentation andfull augmentation . The median rank, for ex-\nample, drops from 1042 in case of no augmentation to 168\nfor full augmentation, which is a substantial improvement.118 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Audio Augmentation R@1 R@10 R@25 MR\n1 Synth, 100-130bpm 0.37 3.73 7.05 771\n3 Synth, 120bpm 0.75 6.26 11.52 559\n3 Synth, 100-130bpm 0.87 8.23 15.29 332\nSheet Augmentation\nimage scaling 0.75 5.60 10.14 524\n\u0001ysystem translation 0.91 6.57 12.21 449\n\u0001xnote translation 0.44 3.66 7.19 808\nfull sheet augmentation 0.70 5.72 11.03 496\nno augmentation 0.33 2.88 5.71 1042\nfull augmentation 1.70 11.67 21.17 168\nrandom baseline 0.00 0.03 0.21 5923\nTable 2 . Inﬂuence of data augmentation on audio-to-sheet\nretrieval. For the audio augmentation experiments no sheet\naugmentation is applied and vice versa. no augmentation\nrepresents 1 Synth, 120bpm without sheet augmentation.\nA ﬁnal note: for space reasons we only present results\non audio-to-sheet music retrieval, but that the opposite di-\nrection using image snippets as search query works analo-\ngously and shows similar performance.\n5. PIECE IDENTIFICATION\nGiven the above model that learns to express similarities\nbetween sheet music snippets and audio excerpts, we now\ndescribe how to use this to solve our ﬁrst targeted task:\nidentifying the respective piece of sheet music when given\nan entire audio recording as a query (despite the relatively\npoor recall and MR for individual queries).\n5.1 Description of Approach\nWe start by preparing a sheet music retrieval database as\nfollows. Given a set of sheet music images along with their\nannotated systems we cut each piece of sheet music jinto\na set of image snippets fijiganalogously to the snippets\npresented to our network for training. For each snippet\nwe store its originating piece j. We then embed all can-\ndidate image snippets into the retrieval embedding space\nby passing them through the image part fof the multi-\nmodal network. This yields, for each image snippet, a 32-\ndimensional embedding coordinate vector xji=f(iji).\nSheet snippet retrieval from audio: Given a whole\naudio recording as a search query we aim at identifying\nthe corresponding piece of sheet music in our database. As\nwith the sheet image we start by cutting the audio (spectro-\ngram) into a set of excerpts fa1;:::;aKgagain exhibiting\nthe same dimensions as the spectrograms used for training,\nand embed all query spectrogram excerpts akwith the au-\ndio network g. Then we proceed as described in Section 4\nand select for each audio its nearest neighbour from the set\nof all embedded image snippets.Augmentation R1 R2 R3 >R3\nno augmentation 4 7 1 14\nfull augmentation 24 2 0 0\nTable 3 . Inﬂuence of data augmentation on piece retrieval.\nPiece selection: Since we know for each of the image\nsnippets its originating piece j, we can now have the re-\ntrieval image snippets xjivote for the piece. The piece\nachieving the highest count of votes is our ﬁnal retrieval\nresult. In our experiments we consider for each query ex-\ncerpt its top 25 retrieval results for piece voting.\n5.2 Evaluation of Approach\nTable 3 summarizes the piece identiﬁcation results on\nour test set of Bach, Haydn, Beethoven and Chopin (26\npieces). Again, we investigate the inﬂuence of data aug-\nmentation and observe that the trend of the experiments in\nSection 4 is directly reﬂected in the piece retrieval results.\nAs evaluation measure we compute Rkas the number of\npieces ranked at position kwhen sorting the result list by\nthe number of votes. Without data augmentation only four\nof the 26 pieces are ranked ﬁrst in the retrieval lists of\nthe respective full audio recording queries. When making\nuse of data augmentation during training, this number in-\ncreases substantially and we are able to recognize 24 pieces\nat position one; the remaining two are ranked at position\ntwo. Although this is not the most sophisticated way of\nemploying our network for piece retrieval, it clearly shows\nthe usefulness of our model and its learned audio and sheet\nmusic representations for such tasks.\n6. AUDIO-TO-SHEET-MUSIC ALIGNMENT\nAs a second usage scenario for our approach we present the\ntask of audio-to-sheet-music alignment. Here, the goal is\nto align a performance (given as an audio ﬁle) to its respec-\ntive score (as images of the sheet music), i.e., computing\nthe corresponding location in the sheet music for each time\npoint in the performance, and vice versa.\n6.1 Description of Approach\nFor computing the actual alignments we rely on Dynamic\nTime Warping (DTW), which is a standard method for se-\nquence alignment [18], and is routinely used in the con-\ntext of music processing [17]. Generally, DTW takes two\nsequences as input and computes an optimal non-linear\nalignment between them, with the help of a local cost mea-\nsure that relates points of the two sequences to each other.\nFor our task the two sequences to be aligned are the\nsequence of snippets from the sheet music image and the\nsequence of audio (spectrogram) excerpts, as described in\nSection 2.2. The neural network presented in Section 3\nis then used to derive a local cost measure by computing\nthe pairwise cosine distances between the embedded sheetProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 1190 100 200 300 400\nAudio (479)050100150200250Sheet (296)\nDistance Matrix and DTW Path\nFigure 6 . Sketch of audio-to-sheet-music alignment by\nDTW on a similarity matrix computed on the embedding\nrepresentation learned by the multi-modal matching net-\nwork. The white line highlights the path of minimum costs\nthrough the sheet music given the audio.\nsnippets and audio excerpts (see Equation 2). The result-\ning cost matrix that relates all points of both sequences to\neach other is shown in Figure 6, for a short excerpt from\na simple Bach minuet. Then, the standard DTW algorithm\nis used to obtain the optimal alignment path.\n6.2 Evaluation of Approach\nFor the evaluation we rely on the same dataset and setup as\ndescribed above: learning the embedding only on Mozart,\nthen aligning test pieces by Bach, Haydn, Beethoven,\nChopin. As evaluation measure we compute the absolute\nalignment error (distance in pixels) of the estimated align-\nment to its ground truth alignment for each of the sliding\nwindow positions. We further normalize the errors by di-\nviding them by the sheet image width to be independent of\nimage resolution. As a naive baseline we compute a lin-\near interpolation alignment which would correspond to a\nstraight line diagonal in the distance matrix in Figure 6.\nWe consider this as a valid reference as we do not consider\nrepetitions for our experiments, yet (in which case things\nwould become somewhat more complicated). We further\nemphasize that the purpose of this experiment is to provide\na proof of concept for this class of models in the context of\nsheet music alignment tasks, not to compete with existing\nspecialized algorithms for music alignment.\nThe results are summarized by the boxplots in Figure 7.\nThe median alignment error for the linear baseline is 0.213\nnormalized image widths ( \u001945mm in a printed page of\nsheet music). When computing a DTW path through the\ndistance matrix inferred by our mutimodal audio-sheet-\nmusic network this error decreases to 0.041 ( \u00199 mm).\nNote that values above 1.0 normalized page widths are pos-\nsible as we handle a piece of sheet music as one single un-\nrolled (concatenated) staff.\n0.0 0.5 1.0 1.5 2.0 2.5\n|Alignment/uni00A0Error|DTW\nLinearAlignment/uni00A0MethodFigure 7 . Absolute alignment errors normalized by the\nsheet image width. We compare the linear baseline with a\nDTW on the cross-modality distance matrix computed on\nthe embedded audio snippets and spectrogram excerpts.\n7. DISCUSSION AND CONCLUSION\nWe have presented a method for matching short excerpts\nof audio to their respective counterparts in sheet music im-\nages, via a multi-modal neural network that learns relation-\nships between the two modalities, and have shown how\nto utilize it for two MIR tasks: score identiﬁcation from\naudio queries and ofﬂine audio-to-sheet-music alignment.\nOur results provide a proof of concept for the proposed\nlearning-retrieval paradigm and lead to the following con-\nclusions: First, even though little training data is available,\nit is still possible to use powerful state of the art image and\naudio models by designing appropriate (task speciﬁc) data\naugmentation strategies. Second, as the best regularizer in\nmachine learning is still a large amount of training data,\nour results strongly suggest that annotating a truly large\ndataset will allow us to train general audio-sheet-music-\nmatching models. Recall that for this study we trained on\nonly 13 Mozart pieces, and our model already started to\ngeneralize to unseen scores by other composers.\nAnother aspect of our method is that it works by project-\ning observations from different modalities into a very low-\ndimensional joint embedding space. This compact repre-\nsentation is of particular relevance for the task of piece\nidentiﬁcation as our scoring function – the cosine distance\n– is a metric that permits efﬁcient search in large reference\ndatabases [23]. This identiﬁcation-by-retrieval approach\npermits us to circumvent solving a large number of local\nDTW problems for piece identiﬁcation as done, e.g., in [8].\nFor now, we have demonstrated the approach on sheet\nmusic of realistic complexity, but with synthesized audio\n(this was necessary to establish the ground truth). The\nnext challenge will be to deal with real audio and real per-\nformances, with challenges such as asynchronous onsets,\npedal, and varying dynamics.\nFinally, we want to stress that our claim is by no means\nthat our proposal in its current stage is competitive with\nengineered approaches [8, 11, 15] or methods relying on\nsymbolic music or reference performances. These meth-\nods have already proven to be useful in real world scenar-\nios, with real performances [1]. However, considering the\nprogress that has been made in terms of score complexity\n(compared for example to the simple monophonic music\nused in [6]) we believe it is a promising line of research.120 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20178. ACKNOWLEDGEMENTS\nThis work is supported by the Austrian Ministries BMVIT\nand BMWFW, and the Province of Upper Austria via the\nCOMET Center SCCH, and by the European Research\nCouncil (ERC Grant Agreement 670035, project CON\nESPRESSIONE). The Tesla K40 used for this research was\ndonated by the NVIDIA corporation.\n9. REFERENCES\n[1] Andreas Arzt, Harald Frostel, Thassilo Gadermaier,\nMartin Gasser, Maarten Grachten, and Gerhard Wid-\nmer. Artiﬁcial intelligence in the concertgebouw. In In-\nternational Joint Conference on Artiﬁcial Intelligence\n(IJCAI) , 2015.\n[2] Sebastian B ¨ock and Markus Schedl. Polyphonic piano\nnote transcription with recurrent neural networks. In\nProceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\npages 121–124, Kyoto, Japan, 2012.\n[3] Donald Byrd and Jakob Grue Simonsen. Towards a\nstandard testbed for optical music recognition: Deﬁni-\ntions, metrics, and page images. Journal of New Music\nResearch , 44(3):169–195, 2015.\n[4] Tian Cheng, Matthias Mauch, Emmanouil Benetos, Si-\nmon Dixon, et al. An attack/decay model for piano\ntranscription. In Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2016.\n[5] Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learn-\ning by exponential linear units (elus). Interna-\ntional Conference on Learning Representations (ICLR)\n(arXiv:1511.07289) , 2015.\n[6] Matthias Dorfer, Andreas Arzt, and Gerhard Widmer.\nTowards score following in sheet music images. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , 2016.\n[7] Matthias Dorfer, Jan Schl ¨uter, Andreu Vall, Filip Ko-\nrzeniowski, and Gerhard Widmer. End-to-end cross-\nmodality retrieval with cca projections and pairwise\nranking loss. arXiv preprint (arXiv:1705.06979) , 2017.\n[8] Christian Fremerey, Michael Clausen, Sebastian Ew-\nert, and Meinard M ¨uller. Sheet music-audio identi-\nﬁcation. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2009.\n[9] Jan Haji ˇc jr, Jirı Novotn `y, Pavel Pecina, and Jaroslav\nPokorn `y. Further steps towards a standard testbed for\noptical music recognition. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , 2016.[10] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. CoRR , abs/1502.03167, 2015.\n[11] ¨Ozg¨ur˙Izmirli and Gyanendra Sharma. Bridging\nprinted music and audio through alignment using a\nmid-level score representation. In Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2012.\n[12] Rainer Kelz, Matthias Dorfer, Filip Korzeniowski, Se-\nbastian B ¨ock, Andreas Arzt, and Gerhard Widmer. On\nthe potential of simple framewise approaches to piano\ntranscription. In Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2016.\n[13] Diederik Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. International Conference on\nLearning Representations (ICLR) (arXiv:1412.6980) ,\n2015.\n[14] Ryan Kiros, Ruslan Salakhutdinov, and Richard S\nZemel. Unifying visual-semantic embeddings with\nmultimodal neural language models. arXiv preprint\n(arXiv:1411.2539) , 2014.\n[15] Frank Kurth, Meinard M ¨uller, Christian Fremerey,\nYoon-ha Chang, and Michael Clausen. Automated\nsynchronization of scanned sheet music with audio\nrecordings. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 261–266, 2007.\n[16] Min Lin, Qiang Chen, and Shuicheng Yan. Network in\nnetwork. CoRR , abs/1312.4400, 2013.\n[17] Meinard M ¨uller. Fundamentals of Music Processing .\nSpringer Verlag, 2015.\n[18] Lawrence Rabiner and Bing-Hwang Juang. Funda-\nmentals of Speech Recognition . Prentice Hall Signal\nProcessing Series, 1993.\n[19] Ana Rebelo, Ichiro Fujinaga, Filipe Paszkiewicz, An-\ndre R. S. Marcal, Carlos Guedes, and Jaime S. Car-\ndoso. Optical music recognition: state-of-the-art and\nopen issues. International Journal of Multimedia In-\nformation Retrieval , 1(3):173–190, 2012.\n[20] S. Sigtia, E. Benetos, and S. Dixon. An end-to-end neu-\nral network for polyphonic piano music transcription.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 24(5):927–939, 2016.\n[21] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recogni-\ntion. International Conference on Learning Represen-\ntations (ICLR) (arXiv:1409.1556) , 2015.\n[22] Richard Socher, Andrej Karpathy, Quoc V Le, Christo-\npher D Manning, and Andrew Y Ng. Grounded com-\npositional semantics for ﬁnding and describing im-\nages with sentences. Transactions of the Association\nfor Computational Linguistics , 2:207–218, 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 121[23] Stijn Van Dongen and Anton J Enright. Met-\nric distances derived from cosine similarity and\npearson and spearman correlations. arXiv preprint\narXiv:1208.3145 , 2012.\n[24] Cuihong Wen, Ana Rebelo, Jing Zhang, and Jaime Car-\ndoso. A new optical music recognition system based on\ncombined neural network. Pattern Recognition Letters ,\n58:1–7, 2015.122 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Metrical-Accent Aware Vocal Onset Detection in Polyphonic Audio.",
        "author": [
            "Georgi Dzhambazov",
            "Andre Holzapfel",
            "Ajay Srinivasamurthy",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415086",
        "url": "https://doi.org/10.5281/zenodo.1415086",
        "ee": "https://zenodo.org/records/1415086/files/DzhambazovHSS17.pdf",
        "abstract": "The goal of this study is the automatic detection of onsets of the singing voice in polyphonic audio recordings. Start- ing with a hypothesis that the knowledge of the current po- sition in a metrical cycle (i.e. metrical accent) can improve the accuracy of vocal note onset detection, we propose a novel probabilistic model to jointly track beats and vocal note onsets. The proposed model extends a state of the art model for beat and meter tracking, in which a-priori prob- ability of a note at a specific metrical accent interacts with the probability of observing a vocal note onset. We carry out an evaluation on a varied collection of multi-instrument datasets from two music traditions (English popular music and Turkish makam) with different types of metrical cy- cles and singing styles. Results confirm that the proposed model reasonably improves vocal note onset detection ac- curacy compared to a baseline model that does not take metrical position into account.",
        "zenodo_id": 1415086,
        "dblp_key": "conf/ismir/DzhambazovHSS17",
        "keywords": [
            "automatic detection",
            "singing voice",
            "polyphonic audio recordings",
            "metrical cycle",
            "vocal note onset detection",
            "novel probabilistic model",
            "beat and meter tracking",
            "apriori probability",
            "vocal note onset",
            "results confirm"
        ],
        "content": "METRICAL-ACCENT AWARE VOCAL ONSET DETECTION IN\nPOLYPHONIC AUDIO\nGeorgi Dzhambazov1Andre Holzapfel2\nAjay Srinivasamurthy1Xavier Serra1\n1Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n2Media Technology and Interaction Design, KTH Royal Institute of Technology, Stockholm, Sweden\n{georgi.dzhambazov,ajays.murthy,xavier.serra }@upf.edu, holzap@kth.se\nABSTRACT\nThe goal of this study is the automatic detection of onsets\nof the singing voice in polyphonic audio recordings. Start-\ning with a hypothesis that the knowledge of the current po-\nsition in a metrical cycle (i.e. metrical accent) can improve\nthe accuracy of vocal note onset detection, we propose a\nnovel probabilistic model to jointly track beats and vocal\nnote onsets. The proposed model extends a state of the art\nmodel for beat and meter tracking, in which a-priori prob-\nability of a note at a speciﬁc metrical accent interacts with\nthe probability of observing a vocal note onset. We carry\nout an evaluation on a varied collection of multi-instrument\ndatasets from two music traditions (English popular music\nand Turkish makam) with different types of metrical cy-\ncles and singing styles. Results conﬁrm that the proposed\nmodel reasonably improves vocal note onset detection ac-\ncuracy compared to a baseline model that does not take\nmetrical position into account.\n1. INTRODUCTION\nSinging voice analysis is one of the most important topics\nin the ﬁeld of music information retrieval because singing\nvoice often forms the melody line and creates the impres-\nsion of a musical piece. The automatic transcription of\nsinging voice can be considered to be a key technology in\ncomputational studies of singing voice. It can be utilized\nfor end-user applications such as enriched music listening\nand singing education. It can as well enable other compu-\ntational tasks including singing voice separation, karaoke-\nlike singing voice suppression or lyrics-to-audio alignment\n[5].\nThe process of converting an audio recording into some\nform of musical notation is commonly known as auto-\nmatic music transcription. Current transcription methods\nuse general purpose models, which are unable to capture\nc\u0000Georgi Dzhambazov, Andre Holzapfel, Ajay Srini-\nvasamurthy, Xavier Serra. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: Georgi\nDzhambazov, Andre Holzapfel, Ajay Srinivasamurthy, Xavier Serra.\n“Metrical-accent aware vocal onset detection in polyphonic audio”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.the rich diversity found in music signals [2]. In particular,\nsinging voice poses a challenge to transcription algorithms\nbecause of its soft onsets, and phenomena such as porta-\nmento and vibrato. One of the core subtasks of singing\nvoice transcription (SVT) is detecting note events with a\ndiscrete pitch value, an onset time and an offset time from\nthe estimated time-pitch representation. Detecting the time\nlocations of vocal note onsets can beneﬁt from automati-\ncally detected events from musical facets, such as musical\nmeter. In fact, the accents in the metrical cycle determine\nto a large extent the temporal backbone of singing melody\nlines. Studies on sheet music showed that the locations\nof vocal note onsets are inﬂuenced by the their position\nin a metrical cycle [7, 10]. Despite that, there have been\nfew studies on meter aware analysis of onsets in music au-\ndio [4].\nIn this work we propose a novel probabilistic model\nthat tracks simultaneously note onsets of singing voice and\ninstrumental energy accents in a metrical cycle. We ex-\ntend a state of the art model for beat and meter tracking,\nbased on dynamic Bayesian networks (DBN). A model\nvariable is added that models the temporal segments of a\nnote and their interaction with metrical position. The pro-\nposed model is applied for the automatic detection of vocal\nnote onsets in multi-instrumental recordings with predom-\ninant singing voice. Evaluation is carried out on datasets\nfrom music traditions, for which there is a clear correlation\nbetween metrical accents and the onset times in the vocal\nline.\n2. RELATED WORK\n2.1 Singing Voice Transcription\nA probabilistic note hidden Markov model (HMM) is pre-\nsented in [18], where a note has 3 states: attack (onset),\nstable pitch state and silent state. The transition proba-\nbilities are learned from data. Recently [14] suggested to\ncompact musical knowledge into rules as a way to describe\nthe observation and transition likelihoods, instead of learn-\ning them from data. The authors suggest covering a range\nwith distinct pitch from lowest MIDI C2 up to B7. Each\nMIDI pitch is further divided into 3 sub-pitches, result-\ning in n= 207 notes with different pitch, each having702the 3 note states. Although being conceptually capable\nof tracking onsets in singing voice audio with accompa-\nniment, these approaches were tested only on a cappella\nsinging.\nIn multi-instrumental recordings, an essential ﬁrst step\nis to extract reliably the predominant vocal melody.\nThere have been few works dealing with SVT in multi-\ninstrumental recordings in general [13, 15], and with onset\ndetection, in particular [3]. Some of them [13, 15] rely on\nthe algorithm for predominant melody extraction of [19].\n2.2 Beat Detection\nRecently a Bayesian approach, referred to as the bar-\npointer model, has been presented [20]. It describes events\nin music as being driven by their current position in a met-\nrical cycle (i.e. musical bar). The model represents as hid-\nden variables in a Dynamic Bayesian network (DBN) the\ncurrent position in a bar, the tempo, and the type of musical\nmeter, which can be referred to as bar-tempo state space.\nThe work of [9] applied this model to recordings from\nnon-Western music, in order to handle jointly beat and\ndownbeat tracking. The authors showed that the original\nmodel can be adapted to different rhythmic styles and time\nsignatures, and an evaluation is presented on Indian, Cre-\ntan and Turkish music datasets.\nLater [12] suggested a modiﬁcation of the bar-tempo\nstate space, in order to reduce the computational burden\nfrom its huge size.\n3. DATASETS\n3.1 Turkish Makam\nThe Turkish dataset has two meter types, referred to as\nusuls in Turkish makam: the 9/8-usul aksak and the 8/8-\nusul d ¨uyek. It is a subset of the dataset presented in [9],\nincluding only the recordings with singing voice present.\nThe beats and downbeats were annotated by [9]. The vo-\ncal note onsets are annotated by the ﬁrst author, whereby\nonly pitched onsets are considered (2100 onsets). To this\nend, if a syllable starts with an unvoiced consonant, the\nonset is placed at the beginning of the succeeding voiced\nphoneme1.\nFor this study we divided the dataset into training and\ntest subsets. The test dataset comprises 5 1-minute ex-\ncerpts from recordings with solo singing voice only for\neach of the two usuls (on total 780 onsets). The training\ndataset spans around 7 minutes of audio from each of the\ntwo usuls. Due to the scarcity of material with solo singing\nvoice, several excerpts with choir sections were included in\nthe training data.\n3.2 English Pop\nThe datasets, on which singing voice transcription in multi-\ninstrumental music is evaluated, are very few [2]: Often a\nsubset of the RWC dataset is employed, which does not\n1The dataset is described at http://compmusic.upf.edu/\nnode/345contain diverse genres and singers [6]. To overcome this\nbias, we compiled the lakh-vocal-segments dataset: We\nselected 14 30-second audio clips of English pop songs,\nwhich have been aligned to their corresponding MIDIs in\na recent study [17]. Criteria for selecting the clips are the\npredominance of the vocal line; 4/4 meter; correlation be-\ntween the beats and the onset times. We derived the loca-\ntions of the vocal onsets (850 on total) from the aligned\nvocal MIDI channel, whereby some imprecise locations\nwere manually corrected. To encourage further studies on\nsinging voice transcription we make available the derived\nannotations2.\n4. APPROACH\nThe proposed approach extends the beat and meter track-\ning model, presented in [12]. We adopt from it the vari-\nables for the position in the metircal cycle (bar position)\n\u0000and the instantaneous tempo ˙\u0000. We also adopt the ob-\nservation model, which describes how the metrical accents\n(beats) are related to an observed onset feature vector yf.\nAll variables and their conditional dependencies are repre-\nsented as the hidden variables in a DBN (see Figure 1). We\nconsider that the a priori probability of a note at a speciﬁc\nmetrical accent interacts with the probability of observing\na vocal note onset. To represent that interaction we add a\nhidden state for the temporal segment of a vocal note n,\nwhich depends on the current position in the metrical cy-\ncle. The probability of observing a vocal onset is derived\nfrom the emitted pitch ypof the vocal melody.\nIn the proposed DBN, an observed sequence of fea-\ntures derived from an audio signal y1:K={y, .., y K}is\ngenerated by a sequence of hidden (unknown) variables\nx1:K={x1,. . . ,x K}, where K is the length of the se-\nquence (number of audio frames in an audio excerpt). The\njoint probability distribution of hidden and observed vari-\nables factorizes as:\nP(x1:K,y1:K)=P(x0)⇧K\nk=1P(xk|xk\u00001)P(yk|xk)(1)\nwhere P(x0)is the initial state distribution;\nP(xk|xk\u00001)is the transition model and P(yk|xk)is\nthe observation model.\n4.1 Hidden Variables\nAt each audio frame k, the hidden variables describe the\nstate of a hypothetical bar pointer xk=[ ˙\u0000k,\u0000k,nk], rep-\nresenting the instantaneous tempo, the bar position and the\nvocal note respectively.\n4.1.1 Tempo State ˙\u0000and Bar Position State \u0000\nThe bar position \u0000points to the current position in the met-\nrical cycle (bar). The instantaneous tempo ˙\u0000encodes how\nmany bar positions the pointer advances from the current\nto the next time instant. To assure feasible computational\ntime we relied on the combined bar-tempo efﬁcient state\nspace, presented in [12]. To keep the size of the bar-tempo\n2https://github.com/georgid/lakh_vocal_\nsegments_datasetProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 703Figure 1 : A dynamic Bayesian network for the pro-\nposed beat and vocal onset detection model. Circles and\nsquares denote continuous and discrete variables, respec-\ntively. Gray nodes and white nodes represent observed and\nhidden variables, respectively.\nstate space small, we input the ground truth tempo for each\nrecording, allowing a range for ˙\u0000within ±10bpm from\nit, in order to accommodate gradual tempo changes. This\nwas the minimal margin at which beat tracking accuracy\ndid not degrade substantially. For a study with data with\nhigher stylistic diversity, it would make sense to increase it\nto at least 20% as it is done in [8, Section 5.2]. This yields\naround 100-1000 states for the bar positions within a sin-\ngle beat (in the order of 5000 for 4 beats, and 10000 for\n8-9 beats for the usuls ).\n4.1.2 Vocal Note State n\nThe vocal note states represent the temporal segments of a\nsung note. They are a modiﬁed version of these suggested\nin the note transcription model of [14]. We adopted the\nﬁrst two segments: attack region (A), stable pitch region\n(S). We replaced the silent segment with non-vocal state\n(N). Because full-ﬂedged note transcription is outside the\nscope of this work, instead of 3 steps per semitone, we used\nfor simplicity only a single one, which deteriorated just\nslightly the note onset detection accuracy. Also, to reﬂect\nthe pitch range in the datasets, on which we evaluate, we\nset as minimal MIDI note E3 covering almost 3 octaves up\nto B5 (35 semitones). This totals to 105 note states.\nTo be able to represent the DBN as an HMM, the bar-\ntempo efﬁcient state space is combined with the note state\nspace into a joint state space x. The joint state space is a\ncartesian product of the two state spaces, resulting in up to\n10000 ⇥105⇡1M states.4.2 Transition Model\nDue to the conditional dependence relations in Figure 1 the\ntransitional model factorizes as\nP(xk|xk\u00001)= P(˙\u0000k|˙\u0000k\u00001)⇥\nP(\u0000k|\u0000k\u00001,˙\u0000k\u00001)⇥P(nk|nk\u00001,\u0000k)(2)\nThe tempo transition probability p(˙\u0000k|˙\u0000k\u00001)and bar\nposition probability p(\u0000k|\u0000k\u00001,˙\u0000k\u00001)are the same as in\n[12].Transition from one tempo to another is allowed only\nat bar positions, at which the beat changes. This is a rea-\nsonable assumption for the local tempo deviations in the\nanalyzed datasets, which can be considered to occur rela-\ntively beat-wise.\n4.2.1 Note Transition Probability\nThe probability of advancing to a next note state is based\non the transitions of the note-HMM, introduced in [14].\nLet us brieﬂy review it: From a given note segment the\nonly possibility is to progress to its following note seg-\nment. To ensure continuity each of the self-transition prob-\nabilities is rather high, given by constants cA,cSandcN\nfor A, S and N segments respectively ( cA=0.9; cS=0.99;\ncN=0.9999 ). Let PNiAjbe the probability of transition\nfrom non-vocal state Niafter note ito attack state Ajof\nits following note j.The authors assume that it depends on\nthe difference between the pitch values of notes iandjand\nit can be approximated by a normal distribution centered at\nchange of zero ( [14], Figure 1.b). This implies that small\npitch changes are more likely than larger ones. Now we\ncan formalize their note transition as:\np(nk|nk\u00001)=8\n>>>>>>>>>>><\n>>>>>>>>>>>:PNiAj,n k\u00001=Nink=Aj\ncN,n k\u00001=nk=Ni\n1\u0000cA,n k\u00001=Aink=Sj\ncA,n k\u00001=nk=Ai\n1\u0000cSnk=1=Sink=Nj\ncS,n k\u00001=nk=Si\n0 else(3)\nNote that the outbound transitions from all non-vocal\nstates Nishould sum to 1, meaning that\ncN=1\u0000X\niPNiAj (4)\nIn this study, we modify PNiAjto allow variation in\ntime, depending on the current bar position \u0000k.\np(nk|nk\u00001,\u0000k)=8\n><\n>:PNiAj⇥(\u0000k),n k\u00001=Ni,nk=Aj\ncN,n k\u00001=nk=Ni\n...\n(5)\nwhere\n⇥(\u0000k):function weighting the contribution of a beat ad-\njacent to current bar position \u0000k704 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017and\ncN=1\u0000⇥(\u0000k)X\niPNiAj (6)\nThe transition probabilities in all the rest of the cases\nremain the same. We explore two variants of the weighting\nfunction ⇥(\u0000k):\n1. Time-window redistribution weighting: Singers\noften advance or delay slightly note onsets off the loca-\ntion of a beat. The work [15] presented an idea on how to\nmodel vocal onsets, time-shifted from a beat, by stochastic\ndistribution. Similarly, we introduce a normal distribution\nN0,\u0000, centered around 0 to re-distribute the importance of\na metrical accent (beat) over a time window around it. Let\nbkbe the beat, closest in time to a current bar position \u0000k.\nNow:\n⇥(\u0000k)=[ N0,\u0000(d(\u0000k,bk))]we(bk) (7)\nwhere\ne(b):probability of a note onset co-occurring with the bth\nbeat (b 2B);Bis the number of beats in a metrical\ncycle\nw:sensitivity of vocal onset probability to beats\nd(\u0000k,bk):the distance from current bar position \u0000kto the\nposition of the closest beat bk\nEquation 5 means essentially that the original PNiAjis\nscaled according to how close in time to a beat it is.\n2. Simple weighting: We also aim at testing a more\nconservative hypothesis that it is sufﬁcient to approximate\nthe inﬂuence of metrical accents only at the locations of\nbeats. To reﬂect that, we modify the PNiAjonly at bar\npositions corresponding to beat positions, for which the\nweighting function is set to the peak of N0,\u0000, and to 1 else-\nwhere.\n⇥(\u0000k)=(\n[N0,\u0000(0)]we(bk),d(\u0000k,bk)=0\n1 else(8)\n4.3 Observation Models\nThe observation probability P(yk|xk)describes the rela-\ntion between the hidden states and the (observed) audio\nsignal. In this work we make the assumption that the ob-\nserved vocal pitch and the observed metrical accent are\nconditionally independent from each other. This assump-\ntion may not hold in cases when energy accents of singing\nvoice, which contribute to the total energy of the signal, are\ncorrelated to changes in pitch. However, for music with\npercussive instruments the importance of singing voice ac-\ncents is diminished to a signiﬁcant extent by percussive\naccents. Now we can rewrite Eq. 1 as\nP(x1:K,yf\n1:K,yp\n1:K)=\nP(x0)⇧K\nk=1P(xk|xk\u00001)P(yf\nk|xk)P(yp\nk|xk)(9)This means essentially that the observation probability can\nbe represented as the product of the observation probability\nof a metrical accent P(yf\nk|xk)and the observation proba-\nbility of vocal pitch P(yp\nk|xk).\n4.3.1 Accent Observation Model\nIn this paper for P(yf\nk|xk)we train GMMs on the spectral\nﬂux-like feature yf, extracted from the audio signal using\nthe same parameters as in [12] and [9]. The feature vector\nyfsummarizes the energy changes (accents) that are likely\nto be related to the onsets of all instruments together. This\nforms a rhythmic pattern of the accents, characteristic for\na given metrical type. The probability of observing an ac-\ncent thus depends on the position in the rhythmic pattern,\nP(yf\nk|xk)=P(yf\nk|\u0000k).\n4.3.2 Pitch Observation Model\nThe pitch probability P(yp\nk|xk)reduces to P(yp\nk|nk), be-\ncause it depends only on the current vocal note state. We\nadopt the idea proposed in [14] that a note state emits pitch\nypaccording to a normal distribution, centered around its\naverage pitch. The standard deviation of stable states and\nthe one of the onset states are kept the same as in the orig-\ninal model, respectively 0.9 and 5 semitones. The melody\ncontour of singing is extracted in a preprocessing step. We\nutilized for English pop a method for predominant melody\nextraction [19]. For Turkish makam, we instead utilized\nan algorithm, extended from [19] and tailored to Turk-\nish makam [1]. In both algorithms, each audio frame k\ngets assigned a pitch value and probability of being voiced\nvk. Based on frames with zero probabilities, one can in-\nfer which segments are vocal and which not. Since cor-\nrect vocal segments is crucial for the sake of this study\nand the voicing estimation of these melody extraction al-\ngorithms are not state of the art, we manually annotated\nsegments with singing voice, and thus assigned vk=0 for\nall frames, annotated as non-vocal.\nFor each state the observation probability P(yp\nk|nk)of\nvocal states is normalized to sum to vk(unlike the original\nmodel which sums to a global constant v). This leaves the\nprobability for each non-vocal state be 1\u0000vk/n.\n4.4 Learning Model Parameters\n4.4.1 Accent Observation Model\nWe trained the metrical accent probability P(yf\nk|\u0000k)sepa-\nrately for each meter type: The Turkish meters are trained\non the training subset of the makam dataset (see section\n3.1). For each usul (8/8 and 9/8) we trained a rhythmic\npattern by ﬁtting a 2-mixture GMM on the extracted fea-\nture vector yf. Analogously to [12], we pooled the bar\npositions down to 16 patterns per beat. For English pop we\nused the 4/4 rhythmic pattern, trained by [11] on ballroom\ndances. The feature vector is normalized to zero mean,\nunit variance and taking moving average. Normalization is\ndone per song.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 7054.4.2 Probability of Note Onset\nThe probability of a vocal note onset co-occurring at a\ngiven bar position e(b)is obtained from studies on sheet\nmusic. Many notes are aligned with a beat in the music\nscore, meaning a higher probability of a note at beats com-\npared to inter-beat bar positions. A separate distribution\ne(b)is applied for each different metrical cycle. For the\nTurkish usuls e(b)has been inferred from a recent study [7,\nFigure 5. a-c]. The authors used a corpus of music scores,\non data from the same corpus, from which we derived the\nTurkish dataset. The patterns reveal that notes are expected\nto be located with much higher likelihoods on those beats\nwith percussive strokes than on the rest.\nIn comparison to a classical tradition like makam, in\nmodern pop music the most likely positions of vocal ac-\ncents in a bar are arguably much more heterogeneous, due\nto the big diversity of time-deviations from one singing\nstyle to another [10]. Due to lack of a distribution pat-\nterne(b), characteristic for English pop, we set it manually\nwith probabilities (0.8,0.6,0.8,0.6)for the 4 beats.\n4.5 Inference\nWe obtain the most optimal state sequence x1:Kby decod-\ning with the Viterbi algorithm. A note onset is detected\nwhen the state path enters an attack note state after being\nin non-vocal state.\n4.5.1 With Manually Annotated Beats\nWe explored the option that beats are given as input from\na preprocessing step (i.e. when they are manually anno-\ntated). In this case, the detection of vocal onsets can be\ncarried out by a reduced model with a single hidden vari-\nable: the note state. The observation model is then re-\nduced to the pitch observation probability. The transition\nmodel is reduced to a bar-position aware transition prob-\nability aij(k)= p(nk=j|nk\u00001=i, \u0000k)(see Eq. 5).\nTo represent the time-dependent self-transition probabili-\nties we utilize time-varying transition matrix. The standard\ntransition probabilities in the Viterbi maximization step are\nsubstituted for the bar-position aware transitions aij(k)\n\u0000k(j) = max\ni2(j, j\u00001)\u0000k\u00001(i)aij(k)bj(Ok) (10)\nHere bj(Ok)is the observation probability for state ifor\nfeature vector Okand\u0000k(j)is the probability for the path\nwith highest probability ending in state jat time k(com-\nplying with the notation of [16, III. B]\n4.5.2 Full Model\nIn addition to onsets, a beat is detected when the bar po-\nsition variable hits one of Bpositions of beats within the\nmetrical cycle.\nNote that the size of the state space xposes a memory\nrequirement. A recording of 1 minute has around 10000\nframes at a hopsize of 5.8ms. To use Viterbi thus requires\nto store in memory pointers to up to 4G states, which\namounts to 40G RAM (with uint32 python data type).5. EXPERIMENTS\nThe hopsize of computing the spectral ﬂux feature, which\nresulted in most optimal beat detection accuracy in [12]\nishf= 20 ms. In comparison, the hopsize of predom-\ninant vocal melody detection is usually of smaller order\ni.e.hp=5.8ms (corresponding to 256 frames at sampling\nrate of 44100). Preliminary experiments showed that ex-\ntracting pitch with values of hpbigger than this values rea-\nsonably deteriorates the vocal onset accuracy. Therefore\nin this work we use hopsize of 5.8ms for the extraction of\nboth features. The time difference parameter for the spec-\ntral ﬂux computation remains unaffected by this change in\nhopsize, because it can be set separately.\nAs a baseline we run the algorithm of [14] with the 105\nnote states, we introduced in Section 4.1.23. The note\ntransition probability is the original as presented in Eq. 3,\ni.e. not aware of beats. Note that in [14] the authors intro-\nduce a post-processing step, in which onsets of consecutive\nsung notes with same pitch are detected considering their\nintensity difference. We excluded this step in all system\nvariants presented, because it could not be integrated in the\nproposed observation model in a trivial way. This means\nthat, essentially, in this paper cases of consecutive same-\npitch notes are missed, which decreases inevitably recall,\ncompared to the original algorithm.\n5.1 Evaluation Metrics\n5.1.1 Beat Detection\nSince improvement of the beat detector is outside the scope\nof this study, we report accuracy of detected beats only in\nterms of their f-measure4. This serves solely the sake of\ncomparison to existing work5. The f-measure can take a\nmaximum value of 1, while beats tapped on the off-beat\nrelative to annotations will be assigned an f-measure of 0.\nWe used the default tolerance window of 70ms, also ap-\nplied in [9].\n5.1.2 Vocal Onset Detection\nWe measured vocal onset accuracy in terms of precision\nand recall6. Unlike a cappella singing, the exact onset\ntimes of singing voice accompanied by instruments, might\nbe much more ambiguous. To accommodate this fact, we\nadopted the tolerance of t= 50 ms, used for vocal onsets\nin accompanied ﬂamenco singing by [13], which is much\nbigger than the t=5ms used by [14] for a cappella. Note\nthat measuring transcription accuracy remains outside the\nscope of this study.\n3We ported the original V AMP plugin im-\nplementation to python, which is available at\nhttps://github.com/georgid/pypYIN\n4The evaluation script used is at https://github.com/CPJKU/\nmadmom/blob/master/madmom/evaluation/beats.py\n5Note that the f-measure is agnostic to the phase of the detected beats,\nwhich is clearly not optimal\n6We used the evaluation script available at https://github.\ncom/craffel/mir_eval/blob/master/mir_eval/onset.\npy#L56706 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017meter beat Fmeas P R Fmeas\naksakMauch - 33.1 31.6 31.6\nEx-1 - 37.5 38.4 37.2\nEx-2 86.4 37.8 36.1 36.1\nd¨uyekMauch - 42.1 36.9 37.9\nEx-1 - 44.3 41.0 41.4\nEx-2 72.9 45.0 39.0 40.3meter beat Fmeas P R Fmeas\n4/4Mauch - 29.6 38.3 33.2\nEx-1 - 30.3 42.5 35.1\nEx-2 94.2 31.6 39.4 34.4\ntotalMauch - 34.8 35.6 35.2\nEx-1 - 38.3 40.6 39.5\nEx-2 84.3 38.1 38.2 38.1\nTable 1 : Evaluation results for Experiment 1 (shown as Ex-1) and Experiment 2 (shown as Ex-2). Mauch stands for the\nbaseline, following the approach of [14]. P, R and Fmeas denote the precision, recall and f-measure of detected vocal\nonsets. Results are averaged per meter type.\n5.2 Experiment 1: With Manually Annotated Beats\nAs a precursor to evaluating the full-ﬂedged model, we\nconducted an experiment with manually annotated beats.\nThis is done to test the general feasibility of the proposed\nnote transition model (presented in 4.2.1), unbiased from\nerrors in the beat detection.\nWe did apply both the simple and the time-\nredistribution weighting schemes, presented respectively\nin Eq. 8 and in Eq. 7. In preliminary experiments we\nsaw that with annotated beats the simple weighting yields\nmuch worse onset accuracy than the time-redistributed\none. Therefore the results reported are conducted with the\nlatter weighting.\nWe have tested different pairs of values for wand\u0000\nfrom Eq. 5. For Turkish makam the onset detection ac-\ncuracy peaks at w=1.2and\u0000= 30 ms, whereas for the\nEnglish pop optimal are w=1 .1and\u0000= 45 ms. Ta-\nble 1 presents metrics compared to the baseline7. Inspec-\ntion of detections revealed that the metrical-accent aware\nmodel could successfully detect certain onsets close to\nbeats, which are omitted by the baseline.\n5.3 Experiment 2: Full Model\nTo assure computational efﬁcient decoding, we did an efﬁ-\ncient implementation of the joint state space of [12]8.T o\ncompare to that work, we measured the beat detection with\nboth their original implementation and our proposed one.\nExpectedly, the average f-measure of the detected beats\nwere the same for each of the three metrical cycle types\nin the datasets, which can be seen in Table 1. For aksak\nand d ¨uyek usuls, the accuracy is somewhat worse than the\nresults of 91and85.2respectively, reported in [9, Table\n1.a-c, R=1]. We believe the reason is in the smaller size of\nour training data. Table 1 evidences also a reasonable im-\nprovement of the vocal onset detection accuracy for both\nmusic traditions. The results reported are only with the\nsimple weighting scheme for the vocal note onset transi-\ntion model (the time-redistribution weighting was not im-\nplemented in this experiment).\n7Per-recording results for the makam dataset are available at https:\n//tinyurl.com/y8r73zfh and for the lakh-vocal-segments dataset\nathttps://tinyurl.com/y9a67p8u\n8We extended the python toolbox for beat tracking\nhttps://github.com/CPJKU/madmom/, which we make available at\nhttps://github.com/georgid/madmomAdding the automatic beat tracking improved the base-\nline, whereas this was not the case with manual beats for\nsimple weighting. This suggests that the concurrent track-\ning of beats and vocal onsets is a ﬂexible strategy and\ncan accommodate some vocal onsets, slightly time-shifted\nfrom a beat. We observe also that the vocal onset accu-\nracy is on average a bit inferior to that with manual beat\nannotations (done with the time-redistribution weighting).\nFor the 4/4 meter, despite the highest beat detection ac-\ncuracy, the improvement of onset accuracy over the base-\nline is the least. One reason for that may be that the note\nprobability pattern e(b), used for 4/4 is not well represen-\ntative for the singing style differences.\nA paired t-test between the baseline and each of Ex-1\nand Ex-2 resulted in p-values of respectively 0.28and0.31\non total for all meter types. We expect that statistical sig-\nniﬁcance can be evaluated more accurately with a bigger\nnumber of recordings.\n6. CONCLUSIONS\nIn this paper we presented a Bayesian approach for track-\ning vocal onsets of singing voice in polyphonic music\nrecordings. The main contribution is that we integrate in\none coherent model two existing probabilistic approaches\nfor different tasks: beat tracking and note transcription.\nResults conﬁrm that the knowledge of the current posi-\ntion in the metrical cycle can improve the accuracy of vo-\ncal note onset detection over different metrical cycle types.\nThe model has a comprehensive set of parameters, whose\nappropriate tuning allows application to material with dif-\nferent singing style and meter.\nIn the future the manual adjustment of these parameters\ncould be replaced by learning their values from sufﬁciently\nbig training data, which was not present for this study. In\nparticular, the lakh-vocal-segments dataset could be eas-\nily extended substantially, which we plan to do in the fu-\nture. Moreover, one could decrease the expected parame-\nter values range, based on learnt values, and thus decrease\nthe size of the state space, which is a current computa-\ntional limitation. We believe that the proposed model could\nbe applied as well to full-ﬂedged transcription of singing\nvoice.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 707Acknowledgements We thank Sebastian B ¨ock for the\nimplementation hints. Ajay Srinivasamurthy is currently\nwith the Idiap Research Institute, Martigny, Switzerland.\nThis work is partly supported by the European Research\nCouncil under the European Union’s Seventh Framework\nProgram, as part of the CompMusic project (ERC grant\nagreement 267583) and partly by the Spanish Ministry\nof Economy and Competitiveness, through the ”Mar ´ıa de\nMaeztu” Programme for Centres/Units of Excellence in\nR&D” (MDM-2015-0502).\n7. REFERENCES\n[1]Hasan Sercan Atlı, Burak Uyar, Sertan S ¸ent ¨urk, Barıs ¸\nBozkurt, and Xavier Serra. Audio feature extraction\nfor exploring Turkish makam music. In Proceedings\nof 3rd International Conference on Audio Technologies\nfor Music and Media (ATMM 2014) , pages 142–153,\nAnkara, Turkey, 2014.\n[2]Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, 2013.\n[3]Sungkyun Chang and Kyogu Lee. A pairwise approach\nto simultaneous onset/offset detection for singing voice\nusing correntropy. In Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 629–633. IEEE, 2014.\n[4]Norberto Degara, Antonio Pena, Matthew EP Davies,\nand Mark D Plumbley. Note onset detection using\nrhythmic structure. In Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 5526–5529. IEEE, 2010.\n[5]Masataka Goto. Singing information processing. In\n12th International Conference on Signal Processing\n(ICSP) , pages 2431–2438. IEEE, 2014.\n[6]Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nPopular, classical, and jazz music databases. In Pro-\nceedings of the 3rd International Conference on Music\nInformation Retrieval (ISMIR 2002) , pages 287–288,\n2002.\n[7]Andre Holzapfel. Relation between surface rhythm and\nrhythmic modes in turkish makam music. Journal of\nNew Music Research , 44(1):25–38, 2015.\n[8]Andre Holzapfel and Thomas Grill. Bayesian meter\ntracking on learned signal representations. In Proceed-\nings of the 17th International Society for Music In-\nformation Retrieval Conference (ISMIR 2016) , pages\n262–268, 2016.\n[9]Andre Holzapfel, Florian Krebs, and Ajay Srini-\nvasamurthy. Tracking the “odd”: Meter inference in\na culturally diverse music corpus. In Proceedings ofthe 15th International Society for Music Information\nRetrieval Conference (ISMIR 2014) , pages 425–430,\nTaipei, Taiwan, 2014.\n[10] David Brian Huron. Sweet anticipation: Music and the\npsychology of expectation . MIT press, 2006.\n[11] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nRhythmic pattern modeling for beat and downbeat\ntracking in musical audio. In Proceedings of the 14th\nInternational Society for Music Information Retrieval\nConference (ISMIR 2013) , Curitiba, Brazil, 2013.\n[12] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nAn Efﬁcient State-Space Model for Joint Tempo and\nMeter Tracking. In Proceedings of the 16th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR 2015) , pages 72–78, Malaga, Spain, Oc-\ntober 2015.\n[13] Nadine Kroher and Emilia G ´omez. Automatic tran-\nscription of ﬂamenco singing from polyphonic music\nrecordings. IEEE Transactions on Audio, Speech and\nLanguage Processing , 24(5):901–913, 2016.\n[14] Matthias Mauch, Chris Cannam, Rachel Bittner,\nGeorge Fazekas, Justin Salamon, Jiajie Dai, Juan\nBello, and Simon Dixon. Computer-aided melody note\ntranscription using the tony software: Accuracy and ef-\nﬁciency. In Proceedings of the First International Con-\nference on Technologies for Music Notation and Rep-\nresentation (TENOR 2015) , pages 23–30, 2015.\n[15] Ryo Nishikimi, Eita Nakamura, Katsutoshi Itoyama,\nand Kazuyoshi Yoshii. Musical note estimation for F0\ntrajectories of singing voices based on a bayesian semi-\nbeat-synchronous HMM. In Proceedings of the 17th\nInternational Society for Music Information Retrieval\nConference, (ISMIR 2016) , pages 461–467, 2016.\n[16] Lawrence Rabiner. A tutorial on hidden Markov mod-\nels and selected applications in speech recognition.\nProceedings of the IEEE , 77(2):257–286, 1989.\n[17] Colin Raffel. Learning-Based Methods for Comparing\nSequences, with Applications to Audio-to-MIDI Align-\nment and Matching . PhD thesis, Columbia University,\n2016.\n[18] Matti Ryyn ¨anen. Probabilistic modelling of note events\nin the transcription of monophonic melodies. Master’s\nthesis, 2004.\n[19] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. IEEE Transactions on Audio, Speech,\nand Language Processing , 20(6):1759–1770, 2012.\n[20] Nick Whiteley, Ali Taylan Cemgil, and Simon God-\nsill. Bayesian modelling of temporal structure in musi-\ncal audio. In Proceedings of the 7th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR 2006) , pages 29–34, Victoria, Canada, October\n2006.708 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "The Significance of the Low Complexity Dimension in Music Similarity Judgements.",
        "author": [
            "Jeffrey Ens",
            "Bernhard E. Riecke",
            "Philippe Pasquier"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416400",
        "url": "https://doi.org/10.5281/zenodo.1416400",
        "ee": "https://zenodo.org/records/1416400/files/EnsRP17.pdf",
        "abstract": "Previous research has demonstrated that similarity judgements are context specific, as they are shaped by cultural exposure, familiarity, and the musical aesthetic of the content being compared. Although such research suggests that the criterion for similarity judgement varies with respect to the musical style of the content being com- pared, the specific musical factors which shape this cri- terion are unknown. Since dimensional complexity dif- ferentiates musical genres, and has been shown to affect similarity judgements following lifelong exposure, this ex- periment investigates the short-term influence of dimen- sional complexity on similarity judgements. Rhythmic and pitch sequences with two levels of complexity were facto- rially combined to create four distinct types of prototype melodies. 51 participants rated the similarity of each type of prototype melody (M) to two variations, one in which the pitch content was modified ( ¯ Mp), and another in which the rhythmic content was modified ( ¯ Mr). The results in- dicate that rhythm and pitch complexity both play a sig- nificant role, influencing the perceived similarity of ¯ Mp, and ¯ Mr. The dimension bearing low complexity informa- tion was found to be the predominant factor in similarity judgements, as participants found modifications to this di- mension to significantly decrease perceived similarity.",
        "zenodo_id": 1416400,
        "dblp_key": "conf/ismir/EnsRP17",
        "keywords": [
            "Previous research",
            "cultural exposure",
            "familiarity",
            "musical aesthetic",
            "context specific",
            "dimensional complexity",
            "musical genres",
            "lifelong exposure",
            "short-term influence",
            "prototype melodies"
        ],
        "content": "THE SIGNIFICANCE OF THE LOW COMPLEXITY DIMENSION IN\nMUSIC SIMILARITY JUDGEMENTS\nJeff Ens\nSimon Fraser University\njeffe@sfu.caBernhard E. Riecke\nSimon Fraser University\nber1@sfu.caPhilippe Pasquier\nSimon Fraser University\npasquier@sfu.ca\nABSTRACT\nPrevious research has demonstrated that similarity\njudgements are context speciﬁc, as they are shaped by\ncultural exposure, familiarity, and the musical aesthetic\nof the content being compared. Although such research\nsuggests that the criterion for similarity judgement varies\nwith respect to the musical style of the content being com-\npared, the speciﬁc musical factors which shape this cri-\nterion are unknown. Since dimensional complexity dif-\nferentiates musical genres, and has been shown to affect\nsimilarity judgements following lifelong exposure, this ex-\nperiment investigates the short-term inﬂuence of dimen-\nsional complexity on similarity judgements. Rhythmic and\npitch sequences with two levels of complexity were facto-\nrially combined to create four distinct types of prototype\nmelodies. 51 participants rated the similarity of each type\nof prototype melody (M)to two variations, one in which\nthe pitch content was modiﬁed (\u0016Mp), and another in which\nthe rhythmic content was modiﬁed (\u0016Mr). The results in-\ndicate that rhythm and pitch complexity both play a sig-\nniﬁcant role, inﬂuencing the perceived similarity of \u0016Mp,\nand \u0016Mr. The dimension bearing low complexity informa-\ntion was found to be the predominant factor in similarity\njudgements, as participants found modiﬁcations to this di-\nmension to signiﬁcantly decrease perceived similarity.\n1. INTRODUCTION\nSimilarity directly informs our experience of music, en-\nabling the perception of cohesion within a musical work,\nand the categorization of musical works. Consequently,\ndeveloping models that encapsulate the manner in which\nsimilarity is perceived, is of critical importance within the\nareas of Musicology, Music Cognition and Music The-\nory [30]. In particular, the search for robust and ﬂexi-\nble similarity measures has dominated research in the Mu-\nsic Information Retrieval (MIR) domain, as large digital\ndatabases of music information necessitate content-based\nquerying and retrieval, and classiﬁcation. Although there\nc\rJeff Ens, Bernhard E. Riecke, Philippe Pasquier. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Jeff Ens, Bernhard E. Riecke, Philippe\nPasquier. “The Signiﬁcance of the Low Complexity Dimension in Music\nSimilarity Judgements”, 18th International Society for Music Information\nRetrieval Conference, Suzhou, China, 2017.is a large body of research that explores similarity percep-\ntion within music, many aspects of similarity perception\nare not yet fully understood. The current study corrobo-\nrates previous evidence that similarity criterion vary with\nrespect to the musical content being compared [9], demon-\nstrating that the complexity of pitch and rhythmic content\ninﬂuence similarity perception.\nSince pitch and rhythm are the two most prominent mu-\nsical dimensions in the context of symbolic notation, the\ncurrent study will manipulate complexity along these di-\nmensions and observe the effects on similarity perception.\nAlthough no musical dimensions are completely orthogo-\nnal, as a modiﬁcation in a particular dimension may affect\nthe perception of other dimensions, the complexity of pitch\nand rhythmic content can be measured independently, and\nthere is evidence that these dimensions are processed sep-\narately in cognition [13, 27]. Therefore, pitch and rhythm\ncomplexity were considered to be independent for the pur-\nposes of this study. Pitch content refers to the sequence\nof pitches encapsulated in a particular melody, and rhythm\ncontent refers to the sequence of durations. Dimensional\ncomplexity refers to the absolute level of complexity along\na particular musical dimension. In this study we measure\nthe dimensional complexity of pitch and rhythm content.\n2. RELATED WORK\nPrevious work examining the perception of musical simi-\nlarity, has focused on establishing a hierarchy of musical\ndimensions, ranking their observed contributions to simi-\nlarity perception. On a whole, most research claims that\nrhythmic information is the most important. Halpern [7]\nconstructed 16 melodies — a factorial combination of two\npitch sequences, two rhythmic sequences, two tonal struc-\ntures and forward and reversed versions — and found that\nrhythm was the most important distinguishing factor, fol-\nlowed by pitch, direction and tonal structure. Similarly,\nRosner and Meyer [19] found rhythm to be the strongest\ndeterminant of melodic similarity. Despite the general\nconsensus that rhythm plays a dominant role in similarity\njudgements, pitch still plays a considerable role. Dowl-\ning [2] demonstrated that a modiﬁed imitation of a proto-\ntype melody is often misidentiﬁed as the prototype when it\nhas a similar pitch contour.\nGiven the multidimensional nature of music, many re-\nsearchers have found it useful to make the distinction\nbetween surface-level and structural features. In gen-31eral, surface-level attributes include contour, loudness and\ntempo while structural attributes denote aspects of form,\nthematic development and patterns. In short term contexts,\nwhere participants are unfamiliar with the musical material\nbeing compared, surface-level features are a strong predic-\ntor of both melodic [15,19,22] and polyphonic [9] similar-\nity. Prince [15] found that rhythm was the dominant aspect\ninforming perceived melodic similarity, followed by con-\ntour, meter, and tonal structure.\nHowever, there is increasing evidence which questions\nthe generality of these results, as contextual factors includ-\ning familiarity, cultural exposure, and the aesthetic of the\nmusical content being compared, have been shown to have\na considerable effect on similarity perception. Pollard-Gott\nfound that with repeated listening, surface level features\nbecame less inﬂuential and thematic material became more\nimportant [14]. Similarly, the long term analysis of a col-\nlection of folk melodies by a panel of experts, placed em-\nphasis on thematic and motivic similarity above all other\nfactors [31]. Schubert and Stevens [22] found that contour\nis more important than harmonic structure for making sim-\nilarity comparisons, but with musical expertise, harmonic\nstructure also has an effect.\nOther research has shown that cultural exposure affects\nsimilarity perception. Hannon and Trehub [8] found the\nmetrical bias of North American adults to be the result\nof an enculturation processes, with no evidence of a nat-\nural predisposition for the simple meters which character-\nize much of western music. Goldstone [6] suggests that\nhumans learn by focusing on perceptual features that are\nmore informative, at the cost of decreased attention to-\nwards other dimensions. This phenomenon has been ob-\nserved in a musical context, where the voice that consists\nof immediate and exact repetitions of a short musical frag-\nment tends to perceptually decrease in salience for the lis-\ntener over time [24]. Instead, the listener is naturally drawn\nto focus on the high complexity voice. Since distinct rhyth-\nmic durations occur at a relatively higher frequency than\ndistinct pitches in western music, they demand less atten-\ntion than pitch content. After years of exposure, this likely\nresults in an increased sensitivity to the pitch content in\na melody [17]. Notably, Eerola et al. [3] demonstrated\nthat musical complexity perceptions are shaped by expo-\nsure to different musical culture, which likely results from\nthe mechanisms described above.\nIn addition to the factors mentioned above, music aes-\nthetic has been shown to inﬂuence how similarity is per-\nceived. Lamont and Dibben [9] examined similarity rela-\ntionships in two contrasting musical styles, requiring par-\nticipants to rate the similarity of extracts from a Beethoven\nsonata (op. 10, no. 1, ﬁrst movement) and a dodecaphonic\nwork composed by Schoenberg (Klavierst ¨uck op. 33a).\nNine polyphonic excerpts were selected from each piece,\neach approximately eight measures long, and the similar-\nity of each possible combination was rated by participants,\nresulting in 36 similarity ratings for each piece. Notably,\nboth pieces are composed for solo piano, and have more\nthan one theme which is developed throughout the durationof each work. They found that similarity judgements were\nprimarily based on surface level features, however, the sim-\nilarity judgements for each piece were predominantly in-\nﬂuenced by different surface features. These results sug-\ngested that each piece establishes a different similarity cri-\nterion within which listeners make appropriate similarity\njudgements. Although Lamont and Dibben demonstrated\nthat the criterion for similarity judgements varies with re-\nspect to the musical aesthetic of the stimuli being com-\npared, the speciﬁc musical factors which caused this phe-\nnomenon are still unknown, directly motivating our exper-\niment.\n3. MOTIVATION\nAs evidenced by the brief overview in section 2, nu-\nmerous studies have demonstrated the prevalent inﬂuence\nof contextual factors on musical similarity judgements\n[8, 9, 14, 17, 31], directly motivating further study in this\narea. Since contextual factors like cultural exposure and\nfamiliarity are difﬁcult to integrate into a similarity mea-\nsure, this study examines the third contextual factor, the\nrole of the musical content itself in shaping a criterion for\nsimilarity judgements. The phenomenon that Lamont and\nDibben [9] observed, provides evidence that musical con-\ntent inﬂuences the manner in which music is compared,\nas participants used different musical dimensions to make\ncomparisons depending on the nature of the musical con-\ntent. In light of this evidence, it is worthwhile to examine\nhow speciﬁc musical characteristics of the content being\ncompared shape similarity judgements, which does not ap-\npear to have been examined previously. Due to the fact that\ndimensional complexity differentiates musical genres [3],\nand affects similarity judgements following lifelong ex-\nposure [8], this experiment investigates the short-term in-\nﬂuence of dimensional complexity on melodic similarity\njudgements. More speciﬁcally, this study investigates the\nrole of dimensional complexity in shaping awareness to\nmodiﬁcations in that particular dimension, effectively es-\ntablishing a criterion for melodic similarity judgements.\nPrevious research has shown that limitations on the hu-\nman capacity for musical memory, have an effect on mu-\nsical perception. Participants found it more difﬁcult to\nretain melodies with complex contours, which were de-\nvoid of any repetition, and were often unable to distin-\nguish them from another complex contour [18]. Moreover,\ncomplexity was one of four variables which collectively\npredicted the recognizability of melodies when presented\na second time [20]. In these cases, it seems likely that\nworking memory limitations make it difﬁcult to encapsu-\nlate all aspects of a complex melody on ﬁrst exposure. In\nsummarizing recent research on working memory limita-\ntions, Cowan [1] proposes that there is a capacity of three\nto ﬁve chunks in working memory for young adults. Ac-\ncording to these ﬁndings, modiﬁcations to the musical di-\nmension bearing the least complex musical material should\nbe the easiest to detect, which suggests that this musical\ndimension would have a predominant inﬂuence on simi-\nlarity judgements. Collectively, this research supports the32 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017following hypothesis: modiﬁcations to the musical dimen-\nsion bearing low complexity information will result in a\nsigniﬁcant decrease in similarity, in comparison to similar\nmodiﬁcations to the musical dimension bearing high com-\nplexity information.\n4. METHODOLOGY\n4.1 Participants\nThe participants were recruited online using the Crowd-\nﬂower1crowdsourcing platform, and required to pass a\ntest before participating in the experiment. Participants\nwere paid $0.02 USD for each question they answered,\nin accordance with the typical compensation offered to\nCrowdﬂower users. Of the 96 participants who took the\ntest, 76 passed (79.2%) and 63 completed the experiment.\n12 participants responses were deemed ineligible based on\nthe inconsistent responses to an identical question. In total,\n51 participants came from 25 different countries.\n4.2 Stimuli\n4.2.1 Measuring Complexity\nGiven the multifaceted nature of complexity, it is necessary\nto make the distinction between the entropy based com-\nplexity measures proposed by Eerola et al. [3], and the no-\ntion of complexity which grounds the current study. Shan-\nnon Entropy quantiﬁes the disorder or uncertainty inherent\nin an information source based on a representative proba-\nbility distribution [23]. Eerola et al. calculate entropy us-\ning the marginal probability of each symbol in a sequence.\nThis type of complexity will be referred to as entropym.\nAlthough entropymhas been shown to correlate with the\npercieved complexity of musical sequences [16], this mea-\nsurement of complexity does not provide the necessary res-\nolution to make comparisons between many musical se-\nquences. For an explicit example, consider the follow-\ning pitch sequences, s1=fc, d, e, f, c, d, e, fg, ands2=\nfc, f, e, d, e, c, d, fg. Even though s1exhibits less com-\nplexity than s2, boths1ands2have the same entropym,\nas this measurement does not take the repetition of longer\nphrases into consideration. Clearly, it is necessary to take\nthe repetition of phrases into consideration when measur-\ning complexity.\nAdmittedly, this can be accomplished by calculating the\nentropy rate of an n-th order markov chain derived from\nthe musical sequence being measured, however there are\nstill issues with this approach. In contrast to the manner in\nwhich humans percieve musical content, and by extension\nmusical complexity, the entropy rate is not designed to dis-\ntiguish between repetition which occurs within the prevail-\ning metric structure, and repetition which spans metrical\nboundaries. Research suggests that humans perceive mu-\nsic by breaking it into a series of chunks [5], and have a\nnatural tendency to project metre onto sequences of sound,\ndespite the absence of acoustic cues for metric organiza-\ntion [4]. In addition, when listening to music, humans\n1https://www.crowdﬂower.com/naturally extract motivic patterns [32], and larger formal\nstructures [12]. Since humans segment music in accor-\ndance with metrical boundaries, it is likely that humans\nare less sensitive to repetition which is obscured by these\nboundaries. Consequently, a true measure of musical com-\nplexity must take this distinction into account.\nFurthermore, an entropy based model of complexity is\nnot capable of taking similarity into consideration, as en-\ntropy is based on the lossless encoding of an information\nsource [23]. This becomes more of an issue when entropy\nis being measured with respect to larger subsequences, as\nis the case when measuring the n-th order entropy rate.\nThis formulation of complexity cannot make the distinc-\ntion between a collection of subsequences which share the\nsame contour, and a collection that does not. As a result,\nit seems most reasonable to take the collective dissimilar-\nity of subsequences segmented with respect to the prevail-\ning metric structure, as a measure of complexity. Conse-\nquently, a homogenous collection of segments would be\nperceived as having a low complexity, while a diverse col-\nlection of segments would be perceived as having a high\ncomplexity. We use the term redundancy to refer to this\ntype of complexity throughout the paper.\nIn order to quantify redundancy, two different measures\nwere used. Thul’s [28] adaptation of Tanguiane’s [25, 26]\nalgorithm, measures redundancy by counting the number\nofroot patterns , at several hierarchical levels. This will\nbe referred to as Tanguiane’s Rhythmic Complexity (TRC).\nThe other measure of redudancy is calculated using Eqn\n(1), where (S)is a set of subsequences, derived by seg-\nmenting a sequence of symbols into measures. Notably,\nEqn (1) also requires a distance metric (D). Chrono-\ntonic distance [29] is used to measure Rhythmic Sequence\nComplexity (RSC), and a similarity measure proposed by\nMaid ´ın [10] is used to measure Pitch Sequence Complexity\n(PSC). Admittedly, segmenting a pitch sequence accord-\ning to metre means that PSC is dependant on the rhythmic\ncontent, however, within-measure rhythmic patterns have\nno bearing on PSC in this paradigm, and the metric struc-\nture is not being manipulated in this study. Although PSC\ndoes not account for the complexity of invidual segments,\nsection 4.2.2 describes how complexity is restricted in this\nexperiment, effectively mitigating the variance of segment\ncomplexity in the current study.\nf(S) =1\njSjjSjX\ni=1minfD(Si;Sj) :j6=i; 1\u0014j\u0014jSjg\n(1)\n4.2.2 Prototype Melodies\nIn this experiment, there were four types of melodies;\nrhythms-pitchs,rhythms-pitchc,rhythmc-pitchs, and\nrhythmc-pitchc, wheresdenotes a simple or low complex-\nity sequence, and cdenotes a complex sequence2. In\naddition, eight versions of each melody type were con-\nstructed, resulting in 32 (4\u00028)prototype melodies of equal\n2The melodies used in this experiment can be found at\nhttps://mlab%2Dexperiments.iat.sfu.ca/ismir2017/audio.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 33A\nA4 B4 C5      D5AB  A4                 B4 C5  D5  E5             A4       B4 G4ABC 1/8 1/8 1/4     1/2  1/2                1/8 1/8  1/4  3/8           1/4      1/8  1/4ABA*ABBB*\nINST-0INST-1 = 120\nOPUSMODUSFigure 1 . A melody with complex rhythm and simple pitch, using letters to show the form of each dimension.\nlength (three measures). As mentioned in section 4.2.1,\nredundancy quantiﬁes the degree to which an information\nsource is self similar and contains periodic repetition in\nconjunction with the prevailing metrical structure. In light\nof this aim, melodies were comprised of three measure-\nlength phrases, with phrase repetition varied to create two\ndistinct levels of complexity. Low complexity sequences\nhad a formal pattern AAB, where a pattern is repeated in\nthe ﬁrst two measures, and a new pattern is introduced in\nthe last measure. High complexity sequences had a formal\npattern ABC, where each measure is dissimilar. This con-\nstruction process is demonstrated in Figure 1, which shows\na high complexity rhythm sequence and a low complexity\npitch sequence.\nCare was taken to restrict the variability of entropym\nbased complexity, using measures proposed by Eerola et\nal. [3]. Since the pitch sequences were constructed from\nscales consisting of ﬁve distinct pitch classes, Entropy\nof pitch class distribution andEntropy of interval distri-\nbution did not vary signiﬁcantly. Similarly, rhythm se-\nquences were constructed from four distinct durations, lim-\niting the variance of Entropy of note duration distribution\nandRhythmic variability . Notably, it seemed reasonable to\nhave fewer distinct durations than pitch classes, as research\nhas demonstrated that most listeners are able to perceive\npitch diversity more readily [17]. A One-Way Analysis of\nVariance (ANOV A) across all four prototype melody types\ndemonstrated that none of these entropymbased complex-\nity measures were a signiﬁcant source of variance, while\nPSC,RSC andTRC varied signiﬁcantly. Furthermore, the\nentropy rate – calculated using a ﬁrst order markov chain\n– did not vary signiﬁcantly across melody type. This ver-\niﬁed that our experiment measured the effect of variations\nin redundancy in relative isolation.\nIn order to restrict the variance of segment complexity,\nMean interval size andNote density were restricted, which\nEerola et al. [3] found to be a signiﬁcant source of com-\nplexity. Each melody was constrained to an octave range,\nrestricting the Mean interval size . The Note density , was\ninvariant for each constructed melody, as each melody had\nfour notes per measure, and was three measures long.\n4.2.3 Modiﬁed Melodies\nFor each prototype melody (M), two modiﬁed versions\nwere constructed for the main experiment: a version in\nwhich the pitch is modiﬁed (\u0016Mp), and a version in which\nthe rhythm is modiﬁed (\u0016Mr). This process involved re-\nversing the order of the measures in the dimension whichis to be modiﬁed. As a result, regardless of the nature of the\nprototype melody, the ﬁrst and last measures of the mod-\niﬁed melody were different. Since test questions required\na ground truth answer, three additional types of modiﬁed\nmelodies were constructed: a melody in which the pattern\nform ofMwas transformed from AAB to ABA in the pitch\ndimension (\u0016Mr\u0016p), a melody in which the pattern form of\nMwas transformed from AAB to ABA in the rhythm di-\nmension (\u0016Mp\u0016r), and a melody in which both dimensions\nwere modiﬁed (\u0016Mb).\n4.3 Experimental Design\nThe experiment consisted of two independent variables,\nrhythm and pitch content complexity. Both rhythm and\npitch complexity had two levels, low and high. This re-\nsulted in a 2\u00022repeated measures experimental design,\nwith four distinct types of prototype melodies. Partici-\npants were presented with a series of questions, consist-\ning of a prototype melody (M)and two modiﬁed melodies\n(melody A ,melody B ). There were two types of test ques-\ntions, which were developed using the modiﬁed melodies\ndescribed above. The ﬁrst type of question, compared ei-\nther \u0016Mr\u0016pandMagainst the prototype M, or\u0016Mp\u0016randM\nagainstM. This had an indisputable answer, as one of the\nmodiﬁed melodies was in fact an exact replica of the pro-\ntotype. The second type of question, compared \u0016Mpand\n\u0016Mbto the prototype, or compared \u0016Mrand \u0016Mbto the pro-\ntotype. Given the manner in which these melodies were\nconstructed, \u0016Mpand\u0016Mrare more similar to the prototype,\nas they are identical to the prototype along a single dimen-\nsion, while \u0016Mbis dissimilar in both dimensions.\nFor the actual experiment itself, there was a single\ntype of question, in which \u0016Mrand \u0016Mpwere compared\nagainst the prototype. Irregardless of the type of ques-\ntion, the two modiﬁed melodies were randomly assigned\nto be melody A ormelody B . For each question, partici-\npants rated the similarity of melody A toM, and melody\nBtoM, on a Likert scale from 1 to 20, where 20 indi-\ncates maximal similarity. In the analysis below, the dif-\nference (D=S(M;\u0016Mr)\u0000S(M;\u0016Mp))between the per-\nceived similarity of \u0016MrtoM(S(M;\u0016Mr)), and the per-\nceived similarity of \u0016MptoM(S(M;\u0016Mp)), is taken as the\ndependent variable. As a result, a positive value of Dindi-\ncates that modiﬁcations to the rhythm dimension have less\nof an effect on similarity than modiﬁcations to the pitch\ndimension, while a negative value of Dindicates the op-\nposite.34 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20174.4 Procedure\nBefore participating in the experiment, participants were\nrequired to complete 10 test questions with a minimum\naccuracy of 80%. The test questions served two pur-\nposes, eliminating those who were not taking the task se-\nriously, and familiarizing participants with the similarity\ndomain within which they were being asked to make com-\nparisons. Once the test was successfully completed, par-\nticipants were presented with 10 randomly ordered ques-\ntions, consisting of eight different experiment questions\n(representing each of the eight different types of prototype\nmelodies), a test question, and a repeated experiment ques-\ntion. The repeated experiment question was used to deter-\nmine if participants were answering the questions consis-\ntently. For each question, the prototype melody was se-\nlected randomly from a collection of eight versions, and\nthe key was randomly transposed so that the content var-\nied from question to question. After listening to all three\nmelodies, participants were asked to indicate which of the\ntwo modiﬁed versions was more similar to the prototype,\nand rate the similarity of melody A andmelody B on a Lik-\nert scale from 1 to 20.\n5. RESULTS\nSince the ANOV A is relatively robust to violations of\nnormality [21], the 2-Way ANOV A was conducted with-\nout transforming the data, despite the violation of the as-\nsumption of normality. A 2-Way ANOV A revealed the\nmain effect of rhythm complexity (F(50) = 9:17; p=\n:004; \u00112\np=:155) and pitch complexity (F(50) =\n5:31; p=:025; \u00112\np=:096) , while the interaction between\nrhythm complexity and pitch complexity was insigniﬁcant\n(p=:657) . To be thorough, an Aligned Rank Transform\nwas performed on the data, correcting for the effects of\nthe non-normal distributions of the data [33]. Using the\ntransformed data, a 2-Way ANOV A revealed main effect of\nrhythm complexity (F(50) = 9:82; p=:003; \u00112\np=:164)\nand pitch complexity (F(50) = 6:26; p=:016; \u00112\np=\n:111) , while the interaction between rhythm complexity\nand pitch complexity was insigniﬁcant (p=:601) . These\nresults corroborate the analysis of the untransformed data,\nindicating that 16.4% of the variability in similarity rat-\nings were explained by changes in rhythm complexity, and\n11.1% of the variability was explained by changes in pitch\ncomplexity.\nAs predicted, there was a main effect of rhythm com-\nplexity and pitch complexity, both shown in Figure 2b.\nMelodies containing low complexity rhythmic content\n(M= 0:451; SD = 5:26) were signiﬁcantly lower\nthan those containing high complexity rhythmic content\n(M= 2:49; SD = 5:81), which indicates that partic-\nipants were more sensitive to pitch modiﬁcations when\npitch sequences were less complex. This effect was pro-\nnounced in cases where the rhythmic sequence was more\ncomplex, as participants found pitch modiﬁed melodies\n(\u0016Mp)to be signiﬁcantly less similar to rhythmc-pitchspro-\ntotype melodies than rhythm modiﬁed melodies (\u0016Mr).Conversely, melodies containing low complexity pitch\ncontent (M= 2:26; SD = 5:43) were signiﬁcantly\nhigher than those containing high complexity pitch con-\ntent(M= 0:676; SD = 5:73), which indicates that par-\nticipants were more sensitive to rhythmic modiﬁcations\nwhen rhythmic sequences were less complex. Similarly,\nthis effect was pronounced in cases where the pitch se-\nquence was more complex, as participants found rhythm\nmodiﬁed melodies (\u0016Mr)to be signiﬁcantly less similar\ntorhythms-pitchcprototype melodies than pitch modiﬁed\nmelodies (\u0016Mp). Therefore, the dimension bearing low\ncomplexity musical content was found to play a signiﬁ-\ncant role in similarity judgements, as modiﬁcations to that\ndimension signiﬁcantly decreased perceived similarity.\nAn analysis of the individual prototype melody con-\nditions revealed that the rhythms-pitchccondition (M=\n\u00000:235; SD = 5:21) was signiﬁcantly less than the\nrhythmc-pitchscondition (M= 3:39; SD = 5:39),\nas pitch modiﬁed melodies were the most similar to\nrhythms-pitchcprototypes, and rhythm modiﬁed melodies\nwere the most similar to rhythmc-pitchsprototypes. The\nrhythms-pitchscondition (M= 1:14; SD = 5:27)and the\nrhythmc-pitchccondition (M= 1:59; SD = 6:12)were\nroughly equivalent, and participants did not ﬁnd a partic-\nular type of modiﬁed melody to be more similar, relative\nto the two other conditions. Collectively, these results in-\ndicate that melodies which are modiﬁed in the dimension\nbearing low complexity information are perceived as sig-\nniﬁcantly less similar than melodies which are modiﬁed in\nthe dimension bearing high complexity information.\n6. DISCUSSION\nAs evidenced by the results presented above, modiﬁca-\ntions to the dimension bearing low complexity informa-\ntion result in a signiﬁcant decrease in perceived similarity,\ndemonstrating that the dimension bearing low complex-\nity information plays a more signiﬁcant role in melodic\nsimilarity judgments. On a whole, the values for all four\nconditions were positively skewed (Figure 2a), indicating\nthat modiﬁcations to the pitch content of a melody had a\ngreater inﬂuence on perceived similarity. Since there is\nno benchmark with which to compare rhythmic sequence\ncomplexity and pitch sequence complexity, it was not pos-\nsible to equate the complexity across dimensions. Conse-\nquently, some skew in either direction was expected. The\npositive skew may indicate that the rhythmic content of the\nmelodies in this experiment was on average more complex,\nand participants had difﬁculty noticing modiﬁcations in the\nrhythm dimension. Alternatively, due to the enculturation\nprocess that Hannon and Trehub [8] observed, participants\nmay have paid more attention to the pitch content, resulting\nin the slight positive skew. When these factors are consid-\nered, it is arguably most meaningful to interpret the con-\nditions in relation to each other, as some skew in either\ndirection was inevitable. Viewed from this perspective, the\nhypothesis is directly corroborated, as the rhythms-pitchcProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 35Simple PitchComplex PitchSimple PitchComplex PitchSimple RhythmComplex RhythmSimple Pitch   Complex Pitch   Simple Rhythm   Complex RhythmMr is more similar\nMp is more similarDifference betweenS(M,Mr) and S(M,Mp)\nPrototype MelodyPitch ComplexityRhythm ComplexityFigure 2 . (a) The difference between the perceived similarity of the modiﬁed rhythm melody and the perceived similarity\nof the modiﬁed pitch version for each prototype melody complexity category, with 95% conﬁdence intervals. (b) The main\neffects of pitch and rhythm complexity with 95% conﬁdence intervals\ncondition is the lowest, the rhythmc-pitchsis the highest,\nand the rhythms-pitchsandrhythmc-pitchcconditions are\nin the middle.\nFurther analysis reveals that previous experiments are\nlikely a special case of the generalized theory proposed\nin this paper. Monahan et al. [11] and Halpern [7] both\nmake the claim that rhythm contributes more signiﬁcantly\nto similarity perception, however, the rhythmic component\nof their stimuli is predominantly low complexity, and the\npitch component of their stimuli is relatively higher on av-\nerage. Notably, this was measured using PSC,RSC, and\nTRC. Although Halpern and Monahan et al. attribute their\nresults to an inherent bias towards rhythm, the results of\nthis experiment suggest that the relative complexity of the\nrhythm and pitch content provides a more robust explana-\ntion.\nAdmittedly, there are several limitations to the gener-\nalization of the results of this study. First and foremost,\nthe observed relationship between dimensional complexity\nand similarity judgements may manifest itself quite differ-\nently when working with longer melodies, or polyphonic\nmusic. Secondly, due to the fact that musical complexity\nis multifaceted and far from understood, determining the\nrelatively low complexity dimension may be quite difﬁcult\nin some contexts. Despite the aforementioned limitations,\nthe limited variance of Eerola et al.’s entropy based com-\nplexity measures provides substantial support for the gen-\neralization of these ﬁndings, as most western music makes\nuse of the same limited collection of distinct note durations\nand pitch classes [16]. As a result, although this form of\nentropy based complexity is the source of some variability\nwithin the musical cannon, redundancy arguably accounts\nfor more of this variation. Consequently, the results of this\nstudy are not restricted to a particular genre, and are rele-\nvant across musical genres.7. CONCLUSION\nSimilarity is shaped by several factors, including familiar-\nity, and cultural conditioning. This study asserts the sig-\nniﬁcance of another factor – the nature of the musical con-\ntent which is being compared – by examining the effects\nof dimensional complexity on similarity judgements. The\ngeneral notion that characteristics of the musical content\nbeing compared have some bearing on the criterion used\nto make similarity judgements, is not new, and has been\nobserved in past experiments [9]. However, the manner in\nwhich musical content establishes a criterion for similarity\njudgements has not been explored previously. The results\nof this study provide evidence that pitch and rhythmic com-\nplexity are factors which shape the criterion used in simi-\nlarity judgements, as the dimension bearing relatively low\ncomplexity information has a greater inﬂuence on similar-\nity perception. Furthermore, the results of this experiment\nare corroborated by previous experiments [7, 11], offering\na general explanation for these previous ﬁndings.\nDeveloping robust and ﬂexible similarity measures con-\ntinues to be a dominant area of research in the MIR do-\nmain, as large digital databases of music information ne-\ncessitate accurate methods for comparison and categoriza-\ntion. As a result, adapting existing similarity measures\nto take dimensional complexity into account, is a possi-\nble application of the ﬁndings of this study. Future re-\nsearch is also necessary to investigate the role of complex-\nity along other dimensions, including dynamics, articula-\ntion and timbre. Furthermore, the manner in which com-\nplexity is percieved along a single dimension is in need of\ncontinued exploration, as several issues with pre–existing\nmethods for measuring complexity have been discussed in\nsection 4.2.1. Clearly, musical similarity is a complex phe-\nnomenon which is deserving of continued exploration, as\nthe results of this experiment have explicitly demonstrated\nthat similarity judgements are dependant on another con-\ntextual factor, the complexity of pitch and rhythm content\nin the musical material being compared.36 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20178. REFERENCES\n[1] N. Cowan. The Magical Mystery Four: How Is Work-\ning Memory Capacity Limited, and Why? Current Di-\nrections in Psychological Science , 19(1):51–57, 2010.\n[2] W. J. Dowling. Scale and contour: Two components of\na theory of memory for melodies. Psychological Re-\nview, 85(4):341–354, 1978.\n[3] T. Eerola, T. Himberg, P. Toiviainen, and J. Louhivuori.\nPerceived complexity of western and African folk\nmelodies by western and African listeners. Psychology\nof Music , 34(3):337–371, 2006.\n[4] P. Fraisse. Rhythm and Tempo. In Diana Deutsch, ed-\nitor, The Psychology of Music , pages 149–181. Aca-\ndemic Press, New York, 1982.\n[5] R. I. Godøy, A. R. Jensenius, and K. Nymoen. Chunk-\ning in music by coarticulation. Acta Acustica united\nwith Acustica , 96(4):690–700, 2010.\n[6] R. L. Goldstone. Learning to perceive while perceiving\nto learn. In R Kimchi, M Behrmann, and C Olson, edi-\ntors, Perceptual Organization in Vision: Behavioural\nand Neural Perspectives , pages 233–278. Lawrence\nErlbaum Associates, New Jersey, 2003.\n[7] A. R. Halpern. Perception of structure in novel music.\nMemory & cognition , 12(2):163–170, 1984.\n[8] E. E. Hannon and S. E. Trehub. Metrical cate-\ngories in infancy and adulthood. Psychological Sci-\nence, 16(1):48–55, 2005.\n[9] A. Lamont and N. Dibben. Motivic Structure and the\nPerception of Similarity. Music Perception: An Inter-\ndisciplinary Journal , 18(3):245–274, 2001.\n[10] D. O. Maid ´ın. A geometrical algorithm for melodic dif-\nference. Computing in musicology: a directory of re-\nsearch , (11):65–72, 1998.\n[11] C. B. Monahan and E. C. Carterette. Pitch and Duration\nas Determinants of Musical Space. Music Perception:\nAn Interdisciplinary Journal , 3(1):1–32, 1985.\n[12] C. Neuhaus, T. R. Kn ¨osche, and A. D. Friederici. Sim-\nilarity and repetition: An ERP study on musical form\nperception. Annals of the New York Academy of Sci-\nences , 1169:485–489, 2009.\n[13] I. Peretz and M. Coltheart. Modularity of music pro-\ncessing. Nature neuroscience , 6(7):688–691, 2003.\n[14] L. Pollard-Gott. Emergence of Thematic Concepts in\nRepeated Listening to Music. Cognitive psychology ,\n15(1):66–94, 1983.\n[15] J. B. Prince. Contributions of pitch contour, tonality,\nrhythm, and meter to melodic similarity. Journal of ex-\nperimental psychology. Human perception and perfor-\nmance , 40(6):2319–37, 2014.[16] J. B. Prince and P. Q. Pfordresher. The role of pitch\nand temporal diversity in the perception and production\nof musical sequences. Acta Psychologica , 141(2):184–\n198, 2012.\n[17] J. B. Prince, M. A. Schmuckler, and W. F. Thompson.\nThe effect of task and pitch structure on pitch-time in-\nteractions in music. Memory & cognition , 37(3):368–\n381, 2009.\n[18] T. W. Reiner. Pitch-distance and contour complexity in\nthe recognition of short melodies. Journal of Scientiﬁc\nPsychology , (September):27–36, 2011.\n[19] B. S. Rosner and L. B. Meyer. The perceptual roles of\nmelodic process, contour, and form. Music Perception ,\n4(1):1–39, 1986.\n[20] P. A. Russell. Memory for music: A study of musi-\ncal and listener factors. British Journal of Psychology ,\n78(3):335–347, 1987.\n[21] E. Schmider, M. Ziegler, E. Danay, L. Beyer, and\nM. B ¨uhner. Is It Really Robust?: Reinvestigating the\nrobustness of ANOV A against violations of the normal\ndistribution assumption. Methodology , 6(4):147–151,\n2010.\n[22] E. Schubert and C. Stevens. The effect of implied har-\nmony, contour and musical expertise on judgments of\nsimilarity of familiar melodies. Journal of New Music\nResearch , 35(2):161–174, 2006.\n[23] C.E. Shannon. A Mathematical Theory of Com-\nmunication. The Bell System Technical Journal ,\n27(July):379–423, 1948.\n[24] C. Taher, R. Rusch, and S. McAdams. Effects of Rep-\netition on Attention in Two-Part Counterpoint. Music\nPerception , 33(3):306–318, 2016.\n[25] A. S. Tanguiane. Artiﬁcial Perception and Music\nRecognition . Springer-Verlag, Berlin, 1993.\n[26] A. S. Tanguiane. A Principle of Correlativity of Percep-\ntion and Its Application to Music Recognition. Music\nPerception: An Interdisciplinary Journal , 11(4):465–\n502, 1994.\n[27] W. F. Thompson, M. D. Hall, and J. Pressing. Illu-\nsory conjunctions of pitch and duration in unfamiliar\ntone sequences. J Exp Psychol Hum Percept Perform ,\n27(1):128–140, 2001.\n[28] E. Thul. Measuring the Complexity of Musical Rhythm .\nPhD thesis, McGill University, 2008.\n[29] G. T. Toussaint. A comparison of rhythmic similar-\nity measures. Proc. International Conference on Music\nInformation Retrieval (ISMIR 2004) , pages 242–245,\n2004.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 37[30] A. V olk, E. Chew, E. Hellmuth Margulis, and C. Anag-\nnostopoulou. Music Similarity: Concepts, Cognition\nand Computation. Journal of New Music Research ,\n45(3):207–209, 2016.\n[31] A. V olk and P. Kranenburg. Melodic similarity among\nfolk songs: An annotation study on similarity-based\ncategorization in music. Musicae Scientiae , 16(3):1–\n23, 2012.\n[32] R. L. Welker. Abstraction of Themes from Melodic\nVariations. Journal of Experimental Psychology. Hu-\nman Perception and Performance , 8(3):435–447, 1982.\n[33] J. O. Wobbrock, L. Findlater, D. Gergle, and J. J. Hig-\ngins. The aligned rank transform for nonparametric\nfactorial analyses using only ANOV A procedures. Pro-\nceedings of the SIGCHI Conference on Human Factors\nin Computing Systems , pages 143–146, 2011.38 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Ranking-Based Emotion Recognition for Experimental Music.",
        "author": [
            "Jianyu Fan",
            "Kivanç Tatar",
            "Miles Thorogood",
            "Philippe Pasquier"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417475",
        "url": "https://doi.org/10.5281/zenodo.1417475",
        "ee": "https://zenodo.org/records/1417475/files/FanTTP17.pdf",
        "abstract": "Emotion recognition is an open problem in Affective Computing the field. Music emotion recognition (MER) has challenges including variability of musical content across genres, the cultural background of listeners, relia- bility of ground truth data, and the modeling human hear- ing in computational domains. In this study, we focus on experimental music emotion recognition. First, we present a music corpus that contains 100 experimental music clips and 40 music clips from 8 musical genres. The dataset (the music clips and annotations) is publicly available at: http://metacreation.net/project/emusic/. Then, we present a crowdsourcing method that we use to collect ground truth via ranking the valence and arousal of music clips. Next, we propose a smoothed RankSVM (SRSVM) method. The evaluation has shown that the SRSVM out- performs four other ranking algorithms. Finally, we ana- lyze the distribution of perceived emotion of experi- mental music against other genres to demonstrate the dif- ference between genres.",
        "zenodo_id": 1417475,
        "dblp_key": "conf/ismir/FanTTP17",
        "keywords": [
            "Emotion recognition",
            "Affective Computing",
            "Music emotion recognition",
            "Variability of musical content",
            "Cultural background",
            "Reliability of ground truth",
            "Crowdsourcing method",
            "Smoothed RankSVM (SRSVM)",
            "Evaluation results",
            "Distribution of perceived emotion"
        ],
        "content": "RANKING-BASED EMOTION RECOGNITION FOR EXPERIMENTAL MUSIC  Jianyu Fan, Kıvanç Tatar, Miles Thorogood, Philippe Pasquier   Simon Fraser University Vancouver, Canada jianyuf, ktatar, mthorogo, pasquier@sfu.ca  ABSTRACT Emotion recognition is an open problem in Affective Computing the field. Music emotion recognition (MER) has challenges including variability of musical content across genres, the cultural background of listeners, relia-bility of ground truth data, and the modeling human hear-ing in computational domains. In this study, we focus on experimental music emotion recognition. First, we present a music corpus that contains 100 experimental music clips and 40 music clips from 8 musical genres. The dataset (the music clips and annotations) is publicly available at: http://metacreation.net/project/emusic/. Then, we present a crowdsourcing method that we use to collect ground truth via ranking the valence and arousal of music clips. Next, we propose a smoothed RankSVM (SRSVM) method. The evaluation has shown that the SRSVM out-performs four other ranking algorithms. Finally, we ana-lyze the distribution of perceived emotion of experi-mental music against other genres to demonstrate the dif-ference between genres.  1. INTRODUCTION The research in MER proposes computational approaches to recognize the emotion of music. The increasing num-bers of MER studies in recent years have been focusing on particular musical genres, such as classical music, pop, rock, jazz, and blues [41]. So far, to our knowledge, MER in experimental music has yet to be explored.  The definition and use of the term experimental music have been an ongoing discussion within the last century.   John Cage [15] clarifies the action of experimentalism as “the outcome of which is not foreseen”. Demers [17] de-fined experimental as “anything that has departed signifi-cantly from norms of the time…” [p.7] and continues by the two assumptions of “…that experimental music is dis-tinct from and superior to a mainstream-culture industry and that culture and history determine aesthetic experi-ence\" [p.139]. Experimental music does not only rely on harmony and melody [6]. Experimental music explores the continuum between rhythm, pitch, and noise; the no-tion of organized sound; the expansion of temporal field; and the morphologies of sound. In this study, our defini-tion of experimental music encompasses experimental electronic music such as acousmatic music, electroacous-tic music, noise music, soundscape compositions as well as experimental music with acoustic instruments such as free improvisation or improvised music. We also include Contemporary Art practices that use sound as a medium in our definition of experimental music.  There are many applications in which a computational model of MER for experimental music would be benefi-cial. MER computational models can be used in the sys-tem architecture of Musical Metacreation (MuMe) sys-tems for experimental music. MuMe is the partial or complete automation of musical tasks [34]. A variety of MuMe systems apply machine listening. Machine listen-ing is the computational modeling of the human hearing. In that sense, a computational model for MER in experi-mental music can be useful to design a machine listening algorithm for a MuMe system. Moreover, we can use computational MER models in the analysis of experi-mental music works. Also, we can design mood enabled recommendation systems for experimental music albums using a MER model for experimental music.  Still, MER has several challenges. First, music percep-tion can be dramatically different if listeners are from dif-ferent regions of the world and have various unique cul-tural backgrounds [5,18]. Second, it is difficult for re-searchers to collect ground truth data to cover a wide range of population that well distributed in different parts of the world [5]. Third, in the previous studies, research-ers designed listening tests that asked participants to an-notate the music pieces by rating their emotion perception of the music pieces [41,49]. However, the cognitive load of rating emotion is heavy for participants [9]. This causes the low-reliability of the annotations [19,44]. Fourth, the level of participant’s agreement on the emo-tion of a music clip varies because the perception of mu-sic is subjective. Even for one individual, the ratings can change during a day [49]. Fifth, in the case of experi-mental music emotion recognition, there is no annotated dataset available. The current MIREX MER task is the case of pop music emotion recognition. To overcome these difficulties, we designed a ranking-based experiment to collect ground truth annotations based on a crowdsourcing method. Crowdsourcing meth-od is to elicit a large amount of data from a large group of people from online communities [8]. Our ground truth annotations were gathered from 823 annotators from 66 countries, which covers diverse cultural backgrounds. Then, to reduce the cognitive load, we used a ranking-based method to ask participants to do pairwise compari-sons between experimental music clips. The ranking \n © Jianyu Fan, Kıvanç Tatar, Miles Thorogood, Philippe Pasquier. Licensed under a Creative Commons Attribution 4.0 Interna-tional License (CC BY 4.0). Attribution: Jianyu Fan, Kıvanç Tatar, Miles Thorogood, Philippe Pasquier. “Ranking-Based Emotion Recogni-tion for Experimental Music”, 18th International Society for Music In-formation Retrieval Conference, Suzhou, China, 2017. 368   based approach only needs relative comparisons instead of absolute ratings. This improves the objectiveness of the ground truth data. We applied the Quicksort algorithm to select comparisons during the data collection stage to reduce the workload (see Section 4.1). Then, we proposed a SRSVM method and compared it with other ranking algorithms. The results show that SRSVM is better than four other ranking algorithms regarding experimental music emotion recognition.  The database, containing the 140 music clips and the annotations, can be freely downloaded at http://metacreation.net/project/emusic/. We believe that public release of such a dataset will foster research in the field and benefit MER communities. The main contribu-tions of this paper are thus four-fold:  • We provide a music corpus, EMusic. The corpus includes 100 experimental music clips and 40 mainstream music clips. • We use a crowdsourcing method to collect the pairwise ranking data for experimental music clips, and share an annotated experimental music dataset. • We proposed the SRSVM method for experimental music emotion recognition and compared our ap-proach with other ranking algorithms. • We compared the annotations of experimental mu-sic with that of other music genres.  2. RELATED WORKS The Music Information Research Evaluation eXchange (MIREX) community evaluates systems for Audio Music Mood Classification every year. Studies have been classi-fied into two major categories based on the model of emotion: categorical and dimensional approaches.  2.1 Categorical Approaches in MER Categorical MER approaches use discrete affect models to estimate emotion. Discrete affect models propose that we can describe all emotions using a set of basic emo-tions. These basic emotion categories are happiness, sad-ness, fear, anger and disgust [22, 33], shame, embarrass-ment, contempt and guilt [3], as well as exuberance, anx-ious/frantic and contentment [32]. There is still no con-sensus on the discrete emotion categories of music [32].  In the previous studies with categorical MER ap-proaches, researchers conducted experiments to collect the ground truth annotations. Then, researchers used the audio features of music clips with classification methods to model the relationship between audio features and emotion categories [23, 45, 46]. 2.2 Dimensional Approaches in MER Dimensional affect models use a Cartesian space with continuous dimensions to represent emotions [7,14,40,48]. The simplest dimensional affect model has two dimen-sions: valence and arousal. Other dimensional affect models with additional dimensional such as tension, po-tency, and dominance have also been proposed in the lit-erature [32]. MER studies use dimensional affect models to compute continuous values that represent the emotion of audio samples. These studies focus on continuous ma-chine learning models such as regression models. Re-searchers gather the ground truth data by conducting an evaluation experiment in which the participants label the emotion music clips on a dimensional affect grid.  2.3 Rating or Ranking Affective ratings instruments have been used for collect-ing affective annotations. Researchers have used such tools in video emotion recognition [27, 30], music emo-tion recognition [11], speech emotion recognition [35], soundscape emotion recognition [20] and movement emotion recognition [43]. However, recent studies show that rating based experiments have limitations and fun-damental flaws [13]. Rating-based experiments neglect the existence of interpersonal differences on the rating process. In addition, rating emotion in a continuum is dif-ficult because annotators tend to score the samples based on the previous ratings instead of their non-biased feel-ings [44]. Yang and Lee indicated that the rating-based approach imposes a heavy cognitive load on the subjects [48]. Moreover, the contextual situation of annotators can affect the consistency of ratings [12]. Ranking has been an alternative approach for eliciting responses from subjects [9, 39, 48]. Metallinou and Na-rayanan found that there is a higher Inter-annotator relia-bility when people were asked to describe emotions in relative terms rather than in absolute terms [2]. Yannaka-kis et al. also showed that the inter-rater agreement of the ordinal data is significantly higher than that of the nomi-nal data [12].  Yang and Chen designed a ranking-based experiment to collect ground truth data and build a ranking model recognize the perceived emotion of pop music [9]. The result showed that the ranking-based approach simplifies the annotation process and enhances the Inter-annotator reliability. Hence, we designed a ranking-based method to for experimental music emotion recognition, where annotators made pairwise comparisons between two au-dio clips based on valence and arousal. 2.4 Emotion Taxonomy According to previous studies [1, 24], two types of emo-tions are at play when listening to music. • Perceived emotion: Emotions that are communicat-ed by the source. • Induced emotion: Emotional reaction that the source provokes in listeners. The perceived emotion is more abstract and objective. It is the emotion the source conveys. The perceived emotion of happy songs is always “happy”. However, the induced emotion is more subjective. The same happy music may not necessarily induce happiness in the listener. In this study, we focus on the perceived emotion of music clips because it is more objective. 3. DATA COLLECTION To build a MER system for experimental music, we first built an experimental music corpus: EMusic. Then, we collected emotion annotations using a crowdsourcing method.   Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 369   3.1 Corpus Construction In EMusic corpus, there are 100 experimental music clips and 40 music clips from 8 musical genres, including blues, classical, country, electronic, folk, jazz, pop and rock. The 100 experiment music clips are extracted from 29 experimental music pieces, which are high quality works of Electroacoustic music. The 40 music clips are selected from 1000 songs database [29]. We segmented these compositions using multi-granular novelty segmen-tation [31] provided in the MIRToolbox [32]. Using this automatic segmentation method, we ensure that each segment is consistent. Then, we manually chose novel clips to create a homogeneous and consistent corpus that would not disturb the listeners. A 0.1 seconds fade in/out effect has been added to each audio clip. Music clips are converted to a format in wav (44100 Hz sampling frequency, 32 bits precision and mono channel). All the audio samples are normalized. Regard-ing the duration, Xiao et al. [50] showed that the use of six to eight seconds is good for presenting stable mood for classical music segments. Fan et al. [19] indicated that the duration of six seconds is long enough for soundscape emotion recognition. Following the previous study, we aimed for the average duration of 6 seconds in this exper-iment (Mean: 6.20s, Std: 1.55s). The duration of clips varies because of the automatic segmentation by novelty.  3.2 Select Comparisons To create a robust set of annotations, we need multiple annotations per pairwise comparison of audio clips. Baveyes et al. [44] found that collecting three annotations per comparison is a good compromise between the cost and the accuracy of the experiment. Therefore, we follow this approach for its feasibility within our experiment. To efficiently create pairwise comparisons presented to the listeners, we use a Quicksort algorithm [44]. For the first iteration of the algorithm, we select one audio sample as the pivot. All remaining clips are to be com-pared with the pivot so that the algorithm generates 139 comparisons. We then collect three annotations for each comparison and determine the result to be the one that provided by at least two annotators. In the case that we did not select a pivot that has the lowest or the highest valence or arousal, we end up with two separate sets after the first iteration. Therefore we repeatedly select a new pivot in each set until each audio clip received a rank of valence and a rank of arousal from 1 to 140. The compu-tational complexity of the Quicksort algorithm is O(NlogN). 3.3 Online Experiment We conduct an online experiment to annotate our corpus of experimental music clips with affective labels. We used the CrowdFlower1 platform to crowd source annota-tions from people online. To sort the 140 music clips based on valence and arousal independently, we launched one task for valence and another task for arousal.                                                              1 https://www.crowdflower.com/     Figure 1. The interface of crowdsourcing study. At the beginning of the annotation process, subjects are provided with the terminology of arousal and v                dalence. In our experiment, we used valence to describe perceived  pleasantness of the sound. We provided subjects with the Self-Assessment Manikin [28] at the beginning of the task to make sure the task was understood. The Self-Assessment Manikin is a pictorial system used in experi-ments to represent emotional valence and arousal axes. Its non-verbal design makes it easy to use regardless of age, educational or cultural background. We modified the pic-torial system by adding arrows to inform annotators that we were collecting perceived emotion. We requested annotators to follow a tutorial to get fa-miliar with the annotation interface. Annotators were no-tified that they were required to use headphones to listen to the audio clips. We asked them to turn the volume up to a comfortable level given a test signal. Annotators were then presented with a quiz, where 5 gold standard comparisons were provided. These comparisons were easily comparable regarding valence and arousal, which were carefully selected by experts. The annotators could continue to the task only if they achieve an 80% of accu-racy in the quiz. To ensure the quality of the annotations, we tracked annotators’ performance by inserting gold standard com-parisons throughout the tasks. Similar to the comparisons in the quiz, these 5 comparisons were easily comparable regarding valence and arousal. If their answers were not the same as the default answer, they would be noticed by a pop out window. If they had strong reason to explain their answer, they could message the reason to us. This also affects annotators’ reputation on the CrowdFlower. \n370 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   Annotators could listen repeatedly to an audio clip. Af-ter an annotator had listened to both audio clips, the op-tion to enter the response was presented in the form of an input button. For easing the fatigue that increases natural-ly during manual data annotation [2], they could pause the annotation process at any time and continue at a later stage. The volume control bar was disabled so that anno-tators could not adjust the individual volumes themselves. An annotator had to rank 5 pairs of clips before being paid US$0.05 and was able to exit the task at any time.  3.4 Annotation Results A total of 823 annotators performed the task from 66 dif-ferent countries. Most of the workers are Venezuelans (31.71%), Brazilian (6.93%), Serbian (6.44%), Russian (5.95%) and Bosnians (5.10%). The annotators were from the world population and it is unlikely they have a back-ground in experimental music. This avoids the potential bias brought by experts.  Each pair was displayed to annotators until three anno-tations are collected for this pair. 823 annotators provided 2817 comparisons for arousal and 2445 comparisons for valence. The 823 trusted annotators had an average accu-racy of 91.81% in the quiz. Annotators took approximate-ly 13s to perform a task. This also proves that annotators carefully listened to both music clips.    Categories Arousal Valence Percent Agreement 0.839 0.801 Krippendorff’s 𝛼 0.360 0.222 Table 1. Inter-annotator reliability. We evaluate the Inter-annotator reliability based on percent agreement and Krippendorff’s 𝛼. Percent agree-ment calculates the ratio between the number of annota-tions that are in agreement and the total number of anno-tations. However, percent agreement overestimates inter-annotator reliability because it does not consider the agreement expected by chance. Krippendorff’s 𝛼 is more flexible and allows missing data (comparisons can be an-notated by any number of workers). Thus, no compari-sons are discarded to compute this measure. Their values can range from 0 to 1 for Percent agreement and from -1 to 1 for Krippendorff’s alpha.  In Table 1, the inter-annotator reliability is similar to other emotion studies [30, 44]. The percent agreement indicates that annotators agreed on 83.9% and 80.1% of comparisons. The value of Krippendorff’s 𝛼 is between 0.21 to 0.40, which indicates a fair level of agreement.  4. LEARN TO RANK 4.1 Standard Ranking Algorithms The state-of-the-art ranking algorithms can be three cate-gories: the pointwise approach [42], the pairwise ap-proach [36] and the listwise approach [10]. The pointwise approach learns the score of the samples directly. The pointwise approach takes one train sample at a time and trains a classifier/regressor based on the loss of the single sample. The pairwise approach solves the ranking prob-lems by using a pair of samples to train and provides an optimal ordering for the pair. Listwise methods try to minimize the listwise loss by evaluating the whole rank-ing list. Each ranking algorithm assigns a ranking score to each sample, and rank the sample based on the score.  In the following, we introduce five ranking algorithms: ListNet, Coordinate Ascent, RankNet, RankBoost and RankSVM. ListNet is a listwise ranking algorithm [10], which uses neural networks to predict the ranking score. The algorithm calculates the probability of the sample ranking within top-k, and computes the difference be-tween the probability distribution of predicted ranks and ground truth data based on cross entropy. Coordinate As-cent algorithm is a gradient-based listwise method for multi-variate optimization [16]. It directly optimizes the mean of the average precision scores for each ranking. RankNet is a pairwise ranking algorithm, which predicts the ranking probability of a pair of samples <A, B>. If sample A receives a higher ranking score than that of sample B, then the object probability 𝑃!\" equals 1, oth-erwise, 𝑃!\" equals 0. The loss function of RankNet is the cross-entropy between the predicted probability and the object probability. RankBoost is another pairwise ranking algorithm [47]. It replaces training samples with pairs of samples to learn the association between samples. RankSVM is a common pairwise method extended from support vector machines [36]. The difference between features vectors of a pair of training samples can be trans-formed to a new feature vector to represent the pair. RankSVM converts a ranking task to a classification task. 4.2 Searching Strategies Given a test sample, a ranking model provides a ranking score regarding valence/arousal. A ranking score is a real number. To obtain the predicted rank of the test sample based on the ranking score, we used two search strategies: one-by-one search and smoothed binary search.  4.2.1 One-by-One Search First, we obtain predicted ranking scores of the entire training set and the test sample. Then, we sorted all clips by ranking score to obtain the predicted ranking of the test sample. Ties are unlikely to happen since we set the value of the score retains 6 digits after the decimal point.  4.2.2 Smoothed Binary Search Smoothed binary search compares the ranking score of a test sample with the ranking scores of pivots selected from the training set to find the rankings of a test sample along the valence/arousal axis. We add a smoothed win-dow to traditional binary by selecting a group of pivots instead of one pivot. Following is the description of the smoothed binary search: • Given a test sample, pick an odd number of clips from the training set that are consecutive on the va-lence/arousal axis as pivots. The odd number of clips avoids the ties. The group of pivots has the medium value of valence/arousal among the subset.  • Predict the ranking score for the group of pivots and the test sample, and compare their ranking score. The test sample with a score of less than half of the pivots comes before the pivots, while the test Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 371   sample with a score greater than half of the pivots comes after pivots.  • Recursively apply the above steps until the size of subsets is 2. The average ranking of these two train-ing samples is the predicted rankings. 4.3 SRSVM We propose the SRSVM for experimental music emotion recognition. The training of SRSVM is the same as standard RankSVM. During the testing/ranking stage, SRSVM finds the predicted ranking of the test sample based on the smoothed binary search.  5. PERFORMANCE ANALYSIS 5.1 Features Selection We began with a feature set including rms, brightness, loudness, spectral slope, spectral flux, spectral rolloff, attack leap, regularity, pulse clarity, hcdf, inharmonicity, perceptual sharpness, pitch, key, tempo, and 12 MFCCs. We used 23-ms analysis windows and calculated the mean and standard deviation to represents signals as the long-term statistical distribution of local spectral features, which ended up with a 56-dimension feature vector [21].  We used MIRToolbox [32] and YAAFE [4] libraries to extract audio features.   Selected Features Mean of Root Mean Square  Standard deviation of Root Mean Square Standard deviation of Brightness Mean of MFCC 1 Standard deviation of MFCC 2 Standard deviation of MFCC 8 Mean of MFCC 12 Mean of Hcdf Mean of Loudness Standard deviation of Loudness  Mean of Regularity Table 2. Selected features for predicting valence/arousal   Before training the model, we build a feature selector that removes all low-variance features over the entire corpus to select a subset of discriminative features. The threshold of variance is 0.02, which is chosen as a heuris-tic value. This step kept 43 features out of 56 features. Then, we used a random forests method, which has ten randomized decision trees to evaluate the importance of features based on the Gini impurity index. We ended up having an 11-dimensional feature vector (see Table. 2). Because our dataset includes 100 experimental music clips and 40 clips belong to other genres, we tested the ranking algorithms using the whole dataset and the subset of experiment music separately.  5.2 Comparing with Ranking Algorithms We evaluate the ranking algorithms of experimental MER using Goodman-Kruskal gamma (G). Goodman-Kruskal gamma measures the association between the predicted rankings and the ground truth annotations [37, 38]. G de-pends on two measures: the number of pairs of cases ranked in the same order on both variables (number of concordant, 𝑁!) and the number of pairs of cases ranked in reversed order on both variables (number of discordant, 𝑁!). G ignores ties. In our experiment, we had no ties. G is close to 1 indicate strong agreement, -1 for total disa-greement, and 0 if the rankings are independent. G=NS−NDNS+ND                                   (1) We used the leave-one-out validation method to com-pare the SRSVM with ListNet, RankNet, Coordinate As-cent, and RankBoost. For a given test sample, ranking algorithms output a predicted valence/arousal score. To obtain the predicted rankings of the whole test set, we used one-by-one searching strategy and smoothed binary search strategy. Then, we measured the gamma between the predicted rankings and the ground truth annotation. As we can see from Table 3, when we use SRSVM, we obtain the best performance when the windows size is three samples (G: 0.733, p < 0.001). When the window size is 1, the test sample will be compared with one pivot iteratively until it falls into a small interval. This becomes a standard binary search. After adding a smoothed win-dow, the test sample is compared with a group of pivots. This increases the accuracy of predicting whether the test sample is larger or smaller than the pivots.  Algorithm One-by-One Search Smoothed Binary Search  (Number of samples) 1 3 5 ListNet 0.044 0.088 0.057 0.022 RankNet 0.096 0.386 0.269 0.255 Coordinate Ascent 0.191 0.436 0.387 0.486 Rank-Boost 0.619 0.679 0.697 0.717 RankSVM 0.398 0.690 0.733 SRSVM 0.697 SRSVM Table 3. Goodman-Kruskal gamma of ranking algorithms for arousal recognition using the whole dataset Method One-by-One Search Smoothed Binary Search  (Number of samples) 1 3 5 ListNet   0.015 0.002 0.049 0.002 RankNet 0.063 0.155 0.055 0.260 Coordinate Ascent 0.016 0.130 0.195 0.254 Rank-Boost 0.438 0.467 0.345 0.440 RankSVM 0.333 0.490 0.573 SRSVM 0.556 SRSVM Table 4. Goodman-Kruskal gamma of ranking algorithms for valence recognition using the whole dataset 372 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   When using the whole dataset, the valence recognition is harder than arousal recognition. However, the SRSVM still obtains the best performance (G: 0.573, p < 0.001).  Method One-by-One Search Smoothed Binary Search  (Number of samples) 1 3 5 ListNet 0.001 0.037 -0.013 0.013 RankNet 0.110 0.096 0.242 0.299 Coordinate Ascent 0.237 0.515 0.519 0.556 Rank-Boost 0.698 0.741 0.740 0.748 RankSVM 0.300 0.776 0.801 SRSVM 0.776 SRSVM Table 5. Goodman-Kruskal gamma of ranking algorithms for arousal recognition using the subset that only contains experimental music clips.    As Table 5 shows, when we only consider experi-mental music, the Gamma statistic of SRSVM for arousal recognition has the best result (G: 0.801, p < 0.001). The results of the experimental music case are better than the results of the case including clips of all genres.   Method One-by-One Search Smoothed Binary Search  (Number of samples) 1 3 5 ListNet 0.115 0.037 -0.012 0.036 RankNet 0.058 0.116 0.246 0.277 Coordinate Ascent 0.067 0.100 0.131 0.106 Rank-Boost 0.167 0.236 0.279 0.346 RankSVM 0.434 0.570 0.795 SRSVM 0.628 SRSVM Table 6. Goodman-Kruskal gamma of ranking algorithms for valence recognition using the subset that only contains experimental music clips.     Table 6 shows that when we only consider experi-mental music, the Gamma statistic of SRSVM for va-lence recognition (G: 0.795, p < 0.001) is significantly higher than using the whole dataset.  From Table 3-6, we can see the best performing model is SRSVM with 3 samples as the smoothed window. The second best performing model is SRSVM with 5 samples as the smoothed window. This result implies that a good emotion-recognition can be obtained by using SRSVM. 5.3 Comparing between Experimental Music and Other Genres We convert the rankings to ratings to visualize the distri-bution of the ranking data. This illustration has two as-sumptions. First, the distances between two successive rankings are equal. Second, the valence and arousal are in the range of [-1.0, 1.0].  Figure 2. The distribution of the ground truth annota-tions, the green dots represent experimental music clips From Figure 2, it can be observed that other genres have both higher perceived valence and arousal compar-ing to experimental music. Because we have only 5 sam-ples per genre, we need to have a large ground truth da-taset to prove that assumption. The figure also shows the negative correlation between valence and arousal of ex-perimental music clips. To test this, we run a Pearson cor-relation test on the ground truth data. Our Pearson corre-lation coefficient is -0.3261, which indicates there is a weak negative correlation between the two dimensions. 6. CONCLUSIONS AND FUTURE WORKS We present an annotated dataset for experimental music emotion recognition. 140 music clips are ranked along the valence and arousal axis through a listening experiment. It is available at http://metacreation.net/project/emusic/.  We presented a SRSVM method to predict rankings of experimental music clips regarding valence/arousal and compared SRSVM with other ranking method. We also compared the valence and arousal of experimental music with that of the music of other genres, which shows other genres of music have both higher perceived valence and arousal than experimental music. Even with the smaller number of clips, we found other genres have both higher perceived valence and arousal comparing to experimental music. In the future, we plan to compare the perceived emotion of different genres by collecting a larger dataset.  7. REFERENCES [1] A. Kawakami, K. Furukawa, K. Katahira and K. Okanoya, “Sad music induces pleasant emotion,” Front Psychol Vol. 4, No. 311, 2013. [2] A. Metallinou and S. Narayanan, “Annotation and processing of continuous emotional attributes: chal-lenges and opportunities,” IEEE International Con-ference and Workshops on Automatic Face and Ges-ture Recognition, pp. 1–8, 2013. \nProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 373   [3] A. Ortony and T. J. Turner, “What’s basic about basic emotions?” Psychological review. Vol. 97, No. 3, pp. 315-331, 2014. [4] B. Mathieu, S. Essid, T. Fillon, J. Prado, and G. Richard, “Yaafe, an Easy to Use and Efficient Audio Feature Extraction Software,” Proceedings of the International Symposium on Music Information Retrieval, pp. 441–446. 2010. [5] C. J. Stevens, “Music perception and cognition: A review of recent cross-cultural research,” Topics in Cognitive Science, Vol. 4, No. 4, pp. 653– 667, 2012. [6] C. Palombini, “Pierre Schaeffer. 1953: Towards an Experimental Music,” Music and Letters, Vol. 74, No. 4, pp. 542–57, 1993.  [7] D. Liu, L. Lu, and H.-J. Zhang, “Automatic mood detection from acoustic music data,” Proceedings of the International Symposium Music Information Retrieval,, pp. 81–87, 2003. [8] D. McDuff, “Crowdsourcing affective responses for predicting media effectiveness,” Ph.D. Dissertation. Massachusetts Institute of Technology, 2014.  [9] D. Yang and W.-S. Lee, “Disambiguating music emotion using software agents,” Proceedings of the International Conference on Music Information Retrieval, 2004.  [10] F. Xia, T.-Y. Liu, J. Wang, W.-S. Zhang, and H. Li, “Listwise approach to learning to rank: Theory and algorithm,” Proceedings of the IEEE International Conference on Machine. Learning, pp. 1192–1199, 2008. [11] F. Weninger, F. Eyben, and B. Schuller, “On-line continuous-time music mood regression with deep recurrent neural networks,” Proceedings of the IEEE International Conference Acoustics, Speech and Signal Processing, 2014. [12] G. N. Yannakakis and H. P. Matínez, “Grounding Truth via Ordinal Annotation,” Proceedings of the International Conference on Affective Computing and Intelligent Interaction, 2015.  [13] G. N. Yannakakis, H. P. Mart´ınez, “Ratings are overrated!” Frontiers on Human-Media Interaction, Vol. 2, No. 13, 2015. [14] J. A. Sloboda and P. N. Juslin, “Psychological perspectives on music and emotion,” in Music and Emotion: Theory and Research, Oxford University Press, 2001.  [15] J. Cage, Silence: Lectures and Writings, Wesleyan, 1961. [16] J. Chen, C. Xiong, and J. Callan, “An empirical study of learning to rank for entity search,” Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2016. [17] J. Dermers, Listening through the Noise: The Aesthetics of Experimental Electronic Music, Oxford University Press, 2010. [18] J. Fan and M. Casey, “Study of Chinese and UK hit songs prediction,” Proceedings of the International Symposium on Computer Music Multidisciplinary Research, pp. 640–652, 2013. [19] J. Fan, M. Thorogood, P. Pasquier, “Automatic Soundscape Affect Recognition Using A Dimensional Approach,” Journal of the Audio Engineering Society, Vol. 64, No. 9, pp. 646–653, 2016.  [20] J. Fan, M. Thorogood, and P. Pasquier, “Automatic Recognition of Eventfulness and Pleasantness of Soundscape,” Proceedings of the 10th Audio Mostly, 2015.  [21] J. J. Aucouturier and B. Defreville, “Sounds like a park: A computational technique to recognize soundscapes holistically, without source identification,” Proceedings of the International Congress on Acoustics, pp. 621–626, 2009. [22] J. Panksepp: Affective Neuroscience: The  Foundation of Human and Animal Emotions, Oxford University Press, 1998. [23] K. Bischoff, C. S. Firan, R. Paiu, W. Nejdl, C. Laurier, and M. Sordo, “Music mood and theme classification—a hybrid approach,” Proceedings of the International Conference on Music Information Retrieval, pp. 657-662, 2009. [24] K. Kallinen and N. Ravaja, N, “Emotion perceived and emotion felt: Same and different,” Musicae Scientiae, Vol. 5, No. 1, pp. 123-147, 2006. [25] K. Svore, L. Vanderwende, and C. Burges, “Enhancing single-document summarization by combining RankNet and third-party sources,” Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 448–457, 2007. [26] L. A. Goodman and W. H. Kruskal, “Measures of Association for Cross Classifications,” Journal of the American Statistical Association. Vol. 49, No. 268, pp-732–764, 1954.  [27] L. Devillers, R. Cowie, J.-C. Martin, E. Douglas-Cowie, S.  Abrilian, and M. McRorie, “Real life emotions in French and English TV video clips: an integrated annotation protocol combining continuous and discrete approaches,” Proceedings of the International conference on Language Resources and Evaluation, 2006.  374 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   [28] M. M. Bradley and P. J. Lang, “Measuring emotion: the self- assessment manikin and the semantic dif-ferential,” Journal of behavior therapy and experi-mental psychiatry, Vol. 25, No. 1, pp. 49–59, 1994. [29] M. Soleymani, M. N. Caro, E. M. Schmidt, C.-Y. Sha, and Y.-H. Yang, “1000 songs for emotional analysis of music,” Proceedings of the 2nd ACM In-ternational Workshop on Crowdsourcing for Multi-media, pp. 1–6, 2013. [30] N. Malandrakis, A. Potamianos, G. Evangelopoulos, and A. Zlatintsi, “A supervised approach to movie emotion tracking,” Proceedings of the IEEE Interna-tional Conference Acoustics, Speech and Signal Processing, pp. 2376–2379, 2011 [31] O. Lartillot, D. Cereghetti, K. Eliard, and D. Grandjean, “A simple, high-yield method for assess-ing structural novelty,” Proceedings of the 3rd Inter-national Conference on Music & Emotion, 2013 [32] O. Lartillot, P. Toiviainen, and T. Eerola, “A Matlab Toolbox for Music Information Retrieval,” in: C. Preisach, H. Burkhardt, L. Schmidt-Thieme, P. D. R. Decker, (Eds), Data Analysis, Machine Learning and Applications. Studies in Classification, Data Analysis, and Knowledge Organization, pp. 261–268, Springer, Berlin, Heidelberg, 2008.  [33] P. Ekman, “An argument for basic emotions,” Cognition and Emotion. Vol. 6, No. 3, pp.169–200, 1992. [34] P. Pasquier, A. Eigenfeldt, O. Bown, and S. Dubnov, “An Introduction to Musical Metacreation,” ACM Computers In Entertainment, Special Issue: Musical Metacreation, Vol. 14, No. 2, 2016.  [35] R. Elbarougy and M. Akagi, “Speech Emotion Recognition System Based on a Dimensional Approach Using a Three-Layered Model,” Proceedings of the Signal & Information Processing Association Annual Summit and Conference, 2012. [36] R. Herbrich, T. Graepel, and K. Obermayer, “Support vector learning for ordinal regression,” Proceedings of International Conference on. Artificial Neural Network, 1999. [37] R. Morris, “Crowdsourcing workshop: The emergence of affective crowdsourcing,” Proceedings of the Annual Conference Extended Abstracts on Human Factors in Computing Systems, 2011. [38] R. Morris and D. McDuff, “Crowdsourcing techniques for affective computing,” in R.A. Calvo, S.K. DMello, J. Gratch and A. Kappas (Eds). Handbook of Affective Computing, Oxford University Press, 2014.  [39] S. Ovadia, “Ratings and rankings: Reconsidering the structure of values and their measurement,” International Journal of Social Research Methodology, Vol. 7, No. 5, pp. 403–414, 2004. [40] T. Eerola, O. Lartillot, and P. Toiviainen, “Prediction of multidimensional emotional ratings in music from audio using multivariate regression models,” Proceedings of the International Symposium Music Information Retrieval, pp. 621–626, 2009. [41] T. Eerola, Tuomas, and J. K. Vuoskoski. “A Review of Music and Emotion Studies: Approaches, Emotion Models, and Stimuli,” Music Perception: An Interdisciplinary Journal, Vol. 30, No. 3, pp. 307–340, 2013.  [42] T. Y. Liu, “The Pointwise Approach,” in Learning to rank for information retrieval. Berlin: Springer-Verlag Berlin and Heidelberg GmbH & Co. K, 2011. [43] W. Li, P. Pasquier, “Automatic Affect Classification of Human Motion Capture Sequences in the Valence-Arousal Model,” Proceedings of the International Symposium on Movement and Computing, 2016. [44] Y. Baveye, E. Dellandrea, C. Chamaret, and L. Chen, “LIRIS-ACCEDE: A video database for affective content analysis,” IEEE Transactions on Affective Computing, Vol. 6, No. 1, pp. 43–55, 2015. [45] Y. Feng, Y. Zhuang, and Y. Pan, “Popular music retrieval by detecting mood,” Proceedings of the International Conference on Information Retrieval, pp. 375–376, 2013.   [46] Y. E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton, P. Richardson, J. Scott, J. A. Speck, and D.  Turnbull, “Music emotion recognition: A state of the art review,” Proceedings of the International Conference on Music Information Retrieval, 2010. [47] Y. Freund, R. Iyer, R. Schapire, and Y. Singer, “An efficient boosting algorithm for combining preferences,” Proceedings of the International Conference on Machine Learning, pp. 170–178, 1998. [48] Y.-H. Yang and H. Chen, “Ranking-based emotion recognition for music organization and retrieval,” IEEE Transactions on Audio, Speech, and Language Processing, Vol. 19, No. 4, pp. 762– 774, 2011.  [49] Y.-H. Yang and H.-H. Chen, “Machine recognition of music emotion: A review,” ACM Trans. Intel. Systems & Technology, Vol. 3, No. 3, 2012. [50] Z. Xiao, E. Dellandrea, W. Dou, and L. Chen, “What is the best segment duration for music mood analysis?” Proceedings of the International Workshop on Content-Based Multimedia Indexing, pp. 17–24, 2008. Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 375"
    },
    {
        "title": "Discourse Analysis of Lyric and Lyric-Based Classification of Music.",
        "author": [
            "Jiakun Fang",
            "David Grunberg",
            "Diane J. Litman",
            "Ye Wang 0007"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416946",
        "url": "https://doi.org/10.5281/zenodo.1416946",
        "ee": "https://zenodo.org/records/1416946/files/FangGLW17.pdf",
        "abstract": "Lyrics play an important role in the semantics and the structure of many pieces of music. However, while many existing lyric analysis systems consider each sentence of a given set of lyrics separately, lyrics are more naturally understood as multi-sentence units, where the relations be- tween sentences is a key factor. Here we describe a series of experiments using discourse-based features, which de- scribe the relations between different sentences within a set of lyrics, for several common Music Information Re- trieval tasks. We first investigate genre recognition and present evidence that incorporating discourse features al- low for more accurate genre classification than single- sentence lyric features do. Similarly, we examine the prob- lem of release date estimation by passing features to clas- sifiers to determine the release period of a particular song, and again determine that an assistance from discourse- based features allow for superior classification relative to single-sentence lyric features alone. These results suggest that discourse-based features are potentially useful for Mu- sic Information Retrieval tasks.",
        "zenodo_id": 1416946,
        "dblp_key": "conf/ismir/FangGLW17",
        "keywords": [
            "discourse-based features",
            "genre recognition",
            "release date estimation",
            "Music Information Retrieval tasks",
            "single-sentence lyric features",
            "relations between sentences",
            "semantics and structure of music",
            "Multi-sentence units",
            "classification accuracy",
            "song release period"
        ],
        "content": "DISCOURSE ANALYSIS OF LYRIC AND LYRIC-BASED\nCLASSIFICATION OF MUSIC\nJiakun Fang1David Grunberg1Diane Litman2\nYe Wang1\n1School of Computing, National University of Singapore, Singapore\n2Department of Computer Science, University of Pittsburgh, USA\nfangjiak@comp.nus.edu.sg, wangye@comp.nus.edu.sg\nABSTRACT\nLyrics play an important role in the semantics and the\nstructure of many pieces of music. However, while many\nexisting lyric analysis systems consider each sentence of\na given set of lyrics separately, lyrics are more naturally\nunderstood as multi-sentence units, where the relations be-\ntween sentences is a key factor. Here we describe a series\nof experiments using discourse-based features, which de-\nscribe the relations between different sentences within a\nset of lyrics, for several common Music Information Re-\ntrieval tasks. We ﬁrst investigate genre recognition and\npresent evidence that incorporating discourse features al-\nlow for more accurate genre classiﬁcation than single-\nsentence lyric features do. Similarly, we examine the prob-\nlem of release date estimation by passing features to clas-\nsiﬁers to determine the release period of a particular song,\nand again determine that an assistance from discourse-\nbased features allow for superior classiﬁcation relative to\nsingle-sentence lyric features alone. These results suggest\nthat discourse-based features are potentially useful for Mu-\nsic Information Retrieval tasks.\n1. INTRODUCTION\nAcoustic features have been used as the basis for a wide\nvariety of systems designed to perform various Music In-\nformation Retrieval (MIR) tasks, such as classifying music\ninto various categories. However, a piece of music is not\nentirely deﬁned by its acoustic signal, and so acoustic fea-\ntures alone may not contain sufﬁcient information to allow\nfor a system to accurately classify audio or perform other\nMIR tasks [24]. This has led to interest in analyzing other\naspects of music signals, such as lyrics [16, 22].\nAlthough not all music contains lyrics, for songs that\ndo, lyrics have been proven to be useful for classifying au-\ndio based on topic [17], mood [15], genre, release date, and\neven popularity [7]. This is a natural result since humans\nalso consider lyrics when performing these classiﬁcations.\nc\rJiakun Fang, David Grunberg, Diane Litman, Ye Wang.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Jiakun Fang, David Grunberg, Diane\nLitman, Ye Wang. “Discourse Analysis of Lyric and Lyric-based Clas-\nsiﬁcation of Music”, 18th International Society for Music Information\nRetrieval Conference, Suzhou, China, 2017.But while lyric features have been used in previous MIR\nstudies, such works often use a bag-of-words or bag-of-\nsentences approach which considers each sentence within\na set of lyrics independently. This approach sacriﬁces the\ncontextual information provided by the lyrical structure,\nwhich often contains crucial information. As an example,\nwe consider lyrics from the theme of Andy Williams’ “A\nSummer Place”:\n\u000fYour arms reach out to me.\n\u000fAnd my heart is free from all care.\nThe clause ‘and’ linking these two lines helps to set the\nmood; the listener can observe a connection between the\nsubject reaching out to the singer, and the singer’s heart\nconsequently being at ease. But suppose the word ‘and’\nwere changed to the word ‘but’. In this case, the meaning\nof these lyrics would be entirely different; now the singer’s\nheart is at ease despite the subject reaching for him, not\nimplicitly because of it. A human would no doubt observe\nthis; however, this information would be lost with a bag-\nof-words or bag-of-sentences approach. We therefore hy-\npothesize that lyrics features which operate on a discourse\nlevel, taking into account the relations between textual el-\nements, will better represent the underlying structure of a\nset of lyrics, and that systems using such features will im-\nprove the performances of those using lyric features which\nconsider each sentence independently.\nIn this paper we consider two classical MIR tasks: genre\nclassiﬁcation and release date estimation. Prior research\nhas already demonstrated that lyrics-based features can im-\nprove accuracy for genre classiﬁcation [22] as well as re-\nlease date estimation [7]. This prior work considered indi-\nvidual words without taking into account how those words\nwere linked together with discourse features or other con-\nnectors. However, it is already known that the complexity\nof lyrics often varies between different genres (e.g., rap\nmusic tends to have more complex lyrics than other gen-\nres [7]) as well as between different eras of music [9].\nLyrics of differing complexity are likely to have differ-\ning discourse connectors (e.g., very simple lyrics may only\nconsist of a few unrelated elements and so have almost no\ndiscourse connectors, while dense, complicated lyrics may\ncontain many elements which are connected together via\ndiscourse connectors), so we hypothesize that discourse464connector features may also contribute to the above tasks.\nAs such, we investigate whether discourse features truly\nimprove the accuracy in genre recognition, release-date es-\ntimation, and popularity analysis.\n2. RELATED WORKS\nDiscourse analysis is a process analyzing the meaning of a\ntext by examining multiple component sentences together,\nrather than each sentence on its own [26]. One dimen-\nsion of it is discourse relations , which describes how mul-\ntiple elements of a text logically relate to each other, and\ndifferent discourse relation corpora and frameworks have\nbeen devised, including Rhetorical Structure Theory [21],\nGraphbank [27] and the Penn Discourse Treebank (PDTB)\n[25]. We opted to use PDTB as it is relatively ﬂexible com-\npared to these other frameworks [23] and more able to ac-\ncommodate a wider variety of lyrics structures.\nAnother aspect of discourse analysis is text segmenta-\ntion. In prior MIR studies involving lyrics, acoustic el-\nements were used to help determine lyric segmentation\npoints [3]. However, this approach takes the risk that errors\nin the audio analysis will propagate through to the lyric\nsegmentation step. In contrast, the algorithm TextTiling\ntakes only text as input and attempts to detect the bound-\naries of different subtopics within that text in order to per-\nform meaningful segmentation [13]. Because lyrics can\nchange topics during a song, we determined that a topic-\nbased system like TextTiling could provide useful segmen-\ntation for MIR systems operating on lyrics.\nCoherence and cohesion of a text has been proven to be\nimportant for human understanding [12] and writing qual-\nity [4]. While text coherence is a subjective property of\ntext based on human understanding, text cohesion is an ob-\njective property of explicit text element interpretation pat-\nterns [12]. Various studies focused on elements of this spe-\nciﬁc text analysis, including entity grid [1] and coreference\nresolution systems [18]. A study by Feng et al. [8] showed\nthe appearance pattern of entities may vary according to\ndifferent writing style. Therefore, we hypothesize that the\ncohesion patterns in lyrics may vary according to differ-\nent categories, and we used entity density, entity grid and\ncoreference chain for lyric cohesion analysis.\nMany music classiﬁcation tasks have been investigated\nin the ﬁeld of MIR. However, most systems which incor-\nporate lyrics do not incorporate discourse analysis; they in-\nstead rely on approaches such as analyzing bags of words,\npart-of-speech tags and rhyme [7, 16, 19]. There was still\nlittle analysis of the discourse relations, topic shifts or de-\ntailed cohesion analysis.\n3. FEATURES\n3.1 Discourse-based Features\nPDTB-styled discourse relations : We used a PDTB-\nstyled parser1[20] to generate discourse relation features.\nIn this work, we only focus on explicit discourse relations,\n1http://wing.comp.nus.edu.sg/ linzihen/parser/since implicit relations are both harder to accurately deter-\nmine and more subjective. In order to ﬁnd such explicit\nrelations, the parser ﬁrst identiﬁes all connectives in a set\nof lyrics and determines whether each one serves as a dis-\ncourse connective. The parser then identiﬁes the explicit\nrelation the connective conveys. The system considers four\ngeneral relations and 16 speciﬁc relations which are sub-\ncategories of the 4 general relations.\nAs an example, we consider a lyric from John Lennon’s\n“Just Like Starting Over”: “... I know time ﬂies so quickly/\nButwhen I see you darling/It’s like we both are falling in\nlove again ...” All three of the underlined words are con-\nnectives, but the ﬁrst such word, ‘so,’ is not a discourse\nconnective because it does not connect multiple arguments.\nThe parser thus does not consider this word in its analysis.\nThe other two connectives, ‘but’ and ‘when’, are discourse\nconnectives and so are analyzed to determine what type\nof relation they are; ‘when’ is found to convey a Tempo-\nral (general) and Synchrony (speciﬁc) relation, and ‘but’\nis determined to convey a Comparison and a Contrast re-\nlation. In this way, the connections between the different\nelements of this lyric are understood by the system.\nOnce all the discourse connectives are found and cate-\ngorized, we obtain features by counting the number of dis-\ncourse connectives in each set of lyrics which corresponds\nto a particular discourse relation. For instance, one song\nmight have 18 discourse connectives indicating a Tempo-\nral relation, so its Temporal feature would be set to 18. We\nalso count the number of pairs of adjacent discourse con-\nnectives which correspond to particular relations and these\nadjacent discourse connectives are not necessary consec-\nutive tokens; the same song as before might have 5 in-\nstances where one discourse connective indicates a ‘Tem-\nporal’ relation and the next discourse connective indicates\na ‘Comparison’ relation, so its Temporal-Comparison fea-\nture would be set to 5. This process is performed indepen-\ndently for the general and the speciﬁc relations. Ultimately,\nwe obtain 20 features corresponding to the 4 general re-\nlations (4 individual relations and 16 pairs of relations),\nand 272 features corresponding to the 16 speciﬁc relations\n(16 individual relations, and 256 pairs of relations). Af-\nter removing features which are zero throughout the entire\ndataset, 164 features corresponding to speciﬁc relations re-\nmain. Finally, we calculate the mean and standard devia-\ntion of the sentence positions of all discourse connectives\nin a set of lyrics, as well as all connectives in that set of\nlyrics in general.\nTextTiling segmentation : We ran the TextTiling algo-\nrithm to estimate topic shifts within a piece of lyric, us-\ning the Natural Language Toolkit Library2, setting the\npseudo-sentence size to the average length of a line and\ngrouping 4 pseudo-sentences per block. Lyrics with fewer\nthan 28 words and 4 pseudo-sentences were set as one\nsegment, since they were too short for segmentation, and\nlyrics with no line splits were arbitrarily assigned a pseudo-\nsentence size of 7 words (average length in the dataset).\nFeatures were then calculated by computing the mean and\n2http://www.nltk.org/api/nltk.tokenize.htmlProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 465standard deviation in the number of words in a lyric’s seg-\nments and the number of segments.\nEntity-density features : General nouns and named en-\ntities (i.e., locations and names) usually indicate concep-\ntual information. Previous research have shown that named\nentities are useful to convey summarized ideas [11] and we\nhypothesized that entity distribution could vary between\nsong categories. We implemented features including: ratio\nof the number of named entities to the number of all words,\nratio of the number of named entities to the number of all\nentities, ratio of the number of union of named entities and\ngeneral nouns to the number of all entities, average number\nof named entities per sentence, and average number of all\nentities per sentence. We used OpenNLP3to ﬁnd named\nentities and Stanford Part-Of-Speech Tagger4to extract\ngeneral nouns.\nCoreference inference features : Entities and their\npronominal references in a text which represent a same ob-\nject build a coreference chain [18]. The pattern of how an\nentity represented by different text elements with same se-\nmantic meanings through text may vary in different song\nstyles. We used Stanford Coreference Resolution System5\nto generate coreference chain. The total number of corefer-\nence chains, the number of coreference chains which span\nmore than half of lyric length, the average number of coref-\nerences per chain, the average length per chain, the aver-\nage inference distance per chain and the number of active\ncoreference chains per word were extracted. The inference\ndistance was computed as the minimum line distance be-\ntween the referent and its pronominal reference. The chain\nis active on a word if the chain passes its location.\nEntity-grid features : Barzilay and Lapata’s [1] entity\ngrid model was created to measure discourse coherence\nand can be used for authorship attribution [8]. We thus\nhypothesized that subjects and objects may also be related\ndifferently in different genres, just as they may be related\ndifferently for artists. Brown Coherence Toolkit [6] was\nused to generate an entity grid for each lyric. Each cell\nin a grid represent one of the roles of subject (S), object\n(O), neither of the two (X) and absent in the sentence (-)\nof a entity in a sentence. We calculated the frequency of\n16 adjacent entity transition patterns (i.e., ‘SS’, ‘SO’, ‘SX’\nand ‘S-’) and the number of total adjacent transitions, and\ncomputed percentage of each pattern.\n3.2 Baseline: Previously Used Textual Features\nWe selected several lyric-based features from the MIR lit-\nerature to form comparative baselines against which the\ndiscourse-based features could be tested (Table 1) [7]:\nVocabulary : We used the Scikit-learn library6to cal-\nculate the top 100 n-grams (n = 1, 2, 3) according to\ntheir tf-idf values. When performing genre classiﬁcation,\nwe obtained the top 100 unigrams, bigrams, and trigrams\nfor the lyrics belonging to each genre. When performing\n3https://opennlp.apache.org\n4http://nlp.stanford.edu/software/tagger.shtml\n5http://nlp.stanford.edu/projects/coref.shtml\n6http://scikitlearn.org/stable/modules/feature extraction.htmlyear classiﬁcation, we obtained approximately 300 n-gram\nfeatures evenly from three year classes. These n-grams\nwere represented by a feature vector indicating the impor-\ntance of each n-gram in each lyric. We also computed\nthe type/token ratio to represent vocabulary richness and\nsearched for non-standard words by ﬁnding the percentage\nof words in each lyric that could be found in the Urban Dic-\ntionary7, a dictionary of slang, but not in Wiktionary8.\nPart-of-Speech features : We used Part-of-Speech tags\n(POS tags) obtained from the Stanford POS Tagger9to\ndetermine the frequencies of each super-tags (Adjective,\nAdverb, Verb and Noun) in lyrics.\nLength : Length features such as lines per song, tokens\nper song, and tokens per line were calculated.\nOrientation : The frequency of ﬁrst, second and third\npronouns as well as the ratio of self-referencing pronouns\nto non-self-referencing ones and the ratio of ﬁrst person\nsingular pronouns to second person were used to model\nthe subject of given sets of lyrics. We also calculated the\nratio of past tense verbs to all verbs to quantify the overall\ntense of songs.\nStructure : Each set of lyrics was checked against it-\nself for repetition. If the title appeared in the lyrics, the\ntitle feature for that song was given a ‘True’ value, which\nwas otherwise set to false. Similarly, if there were long\nsequences which exactly matched each other, the ‘Chorus’\nfeature was set to ‘True’ for a given song. Table 1 shows\nthe number of elements in each feature set in the classiﬁ-\ncation tasks.\nDimension Abbreviation Length\ndiscourse-based features DF 250\nPDTB-based discourse relation DR 204\nTextTiling segmentation TT 3\nentity density ED 5\ncoreference inference CI 5\nentity grid EG 33\ntextual baseline features TF 318\nvocabulary VOCAB 303\nPOS tags POS 4\nlength LEN 3\norientation OR 6\nstructure STRUC 2\nTable 1 : Features used in classiﬁcation tasks.\n3.3 Normalization\nSince features used for these tasks are not on the same\nscale, we then performed normalization on features. Each\nfeature was normalized by its maximum value and mini-\nmum value to range from 0 to 1 (Equation 1). Then all\nnormalized features were put into classiﬁcation tasks. This\nnormalization step was expected to improve the results of\n7http://www.urbandictionary.com\n8https://www.wiktionary.org\n9http://nlp.stanford.edu/software/tagger.shtml466 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017combination of different feature sets, as differences in vari-\nable ranges could potentially affect negatively to the per-\nformance of classiﬁcation algorithm.\nvn=v\u0000vmin\nvmax\u0000vmin(1)\n4. DATASET AND ANNOTATION\nA previously collected corpus of 275,905 sets of full lyrics\nwas used for these experiments and we pre-processed the\ndataset in 6 different types to clean up lyrics [5], including\nsplitting of compounds or removal of hyphenated preﬁxes,\nelimination of contractions, restoration of dropped initial,\nabbreviation elimination, adjustment to American English\nspellings, and correction of misspelled words. Unlike other\ncorpora, such as musiXmatch lyrics dataset for the Million\nSong Dataset [2], lyrics from the selected corpus are not\nbags-of-words but are stored in full sentences, allowing for\nthe retention of discourse relations. We split song lyrics by\npunctuations and lines to make sentences and paragraphs\nto run discourse analysis algorithm in this work. We also\ndownloaded corresponding genre tags and album release\nyears for the songs represented in this dataset from Rovi10.\nThe speciﬁc number of lyrics for each experiment is shown\nin Table 2.\nGenre classiﬁcation : We kept all 70,225 songs with\na unique genre tag from Rovi for this speciﬁc task. The\ntags indicated that songs in the dataset came from 9 dif-\nferent genres: Pop/Rock (47,715 songs in the dataset),\nRap (8,274), Country (6,025), R&B (4,095), Electronic\n(1,202), Religious (1,467), Folk (350), Jazz (651) and Reg-\ngae (446). All of these songs were then used for the genre\nclassiﬁcation experiments.\nRelease date estimation : Rovi provided release dates\nfor 52,244 unique lyrics in the dataset. These release dates\nranged from 1954-2014. However, some genres were not\nrepresented in certain years; no R&B songs, for instance,\nhad release dates after 2010, and no rap songs had release\ndates before 1980. To prevent this from biasing our re-\nsults we chose to just use one single genre and settled on\nPop/Rock, for which we had 46,957 songs annotated with\nrelease dates throughout the 1954-2014 range. We then ex-\ntracted all the songs labeled as having been released in one\nof three time ranges: 1969-1971 (536 songs total), 1989-\n1991 (3,027), and 2009-2011 (4,382). We put gaps of sev-\neral years between each range on the basis that, as indi-\ncated in prior literature, lyrics are unlikely to change much\nin a single year [7].\n5. GENRE CLASSIFICATION\nWe ran SVM classiﬁers using 10-fold cross-validation.\nThese classiﬁers were implemented with Weka11using\nthe default settings. We chose SVM classiﬁers because\nthey have been proven to be of use in multiple MIR tasks\n[7, 15]. Because each genre had a different number of\n10http://developer.rovicorp.com\n11http://cs.waikato.ac.nz/ml/wekaClassiﬁcation Task Number of lyric used\n(after undersampling)\nGenre Pop/Rock: 45,020; Rap: 16,548;\nCountry: 12,050; Jazz: 1,302;\nR&B: 8,190; Electronic: 2,404;\nReligious: 2,934; Folk: 700;\nReggae: 892\nRelease Period 1,608 sets of lyrics,\nsplit evenly into three time spans\nTable 2 : Data sizes for experiments.\nsamples, undersampling [10] was performed for both train-\ning and testing to ensure that each genre was represented\nequally before cross-validation classiﬁcation. Each song\nwas classiﬁed in a 2-class problem: to determine if the\nsong was of the correct genre or not. The undersampling\nand classiﬁcation process was repeated 10 times and we\npresent the averages of F-score for each independent clas-\nsiﬁcation task. The value of F-score by random should be\n0.5.\nWe ﬁrst implemented previously-used textual features\nto generate a baseline for the genre classiﬁcation task.\nModels were built based on vocabulary (VOCAB), POS\ntags (POS), length (LEN), orientation (OR), structure\n(STRUC) and all combined baseline features (TF) sepa-\nrately. The average F-scores are depicted in Table 3. It is\napparent that using vocabulary features can achieve high\nperformance in average, but one thing to be noted is that\nit heavily depends on which corpus the language model\ntrains on to generate the n-gram vector. Here we used\nall lyrics from each genre to get top n-grams. Orientation\nfeatures were useful for R&B recognition since we found\nmore ﬁrst pronouns in such genre. We then used these fea-\ntures to compare with proposed discourse-based features.\nWe then evaluated the utility of discourse-based fea-\ntures for this speciﬁc task. Table 3 presents the results\nfrom using discourse relation (DR), TextTiling topic seg-\nmentation (TT), entity density (ED), coreference inference\n(CI), and entity grid (EG) features to perform genre clas-\nsiﬁcation with the SVM classiﬁers. Because the discourse\nrelation and TextTiling features showed very promising re-\nsults, we also tested a system which combined those fea-\ntures (DR+TT). Finally, we tested all discourse features\ntogether (DF), and then all discourse and all baseline fea-\ntures together. Statistical signiﬁcance were computed us-\ning a standard two-class t-test between the highest F-score\nand each result from other feature set for each genre, and\neach column’s best result were found to be signiﬁcant with\np<0.01.\nFirst, we note that, for every single genre as well as the\noverall average, the system’s classiﬁcation accuracy when\nusing DR+TT discourse features is better than its accuracy\nusing any and all baseline features. In fact, DR features\nalone outperform any and all baseline features for 7 of\nthe 9 genres as well as overall. This serves to demon-\nstrate the utility of these particular discourse features forProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 467Feature Set R&B Folk Country Rap Elect. Reli. Jazz Reggae Pop Avg.\nVOCAB 58.5 51.4 59.4 90.8 53.7 53.5 55.3 60.7 65.7 61.0\nPOS 55.4 47.3 53.6 73.1 49.9 50.3 56.3 47.4 60.0 54.8\nLEN 55.2 49.3 55.4 85.8 48.6 50.0 50.3 48.8 59.2 55.4\nOR 66.0 54.7 58.1 84.6 54.4 52.6 58.7 54.9 63.4 60.8\nSTRUC 45.0 46.4 44.5 45.6 46.0 45.7 45.3 47.0 44.6 45.6\nTF (All) 62.5 56.5 60.1 81.3 50.7 51.8 58.1 56.5 63.6 60.1\nDR 64.9 61.7 65.7 89.8 59.1 56.2 62.8 64.0 66.7 65.7\nTT 63.3 51.1 58.2 90.4 53.1 53.0 58.0 55.9 65.9 61.0\nED 55.4 58.3 53.2 76.5 53.8 53.7 46.8 57.1 61.2 57.3\nCI 59.1 47.8 62.7 82.4 50.5 52.8 55.7 54.1 63.7 58.8\nEG 58.7 48.3 57.1 83.9 50.5 52.6 54.9 51.4 62.9 57.8\nDR + TT 67.4 59.1 66.6 91.0 58.3 55.3 62.3 62.3 67.7 65.6\nDF (All) 58.2 53.3 60.9 75.8 49.9 54.0 57.5 49.1 61.5 57.8\nAll 50.0 34.5 35.7 49.6 45.2 48.3 41.1 49.4 45.8 44.4\nTable 3 : Accuracy of classiﬁer using different unnormalized feature sets to estimate genre (F-Score*100).\nFeature Set R&B Folk Country Rap Elect. Reli. Jazz Reggae Pop Avg.\nVOCAB 59.3 55.6 61.0 91.3 52.7 63.0 61.8 65.1 66.5 64.4\nPOS 63.5 57.8 55.9 90.9 49.4 48.9 61.8 61.6 65.3 62.4\nLEN 61.9 50.5 59.4 86.7 49.2 49.1 61.1 59.4 63.5 60.2\nOR 68.2 55.8 55.1 85.4 47.3 46.6 60.0 55.7 64.3 60.4\nSTRUC 46.9 45.1 45.8 45.8 46.9 44.8 43.8 47.1 44.6 45.6\nTF (All) 71.1 59.6 67.4 93.3 55.4 65.0 65.6 68.7 68.3 68.4\nDR 60.9 59.0 62.3 88.4 54.9 54.6 61.1 61.0 64.7 63.1\nTT 64.1 49.8 54.6 90.9 48.7 51.0 62.7 60.6 66.0 61.7\nED 37.5 45.2 38.3 65.5 45.1 45.5 47.8 47.3 51.6 48.2\nCI 63.5 53.2 61.5 84.5 50.5 55.1 62.2 63.7 62.2 61.9\nEG 63.7 55.5 64.5 94.1 57.8 49.5 65.5 62.1 64.4 64.1\nDF (All) 71.2 61.3 67.3 94.5 58.5 58.5 64.5 66.5 66.3 67.7\nAll 73.7 60.6 71.5 94.8 58.9 65.6 66.9 69.6 69.4 69.9\nTable 4 : Accuracy of classiﬁer using different normalized feature sets to estimate genre (F-Score*100).\nthis task, since they consistently outperform the baseline\nfeatures. Second, we note that the entity and coreference\nfeatures did not enable the classiﬁer to achieve maximal\nresults in this task, indicating that these features may not\nvary as much between genres compared to the DR and TT\nfeatures. Third, we note that the system’s accuracy when\nall features was used decreased relative to the DR+TT and\nDR features in every case. We then performed the normal-\nization and each feature was normalized by its maximum\nvalue and minimum value to range from 0 to 1.\nTable 4 shows the results and the combination of all fea-\nture outperformed all baseline features, while the combina-\ntion of all discourse-based features can achieve higher per-\nformance than all baseline feature sets in 3 classes. Best\nresult for each genre were found to be signiﬁcant with p<\n0.01. This further emphasized the importance of discourse-\nbased features in this speciﬁc task.\nOne interesting trend in these results is in the ‘Rap’ col-\numn, which shows that not only was the classiﬁcation ac-\ncuracy for Rap songs far higher than the other classes, but\nit was also the one genre where TT features outperformedDR features. Although the discourse-based features did\nnot outperform the baseline features in this genre, it should\nbe noted that the TextTiling segmentation features did ob-\ntain virtually identical performance to the best baseline\nfeatures with only a 3-dimensional feature vector; the VO-\nCAB features, by contrast, encompassed hundreds of di-\nmensions. We investigated this further and found that Rap\nmusic tended to have more topic segments (5.9/song on\naverage, while the average for other genres was 4.9), and\nmore varied adjacent discourse relations as well (for in-\nstance, each rap song had on average 6.6 different types of\nadjacent discourse relations; non-rap songs averaged 4.0).\nThis suggests that TextTiling segmentation features may be\na more compact way to accurately represent topic-heavy\nlyrics, such as those commonly found in rap music.\nWe ﬁnally analyzed the portion of each type of dis-\ncourse connective for the four ﬁrst-level PDTB-styled dis-\ncourse relations of all discourse connectives in each genre.\nWe found that Religious songs use more expansion rela-\ntions than other genres (42% and 37% in average), while\nless expansion relations are written in Rap songs (34%).468 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Connectives standing for temporal relations present more\nin Rap songs (26% and 23% in average). R&B songs con-\ntains more contingency connectives (24% and 26% in av-\nerage).\n6. RELEASE DATE ESTIMATION\nWe investigated whether discourse-based features can help\nto estimate the release date of a song, on the basis that\nthe lyric structure of song texts is likely to change over\ntime [7, 14]. We ﬁrst formed a subset of all the Pop/Rock\nsongs in our dataset, since as mentioned before these songs\nspanned a greater time period than the other genres. We\nthen extracted all the songs labeled as having been released\nin one of three time ranges: 1969-1971 (536), 1989-1991\n(3,027), and 2009-2011 (4,382). Based on the idea from\nprior study [7], we made gaps since that the lyrics would be\nunlikely to change very much in a single year. Undersam-\npling was used to balance the dataset building a sub-dataset\nbefore each classiﬁcation with an SVM with 10-fold cross\nvalidation for three-class classiﬁcation. The process was\nrepeated 10 times.\nTable 5 shows results. As can be seen from the table,\ndiscourse relation features alone outperformed the baseline\nfeature sets in average F-score for each three year class\n(p<0.001), which indicates that the sentence relations\nin lyrics likely vary over years, and that discourse rela-\ntion features are useful at indicating this. Although not\nas much as the discourse relation features, the topic seg-\nments and coreference inference features contribute to this\nspeciﬁc classiﬁcation task as well, showing topic presenta-\ntion and cohesion structure changed over time. TextTiling\nfeatures proved to increase accuracy for one year range,\n2009-2011, indicating that the number and relations of top-\nics of music released in this era likely varied as compared\nto previous eras, and also that text segmentation-based fea-\ntures are useful in noting this change. The number of top-\nics and the number of words in each topics in average in-\ncreases over time. As for the coreference inference fea-\ntures, the number of coreference chains and the number of\nlong coreference chains showed raising values according to\nrelease periods. More coreference chains and long coref-\nerence appeared more often in the recent years, indicating\na ﬂuent and centric content. The other discourse features\nwere again shown to be less useful than these ones. Fi-\nnally, the early ages and recent ages were more likely to\nbe recognized, while the middle ages generally achieved\nthe lowest F-scores among all feature sets except structure\nfeatures. This result is intuitive; music will likely be more\nsimilar to music that were produced closer together.\nWe then normalized to 0 to 1 for all features and re-\npeated the task to show whether discourse features can im-\nprove the performance of baseline features for this task.\nTable 6 shows that the combination of all features outper-\nformed the other feature sets in this three-class classiﬁca-\ntion task ( p<0.001).Feature 1969-1971 1989-1991 2009-2011 Avg.\nVOCAB 46.8 33.7 34.9 38.5\nPOS 30.0 24.5 52.8 35.8\nLEN 34.6 26.7 50.6 37.3\nOR 43.4 32.0 50.6 42.0\nSTRUC 0.00 29.1 50.7 26.6\nTF (All) 42.2 27.6 53.6 41.2\nDR 59.7 43.0 55.0 52.6\nTT 46.5 34.8 47.6 43.0\nED 40.4 29.5 41.7 37.2\nCI 47.7 29.3 53.8 43.6\nEG 41.2 32.5 44.3 39.4\nDR + TT 58.5 40.7 56.3 51.8\nDF (All) 43.3 28.3 53.8 41.8\nAll 36.2 30.6 30.4 32.4\nTable 5 : Accuracy of classiﬁer using different unnormal-\nized feature sets to estimate release date (F-Score*100).\nFeature 1969-1971 1989-1991 2009-2011 Avg.\nVOCAB 51.4 41.6 42.3 45.1\nPOS 58.7 24.5 46.7 43.3\nLEN 61.4 27.9 45.8 45.0\nOR 58.1 17.4 48.3 41.3\nSTRUC 0.0 22.0 87.3 36.4\nTF (All) 63.4 42.0 53.1 52.8\nDR 57.6 34.5 47.7 46.6\nTT 59.9 29.9 37.8 42.5\nED 30.0 16.3 47.4 31.2\nCI 62.0 27.2 52.3 47.2\nEG 57.4 46.6 42.0 48.7\nDF (All) 57.0 44.9 48.8 50.3\nAll 61.0 48.8 54.7 54.7\nTable 6 : Accuracy of classiﬁer using different normalized\nfeature sets to estimate release date (F-Score*100).\n7. CONCLUSION AND FUTURE WORK\nWe investigated the usefulness of discourse-based features\nand demonstrated that such features can provide useful in-\nformation for two MIR classiﬁcation tasks. Genre classi-\nﬁcation and release date estimation were all enhanced by\nincorporating discourse features into the classiﬁers. How-\never, since discourse-based features rely on passages with\nmultiple text elements, it may be noisy when used on music\nwith short lyrics. As this work is an exploration work, fur-\nther analysis is required. For instance, we split song lyrics\nby lines and punctuations in this work, which ﬁtted most\nof the cases in our dataset. The split rules of sentences can\ninﬂuence the results from discourse analysis algorithms.It\nwill be potentially useful to use these features for other\nMIR tasks such as keyword extraction and topic classiﬁ-\ncation. In the future, we will explore all these discourse-\nbased features on other MIR tasks and ﬁnd sensible sets of\nfeatures and fusion strategies for further improving perfor-\nmance for these tasks.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4698. REFERENCES\n[1] R. Barzilay and M. Lapata. Modeling local coherence:\nan entity-based approach. Computational Linguistics ,\n34(1):141–148, 2008.\n[2] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and\nP. Lamere. The million song dataset. In Proceedings of\nInternational Society for Music Information Retrieval\nConference , pages 591–596, 2011.\n[3] H. T. Cheng, Y . H. Yang, Y . C. Lin, and H. H.Chen.\nMultimodal structure segmentation and analysis of mu-\nsic using audio and textual information. In Proceedings\nof IEEE International Symposium on Circuits and Sys-\ntems, pages 1677–1680, 2009.\n[4] S. A. Crossley and D. S. Mcnamara. Cohesion, co-\nherence, and expert evaluations of writing proﬁciency.\nInProceedings of the 32nd Annual Conference of the\nCognitive Science Society , pages 984–989, 2010.\n[5] R. J. Ellis, Z. Xing, J. Fang, and Y . Wang. Quantifying\nlexical novelty in song lyrics. In Proceedings of Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 694–700, 2015.\n[6] M. Elsner, J. Austerweil, and E. Charniak. A uniﬁed lo-\ncal and global model for discourse coherence. In Pro-\nceedings of the Conference on Human Language Tech-\nnology and North American Chapter of the Association\nfor Computational Linguistics , pages 436–447, 2007.\n[7] M. Fell and C. Sporleder. Lyrics-based analysis and\nclassiﬁcation of music. In Proceedings of International\nConference on Computational Linguistics , pages 620–\n631, 2014.\n[8] V . W. Feng and G. Hirst. Patterns of local discourse co-\nherence as a feature for authorship attribution. Literary\nand Linguistic Computing , 29(2):191–198, 2014.\n[9] Y . Gao, J. Harden, V . Hrdinka, and C. Linn. Lyric com-\nplexity and song popularity: Analysis of lyric compo-\nsition and relation among billboard top 100 songs. In\nSAS Global Forum , 2016.\n[10] J. D. Gibbons and S. Chakraborti. Nonparametric Sta-\ntistical Inference . Chapman & Hall, London, 2011.\n[11] J. Goldstein, M. Kantrowitz, V . Mittal, and J. Car-\nbonell. Summarizing text documents: Sentence se-\nlection and evaluation metrics. In Proceedings of the\n22nd Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval ,\npages 121–128, 1999.\n[12] A. C. Graesser, D. S. Mcnamara, M. M. Louwerse, and\nZ. Cai. Coh-metrix: Analysis of text on cohesion and\nlanguage. Behavior Research Methods Instruments and\nComputers , 36(2):193–202, 2004.[13] M. A. Hearst. Texttiling: Segmenting text into multi-\nparagraph subtopic passages. Computational linguis-\ntics, 23(1):33–64, 1997.\n[14] H. Hirjee and D. G. Brown. Using automated rhyme\ndetection to characterize rhyming style in rap music.\nEmpirical Musicology Review , 5(4):121–145, 2010.\n[15] X. Hu and J. S. Downie. When lyrics outperform audio\nfor music mood classiﬁcation: A feature analysis. In\nProceedings of International Society for Music Infor-\nmation Retrieval Conference , pages 619–624, 2010.\n[16] X. Hu, J. S. Downie, and A. F. Stephen. Lyric text\nmining in music mood classiﬁcation. In Proceedings of\nInternational Society for Music Information Retrieval\nConference , pages 411–416, 2009.\n[17] F. Kleedorfer, P. Knees, and T. Pohle. Oh oh oh whoah!\ntowards automatic topic detection in song lyrics. In\nProceedings of International Society for Music Infor-\nmation Retrieval Conference , pages 287–292, 2008.\n[18] H. Lee, Y . Peirsman, A. Chang, N. Chambers, M. Sur-\ndeanu, and D. Jurafsky. Stanford’s multi-pass sieve\ncoreference resolution system at the conll-2011 shared\ntask. In Proceedings of the Fifteenth Conference on\nComputational Natural Language Learning: Shared\nTask, pages 28–34, 2011.\n[19] T. Li and M. Ogihara. Music artist style identiﬁcation\nby semi-supervised learning from both lyrics and con-\ntent. In Proceedings of the 12th Annual ACM Inter-\nnational Conference on Multimedia , pages 364–367,\n2004.\n[20] Z. Lin, H. T. Ng, and M. Y . Kan. A pdtb-styled end-to-\nend discourse parser. Natural Language Engineering ,\n20(2):151–184, 2014.\n[21] W. C. Mann and S. A. Thompson. Rhetorical struc-\nture theory: Toward a functional theory of text organi-\nzation. Text-Interdisciplinary Journal for the Study of\nDiscourse , 8(3):243–281, 1988.\n[22] R. Neumayer and A. Rauber. Integration of text and au-\ndio features for genre classiﬁcation in music informa-\ntion retrieval. In Proceedings of the European Confer-\nence on Information Retrieval , pages 724–727, 2007.\n[23] J. P. Ng, M. Y . Kan, Z. Lin, V . W. Feng, B. Chen,\nJ. Su, and C. L. Tan. Exploiting discourse analysis for\narticle-wide temporal classiﬁcatio. In Proceedings of\nthe Conference on Empirical Methods in Natural Lan-\nguage Processing , pages 12–23, 2013.\n[24] F. Pachet and J. J. Aucouturier. Improving timbre simi-\nlarity: How high is the sky. Journal of Negative Results\nin Speech and Audio Sciences , 1(1):1–13, 2004.\n[25] R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,\nL. Robaldo, A. K. Joshi, and B. L. Webber. The penn\ndiscourse treebank 2.0. In Proceedings of Language470 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Resources and Evaluation Conference , pages 2961–\n2968, 2008.\n[26] B. Webber, M. Egg, and V .Kordoni. Discourse struc-\nture and language technology. Natural Language En-\ngineering , 18(4):1–54, 2012.\n[27] F. Wolf and E. Gibson. Representing discourse coher-\nence: A corpus-based study. Computational Linguis-\ntics, 31(2):249–287, 2005.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 471"
    },
    {
        "title": "Freesound Datasets: A Platform for the Creation of Open Audio Datasets.",
        "author": [
            "Eduardo Fonseca",
            "Jordi Pons",
            "Xavier Favory",
            "Frederic Font",
            "Dmitry Bogdanov",
            "Andres Ferraro",
            "Sergio Oramas",
            "Alastair Porter",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.2583796",
        "url": "https://doi.org/10.5281/zenodo.2583796",
        "ee": "https://ismir2017.smcnus.org/wp-content/uploads/2017/10/161_Paper.pdf",
        "abstract": "Synthetic data for DCASE 2019 task 4\n\nFreesound dataset [1,2]: A subset of FSD is used as foreground sound events for the synthetic subset of the dataset for DCASE 2019 task 4. FSD is a large-scale, general-purpose audio dataset composed of Freesound content annotated with labels from the AudioSet Ontology [3].\n\nSINS dataset [4]: The derivative of the SINS dataset used for DCASE2018 task 5 is used as background for the synthetic subset of the dataset for DCASE 2019 task 4.\nThe SINS dataset contains a continuous recording of one person living in a vacation home over a period of one week.\nIt was collected using a network of 13 microphone arrays distributed over the entire home.\nThe microphone array consists of 4 linearly arranged microphones.\n\nThe synthetic set is composed of 10 sec audio clips generated with Scaper [5]. The foreground events are obtained from FSD. Each event audio clip was verified manually to ensure that the sound quality and the event-to-background ratio were sufficient to be used an isolated event. We also verified that the event was actually dominant in the clip and we controlled if the event onset and offset are present in the clip. Each selected clip was then segmented when needed to remove silences before and after the event and between events when the file contained multiple occurrences of the event class.\n\nLicense:\n\nAll sounds comming from FSD are released under Creative Commons licences. Synthetic sounds can only be used for competition purposes until the full CC license list is made available at the end of the competition.\n\n\n\nFurther information on dcase website.\n\nReferences:\n\n\n\t[1] F. Font, G. Roma  X. Serra. Freesound technical demo. In Proceedings of the 21st ACM international conference on Multimedia. ACM, 2013.\n\t[2] E. Fonseca, J. Pons, X. Favory, F. Font, D. Bogdanov, A. Ferraro, S. Oramas, A. Porter  X. Serra. Freesound Datasets: A Platform for the Creation of Open Audio Datasets. In Proceedings of the 18th International Society for Music Information Retrieval Conference, Suzhou, China, 2017.\n\t[3] Jort F. Gemmeke and Daniel P. W. Ellis and Dylan Freedman and Aren Jansen and Wade Lawrence and R. Channing Moore and Manoj Plakal and Marvin Ritter. Audio Set: An ontology and human-labeled dataset for audio events. In Proceedings IEEE ICASSP 2017, New Orleans, LA, 2017.\n\t\n\t[4] Gert Dekkers, Steven Lauwereins, Bart Thoen, Mulu Weldegebreal Adhana, Henk Brouckxon, Toon van Waterschoot, Bart Vanrumste, Marian Verhelst, and Peter Karsmakers.\n\tThe SINS database for detection of daily activities in a home environment using an acoustic sensor network.\n\tIn Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017), 3236. November 2017.\n\t\n\t[5] J. Salamon, D. MacConnell, M. Cartwright, P. Li, and J. P. Bello. Scaper: A library for soundscape synthesis and augmentation In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), New Paltz, NY, USA, Oct. 2017.\n",
        "zenodo_id": 2583796,
        "dblp_key": "conf/ismir/FonsecaPFFBFOPS17",
        "keywords": [
            "DCASE 2019",
            "Synthetic Data",
            "Freesound Dataset",
            "FSD",
            "SINS Dataset",
            "AudioSet Ontology",
            "Scaper",
            "Sound Events",
            "Foreground Sounds",
            "Background Sounds"
        ]
    },
    {
        "title": "Decoding Neurally Relevant Musical Features Using Canonical Correlation Analysis.",
        "author": [
            "Nick Gang",
            "Blair Kaneshiro",
            "Jonathan Berger",
            "Jacek P. Dmochowski"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417137",
        "url": "https://doi.org/10.5281/zenodo.1417137",
        "ee": "https://zenodo.org/records/1417137/files/GangKBD17.pdf",
        "abstract": "Music Information Retrieval (MIR) has been dominated by computational approaches. The possibility of lever- aging neural systems via brain-computer interfaces is an alternative approach to annotating music. Here we test this idea by measuring correlations between musical fea- tures and brain responses in a statistically optimal fash- ion. Using an extensive dataset of electroencephalographic (EEG) responses to a variety of natural music stimuli, we employed Canonical Correlation Analysis to identify spa- tial EEG components that track temporal stimulus compo- nents. We found multiple statistically significant dimen- sions of stimulus-response correlation (SRC) for all songs studied. The temporal filters that maximize correlation with the neural response highlight harmonics and subhar- monics of that song’s beat frequency, with different har- monics emphasized by different components. The most stimulus-driven component of the EEG has an anatomi- cally plausible, symmetric frontocentral topography that is preserved across stimuli. Our results suggest that differ- ent neural circuits encode different temporal hierarchies of natural music. Moreover, as techniques for decoding EEG advance, it may be possible to automatically label music via brain-computer interfaces that capture neural responses that are then translated into stimulus annotations.",
        "zenodo_id": 1417137,
        "dblp_key": "conf/ismir/GangKBD17",
        "keywords": [
            "Music Information Retrieval",
            "neural systems",
            "brain-computer interfaces",
            "Canonical Correlation Analysis",
            "electroencephalographic responses",
            "stimulus-response correlation",
            "harmonics and subharmonics",
            "temporal filters",
            "anatomically plausible",
            "automatic labeling"
        ],
        "content": "DECODING NEURALLY RELEVANT MUSICAL FEATURES USING\nCANONICAL CORRELATION ANALYSIS\nNick Gang Blair Kaneshiro Jonathan Berger\nCenter for Computer Research in Music and Acoustics\nStanford University\n{ngang,blairbo,brg }@ccrma.stanford.eduJacek P. Dmochowski\nDepartment of Biomedical Engineering\nCity College of New York\njdmochowski@ccny.cuny.edu\nABSTRACT\nMusic Information Retrieval (MIR) has been dominated\nby computational approaches. The possibility of lever-\naging neural systems via brain-computer interfaces is an\nalternative approach to annotating music. Here we test\nthis idea by measuring correlations between musical fea-\ntures and brain responses in a statistically optimal fash-\nion. Using an extensive dataset of electroencephalographic\n(EEG) responses to a variety of natural music stimuli, we\nemployed Canonical Correlation Analysis to identify spa-\ntial EEG components that track temporal stimulus compo-\nnents. We found multiple statistically signiﬁcant dimen-\nsions of stimulus-response correlation (SRC) for all songs\nstudied. The temporal ﬁlters that maximize correlation\nwith the neural response highlight harmonics and subhar-\nmonics of that song’s beat frequency, with different har-\nmonics emphasized by different components. The most\nstimulus-driven component of the EEG has an anatomi-\ncally plausible, symmetric frontocentral topography that is\npreserved across stimuli. Our results suggest that differ-\nent neural circuits encode different temporal hierarchies of\nnatural music. Moreover, as techniques for decoding EEG\nadvance, it may be possible to automatically label music\nvia brain-computer interfaces that capture neural responses\nthat are then translated into stimulus annotations.\n1. INTRODUCTION\nComputationally extracted audio features have been used\nin Music Information Retrieval (MIR) research to model\nthe perceptual attributes of music. Music-speciﬁc features\nwere ﬁrst introduced by Tzanetakis & Cook for genre clas-\nsiﬁcation [43]. These and other features have been used\nin subsequent work for further study of genre [14, 21]\nand other music-tagging applications including emotion\nand mood classiﬁcation [25, 35, 41] and artist identiﬁca-\ntion [26].\nBy contrast, the music neuroscience community hasc\u0000Nick Gang, Blair Kaneshiro, Jonathan Berger, and\nJacek P. Dmochowski. Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Nick Gang, Blair\nKaneshiro, Jonathan Berger, and Jacek P. Dmochowski. “Decoding neu-\nrally relevant musical features using Canonical Correlation Analysis”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.historically focused primarily on experimental stimuli con-\nsisting of simple tones or short, synthesized instrumental\nmelodies. This controlled paradigm allows for precise ex-\nperimental manipulations, with the goal of investigating\nspeciﬁc musical parameters; it also permits event-related\naveraging of responses over repeated trials. However, these\nstimuli lack the complexity and ecological validity of mu-\nsic that is consumed in real life, and preclude the study of\nglobal music processing [20].\nIn recent years, however, this ﬁeld has increasingly\nutilized “naturalistic” music stimuli, including complete,\nreal-world musical works. Here, the computationally ex-\ntracted features developed for MIR research have found\ndirect application as they provide objective, time-varying\nstimulus representations for which neural correlates can be\ninvestigated. To date, this approach has been successfully\napplied to a variety of brain imaging modalities includ-\ning functional magnetic resonance imaging (fMRI) [1, 2,\n40, 42], electroencephalography (EEG) [6, 22, 30, 38], and\nelectrocorticography (ECoG) [31, 32, 37]. Both encoding\n(predicting neural activations from stimulus features) and\ndecoding (predicting stimulus features from neural activa-\ntions) approaches have been explored [2, 27, 40].\nWhile neuroscience is not yet an established subﬁeld\nof MIR, the approaches and insights of each ﬁeld are ar-\nguably complementary [3, 16]. In the present study, we\nextend this interdisciplinary approach and investigate the\nrelationship between time-varying features of naturalis-\ntic music and their EEG responses. We employ a hy-\nbrid encoding-decoding model to derive features and brain\nsignals that maximally covary. The model temporally\nﬁlters musical features while spatially ﬁltering the EEG\nto learn a multidimensional mapping between stimulus\nand response, implemented here by Canonical Correlation\nAnalysis. We uncover multiple statistically signiﬁcant di-\nmensions of stimulus-response correlation, with the ﬁrst\ndimension showing a consistent EEG ﬁlter across differ-\nent songs. Moreover, the temporal ﬁlters that maximize\nSRC emphasize harmonics and subharmonics of the beat\nfrequency, with different harmonics selected by different\ndimensions of SRC. Our ﬁndings suggest that musical fea-\ntures can potentially be annotated by processing neural re-\nsponses, opening up an entirely novel approach to MIR.\nFinally, all data and code will be made publicly available.\nThe remainder of the paper is organized as follows. In\nSection 2, we describe the EEG dataset, audio stimulus131feature extraction, and analysis procedures. We present the\nresults of our analyses in Section 3, and conclude with a\ndiscussion in Section 4.\n2. METHODS\nAll analyses were performed using Matlab.1\n2.1 EEG Dataset\nSeeking ready-to-use EEG data reﬂecting natural music\nlistening and for which we could obtain the stimuli, we\nused the publicly available NMED-H dataset [18]. This\ndataset contains EEG responses to intact and scrambled\nversions of full-length “Bollywood” songs, each approx-\nimately 4.5 minutes long. We used the responses to intact\nsongs only, which comprise data from 48 unique partici-\npants (12 per song), who each heard their song twice—a\ntotal of 24 EEG trials per song. The data frames have been\nﬁltered and cleaned of ocular and noise artifacts, and con-\ntain recordings from 125 electrodes at a sampling rate of\n125 Hz with average reference. Full details of data acquisi-\ntion and preprocessing are given in Kaneshiro (2016) [15].\nAs the downloaded data contained missing values, we im-\nputed missing data using a spatial average from neighbor-\ning electrodes before proceeding with analysis.\n2.2 Stimulus Feature Extraction\nThe NMED-H documentation provides links to purchase\nthe songs from iTunes, and instructions for converting\nthem to the intact versions of the experimental stimuli [18].\nAfter following those procedures, we extracted acoustical\nfeatures using the MIR Toolbox, Version 1.5 [19]. We ex-\ntracted the same collection of 20 short-term features that\nwere used in a recent fMRI study by Alluri et al. [1]:\nZero crossing rate, spectral centroid, high/low energy ra-\ntio, spectral spread, spectral rolloff, spectral entropy, spec-\ntral ﬂatness, roughness, RMS energy, broadband spectral\nﬂux, and spectral ﬂux for 10 octave-wide subbands. Fea-\ntures were extracted in 25-msec analysis windows with\na 50% overlap between frames (standard parameters for\nshort-term features [1, 43]), yielding a feature sampling\nfrequency of 80 Hz. As in the Alluri study, we also or-\nthogonalized the features using PCA, providing a lower-\ndimensional stimulus representation that contains contri-\nbutions of all features under consideration [1]. We per-\nformed all subsequent analyses using PC1, as well as two\nindividual features. RMS and spectral ﬂux were chosen as\nthey reﬂect amplitude envelope and timbre, respectively,\nand have been used in previous studies mapping music\nstimulus features to brain responses [1, 2, 30, 42].\nAs a reference for interpreting results, we extracted\nbeat and tempo information from the stimulus audio ﬁles\nusing a publicly available Matlab implementation [8].2\nFrom the global tempo estimates, we computed frequen-\ncies relevant to processing hierarchical timescales in mu-\n1https://www.mathworks.com/\n2https://labrosa.ee.columbia.edu/projects/\nbeattrack/sic, namely those corresponding to the beat (quarter note),\nas well as one fourth (whole note), half (half note), twice\n(eighth note), and four times (sixteenth note) the beat fre-\nquency. Previous studies have investigated contributions\nof beat frequencies to stimulus amplitude envelopes [28];\nhere we have taken a similar approach, visualizing low-\nfrequency magnitude spectra of the three features used for\nanalysis.\nThe audio waveforms, spectrograms, low-frequency\nmagnitude spectra, and PC1 loadings of the four stimuli\nare shown in Fig. 1. By visual inspection, it is apparent\nthat the four songs have different structures, and a variety\nof tempos. Furthermore, the feature FFTs show spectral\npeaks at both beat-relevant frequencies and other frequen-\ncies not occurring at multiples of the beat. Interestingly,\nPC1 loadings computed across the full set of 20 features\nare mostly consistent from song to song.\n2.3 Canonical Correlation Analysis\nCanonical Correlation Analysis (CCA) involves projecting\ntwo data sets onto subspaces such that the projections are\nmaximally correlated across time [9, 10, 13]. It has been\nused extensively in neuroscience, most recently as a tech-\nnique for investigating links between visual stimuli and\ntheir EEG responses [7]. This approach may be thought of\n“hybrid encoding-decoding”, in that the stimulus is tem-\nporally ﬁltered (encoded) and the neural response spatially\nﬁltered (decoded), with the ﬁltering optimized by CCA.\nThe result is a multidimensional measure of the stimulus-\nresponse correlation (SRC), where each dimension empha-\nsizes a different temporal component of the stimulus and a\ndifferent spatial component of the EEG.\nThe inputs to the CCA are two matrices. For the present\napplication, X2RL⇥Tis a convolution matrix of the\nstimulus feature where the row dimension spans time de-\nlays (“lags”) and the column dimension spans time. In this\nconstruction, temporal ﬁltering of the stimulus feature is\nachieved by multiplication with X. Matrix Y2RD⇥Tis\nthe EEG data, where the row dimension spans electrodes\nand the column dimension spans time. CCA on XandY\nproduces a matrix of temporal ﬁlters H2RL⇥Kand a\ncorresponding matrix of spatial ﬁlters W2RD⇥Kthat\nextract temporal and spatial components from the stimulus\nand EEG, respectively, where Kis the number of compo-\nnents. Therefore we obtain U=HTXandV=WTY,\nwhere Uis a matrix of temporally ﬁltered stimulus compo-\nnents, and Vis a matrix of spatially ﬁltered EEG compo-\nnents. The ﬁlters HandWare computed to maximize the\ncorrelation among corresponding rows of UandV(i.e.,\nthe components), under the constraint that the rows of U\nandVare temporally uncorrelated. The components are\nsorted in descending order of correlation, such that the ﬁrst\ncomponent pair (ﬁrst rows of UandV) are most strongly\ncorrelated.\nOn a per-song basis, we pooled the data across trials to\nlearn the model parameters. As the input sampling rates\nof the EEG and acoustical feature were not identical, we\nresampled the EEG to the sampling rate of the acoustical132 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017(a) Song 1: “Ainvayi Ainvayi”.\n(b) Song 2: “Daaru Desi”.\n(c) Song 3: “Haule Haule”.\n(d) Song 4: “Malang”.\nFigure 1 : Features of the songs used here as stimuli. From top left to bottom left in each pane are the waveform, spectro-\ngram, and low frequency spectrum of each individual feature used (PC1, RMS energy, and spectral ﬂux). On the right is\nthe loading vector for the ﬁrst PC computed across the 20 short-term features for that song.\nfeatures (80 Hz) beforehand. We performed separate CCA\ncomputations for each acoustical feature for each song,\nconsidering samplewise shifts up to 2 seconds in the con-\nstruction of the feature input matrix X.\nIn sum, the CCA procedure outputs a delay-by-\ncomponent stimulus ﬁlter matrix H, an electrodes-by-\ncomponent response ﬁlter W, as well as the time samples-\nby-component ﬁltered data outputs VandU. The Matlab\ncode used to perform these analyses is made publicly avail-\nable through GitHub.3\n2.4 Visualizing the CCA Filters\nWhile the columns of Wprovide the spatial ﬁlter weights,\na “forward model” is recommended for visualizing com-\nponent topographies on the scalp [12]. Thus, we used\nthe EEG covariance matrix R=YYTto compute the\nforward-model projection A=RW(WTRW)\u00001[29].\nThe columns of Arepresent the projection of the compo-\nnent onto the scalp and are visualized topographically.\nFor the temporal ﬁlters, we are interested primarily in\ntheir spectral characteristics, particularly at musically rel-\nevant (beat-related) frequencies. Therefore, we computed\nthe FFT of each temporal ﬁlter and plotted its magnitude\nspectrum.\n3http://jd-lab.org/resources/2.5 Stimulus-to-Response Correlations\nThe CCA procedure described above outputs UandVma-\ntrices containing the ﬁltered data on a per-song, per-feature\nbasis. We computed SRC for the ﬁrst 5 components on\na per-trial basis across the full duration of the trial. We\nreport the mean correlation coefﬁcient across trials, on a\nper-component, per-feature, per-song basis.\nDue to autocorrelation characteristics of the stimulus\nand response data [37], we assessed statistical signiﬁcance\nusing a permutation test approach [39]. This was done by\nimplementing the following procedure for each CCA com-\nputation performed above: First, we disrupted the tem-\nporal structure of individual trials of input EEG (while\npreserving aggregate spectral content) by phase scram-\nbling the data from each electrode. Following that, the\nCCA and SRC computations were performed using the\nphase-scrambled EEG and intact acoustical feature as in-\nputs. This procedure was repeated 500 times. We com-\npared the SRC from intact data to the distribution of SRC\nacross permutation iterations for computation of p-values.\nWe corrected for multiple comparisons using False Dis-\ncovery Rate (FDR) [4]. Reported statistical signiﬁcance\n(p<0.05) and marginal signiﬁcance ( 0.05p<0.1)\nreﬂect FDR correction.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 133Figure 2: CCA ﬁlters. The spatial and temporal ﬁlters comprising each CCA pair are visualized for all songs and inputfeatures. Shown are the component topographies (spatial ﬁlters), as well as the frequency-domain representations of thetemporal ﬁlters. The ﬁrst 3 CCs are plotted. In each spectrum, vertical lines denote one fourth the beat frequency (blue),half the beat (orange), beat (green), twice the beat (red), and four times the beat (purple).3. RESULTS3.1 Spatial and Temporal FiltersWe ﬁrst probed the spatial topographies of the EEG com-ponents that best represented the musical features. Fig. 2shows that the topography of component 1 is commonacross songs and features, up to a sign ambiguity inher-ent to CCA. The symmetric frontocentral topography ofCC1 matches various past results involving spatial decom-position of brain responses during natural music listening[17,33,38]. Unlike the ﬁrst CC, the second and third com-ponents tend to vary with the stimulus, but possess smoothand broad topographies consistent with the projections ofcortical sources onto the scalp.Interestingly, the temporal ﬁlters of each component arefocused on harmonics and subharmonics of the song’s beatfrequency. In the case of CC1, the frequency responsesof these ﬁlters tend to show peaks at higher beat-relatedfrequencies (eighth and sixteenth notes). Subsequent CCstend to show peaks at lower beat related frequencies (wholeand half notes). Two exceptions to this are the ﬁlters forPC1 and spectral ﬂux in Song 1. In both cases CC3 heavilyemphasizes the sixteenth note frequency.While some temporal ﬁlters within a single song andfeature show similar frequency responses, these compo-nents can be differentiated by their phase. For example,Fig. 3 shows the time-domain representation of the tem-poral ﬁlters output for Song 1 RMS and Song 2 RMS. In\nFigure 3: Time-domain examples of temporal ﬁlters withsimilar spectra but different phasing. Left: CC2 and CC3for Song 1 RMS. Right: CC2 and CC3 for Song 2 RMS.both cases, the second and third ﬁlters emphasize wholenote frequencies, but with a different phase. In general, alltemporal ﬁlters show far more energy in beat-related fre-quencies than elsewhere.3.2 Stimulus-to-EEG CorrelationsThe results of our CCA procedure show multiple dimen-sions of signiﬁcant correlation between the stimulus andbrain response. As shown in Table 1, CC1 produces statis-tically signiﬁcant SRC (p<0.05after FDR) for all songsand stimulus features. For the remaining components, thecoefﬁcients vary in strength across songs and features, butmultiple dimensions of signiﬁcance and marginal signiﬁ-cance (0.05p<0.1) are observed. We note that thereis not a universal correspondence between correlation co-134 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Comp. PC1 Flux RMSSong 1CC1 0.0652** 0.0690** 0.0630**\nCC2 0.0280** 0.0242** 0.0275*\nCC3 0.0212** 0.0179** 0.0153\nCC4 0.0177** 0.0179** 0.0123\nCC5 0.0135** 0.0102** 0.0115Song 2CC1 0.0522** 0.0573** 0.0524**\nCC2 0.0301** 0.0244** 0.0268**\nCC3 0.0183* 0.0177** 0.0203**\nCC4 0.0158** 0.0109** 0.0137*\nCC5 0.0119** 0.0062 0.0104**Song 3CC1 0.0460** 0.0530** 0.0437**\nCC2 0.0225 0.0306** 0.0254**\nCC3 0.0171* 0.0213** 0.0254**\nCC4 0.0139** 0.0119 0.0110\nCC5 0.0099* 0.0084 0.0074Song 4CC1 0.0536** 0.0511** 0.0475**\nCC2 0.0248 0.0324** 0.0261*\nCC3 0.0194* 0.0192** 0.0203**\nCC4 0.0170** 0.0178** 0.0135\nCC5 0.0114** 0.0102* 0.0089\nTable 1 : Multidimensional stimulus-response correlations\ncaptured by CCA. ‘**’ denotes statistical signiﬁcance ( p<\n0.05) and ‘*’ denotes marginal signiﬁcance ( 0.05p<\n0.1) after correcting for FDR.\nefﬁcients and statistical signiﬁcance. For example, in CC5\nof Song 1, the correlation coefﬁcient of ⇢=0.0102 for\nFlux is signiﬁcant, while the slightly larger ⇢=0.0115 for\nRMS is not. This is due to the fact that separate permuta-\ntion tests were performed, and surrogate EEG data gener-\nated, for each song and audio feature.\n4. DISCUSSION\nThe technique outlined here provides a way to study mu-\nsic processing by direct comparison of an auditory stim-\nulus and its corresponding brain response. Using CCA,\nmatching spatial and temporal ﬁlters emerge that maxi-\nmally correlate the stimulus and response in time. We\nfound multiple dimensions of statistically signiﬁcant cor-\nrelation between stimulus and response. While the magni-\ntudes of these correlations are small, the fact that they are\nnot conﬁned to a single dimension suggests that multiple\nbrain areas process distinct portions of the stimulus. Such\na multidimensional correlation could not be detected using\nsensor-space processing.\nIn past CCA studies using audio-visual stimuli [7],\nanalysis of temporal ﬁlter resonances lacked clear relation-\nships to the stimuli. However, the music studied here is\norganized by a hierarchy of beat- and measure-related pe-\nriodicities, providing direct references with which to com-\npare the temporal ﬁlter frequency responses. Here we\nfound that the temporal ﬁlters that extract neurally rele-\nvant musical features are focused at harmonics of the beat\nfrequency, independent of the song or feature.\nEach CCA dimension emphasizes different brain\nsources (e.g., spatial topographies) and different combina-\ntions of harmonics. These results thus suggest that dif-\nFigure 4 : Total stimulus-response correlation for the ﬁrst\n3 CCs of each song and feature. The stacked bar graphs\ndepict the proportion of stimuls-response correlation con-\ntributed by each CCA dimension.\nferent temporal hierarchies of music are processed by dis-\ntinct neural circuits. The topographical consistency of\nthe strongest component, CC1, across all songs suggests\na common mechanism for natural music listening. This\nconsistency across songs is especially intriguing since the\nEEG data reﬂect disjoint sets of listeners for each song.\nFrom the analysis of SRC signiﬁcance, PC1 does not\nappear to outperform other input features. In fact, spectral\nﬂux is the only feature that produces at least three signiﬁ-\ncant components in each of the four songs. This may seem\ncounterintuitive, but there is no guarantee that a feature ex-\nplaining the most variance in the audio data will do the\nsame for brain data. Indeed, the objective of CCA is to\nmaximize covariance between the two data sets.\nThe correlations between musical features and EEG\nresponses found here, while statistically signiﬁcant, are\nfairly low ( <0.1). The low signal-to-noise ratio (SNR) of\nEEG severely limits the magnitudes of stimulus-response\ncorrelations, particularly with linear techniques as were\nused here. Moreover, the EEG recorded during music lis-\ntening is driven mostly by sources unrelated to the au-\nditory stimulus. The response to the stimulus comprises\nonly a fractional component of the overall neural activ-\nity. Even with more sophisticated imaging modalities such\nas fMRI, correlation coefﬁcients on the order of 0.1are\ntypically observed [11]. In order to increase the correla-\ntions between stimulus features and brain responses, non-\nlinear techniques such as deep neural networks could be\nemployed in order to account for higher-order correlations\nand complex relationships not captured by linear CCA.\nIn research combining acoustical feature extraction and\nbrain responses, it is important to consider the relative\ntime scales on which stimuli and corresponding brain re-\nsponses are sampled. Acoustical features are broadly sep-\narated into short- and long-term features. Past research has\nused the short-term features described above, as well as\nlong-term “texture windows” with temporal resolution of\naround 1 Hz (e.g., 3-sec window with 33% overlap) [43].\nRecording modalities for cortical responses can similarlyProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 135be grouped by their temporal resolution. For example,\nEEG provides high temporal resolution (typically up to\n1000 Hz for cortical responses), while fMRI offers a sam-\npling frequency of only around 0.5 Hz [1]. Thus, in terms\nof time scales, EEG is amenable to short-term features and\nfMRI to long-term. Interestingly, however, many studies\nto date seem mismatched in this regard. There exist both\nfMRI studies utilizing short-term features [1,42] and EEG\nstudies utilizing long-term features [6,22,44], meaning sig-\nniﬁcant upsampling or downsampling was needed to com-\npare stimulus features with responses. The present study is\nthe ﬁrst to our knowledge to non-invasively examine stim-\nulus to response mapping in natural listening using exclu-\nsively matching timescales.\nWhen choosing brain response recording modalities for\nthis type of research, it is important to understand the trade-\noffs in temporal and spatial resolution. Unlike hemody-\nnamic signals of fMRI, the electrical signals recorded by\nEEG have been refracted through the skull and scalp; thus,\nobserved topographies represent signals at speciﬁc elec-\ntrodes, but not necessarily activations of speciﬁc underly-\ning brain regions. ECoG methods solve this problem by\nplacing electrodes directly on the cortex, but require inva-\nsive procedures and generally record from a smaller num-\nber of electrodes over a small region of the brain.\nThe present study correlated time-domain representa-\ntions of both the acoustical features and EEG responses.\nThe CCA approach could also be applied to transforms of\neither input. Past EEG and ECoG studies have examined\ntime-frequency representations [6,22, 31, 32, 37] and com-\npared audio features with oscillatory band power in brain\nresponses. Alternative stimulus input representations can\nalso be considered. Time frequency transforms of the au-\ndio such as the Constant-Q Transform or other ﬁlterbank\ndecompositions could be used as long- or short-term input\nfeatures depending on the temporal resolution of interest.\nUsing predetermined and hand-engineered features, as we\ndid here, can also be limiting. The features used here are\nwell represented in past research, but it could be beneﬁcial\nfor a system to learn the audio features themselves with\nthe goal of improving the output of the optimization—for\nexample with deep neural network approaches that have\nbeen applied to learn features for music tagging and signal\nprocessing systems [5, 34].\nHere we have chosen to average SRC coefﬁcients for\neach song and feature across the full duration of the stimu-\nlus, producing a global correlation measure for each set of\ncomponents. It is also possible to compute a time-varying\nmeasure of SRC and further investigate the musical events\ncorresponding to moments of especially high or low SRC.\nPast research has even linked time-varying SRC to the at-\ntentional state of participants [7], pointing to application\nas a surrogate measure of listener attention. This approach\ncould prove useful in an MIR context, providing a con-\ntinuous, objective (brain-based) measure of attention to a\nreal-world musical work.\nWhile public access to naturalistic listening data re-\nmains limited, additional options exist. Given the limita-tions of the NMED-H dataset, it would be helpful to test\nthis method on EEG datasets that reﬂect a wider range of\nmusical genres [36] and tempos [23, 24].\nFuture research may also consider differing stimuli\nacross participants. Here, each CCA computation oper-\nated over concatenated EEG responses to a shared stimu-\nlus (e.g., all responses to Song 1). However, CCA has also\nbeen used to derive correlated components for unique per-\nceptual experiences such as video game play [7]. MIR ap-\nplications of this approach could involve pooling responses\nto different performances of the same song, or allowing\nparticipants to choose personal favorites. In addition, it\nwill be interesting to investigate further the composition of\nthe temporal stimulus ﬁlters, which for the present analy-\nses are tightly coupled to beat frequencies, when songs of\nvarious tempos are analyzed together.\n5. ACKNOWLEDGMENTS\nThis research was supported by the Wallenberg Network\nInitiative: Culture, Brain, Learning; the Roberta Bowman\nDenning Fund for Humanities and Technology; and the\nPatrick Suppes Gift Fund. The authors thank Anthony Nor-\ncia for helpful feedback on the analyses presented here.\n6. REFERENCES\n[1]V. Alluri, P. Toiviainen, I. P. J ¨a¨askel ¨ainen, E. Glerean,\nM. Sams, and E. Brattico. Large-scale brain networks\nemerge from dynamic processing of musical timbre,\nkey and rhythm. NeuroImage , 59(4):3677–3689, 2012.\n[2]V. Alluri, P. Toiviainen, T. E. Lund, M. Wallentin,\nP. Vuust, A. K. Nandi, T. Ristaniemi, and E. Brattico.\nFrom Vivaldi to Beatles and back: predicting lateral-\nized brain responses to music. NeuroImage , 83:627–\n636, 2013.\n[3]J. J. Aucouturier and E. Bigand. Seven problems that\nkeep MIR from attracting the interest of cognition and\nneuroscience. Journal of Intelligent Information Sys-\ntems, 41(3):483–497, 2013.\n[4]Y. Benjamini and D. Yekutieli. The control of the false\ndiscovery rate in multiple testing under dependency.\nThe Annals of Statistics , 29(4):1165–1188, 2001.\n[5]K. Choi, G. Fazekas, and M. Sandler. Automatic tag-\nging using deep convolutional neural networks. In IS-\nMIR, pages 805–811, 2016.\n[6]F. Cong, V. Alluri, A. K. Nandi, P. Toiviainen, R. Fa,\nB. Abu-Jamous, L. Gong, B. G. W. Craenen, H. Poiko-\nnen, M. Huotilainen, and T. Ristaniemi. Linking brain\nresponses to naturalistic music through analysis of on-\ngoing EEG and stimulus features. IEEE Transactions\non Multimedia , 15(5):1060–1069, 2013.\n[7]J. P. Dmochowski, J. Ki, P. De Guzman, P. Sajda,\nand L. C. Parra. Extracting multidimensional stimulus-\nresponse correlations using hybrid encoding-decoding\nof neural activity. NeuroImage , 2017.136 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[8]D. P. W. Ellis. Beat tracking by dynamic programming.\nJournal of New Music Research , 36(1):51–60, 2007.\n[9]H. R. Glahn. Canonical correlation and its relationship\nto discriminant analysis and multiple regression. Jour-\nnal of the Atmospheric Sciences , 25(1):23–31, 1968.\n[10] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor.\nCanonical correlation analysis: An overview with ap-\nplication to learning methods. Neural Computation ,\n16(12):2639–2664, 2004.\n[11] Uri Hasson, Yuval Nir, Ifat Levy, Galit Fuhrmann,\nand Rafael Malach. Intersubject synchronization\nof cortical activity during natural vision. science ,\n303(5664):1634–1640, 2004.\n[12] S. Haufe, F. Meinecke, K. G ¨orgen, S. D ¨ahne, J.-D.\nHaynes, B. Blankertz, and F. Bießmann. On the inter-\npretation of weight vectors of linear models in multi-\nvariate neuroimaging. NeuroImage , 87:96–110, 2014.\n[13] H. Hotelling. Relations between two sets of variates.\nBiometrika , 28(3/4):321–377, 1936.\n[14] Y. F. Huang, S. M. Lin, H. Y. Wu, and Y. S. Li. Music\ngenre classiﬁcation based on local feature selection us-\ning a self-adaptive harmony search algorithm. Data &\nKnowledge Engineering , 92:60–76, 2014.\n[15] B. Kaneshiro. Toward an Objective Neurophysiologi-\ncal Measure of Musical Engagement . PhD thesis, Stan-\nford University, 2016.\n[16] B. Kaneshiro and J. P. Dmochowski. Neuroimaging\nmethods for music information retrieval: Current ﬁnd-\nings and future prospects. In ISMIR , pages 538–544,\n2015.\n[17] B. Kaneshiro, J. P. Dmochowski, A. M. Norcia, and\nJ. Berger. Toward an objective measure of listener en-\ngagement with natural music using inter-subject EEG\ncorrelation. In ICMPC13 , 2014.\n[18] B. Kaneshiro, D. T. Nguyen, J. P. Dmochowski,\nA. M. Norcia, and J. Berger. Naturalistic music EEG\ndataset—Hindi (NMED-H). In Stanford Digital Repos-\nitory, 2016.\n[19] O. Lartillot and P. Toiviainen. A Matlab toolbox for\nmusical feature extraction from audio. In DAFx , pages\n237–244, 2007.\n[20] D. J. Levitin and V. Menon. Musical structure is pro-\ncessed in language areas of the brain: a possible role\nfor brodmann area 47 in temporal coherence. NeuroIm-\nage, 20(4):2142–2152, 2003.\n[21] T. Li, M. Ogihara, and Q. Li. A comparative study\non content-based music genre classiﬁcation. In SIGIR ,\npages 282–289, 2003.\n[22] Y.-P. Lin, J.-R. Duann, W. Feng, J.-H. Chen, and T.-\nP. Jung. Revealing spatio-spectral electroencephalo-\ngraphic dynamics of musical mode and tempo percep-\ntion by independent component analysis. Journal of\nNeuroengineering and Rehabilitation , 11(1):18, 2014.[23] S. Losorelli, D. T. Nguyen, J. P. Dmochowski,\nand B. Kaneshiro. Naturalistic music EEG dataset—\nTempo (NMED-T). In Stanford Digital Repository ,\n2017.\n[24] S. Losorelli, D. T. Nguyen, J. P. Dmochowski, and\nB. Kaneshiro. NMED-T: A tempo-focused dataset of\ncortical and behavioral responses to naturalistic music.\nInISMIR , 2017.\n[25] L. Lu, D. Liu, and H. J. Zhang. Automatic mood detec-\ntion and tracking of music audio signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n14(1):5–18, 2006.\n[26] M. I. Mandel and D. P. W. Ellis. Song-level features\nand support vector machines for music classiﬁcation.\nInISMIR , pages 594–599, 2005.\n[27] T. Naselaris, K. N. Kay, S. Nishimoto, and J. L. Gal-\nlant. Encoding and decoding in fMRI. NeuroImage ,\n56(2):400–410, 2011.\n[28] S. Nozaradan, I. Peretz, and A. Mouraux. Selective\nneuronal entrainment to the beat and meter embed-\nded in a musical rhythm. The Journal of Neuroscience ,\n32(49):17572–17581, 2012.\n[29] L. C. Parra, C. D. Spence, A. D. Gerson, and P. Sajda.\nRecipes for the linear analysis of EEG. NeuroImage ,\n28(2):326–341, 2005.\n[30] H. Poikonen, V. Alluri, E. Brattico, O. Lartillot,\nM. Tervaniemi, and M. Huotilainen. Event-related\nbrain responses while listening to entire pieces of mu-\nsic.Neuroscience , 312:58–73, 2016.\n[31] C. Potes, P. Brunner, A. Gunduz, R. T. Knight, and\nG. Schalk. Spatial and temporal relationships of elec-\ntrocorticographic alpha and gamma activity during au-\nditory processing. NeuroImage , 97:188–195, 2014.\n[32] C. Potes, A. Gunduz, P. Brunner, and G. Schalk. Dy-\nnamics of electrocorticographic (ECoG) activity in hu-\nman temporal and frontal cortical areas during music\nlistening. NeuroImage , 61(4):841–848, 2012.\n[33] R. S. Schaefer, J. Farquhar, Y. Blokland, M. Sadakata,\nand P. Desain. Name that tune: Decoding music from\nthe listening brain. NeuroImage , 56(2):843–849, 2011.\n[34] S. Sigtia and S. Dixon. Improved music feature learn-\ning with deep neural networks. In ICASSP , pages\n6959–6963, 2014.\n[35] Y. Song, S. Dixon, and M. Pearce. Evaluation of musi-\ncal features for emotion classiﬁcation. In ISMIR , pages\n523–528, 2012.\n[36] S. Stober, A. Sternin, A. M. Owen, and J. A. Grahn.\nTowards music imagery information retrieval: Intro-\nducing the OpenMIIR dataset of EEG recordings from\nmusic perception and imagination. In ISMIR , 2015.\n[37] I. Sturm, B. Blankertz, C. Potes, G. Schalk, and G. Cu-\nrio. ECoG high gamma activity reveals distinct cor-\ntical representations of lyrics passages, harmonic and\ntimbre-related changes in a rock song. Frontiers in Hu-\nman Neuroscience , 8:798, 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 137[38] I. Sturm, S. D ¨ahne, B. Blankertz, and G. Curio.\nMulti-variate EEG analysis as a novel tool to examine\nbrain responses to naturalistic music stimuli. PloS one ,\n10(10):e0141281, 2015.\n[39] J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, and\nJ. D. Farmer. Testing for nonlinearity in time series: the\nmethod of surrogate data. Physica D: Nonlinear Phe-\nnomena , 58(1):77–94, 1992.\n[40] P. Toiviainen, V. Alluri, E. Brattico, M. Wallentin, and\nP. Vuust. Capturing the musical brain with lasso: Dy-\nnamic decoding of musical features from fMRI data.\nNeuroimage , 88:170–180, 2014.\n[41] K. Trohidis, G. Tsoumakas, G. Kalliris, and Ioannis P.\nVlahavas. Multi-label classiﬁcation of music into emo-\ntions. In ISMIR , pages 325–330, 2008.\n[42] W. Trost, S. Fr ¨uhholz, T. Cochrane, Y. Cojan, and\nP. Vuilleumier. Temporal dynamics of musical emo-\ntions examined through intersubject synchrony of brain\nactivity. Social cognitive and affective neuroscience ,\npage nsv060, 2015.\n[43] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on speech and\naudio processing , 10(5):293–302, 2002.\n[44] D. Wang, F. Cong, Q. Zhao, P. Toiviainen, A. K. Nandi,\nM. Huotilainen, T. Ristaniemi, and A. Cichocki. Ex-\nploiting ongoing EEG with multilinear partial least\nsquares during free-listening to music. In IEEE 26th\nInternational Workshop on Machine Learning for Sig-\nnal Processing (MLSP) , pages 1–6, 2016.138 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Towards Computational Modeling of the Ungrammatical in a Raga Performance.",
        "author": [
            "Kaustuv Kanti Ganguli",
            "Preeti Rao"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417349",
        "url": "https://doi.org/10.5281/zenodo.1417349",
        "ee": "https://zenodo.org/records/1417349/files/GanguliR17.pdf",
        "abstract": "Raga performance allows for considerable flexibility in interpretation of the raga grammar in order to incorporate elements of creativity via improvisation. It is therefore of much interest in pedagogy to understand what ungrammat- icality might mean in the context of a given raga, and pos- sibly develop means to detect this in an audio recording of the raga performance. One prominent notion is that un- grammaticality is considered to occur only when the per- former “treads” on another, possibly allied, raga in a lis- tener’s perception. With this view, we consider modeling the technical boundary of a raga as that which separates it from another raga that is closest to it in its distinctive features. We wish to find computational models that can indicate ungrammaticality using a data-driven estimation of the model parameters; i.e. the raga performances of great artists are used to obtain representations that discrim- inate most between same and different raga performances. We choose a well-known pair of allied ragas (Deshkar and Bhupali in north Indian classical music) for an empirical study of computational representations for the distinctive attributes of tonal hierarchy and melodic shape of a chosen common descending phrase.",
        "zenodo_id": 1417349,
        "dblp_key": "conf/ismir/GanguliR17",
        "keywords": [
            "Raga performance",
            "interpretation flexibility",
            "improvisation",
            "pedagogy interest",
            "ungrammaticality",
            "audio recording",
            "performers perception",
            "technical boundary",
            "computational models",
            "data-driven estimation"
        ],
        "content": "TOWARDS COMPUTATIONAL MODELING OF THE UNGRAMMATICAL\nIN A RAGA PERFORMANCE\nKaustuv Kanti Ganguli Preeti Rao\nDepartment of Electrical Engineering,\nIndian Institute of Technology Bombay, Mumbai, India.\nfkaustuvkanti,prao g@ee.iitb.ac.in\nABSTRACT\nRaga performance allows for considerable ﬂexibility in\ninterpretation of the raga grammar in order to incorporate\nelements of creativity via improvisation. It is therefore of\nmuch interest in pedagogy to understand what ungrammat-\nicality might mean in the context of a given raga, and pos-\nsibly develop means to detect this in an audio recording\nof the raga performance. One prominent notion is that un-\ngrammaticality is considered to occur only when the per-\nformer “treads” on another, possibly allied, raga in a lis-\ntener’s perception. With this view, we consider modeling\nthe technical boundary of a raga as that which separates\nit from another raga that is closest to it in its distinctive\nfeatures. We wish to ﬁnd computational models that can\nindicate ungrammaticality using a data-driven estimation\nof the model parameters; i.e. the raga performances of\ngreat artists are used to obtain representations that discrim-\ninate most between same and different raga performances.\nWe choose a well-known pair of allied ragas (Deshkar and\nBhupali in north Indian classical music) for an empirical\nstudy of computational representations for the distinctive\nattributes of tonal hierarchy and melodic shape of a chosen\ncommon descending phrase.\n1. INTRODUCTION\nThe melodic framework in Indian art music is governed by\nthe system of ragas. A raga can be viewed as falling some-\nwhere between a scale and a tune in terms of its deﬁning\ngrammar which includes the tonal material, tonal hierar-\nchy, and characteristic melodic phrases [26, 31]. Descrip-\ntion of the raga grammar, as found in text resources or\nas verbalized in pedagogy, typically comprises of a list-\ning of the allowed notes (svara) of the 12-tone scale, as-\ncending and descending svara patterns, the mention of the\nmost important svaras and a list of common phrases (svara\nsequences). While the texts do not explicitly describe\nthe precise svara intonations or the actual melodic shapes\nc\rKaustuv Kanti Ganguli and Preeti Rao. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Kaustuv Kanti Ganguli and Preeti Rao. “Towards\nComputational Modeling of the Ungrammatical in a Raga Performance”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.of the phrases in terms of the transitions between svaras,\nthe audio analyses of raga performances has demonstrated\nwhat is well-known to practitioners, i.e. the shape of the\ncontinuous pitch contour corresponding to the phrase is\ncharacteristic of the raga and therefore relatively invariant\nacross performances in a given raga [28, 29].\nThe raga grammar can be viewed as a set of constraints\nwithin which creativity is given a free hand to realise what-\never is aesthetically pleasing in the all-important melodic\nimprovisation component, so characteristic of the genre. It\nis therefore of interest to understand, and possibly model,\nthe technical boundary of a raga in terms of what might\nconstitute ungrammaticalilty in a performance. The tech-\nnical boundary would be speciﬁed in terms of the deﬁning\nattributes such as tonal material and hierarchy, and phrase\nshapes. Such an exercise could lead to the development of\ncomputational tools for assessing performance accuracy in\npedagogy together with the complementary aspect of cre-\native skill. A popular notion of grammaticality in perfor-\nmance occurs around the notion of preserving a raga’s es-\nsential distinctiveness in terms of the knowledgeable lis-\ntener’s perception [1–3, 5, 9, 24, 27, 30]. Thus, a perfor-\nmance with possibly many creative elements can still be\nconsidered not to transgress the raga grammar as long as\nit does not “tread on another raga” [24, 27, 35]. The tech-\nnical boundary of a raga should therefore ideally be speci-\nﬁed in terms of limits on the deﬁning attributes where it is\nexpected that the limit depends on the proximity of other\nragas with respected to the selected attribute.\nThe computational modeling of the distinctive attributes\nof a raga has been the subject of previous research mo-\ntivated by the task of raga recognition from audio given\na large training dataset of performances across several ra-\ngas. The tonal material has been represented by a vari-\nety of ﬁrst order pitch distributions and experimental out-\ncomes based on recognition performance have been used to\ncomment on the relative superiority of a given representa-\ntion [4, 6–8, 11, 17, 22]. Motivated by the pitch-continuous\nnature of the melody, ﬁnely-binned histograms of octave-\nfolded instantaneous pitch values have been used as tem-\nplates in raga recognition tasks [8, 22]. Alternately, 12-bin\ndistributions of pitch values within detected stable svara\nregions have been used to represent a raga’s tonal con-\ntent [7, 11]. Melodic shape invariance of phrases, on the\nother hand, has been used in the modeling of similarity39measures for the task of melodic motif detection within\nand across performances of a given raga [12,19–21,28,29].\nPitch contour shape is typically represented by a tonic-\nnormalized time-series or reduced to a symbol string of\nthe raga notes [14, 16].\nIn this work, we consider the computational modeling\nof tonal hierarchy and phrase shape based on maximiz-\ning the discrimination of ‘close’ ragas with respect to the\ngiven attribute. Such an approach has not been used in\nprevious work. The notion of “allied ragas” is helpful\nhere where we consider ragas that share the same gram-\nmar in major attributes while differing in a few. For ex-\nample, the pentatonic ragas Deshkar and Bhupali have the\nsame set of svaras ( S;R;G;P;Dcorresponding to 0, 200,\n400, 700, and 900 cents respectively) and common phrases\nin terms of svara sequences, e.g. the descending phrase\nGRS . Learners are typically introduced to the two ragas\ntogether and warned against confusing them [24, 30, 34].\nRecently, subjective experiments on perceived similarity\nby musicians of synthetically manipulated raga phrases\nclearly demonstrated the existence of a sharp boundary be-\ntween valid variants of a given raga phrase from variants\nof the same phrase (i.e. in terms of svara sequence) from\nan allied raga [15].\nIn the present work, we consider the pair of allied ragas,\nDeshkar and Bhupali, and use the performances of eminent\nHindustani vocalists as proxy for creatively expressed, but\ngrammatically accurate, examples of the stated raga. The\nperformances in the allied raga are likewise considered to\nbe examples of the corresponding ungrammatical render-\nings. We evaluate known, as well as some new, represen-\ntations in terms of the achieved discrimination on a dataset\nof performances across the two ragas. Although the scope\nof the experiments is restricted to the given pair of ragas\nand chosen attributes, we expect our outcomes to be gen-\neralizable.\nIn the next section, we introduce our dataset along with\nthe necessary musicological background, and describe the\naudio processing required to obtain the continuous pitch\ntrack that forms the basis for the computational representa-\ntions under study. This is followed by sections that discuss\npotential representations for tonal hierarchy and phrase\nshape with associated distance measures. We next present\nan experimental study of the discrimination performance\nfollowed by our conclusions.\n2. DATASET AND AUDIO PROCESSING\nTable 1 presents a comparison of the melodic attributes\ncorresponding to the grammars of the allied ragas as com-\npiled from musicology texts. These cover the aspects\nof duration and intonation of the tonal material that in-\ncludes ascending ( Ar) and descending ( Av) scales, domi-\nnant ( Vadi) and subdominant ( Samvadi ), and characteristic\nphrases. ‘Natural shruti’ (last row) refers to the Just In-\ntonation tuning, but there is no quantiﬁcation of the term\n‘higher’. Also, there is some indication of a duration con-\nstraint on R(as a short or passing svara relative to its neigh-\nbours) in the form of braces (e.g. G(R)S in raga Deshkar).2.1 Dataset and Annotation\nThe audio recordings used in this study are drawn from\nthe Hindustani music corpus from ‘Dunya’1compiled\nas a representative set of the vocal performances in the\ngenre [33]. The editorial metadata for each audio record-\ning is publicly available on the metadata repository Mu-\nsicBrainz2. The Dunya corpus for raga Deshkar com-\nprises 5 concerts of which 4 are selected for the current\nstudy, omitting the drut (fast tempo) concert due to the dis-\ntinctly different style of realising phrases associated with\nsuch tempi [24]. Similarly, we selected 5 concerts for the\nBhupali test set. We augmented the overall dataset, as de-\nscribed in Table 2, by additional concerts from personal\ncollections.\nNext, we annotate the occurrences of the chosen phrase,\nGRS . As discussed earlier, the GRS phrase is common\nto the two ragas and a frequently used descending motif.\nTheGRS phrases are distributed across three octaves (up-\nper, middle, and lower octaves), although lower octave in-\nstances are fewer. The segmentation of the phrases from\nthe alap and vistar (i.e. improvised sections of the concert)\nis carried out semi-automatically as follows. A musician\nindicated the coarse location of each instance of the cho-\nsen phrase; this was then reﬁned to obtain segmentation\nboundaries by automatic onset and offset detection meth-\nods described later. A count of the phrases used in this\nstudy is presented per concert in Table 2. The phrase-level\nannotation of the remaining concerts is underway for fu-\nture work.\n2.2 Pitch Time-series Extraction from Audio\nPredominant-F0 detection is implemented by an algorithm\nproposed by [32] that exploits the spectral properties of the\nvoice with temporal smoothness constraints on the pitch\ncontour. The pitch is detected at 10 ms intervals with zero\npitch assigned to the detected purely instrumental regions.\nThe pitch values in Hz are converted to the cents scale by\nnormalizing with respect the concert tonic determined au-\ntomatically using a multi-pitch approach [18]. The ﬁnal\npre-processing step is to interpolate short silence regions\n1https://dunya.compmusic.upf.edu/Hindustani\n2https://musicbrainz.org/\nDeshkar Bhupali\nTonal material: SRGPD Tonal material: SRGPD\nAr:SGPD ;SPDS\nAv:S;PDGP ;DPG(R)SAr:SRG;PDS\nAv:SDP;GDP;GRS\nVadi:D,Samvadi :G Vadi:G,Samvadi :D\nPhrases: SG;G(P)DPD ;\nP(D)SP ;DGP;DPG(R)SPhrases: RDS;RPG;\nPDS;SDP;GDP;GRS\nHigher shrutis of R;G;D Natural shrutis of R;G;D\nTable 1 . Speciﬁcation of raga grammar for the two allied\nragas of the present study [1, 5, 25, 30]40 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Raga # ConcertsDuration\n(hours)# Artists#GRS\nphrases\nDeshkar 6 2:16:50 5 52\nBhupali 6 3:22:00 5 107\nTable 2 . Description of the test dataset.\nbelow a threshold (250 ms which is empirically chosen as\nproposed by [13]) indicating musically irrelevant breath\npauses or unvoiced consonants, by cubic spline interpo-\nlation, to ensure the integrity of the melodic shape. A me-\ndian ﬁltering with a 50 ms window is performed to get rid\nof spurious pitch excursions. Eventually, we obtain a con-\ntinuous time-series of pitch values representing the melody\nline throughout the vocal regions of the concert.\n3. MELODIC REPRESENTATIONS\nOur goal is to propose computational representations that\nrobustly capture particular melodic characteristics of the\nraga in a performance while being sensitive enough to the\ndifferences between allied ragas. Given that tonal material\nand hierarchy of svaras are an important component of the\nraga grammar, we consider representations of tonal hierar-\nchy computable from the melody extracted from the audio\nrecording of the performance. We also consider the repre-\nsentation of the melodic shape of a selected characteristic\nphrase.\n3.1 Representation of Tonal Hierarchy\nTonal hierarchies, manifested in the relative frequencies\nand durations with which the tones are sounded in a musi-\ncal piece, have been linked to key identiﬁcation in Western\nart music. In a widely known work, Krumhansl [23] used\na 12-element vector to code the total duration (in terms of\nnumber of beats) of each note of the chromatic scale in a\npiece and correlated it with each of 24 templates represent-\ning the major and minor keys to obtain accurate predictions\nof key. It is therefore logical to consider the same represen-\ntation for raga discrimination. However, given the pitch-\ncontinuous nature of the music, we are faced with multiple\ncompeting options in the deﬁnition of a tonal representa-\ntion. Closest to the tonal hierarchy vector of Krumhansl\nis the 12-element histogram of the total duration of each\nof the “stable notes” detected from the melodic contour.\nConsidering the importance of the transitions connecting\nstable notes as well as microtonal differences in intona-\ntion between the same svara in different ragas, a histogram\nderived from all the pitch values in the melodic contour\nwould seem more suitable. The bin width for such a pitch\ncontinuous distribution is also a design choice we must\nmake. Finally, we need a distance measure computable\nbetween the histogram representations that correlates well\nwith closeness of the compared performances in terms of\nraga identity.3.1.1 Pitch Salience Histogram\nThe input to the system is tonic normalized pitch contour\n(cents vs time). The pitch values are octave-folded (0 -\n1200 cents) and quantized to pbins of equal width (the bin\nresolution is1200\np). The bin centres are arithmetic mean of\nthe adjacent bin edges. The salience of each bin is propor-\ntional to the accumulated duration of the pitch value cor-\nresponding to that bin. The normalization is to construct a\nprobability distribution fuction (pdf) where the area under\nthe histogram sums to unity. Given the number of bins, the\nhistogram is computed as:\nHk=NX\nn=11[ck\u0014F(n)\u0014ck+1] (1)\nwhere Hkis the salience of the kthbin,F(n)is the ar-\nray of pitch values, (ck; ck+1)are the bounds of the kthbin\nand 1is an indicator random variable3. Figure 1 shows\nthe pitch salience histogram for p= 1200 (1 cent bin res-\nolution). For a bin resolution of 100 cents, the representa-\ntion is equivalent to the conventional pitch class distribu-\ntion (PCD) [7].\nFigure 1 . Pitch salience histograms (octave folded, 1 cent\nbin resolution) of 6 concerts each in ragas Deshkar (left)\nand Bhupali (right).\n3.1.2 Svara Salience Histogram\nThe svara salience histogram is not equivalent to the PCD.\nThe input to the system is segmented stable svaras which is\na subset of the pitch contour. We use a previously proposed\nalgorithm [16] that obtains a simple melodic transcription\nretaining only the stable svara regions of a pitch contour\nwhile discarding the transitory pitch regions. The stable\nsvara regions are segmented by identifying the fragments\nof pitch contour that are within Ttol(35 cents) of the svara\nfrequencies that are located via the peaks of a continuous\npitch histogram. Next, the svara fragments that are smaller\nthanTdur(250 ms) in duration are ﬁltered out, as they are\ntoo short to be considered as perceptually meaningful held\nsvaras [28]. This leaves a string of fragments each labeled\nby a svara. Fragments with the same svara value that are\nseparated by gaps less than 100 ms are merged. The svara\nsalience histogram is obtained as:\n3A random variable that has the value 1 or 0, according to whether a\nspeciﬁed event occurs or not is called an indicator random variable for\nthat event.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 41Hk=NX\nn=11[F(n)2Sk;k2(1;2;\u0001\u0001\u0001;12)] (2)\nwhere Hkis the salience of the kthbin,F(n)is the ar-\nray of pitch values, and Skis the kthsvara of the octave.\nHkis always a 12-element vector. Figure 2 shows the tonal\nhierarchy in the form of svara salience histogram. One ma-\njor difference between pitch salience histogram and svara\nsalience histogram is that the precise intonation informa-\ntion is lost in the latter.\nFigure 2 . Svara salience histograms (octave folded) of 6\nconcerts each in ragas Deshkar (left) and Bhupali (right).\n3.2 Representation of Phrase Shape\nThe phrase is a sequence of svara whose melodic realiza-\ntion includes speciﬁc intonations and transitions to/from\nneighboring svaras [24]. While computational models for\nmeasuring melodic similarity between phrases have em-\nployed distance measures between time-series of pitch val-\nues of the phrase segments, we might expect that a more\ndiscriminative representation is possible by explicitly in-\ncorporating features that contrast the two ragas.\nFigure 3 shows a representative GRS phrase from each\nof the ragas. Distinctive features suggested by the compar-\nison are: (i) durations of each of the stable svara regions,\n(ii) the durations of the glides connecting the svaras, and\n(iii) the pitch interval of the svara G. The implementa-\ntion of these features would involve decisions on segmen-\ntation of stable svaras, and determining the pitch interval\nvalue from the pitch continuum in the region. Further, it\nis important to ﬁgure out the kind of normalization that is\nneeded to reduce possible variability due to the tempo of\nthe performance.\nWe describe a phrase as a sequence of melodic ‘events’\nthat can each be described by the chosen features. For the\nGRS phrase in question, we consider the following ﬁve\nevents, i.e. svaras G;R;S, and the G\u0000RandR\u0000Stran-\nsitions. The selected features are: (i) Start time :on-\nset of an event, (ii) End time :offset of an event, (iii)\nDuration :difference of the two, (iv) Intonation :pre-\ncise pitch interval location of a stable svara in the octave\nobtained as the median pitch value over the duration of the\nsvara, and (v) Slope :gradient between the mean of last\n20% and the ﬁrst 20% pitch samples of a stable svara seg-\nment.\nFigure 3 . Two representative GRS phrases from ra-\ngas Deshkar (left) and Bhupali (right). The tuple cor-\nresponding to each svara denotes the extracted features\n(Intonation ,Duration ,Slope ) for that event.\n4. EXPERIMENTAL RESULTS AND DISCUSSION\nWe present experiments that help us identify the aspects\nof optimal representation, for each of tonal hierarchy and\nphrase shape, that discriminate the two ragas maximally\nbased on our labeled dataset of 12 concerts across two ra-\ngas. Our common approach for both attributes is to use\nunsupervised clustering ( k-means with k=2) of the feature\nvectors and optimise the separation between the clusters\nover the considered choices for feature implementation.\nPerformance in unsupervised clustering can be measured\nvia the ‘cluster purity’ which is obtained by assigning each\nobtained cluster to the underlying class that is most fre-\nquent in that cluster, and computing the resulting classiﬁ-\ncation accuracy.\nWe also use the receiver operated characteristic (ROC)\ncurve, with respect to detecting similarity within a raga\npair given the feature vectors and distance measure, to\nevaluate the tuning parameters of tonal hierarchy. An ob-\njective function to compare ROCs across conﬁgurations is\nthe area under curve (AUC) measure. Closer the AUC\nvalue to 1, better is the performance. For the evaluation\nof phrase shape, we additionally use ‘feature selection’ to\nestimate the most signiﬁcant features.\n4.1 Tonal Hierarchy\nThere are three main aspects of tonal hierarchy that need to\nbe addressed in order to maximize the separation between\nthe two clusters, these are: (i) representation, (ii) distance\nmeasure, and (iii) time-scale of analyses. The experiments\napply to both pitch salience and svara salience histograms.\nWe describe them in order with discussions on the insights\nobtained.\n4.1.1 Optimal Representation\nOur base representation is the octave folded pitch salience\nhistogram normalized to be a pdf. We try different bin res-\nolutions ranging from 1 to 100 cents, with a denser sam-\npling between 20 and 40 cents, motivated from a previous\nstudy by Datta et al. [10]. We perform an unsupervised\nclustering (euclidean distance) to obtain labels for each el-\nement in the 2 classes. Figure 4 shows the cluster purity\nvalues, where we note that no degradation in the evaluation\nmeasure is observed for 1 through 30 cents bin resolution.\nEach value on the curve is obtained by an average of 5 runs42 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017of the clustering algorithm to nullify the effect of any lo-\ncal minima. For the case of svara salience histograms, the\naverage cluster purity value is obtained as 0.96.\nFigure 4 . Cluster purity values obtained for different val-\nues of bin resolution for the pitch salience histograms.\n4.1.2 Comparison of Distance Measures\nIn order to determine a suitable distance metric between\nthe histograms representing raga tonal hierarchy, we test\ndifferent metrics on the 25 cent binned pitch salience his-\ntogram (given that no degradation was observed at this\nresolution in the previous experiment) and on the svara\nsalience histogram. As the distribution is a pdf, a natural\ndistance measure suggested in the literature is the (sym-\nmetric) KL divergence [22]. We also try Cityblock (L-1\nnorm), Euclidean (L-2 norm), and Correlation distances.\nThe last one is strongly motivated by the cognitive model\nof Krumhansl [23]. We present ROC curves (and AUC)\nfor four distance measures in Figure 5. We ﬁnd that the\nbest performing distance measure is the KL divergence for\nsvara salience histograms. The performance of cityblock\ndistance is observed to be comparable to that of correla-\ntion distance for pitch salience histograms. We use the lat-\nter in the following experiments since it is more favored in\nprevious work [6, 23].\nFigure 5 . ROCs obtained for four different distance mea-\nsures from pitch salience (left) and svara salience (right)\nhistograms.\n4.1.3 Time-scale for a Stable Tonal Hierarchy\nIt is of interest to determine at what time-scale, the mea-\nsured tonal hierarchy qualiﬁes as a stable representation.\nWe therefore carry out the previous discrimination exper-\niments on segmented concerts. We divide each concert to\nits\u00001\nn\u0001th(n= 1,\u0001\u0001\u0001, 5) portion and construct a distance\nmatrix with each\u00001\nn\u0001thpart. The goal is to ﬁnd out theminimum proportion of the full concert that is necessary to\nrobustly discriminate between the two classes. The ROC\nobtained from\u00001\n4\u0001th(and below this) of a concert results\nin an AUC <0.5 which indicates that the time-scale is\ntoo small to constitute a stable tonal hierarchy. We aug-\nment our test dataset by considering each half and\u00001\n3\u0001rd\nof each concert, making the dataset size 24 and 36 respec-\ntively. Figure 6 shows a comparison of best performing\nsystems for the full ( n=1) and partial ( n=2,3) concerts.\nWe additionally investigate if a ﬁner binned pitch salience\nhistogram shows improvement. A ﬁner bin resolution of\n12.5 cents ( p=96) is observed to perform better that the 25\ncent binned pitch salience histogram ( p=48). This suggests\nthat the microtonal differences in intonation become more\nimportant when the concert segment duration is not long\nenough to capture 12-tone hierarchy in a stable manner.\nWhile the improvement (in terms of AUC value) might\nseem rather small, it was observed to be consistent with\neach distance measure under test. Moreover with full-\nconcerts ( n=1) for p=96, the maximum true positive rate\nfor zero false positive rate is higher ( \u00190:95) than that for\np=48 (\u00190:75) indicating an improved performance. We\nalso separately observed that considering only the begin-\nning segments in the\u00001\nn\u0001thportions shows better perfor-\nmance compared to other locations. The ﬁrst few minutes\nof each concert spans the alap and bandish (composition)\nthat play a crucial role in raga delineation thus adhering\nclosely to raga grammar.\n4.2 Phrase Shape\nWe use the svara segmentation method outlined\nin Section 3.1.2 to obtain the components of the phrase\nshape corresponding to the sequence of stable svaras as\nwell as the transient regions. In this section we present\na statistical description of the features corresponding\nto the different events. We also compare the discrimi-\nnation powers of the different features via a clustering\nexperiment.\n4.2.1 Distribution of Event Duration s\nGiven the Duration values of each event in the GRS\nphrases (52 instances in raga Deshkar and 107 instances\nin raga Bhupali), we present the distributions in Figure 7\nof the event Duration s in the form of boxplots of the\nraw measurements in seconds. We observe distinctions be-\ntween the two ragas in nearly all the duration parameters,\nmost notably in the RDuration s. That the Rduration is\nsmall and constrained in raga Deshkar is supported by the\nraga grammar speciﬁcation in Table 1 which indicates R\nin parantheses, suggesting a “weak note that is never sus-\ntained” [30]. Overall, the dispersion in the parameters is\nsmaller in the phrase in raga Deshkar compared with Bhu-\npali, consistent with the fact that it is a grammatically more\nconstrained raga [1, 5, 30]. An exception is the realisation\nof the Ssvara with several outlying values of duration due\nto its location at phrase end, where a number of contextual\nconsiderations inﬂuence the note offset.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 43Figure 6 . Comparison of ROCs obtained with correlation distance (for pitch salience histograms) and KL divergence (for\nsvara salience histogram) for different time-scales ( n=1,2,3) of the concerts.\nFigure 7 . Distributions of event Duration s across the\nGRS phrase instances in the two ragas.\n4.2.2 Feature Selection and Evaluation\nTo compare the predictive powers of the measured acous-\ntic features, we perform ‘Feature Selection’ using Weka4\ndatamining tool. We use the “InfoGainAttributeEval” as\nthe attribute selector that evaluates the value of an at-\ntribute by measuring the information gain with respect to\nthe class, in conjunction with the “Ranker” search method\nthat ranks attributes by their individual performances. We\nconstruct a feature vector for each instance of the GRS\nphrase with 5 Duration features, one for each event, and\nIntonation andSlope features corresponding to each of\nthe three stable svaras as implemented in Section 3.2. Of\nthese 11 features, we obtained the most signiﬁcant fea-\ntures in terms of predictive power as the following: (i) R\nDuration , and (ii) GIntonation , with the third feature\nin the list placed considerably lower. This outcome is con-\nsistent with the raga grammars where these two aspects are\nconsidered distinctive properties of raga Deshkar.\nNext, we perform an unsupervized clustering of the 159\nphrases into two classes using the Euclidean distance be-\ntween the 2-element vectors of the two selected features.\nThe achieved cluster purity value is 0.99 (i.e. only 2 in-\nstances of the 159 are misclassiﬁed). As a next step, we in-\nvestigate whether duration normalization is helpful. Given\nthat overall phrase duration is correlated with tempo [34],\nit is natural to expect that the variability of phrase event\ndurations may be reduced by normalization by the over-\nall phrase duration. However, it turned out the the clus-\n4http://www.cs.waikato.ac.nz/ml/weka/ter purity with the duration-normalized Duration (ofR\nsvara) feature coupled with the previous Intonation (ofG\nsvara) feature reduced to 0.95 (i.e. 8 instances were mis-\nclassiﬁed). This indicates that musicians interpret the raga\ngrammar in terms of raw durations rather than relative to\nthe tempo.\n5. CONCLUSION AND FUTURE WORK\nBased on the notion of grammaticality in raga per-\nformance, we examined computational representations\nfor some of the key attributes of raga grammar based\non discriminating allied ragas. In particular, both the\npitch salience histograms and the stable-note based svara\nsalience histograms were considered for tonal hierarchy\nwith a variety of distance measures to derive a combina-\ntion of histogram parameters and distance metric that best\nseparated same-raga pairs from the allied-raga pairs. It\nwas found that svara salience histograms worked best at\nthe time-scale of full concerts whereas ﬁner bin-widths\nof pitch salience histograms were superior for segmented\nconcerts. Overall, full concerts with KL divergence as\ndistance measure between 12-tone svara histograms per-\nformed best. A phrase level representation that considered\nonly the discriminating elements of the same-phrase vari-\nants across the ragas in terms of absolute duration and pitch\ninterval of key events (i.e. for RandGsvaras respectively)\nwas able to achieve a high degree of separation between the\ntwo allied ragas. Our results suggest that a pedagogy tool\nthat measures ungrammaticality can indeed be designed\nbased on modeling the raga attributes for any raga with the\nmethodology presented here. Future work involves: (i) val-\nidation on other allied raga sets, (ii) correlating predicted\nungrammaticality with perceived ungrammaticality by ex-\npert listeners, and (iii) determining the relative weighting\nof the different raga attributes for an overall rating, possi-\nbly at different time-scales, based on the expert judgments.\n6. ACKNOWLEDGEMENT\nThis work received partial funding from the European\nResearch Council under the European Union’s Sev-\nenth Framework Programme (FP7/2007-2013)/ERC grant\nagreement 267583 (CompMusic).44 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20177. REFERENCES\n[1] Music in Motion: The automated transcription for In-\ndian music (AUTRIM) project by NCPA and UvA.\nurl: https://autrimncpa.wordpress.com/. Last accessed:\nApril 26, 2017.\n[2] S. Bagchee. N¯ad: Understanding Raga Music . Busi-\nness Publications Inc, 1998.\n[3] S. Bagchee. Shruti: A Listener’s Guide to Hindustani\nMusic . Rupa Co, 2006.\n[4] S. Belle, R. Joshi, and P. Rao. Raga identiﬁcation by\nusing swara intonation. Journal of ITC Sangeet Re-\nsearch Academy , 23, 2009.\n[5] Distinguishing between Similar Ragas. url:\nhttp://www.itcsra.org/Distinguishing-between-\nSimilar-Ragas. Last accessed: April 26, 2017.\n[6] B. Bozkurt. An automatic pitch analysis method for\nTurkish Maqam music. Journal of New Music Research\n(JNMR) , 37(1):1–13, 2008.\n[7] P. Chordia and A. Rae. Raag recognition using pitch-\nclass and pitch-class dyad distributions. In Proc. of Int.\nSoc. for Music Information Retrieval Conf. , pages 431–\n436, 2007.\n[8] P. Chordia and S. S ¸ent ¨urk. Joint recognition of raag and\ntonic in north Indian music. Computer Music Journal ,\n37(3):82–98, 2013.\n[9] A. Danielou. The ragas of Northern Indian music .\nMunshiram Manoharlal Publishers, 2010.\n[10] A. K. Datta, R. Sengupta, N. Dey, and D. Nag. Exper-\nimental analysis of Shrutis from performances in Hin-\ndustani music . Scientiﬁc Research Department, ITC\nSangeet Research Academy, 2006.\n[11] P. Dighe, H. Karnick, and B. Raj. Swara histogram\nbased structural analysis and identiﬁcation of Indian\nclassical ragas. In Proc. of Int. Soc. for Music Infor-\nmation Retrieval Conf. (ISMIR) , pages 35–40, 2013.\n[12] S. Dutta, S. PV Krishnaraj, and H. A. Murthy. Raga\nveriﬁcation in Carnatic music using longest common\nsegment set. In Int. Soc. for Music Information Re-\ntrieval Conf. (ISMIR) , pages 605–611, 2015.\n[13] K. K. Ganguli, S. Gulati, X. Serra, and P. Rao. Data-\ndriven exploration of melodic structures in Hindustani\nmusic. In Proc. of the International Society for Mu-\nsic Information Retrieval (ISMIR) , pages 605–611, Au-\ngust 2016. New York, USA.\n[14] K. K. Ganguli, A. Lele, S. Pinjani, P. Rao, A. Srini-\nvasamurthy, and S. Gulati. Melodic shape stylization\nfor robust and efﬁcient motif detection in hindustani\nvocal music. In National Conference on Communica-\ntions (NCC) , 2017.\n[15] K. K. Ganguli and P. Rao. Exploring melodic similarity\nin hindustani classical music through the synthetic ma-\nnipulation of raga phrases. In Cognitively-based Music\nInformatics Research , 2016.\n[16] K. K. Ganguli, A. Rastogi, V . Pandit, P. Kantan, and\nP. Rao. Efﬁcient melodic query based audio search for\nHindustani vocal compositions. In Proc. of the Inter-\nnational Society for Music Information Retrieval (IS-\nMIR) , pages 591–597, October 2015. Malaga, Spain.\n[17] A. C. Gedik and B. Bozkurt. Pitch-frequency\nhistogram-based music information retrieval for Turk-\nish music. Signal Processing , 90(4):1049–1063, 2010.[18] S. Gulati, A. Bellur, J. Salamon, H. G. Ranjani, V . Ish-\nwar, H. A. Murthy, and X. Serra. Automatic tonic\nidentiﬁcation in Indian art music: Approaches and\nEvaluation. Journal of New Music Research (JNMR) ,\n43(1):53–71, 2014.\n[19] S. Gulati, J. Serr `a, V . Ishwar, S. S ¸ent ¨urk, and X. Serra.\nPhrase-based r ¯aga recognition using vector space mod-\neling. In IEEE Int. Conf. on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , pages 66–70, 2016.\n[20] S. Gulati, J. Serr `a, V . Ishwar, and X. Serra. Mining\nmelodic patterns in large audio collections of Indian\nart music. In Int. Conf. on Signal Image Technology &\nInternet Based Systems (SITIS-MIRA) , pages 264–271,\n2014.\n[21] S. Gulati, J. Serr `a, and X. Serra. Improving melodic\nsimilarity in Indian art music using culture-speciﬁc\nmelodic characteristics. In Int. Soc. for Music Informa-\ntion Retrieval Conf. (ISMIR) , pages 680–686, 2015.\n[22] G. K. Koduri, S. Gulati, P. Rao, and X. Serra. R ¯aga\nrecognition based on pitch distribution methods. Jour-\nnal of New Music Research , 41(4):337–350, 2012.\n[23] C. L. Krumhansl. Cognitive Foundations of Musical\nPitch , chapter 4: A key-ﬁnding algorithm based on\ntonal hierarchies, pages 77 – 110. Oxford University\nPress, New York, 1990.\n[24] S. Kulkarni. Shyamrao Gharana , volume 1. Prism\nBooks Pvt. Ltd., 2011.\n[25] V . Oak. 22 shruti. url: http://22shruti.com/. Lat ac-\ncessed: April 26, 2017.\n[26] H. S. Powers and R. Widdess. India, subcontinent of ,\nchapter III: Theory and practice of classical music.\nNew Grove Dictionary of Music. Macmillan, London,\n2nd edition, 2001. Contributions to S. Sadie (ed.).\n[27] D. Raja. The Raga-ness of Ragas: Ragas Beyond the\nGrammar . D.K. Print World Ltd., 2016.\n[28] P. Rao, J. C. Ross, and K. K. Ganguli. Distinguishing\nraga-speciﬁc intonation of phrases with audio analysis.\nNinaad , 26-27(1):59–68, December 2013.\n[29] P. Rao, J. C. Ross, K. K. Ganguli, V . Pandit, V . Ishwar,\nA. Bellur, and H. A. Murthy. Classiﬁcation of melodic\nmotifs in raga music with time-series matching. Jour-\nnal of New Music Research (JNMR) , 43(1):115–131,\nApril 2014.\n[30] S. Rao, J. Bor, W. van der Meer, and J. Harvey. The\nRaga Guide: A Survey of 74 Hindustani Ragas . Nim-\nbus Records with Rotterdam Conservatory of Music,\n1999.\n[31] S. Rao and P. Rao. An overview of Hindustani music\nin the context of Computational Musicology. Journal\nof New Music Research (JNMR) , 43(1), April 2014.\n[32] V . Rao and P. Rao. V ocal melody extraction in the pres-\nence of pitched accompaniment in polyphonic music.\nIEEE Trans. on Audio, Speech & Language Process-\ning, 18(8), 2010.\n[33] X. Serra. Creating research corpora for the compu-\ntational study of music: the case of the Compmusic\nproject. In Proc. of the 53rd AES Int. Conf. on Seman-\ntic Audio , London, 2014.\n[34] W. van der Meer. Hindustani Music in the 20th Cen-\ntury. Martinus Nijhoff Publishers, 1980.\n[35] K. G. Vijaykrishnan. The Grammar of Carnatic Music .\nDe Gruyter Mouton, 2007.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 45"
    },
    {
        "title": "Convolutional Neural Networks for Real-Time Beat Tracking: A Dancing Robot Application.",
        "author": [
            "Aggelos Gkiokas",
            "Vassilis Katsouros"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1196302",
        "url": "https://doi.org/10.5281/zenodo.1196302",
        "ee": "https://ismir2017.smcnus.org/wp-content/uploads/2017/10/135_Paper.pdf",
        "abstract": "In this paper a novel approach that adopts Convolutional Neural Networks (CNN) for the Beat Tracking task is proposed. The proposed architecture involves 2 convolutional layers with the CNN filter dimensions corresponding to time and band frequencies, in order to learn a Beat Activation Function (BAF) from a time-frequency representation. The output of each convolutional layer is computed only over the past values of the previous layer, to enable the computation of the BAF in an online fashion. The output of the CNN is post-processed by a dynamic programming algorithm in combination with a bank of resonators for calculating the salient rhythmic periodicities. The proposed method has been designed to be computational efficient in order to be embedded on a dancing NAO robot application, where the dance moves of the choreography are synchronized with the beat tracking output. The proposed system was submitted to the Signal Processing Cup Challenge 2017 and ranked among the top third algorithms.",
        "zenodo_id": 1196302,
        "dblp_key": "conf/ismir/GkiokasK17",
        "keywords": [
            "Convolutional Neural Networks",
            "Beat Tracking task",
            "2 convolutional layers",
            "time and band frequencies",
            "online computation",
            "Dynamic programming algorithm",
            "bank of resonators",
            "synchronized dance moves",
            "Signal Processing Cup Challenge 2017",
            "top third algorithms"
        ]
    },
    {
        "title": "Audio to Score Matching by Combining Phonetic and Duration Information.",
        "author": [
            "Rong Gong",
            "Jordi Pons",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415766",
        "url": "https://doi.org/10.5281/zenodo.1415766",
        "ee": "https://zenodo.org/records/1415766/files/GongPS17.pdf",
        "abstract": "We approach the singing phrase audio to score matching problem by using phonetic and duration information – with a focus on studying the jingju a cappella singing case. We argue that, due to the existence of a basic melodic con- tour for each mode in jingju music, only using melodic information (such as pitch contour) will result in an am- biguous matching. This leads us to propose a match- ing approach based on the use of phonetic and duration information. Phonetic information is extracted with an acoustic model shaped with our data, and duration in- formation is considered with the Hidden Markov Models (HMMs) variants we investigate. We build a model for each lyric path in our scores and we achieve the match- ing by ranking the posterior probabilities of the decoded most likely state sequences. Three acoustic models are in- vestigated: (i) convolutional neural networks (CNNs), (ii) deep neural networks (DNNs) and (iii) Gaussian mixture models (GMMs). Also, two duration models are com- pared: (i) hidden semi-Markov model (HSMM) and (ii) post-processor duration model. Results show that CNNs perform better in our (small) audio dataset and also that HSMM outperforms the post-processor duration model.",
        "zenodo_id": 1415766,
        "dblp_key": "conf/ismir/GongPS17",
        "keywords": [
            "phonetic",
            "duration",
            "melodic",
            "acoustic",
            "Hidden Markov Models",
            "Gaussian mixture models",
            "convolutional neural networks",
            "deep neural networks",
            "post-processor duration model",
            "HSMM"
        ],
        "content": "AUDIO TO SCORE MATCHING BY COMBINING PHONETIC AND\nDURATION INFORMATION\nRong Gong, Jordi Pons and Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona\nrong.gong@upf.edu, jordi.pons@upf.edu, xavier.serra@upf.edu\nABSTRACT\nWe approach the singing phrase audio to score matching\nproblem by using phonetic and duration information – with\na focus on studying the jingju a cappella singing case. We\nargue that, due to the existence of a basic melodic con-\ntour for each mode in jingju music, only using melodic\ninformation (such as pitch contour) will result in an am-\nbiguous matching. This leads us to propose a match-\ning approach based on the use of phonetic and duration\ninformation. Phonetic information is extracted with an\nacoustic model shaped with our data, and duration in-\nformation is considered with the Hidden Markov Models\n(HMMs) variants we investigate. We build a model for\neach lyric path in our scores and we achieve the match-\ning by ranking the posterior probabilities of the decoded\nmost likely state sequences. Three acoustic models are in-\nvestigated: (i)convolutional neural networks (CNNs), (ii)\ndeep neural networks (DNNs) and (iii)Gaussian mixture\nmodels (GMMs). Also, two duration models are com-\npared: (i)hidden semi-Markov model (HSMM) and (ii)\npost-processor duration model. Results show that CNNs\nperform better in our (small) audio dataset and also that\nHSMM outperforms the post-processor duration model.\n1. INTRODUCTION\nThe ultimate goal of our research project is to automat-\nically evaluate the jingju a cappella singing of a student\nin the scenario of jingju singing education – see Figure 1.\nJingju, a traditional Chinese performing art also known as\nPeking or Beijing opera, is extremely demanding in the\nclear pronunciation and accurate intonation of each syl-\nlabic or phonetic singing unit. To this end, during the initial\nlearning stages, students are required to completely imitate\ntutor’s singing. Therefore, the automatic jingju singing\nevaluation system we envision is based on this training\nprinciple and measures the intonation and pronunciation\nsimilarities between the student’s and the tutor’s singings.\nBefore measuring similarities, the singing phrase should be\nautomatically segmented into syllabic or phonetic units in\nc\rRong Gong, Jordi Pons and Xavier Serra. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Rong Gong, Jordi Pons and Xavier Serra. “Au-\ndio to score matching by combining phonetic and duration information”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.\nFigure 1 . System design framework of the entire research\nproject. The modules with bold border lines are addressed\nin this paper.\norder to capture the temporal details. Jingju music scores,\nwhich contain the phonetic and duration information for\neach singing syllable, will be beneﬁcial for this segmenta-\ntion. In the application scenario, the score of a query au-\ndio could be selected from the database by the user itself.\nHowever, to avoid manual intervention and improve the\nuser experience, we tackle the problem of automatically\nﬁnding the corresponding music score for a given query\naudio (bold in Figure 1). Note that achieving successful\nmethods for audio to score matching might be beneﬁcial\nfor several music informatics research (MIR) tasks, such\nas score-informed automatic syllable/phoneme segmenta-\ntion [5] or score-informed source separation [14]. The ob-\njective of this research task is to ﬁnd the corresponding\nscore for a given singing audio query. We restrict this re-\nsearch to the “matching” scope by pre-segmenting both the\nsinging audios and the music scores into the phrase units.\nXipi and erhuang are the main modes in jingju mu-\nsic. Each has two basic melodic contours – an opening\nphrase and a closing phrase. Each basic melodic contour\nis constructed upon characteristic pitch progressions for\neach mode [22]. Therefore, singing phrases from differ-\nent arias sharing the same mode are likely to have a sim-\nilar melodic contour. Figure 2 shows an example of this\nfact. However, melodic information tends to be intuitively\nused for such matching tasks. For example in Query-by-\nSinging/Humming (QBSH) [15], melodic similarities can\nbe obtained by comparing the distance between the F0 con-\ntour of the query audio and those synthesized from the can-\ndidate scores. Then, the best matched music score can\nbe retrieved by selecting the most similar melody. But\nnote that using this approach for jingju music would bring\nmatching errors since the melodic contours of the same\nmode are similar in this sense. In this case, it is more ap-428Figure 2 . An example of different phrases having a similar\nXipi melodic contour in our score dataset. The lengths of\nthese contours are normalized to a ﬁxed sample size.\npropriate to use another notion of similarity. We propose\nusing the lyrics since the stories narrated in different jingju\narias are distinctive and lyrics tend to change through dif-\nferent jingju arias – even when they share the same mode.\nTherefore, phonetic information might be useful to iden-\ntify a similar score given a query audio.\nQBSH is the most related research task to our study,\nwhich retrieves a song by singing a portion of itself. Most\nof the studies use melody information as the only cue.\nThe typical process of such systems was introduced by\nMolina et al. [15]: ﬁrstly, the F0 contour and/or a note-\nlevel transcription for a given singing query are extracted;\nand then, a set of candidate songs are retrieved from a large\ndatabase using a melodic matcher module. The most suc-\ncessful QBSH system, which obtained the best results in\nMIREX 2016 contest, is based on the method of multi-\nple similarity measurements fusion [21]. This system pro-\nposed a melodic matcher which combines several simi-\nlarities that are note-based and frame-based. The authors\nclaim that the fusion mechanism improves the query per-\nformance because no similarity measurement is perfect.\nTherefore, information sources that are complementary to\neach other might be beneﬁcial for this approach. Very few\nstudies have explored the capability of the phonetic infor-\nmation for QBSH. Guo et al. [8] and Wang et al. [20] both\nused a lyric recognizer based on Hidden Markov Models\n(HMMs). Their recognition networks1were constructed\nwith the phonetic information from the query candidates\ndatabase. They used frame-based MFCCs to create the\nacoustic models with GMMs. Then, the Viterbi algorithm\nwas executed over the recognition networks to either ob-\ntain the most likely phonetic state sequence (for Wang et\nal.[20]) or the posterior probability of each possible de-\ncoding path (for Guo et al. [8]). The ﬁnal score of a\nquery candidate is either based on semantic similarity [20]\nor based on the posterior probability of its corresponding\nlyrics [8].\nAnother research task related to our study is singing\nkeyword spotting. The main goal of this task is to\nsearch for one or more keywords in a singing query. The\nsystem proposed by Kruspe [12] searches for a speciﬁc\nsinging keyword on the resulting phoneme observations.\n1The topology of the HMM is deﬁned by the recognition network.A keyword-ﬁller HMMs is employed for this purpose. She\nused two phoneme duration models: the HSMM and the\npost-processor duration model.\nFinally, both phonetic and duration information ex-\ntracted from the score have been extensively used in\nalignment-related tasks, such as audio-to-score alignment\nand audio-to-lyrics alignment. For example, Gong et al.\n[4] construct a left-to-right HSMM using phonetic and du-\nration information. Or Dzhambazov et al. [3] use a simi-\nlar approach for aligning polyphonic audio. Analogously,\nthe proposed approach explores the use of both phonetic\nand duration information (available in scores) to tackle the\nmatching ambiguity problem existing in jingju music.\nThe remainder of this paper is organized as follows:\nthe used dataset is introduced in section 2, section 3 ex-\nplains the modules of the proposed approach – detailing\nhow to incorporate phonetic and duration information. Ex-\nperiments and results are reported in section 4, and section\n5 concludes and points out future work.\n2. DATASET\nThe jingju a cappella singing dataset is composed of two\noverlapping parts: (i)audio and (ii)score datasets.\nThe audio dataset [1] used for this study consists of two\nrole-types singing: dan(young woman) and laosheng (old\nman). The danpart of this dataset has 42 recordings sung\nby 7 singers and the laosheng part has 23 recordings sung\nby 7 laosheng singers. The boundary annotations of the au-\ndio dataset have been done in Praat format (textgrid) con-\nsidering a hierarchy of three levels: phrase, syllable and\nphoneme – using Chinese characters, pinyin notations and\nX-SAMPA notations, respectively. 32 phoneme classes are\nused in the phoneme-level annotation. Two Mandarin na-\ntive speakers and a jingju musicologist have been devoted\nto this annotating work. Annotations and more detailed in-\nformation can be found online2. Some statistics about the\ndataset are reported in Table 2. The average phrase, sylla-\nble and voiced phoneme length of dansinging are ostensi-\nbly greater than those of laosheng singing (bold numbers\nin Table 2), which might indicate that dansinging tends to\nhave more pitch variation and ornamentation – as we could\nobserve empirically by listening to the data.\nNum. Avg. len (s) Std. len (s)\nPhrases 325 16.42 14.11\nSyllables 2933 1.58 2.82\nV oiced phonemes 7198 0.61 0.97\nUnvoiced phonemes 2014 0.10 0.67\nPhrases 247 9.47 8.14\nSyllables 2289 0.88 1.48\nV oiced phonemes 4948 0.39 0.78\nUnvoiced phonemes 1454 0.07 0.05\nTable 1 . Detailed information of the jingju a cappella\nsinging audio dataset: dan(top), laosheng (bottom).\n2http://doi.org/10.5281/zenodo.344932Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 429The audio dataset, along with their boundary annota-\ntions, is split into three parts: training set, development\n(dev) set and test set. We deﬁne the training set to be the\nnon-overlapping part with the score dataset, see Figure 3.\nThe training set will be used for calculating the phonetic\nduration (duration information) and training acoustic mod-\nels (phonetic information). After taking the training set\nout, we deﬁne the development set to be the half of the re-\nmaining phrases in the audio dataset (randomly selected)\n– it will be used for parameters optimization. The test set\nconsists on the remaining phrases of the audio dataset – it\nwill be used for testing the acoustic models performance\nand the matching performance.\nFigure 3 . The intersection between the audio and the score\ndatasets. The partition of the audio dataset.\nOn the other hand, the score dataset contains 435 dan\nphrases and 481 laosheng phrases. The scores have been\ntyped in stave notation (including lyrics in Chinese char-\nacters) using MuseScore from different printed sources in\njianpu notation. Since tempo is usually not clearly noted in\nthe printed score, we do not include this information in the\ndataset. The relative syllabic durations are indicated by the\nnote durations corresponding to the lyrics, which will be\nused to calculate the phonetic duration (duration informa-\ntion) and the matching network. The whole score dataset\nwill be used as candidates for testing the matching perfor-\nmance and for parameter optimization.\n3. APPROACH\nThe proposed approach aims to match the query audio to\nits score by using phonetic and duration information. Dur-\ning the training process (red boxes in Figure 4): the acous-\ntic models of each phoneme are shaped by using the au-\ndio training set and its phonetic boundary annotations; the\nscore dataset is used to construct a matching network; and\nphoneme duration distributions are estimated by using both\naudio training set and scores. During the matching process\n(green boxes in Figure 4): two duration models –HSMM\nand post-processor– are explored for the Viterbi decoding\nstep. Finally, the best-matched phrase is found by ranking\nthe decoded state sequence probabilities.\n3.1 Acoustic models\nHere presented acoustic models aim to represent the re-\nlationship between an audio signal and the 32 phoneme\nFigure 4 . Diagram of the proposed approach.\nclasses present in our dataset. The output of these models\nyield probability scores for each phoneme class.\nThe most popular way to approach acoustic modeling\nis by using GMMs and MFCCs features [8, 20]. For that\nreason, we set as baseline a 40-component GMM with the\nfollowing input vector: 13 MFCCs, their deltas and delta-\ndeltas. Moreover, DNNs have been found very useful for\nacoustic modeling [9, 13]. Therefore, we propose an addi-\ntional baseline: a DNN with 2 hidden layers followed by\nthe 32-way softmax output layer – the input is set to be a\nlog-mel spectrogram.\nHowever, DNNs are very prone to over-ﬁtting and\nthe available dataset is relatively small. For that rea-\nson we propose using CNNs since these are more robust\nagainst over-ﬁtting – note that CNNs allow parameter shar-\ning. Additionally, Pons et al. [17] have successfully used\nspectrograms-based CNNs for learning music timbre rep-\nresentations from small datasets. Given that timbre is an\nimportant feature for acoustic modeling, we propose using\nthe same architecture: a single convolutional layer with\nﬁlters of various sizes [16, 17]. The input is set to be a\nlog-mel spectrogram. We use 128ﬁlters of sizes 50\u00021and\n70\u00021,64ﬁlters of sizes 50\u00025and70\u00025, and 32ﬁlters of\nsizes 50\u000210and70\u000210– where the ﬁrst and second num-\nbers denote the frequential and temporal size of the ﬁlter,\nrespectively. A max-pool layer of 2\u0002N0is followed by a\n32-way softmax output layer with 30% dropout – where\nN0denotes the temporal dimension of the resulting feature\nmap. 2\u0002N0max-pool layer was chosen to achieve time-\ninvariant representations while keeping the frequency res-\nolution. And same padding is used to preserve the dimen-\nsions of the feature maps so that these are concatenable.\nFilter shapes are designed so that ﬁlters can capture the\nrelevant time-frequency contexts for learning timbre rep-\nresentations – according to the design strategy proposed\nby Pons et al. [17]\nLog-mel spectrograms are of size 80\u000221– the network\ntakes a decision for a frame given its context: \u000610 frames,\n21 frames in total. Activation functions are ELUs [2]\nfor all deep learning models and these are optimized with430 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017stochastic gradient descent (batch size: 64), using ADAM\n[11] and early stopping – when validation loss (categorical\ncross-entropy) does not decrease after 10 epochs.\nSpectrograms are computed from audio recordings\nsampled at 44.1 kHz. STFT is performed using a window\nlength of 25ms (2048 samples with zero-padding) with a\nhop size of 10ms. The 80 log-mel bands energies are calcu-\nlated on frequencies between 0Hz and 11000Hz and these\nare standardized to have zero mean and unit variance.\nThe acoustic models are trained separately for each\nrole-type and their performance is reported in section 4.2.\n3.2 Matching network\nThe matching network deﬁnes the topology of the hidden\nMarkov model. By using each candidate phrase in the\nscore dataset as an isolated unit, isolated-phrase matching\nnetworks can be constructed. Figure 5 shows the structure\nof this matching network, which has K= 916 lyric paths.\nFigure 5 . The structure of the Kpaths isolated-phrase\nmatching network. Path 3 shows an example of the left-\nto-right state chain structure of an individual lyric path.\nThe matching network uses HMMs or HSMMs, de-\npending on how the internal duration is modeled. Each\npath is a left-to-right state chain which represents the\nphoneme transcription of its lyrics. In order to construct\nthe lyric path, pinyin lyrics are segmented into phonetic\nunits and transcribed into X-SAMPA notations by using\na predeﬁned dictionary. For example, a path which has\nthe lyrics yan jian de hong ri in pinyin is a chain consist-\ning of 12 states: j, En, c, j, En, c, 7, x, UN, N, r n’, 1 in\nX-SAMPA notation. When the decoding process has ﬁn-\nished, each lyric path can get a posterior probability which\nwill be used as the similarity measure between the query\nphrase and the candidate phrase.\n3.3 Phonetic duration distributions\nPhonetic duration information comes from two sources:\nthe boundary annotations of audio training dataset and the\nscore dataset. The phonetic duration is not directly indi-\ncated in the score. However, it is indispensable for model-\ning the phonetic duration distribution for each state in the\nmatching network. The syllable, of which duration can be\ndeduced by the corresponding note(s), is used to restrict\nthe durations of the phonemes.\nFigure 6 . Flowchart example of estimating the phonetic\ndurations of a syllable.\nIn the following, we propose a method for estimating\nthe absolute phonetic duration given: (i)the score, and (ii)\nthe phonemes duration histograms computed from the au-\ndio dataset annotations. First, we omit silence parts in the\nquery audio (with a simple voice activity detection method\n[19]) and also in the score by removing the rest notes. Sec-\nond, we compute the duration histogram and its duration\ncentroid for each phoneme class – by aggregating the pho-\nnetic durations indicated in the boundary annotations of the\naudio training dataset. Then, we segment each syllabic du-\nration in the score dataset into phonetic durations accord-\ning to the proportion of their duration centroids. Finally,\nas the scores do not contain tempo, we normalize the pho-\nnetic durations of each phrase such that their summation is\nequal to the duration of the query audio. See Figure 6 for\nan equivalent graphic explanation. In Figure 6, the cen-\ntroid durations of these three phonemes are: 0.46s, 0.9s\nand 0.1s, summing: 1.46s – alternatively, these can be ex-\npressed as a proportion of 1.46s: 0.32, 0.62 and 0.06. With\nthese proportions and the absolute syllable duration (2s),\nwe can compute the absolute phoneme durations: 0.32 \u00012s\n= 0.64s, 0.62\u00012s=1.24s and 0.06\u00012s=0.12s.\nThe phonetic duration distribution needs to be calcu-\nlated for each state in the matching network in order to\nincorporate the a priori phonetic duration information. We\nmodel it by Gaussian distributions:\nN(x;\u0016l;\u001b2\nl) =1p\n2\u0019\u001blexp\u0012\n\u0000(x\u0000\u0016l)2\n2\u001b2\nl\u0013\n:(1)\nwhere\u0016lis the duration of the phoneme ldeduced by the\nabove method and the standard deviation \u001blis proportional\nto\u0016l:\u001bl=\r\u0016l. The proportionality constant \rwill be\noptimized in section 4.3 for each role-type.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4313.4 Duration modeling\nStandard Markovian state does not impose explicitly dura-\ntion distribution, instead, imposing an implicit state occu-\npancy distribution which corresponds to a “1-shifted” geo-\nmetric distribution [6]:\ndj(u) = (1\u0000~pjj)~pu\u00001\njj (2)\nwhereudenotes the occupancy or sojourn time in a Marko-\nvian statejand~pjjdenotes the self-transition probability\nof the statej. Because of the implicity of the Markovian\nstate occupancy, the phonetic duration distribution intro-\nduced in section 3.3 can not be imposed. Kruspe [12]\npresents two duration modeling techniques for HMMs:\nHidden semi-markov model (HSMM) and post-processor\nduration model.\n3.4.1 Hidden semi-markov model\nGu´edon [6] deﬁned a semi-Markov chain Stwith ﬁnite\nstate space 0;:::;J\u00001by the following parameters:\n- initial probabilities \u0019j=P(S0=j)withP\nj\u0019j= 1\n- transition probability of semi-Markovian state j: for\neachk6=j;pjk=P(St+1=kjSt+16=j;St=j)\nwithP\nk6=jpjk= 1andpjj= 0\nAn explicit occupancy distribution is attached to each\nsemi-Markovian state:\ndj(u) =P(St+u+16=j;St+u\u0000v=j;v= 0;:::;u\u00002\njSt+1=j;St6=j);u= 1;:::;M j\n(3)\nwhereMjdenotes the upper bound to the time spent in\nstatej.dj(u)deﬁnes the conditional probability of leaving\nstatejat timet+u+ 1and entering state jat timet+ 1.\nTo apply HSMMs to the matching network, we ﬁrst use\nthe matching network as the HSMMs topology. Thus the\nstate occupancy distribution is set to its corresponding pho-\nnetic duration distribution. Then the probabilities of each\nleft-to-right state transition are set to 1 because all self-\ntransition probabilities in HSMMs are 0. The goal is to ﬁnd\nthe most likely sequence of hidden states for each lyric path\nand collect its posterior probability. The Viterbi algorithm\nmeets this speciﬁc goal and its complete implementation is\nprovided in [7].\n3.4.2 Post-processor duration model\nThe post-processor duration model was ﬁrst introduced by\nJuang et al. [10]. It was then experimentally proved in\nKruspe’s paper [12] that this duration model works better\nthan HSMMs for the keyword spotting task in English pop\nsinging voice. The post-processor duration model uses the\noriginal HMMs Viterbi algorithm – therefore, during the\ndecoding process, no explicit occupancy duration distribu-\ntion is imposed.\nThe log posterior probability of the decoded most likely\nstate sequence is augmented by the log duration probabili-\nties:\nlog^f= logf+\u000bNX\nj=1N(uj;\u0016j;\u001b2\nj) (4)wherefis the HMMs posterior probability, \u000bis a weight-\ning factor which will be optimized in section 4.3, j=\n1;:::;N is the decoded state number in the most likely state\nsequence, andN(uj;\u0016j;\u001b2\nj)is the occupancy probability\nof being in state jfor the occupancy uj.\n4. EXPERIMENTS AND RESULTS\n4.1 Performance metrics\nTwo experiments3are performed: the ﬁrst is to evalu-\nate the performance of the acoustic models, and the sec-\nond is to evaluate the proposed matching approaches. For\nthe ﬁrst task, we use one simple evaluation metric: the\noverall classiﬁcation accuracy which is deﬁned as the frac-\ntion of instances that are correctly classiﬁed. For the sec-\nond task, our goal is to evaluate the ability to match the\nground-truth phrase in the score dataset to the query one,\nwhich is almost identical to the goal of a QBSH system:\n”ﬁnding the ground-truth song in a song database from\na given singing/humming query” . Therefore, we borrow\nthe standard performance metrics used in QBSH task to\nevaluate our approaches: Top-M hit and Mean Recipro-\ncal Rank (MRR) [8]. The Top-M hit rate is the proportion\nof queries for which ri\u0014M, whereridenotes the rank\nof the ground-truth score phrase. MRR is the average of\nthe reciprocal ranks across all queries, nis the number of\nqueries, and rank iis the posterior probability rank of the\nground-truth phrase corresponding to the i-th query.\nMRR =1\nnnX\ni=11\nrank i(5)\n4.2 Acoustic models\nCNN, DNN and GMM acoustic models yield probability\nscores for each phoneme class. In order to evaluate the\nclassiﬁcation accuracy, we choose the phoneme class with\nthe maximum probability score as the prediction. Table 4.2\nreports the performance of CNN, DNN and GMM acoustic\nmodels evaluated on the test set.\ndan(#parameters) laosheng (#parameters)\nCNNs 0.484(222k) 0.432(222k)\nDNNs 0.284(481k) 0.282(430k)\nGMMs 0.290(-) 0.322(-)\nTable 2 . Overall classiﬁcation accuracies of CNN and\nbaseline acoustical models for danandlaosheng datasets.\nThe relatively low classiﬁcation accuracies for all three\nmodels show that modeling the phonetic characteristics of\njingju singing voice is a challenging problem. Our best\nresults are achieved with CNNs – and GMMs perform bet-\nter than DNNs. Interestingly, these results contrast with\nthe literature where Hinton et al. [9] describe that DNNs\nacoustic models largely outperform GMMs for automatic\nspeech recognition, and Maas et al. [13] showed that CNNs\n3Code:https://goo.gl/1XB6j1432 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017perform worse than DNNs for building speech acoustic\nmodels. First, we argue that in our case DNNs perform\nworse than GMMs and CNNs because a small amount of\ntraining data is available. DNNs require a lot of train-\ning data to achieve good performance and note that large\namounts of training data are typically not available for\nmost MIR tasks. And second, note the CNNs used here are\nspeciﬁcally designed to efﬁciently learn timbre representa-\ntions [17] while Maas et al. [13] used small square ﬁlters,\nwhich proved successful in computer vision tasks. These\nresults show that using CNN architectures designed for the\ntask at hand is especially beneﬁcial in small data scenarios.\nA CNN model is used in the following experiments.\n4.3 Parameters optimization\nThe parameters which need to be optimized for danand\nlaosheng role-types are: the weighting factor \u000bfor the\npost-processor duration model, and the proportionality\nconstant\rfor both models: HSMMs and post-processor\nduration model. Table 4.3 reports the optimal values we\nobtained by doing grid search on the development set –\nMRR metric was maximized.\nSearch Optimal\nParameters bounds values\n\u000b [0:25;2]with step 0.25 1.0 / 1.0\n\rHSMMs [0:1;2]with step 0.1 0.1 / 0.1\n\rpost-processor [0:1;2]with step 0.1 0.7 / 1.5\nTable 3 . Parameters to be optimized, search bounds and\nresulting optimal values ( dan/laosheng ).\n4.4 Duration modeling\nTo highlight the advantage of using duration modeling\nmethods for audio to score matching, a standard HMM\nwithout explicitly imposing the occupancy distribution is\nused as a baseline. Results in Figure 7 show that its perfor-\nmance is inferior to the HSMM duration model.\nFigure 7 . Phrase matching performance of HSMM and\npost-processor duration model with CNN acoustic model:\ndan(top), laosheng (bottom).\nOne can also observe in Figure 7 that HSMM performs\nthe best, improving the baseline MRR metric performance\nby 13.2% for danrole-type and 15.1% for laosheng role-\ntype. This means that HSMMs explicit duration modelingcan help achieve a better audio to score matching by using\nphonetic information.\nThe post-processor duration model does not signiﬁ-\ncantly improve the baseline performance. This result con-\ntrasts with the literature, where the post-processor dura-\ntion model worked better than HSMMs for singing voice\nkeyword spotting [12]. This inconsistency might result\nfrom (i)the length difference of the matching unit (singing-\nwords vs. singing-phrases), and (ii)the large standard de-\nviation of the jingju singing phonemes length. First, in\nKruspe’s work [12], the matching unit is the singing key-\nword – which usually contains fewer phonemes than a\nsinging phrase (as in our case). And second, the vowel\nlength standard deviation of the a cappella dataset used by\nKruspe [12] (around 0.3s) is much short than in our dataset\n(dan: 0.97s, laosheng : 0.78s) – denoting less vowel du-\nration variance than in our study case. Moreover, a sig-\nniﬁcant deﬁciency of the post-processor duration model is\nthat it does not provide the most likely state sequence by\ninternally considering the durations, but it computes a new\nweighted likelihood given the obtained sequence [12]. If\nthe most likely state sequence is decoded poorly, it can’t\nbe restored by the post-processor duration model.\n5. CONCLUSIONS AND FUTURE WORK\nIn this paper, we presented an audio to score matching ap-\nproach that uses phonetic and duration information.\nWe explored two duration models: HSMM and post-\nprocessor duration model. HSMMs achieved better results\nthan the post-processor duration model – probably due to\n(i)the matching units length, (ii)the large standard devia-\ntion of the considered phonemes, and (iii)because for the\npost-processor duration model it is hard to recover a decod-\ning mistake. Moreover, HSMMs achieved a better match-\ning performance than the baseline-HMMs approach, which\nonly took into account phonetic information, denoting the\nutility of using duration information.\nWe also compared CNN, DNN and GMM acoustic\nmodels, and CNNs have shown to be superior in our small\nsinging voice audio dataset. The used CNN architecture\nwas speciﬁcally designed to learn timbral representations\nefﬁciently [17] – this being the key factor for enabling\nCNNs (a deep learning method requiring large amounts of\ndata) to perform so well on such a small dataset.\nThere are many possibilities to improve our approach.\nIt has been shown in the speech research ﬁeld that LSTM\nRNNs achieved the best acoustic modeling performance\n[18]. However, this method requires a large training\ndataset in order prevent from over-ﬁtting. Another pos-\nsibility to improve our acoustic model is to go deeper with\nthe current single-layer CNN architecture, but this will also\nrequire more training data. We plan to collect more jingju\na cappella singing recordings and perform data augmenta-\ntion to leverage the capability of the acoustic models. Fur-\nthermore, in order to take advantage of the melodic infor-\nmation existing in both audio and score datasets, we also\nplan to investigate methods which can fuse melodic, pho-\nnetic and duration information.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4336. ACKNOWLEDGEMENTS\nWe are grateful for the GPUs donated by NVidia. This\nwork is partially supported by the Maria de Maeztu Pro-\ngramme (MDM-2015-0502) and by the European Re-\nsearch Council under the European Union’s Seventh\nFramework Program, as part of the CompMusic project\n(ERC grant agreement 267583).\n7. REFERENCES\n[1] D. A. A. Black, M. Li, and M. Tian. Automatic Identi-\nﬁcation of Emotional Cues in Chinese Opera Singing.\nInICMPC , Seoul, South Korea, August 2014.\n[2] Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learning\nby exponential linear units (elus). arXiv:1511.07289 ,\n2015.\n[3] G. B. Dzhambazov and X. Serra. Modeling of\nphoneme durations for alignment between polyphonic\naudio and lyrics. In SMC , Maynooth, Ireland, 2015.\n[4] R. Gong, P. Cuvillier, N. Obin, and A. Cont. Real-Time\nAudio-to-Score Alignment of Singing V oice Based on\nMelody and Lyric Information. In Interspeech , Dres-\nden, Germany, September 2015.\n[5] Rong Gong, Nicolas Obin, Georgi Dzhambazov, and\nXavier Serra. Score-informed syllable segmentation\nfor jingju a cappella singing voice with mel-frequency\nintensity proﬁles. In International Workshop on Folk\nMusic Analysis , M´alaga, Spain, June 2017.\n[6] Y . Gu ´edon. Hidden hybrid markov/semi-markov\nchains. Computational Statistics & Data Analysis ,\n49(3):663 – 688, 2005.\n[7] Yann Gu ´edon. Exploring the state sequence space for\nhidden markov and semi-markov chains. Computa-\ntional Statistics & Data Analysis , 51(5):2379–2409,\n2007.\n[8] Z. Guo, Q. Wang, G. Liu, J. Guo, and Y . Lu. A mu-\nsic retrieval system using melody and lyric. In 2012\nIEEE International Conference on Multimedia and\nExpo Workshops , pages 343–348, July 2012.\n[9] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed,\nN. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen, T. N.\nSainath, et al. Deep neural networks for acoustic mod-\neling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine ,\n29(6):82–97, 2012.\n[10] B. Juang, L. Rabiner, S. Levinson, and M. Sondhi. Re-\ncent developments in the application of hidden markov\nmodels to speaker-independent isolated word recogni-\ntion. In ICASSP , volume 10, pages 9–12, Apr 1985.\n[11] D. Kingma and J. B. Adam. Adam: A method for\nstochastic optimization. arXiv:1412.6980 , 2014.[12] A. M. Kruspe. Keyword spotting in singing with\nduration-modeled hmms. In EUSIPCO , pages 1291–\n1295, August 2015.\n[13] Andrew L Maas, Peng Qi, Ziang Xie, Awni Y Han-\nnun, Christopher T Lengerich, Daniel Jurafsky, and\nAndrew Y Ng. Building dnn acoustic models for large\nvocabulary speech recognition. Computer Speech &\nLanguage , 41:195–213, 2017.\n[14] Marius Miron, Julio J. Carabias-Orti, Juan J. Bosch,\nEmilia G ´omez, and Jordi Janer. Score-informed source\nseparation for multichannel orchestral recordings.\nJECE , 2016, December 2016.\n[15] E. Molina, L. J. Tard ´on, I. Barbancho, and A. M. Bar-\nbancho. The importance of f0 tracking in query-by-\nsinging-humming. In ISMIR , Taipei, Taiwan, 2014.\n[16] Jordi Pons and Xavier Serra. Designing efﬁcient archi-\ntectures for modeling temporal features with convolu-\ntional neural networks. In ICASSP , New orleans, USA,\n2017.\n[17] Jordi Pons, Olga Slizovskaia, Rong Gong, Emilia\nG´omez, and Xavier Serra. Timbre analysis of mu-\nsic audio signals with convolutional neural networks.\narxiv.org:1703.06697 , 2017.\n[18] Hasim Sak, Andrew W. Senior, Kanishka Rao, and\nFranc ¸oise Beaufays. Fast and accurate recurrent neu-\nral network acoustic models for speech recognition.\nCoRR , abs/1507.06947, 2015.\n[19] J. Sohn, N. S. Kim, and W. Sung. A statistical model-\nbased voice activity detection. IEEE signal processing\nletters , 6(1):1–3, 1999.\n[20] C. C. Wang and J. S. R. Jang. Improving query-by-\nsinging/humming by combining melody and lyric in-\nformation. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing , 23(4):798–806, April 2015.\n[21] L. Wang, S. Huang, S. Hu, J. Liang, and B. Xu. An\neffective and efﬁcient method for query by humming\nsystem based on multi-similarity measurement fusion.\nInInternational Conference on Audio, Language and\nImage Processing , pages 471–475, July 2008.\n[22] E. Wichmann. Listening to Theatre: The Aural Di-\nmension of Beijing Opera . University of Hawaii Press,\n1991.434 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Towards Automatic Mispronunciation Detection in Singing.",
        "author": [
            "Chitralekha Gupta",
            "David Grunberg",
            "Preeti Rao",
            "Ye Wang 0007"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1418073",
        "url": "https://doi.org/10.5281/zenodo.1418073",
        "ee": "https://zenodo.org/records/1418073/files/GuptaGRW17.pdf",
        "abstract": "A tool for automatic pronunciation evaluation of singing is desirable for those learning a second language. How- ever, efforts to obtain pronunciation rules for such a tool have been hindered by a lack of data; while many spoken- word datasets exist that can be used in developing the tool, there are relatively few sung-lyrics datasets for such a pur- pose. In this paper, we demonstrate a proof-of-principle for automatic pronunciation evaluation in singing using a knowledge-based approach with limited data in an auto- matic speech recognition (ASR) framework. To demon- strate our approach, we derive mispronunciation rules spe- cific to South-East Asian English accents in singing based on a comparative study of the pronunciation error patterns in singing versus speech. Using training data restricted to American English speech, we evaluate different methods involving the deduced L1-specific (native language) rules for singing. In the absence of L1 phone models, we in- corporate the derived pronunciation variations in the ASR framework via a novel approach that combines acoustic models for sub-phonetic segments to represent the miss- ing L1 phones. The word-level assessment achieved by the system on singing and speech is similar, indicating that it is a promising scheme for realizing a full-fledged pronun- ciation evaluation system for singing in future.",
        "zenodo_id": 1418073,
        "dblp_key": "conf/ismir/GuptaGRW17",
        "keywords": [
            "automatic pronunciation evaluation",
            "singing",
            "knowledge-based approach",
            "limited data",
            "speech recognition framework",
            "mispronunciation rules",
            "South-East Asian English accents",
            "comparative study",
            "ASR framework",
            "acoustic models"
        ],
        "content": "TOWARDS AUTOMATIC MISPRONUNCIATION DETECTION IN\nSINGING\nChitralekha Gupta1;2David Grunberg1Preeti Rao3Ye Wang1\n1School of Computing, National University of Singapore, Singapore\n2NUS Graduate School for Integrative Sciences and Engineering,\nNational University of Singapore, Singapore\n3Department of Electrical Engineering, Indian Institute of Technology Bombay, India\nchitralekha@u.nus.edu, wangye@comp.nus.edu.sg\nABSTRACT\nA tool for automatic pronunciation evaluation of singing\nis desirable for those learning a second language. How-\never, efforts to obtain pronunciation rules for such a tool\nhave been hindered by a lack of data; while many spoken-\nword datasets exist that can be used in developing the tool,\nthere are relatively few sung-lyrics datasets for such a pur-\npose. In this paper, we demonstrate a proof-of-principle\nfor automatic pronunciation evaluation in singing using a\nknowledge-based approach with limited data in an auto-\nmatic speech recognition (ASR) framework. To demon-\nstrate our approach, we derive mispronunciation rules spe-\nciﬁc to South-East Asian English accents in singing based\non a comparative study of the pronunciation error patterns\nin singing versus speech. Using training data restricted to\nAmerican English speech, we evaluate different methods\ninvolving the deduced L1-speciﬁc (native language) rules\nfor singing. In the absence of L1 phone models, we in-\ncorporate the derived pronunciation variations in the ASR\nframework via a novel approach that combines acoustic\nmodels for sub-phonetic segments to represent the miss-\ning L1 phones. The word-level assessment achieved by the\nsystem on singing and speech is similar, indicating that it\nis a promising scheme for realizing a full-ﬂedged pronun-\nciation evaluation system for singing in future.\n1. INTRODUCTION\nEducators recommend singing as a fun and effective lan-\nguage learning aid [6]. In fact, it has been observed that\nthe use of songs and karaoke is helpful in teaching and\nimproving pronunciation in adult second language (L2)\nclasses [1, 17] . Scientiﬁc studies have shown that there is\na connection between the ability of phonemic production\nof a foreign language and singing ability [16], and singing\nc\rChitralekha Gupta, David Grunberg, Preeti Rao, Ye\nWang. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Chitralekha Gupta, David Grunberg,\nPreeti Rao, Ye Wang. “Towards automatic mispronunciation detection in\nsinging”, 18th International Society for Music Information Retrieval Con-\nference, Suzhou, China, 2017.ability often leads to better imitation of phrases in an un-\nknown language [15]. More recently, evidence from exper-\nimental psychology suggests that learning a new language\nthrough songs helps improve vocabulary gain, memory re-\ncall, and pronunciation [11]. Additionally, singing releases\nthe need to focus on prosody, as melody of the song over-\nrides the prosodic contrasts while singing [13]. So, given a\nfamiliar melody, all the attention can be on articulating the\nlyrics correctly.\nGiven the potential of singing in pronunciation training,\nit is of interest to research automatic pronunciation evalu-\nation for sung lyrics similar to the large body of work in\ncomputer-aided pronunciation training (CAPT) for speech\n[18, 29]. There is little work on mispronunciation detec-\ntion for sung lyrics. Jha et al. attempted to build a system\nfor evaluating vowels in singing with Gaussian Mixture\nModel (GMM) and linear regression using Mel Frequency\nCepstral Coefﬁcients (MFCC) and pitch as features [12].\nHowever, they did not account for possible pronunciation\nerror patterns in singing, and further, their work did not\nextend to consonants. There have been a few other stud-\nies that have subjectively compared the pronunciation in\nsinging versus that in speech. Yoshida et al. [26] con-\nducted a subjective mispronunciation analysis in singing\nand speech in English for Japanese natives and found that\nthe subjects familiar with singing tend to make less mis-\ntakes in pronunciation while singing than speaking. An-\nother study found that the most frequent pronunciation er-\nrors by Indonesian singers in singing English songs occur\nin the consonants [21]. None of these studies however at-\ntempted to build an automatic evaluator of pronunciation\nin singing.\nThough studies have been conducted to compare\nsinging and speech utterances [5, 19], the automated as-\nsessment of singing pronunciation is hampered by the lack\nof training datasets of phone-level annotated singing. Duan\net al. created a dataset to analyse the similarities and dif-\nferences between spoken and sung phonemes [8]. This\ndataset consists of sung and spoken utterances from 12\nunique subjects, out of which 8 were noted as non-native\nspeakers. But their work did not study the pronuncia-\ntion error patterns in singing or speech. A part of this390dataset was phonetically transcribed in 39 CMU phones\n[25], which is inadequate for annotating non-native pro-\nnunciations. We use a subset of audio clips from this\ndataset for our work (as explained in Section 2.2). But we\ndid not use their phonetic annotations due to these limita-\ntions.\nIn this work, we demonstrate a knowledge-based ap-\nproach with limited data to automatically evaluate pro-\nnunciation in singing in an automatic speech recognition\n(ASR) framework. We will adopt a basic method of pho-\nnetic evaluation that is used for speech, i.e. deriving pro-\nnunciation variants based on L1-L2 pair error patterns,\nand incorporating this knowledge in the ASR framework\nfor evaluation [3, 23]. In our study, we analyze the error\npatterns in singing versus those in speech in the accents\nof South-East Asia - Malaysian, Indonesian, and Singa-\nporean. South-East Asia is one of the most populous re-\ngions of the world, where the importance of speaking stan-\ndard English has been recognized [14], and hence such a\npronunciation evaluation system is desired. Given that the\ndata available to train acoustic models is restricted to a na-\ntive American English speech database [10], we present\na novel approach of combining sub-phonetic segments to\nrepresent missing L1-phones. Also, we demonstrate how\nthe systematic incorporation of the knowledge of the error\npatterns helps us obtain a reliable pronunciation evaluation\nsystem for non-native singing.\n2. PRONUNCIATION ERROR PATTERNS\n2.1 Previous Studies\nIn the process of learning a second language L2, a common\ntype of mispronunciation is replacing phones of L2 that do\nnot exist in the native language (L1) with the closest sound-\ning phoneme of L1 [4]. In Malaysian, Singaporean, and In-\ndonesian English, the dental fricatives /th/ and /dh/ are of-\nten substituted by alveolar stops /t/ and /d/ respectively (eg.\n“three”!“tree”, “then”!“den”). These accents are inﬂu-\nenced by Malay, Mandarin, and Indonesian languages, in\nwhich the dental fricatives /th/ and /dh/ are absent [2, 7, 9].\nAlso, a pattern particularly seen in Indonesian English ac-\ncent is that the alveolar stop consonants /t/ and /d/ tend to\nbe substituted by their apico-dental unaspirated stop vari-\nant. The reason for this confusion is that in the Indonesian\nlanguage, the phones /d/ and /t/ can be both alveolar or den-\ntal [2, 22]. Another pattern in Singaporean and Malaysian\naccents is that they tend to omit word-end consonants, or\nreplace them with glottal stops. Note the lack of word-end\nconsonants in the Malay counterparts of words like “prod-\nuct” is “produk”. Also in Mandarin, most words do not end\nwith a consonant, except /ng/ and /n/. V owel difﬁculties are\nseen in all these accents, such as long-short vowel confu-\nsions like “bead”!“bid”, because the long /iy/ is absent in\nthe Indonesian language. Another clear pattern reported is\nthe voiced post-alveolar approximant /r/ in English being\npronounced as an apical post-dental trill /r/ in Indonesian,\nthat sounds like a rolling “r”.\nHere, we investigate the general rules of mispronuncia-\nFigure 1 : Example of word-level subjective evaluation on\nthe website (incorrect words marked in red).\ntion in singing, which will be then used for automatic pro-\nnunciation evaluation in singing. We will, henceforth, refer\nto the L1 roots of Malaysian, Singaporean, and Indonesian\nEnglish as M, S, and I, respectively.\n2.2 Dataset\nThe dataset (a subset of a published dataset [8]) consists of\na total of 52 audio ﬁles: 26 sung and 26 spoken, from 15\npopular English songs (like Do Re Mi, Silent Night, etc.).\nEach song has 28 lines (phrases) on an average. These\nsongs were sung and spoken by 8 unique subjects (4 male,\n4 female) - 3 Indonesian, 3 Singaporean, and 2 Malaysian.\nThe subjects were students at National University of Singa-\npore, with experience in singing. The subjects were asked\nto familiarize themselves with lyrics of the songs before\nthe recording session and could use a printed version of\nthe lyrics for their reference during recording. No back-\nground accompaniments were used while recording except\nfor metronome beats which were sent to headphones.\nWe developed a website to collect subjective ratings for\nthis dataset. The website consisted of the audio tracks,\ntheir corresponding lyrics, and a questionnaire. Each word\nin the lyrics could be clicked by the rater to mark it as in-\ncorrectly pronounced (red), as shown in the screenshot of\nthe webpage in Figure 1. For each sung and spoken au-\ndio clip, the raters were asked to ﬁrst listen to the track\nand mark the words in the lyrics that were incorrectly pro-\nnounced, and then ﬁll up the questionnaire based on their\nword-error judgment, as shown in Figure 2. We asked 3\nhuman judges (two North American native English speak-\ners, and one non-native speaker proﬁcient in English), to\ndo this task. Here, native English pronunciation (North\nAmerican) is considered as the benchmark for evaluating\npronunciation.\nIn the questionnaire, the judges evaluated the overall\npronunciation quality on a 5 point scale. On a 3 point\nscale, they evaluated the presence of each consonant sub-\nstitution (C1-C4), vowel replacement (V), word-end con-\nsonant deletion (CD), and rolling “r” (R), each correspond-\ning to the rules listed in Table 1, where rating 1 means thereProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 391Figure 2 : Questionnaire for every audio clip on the subjective evaluation webpage.\nare hardly any errors of that category, while rating 3 means\nalmost all of the occurrences have error. We shall refer to\nthese ratings as Rating Set 1. These questions cover all the\nphone error categories in speech in the accents concerned\naccording to the literature, as described in section 2.1. Ad-\nditionally, the questionnaire included a comment text-box\nin which the rater could mention about any other kinds of\nerrors that they observed, which were not covered by the\nother questions. In this way, we tried to ensure that the\nsubjective evaluation was not biased by the mentioned er-\nror categories in the questionnaire.\nThe average inter-judge correlation (Pearson’s) of the\noverall rating question was 0.68 for the sung clips and 0.62\nfor the spoken clips, and that for the questions on error-\ncategories was 0.89 for the sung clips and 0.74 for the\nspoken clips. Thus the inter-judge agreement was high.\nAlso, in the comment text-box, the judges provided only\nrare minor comments, such as mispronouncing “want to”\nas “wanna”, which could not be categorized as systematic\nerrors due to L1 inﬂuence, and hence are not included in\nthe current study.\nWe chose the word-level pronunciation assessment\n(“correct”/ “wrong”) of one of the North American native\nEnglish speaking judges as the ground truth. We shall refer\nto these ratings as Rating Set 2.\nC1 C2 C3 C4 CD R V1.01.21.41.61.82.02.2Avg. RatingsSung\nSpokenFigure 3 : Average subjective rating for seriousness of each\nerror category for singing and speech. Error category la-\nbels are as per Table 1.\n2.3 Analysis of Error Patterns: Singing vs. Speech\nFrom the rating set 2, we obtained a list of consonant and\nvowel error patterns in speech and singing, and examples\nof such words, as shown in Table 2. These error categories\ncan be directly mapped to the questions in the question-\nnaire, and are consistent with the literature on error pat-\nterns in South-East Asian accents.\nOur aim here is to derive a list of rules relevant to mis-392 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Table 1 : Statistical signiﬁcance of the difference of each\nerror category between singing and speech (WB: Word Be-\nginning, WM: Word Middle, WE: Word Ending).\npronunciation in singing. From rating set 1, we compute\nthe average subjective rating for each of the mispronun-\nciation rules for singing and speech, as shown in Fig-\nure 3. To identify the rules that are relevant for singing,\nwe compute the difference of the average ratings between\nsinging and speech for every rule for each of the 26 pairs\nof audio ﬁles, and compute the p\u0000value of this differ-\nence. For a particular rule, if the overall average rating\nof singing is less than that of speech, and the difference\nof average ratings between singing and speech is signiﬁ-\ncant ( p\u0000value < 0:05), then that particular kind of mis-\npronunciation is not frequent in singing, and thus the rule\nis not relevant for singing. For example, we found that\nmost of the detectable mispronunciations in singing were\nseen in consonants, which agrees with the literature pre-\nviously discussed [2, 7, 9, 21, 22]. The mean rating for\nthe question “Are there vowel errors?” was much lower\nfor singing than for speech, meaning that there are fewer\nvowel errors perceived in singing than in speech (as shown\nin Figure 3). The difference of the 26 ratings for this ques-\ntion between singing and speech is statistically signiﬁcant\n(p\u0000value = 5\u000210\u00005) (Table 1), and hence conﬁrms this\ntrend. In singing, the vowel duration and pitch in singing\nare usually dictated by the corresponding melodic note at-\ntributes, which makes it different from spoken vowels. For\nexample, the word “sleep” is stretched in duration in the\nsong “Silent Night”, thus improving the pronunciation of\nthe vowel. However, in speech the word might tend to be\npronounced as “slip”. The explanation could lie in the way\nsingers imitate vowels based on timbre (both quality and\nduration) rather than by the categorical mode of speech\nperception which is applied only to the consonants. In\nthe same way, we also found that the “word-end consonant\ndeletion” category of errors is signiﬁcantly less frequent\nin singing than in speech ( p\u0000value = 0:032) (Table 1).\nThis implies that either the word-end stop consonants like\n/t/ and /d/ are hardly ever omitted in singing or are imper-\nceptible to humans. This leads us to the conclusion that\nonly a subset of the error patterns that occur in speech are\nseen to be occurring in singing. This is a key insight that\nsuggests a possible learning strategy: learning this “sub-\nset” of phoneme pronunciation through singing, and the\nrest through speech.\nAnother interesting inference from Figure 3 is that on\nan average, singing has a lower extent of perceived pro-\n Consonants Error WB WM WE /dh/ ! /d/ the, they, thy, then mother, another,   /th/ ! /t/ thought, thread,  nothing  /t/ ! /th/ to, tea, take spirits, into, sitting note, it, got /d/ ! /dh/ drink, dear outdoors, wonderful cloud, world Consonant deletion (only WE) night, moment, child rolling \"r\" run, ray,round bread, drop, bright brighter, after Vowels Error Actual word What is spoken ow-->ao golden gawlden uw-->uh fool full iy-->ih,ix seem, sees, sleeping,  sim, sis, slipping eh-->ae every avry  Table 2 : Error categories in singing and speech, and ex-\namples of words where they occur.\nL1 Label Rule Example M, S, I C1 WB,WM /dh/ → /d/ “that” → “dat”  M, S, I C2 WB,WM / th/ → /t/  “thought” → “taught”  I C3 WB,WM,WE  /t/ → /th/  “sitting” → “sithing”,  “take” → “thake”  I C4 WB,WM,WE  /d/ → /dh/  “dear” → “dhear”,  “cloud” → “cloudh”     L1 Label Rule Dictionary A Dictionary B Can. Mis. Can. Mis. M, S, I C1 WB,WM /dh/ → /d/ dh → vcl d  dh → vcl d  vcl dh →vcl d M, S, I C2 WB,WM / th/ → /t/  th → cl t  th  → cl t cl th → cl t I C3 WB,WM,WE  /t/ → /th/  cl t → th cl t →  cl th I C4 WB,WM,WE  /d/ → /dh/  vcl d → dh vcl d→ vcl dh  \nTable 3 : Mispronunciation rules for singing, and corre-\nsponding transcriptions for Dictionaries A and B. Can.:\nCanonical, Mis.: Mispronunciation (cl: unvoiced closure,\nvcl: voiced closure, dh: dental voiced fricative, d: alve-\nolar voiced stop, th: dental unvoiced fricative, t: alveolar\nunvoiced stop).\nnunciation errors compared to speech, which is also indi-\ncated by the average of the overall rating, which is higher\nfor singing (singing = 3.87, speech = 3.80). This suggests\nthat if the non-native subject is familiar with a song and\nits lyrics, he/she makes fewer pronunciation mistakes in\nsinging compared to speech. Also, a non-native speaking\naccent is typically characterised by L1-inﬂuenced prosody\nas well such as stress and intonation aspects, which can\ninﬂuence subjective ratings. Singing on the other hand\nuses only the musical score and is therefore devoid of L1\nprosody cues.\nTable 3 lists the L1-speciﬁc mispronunciation rules for\nsinging that we derived, in which the word-end consonant\ndeletion and vowel substitution rules have been omitted for\nreasons mentioned above. In the Indonesian accent, the\nphone “r” was often replaced with a rolling “r” (trill) (Fig-\nure 3), which occurs frequently in singing as well (Table\n1). But this phone is absent in American English, so we\ndo not have a phonetic model to evaluate it. So, we have\nexcluded this error pattern in this study.\nWith our dataset of sung utterances from 8 subjects, we\ncould see clear and consistent mispronunciation patterns\nacross the speakers in our subjective assessment study, and\nthese patterns agree with the phonetic studies of L1 inﬂu-\nence on English from these accents in the literature. There-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 393fore, even if the dataset is small, it captures all the expected\ndiversity.\n3. AUTOMATIC DETECTION OF\nMISPRONUNCIATION IN SINGING\nOur goal is to use an automatic speech recognition (ASR)\nsystem to detect mispronunciations in singing. In previ-\nous studies, vowel error patterns in Dutch pronunciation\nof L2 learners were used by Doremalen et al. to improve\nautomatic pronunciation error detection systems [23]. In\nanother work, Black et al. derived common error patterns\nin children’s speech and used an adapted lexicon in an ex-\ntended decoding tree to obtain word-level pronunciation\nassessment [3]. A standard way to detect mispronuncia-\ntion is to let the ASR recognize the most likely sequence\nof phonemes for a word from a given set of acceptable\n(canonical) and unacceptable (mispronounced) pronunci-\nation variants of that word. A pronunciation is detected as\nunacceptable if the chosen sequence of phonemes belongs\nto the list of unacceptable pronunciation variants of the\nword (called the “lexicon” or the “LEX” method in [3]).\nWhile the present work is similar in principle to the above,\nwe face the additional challenge of lack of training data for\nthe L1 phones not present in L2. Yu et al. [27] have used a\ndata-driven approach to convert the foreign-language lexi-\ncon (L2) to native-language lexicon (i.e. using L1 phones\nonly), where they had large L1 speech training data, in\ncontrast to our case of availability of L2 training speech\nonly. In the current work, we use a novel knowledge-based\napproach to overcome the constraints of lack of L1 train-\ning data for both speech and singing. We compare the\ncase of restricting ourselves to L2 phones with a method\nthat uses L1 phones derived from a combination of sub-\nphonetic segments of L2 speech to approximate unavail-\nable L1 phones.\nWe compare a Dictionary A that contains only Ameri-\ncan English (TIMIT [10]) phones (L2), with a Dictionary\nB that contains TIMIT phones+modiﬁed (L1-adapted)\nphones. To design Dictionary B, we compared the phones\nof the South-East Asian accents with that of the Ameri-\ncan English TIMIT speech dataset [10]. We found that the\ndental fricatives /th/ and /dh/ are often mispronounced as\nalveolar stops /t/ and /d/ respectively (rules C1, C2). Both\nof the substituted phones /t/ and /d/ are present in Amer-\nican English, and hence their phone models are available\nin TIMIT. But when L1 is Indonesian, the alveolar stop\nconsonants /t/ and /d/ tend to be substituted by their apico-\ndental unaspirated stop variant (rules C3, C4), as explained\nin Section 2.1. But dental stop phones are not annotated\nin American English datasets like TIMIT [10]. In order\nto solve this problem of lack of dental stop phone mod-\nels in L2, we combined sub-phonetic TIMIT models. We\nobserved that the dental stop phones consist of a closure\nperiod followed by a burst period with dental place of artic-\nulation. So we combined the TIMIT models for unvoiced\nclosure model /cl/ with the unvoiced dental fricative model\n/th/ to approximate unvoiced dental stop /t/, as shown in\nFigure 4, and voiced closure model /vcl/ with the voiced\nsihclihngth\nn      ahthihng(a)\nsihclihngthg      ehern cltiyvcl\n(b)\nFigure 4 : (a) American speaker (TIMIT) articulating\nword-middle unvoiced dental fricative /th/ in “nothing”\n(note: there is no closure) (b) Indonesian speaker substitut-\ning unvoiced alveolar stop with unvoiced dental stop (“sit-\nting” as “sithing”) modeled as /cl th/.\ndental fricative model /dh/ to obtain voiced dental stop /d/.\nIt is important to note that in these accents, the dental frica-\ntives /th/ and /dh/ are also often substituted by dental stops\n/cl th/ and /vcl dh/. But this particular substitution pat-\ntern is common in American English [28], and hence not\nconsidered to be mispronunciation. Hence, we add these\nvariants to the list of acceptable variants (canonical).\nIn summary, the mispronunciation rules in Dictionary\nB are: dental fricative and stop /dh/ being mispronounced\nas alveolar stop /d/ (L1: M, S, I); dental fricative and stop\n/th/ being mispronounced as alveolar stop /t/ (L1: M, S,\nI); alveolar stop /t/ being mispronounced as dental stop\n/th/ (L1: I); and alveolar stop /d/ being mispronounced as\ndental stop /dh/ (L1: I). These mispronunciation rules are\nlisted in Table 3.\n3.1 Methodology\nWe use the toolkit KALDI [20] for training 48 context in-\ndependent GMM-HMM and DNN-HMM phonetic mod-\nels using the TIMIT train set [10] with the parameters\nset by Vesely et al. [24]. The HMM topology is 3 ac-\ntive states, the MFCC features are frame-spliced by 11\nframes, dimension-reduced by Linear Discriminant Analy-\nsis (LDA) to 40 dimensions. Maximum Likelihood Linear\nTransformation (MLLT), feature-space Maximum Likeli-\nhood Linear Regression (fMLLR), and Cepstral Mean and\nVariance Normalization (CMVN) are applied for speaker\nadaptive training. The DNN has 6 hidden layers, 2048\nhidden units per layer. The Restricted Boltzmann Ma-\nchine (RBM) pre-training algorithm is contrastive diver-\ngence and the frame cross-entropy training is done by\nmini-batch stochastic gradient descent. Phone recognition\nperformance of the acoustic models trained and tested on\nTIMIT was consistent with the literature [24].394 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Forced Alignment Lyrics \nSpeech Phonetic Models \nMispronunciation decision per word Song Phrase \nMost-likely word transcriptions Variant Dictionary \nMapping \nCanonical Dictionary \nError Patterns \nCanonical Dictionary \nEvaluator Figure 5 : Overview of automatic mispronunciation detec-\ntion in singing.\n  L1 (#EP-W) AM Speech Singing Dictionary A Dictionary B Dictionary A Dictionary B P R F P R F P R F P R F M,S  (245) DNN-HMM 0.51 0.58 0.54 0.60 0.68 0.63 0.62 0.65 0.63 0.69 0.66 0.67 GMM-HMM 0.41 0.54 0.47 0.49 0.63 0.55 0.54 0.52 0.53 0.55 0.51 0.53 #GT-E 78 86 I  (834) DNN-HMM 0.29 0.55 0.38 0.56 0.46 0.50 0.23 0.59 0.33 0.42 0.54 0.47 GMM-HMM 0.27 0.50 0.35 0.46 0.40 0.43 0.21 0.58 0.31 0.33 0.34 0.34 #GT-E 219 176 \nTable 4 : Performance of automatic mispronunciation de-\ntection for singing and speech. P: Precision = TP/(TP+FP);\nR: Recall = TP/(TP+FN); F: F-score = 2.P.R/(P+R); AM:\nAcoustic Models; #GT-E: no. of error-prone words mispro-\nnounced; #EP-W: no. of error-prone words. L1 languages\n- M: Malaysian, S: Singaporean, and I: Indonesian.\nWe use these speech trained acoustic-phonetic models,\nalong with the L1-speciﬁc variant dictionary (A or B) to\nforce-align the lyrics to the sung and spoken audio ﬁles, to\nobtain word-level automatic pronunciation evaluation by\nthe “LEX” method, as described before. An overview of\nthis system is shown in Figure 5. We ﬁrst segment the\naudio ﬁles at phrase level by aligning its pitch track with\na template containing a reference pitch track and marked\nphrase-level boundaries, using dynamic time warping. The\n26 songs are segmented into 740 phrases, containing a to-\ntal of 5107 words. For singing, out of these 5107 words,\n1079 words are the error-prone words, i.e. they fall un-\nder the mispronunciation rules. Only 14 out of the rest\n4028 non-error-prone words (0.3%) are subjectively evalu-\nated as mispronounced in singing, which conﬁrms that the\nmispronunciation rules for singing are correctly identiﬁed.\nTo compare speech and singing, we apply the same rules\nfor the speech phrases because we expect that the words\nthat are mispronounced in singing are likely to be mispro-\nnounced in speech as well.\nTable 4 shows the validation results for the L1-speciﬁc\nerror-prone words from the two acoustic model conﬁgu-\nrations in singing and speech, using the dictionaries A and\nB, where the ground truth is the word-level subjective eval-\nuation as obtained in rating set 2. To evaluate the perfor-\nmance of the system, we compute the metrics precision, re-call, and F-score [3], where TP (True Positive) is the num-\nber of mispronounced words detected as mispronounced,\nFP (False Positive) is the number of correctly pronounced\nwords detected as mispronounced, and FN (False Nega-\ntive) is the number of mispronounced words detected as\ncorrectly pronounced (Table 4).\n4. RESULTS AND DISCUSSION\nWe note that the method of combining sub-phonetic Amer-\nican English models for approximating the missing phone\nmodels of L1 is effective as the F-scores indicate that the\nsystem using dictionary B outperforms the one using A\nin all the cases. DNN-HMM outperforms GMM-HMM\nconsistently for the task for pronunciation evaluation in\nsinging, as it has been widely observed in speech recog-\nnition. Also, the F-score values of singing and speech are\nsimilar, which shows that our knowledge-based approach\nfor singing pronunciation evaluation is promising.\nA source of false positives is the rule /t/ !/th/ which\ncauses error when /t/ is preceded by a fricative (eg. /s/),\nfor example “just” [jh, ah, s, cl, t]. Since both /s/ and /th/\nare fricatives, the system gets confused and aligns /th/ at\nthe location of /s/. A way to handle such errors is to obtain\nfeatures speciﬁc to classifying the target and the competing\nphonemes, which will be explored in the future.\n5. CONCLUSION\nIn this paper, we have analysed pronunciation error pat-\nterns in singing vs. those in speech, derived rules for pro-\nnunciation error patterns speciﬁc to singing, and demon-\nstrated a knowledge-based approach with limited data to-\nwards automatic word-level assessment of pronunciation\nin singing in an ASR framework. From subjective evalua-\ntion of word pronunciation, we learn that nearly all identi-\nﬁed mispronunciations have an L1-based justiﬁcation, and\nsinging has only a subset of the errors found in speech. We\nprovide the rules that predict singing mispronunciations for\na given L1. In order to solve the problem of unavailable\nL1 phones due to the lack of training speech data from L1\nspeakers, we propose a method that uses a combination of\nsub-phonetic segments drawn from the available native L2\nspeech to approximate the unavailable phone models. This\nmethod is shown to perform better than the one that re-\nstricts to only L2 phones. And ﬁnally, the performance of\nthis system on singing and speech is comparable, indicat-\ning that this approach is a promising method for develop-\ning a full-ﬂedged pronunciation evaluation system. In fu-\nture, we would explore a combination of data-driven meth-\nods such as in [27] and our knowledge-based methods to\nimprove the mispronunciation detection accuracy.\n6. REFERENCES\n[1] Developing pronunciation through songs. https://\nwww.teachingenglish.org.uk/article/\ndeveloping-pronunciation-through-songs .\nAccessed: 2017-04-27.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 395[2] B. Andi-Pallawa and A.F. Abdi Alam. A comparative\nanalysis between English and Indonesian phonological\nsystems. International Journal of English Language\nEducation , 1(3):103–129, 2013.\n[3] M. Black, J. Tepperman, and S. Narayanan. Auto-\nmatic prediction of children’s reading ability for high-\nlevel literacy assessment. IEEE Transactions on Audio,\nSpeech, and Language Processing , 19(4):1015–1028,\n2011.\n[4] P. Bonaventura, D. Herron, and W. Menzel. Phonetic\nrules for diagnosis of pronunciation errors. In KON-\nVENS: Sprachkommunikation , pages 225–230, 2000.\n[5] W. Chou and G. Liang. Robust singing detection in\nspeech/music discriminator design. In IEEE Interna-\ntional Conference on Acoustics, Speech, and Signal\nProcessing Proceedings 2001 , pages 865–868, 2001.\n[6] F. Dege and G. Schwarzer. The effect of a music pro-\ngram on phonological awareness in preschoolers. Fron-\ntiers in Psychology , 2(124):7–13, 2011.\n[7] D. Deterding. Singapore English . Edinburgh Univer-\nsity Press, 2007.\n[8] Z. Duan, H. Fang, L. Bo, K. C. Sim, and Y . Wang.\nThe NUS sung and spoken lyrics corpus: A quantita-\ntive comparison of singing and speech. In Asia-Paciﬁc\nSignal and Information Processing Association Annual\nSummit and Conference (APSIPA) 2013, IEEE , pages\n1–9, 2013.\n[9] S.Y . Enxhi, T.B. Hoon, and Y .M. Fung. Speech dis-\nﬂuencies and mispronunciations in English oral com-\nmunication among Malaysian undergraduates. Interna-\ntional Journal of Applied Linguistics and English Lit-\nerature , 1(7):19–32, 2012.\n[10] J. Garofolo. TIMIT acoustic-phonetic continuous\nspeech corpus. In Philadelphia: Linguistic Data Con-\nsortium , 1993.\n[11] A. Good, F. Russo, and J. Sullivan. The efﬁcacy of\nsinging in foreign-language learning. Psychology of\nMusic , 43(5):627–640, 2015.\n[12] P. Jha and P. Rao. Assessing vowel quality for singing\nevaluation. In National Conference on Communica-\ntions (NCC) 2012, IEEE , pages 1–5, 2012.\n[13] I. Lehiste. Prosody in speech and singing. In Speech\nProsody 2004, International Conference , 2004.\n[14] L. Lim, A. Pakir, and L. Wee. English in Singapore:\nModernity and management , volume 1. Hong Kong\nUniversity Press, 2010.\n[15] C. Markus and S.M. Reiterer. Song and speech: exam-\nining the link between singing talent and speech imita-\ntion ability. Frontiers in psychology , 4:874, 2013.\n[16] R. Milovanov, P. Pietil, M. Tervaniemi, and P. Es-\nquef. Foreign language pronunciation skills and musi-\ncal aptitude:a study of Finnish adults with higher edu-\ncation. Learning and Individual Differences , 20(1):56–\n60, 2010.[17] H. Nakata and L. Shockey. The effect of singing on\nimproving syllabic pronunciation–vowel epenthesis in\njapanese. In International Conference of Phonetic Sci-\nences , 2011.\n[18] L. Neumeyer, H. Franco, V Digalakis, and M. Wein-\ntraub. Automatic scoring of pronunciation quality.\nSpeech Communication , 30(2):83–93, 2000.\n[19] Y . Ohishi, M. Goto, K. Itou, and K. Takeda. Dis-\ncrimination between singing and speaking voices. In\nProceedings of the Annual Conference of the Inter-\nnational Speech Communication Association (Inter-\nspeech 2005) , pages 1141–1144, 2005.\n[20] D. Povey, A. Ghoshal, G. Boulianne, L. Burget,\nO. Glembek, N. Goel, M. Hannemann, P. Motlicek,\nY . Qian, P. Schwarz, et al. The Kaldi speech recog-\nnition toolkit. In IEEE 2011 workshop on automatic\nspeech recognition and understanding , number EPFL-\nCONF-192584. IEEE Signal Processing Society, 2011.\n[21] I. Riyani and J. Prayogo. An analysis of pronunci-\nation errors made by Indonesian singers in Malang\nin singing English songs. SKRIPSI Jurusan Sastra\nInggris-Fakultas Sastra UM , 2013.\n[22] E. Setyati, S. Sumpeno, M. Purnomo, K. Mikami,\nM. Kakimoto, and K. Kondo. Phoneme-Viseme map-\nping for Indonesian language based on blend shape ani-\nmation. IAENG International Journal of Computer Sci-\nence, 42(3), 2015.\n[23] J. van Doremalen, C. Cucchiarini, and H. Strik. Au-\ntomatic pronunciation error detection in non-native\nspeech: The case of vowel errors in Dutch. The Jour-\nnal of the Acoustical Society of America , 134(2):1336–\n1347, 2013.\n[24] K. Vesely, A. Ghoshal, L. Burget, and D. Povey.\nSequence-discriminative training of deep neural net-\nworks. In Interspeech , pages 2345–2349, 2013.\n[25] R. Weide. The Carnegie Mellon pronouncing dictio-\nnary [cmudict. 0.6], 2005.\n[26] K. Yoshida, N. Takashi, and I. Akinori. Analysis of En-\nglish pronunciation of singing voices sung by Japanese\nspeakers. In International Conference on Intelligent In-\nformation Hiding and Multimedia Signal Processing\n(IEEE) , 2014.\n[27] D. Yu, L. Deng, P. Liu, J. Wu, Y . Gong, and A. Acero.\nCross-lingual speech recognition under runtime re-\nsource constraints. In 2009 IEEE International Con-\nference on Acoustics, Speech and Signal Processing ,\npages 4193–4196. IEEE, 2009.\n[28] S. Zhao. Stop-like modiﬁcation of the dental fricative\n/dh/: An acoustic analysis. The Journal of the Acousti-\ncal Society of America , 128(4):2009–2020, 2010.\n[29] Y . Zheng, R. Sproat, L. Gu, I. Shafran, H. Zhou, Y . Su,\nD. Jurafsky, R. Starr, and S. Yoon. Accent detection\nand speech recognition for shanghai-accented man-\ndarin. In Interspeech , pages 217–220. Citeseer, 2005.396 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Automatic Sample Detection in Polyphonic Music.",
        "author": [
            "Siddharth Gururani",
            "Alexander Lerch 0001"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1418331",
        "url": "https://doi.org/10.5281/zenodo.1418331",
        "ee": "https://zenodo.org/records/1418331/files/GururaniL17.pdf",
        "abstract": "The term ‘sampling’ refers to the usage of snippets or loops from existing songs or sample libraries in new songs, mashups, or other music productions. The ability to auto- matically detect sampling in music is, for instance, benefi- cial for studies tracking artist influences geographically and temporally. We present a method based on Non-negative Matrix Factorization (NMF) and Dynamic Time Warping (DTW) for the automatic detection of a sample in a pool of songs. The method comprises of two processing steps: first, the DTW alignment path between NMF activations of a song and query sample is computed. Second, features are extracted from this path and used to train a Random Forest classifier to detect the presence of the sample. The method is able to identify samples that are pitch shifted and/or time stretched with approximately 63% F-measure. We evaluate this method against a new publicly available dataset of real-world sample and song pairs.",
        "zenodo_id": 1418331,
        "dblp_key": "conf/ismir/GururaniL17",
        "keywords": [
            "Non-negative Matrix Factorization",
            "Dynamic Time Warping",
            "Automatic detection",
            "Sampling in music",
            "Pitch shifting",
            "Time stretching",
            "Random Forest classifier",
            "F-measure",
            "Publicly available dataset",
            "Geographical and temporal tracking"
        ],
        "content": "AUTOMATIC SAMPLE DETECTION IN POLYPHONIC MUSIC\nSiddharth Gururani, Alexander Lerch\nGeorgia Institute of Technology, Center for Music Technology\nfsiddgururani, alexander.lerch g@gatech.edu\nABSTRACT\nThe term ‘sampling’ refers to the usage of snippets or\nloops from existing songs or sample libraries in new songs,\nmashups, or other music productions. The ability to auto-\nmatically detect sampling in music is, for instance, beneﬁ-\ncial for studies tracking artist inﬂuences geographically and\ntemporally. We present a method based on Non-negative\nMatrix Factorization (NMF) and Dynamic Time Warping\n(DTW) for the automatic detection of a sample in a pool\nof songs. The method comprises of two processing steps:\nﬁrst, the DTW alignment path between NMF activations\nof a song and query sample is computed. Second, features\nare extracted from this path and used to train a Random\nForest classiﬁer to detect the presence of the sample. The\nmethod is able to identify samples that are pitch shifted\nand/or time stretched with approximately 63% F-measure.\nWe evaluate this method against a new publicly available\ndataset of real-world sample and song pairs.\n1. INTRODUCTION\nIn the context of music composition and production, sam-\npling stands for the concept of reusing pre-existing digital\nrecordings in new compositions in a way that it ﬁts the\nmusical context. In digital sampling, an artist records a seg-\nment of a song or sound that they wish to sample, possibly\nmodiﬁes it, and then reuses it (and possibly other samples)\nby incorporating it into a new composition [11]. Sampling\nof audio has become popular in mainstream pop, hip-hop,\nand rap music.\nA Sample Detection (SD) system automatically detects\nsamples from a pool of songs and thus enables musico-\nlogical studies of the inﬂuence of older artists over newer\ngeneration artists by observing sampling patterns over the\nyears and geographically. Another possible use case of a SD\nsystem could be to detect plagiarism or copyright infringe-\nment. Sampling is legally controversial and determining\nfair use is largely left to the judicial system. A system that\ngives an objective measure of the likelihood of a sample\nbeing present in an audio ﬁle could add weight to either\nparty’s argument in a lawsuit.\nc\rSiddharth Gururani, Alexander Lerch. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Siddharth Gururani, Alexander Lerch. “Automatic Sample\nDetection in Polyphonic Music”, 18th International Society for Music\nInformation Retrieval Conference, Suzhou, China, 2017.The algorithm discussed in this paper focuses on solving\nthe problem of detecting the presence of a given sample in\na set of songs as well as its time location in the song.\n2. RELATED WORK\nThe task of SD has been addressed in only few previous\npublications. There exist, however, several parallels that\nmay be drawn from other areas of research that are relevant\nto SD such as cover song detection, audio ﬁngerprinting,\nand remix recognition.\n2.1 Audio Fingerprinting\nAudio Fingerprinting (AFP) refers to the method of extract-\ning content-based signatures from audio [1, 5, 10]. It is\ncommonly used in content-based music identiﬁcation sys-\ntems such as Shazam.1Van Balen proposed the use of AFP\nfor sample detection [24,25], using a popular ﬁngerprinting\nalgorithm by Wang [27] in an implementation by Ellis [8].\nFingerprinting systems are robust against noise injection\nand, with appropriate modiﬁcations, pitch shifting and time\nstretching [31]. As long as the level difference between the\nsample and other sources is within the noise level expecta-\ntions, a modiﬁed system could be a good choice for sample\ndetection. Given that a sample might be mixed at a low\nlevel, it is questionable if this assumption really holds true\nfor the majority of cases.\n2.2 Cover Song Detection\nCover Song Detection (CSD) is the task of recognizing\nwhether a given reference track has a cover song in a set of\ntest tracks [2,9,19]. Covers may, for example, be transposed\nand deviate from the original song in terms of tempo and\nother properties. Dynamic Time Warping (DTW) [20] is\noften used to make the comparison tempo-invariant. The\ndifference from SD is that covers are renditions of a musical\npiece, while samples are snippets of audio which are usually\na part of the mix overlaid with multiple other instruments\nand sounds unrelated to the sample. The evaluation of SD\nsystems, however, is similar to that of CSD. Both have a\ntest/reference pair which is then categorized as a positive or\nnegative match with or without a conﬁdence measure.\n2.3 Remix Recognition\nWork done in remix recognition by Casey and Slaney [6]\ndraws inspiration from a method for web crawling called\n1https://www.shazam.com, last accessed: 4/26/2017264‘shingling’ which utilizes a stream of text position-based\nfeatures to detect if a document has already been crawled\nbefore. In their work, they compute audio features such\nas pitch-class proﬁles and Log-Frequency Cepstral Coef-\nﬁcients using 0.1 second frames. They concatenate these\nfeature vectors into 4 second ’shingles’ and model the distri-\nbution of pair-wise distances between shingles from remixes\nand shingles that are not from remixes. A nearest neighbor\nclassiﬁer is used to identify which distribution a query ’shin-\ngle’ belongs to. Such a system is not appropriate for SD\nsince this method relies on long-term similarity between\nthe query and the reference, however, a sample may be a\nshort, one-shot sample (triggered samples without looping).\nA working remix recognition system, however, would be\nhelpful in ways similar to SD in that it helps with tracing\ninﬂuences across artists. In addition, some remixing cases\nmight also involve instances of sampling.\n2.4 Sample Detection with Non-negative Matrix\nFactorization\nDittmar et al. listed sampling as one of three kinds of pla-\ngiarism in music [7]. They utilize Non-negative Matrix\nFactorization (NMF) to learn the spectral templates from\nthe sample and detect the presence of these templates in the\nsuspect audio. Correlating the activations from the sample\nand the song then gives the likelihood of plagiarism. While\nthe authors provide a general outline of a sample detection\nsystem, they neither offer a detailed algorithmic description\nnor a formal evaluation of their proposed system. In [28],\nWhitney uses NMF in a similar manner except that instead\nof factorizing entire spectrograms, NMF is applied to short\ntexture windows in the sample and the song. The detection\nis done using pairwise 2-dimensional cross-correlation of\nthe two activation matrices obtained. To account for pitch\nshifting and time stretching, the audio ﬁles are resampled\nusing factors computed by taking the ratio of the sample and\nsong BPM and multiple NMFs are performed. The issue\nwith this approach is that the time stretching and pitch shift\nfactor are not necessarily inverse when sampling. Nonethe-\nless, NMF appears to be a good choice for a SD system\nas ﬁxed templates allow to obtain activations for only the\ncommon components between the song and the sample.\nThe task of sample detection has been identiﬁed in music\ninformation retrieval literature, and various approaches have\nbeen proposed. However, it has not yet been well deﬁned\nin terms of evaluation methodology or metrics. Reasonably\nsized datasets are also non-existent or proprietary. There-\nfore, there is no formal evaluation that can be performed\nto compare different sample detection methods. This paper\naims to bridge this gap by providing a dataset and propose\na common evaluation framework.\n3. METHOD\nThe algorithm presented in this paper is inspired by the\nproposal of Dittmar et al. [7]. Since the task of sample\ndetection is similar to a source identiﬁcation problem where\nthe sample is one of the sources present in the mix, an NMF-\nPartially-Fixed \nNMF NMF \nCompute \nDistance Matrix Sample \nSong WoHo\nHs\nDTW alignment Feature \nExtraction \nClassification Figure 1 . High Level Block Diagram\nbased approach is ﬁtting due to its prevalence in audio\nsource separation tasks [18, 21]. The block diagram in\nFigure 1 shows the processing steps of the algorithm.\n3.1 Non-Negative Matrix Factorization\nNMF is a widely popular algorithm in unsupervised\nlearning with applications in recommendation systems\n[12, 15, 16] and signal processing tasks, speciﬁcally source\nseparation [13, 23, 26]. NMF factorizes a signal matrix\nV2RM\u0002Ninto a template matrix W2RM\u0002Kand an\nactivation matrix H2RK\u0002Nsuch that:\nV\u0019W\u0001H: (1)\nIfVis the magnitude spectrogram with Mfrequency bins\nandNblocks, Wcontains the Kspectral or harmonic\ncomponent templates in Vwhile Hcontains temporal in-\nformation about each corresponding spectral component in\nthe template matrix [22].\nIn a pre-processing step, both the original sample and the\npaired song are RMS-normalized, downmixed, and down-\nsampled to 22.05 kHz; then, their magnitude spectrogram\nis computed (block size: 4096, hop size: 1024 samples).\nUsing NMF, the sample spectrogram will be factorized into\nKtemplates Woand the activation matrix Ho,oindicating\n’original’. A sample, used in a song, may be thought of as\none source in a mixture of multiple sources in this song.\nThe factorization of the song will then be performed using\npartially ﬁxed NMF (PFNMF) [29, 30]. In this case, the\ntemplate matrix Wconsists of a ﬁxed, not updated, part\ncontaining the extracted templates Woand a randomly ini-\ntialized part WmwithLtemplates that is iteratively learned\nand represents what we refer to as the mixture templates.\nThe dimension of the complete template matrix is thus\nM\u0002(K+L). All activations are iteratively updated as\nwell.\n3.1.1 NMF Rank Selection\nThe rank Kof the sample spectrogram has to be chosen\nbased on how many spectral templates can approximate theProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 265Sample Present\nSample AbsentFigure 2 . Geometric mean of cross-correlation functions.\nBlack function shows sample occurrence, while blue does\nnot have any samples.\nspeciﬁc sample. Similarly, while computing the PFNMF,\nthe rank Lfor approximating the templates representing\nthe remaining mixture in the song has to be selected in a\nway that minimizes the impact on the ﬁxed sample template\nactivations in order to robustly detect the sample.\nDifferent songs will usually require different ranks for\naccurately approximating and factorizing their magnitude\nspectrograms with a low reconstruction error. In the current\nalgorithm, however, ﬁxed ranks are chosen: K= 10 and\nL= 20 . The rationale behind using ﬁxed low ranks is that\nfor this task a perfect reconstruction is not required and that\nthe following processing steps should be robust enough to\ndetect the sample regardless of whether the templates are\nable to combine linearly to an accurate reconstruction of the\noriginal spectrogram. If the sample is used in a song, the\nsame ﬁxed templates should produce a roughly similar set\nof activations independent of the mixture rank. Larger ﬁxed\nranks were tested but no gain in performance was observed\nwhile resulting in increased computational cost.\nStill, a future extension might be to analyze the audio\nseparately as a pre-processing step to obtain a ‘complexity’\nmeasure that could be used to adapt the ranks KandL\nbased on the signals to be modeled.\n3.2 Activation Function Processing\nIn the subsequent analysis, only the activations Hsare of\ninterest. These are the activations corresponding to the orig-\ninal templates Woafter PFNMF and indicate the presence\nof the sample in the song if they match the pattern of the\noriginal activations Ho. If the sample were neither pitch\nshifted nor time stretched, cross-correlation functions be-\ntween each corresponding activation in HoandHscould\nbe computed. Peaks in the aggregated (across the Kdi-\nmensions) cross-correlation function would then indicate\nthe presence of the sample. Figure 2 shows two example\nfunctions after using the geometric mean for aggregation: a\nsample occurs twice in the black song (as indicated by the\ntwo local maxima), while it is not present in the blue song.3.2.1 Activation Normalization\nProper normalization of the activation matrix is essential\nfor accurate sample detection, however, there is no “cor-\nrect” way to do this as the level (or even the presence) of\nthe sample in the paired song is unknown. The relative\nactivation levels between the Ktemplates of one matrix,\nhowever, should remain identical. Thus, each activation\nmatrix His normalized by the absolute maximum across\nall the Kactivations across time, preserving the relative\nactivation strengths for the spectral templates of the sample:\nHnormalized =H\nmax(H).\n3.2.2 Pitch Shifting & Time Stretching\nThe assumption that the sample is neither time stretched nor\npitch shifted is false for the majority of cases. In the dataset\nused for this study, for example, 57.5% of samples are pitch\nshifted. Pitch shifting is often required for the sample to\nmatch the tonality of the song, and time stretching is often\nrequired to adjust for tempo differences between the two.\nIn case of pitch shifting, the original templates Wowill\nno longer be valid templates since the frequency axis of\nthe spectral content will be scaled by the pitch shift factor.\nIn order to account for pitch shifting, we construct new\nspectral templates Wp\noby scaling the frequency axis of\nthe original templates with a set of hypothetical pitch shift\nfactors. Now, a partially ﬁxed NMF will be able to extract\nactivations corresponding to the pitch shifted templates and\nthese may be compared to the activations from the original\nsample. Ideally, we would perform factorizations for a\nset of hypothetical pitch shifts usually ranging from one\noctave lower to one octave higher in semi-tone or quarter-\ntone steps. For this paper, however, we allow the dataset\nto inform our pitch shift steps. The used pitch shift set in\nsemi-tone steps from the original sample templates Wois:\nP=fpjp2f\u0000 5;\u00004;\u00003;\u00002;\u00001;0;0:5;1;2;3;4;5gg:\nPartially-ﬁxed NMF is then performed individually for\neach pitch shift in P, i.e., 12times. The activations Hp\ns\ncorrespond to the pitch shifted templates Wp\no.\nWhen the sample is time stretched, the activations from\nthe song will be stretched or compressed accordingly; there-\nfore, cross-correlation cannot be used. In such a scenario,\nDTW can align the activations Howith the activations Hp\ns\nin the song for each pitch shift p. A distance matrix Dis\nconstructed using the pair-wise correlation distance between\ntheK-dimensional activations. The size of DisNo\u0002Ns,\nwhere Nois the number of frames in the original sample\nactivations and Nsis the number of frames in the song acti-\nvations. Correlation is used as the distance measure because\nit is scale independent. Preliminary tests showed that other\nfrequently used distance measures perform either similarly\nor worse. The resulting distance matrix shows how similar\na set of activations is to the extracted activations at each\ntime instant. The distance matrix Dand properties of the\nextracted path can be used to indicate the presence of a\nsample in a song. Figure 3 shows an example of a distance\nmatrix with a looped sample. The low cost parallel blue\npaths indicate the presence of the sample.266 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170.511.5Figure 3 . Distance matrix computed between activations\nin the case where a sample is looped\nThe problem is now a subsequence search for the sample\nactivations Howithin the series of activations correspond-\ning to the sample templates in the song, Hp\ns. A standard\nDTW implementation would initialize the cost matrix Cby\naccumulating the ﬁrst row and ﬁrst column of the distance\nmatrix D. Such a scheme can be applied when only one\nglobal optimal path that aligns the two entire time series is\nrequired. In the case of sample detection, a sample could\nbe present in multiple locations within the song; hence, the\ndetection of multiple alignment paths at multiple locations\ninDis required. Each of these paths should align the entire\nsample with segments of the song. Therefore, the initial-\nization of Cis modiﬁed to only accumulate the distances\nalong the dimension of the sample, which in our case is the\ncolumn. This subsequence DTW scheme [17] initializes\nthe cost matrix Cas follows:\nC(1; j) =D(1; j)\nC(i;1) =iX\nk=1D(k; i)(2)\nInitializing in this fashion and proceeding to compute the\ncost matrix allows us to obtain backtracking paths from\nevery index in the last row of D(as opposed to one path\nfrom the last element of D, which is the case for “standard”\nDTW). The last row of the cost matrix Ccorresponds to\nthe cost of aligning the sample backtracking from every\nframe of the song. The backtracking paths now satisfy the\nrequirement of aligning the entire sample with a section in\nthe song. To summarize, every alignment path obtained is\nthe path that aligns the sample activations Hobacktracking\nfrom every frame fin the song activations Hp\ns. Note that\nthe subsequence DTW is performed for each p2P.\n3.2.3 Pitch Candidate Selection\nFrom the set of pitch shifts, the most likely pitch shift is\ninferred before feature extraction. Of the 12cost matrices,\nthe one corresponding to the most likely pitch candidate\nwill be the one with the global minimum cost (minimum of\nthe last row of the cost matrix),i.e., the one with a minimum\ncost lower than the minimum cost of all other matrices. All\nsubsequent computations are based on the activation matrix\nof the selected candidate; the results for the 11remaining\npitch shifted templates are discarded.\nFigure 4 . DTW cost function; minima indicate the end of\nthe sample\n3.3 Feature Extraction\nThe last row of the cost matrix C, containing the alignment\npath costs normalized by the length of the path, will be\nreferred to as DTW cost function . Local minima in this\nDTW cost function indicate potential sample end points.\nUsing an absolute threshold on the DTW cost function\nto detect a sample is not meaningful because the absolute\ncost level depends on both sample and song characteristics.\nThe mixing ratio of different samples in different songs will\nbe different. Some samples might occur in a section with no\nother sources while others might be heavily overlaid with\nother sounds, leading to interference and varying strength\nin activations across different (song, sample) pairs. The\nDTW backtracking paths from all possible end points to\ntheir corresponding start location results in a set of unique\nstart locations in the song for each (song, sample) pair. We\nrefer to this mapping from every end point to a start point\nin the song as DTW path start function . Every unique start\nlocation is a candidate for a sample being present. Note\nthat each unique start location can have multiple paths/end\npoints. Figure 5 shows one example of this function. We\nchoose to derive features from the two functions we deﬁned\nabove because we expect that the properties of the DTW\nalignment paths vary depending on whether a sample is\npresent or not. The features extracted from this data are\nexplained in the following sections.\n3.3.1 Cost-based Features\nFrom the multiple path end points of each start location,\nthe following three features are extracted: (i) the minimum\nDTW cost across all end points, (ii) the average DTW cost\nacross all end points, and (iii) the standard deviation of the\nDTW cost across all end points.\n3.3.2 Path-based Features\nThe properties of the DTW alignment path should be mean-\ningful for detecting the presence of the sample. For exam-\nple, the tempo of the sample should stay roughly constant,\nmeaning that the slope of the path stays constant as well.\nThus, the idealized path would connect start point and end\npoint with a straight line. Note that when we refer to endProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 267Figure 5 . DTW path start function; Longer steps indicate\nsample; Long steps indicate that several DTW paths back-\ntracked to the same start point.\npoints here, we refer to the group of end points that map\nto one unique start location. Overall, the following fea-\ntures are extracted for every unique start location: (i) the\nabsolute length of the minimum cost path normalized by\nthe sample length, (ii) the slope of the minimum cost path,\n(iii) the average perpendicular deviation of the minimum\ncost path from the idealized path, normalized by the length\nof the path, (iv) the average slope across all end point paths,\n(v) the standard deviation of the slope across all end point\npaths, (vi) the average absolute length of all end point paths\nnormalized by the sample length, (vii) the standard devia-\ntion of the absolute length of all end point paths normalized\nby the sample length, (viii) the average perpendicular de-\nviation from the idealized path across all end point paths,\nnormalized by the length of the paths, (ix) the standard\ndeviation of the perpendicular deviation from the idealized\npath across all end point paths, normalized by the length of\nthe path, and (x) the number of end points mapping to this\nunique start location.\n3.4 Classiﬁcation\nEach unique start location is represented by a 13-\ndimensional feature vector, which is a data point that can be\nused as the input to a binary classiﬁer for detecting whether\na sample is present or not. A random forest classiﬁer with an\nensemble of 200 decision trees was chosen [3]. The number\nof features chosen for each decision split is 4 and has been\ndecided based on the convention of choosing round (pn)\nwhere nis the number of features, 13. The output is a\nprobability of the data point belonging to each class.\n4. EV ALUATION\n4.1 Dataset\nA dataset for Sample Detection was compiled using data\nfrom whosampled.com2which aggregates information\nabout songs that sample or cover other songs. The au-\ndio was downloaded using web services from streaming\nwebsites such as Youtube or Dailymotion.\n2www.whosampled.com, last accessed: 1/22/2017Eighty popular samples (according to the users of\nWhosampled) were selected from the catalog of songs\nby popular and frequently sampled artists such as James\nBrown, Stevie Wonder, Michael Jackson, and Queen. The\nresulting set has a balanced distribution among the genres\nPop, Rock, Funk and Hip-Hop.\nThe samples cover several variations of sampling such\nas: one-shot samples of musical snippets or voice samples,\nlooped drums, and looped melodies. The length of the\nlongest sample is 25 s, the shortest is half a second and the\naverage length of the samples is 4.5 s seconds. The total\nnumber of sampling instances is 876. The overall dataset\ncontains 80pairs of original song and sampling song.\nThe following annotations were added manually with\nthe software Sonic Visualizer [4]: (i) start and end time in\nseconds of the sample in the original song, (ii) start time in\nseconds of the sample in the sampling song, and (iii) pitch\nshift (in semi-tones) of the sample in the sampling song.\n57.5% of the samples are pitch shifted. Pitch shift was\nannotated by a human listener by ear. In cases where the\npitch shift was difﬁcult to ascertain, the sample and song\nsnippet was compared in a DAW and different pitch shifts\nwere tested until a match was found. These annotations\nplus additional meta-data including the song names, iden-\ntiﬁers, and URLs for obtaining the audio have been made\navailable publicly in an online repository.3The repository\nalso contains the MATLAB source code for the algorithm.\n4.2 Experiments\nFor each of the 80 samples in our dataset, there are 79\nsongs in which the sample does not occur. We randomly\npick 9 songs from these 79 songs. This, in combination\nwith the one song that includes the sample, results in a pool\nof 10 songs to be paired with each sample, resulting in an\noverall number of queries: 80\u000110 = 800 for the 80samples.\nPerforming experiments with all possible pairs in the entire\ndataset would be impractical without more compute power.\nThis overall set is split into a training set of 50 samples/500\nqueries and a test set of 30 samples/300 queries.\nIn order to use the ground truth data for training, one\nadditional step of interaction is necessary: all unique start\npoints have to be labeled as ‘0’ or ‘1’ based on whether\na sample is present. More speciﬁcally, the start points\nassociated with minimum DTW cost within a 1 s window\nof the ground truth annotation are labeled ‘1’ and the rest\nare labeled ‘0’. Multiple start points within the tolerance\nrange are merged so that the minimum cost path remains.\nFor the test set, any positive detection of the sample\nwithin a 1 s tolerance window of the ground truth anno-\ntation will be regarded as True Positive. Every positive\ndetection outside of this tolerance window and for queries\nnot containing the sample will be regarded as False Positive.\nWith respect to the metrics, the False Positive Rate, Preci-\nsion, Recall, and the F-measure are reported for the sample\nlocation detection. We refer to these as Micro -accuracy\nmeasures as they take into account the sample location and\nthe number of occurrences.\n3https://github.com/SiddGururani/sample detection268 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Precision Recall F-measure Fp-rate\nMicro 79.37% 34.60% 48.19% 0.04%\nMacro 83.33% 50.00% 62.50% 1.11%\nTable 1 . Results for song-level macro accuracy and sample\nlocation-level micro accuracy measures\nMacro -accuracy measures, on the other hand, report the\nsong-level sample detection results and indicate whether\na sample is present in a song or not regardless of position\nand number of occurrences. The same metrics are reported,\nbut the sample detection is evaluated per song rather than\nper sample instance. In summary, using both the Macro\nand Micro-level accuracy metrics we are able to report the\nperformance of the method in two usage scenarios: First,\ndetecting whether a sample is present in a song (Macro),\nand second, detecting where in a song and how often a\ngiven sample is present (Micro).\n5. RESULTS AND DISCUSSION\nTable 1 reports the test accuracy of the classiﬁer. The re-\nsults show that the presented method is somewhat effective\nat detecting the presence of sampling in a set of songs with\na low false-positive rate and a reasonably high precision.\nThe low recall of the method can be attributed to the highly\nimbalanced nature of the problem as depicted in the confu-\nsion matrix in Table 5 : the testing dataset has 289 instances\nof sampling against around 74,000 instances that are not\nlocations of sampling. The training set is similarly skewed\nin its distribution of positive and negative classes. We ob-\nserve an area under receiver operating characteristic curve\n(AUROC) of 72.54%, a result strongly impacted by the low\nfalse positive rate due to the imbalanced classes.\nA possible reason for the low recall is also that the\nmethod is not always accurate when it comes to picking\ncandidates. More speciﬁcally, it already misses approx-\nimately 7% of sampling instances during the candidate\nselection stage. This number is computed by classifying all\nunique start locations as sample instances and calculating\nthe number of false negatives. A closer investigation of\nsome training samples showed that sometimes the DTW\ncost function did not have a minimum at respective sam-\nple locations. In these cases, the distance matrix would\nnot contain clear alignment paths like the one shown in\nFigure 3 . The absence of clear alignment paths might stem\nfrom incorrect modeling of the ﬁxed sample templates in\nthe song NMF step or pitch shifts not considered in our\nalgorithm. The choice of the distance measure might also\nimpact the results: while the use of the correlation distance\nmakes sense because it is scale invariant, custom distance\nmeasures might outperform it in this particular use-case.\nIt is worth pointing out that most problematic cases that\nwe came across were hard to detect for humans as well.\nThese cases included sparse drum loops, very short sam-\nples, samples that were mixed at a very low level, and\nsamples with excessive use of audio effects applied to them.\nThe high precision of our method, especially in the macro-Micro Not Sample Sample\nNot Sample 74126 26\nSample 189 100\nMacro No Sample Has Sample\nNo Sample 267 3\nHas Sample 15 15\nTable 2 . Confusion Matrices for Micro & Macro Accuracy\naccuracy use case, enables utilizing this system as a pre-\nprocessing step to a manual detection of sampling instances,\ne.g., for studies that trace artist inﬂuences. Given a database\nof songs and a set of samples to look for in the database,\nthis method can be used to pre-label data as cases of sam-\npling with high conﬁdence on songs where samples are\ndetected, allowing the human operator to focus on the re-\nmaining database. In the context of plagiarism detection, a\nhigh precision enables such a system to be used with high\nconﬁdence in case of a positive detection of plagiarism.\nHowever, a low recall system “favors” the sampling artists\nso the involvement of human experts remains a requirement.\n6. CONCLUSION AND FUTURE WORK\nWe introduced a method to detect the presence of a sam-\nple in a set of songs robust against common sampling modi-\nﬁcations such as pitch shifting and time stretching. PFNMF\nis used to extract sample activations from the song, and\nDTW is used to align the set of activations obtained from\nthe song and the sample. We also present a new publicly\navailable dataset of real-world samples and songs contain-\ning ﬁne-grained annotations for exact time locations of\nsample occurrences within the song. The presented method\nis evaluated against this dataset and we obtain 79.37% pre-\ncision in detecting the exact location of the sample and\n83.33% precision in song-level detection of a given sample.\nFurther research is required in order to improve the us-\nability of this method. Since the algorithm works for many\ncases, a systematic way to improve it would be to more\nclosely investigate the problematic cases in order to design\nmodiﬁcations and algorithmic extensions to increase recall.\nAs this problem is inherently unbalanced, a possible\ndirection is to observe best practices for machine learn-\ning on imbalanced datasets. In addition to undersampling,\nother techniques such as algorithmic modiﬁcations and cost-\nsensitive learning that may be employed to solve imbal-\nanced classiﬁcation problems [14].\nInvestigating custom distance measures for the DTW\nis another possible avenue to explore. Applying a non-\nlinearity to the NMF activations may help in increasing the\nsparsity, possibly improving the differentiation between an\ninstance containing a sample and one that does not.\nSample detection is an intriguing and challenging, yet\nlargely untouched MIR task and it is our hope that the\ndataset and this paper will encourage future work on this\ntopic in the MIR community.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 2697. REFERENCES\n[1]Shumeet Baluja and Michele Covell. Audio ﬁngerprint-\ning: Combining computer vision & data stream process-\ning. In Proc. of the IEEE International Conference on\nAcoustics, Speech and Signal Processing , pages 213–\n216, Honolulu, HI, USA, 2007.\n[2]Thierry Bertin-Mahieux and Daniel P. W. Ellis. Large-\nscale cover song recognition using hashed chroma land-\nmarks. In Proc. of the IEEE Workshop on Applications\nof Signal Processing to Audio and Acoustics , pages\n117–120, New Paltz, NY , USA, 2011.\n[3]Leo Breiman. Random forests. Machine learning ,\n45(1):5–32, 2001.\n[4]Chris Cannam, Christian Landone, and Mark Sandler.\nSonic visualiser: An open source application for view-\ning, analysing, and annotating music audio ﬁles. In Proc.\nof the ACM International Conference on Multimedia ,\npages 1467–1468, Firenze, Italy, October 2010.\n[5]Pedro Cano, Eloi Batlle, Ton Kalker, and Jaap Haitsma.\nA review of audio ﬁngerprinting. Journal of VLSI Sig-\nnal Processing Systems for Signal, Image and Video\nTechnology , 41(3):271–284, 2005.\n[6]Michael Casey and Malcolm Slaney. Fast recognition of\nremixed music audio. In Proc. of the IEEE International\nConference on Acoustics, Speech and Signal Processing ,\npages 1425–1428, Honolulu, HI, USA, 2007.\n[7]Christian Dittmar, Kay F Hildebrand, Daniel Gaertner,\nManuel Winges, Florian M ¨uller, and Patrick Aichroth.\nAudio forensics meets music information retrieval —\nA toolbox for inspection of music plagiarism. In Proc.\nof the European Signal Processing Conference , pages\n1249–1253, Bucharest, Romania, 2012.\n[8]Daniel P. W. Ellis. Robust landmark-based audio\nﬁngerprinting. http://labrosa.ee.columbia.\nedu/matlab/fingerprint/ . Online; accessed:\n28-April-2017.\n[9]Daniel P. W. Ellis and Graham E. Poliner. Identifying\n‘cover songs’ with chroma features and dynamic pro-\ngramming beat tracking. In Proc. of the IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing , pages 1429–1432, Honolulu, HI, USA, 2007.\n[10] Jaap Haitsma and Ton Kalker. A highly robust audio ﬁn-\ngerprinting system. In Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 107–115, Paris, France, 2002.\n[11] Mark Katz. Capturing Sound: How Technology has\nChanged Music . University of California Press, Berke-\nley and Los Angeles, 2004.\n[12] Yehuda Koren, Robert Bell, and Chris V olinsky. Ma-\ntrix factorization techniques for recommender systems.\nComputer , 42(8):30–37, 2009.[13] Daniel D Lee and H Sebastian Seung. Learning the parts\nof sbjects by non-negative matrix factorization. Nature ,\n401(6755):788–791, 1999.\n[14] Victoria L ´opez, Alberto Fern ´andez, Salvador Garc ´ıa,\nVasile Palade, and Francisco Herrera. An insight into\nclassiﬁcation with imbalanced data: Empirical results\nand current trends on using data intrinsic characteristics.\nInformation Sciences , 250:113–141, 2013.\n[15] Xin Luo, Mengchu Zhou, Yunni Xia, and Qingsheng\nZhu. An efﬁcient non-negative matrix-factorization-\nbased approach to collaborative ﬁltering for recom-\nmender systems. IEEE Transactions on Industrial Infor-\nmatics , 10(2):1273–1284, 2014.\n[16] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin\nKing. Sorec: Social recommendation using probabilistic\nmatrix factorization. In Proc. of the ACM Conference\non Information and Knowledge Management , pages\n931–940, Napa Valley, CA, USA, 2008.\n[17] Meinard M ¨uller. Dynamic time warping. In Information\nRetrieval for Music and Motion , pages 69–84. Springer\nBerlin Heidelberg, Berlin, Heidelberg, 2007.\n[18] Alexey Ozerov and C ´edric F ´evotte. Multichannel non-\nnegative matrix factorization in convolutive mixtures\nfor audio source separation. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 18(3):550–563,\n2010.\n[19] Suman Ravuri and Daniel P.W. Ellis. Cover song de-\ntection: From high scores to general classiﬁcation. In\nProc. of the IEEE International Conference on Acous-\ntics Speech and Signal Processing , pages 65–68, Dallas,\nTX, USA, 2010.\n[20] Hiroaki Sakoe and Seibi Chiba. Dynamic programming\nalgorithm optimization for spoken word recognition.\nIEEE Transactions on Acoustics, Speech, and Signal\nProcessing , 26(1):43–49, 1978.\n[21] Mikkel N Schmidt and Morten Mørup. Nonnegative\nmatrix factor 2-d deconvolution for blind single chan-\nnel source separation. In Proc. of the International\nConference on Independent Component Analysis and\nBlind Source Separation , pages 700–707, Chareston,\nSC, USA, 2006. Springer.\n[22] Paris Smaragdis and Judith C Brown. Non-negative ma-\ntrix factorization for polyphonic music transcription.\nInProc. of the IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics , pages 177–180,\nNew Paltz, NY , USA, 2003.\n[23] Paris Smaragdis, Cedric Fevotte, Gautham J Mysore,\nNasser Mohammadiha, and Matthew Hoffman. Static\nand dynamic source separation using nonnegative fac-\ntorizations: A uniﬁed view. IEEE Signal Processing\nMagazine , 31(3):66–75, 2014.270 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[24] Jan Van Balen. Automatic recognition of samples in mu-\nsical audio. Master’s thesis, Universitat Pompeu Fabra,\n2011.\n[25] Jan van Balen, Mart ´ın Haro, and Joan Serr `a. Automatic\nidentiﬁcation of samples in hip hop music. In Proc. of\nthe 9th International Symposium on Computer Music\nModeling and Retrieval (CMMR) , pages 544–551, Lon-\ndon, UK, 2012.\n[26] Tuomas Virtanen. Monaural sound source separation by\nnonnegative matrix factorization with temporal continu-\nity and sparseness criteria. IEEE transactions on Audio,\nSpeech, and Language Processing , 15(3):1066–1074,\n2007.\n[27] Avery Wang. An industrial-strength audio search al-\ngorithm. In Proc. of the International Conference on\nMusic Information Retrieval , pages 7–13, Baltimore,\nMD, USA, 2003.\n[28] Jordan L Whitney. Automatic Recognition of Samples\nin Hip-Hop Music Through Non-Negative Matrix Fac-\ntorization . PhD thesis, University of Miami, 2013.\n[29] Chih-Wei Wu and Alexander Lerch. Drum transcription\nusing partially ﬁxed non-negative matrix factorization.\nInProc. of the European Signal Processing Conference ,\npages 1281–1285, Nice, France, 2015. EURASIP.\n[30] Chih-Wei Wu and Alexander Lerch. Drum transcription\nusing partially ﬁxed non-negative matrix factorization\nwith template adaptation. In Proc. of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , pages 257–263, Malaga, Spain, 2015.\n[31] Bilei Zhu, Wei Li, Zhurong Wang, and Xiangyang Xue.\nA novel audio ﬁngerprinting method robust to time scale\nmodiﬁcation and pitch shifting. In Proc. of the ACM\nInternational Conference on Multimedia , pages 987–\n990, Firenze, Italy, 2010. ACM.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 271"
    },
    {
        "title": "Cover Song Identification with Metric Learning Using Distance as a Feature.",
        "author": [
            "Hoon Heo",
            "Hyunwoo J. Kim",
            "Wan Soo Kim",
            "Kyogu Lee"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416556",
        "url": "https://doi.org/10.5281/zenodo.1416556",
        "ee": "https://zenodo.org/records/1416556/files/HeoKKL17.pdf",
        "abstract": "Most of cover song identification algorithms are based on the pairwise (dis)similarity between two songs which are represented by harmonic features such as chroma, and therefore the choice of a distance measure and a feature has a significant impact on performance. Furthermore, since the similarity measure is query-dependent, it cannot rep- resent an absolute distance measure. In this paper, we present a novel approach to tackle the cover song identi- fication problem from a new perspective. We first con- struct a set of core songs, and represent each song in a high-dimensional space where each dimension indicates the pairwise distance between the given song and the other in the pre-defined core set. There are several advantages to this. First, using a number of reference songs in the core set, we make the most of relative distances to many other songs. Second, as all songs are transformed into the same high-dimensional space, kernel methods and metric learning are exploited for distance computation. Third, our approach does not depend on the computation method for the pairwise distance, and thus can use any existing algo- rithms. Experimental results confirm that the proposed ap- proach achieved a large performance gain compared to the state-of-the-art methods.",
        "zenodo_id": 1416556,
        "dblp_key": "conf/ismir/HeoKKL17",
        "keywords": [
            "cover song identification",
            "pairwise (dis)similarity",
            "harmonic features",
            "distance measure",
            "feature representation",
            "query-dependent similarity",
            "novel approach",
            "core songs",
            "high-dimensional space",
            "kernel methods"
        ],
        "content": "COVER SONG IDENTIFICATION WITH METRIC LEARNING USING\nDISTANCE AS A FEATURE\nHoon Heo1Hyunwoo J. Kim2Wan Soo Kim1Kyogu Lee1\n1Music and Audio Research Group, Seoul National University, Republic of Korea\n2Department of Computer Sciences, University of Wisconsin–Madison, USA\ncubist04@snu.ac.kr, hwkim@cs.wisc.edu, wansookim@snu.ac.kr, kglee@snu.ac.kr\nABSTRACT\nMost of cover song identiﬁcation algorithms are based\non the pairwise (dis)similarity between two songs which\nare represented by harmonic features such as chroma, and\ntherefore the choice of a distance measure and a feature has\na signiﬁcant impact on performance. Furthermore, since\nthe similarity measure is query-dependent, it cannot rep-\nresent an absolute distance measure. In this paper, we\npresent a novel approach to tackle the cover song identi-\nﬁcation problem from a new perspective. We ﬁrst con-\nstruct a set of core songs, and represent each song in a\nhigh-dimensional space where each dimension indicates\nthe pairwise distance between the given song and the other\nin the pre-deﬁned core set. There are several advantages\nto this. First, using a number of reference songs in the\ncore set, we make the most of relative distances to many\nother songs. Second, as all songs are transformed into the\nsame high-dimensional space, kernel methods and metric\nlearning are exploited for distance computation. Third, our\napproach does not depend on the computation method for\nthe pairwise distance, and thus can use any existing algo-\nrithms. Experimental results conﬁrm that the proposed ap-\nproach achieved a large performance gain compared to the\nstate-of-the-art methods.\n1. INTRODUCTION\nA cover song, or simply cover, is a new version of existing\nmusic that is recorded or arranged by another musician. A\ncover reuses the melody and lyrics of the original song,\nbut it is performed with new singers and instruments. The\nother musical factors such as key, rhythm, and genre can\nbe reinterpreted by the new artist. Since the copyright of\ncomposition and lyrics of the cover still belongs to the au-\nthor of the original song, releasing a cover song without\npermission of the original author may cause a legal con-\nﬂict. Another case is music sampling, which is the act of\nprocess that reuses a snippet of existing sound recordings.\nThe sampling is widely considered to be a technique for\nc\rHoon Heo, Hyunwoo J. Kim, Wan Soo Kim, Kyogu Lee.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Hoon Heo, Hyunwoo J. Kim, Wan\nSoo Kim, Kyogu Lee. “Cover Song Identiﬁcation with Metric Learning\nUsing Distance as a Feature”, 18th International Society for Music Infor-\nmation Retrieval Conference, Suzhou, China, 2017.creating music today, but licensing that the original cre-\nator authorizes its reuse is a legal requirement. Cover song\nidentiﬁcation is a task that aims to measure the similarity\nbetween two songs. It can be used to prevent the infringe-\nment of copyright, and also to be an objective reference in\ncase of conﬂict.\nFor a decade, many approaches for cover song identi-\nﬁcation have been proposed. Humans generally recognize\nthe cover through the melodic or lyric similarity, but sep-\naration of the predominant melody from a mixed music\nsignal is still not at a reliable level, and extraction of the\nlyrics can be attempted only if it is clearly separated. For\nthis reason, most of the existing algorithms use the har-\nmonic progression represented by an acoustic feature such\nas chroma [6], and measure the similarity in the features to\ndetermine the distance between two songs.\nCover song identiﬁcation generally consists of two main\nstages: feature extraction and distance calculation. In\nmost related works, chroma or harmonic pitch class pro-\nﬁle (HPCP) are usually chosen, as well as its variants such\nas CENS [9], CRP [8], and MPLPLC [2]. It is reported that\nthe abstraction of the chroma-like feature to focus on the\nchord progression rather than instantaneous note changes\nimproves the identiﬁcation performance [2, 15]. In early\ndays, the feature was synchronized with the beat to take\ninto account the covers with different tempo [4]. However,\nsince the error in beat tracking degrades the performance\nand the tempo change is usually not extreme, the hop size\nwith a ﬁxed length is recently preferred [14]. Besides,\ntwo-dimensional Fourier transform magnitude (2DFTM)\nof the chroma feature is applicable for large-scale cover\nsong identiﬁcation [1]. The 2DFTM is key-invariant and\nthus does not require any preprocessing for key transposi-\ntion. Also, regardless of the duration of the song, its ﬁxed\nsize has the advantage of keeping the locality.\nIn respect to the distance calculation, an early approach\nﬁnds the best-correlated point using cross-correlation of\nthe beat-synchronous chroma [4]. The next popular ap-\nproach is based on dynamic time warping (DTW), which\ncan be sensitive to tempo changes even when the hop size\nis ﬁxed [14]. This approach uses the overall distance af-\nter aligning over the whole region of the two given songs.\nOn the other hand, a more recent approach called similar-\nity matrix proﬁle (SiMPle) yields a high similarity when\nmany local similar regions are found [15].\nThe conventional approaches described above calculate628ℂq4\nq5\nq6\nq7q1\nq2\nq3\nq1q2q3q4q5q7q6 q1\nq2\nq3q4\nq5\nq6\nq7\n(a) (b) (c)Figure 1 . (a) The original distance between a query q1\nand the other songs. (b) The distance between each query\nand the core set C. (c) New representation of songs in the\njCj-space.\nthe distance between a query and the songs to be com-\npared, and determine that the song with the nearest dis-\ntance is highly likely to be a cover. Since this process is\nseparate from each query, the result from “another version\nof the same cover” cannot be taken into account. If it is\npossible, songs with different lengths can be represented\nin the same space. Furthermore, if similar/dissimilar song\npairs are known, the metric to measure the song distance\ncan be optimized, rather than using the Euclidean distance.\nInstead of taking the distance matrix directly to rank the\nsimilarity, we ﬁrst perform a nonlinear transformation us-\ning kernel principal component analysis (KPCA) to rear-\nrange each song in the high-dimensional space. Next, the\ndistance metric is learned from song pairs in the new rep-\nresentation and their labels. We select “core songs” with\ndiverse musical properties and use them for both embed-\nding and training. In summary, our approach assumes that\nthe distance between the core set and each song can be a\ndiscriminating feature to easily group the same covers. The\nconceptual illustration of this new representation is shown\nin Figure 1.\nThe goal of this paper is to examine whether the dis-\ntance metric learning can be effective to retrieve the simi-\nlarity between songs. Also, this paper aims to achieve the\nbest performance in cover song identiﬁcation by applying\nthe metric learning to the distance matrix generated by ex-\nisting algorithms. Currently, MIREX hosts an annual task\nfor cover song identiﬁcation, but the dataset is not publicly\navailable. In the later section, we report a performance\ncomparison using our own dataset with the same speciﬁca-\ntion as that of the MIREX.\nThe rest of this paper is organized as follows. Sec-\ntion 2 deﬁnes some important terms throughout this pa-\nper, and summarizes three popular algorithms for measur-\ning the distance between songs. In section 3, we describe\nthe technical method for better representation of songs and\nmetric learning. After that, the experimental setup and re-\nsults are presented in section 4. Finally, the conclusions of\nthis paper are drawn in section 5.\n2. DISTANCE MATRIX\nThe distance matrix is deﬁned by a two-dimensional ma-\ntrix that contains the pairwise distances for all possible\nEvaluation\nset 𝔼\nSong -wise distance\nKernel PCA\nMetric learning\nSimilarity rank in 𝔼for each query in ℚCore set ℂ Query set ℚFigure 2 . Block diagram of the proposed method.\ncombinations of two songs. The range of distance may\nvary depending on the algorithm, but it should be low be-\ntween songs belonging to the same cover group, and should\nbe high if they are not associated.\nWe deﬁne three sets of songs as follows:\n\u000fQuery set ( Q): A set of songs to be a query for iden-\ntiﬁcation. Each cover group consists of the same\nnumber of versions.\n\u000fEvaluation set ( E): A set for performance evaluation\nwhich includes the query set Q. The remainders are\n“confusing songs” that are not associated with any\ncover groups.\n\u000fCore set ( C): An additional set of songs for embed-\nding and training in the proposed method. It is good\nto select songs in the core set with diverse musical\nstyles (i.e. genre, tempo, instruments).\nAmong these sets, Q\u001aEandE\\C=?should be\nsatisﬁed.\nThe distance matrix is a square matrix calculated from\nall the songs in the three sets. We employed three al-\ngorithms for measuring the song-wise distance: dynamic\ntime warping (DTW), Smith–Waterman algorithm, and\nsimilarity matrix proﬁle (SiMPle). In the following sub-\nsections, we give a brief overview of each algorithm to\nconstruct the distance matrix.\n2.1 Dynamic Time Warping\nDTW performs dynamic programming to retrieve the op-\ntimal path that minimizes the warping cost. Given a se-\nquenceAof lengthnand a sequence Bof lengthm, it\nconstructs an n-by-mmatrix that contains the Euclidean\ndistance\u000ei;jbetween both sequences at two time instances\niandj. The cumulative distance \ri;jis the sum of the\ndistance in the current point and the minimum cumulative\ndistance from the three adjacent points,\n\ri;j=\u000ei;j+ min (\ri\u00001;j\u00001;\ri\u00001;j;\ri;j\u00001):(1)\nThe overall distance between two sequences A and B is\ndetermined by the cumulative distance at the end of theProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 629path,\ndA;B=\rn;m: (2)\nTo prevent unrealistic warping and reduce the number\nof paths to consider, DTW can be implemented with global\nand local constraints. The two popular global constraints\nare Sakoe–Chiba band [11] and Itakura parallelogram [5].\nOn the other hand, the local constraints allows deviations\nof the double or half the original tempo by using warpings\n(i\u00001;i\u00001),(i\u00002;j\u00001), and (i\u00001;j\u00002)[10].\n2.2 Smith–Waterman Algorithm\nSimilar to DTW, Smith–Waterman algorithm performs dy-\nnamic programming to ﬁnd the optimal path that max-\nimizes the similarity score between two sequences [16].\nThe main difference to the classic DTW is that the optimal\npath is produced locally. That is, it is not necessary that\nthe path with the maximum similarity covers the whole se-\nquence. Given a sequence Aof lengthnand a sequence B\nof lengthm, it constructs an (n+ 1) -by-(m+ 1) scoring\nmatrixH. The ﬁrst row and column are initialized with 0.\nThe recursion formula to ﬁll the rest of the scoring matrix\nis,\nHi;j= max8\n>>><\n>>>:Hi\u00001;j\u00001+s(ai;bj);\nmaxk\u00151fHi\u0000k;j\u0000Wkg;\nmaxl\u00151fHi;j\u0000l\u0000Wlg;\n0(3)\nwheres(ai;bj)is the similarity score between ith element\nofAandjth element of B, andWnis the penalty of a\ngap with length n. The overall similarity of the Smith–\nWaterman algorithm is deﬁned as the maximum value on\nthe scoring matrix.\n2.3 Similarity Matrix Proﬁle\nSimilarity matrix proﬁle (SiMPle) efﬁciently evaluates\nsimilarities between songs based on subsequence similar-\nity joins in the features [15]. For a time-frequency repre-\nsentationAof lengthmandBof lengthn, SiMPle iden-\ntiﬁes the nearest neighbor of each continuous subsets in A\nfrom all continuous subsets in B. Euclidean distance be-\ntween the subset of Awith time index iand the subset of B\nwith time index j,di;j, is calculated using MASS (Mueen’s\nAlgorithm for Similarity Search), the fastest known algo-\nrithm for distance vector computation [7].\ndi;j=MASS (A[i];B[j]) (4)\nSiMPlePiis obtained by choosing the minimum value in\nthe distance between a subset of Aand each subset of B.\nPi= min(di;1;di;2;\u0001\u0001\u0001;di;n) (5)\nThe overall distance between two sequences AandBis\ndeﬁned as the median value of SiMPle [15].\ndA;B=median (Pi) (6)\nNote that SiMPle is not a symmetric distance measure, i.e.,\ndB;A6=dA;B.3. DISTANCE METRIC LEARNING\nDistance metric learning has been studied in machine\nlearning literature. Classical metric learning algorithms are\nmotivated by Mahalanobis distance given as\nd(x1;x2) =q\n(x1\u0000x2)T\u0006\u00001(x1\u0000x2); (7)\nwhere \u0006is the covariance matrix of X. The main intu-\nition behind Mahalanobis distance is that it calculates the\nEuclidean distance in a linearly transformed space by R,\nwhereRTR=S\u00001. Mahalanobis distance is a conve-\nnient metric since it is scale-invariant, and it takes the cor-\nrelations of data set into account. The linear transform R\nmakes the data have the isotropic covariance as the same\nas the covariance of multivariate normal distribution. The\ngoal of metric learning algorithms is to learn A, which cor-\nresponds to the precision matrix ( \u0006\u00001) based on a variety\nof criterion.\nd(x1;x2) =q\n(x1\u0000x2)TA(x1\u0000x2); (8)\nwhereAis a symmetric positive semideﬁnite matrix A\u0017\n0,A=AT. Training may require additional labels such as\nclasses and similar/dissimilar pairs depending on the ob-\njective of the frameworks.\nThe main difﬁculty to apply the classical metric learn-\ning algorithms to cover song identiﬁcation problems is that\nthe songs should be represented in a vector space. One\nsimple approach is to extract a set of ﬁxed length features\nfrom songs, e.g., mean MFCCs, mean Chroma, and beats\nper minute (BPM). But these features do not capture the\ntemporal information within a song. So, a variety of time\nseries analysis methods has been shown to be more effec-\ntive such as dynamic time warping (DTW).\nCan we embed songs in a vector space preserving the\ntemporal information? If this is possible, then distance\nmetric learning algorithms are able to ﬁnd a better distance\nbetween songs with both the temporal information and ad-\nditional labels (similar/dissimilar pairs or classes). One op-\ntion is kernel PCA. Fortunately, distance metric learning\ncan be extended in the context of kernel methods as well.\nThe kernel methods do not require the original data to be in\na vector space. We can get a gram matrix (or inner product\nmatrix) by pairwise dissimilarity measures. For embed-\nding, other embedding algorithms can be used for instance\nmultidimensional scaling (MDS), ISOMAP, locally linear\nembedding (LLE) and so on. We discuss our framework\nto calculate the gram matrix and embed songs in a vector\nspace shortly.\n3.1 Embedding of songs\nAs discussed above, we start from a pairwise dissimilarity\nmeasures. We calculate the distance matrix as described\nin Section 2. The gram matrix in the conventional kernel\nmethods should be symmetric positive-semideﬁnite ma-\ntrix. If the matrix is given as not symmetric (e.g. SiMPle),\nit needs to be symmetrized by d0\ni;j=1\n2(di;j+dj;i), where\ndi;jis deﬁned in Eqn (6).630 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017After symmetrization of the distance matrix, we per-\nform a kernel PCA. PCA seeks for eigenvectors of the co-\nvariance matrix of the data given as\nC=1\nNNX\nixi;xT\ni: (9)\nSimilarly, kernel PCA seeks for eigen functions of the co-\nvariance function. In other words, Given a nonlinear func-\ntion\b(\u0001)to map data to feature space, the covariance ma-\ntrix is calucated by\n\u0016C=1\nNNX\ni\b(xi)\b(xi)T; (10)\nwhere \b(x)is centered, i.e.,PN\ni\b(xi) = 0 . Thanks\nto the kernel trick, without performing the map \b, ker-\nnel methods can be computed by kernel functions Kij=\nk(xi;xj) =h\b(xi);\b(xj)i. In this paper, we used the Ra-\ndial basis function (Gaussian kernel). The kernel function\nis given by\nk(xi;xj) = exp\u0012\n\u0000kxi\u0000xjk2\n2\u001b2\u0013\n= exp \n\u0000(d0\nij)2\n2\u001b2!\n(11)\nwhered0\nijis the symmetrized dissimilarity measure (dis-\ntance) and\u001bis a tuning parameter. So only with the\npairwise dissimilarity measure, the gram matrix for ker-\nnel PCA is obtained. The remaining procedure is similar to\nclassical PCA. For more details, we refer the reader to [12].\nLetz1;\u0001\u0001\u0001;zNbe the new representation of songs from\nKPCA described above. In our experiments, the number\nof basis functions and the bandwidth \u001bin Eqn (11) were\nempirically selected.\nRemarks. When KPCA embeds songs in a vector space\nbased on dissimilarity measured by SiMPle, we found that\nin the vector representations of some songs may have ex-\ntremely large norms. So regardless of the metric learned by\nAin Eqn (8), these songs tend to have large distance from\nmost of other songs. In other words, these songs cannot be\ndetected as a cover song. To prevent this problem, we nor-\nmalized the vector representation of songs z1;\u0001\u0001\u0001;zNby\ntheir`2norms. All songs now are on the unit sphere and\nthe problem can be alleviated. The empirical performance\ngain is provided in Section 4.3. The normalized vector rep-\nresentation will be used for metric learning.\n3.2 Metric Learning\nWe adopt the Information-Theoretic Metric Learning\n(ITML) [3] except the regularization to make Aclose to\nthe priorA0, which is selected by users. Let SandDbe\na similar set and a dissimilar set, respectively. Then opti-mization program is given as\nmin\nAX\n(i;j)2Smax(0;Tr(AZijZT\nij)\u0000u)\n+X\n(i;j)2Dmax(0;l\u0000Tr(AZijZT\nij)); (12)\ns.t.A\u00170andAT=A;\nwhereZij=zi\u0000zjandTr(\u0001)is the trace. The input\nzifor the metric learning in Eqn (12) is the new (normal-\nized) representation of ith song obtained by KPCA. The\nobjective of this metric learning is to seek for an Amatrix,\nwhich make the distance of dissimilar pairs larger than a\nthresholdl(and the distance of similar pairs smaller than\na thresholdu). A similar pair consists of an original song\nand its cover song, or it can be two cover songs from an\noriginal song. The dissimilar pairs in our experiments are\nall possible pairs of songs except the similar pairs.\nThe way we label the relationship between songs natu-\nrally yields highly skewed labels. For example, if two out\nof ten songs are the only covers, then we have one similar\npair against\u000010\n2\u0001\n\u00001 = 44 dissimilar pairs. Interestingly,\nit turns out that the skewness of labels does not hurt the\nperformance of our framework. Rather, as the number of\ndissimilar pairs increases, the performance increases. Our\nexperiment evidences this phenomenon, see Section 4.3.\nThe formulation in Eqn (12) is optimized by projected\nstochastic subgradient descent as in Alg. 1. Since the ob-\njective function is a nonsmooth and convex function, we\nused the subgradient descent function. Also for the sym-\nmetric positive semideﬁnite constraint, the projection is\nadded in line 12. The step size \u000bcan be updated by any\nreasonable method.\nAlgorithm 1 Projected SSGD for metric learning.\n1:fork=1:maxiter do\n2: DATA0= randperm(DATA)\n3: for(i;j)= DATA0do\n4:p= 0\n5: if(i;j)2Sthen\n6: ifmax(0;Tr(AZijZT\nij)\u0000u)>0then\n7: p=ZijZT\nij\n8: else\n9: ifmax(0;l\u0000Tr(AZijZT\nij))>0then\n10: p=\u0000ZijZT\nij\n11:A=A\u0000\u000bp\n12:A=\u0019psd(A)\n13: update\u000b\n4. EV ALUATION\n4.1 Dataset and Metrics\nWe used two separate datasets to evaluation and train our\nmethod. The speciﬁcation of our evaluation dataset resem-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 631MNIT10\n5.16.56.37.47.57.8\n7.37.47.9\n0246810\nOriginal\nDMKPCA KPCA\n+MLDTW\nSmith-Waterman\nSiMPleMAP\n0.530.670.660.770.770.81\n0.750.760.81\n00.20.40.60.81\nOriginal\nDMKPCA KPCA\n+MLDTW\nSmith-Waterman\nSiMPleMR1\n15.8\n4.35.58.16.7\n4.726.6\n10.715.1\n05101520253035\nOriginal\nDMKPCA KPCA\n+MLDTW\nSmith-Waterman\nSiMPleFigure 3 . Improved performance by each step in the proposed method: the original distance matrix, kernel PCA, and\nmetric learning with kernel PCA.\nbles as in the MIREX cover song identiﬁcation task1. The\nevaluation set Econsists of 330 cover songs, which make\nthe query set Q, and 670 non-covers. There are 30 differ-\nent kinds of cover songs and each has 11 cover versions.\nThe training dataset consists of 254 covers and each cover\nhas two to ﬁve different versions, and have 1,175 songs in\ntotal. It was used as the core set Cin the experiments. Both\ndatasets are disjoint, and contain various genres of Korean\npops released from 1980 to 2016.\nWe employed four conventional metrics that have been\nused in the MIREX: total number of covers identiﬁed in top\n10, mean number of covers identiﬁed in top 10 (MNIT10),\nmean average precision (MAP), and mean rank of the ﬁrst\ncorrectly identiﬁed cover (MR1). In the experimental re-\nsults, we skipped the ﬁrst one because it is exactly the same\nas the second metric multiplied by jQj.\n4.2 Experiments\nSince selection of features and calculation of pairwise song\ndistance are not our interest, the chroma energy normalized\nstatistics (CENS) [9] was ﬁxed as the feature vector and\nextracted for every half a second in all the following ex-\nperiments. Also, before calculating the distance between\ntwo songs, we transposed one using the optimal transpose\nindex (OTI) [13] so that both songs have the same key.\nIn the ﬁrst experiment, we examined the effect of two\nproposed steps on identiﬁcation performance: new rep-\nresentation transformed by the kernel PCA, and the met-\nric learning using similar/dissimilar pairs in the core set.\n135 basis functions were empirically selected, and 2435\nsimilar pairs (for covers) and 687k dissimilar pairs (for\nnon-covers) were used as training data for metric learning.\nThis experiment allows reporting the maximum perfor-\nmance we could achieve and how each part of the proposed\nmethod contributes to the performance improvement.\nThe second experiment aims to verify that the metric\nlearning converges to a higher performance as more train-\ning data are used. We tested different numbers of the train-\ning data, which are song pairs in the core set. Songs are\nrandomly chosen with the given number of pairs in each\nclass. Since we have much less similar pairs than dissimi-\n1http://www.music-ir.org/mirex/wiki/2016:\nAudio_Cover_Song_Identificationlar pairs, the training will be imbalanced when all possible\nsimilar pairs are used. In this experiment, we ﬁxed the\noriginal distance measure by the SiMPle algorithm.\n4.3 Results and Discussions\nThe ﬁrst experimental result is shown in Figure 3. When\ncomparing the original performance of the existing algo-\nrithms, Smith–Waterman algorithm achieved 26% higher\nperformance than classic DTW. This is almost the same re-\nsult as reported in a previous work [15]. The SiMPle algo-\nrithm, which we consider to be the state-of-the-art method,\noriginally scored a slightly lower performance than Smith–\nWaterman algorithm in our experiment. However, the pro-\nposed method improved its original performance by 25%\n(in MNIT10), which was the largest improvement. Algo-\nrithms based on dynamic programming (DP) seem to have\nlimitations in potential performance gain. One possible\nreason is that the differences in distance between similar\nand dissimilar pairs are not so discriminated; while the\nSiMPle mainly depends on local similarity joins with a\nﬁxed length of 10 seconds, DP-based algorithms may take\nmuch longer sequences into account. Meanwhile, MR1\nwas increased by the metric learning. This will be dis-\ncussed in detail in the next paragraph.\nFigure 4 shows the learning curve of the metric learn-\ning with different number of pairs. A hundred pairs for\neach class were not sufﬁcient to converge. As more pairs\nwere used for training, both MNIT10 and MAP converged\nto higher performance. This result was also obtained when\nmore but imbalanced training data was used. Interestingly,\nthe trend of MR1 increased after a certain number of it-\nerations. This is caused by that the metric learning con-\ncentrates on the performance for a large majority of query\nsongs, while it fails for very few queries. To support this,\nwe ﬁrst calculated the median instead of the arithmetic\nmean rank, and noticed that the correct cover had the high-\nest similarity in most queries (i.e. median = 1) for every\nnumber of pairs and iteration. Nevertheless, since it is not\nsuitable to show that the performance is getting improved\nwith more iterations, the 90th percentile of rank of the ﬁrst\ncorrectly identiﬁed cover (P 90R1) is shown instead in the\nﬁgure.\nIn summary, our experiments conﬁrm that the use632 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017102030405060\nIteration0246810MNIT10\n102030405060\nIteration00.20.40.60.81MAP\n102030405060\nIteration0102030405060MR1\n(100,100) (300,300) (2435,2435) (2435,100k) (2435,687k)102030405060\nIteration0102030405060P90R1Figure 4 . Learning curve of the metric learning with dif-\nferent number of pairs (similar, dissimilar). The black dot-\nted line indicates each metric resulted from the original dis-\ntance matrix.\nof KPCA and metric learning on the SiMPle algorithm\nachieves the highest performance in a general situation.\nAlthough MR1 was increased by metric learning, it was ex-\nplained by the second experiment showing that the trained\nmetric failed only for a very small number of queries,\nwhile it was optimized for the most of queries. Since met-\nric learning takes longer computation time and its perfor-\nmance improvement was not prominent as much as KPCA,\nit is possible to expect a good performance gain using\nempirically optimized parameters of KPCA for a ﬁxed\ndataset. However, considering that scalability is an im-\nportant issue in cover song identiﬁcation, metric learning\ncannot be excluded especially for large-scale collections.\nIn the new representation through KPCA, each dimen-\nsion represents the distance from each core song. This\nimplies that core songs with diverse styles of music al-\nlows dimensions to be nearly orthogonal, and may yield\nbetter performance. In the metric learning, on the other\nhand, higher performance could be achieved with a sufﬁ-\ncient number of similar and dissimilar pairs for training. It\nis not easy to satisfy both of the above conditions simul-\ntaneously, because collection of songs with various styles\nincludes songs that are not very popular and rarely cov-\nered. Therefore, when a high recall is required (to avoid\nvery low identiﬁcation performance for very few queries),\nit is expected that it can be more important to have many\nsimilar pairs than various styles.\n5. CONCLUSIONS\nIn this paper, we have presented a novel approach to im-\nprove the performance of existing algorithms for coversong identiﬁcation. Our approach exploits an external set\nof core songs so that all the given songs are newly repre-\nsented by the distance between each core song. Through\nthe distance metric learning after embedding of songs us-\ning kernel PCA, the original performance of the state-of-\nthe-art method was improved by more than 20%.\nWith different features and distance measures, the pro-\nposed method can be easily applied to similarity analysis\nof other tag-based data such as genre, mood, and style. We\nplan to further explore our approach to many other MIR\ntasks, and seek for proper criteria to choose the core set\nfrom large-scale collections. A sufﬁcient number of well-\norganized core songs and efﬁcient computation for metric\nlearning will be also studied in the next step.\n6. ACKNOWLEDGEMENTS\nThis research project was supported by Ministry of Cul-\nture, Sports and Tourism (MCST) and from Korea Copy-\nright Commission in 2017. [Development of predictive de-\ntection technology for the search for the related works and\nthe prevention of copyright infringement]\n7. REFERENCES\n[1] Thierry Bertin-Mahieux and Daniel PW Ellis. Large-\nscale cover song recognition using hashed chroma\nlandmarks. In Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA), 2011 IEEE Workshop\non, pages 117–120. IEEE, 2011.\n[2] Ning Chen, J Stephen Downie, Haidong Xiao, Yu Zhu,\nand Jie Zhu. Modiﬁed perceptual linear prediction\nliftered cepstrum (mplplc) model for pop cover song\nrecognition. In International Society for Music Infor-\nmation Retrieval Conference , pages 598–604, 2015.\n[3] Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra,\nand Inderjit S Dhillon. Information-theoretic metric\nlearning. In Proceedings of the 24th international con-\nference on Machine learning , pages 209–216. ACM,\n2007.\n[4] Daniel PW Ellis and Graham E Poliner. Identifying\ncover songs’ with chroma features and dynamic pro-\ngramming beat tracking. In Acoustics, Speech and Sig-\nnal Processing, 2007. ICASSP 2007. IEEE Interna-\ntional Conference on , volume 4, pages IV–1429. IEEE,\n2007.\n[5] Fumitada Itakura. Minimum prediction residual prin-\nciple applied to speech recognition. IEEE Transac-\ntions on Acoustics, Speech, and Signal Processing ,\n23(1):67–72, 1975.\n[6] Kyogu Lee. Identifying cover songs from audio using\nharmonic representation. MIREX task on Audio Cover\nSong Identiﬁcation , 2006.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 633[7] Abdullah Mueen, Krishnamurthy Viswanathan, Chetan\nGupta, and Eamonn Keogh. The fastest sim-\nilarity search algorithm for time series subse-\nquences under euclidean distance, August 2015.\nAvailable: http://www.cs.unm.edu/ ˜mueen/\nFastestSimilaritySearch.html .\n[8] Meinard Muller and Sebastian Ewert. Towards timbre-\ninvariant audio features for harmony-based music.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 18(3):649–662, 2010.\n[9] Meinard M ¨uller, Frank Kurth, and Michael Clausen.\nAudio matching via chroma-based statistical features.\nInInternational Society for Music Information Re-\ntrieval Conference , volume 2005, page 6th, 2005.\n[10] Cory S Myers and Lawrence R Rabiner. A compara-\ntive study of several dynamic time-warping algorithms\nfor connected-word recognition. Bell System Technical\nJournal , 60(7):1389–1409, 1981.\n[11] Hiroaki Sakoe and Seibi Chiba. Dynamic program-\nming algorithm optimization for spoken word recogni-\ntion. IEEE transactions on acoustics, speech, and sig-\nnal processing , 26(1):43–49, 1978.\n[12] Bernhard Sch ¨olkopf, Alexander Smola, and Klaus-\nRobert M ¨uller. Kernel principal component analysis.\nInInternational Conference on Artiﬁcial Neural Net-\nworks , pages 583–588, 1997.\n[13] Joan Serra, Emilia G ´omez, and Perfecto Herrera.\nTransposing chroma representations to a common key.\nInIEEE CS Conference on The Use of Symbols to Rep-\nresent Music and Multimedia Objects , pages 45–48,\n2008.\n[14] Joan Serra, Emilia G ´omez, Perfecto Herrera, and\nXavier Serra. Chroma binary similarity and local align-\nment applied to cover song identiﬁcation. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n16(6):1138–1151, 2008.\n[15] Diego F Silva, Chin-Chin M Yeh, Gustavo Enrique de\nAlmeida Prado Alves Batista, Eamonn Keogh, et al.\nSimple: assessing music similarity using subsequences\njoins. In International Society for Music Information\nRetrieval Conference , 2016.\n[16] Temple F Smith and Michael S Waterman. Identiﬁca-\ntion of common molecular subsequences. Journal of\nmolecular biology , 147(1):195–197, 1981.634 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Exploring the Music Library Association Mailing List: A Text Mining Approach.",
        "author": [
            "Xiao Hu 0001",
            "Kahyun Choi",
            "Yun Hao",
            "Sally Jo Cunningham",
            "Jin Ha Lee 0001",
            "Audrey Laplante",
            "David Bainbridge 0001",
            "J. Stephen Downie"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415824",
        "url": "https://doi.org/10.5281/zenodo.1415824",
        "ee": "https://zenodo.org/records/1415824/files/HuCHCLLBD17.pdf",
        "abstract": "Music librarians and people pursuing music librarianship have exchanged emails via the Music Library Association Mailing List (MLA-L) for decades. The list archive is an invaluable resource to discover new insights on music information retrieval from the perspective of the music librarian community. This study analyzes a corpus of 53,648 emails posted on MLA-L from 2000 to 2016 by using text mining and quantitative analysis methods. In addition to descriptive analysis, main topics of discus- sions and their trends over the years are identified through topic modeling. We also compare messages that stimulated discussions to those that did not. Inspection of semantic topics reveals insights complementary to previ- ous topic analyses of other Music Information Retrieval (MIR) related resources.",
        "zenodo_id": 1415824,
        "dblp_key": "conf/ismir/HuCHCLLBD17",
        "keywords": [
            "Music librarians",
            "Music librarianship",
            "Email exchanges",
            "MLA-L mailing list",
            "Archive",
            "Music information retrieval",
            "Text mining",
            "Quantitative analysis",
            "Corpus",
            "Descriptive analysis"
        ],
        "content": "Exploring the Music Library Association Mailing List: A Text \nMining Approach  \nXiao Hu1      Kahyun Choi2    Yun Hao2          Sally Jo Cunningham3     Jin Ha Lee4       \nAudrey Laplante5     David Bainbridge3      J. Stephen Downie2 \n1University of Hong Kong  \nxiaoxhu@ hku.hk 2University of Illinois  \n{ckahyu2,  yunhao2, \njdownie}@ illinois.edu  3University of Waikato  \n{sallyjo, davidb}  \n@waikato.ac.nz  \n4Univeristy of Washington  \njinhalee @uw.edu 5Université de Montréal  \naudrey.laplante@umontreal.ca  \nABSTRACT  \nMusic l ibrarians and people pursuing music librarianship \nhave exchanged emails via the Music Library Association \nMailing List (MLA -L) for decades . The list archive is an \ninvaluable resource to discover new insights on music \ninformation  retrieval  from the  perspective of the  music \nlibrarian community . This study analy zes a corpus of \n53,648 emails posted on MLA -L from 2000 to 2016 by \nusing text mining and quantitative analysis methods. In \naddition to descriptive analysis, main topics of discus-\nsions and their trends over the years are identified \nthrough  topic modeling. We also compare messages  that \nstimulated discussions to those that did not . Inspection of \nsemantic topics reveals insights complementary to previ-\nous topic analyses of other Music Information Retrieval \n(MIR) related resources.  \n1. INTRODUCTION  \nThe Music Libra ry Association Mailing List (MLA -L)1 \nhas a variety of subscribers  including music librarians, \nMLA members, and professionals  and students  in music \nlibrarianship as well as musicology . The archive of this \nlist serves as an invaluable resource for studying  discus-\nsions  among  these people ; it is the o ldest and largest re-\npository for studying issues in this profession [11].  \nMusic l ibrarians are an integral and important  part \nof the larger Music Information Retrieval (MIR) commu-\nnity. The experiences, e xpertise, interests and concerns of \nmusic l ibrarians are highly relevant to the advancement \nof MIR research. Similarly, MIR rese arch can improve \npractices in music librari anship , ultimately enhancing the \ndiscovery of music information for diverse types of users \n[23]. Given the abundance of emails archived in  the \nMLA -L, we can identify main topics discussed through-\n                                                           \n1 https://www.musiclibraryassoc.org/?page=mlal  out the years  which  can offer insights into real -world  mu-\nsic information  interactions, particularly concerning the  \nuse and management of music information,  from the  per-\nspectives of music information professionals  and their \nclients . This study seeks to  uncover the key topics of dis-\ncussion related to music information needs and uses  in \nthe MLA mailing list by using text mining and quantita-\ntive analysis methods.  \n2. LITERA TURE RE VIEW \n2.1 MLA -L Content Analysis  \nInterest in, and analysis of, the MLA -L collection is not \nnew: ne arly three decad es ago when e -mail was the new-\nest form of written communication , the list became the \nobject of investigation (e.g., [5], [26]). Griscom [11] of-\nfered a detailed  and relatively more recent  account of the \nhistory and developme nt of the MLA -L. Through qualita-\ntive content analysis, Griscom  organized  postings in the \n“E-Mail Digest”  column of the MLA -L archive  into nine \ncategories  as shown in Table 1.  \nCategory  Definition  \nReference \nquestions  questions on locating songs or music work of \nspecific topics or sources  \nCataloging  extended discussions on catalogs of music li-\nbrary collections  \nPractical  \nmatters  problems unique to music libraries such as cir-\nculation and preservation of holdings  \nTechnology  questions and opinions about adapting to tech-\nnological advancements  \nEthics  questions on unexpected and controversial top-\nics such as illegal items  \nCopyright  questions and comments on reproduction mat-\nters and copyright laws  \nCirculation  \npolicies  policies and procedures posed by special for-\nmats in music libraries  \nAssisting  \ncolleagues  alerts on problems and peculiarities such as \nproduction errors.  \nMLA  matters  communications from the board of directors of \nthe association  \nTable 1 . Main categories of MLA -L postings in [11]. \nWhile it is noteworthy that a majority of these cate-\ngories were consistent with the categorization in earlier \nstudies on the MLA -L [5], [26], analytical research on  \nmailing lists or discussion forums of library professionals  © Xiao Hu, Kahyun Choi, Yun Hao, Sally Jo Cunningham, \nJin Ha Lee, Audrey Laplante, David Bainbridge and J. Stephen Downie  \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Xiao Hu, Kahyun Choi, Yun Hao, \nSally J o Cunningham, Jin Ha Lee, Audrey Laplante, David Bainbridge \nand J. Stephen Downie. “Exploring the Music Library Association \nMailing List: A Text Mining Approach”, 18th International Society for \nMusic Information Retrieval Conference, Suzhou, China, 2017.  \n302  \n \nshoul d not only identify categories of messages, but also \nconsider what specific topics were discussed  [10]. More \nimportantly, the  data analyzed in the  most recent  endeav-\nor of MLA -L content analysis [21] were up to year 1998  \nwhich was nearly two decades ago , and thus inspiring the  \nupdated account  via current exploration of the data .  \n2.2 Text Analysis  in MIR  \nText analyses  of MIR -related resources have increasingly \nappeared  in recent years  in the MIR community. Downie \nand Cunningham [9] contributed  an early analysis of  \npostings of  music -related information requests on a music \nnewsgroup.  In their results, “locate ” was identified as a \npredominant intended use for the requested music infor-\nmation . This is consistent with the category of “reference \nquestions” shown in Table 1 . In fact,  the category “refer-\nence questions”  was identified in all the previous studies \non the MLA -L [5], [11], [26].  \nMusic related Q&A ( question & answer s) websites  \nare also a rich resource  of MIR -related discussion . Bain-\nbridge et al. [1] analyzed users’ music queries in Google \nAnswers using a grounded theory approach, revealing \nthat bibliographic metadata were commonly included in \nusers’  queries . This l eads to corroboration with the “c ata-\nloging ” category of MLA -L postings  [11]; bibliographic \nmetadata  such as performer  and title are necessary for \ncatalogs in music libraries . A later study by Lee [16] em-\nployed content analysis, also on Google Answer queries, \nto look into music in formation seeking behaviors. The \nresults revealed that the “location” of a music information \nobject (e.g., a recording) was also one of the most promi-\nnent information need s.  \nIn addition to newsgroups  and Q&A  websites, pub-\nlications in MIR have also been an alyzed. Lee et al. [16] \nexamined papers  in the proceedings of the Conference of \nthe International Society for Music Information Retrieval \n(ISMIR).  Analysis  of keyword s in titles and abstracts of \nISMIR papers identif ied “audio”  and “classification”  as \nthe most frequent terms . Most recently, Hu et al . [13] \ncompared the keywords in titles of ISMIR papers written \nby female to those by male authors, revealing the gender -\nbased differences of topic preferences between  authors.   \nLyrics, music reviews , users’ interpretations of mu-\nsic, and other types of listeners’ input in social media  \n(e.g., social tags and tweets)  have also been analyzed, \nmainly by automated methods  [24], for various tasks in \nMIR such as genre classification  [22], mood classifica-\ntion [14], and subject classification  [7]. In these studies, \nnatural language processing methods are applied to con-\nvert textual data into numerical data in large scales, \nwhich are then fed into a wide range of machine learning \napproaches to fulfill aforementioned M IR tasks. More \noften than not,  such texts are used in combination with \naudio signals to further improve performances via multi-\nmodal approaches (e.g.  [14]).          2.3 Topic Modelling and Trend Analysis  \nMachine learning and quantitative methods have demon-\nstrated their capability in automating the processing of  \nemail messages [6] and other text input by users . Barua et \nal. [2] used a prevalent statistical mode ling technique \ncalled Latent Dir ichlet Allocation (LDA) [3] to discover \ntopics and their trends in Q&A  websites , in order to gain \ninsights into the wants and needs of the participa nts. Prior \nstudies employing topic modelling on email  messages \nand replies  are seemingly  scarce. McCallum et al. [20], \nbeing one ra re case, also used the LDA in  analyz ing an \nemail corpus.  \nExploration of the changes in  topics found in a body \nof messages  over tim e—trend  analysi s—is particularly \nimportant due to the evolving focus  in documents such as \nemails and queries (Blei and Lafferty, 2003 as cited in \n[27]). Unsupervised topic modeling as an extension to \nLDA is one way to generate  the temporal relationships of \ntopics [12]. \nMishne and Glance [21] showed that comments \nmade  by users on weblogs could be an indicator of popu-\nlarity of post s or the weblogs themselves. The mechanism \nof MLA -L is also in the form of postings welcoming po-\ntential replies , and thus it is reasonable to evaluate  the \npopularity of topics based on their corresponding replies.  \nTo bridge the gap s in previous research , this study \naims to answer the following research questions: 1) What \nare the primary  topics  discussed in t he MLA -L list from \n2000 to 2016? ; 2) How did the strength of the topics \nchange over time? ;  and 3) Which topics attracted replies \nand which did not ? Answers to these questions will help \nMIR researchers and practitioners understand information \nneeds of the community and identify potential use cases \nof MIR tasks and applications.  \n3. DATA STATISTICS  \nThe corpus used in this study consists of 53,648 emails \nposted on MLA -L from 2000 to 2016  by approximately \n2,713 people  (Figure 1 ). Among these emails, 33,250 \n(61.98%)  received no replies while the other 20,398 \nemails  (38.02%)  formed 8,384 distinct onlin e conversa-\ntions (email threads) with the largest thread conta ining 52 \nemails. The average length of an email is 177.5  word s \n(after removing reply and signature blocks ; blank emails \nnot included)  with the longest email  containing  1,389 \nwords , indicating that most of the emails contain substan-\ntial content.   \n \nFigure 1. Number of emails in MLA -L across years . \nProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 303  \n \n4. PREPROCESSING  \nThe data cleaning steps  for preprocessing the large corpus  \ninclude : 1) removing special formats and converting \nemails into plain text using OpenRefine2; 2) removing  \nreply blocks (quoted emails included in reply emails ); 3) \ndetecting signature blocks  by detecting variations of the \nsender’s name using the tool Jangada3 from the last 10 \nlines of the emails as suggested in [6]; 4) removing mail-\ning l ist footers; and, finally 5) removing various leave -\ntakings such as “best wishes ” and “sincerely .” Given the \nlarge amount of data, it is not possible to manually evalu-\nate the accuracy of preprocessing on the entire corpus. \nInstead, inspection of a  random sample  of 100 pre-\nprocessed emails show 89% had the aforementioned \nnoisy parts  correctly removed . \nThe cl eaned emails underwent  text processing pro-\ncedures for text mining purpos es. Stopwords4 that are ex-\ntremely common in English  were  removed to increase the \nquality of discovered topics.  In addition, words that oc-\ncurred in more than 15% of the emails , such as “music ” \nand “send”, were also removed , since they had little dis-\ncriminant power and thus could be regarded as domain \nstopwords . To enhance the readability of the topics, l em-\nmatization5 was applied to convert words in derivative \nforms into their lemmas, instead of crude ste mming that \nusually c uts out suffixes  of words. Finally, emails that \nwere  left with less than four words were eliminated  as it \nwould  not be reliable to assign them  to a certain topic \nbased on  too few words . Figure 2 illustrates an example \nemail before and after preprocessing.  \n \nFigure 2 . An example email from the MLA -L postings \nbefore and after preprocessing . \n5. METHODS  \n5.1 Topic Modeling  Setup and Labeling Procedure  \nWe employ ed one of the popular topic modeling algo-\nrithm s, Latent Dirichlet Allocation (LDA) . It is a genera-\ntive model that represents documents as probability dis-\ntributions over topics, and represents topics as probability \ndistributions over words  [3]. In other wor ds, given a set \nof documents, LDA can identify latent topics discussed in \nthese documents  based on word -document co -\noccurrences . As a probabilistic method,  LDA can assign  \neach document to  a small number of  topics with different \n                                                           \n2 http://openrefine.org/  \n3 http://www.cs.cmu.edu/~vitor/codeAndDa ta.html  \n4 https://code.google.com/archive/p/stop -words/  \n5 http://morphadorner.northwestern.edu/morphadorner / probabilities.  At the same t ime, each topic is represented \nby a small set of words that are highly related to this top-\nic. Due to its superior performances compared to other \ntopic modeling methods, LDA has been widely used to \ndiscover topics from diverse corpora such as papers , web \npostings, and even tweets.  It is believe d that LDA can \nalso discover topics well from email content  [20]. \nIn this study , we ran the LDA implementation in the \nMALLET machine learning toolkit , which has been  \nwidely used in topic modeling research  [19]. In LDA, t he \nnumber of topics is tightly linked to the granular ity of the \nlearned topics. After changing  the number of topics from \n10 to 200, we report the case when the number of topics \nwas set as 50 , for a proper granularity of topics . To in-\ncrease the quality of text analysis, we use d bigrams  (i.e., \ncombinations of two consecutive  terms)  as well as uni-\ngrams (i.e., individual terms) as “words” when learning \nthe topics  [25]. \nThe resultant topics then under went  a manual screen-\ning process, to filter out noisy, meaningless topics. This is \na common practice in applying topic modeling (e.g., \n[12]). Those noisy topics were int roduced by trivial text \npatterns that frequently  appeared in the corpus. In our \ncontext , due to the inevitable side effect of the automatic \napproach to data preprocessing, which was mandated by \nthe sca le of the corpus, some signatures and forwarded \nmessage  headers remained and were inputted into the top-\nic modeling process. As a result, these signatures and \nsenders were grouped into a noisy topic. Another exam-\nple of a noisy topic is the agglomeration of words in lan-\nguages other than English. Common email ter ms (e.g., \n“post”, “email”) and greeting/closing words also formed \na topic which conveyed little meaning.  Fortunately, topi c \nmodeling associate s each identified topic with representa-\ntive words, and thus  it is convenient and reliable for re-\nsearcher s to exami ne and weed out noisy topics.  \nThe remaining topics appear to be  meaningful. To \nenhance readability of the topics , we manually labeled \neach topic  with one or two phrases  based on the meanings \nof the top 10 words and the top 50 emails associated with  \nthe topic. In particular, f requently appearing word s in the \nsubject  lines  of the highly ranked emails in each topic \nhelp us gain a  deeper understanding of the topic.  \n5.2 Topic Trend Analysis  \nBesides identifying topics in the entire corpus, w e also \ncalculate d the probability over topics given a particular \nyear, 𝑃(𝑡|𝑦), to examine  the topical trend over time. To \nthis end, we followed the empirical probability calcula-\ntion procedure proposed in [12]. Based on the topic mod-\neling result, we first compute d a matrix 𝑪 of 𝐷×𝑇 dimen-\nsions where D refers to the number of documents (emails) \nin our corpus and T the number of topics identified. \nThe (𝑑,𝑡)-th element  of the matrix  𝑪 holds the number of \nwords assigned to  the 𝑡-th topics in the 𝑑-th document. \nFrom here, we can induce the probability over the topics \nper year 𝑦 as follows:  \n304 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017  \n \n𝑃(𝑡|𝑦)=∑ 𝑪(𝑑,𝑡) 𝑑=𝐷𝑦\n∑ 𝑁𝑑 𝑑=𝐷𝑦,                   (1) \nwhere 𝐷𝒚 is the set of documents that belong to year \n𝑦, and 𝑁𝒅 is the number of words in  the 𝑑-th document.  \nThe probability 𝑃(𝑡|𝑦) over the years can then form a \ntime series for topic t.  \nIn order to  determine whether there is a statistically \nsignificant upward or downward trend for a topic over \ntime, Cox Stuart trend analysis was used with a signifi-\ncance level of 0.05 [18]. Cox Stuart trend analysis splits a \ntime series into two halves and counts  the numbers of \npositive and negative differences between pairs of data \npoints drawn from the two halves. If there are more nega-\ntive differences than positive  ones, th en an upward trend \nis detected, indicating that  the topic gained more attention \nover the years , and vice versa.  \n5.3 Topics with and without Replies  \nTo answer the third research question, we compared the \ntopics associated with emails with replies and those with-\nout. As mentioned before, in the results of LDA, each \nemail is assigned to multiple topics with different proba-\nbilities. In this analysis, we aggregated the topics and \ntheir probabilities across all emails either with or without \nreplies. Then we rank ed the topics based on their aggre-\ngated and normalized probabilities. By comparing the top \ntopics of the two sets of emails, we can discover which \ntopics were more engaging and generated more discus-\nsions among the MLA community.     \n6. RESULT S AND DISCUSSION  \n6.1 Discovered Topics   \nUpon careful screening, 27 of the resultant topics were \nidentified as meaningful . Table 2 presents these topics , \nranked by  their topic weights.  For each topic, Table 2 \npresents the  topic IDs, the labels, the topic weight  (in pa-\nrentheses) , frequent words in the subjects of top emails  \nassociated with this topic , the top 10 words assigned to \nthis topic in the LDA results , trend  over time (up ward or \ndown ward  as indicate d by arrows), and the trendline  that \nplots the probabilit y change of  this topic across the years.   \nThe weight of the topic (the Dirichlet parameter ) is \nroughly proportional t o the overall portion of the docu-\nments  assigned to a given topic  [19]. Therefore, t opics \nwith higher weights  are more popular in the  corpus , while \nthose with lower weights rarely appear . As shown in Ta-\nble 2,  it is not surprising that the most important topic in \nour corpus is about request s and questi ons from patrons \n(i.e., clients) . Unlike other topics, there is no frequent \nword in subject lines for this topic. A closer examination \nrevealed that it is because  each subject belonging to this \ntopic was unique. Based on this topic ’s highest weight, \nwe can  infer  that there must have been a wide range of \nrequests and questions from patrons. The second highest \nranked topic is musical terms whose top words include a range of music genres, indicating  that music librarians not \nonly focus on Classical music (topi c #35), but also on  a \nwide diversity of music. Other top ranked topic s cover \nvarious aspects of librarianship ( e.g., #6: cataloging; #14: \ncirculation; #22: collection), and music -specific materi-\nals: scores (#30), recordings (#47) , and  songs (#12 ).  \nIn order to compare  our results  to categories  identi-\nfied in previous studies using content analysis, t he 27  \nmeaningful  topics discovered in this study  were manually \ngrouped into n ine broad er topic categories  based on their \nsemantic similarity : Cataloging, Referen ce Questions, \nCirculation Policies, Copyright, Audio Technology, \nMLA, Advertisement s, Music Related Terms, and Others. \nFive of the categories  (Reference questions, Cataloging, \nCopyright, Circulation Policies , MLA ) were equivalent \nwith those uncovered by Gr iscom  [11] (c.f. Table 1) , \nwhile Audio Technology appears closely related  to Gris-\ncom’s “Technology .” The Advertisement (CD/DVD \nsales, Travel information, Job postings) categor y is novel \nto our findings.  \nOur discovered topics are also consistent  with re-\nsults from earlier topic analyses on MIR -related discus-\nsions. For example , some of the most frequent words in \ntopic #31 (e.g., “bibliographic, ” “metadata ”) are exactly \nthe same as how users of MIR systems predominantly \ndescribed their needs, particular ly on music -related dis-\ncussion platforms  [1], [9]. Similarly, Audio Technology \nwas one of the main topic s revealed in this analysis , \nwhereas “ audio” has been one of the most commonly \nused title terms in ISMIR research topics [16]. With the \nrapid growth of audio -based research in the MIR com-\nmunity, insights and needs on audio technology from the \nmusic librarian community can provide impo rtant real -\nlife use cases for MIR studies. Another example is Topic \n#42 which was labelled “Grove music  online .” It echoes \nthe trend of online access to digital music information, \nwhich is also a major t heme in MIR research community. \nAs a major research -oriented online resource serving \nscholars and music professionals,  Grove Music Online \ncan be used by MIR research ers for improving  MIR ser-\nvices and applications targeting  the scholarly and profes-\nsional user groups.  \n6.2 Topic Trend Analysis   \nThe temporal trends  of these topics  are reported in the \n“trend column” in Table 2 . Results of Cox Stuart tests \nshow that six topics had increasing trends (#20, #31, #7, \n#49, #24, #27.) while  four had decreasing trends (#41, \n#14, #12, and #2). Other topics had no significant trend. \nIt is noteworthy that the topics with decreasing trends \nranked higher in Table 2 than those with increasing \ntrends, reflecting a phenomenon that popular topics be-\ncame less dominating over time while topics with lower \nweights started gaining populari ty in recent years, result-\ning in diversified topics.  \nThe topics showing downward trends are related to \nthe traditional functions of music libraries:  #41 (Patron's \nrequest s and  question s), #14 (C irculation and library pol-\nicy in colleges ), and #12 (Song req uests). These indicat e Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 305  \n \nID Topic Label  (weight)  Frequent words in Subjects  Top 10 Words  Trend  Trendl ine \n41 Patron's request & question  \n(0.086)  N/A dear, copy , cw, patron , source , dear, cw, piece , \nscore , advance , check  ↓  \n28 Musical terms  (0.062)  Jazz, band, blues, rock music, songs  Jazz, band , record , blue, play, album , rock, live, \njohn, sing -  \n4 MLA Event  (0.054)  MLA meeting, conference, mentoring  mla, meeting , program , meet , conference , \nsession , attend , pm, friday , time -  \n6 Cataloging (0.054)  oclc, lc, classification, cataloging, \nauthority record, bib record  Record , title, oclc, number , catalog ue, \nauthority , catalog , add, authority  record , item -  \n14 Circulation and library \npolicy in colleges  \n(0.046)  school, due date, except to faculty, \ncirculate, Music Practice Room(s) in \nLibrary, students, faculty  student , collection , faculty , material , item, \nclass , patron , circulate , score , staff ↓  \n35 Classical music  (0.045)  orchestra, chamber music, piano, concerto  piano , orchestra , symphony , violin , string , \nquartet , op, concerto , sonata , piece  -  \n22 Music collection  (0.039)  Music collection, catalogue, material, \narchive, donation  collection , manuscript , sheet , material , book , \nscore , item, special , rare, project  -  \n30 Scores, edition  (0.037)  score, edition, need, actual music piece \ninformation (op, major, ..)  score , edition , publish , publisher , volume , \ncopy , print, major , publication , complete  -  \n47 Audio recording  (0.034)  audio, recording, streaming, iPod, I -tunes, \nphysical format, mp3  naxos , audio , recording , classical , stream , file, \nlisten , digital , service , record  -  \n12 Requesting songs  \n(0.032)  Folk song for a wedding, Song in a \nSopranos episode, Animal Songs, Songs \nabout aging, Lyrics question,  song, lyric, sing, tune, word , folk, title, tina, \npopular , dallas  ↓  \n23 Journal and periodical  \n(0.030)  journal, JSTOR, RIPM publications, ECO \nmusic journal, IIMP  journal , article , review , publish , issue , rilm, \nonline , editor , title, publication  -  \n39 Job posting  (0.028)  Job opening, Job posting, position, job \nannouncement  service , librarian , experience , collection , \nposition , reference , application , degree , faculty , \nprofessional  -  \n2 Music storage  (0.028)  cd, lp, case, vinyl cd sleeves, cd box lids  cd, disc, lp, dvd, record , tape, label, case, box, booklet  ↓  \n20 MLA board, member  \n(0.027)  MLA Newletter, roundtable, Note -Book, \nCall for new members, Board meeting, \nBoard reports  mla, committee , member , board , association , \nreport , chair , membership , year, annual  ↑  \n33 List of books  (0.027)  Encylopedia, books, bibliography, Reference \ntitles to give away, book titles  book , press , author , title, publication , york, \nyear, isbn, publish , history  -  \n0 Subject Heading, lc, genre, \ncode (0.027)  call numbers(lc , Dewey), language code \nzxx, Genre heading, field, marc, aacr2, \nclassification  subject , head, heading , term, title, instrument , \nscore , code, form , musical  -  \n29 Copyright  (0.025)  Copyright question, copyright tips, \ncopyright courses, royalties, purchasin g a \ndownload, ILL(InterLibrary Loan)  copyright , copy , law, fair, public , license , \ndomain , legal , public_domain , permission  -  \n44 Journal  and periodical \n(0.024)  Journal, issue, periodical  issue , journal , volume , spring , copy , fall, \namerican , june, summer , july -  \n10 Conference \nroommate/transportation\n-mate solicitation (0.022)  roommates for, Shuttles to, registration  registration , conference , hotel , room , rate, \nregister , tour, roommate , reservation , fee -  \n31 Metadata (0.020)  Music metadata, RDA, MOUG,  \nBibliographic Control Committee (BCC), \nISBD, OCLC -MARC, music cataloging  catalog ue, rda, bibliographic , metadata , marc , \nmoug , oclc, service , indiana , access  ↑  \n7 Call for papers, \nproposals, awards, etc.  \n(0.019)  Call for Papers, Call for Submissions, Call \nfor Proposals, Call for Applications, Call \nfor poster sessions, call for seminar topics  conference , proposal , paper , submission , \nsession , presentation , submit , poster , deadline , \ntopic ↑  \n13 Travel info rmation \n(0.019)  currency exchange, meeting, travel saving, \ntransportation  san, travel , city, food, train, water , station , \nstreet , building , bus -  \n42 Grove music online  \n(0.019)  grove, grove online, grove dictionary, new \ngrove 2(ng2)  grove , online , article , dictionary , reference , \nprint, oxford , grove_online , edition , resource  -  \n15 Hymn  (0.019)  Hymnals, hymn tune, chant, mass, choral \nmusic  church , organ , hymn , choral , saint, psalm , sing, \nchoir , antoinette , mass -  \n49 Job posting  (0.019)  Job posting  job, position , service , placement , librarian , \napply , mla, placement_service , mla_placement , \nhire ↑  \n24 CD/DVD  sales (0.018)  CD HotList, Music Media Monthly, MLA \nDiscount  order , sale, label , cd, offer , release , special , set, \ntime, mla ↑  \n27 Call for papers \nproposals, awards, etc.  \n(0.014)  Call for Papers, Call for Submissions, Call \nfor Proposals, Call for Applications, Call \nfor poster sessions, call for seminar topics  letter , support , grant , application , travel , award , \nyear, annual , meeting , moug  ↑  \nTable 2 . Identified topics by topic modeling\n306 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017  \n \nthat MLA members tended to engage more  on other is-\nsues than these traditional library functions  in recent \nyears. This shift of attention may be attributed to ad-\nvancement of technologies in recent years, including \nthose in MIR. For instance,  the downward trend of topic \n#12 (Requesting songs) might be related to the fact that \nmusic librarians  and their clients  have been equipped \nwith alterative means to discover and access song s and \nother musical materials , such as search engines and \nonline music repositories . Another trendline that may also \nreflect technology advancement is topic # 2 (music stor-\nage). The sharp  decrease of  this topic corresponds to the \ndecline of CDs, LPs, tapes  as formats of physical music \nmaterials . In this regard, technologies and resources facil-\nitating music information retrieval and access are of great \ndemand, particularly when the ways and  channels people  \nlook for music information  are changing so rapidly .  \nOn the other hand, topics with upward trends in-\nclude #20 , which consists of messages about MLA board \nand members , demonstrating a vibrant and active profes-\nsional association in the  field of music  librarianship. The \nsecond topic with a growing popularity i s #31 (Metadata) , \nwhich corroborates with recent developments in the \nmetadata field such as RDA ( Resource Description and \nAccess, a standard for cataloging released in 2010 ). The \nother topics with upward trends all fall into the Adver-\ntisement category: #7 and #27 (Call for papers, proposals, \nawards), #49 (Job postings) and #24 (CD/DVD sales). \nThis again reflects the flourishing development of the \nfield and the community. In fac t, the MLA and the U.S. \nbranch of the International Association of Music Librar-\nies (IAML -US) were merged in 2011 , which has substan-\ntially boosted the status  of MLA in the profession .6  \n6.3 Topics of Emails with/without Replies  \nWe compared the e mail messages th at stimulated discus-\nsion among participants of the MLA -L to those that did \nnot. Figure 3 show s topics and their normalized weights \nin emails with and without replies respectively.  Among \nthe 27 topics identified by topic modeling, three pairs \nwere merged fo r this analysis as the semantics of each \npair are almost identical:  #39 and #49 (J ob postings), # \n23 and #44 (J ournal s and periodical s) and # 7 and # 27 \n(Call for papers /proposals/awards ). As shown in Figure 3, \nthere is an overlap  among  highly -ranked topics between \nthe two lists, such as  “Cataloging”, “Patron’s request & \nquestions”, and “Classical music”.  This is not surprising  \nas these are the most popular topics in the entire dataset \n(Table 2) . \nIt is more interesting to see that there are topics \nranked high  in emails with replies but low in those  with-\nout, such as  “Subject heading, LC, genre, code”  (#0), \n“Requesting songs”  (#12) , “Audio recording”  (#47), and  \n“Copyright” (#29) —indicating that emails in these topics  \noften started discussions among  subscribers. In particular, \nemails in “Requesting songs” (#12) and “Audio record-\ning” (#47) are likely to contain music information needs  \n                                                           \n6 https://www.musiclibraryassoc.org/?page=AboutMLA  and queries for music information. Similar to postings in \nQ&A websites, these email exchanges provide insights \non 1) what kind of mu sic information was needed by mu-\nsic librarians who in turn were trying to meet the needs of \ntheir patrons (i.e., the end users); and 2) how well -trained \nmusic information professionals looked for music infor-\nmation. Some of the heated discussions in threads  with a \nlarge number of replies are likely to include queries that \nwere interesting yet hard to find information for. These \nare excellent resources to discover not only new use cases \nbut also search strategies for novel MIR systems.  \n \n \nFigure 3. Topics in  emails with and without replies .  \nTopics that are much more popular in emails with-\nout replies than in those with replies include “MLA \nevent” (#4), “Call for papers/proposals/awards” (#7 and \n#27), “MLA board, members” (#20), and “CD/DVD \nsales” (#24). These  topics are mostly of the nature of an-\nnouncement and thus are unlikely to trigger  discussions.  \n7. CONCLUSION  \nThis study collected email messages posting in the Music \nLibrarian Association mailing list (MLA -L) from 2000 to \n2016, and analyzed the content throu gh text mining. Main \ntopics of discussions and their trends over the years are \nidentified using Latent Dirichlet allocation (LDA). Twen-\nty-seven  meaningful topics were found  and their seman-\ntics and trends were discussed in the context of MIR re-\nsearch. Topic s in emails with and without replies were \ncompared. As music librarians are gateways and bridges \nbetween music resources and users, the goals of music \nlibrarians and those of MIR researchers and practitioners \nare consistent: to help and facilitate users to  access and \nmake better use of music information. Therefore, the \nconcerns and focuses reflected in the MLA -L are worthy \nof attention from the MIR community.  \nFuture work will include detailed content analysis of \nthe emails in such topics as “Requesting songs” and “Pa-\ntron’s Request & questions”, to identify the needs of mu-\nsic information professionals and their users, and to learn \nabout effective strategies of identifying and locating hard -\nto-find music information .   \nProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 307  \n \n8. ACKNOWLEDGEMENT  \nWe thank the anonymous reviewer s for their helpful \ncomments. This study is partially supported by a seed re-\nsearch grant fu nded by the University of Hong Kong, and \nthe Data -Driven Scholarship Autumn Institute  jointly \nsponsored by the iSchool at the University  of Illinois and \nthe Faculty of Education at the University of Hong Kong.  \n9. REFERENCES  \n[1] D. Bainbridge et al., “ How People Describe Their \nMusic Information Needs: A Grounded Theory \nAnalysis of Music Queries,”  in Proc. of ISMIR , \n2003.  \n[2] A. Barua et al. , “What Are Developers Talking \nAbout? An Analysis of Topics and Trends in Stack \nOverflow, ” Empirical Software Eng. , 19(3), pp. 619 -\n654, 2014 . \n[3] D. M. Blei  and J. D. Lafferty, “Dynamic Topic \nModels,”  in Proc.  of the 23rd Int. Conf. on Machine  \nLearning , Pittsburgh, PA, 2006, pp. 113 -120. \n[4] D. M. Blei et al., “ Latent Dirichlet Allocatio n,” J. \nMach. Learning Research , vol. 3, pp. 993 -1022, \n2003.  \n[5] D. Campana, “ Information Flow: Written \nCommun ication Among Music Librarians,”  Notes , \nser. 2, 47 (3), pp. 686 -707, Mar. 1991 . \n[6] V. R. Carvalho and W. W. Cohen, “ Learning to \nExtract Signat ure and Reply Lines from Email,”  in \nProc. Conf. Email and Anti -Spam , 2004.  \n[7] K. Choi et al., “ Topic Modeling Users ’ \nInterpretations of Songs to Inform Subject Acc ess in \nMusic Digital Libraries,”  in Proc. of JCDL , 2015, pp. \n183-186. \n[8] D. R. Cox and A. Stuart, “ Some Quick Sign Tests \nfor Trend in Location and Dispersion,”  Biometrika , \n42(1/2), pp. 80-95, 1955 . \n[9] J. S. Downie and S. J. Cunningham, “ Toward a \nTheory of Music Information Retrieval Queries: \nSystem Design Implications,”  in Proc. of ISMIR , \n2002.  \n[10] M. M. Edwards, “ A Content Analysis  of the \nPUBYAC Discussion List,” M.S. thesis, UNC , \nChapel Hill,  1999.  \n[11] R. Griscom, Richard, “MLA -L at Twenty,”  Notes , \nvol. 65, no. 3, pp. 433 -463, 2009 . \n[12] D. Hall et al., “ Studying the History of Ideas Using \nTopic Models,”  in Proc. Conf. Empirical Methods in \nNatural Language Processing , 2008, pp. 363 -371. \n[13] X. Hu et al., “ WiMIR: An Informetric S tudy on \nWomen Authors In ISMIR,”  in Proc. of ISMIR , \n2016, pp. 765 -771. [14] X. Hu et al., “ A Framework for Evaluating \nMultim odal Music Mood Classification,”  JASIST , \n68(2), pp. 273 -285, 2017.  \n[15] X. Hu and J. S. Downie, “ Improving Mood \nClassif ication in Music Digital Libraries  by \nCombining Lyrics and Audio,”  in Proc. of JCDL , \n2010, pp. 159 -168. \n[16] J. H. Lee, “ Analysis of User Needs and Information \nFeatures in Natural Language Queries Seeking \nMusic Information,”  JASIST , 61(5), pp. 1025 -1045, \n2010 . \n[17] J. H. Lee et al., “ An Analysis of ISMIR \nProceedings: Patterns of A uthorship, Topic, and \nCitation,”  in Proc. of ISMIR , 2009, pp. 57 -62. \n[18] T. Martino. (2009). Trend Analysis with the Cox -\nStuart Test in R . http://statistic -on-air.blogspot. com \n/2009/08/trend -analysis -with-cox-stuart -test-in. html \n[19] A. K. McCallum. (2002). Mallet: A machine \nLearning for Language Toolkit  [Online]. Available: \nhttp://mallet.cs.umass.edu  \n[20] A. McCallum et a l., “The Author -Recipient -Topic \nModel for Topic and Role Discovery  in Social \nNetworks: Experiments  with Enron and Academic \nEmail,”  in Workshop on Link Analysis, \nCounterterrorism and Security , 2005, pp. 33 . \n[21] G. Mishne and N. Glance, “ Leave a Reply: An \nAnalys is of Weblog Comments,”  in 3rd Annual \nWorkshop on the Weblogging Ecosystem , 2006 . \n[22] R. Neumayer and A. Rauber, “ Integration of text and \naudio features for genre classification in music \ninformation retrieval,”  in Proc. of ECiR , 2007, pp. \n724-727. \n[23] J. Riley and C. A. Mayer, “ Ask a Librarian: The \nRole of Librarians in the Mus ic I nformation \nRetrieval Community,”  in Proc. of ISMIR , 2006, pp. \n13-18. \n[24] M. Schedl et al., “ Music Information Retrieval: \nRecent  Developments and Applications,”  \nFoundations and Trends® in Inform. Retrieval , 8(2-\n3), pp. 127 -261, 2014 . \n[25] C. Tan et al., “ The Use of Bigrams to Enhance Text \nCategorization,”  Inform. Process. & Manage ., 38(4), \npp. 529 -546, 2002.  \n[26] L. Troutman, “ MLA -L: A New mode of \nCommunication,”  Fontes Artis Musicae , pp. 271 -\n281, 1995 . \n[27] J. W. Uys et al., “ Leveraging Unstructured \nInformation Using Topic Modelling,”  in Portland \nInt. Conf. Management of Engineering & \nTechnology (PICMET) , 2008, pp. 955 -961. 308 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Counterpoint by Convolution.",
        "author": [
            "Cheng-Zhi Anna Huang",
            "Tim Cooijmans",
            "Adam Roberts",
            "Aaron C. Courville",
            "Douglas Eck"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416370",
        "url": "https://doi.org/10.5281/zenodo.1416370",
        "ee": "https://zenodo.org/records/1416370/files/HuangCRCE17.pdf",
        "abstract": "Machine learning models of music typically break up the task of composition into a chronological process, compos- ing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, of- ten revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition. Our model is an instance of orderless NADE [36], which allows more direct ancestral sampling. However, we find that Gibbs sampling greatly improves sample qual- ity, which we demonstrate to be due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from [40] yields better samples than ancestral sampling, based on both log-likelihood and human evaluation.",
        "zenodo_id": 1416370,
        "dblp_key": "conf/ismir/HuangCRCE17",
        "keywords": [
            "machine learning models",
            "chronological process",
            "nonlinear fashion",
            "convolutional neural network",
            "partial musical scores",
            "blocked Gibbs sampling",
            "ancestral sampling",
            "orderless NADE",
            "conditional distributions",
            "cheap approximate blocked Gibbs"
        ],
        "content": "COUNTERPOINT BY CONVOLUTION\nCheng-Zhi Anna Huang?yTim Cooijmans?yAdam Roberts]\nAaron CourvilleyDouglas Eck]\n?Equal contributionsyMILA, Universit ´e de Montr ´eal]Google Brain\nchengzhiannahuang@gmail.com, cooijmans.tim@umontreal.ca\nABSTRACT\nMachine learning models of music typically break up the\ntask of composition into a chronological process, compos-\ning a piece of music in a single pass from beginning to\nend. On the contrary, human composers write music in\na nonlinear fashion, scribbling motifs here and there, of-\nten revisiting choices previously made. In order to better\napproximate this process, we train a convolutional neural\nnetwork to complete partial musical scores, and explore the\nuse of blocked Gibbs sampling as an analogue to rewriting.\nNeither the model nor the generative procedure are tied to\na particular causal direction of composition.\nOur model is an instance of orderless N ADE [36],\nwhich allows more direct ancestral sampling. However,\nwe ﬁnd that Gibbs sampling greatly improves sample qual-\nity, which we demonstrate to be due to some conditional\ndistributions being poorly modeled. Moreover, we show\nthat even the cheap approximate blocked Gibbs procedure\nfrom [40] yields better samples than ancestral sampling,\nbased on both log-likelihood and human evaluation.\n1. INTRODUCTION\nCounterpoint is the process of placing notes against notes\nto construct a polyphonic musical piece. [9] This is a chal-\nlenging task, as each note has strong musical inﬂuences\non its neighbors and notes beyond. Human composers\nhave developed systems of rules to guide their composi-\ntional decisions. However, these rules sometimes contra-\ndict each other, and can fail to prevent their users from\ngoing down musical dead ends. Statistical models of mu-\nsic, which is our current focus, is one of the many compu-\ntational approaches that can help composers try out ideas\nmore quickly, thus reducing the cost of exploration [8].\nWhereas previous work in statistical music modeling\nhas relied mainly on sequence models such as Hidden\nMarkov Models (HMMs [2]) and Recurrent Neural Net-\nworks (RNNs [31]), we instead employ convolutional neu-\nral networks due to their invariance properties and em-\nc\rCheng-Zhi Anna Huang, Tim Cooijmans, Adam\nRoberts, Aaron Courville, Douglas Eck. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron\nCourville, Douglas Eck. “Counterpoint by Convolution”, 18th Inter-\nnational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.\nFigure 1 . Blocked Gibbs inpainting of a corrupted Bach\nchorale by C OCONET . At each step, a random subset of\nnotes is removed, and the model is asked to infer their val-\nues. New values are sampled from the probability distribu-\ntion put out by the model, and the process is repeated. Left:\nannealed masks show resampled variables. Colors distin-\nguish the four voices. Middle: grayscale heatmaps show\npredictionsp(xjjxC)summed across instruments. Right:\ncomplete pianorolls after resampling the masked variables.\nBottom: a sample from NADE (left) and the original Bach\nchorale fragment (right).211phasis on capturing local structure. Nevertheless, they\nhave also been shown to successfully model large-scale\nstructure [37, 38]. Moreover, convolutional neural net-\nworks have shown to be extremely versatile once trained,\nas demonstrated by a variety of creative uses such as Deep-\nDream [29] and style transfer [10].\nWe introduce C OCONET , a deep convolutional model\ntrained to reconstruct partial scores. Once trained, C O-\nCONET provides direct access to all conditionals of the\nformp(xijxC)whereCselects a fragment of a musi-\ncal score xandi =2Cis in its complement. C OCONET\nis an instance of deep orderless N ADE [36], which learns\nan ensemble of factorizations of the joint p(x), each cor-\nresponding to a different ordering. A related approach is\nthe multi-prediction training of deep Boltzmann machines\n(MP-DBM) [12], which also gives a model that can predict\nany subset of variables given its complement.\nHowever, the sampling procedure for orderless N ADE\ntreats the ensemble as a mixture and relies heavily on or-\ndering. Sampling from an orderless N ADE involves (ran-\ndomly) choosing an ordering, and sampling variables one\nby one according to the chosen ordering. This process is\ncalled ancestral sampling , as the order of sampling follows\nthe directed structure of the model. We have found that this\nproduces poor results for the highly structured and com-\nplex domain of musical counterpoint.\nInstead, we propose to use blocked-Gibbs sampling, a\nMarkov Chain Monte Carlo method to sample from a joint\nprobability distribution by repeatedly resampling subsets\nof variables using conditional distributions derived from\nthe joint probability distribution. An instance of this was\npreviously explored by [40] who employed a N ADE in the\ntransition operator for a Markov Chain, yielding a Gen-\nerative Stochastic Network (GSN). The transition consists\nof a corruption process that masks out a subset x:Cof\nvariables, followed by a process that independently resam-\nples variables xi(withi =2C) according to the distribu-\ntionp\u0012(xijxC)emitted by the model with parameters \u0012.\nCrucially, the effects of independent sampling are amor-\ntized by annealing the probability with which variables are\nmasked out. Whereas [40] treat their procedure as a cheap\napproximation to ancestral sampling, we ﬁnd that it pro-\nduces superior samples. Intuitively, the resampling process\nallows the model to iteratively rewrite the score, giving it\nthe opportunity to correct its own mistakes.\nCOCONET addresses the general task of completing par-\ntial scores; special cases of this task include ”bridging” two\nmusical fragments, and temporal upsampling and extrapo-\nlation. Figure 1 shows an example of C OCONET popu-\nlating a partial piano roll using blocked-Gibbs sampling.\nCode and samples are publically available.1Our samples\non a variety of generative tasks such as rewriting, melodic\nharmonization and unconditioned polyphonic music gen-\neration show the versatility of our model. In this work we\nfocus on Bach chorales, and assume four voices are active\nat all times. However, our model can be easily adapted to\n1Code: https://github.com/czhuang/coconet\nData: https://github.com/czhuang/JSB-Chorales-dataset\nSamples: https://coconets.github.io/the more general, arbitrarily polyphonic representation as\nused in [4].\nSection 2 discusses related work in modeling music\ncomposition, with a focus on counterpoint. The details of\nour model and training procedure are laid out in Section 3.\nWe discuss evaluation under the model in Section 4, and\nsampling from the model in Section 5. Results of quantita-\ntive and qualitative evaluations are reported in Section 6.\n2. RELATED WORK\nComputers have been used since their early days for ex-\nperiments in music composition. A notable composition\nis Hiller and Issacson’s string quartet Illiac Suite [18],\nwhich experiments with statistical sequence models such\nas Markov chains. One challenge in adapting such models\nis that music consists of multiple interdependent streams\nof events. Compare this to typical sequence domains such\nas speech and language, which involve modeling a sin-\ngle stream of events: a single speaker or a single stream\nof words. In music, extensive theories in counterpoint\nhave been developed to address the challenge of compos-\ning multiple streams of notes that coordinate. One notable\ntheory is due to Fux [9] from the Baroque period, which in-\ntroduces species counterpoint as a pedagogical scheme to\ngradually introduce students to the complexity of counter-\npoint. In ﬁrst species counterpoint only one note is com-\nposed against every note in a given ﬁxed melody ( cantus\nﬁrmus ), with all notes bearing equal durations and the re-\nsulting vertical intervals consisting of only consonances.\nComputer music researchers have taken inspiration\nfrom this pedagogical scheme by ﬁrst teaching comput-\ners to write species counterpoint as opposed to full-ﬂedged\ncounterpoint. Farbood [7] uses Markov chains to capture\ntransition probabilities of different melodic and harmonic\ntransitions rules. Herremans [16,17] takes an optimization\napproach by writing down an objective function that con-\nsists of existing rules of counterpoint and using a variable\nneighbourhood search (VNS) algorithm to optimize it.\nJ.S. Bach chorales has been the main corpus in com-\nputer music that serves as a starting point to tackle full-\nﬂedged counterpoint. A wide range of approaches have\nbeen used to generate music in the style of Bach chorales,\nfor example rule-based and instance-based approaches\nsuch as Cope’s recombinancy method [6]. This method in-\nvolves ﬁrst segmenting existing Bach chorales into smaller\nchunks based on music theory, analyzing their function and\nstylistic signatures and then re-concatenating the chunks\ninto new coherent works. Other approaches range from\nconstraint-based [30] to statistical methods [5]. In addi-\ntion, [8] gives a comprehensive survey of AI methods used\nnot just for generating Bach chorales, but also algorithmic\ncomposition in general.\nSequence models such as HMMs and RNNs are nat-\nural choices for modeling music. Successful application\nof such models to polyphonic music often requires serial-\nizing or otherwise re-representing the music to ﬁt the se-\nquence paradigm. For instance, Liang in BachBot [27] se-\nrializes four-part Bach chorales by interleaving the parts,212 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017while Allan and Williams [1] construct a chord vocabu-\nlary. Boulanger et al. [4] adopt a piano roll representation,\na binary matrix xwhere xit= 1 iff some instrument is\nplaying pitch iat timet. To model the joint probability\ndistribution of the multi-hot pitch vector xt, they employ\na Restricted Boltzmann Machine (RBM [19, 32]) or Neu-\nral Autoregressive Distribution Estimator (NADE [25]) at\neach time step. Similarly Goel et al. [11] employ a Deep\nBelief Network [19] on top of an RNN.\nHadjeres et al. [14] instead employ an undirected\nMarkov model to learn pairwise relationships between\nneighboring notes up to a speciﬁed number of steps away\nin a score. Sampling involves Markov Chain Monte Carlo\n(MCMC) using the model as a Metropolis-Hastings (MH)\nobjective. The model permits constraints on the state space\nto support tasks such as melody harmonization. However,\nthe Markov assumption can limit the expressivity of the\nmodel.\nHadjeres and Pachet in DeepBach [13] model note pre-\ndictions by breaking down its full context into three parts,\nwith the past and the future modeled by stacked LSTMs\ngoing in the forward and backward directions respectively,\nand the present harmonic context modeled by a third neural\nnetwork. The three are then combined by a fourth neural\nnetwork and used in Gibbs sampling for generation.\nLattner et al. imposes higher-level structure by in-\nterleaving selective Gibbs sampling on a convolutional\nRBM [26] and gradient descent that minimizes cost to tem-\nplate piece on features such as self-similarity. This pro-\ncedure itself is wrapped in simulated annealing to ensure\nsteps do not lower the solution quality too much.\nWe opt for an orderless N ADE training procedure which\nenables us to train a mixture of all possible directed mod-\nels simultaneously. Finally, an approximate blocked Gibbs\nsampling procedure [40] allows fast generation from the\nmodel.\n3. MODEL\nWe employ machine learning techniques to obtain a gen-\nerative model of musical counterpoint in the form of pi-\nano rolls. Given a dataset of observed musical pieces\nx(1):::x(n)posited to come from some true distribution\np(x), we introduce a model p\u0012(x)with parameters \u0012.\nIn order to make p\u0012(x)close top(x), we maximize the\ndata log-likelihoodP\nilogp\u0012(x(i))(an approximation of\nEx\u0018p(x)logp\u0012(x)) by stochastic gradient descent.\nThe joint distribution p(x)overDvariables x1:::xD\nis often difﬁcult to model directly and hence we construct\nour modelp\u0012(x)from simpler factors. In the N ADE [25]\nframework, the joint p\u0012(x)is factorized autoregressively,\none variable at a time, according to some ordering o=\no1:::oD, such that\np\u0012(x) =Y\ndp\u0012(xodjxo<d): (1)\nFor example, it can be factorized in chronological order:\np\u0012(x) =p\u0012(x1)p\u0012(x2jx1):::p\u0012(xDjxD\u00001:::x1)(2)In general, N ADE permits any one ﬁxed ordering, and al-\nthough all orderings are equivalent from a theoretical per-\nspective, they differ in practice due to effects of optimiza-\ntion and approximation.\nInstead, we can train N ADE for all orderings osimulta-\nneously using the orderless N ADE [36] training procedure.\nThis procedure relies on the observation that, thanks to pa-\nrameter sharing, computing p\u0012(xod0jxo<d)for alld0\u0015d\nis no more expensive than computing it only for d0=d.2\nHence for a given oanddwe can simultaneously obtain\npartial losses for all orderings that agree with oup tod:\nL(x;o<d;\u0012) =\u0000X\nodlogp\u0012(xodjxo<d;o<d;od)(3)\nAn orderless N ADE model offers direct access to all dis-\ntributions of the form p\u0012(xijxC)conditioned on any set\nof contextual variables xC=xo<dthat might already be\nknown. This gives us a very ﬂexible generative model;\nin particular, we can use these conditional distributions to\ncomplete arbitrarily partial musical scores.\nTo train the model, we sample a training example xand\ncontextCsuch thatjCj\u0018U(1;D), and update \u0012based\non the gradient of the loss given by Equation 3. This loss\nconsists ofD\u0000d+ 1 terms, each of which corresponds\nto one ordering. To ensure all orderings are trained evenly\nwe must reweight the gradients by 1=(D\u0000d+ 1) . This\ncorrection, due to [36], ensures consistent estimation of the\njoint negative log-likelihood logp\u0012(x).\nIn this work, the model p\u0012(x)is implemented by a deep\nconvolutional neural network [23]. This choice is moti-\nvated by the locality of contrapuntal rules and their near-\ninvariance to translation, both in time and in pitch space.\nWe represent the music as a stack of piano rolls encoded\nin a binary three-dimensional tensor x2 f0;1gI\u0002T\u0002P.\nHereIdenotes the number of instruments, Tthe number\nof time steps, Pthe number of pitches, and xi;t;p= 1 iff\ntheith instrument plays pitch pat timet. We will assume\neach instrument plays exactly one pitch at a time, that is,P\npxi;t;p= 1for alli;t.\nOur focus is on four-part Bach chorales as used in prior\nwork [1,4,11,14,27]. Hence we assume I= 4throughout.\nWe constrain ourselves to only the range that appears in\nour training data (MIDI pitches 36 through 88). Time is\ndiscretized at the level of 16th notes for similar reasons.\nTo curb memory requirements, we enforce T= 128 by\nrandomly cropping the training examples.\nGiven a training example x\u0018p(x), we present the\nmodel with values of only a strict subset of its elements\nxC=fx(i;t)j(i;t)2Cgand ask it to reconstruct its\ncomplement x:C. The input h02f0;1g2I\u0002T\u0002Pis ob-\ntained by masking the piano rolls xto obtain the context\nxCand concatenating this with the corresponding mask:\nh0\ni;t;p= 1(i;t)2Cxi;t;p (4)\nh0\nI+i;t;p= 1(i;t)2C (5)\n2Herexo<dis used as shorthand for variables xo1: : :xod\u00001that\noccur earlier in the ordering.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 213where the time and pitch dimensions are treated as spa-\ntial dimensions to convolve over. Each instrument’s piano\nrollh0\niand mask h0\nI+iis treated as a separate channel and\nconvolved independently.\nWith the exception of the ﬁrst and ﬁnal layers, all con-\nvolutions preserve the size of the hidden representation.\nThat is, we use “same” padding throughout and all acti-\nvations have the same number of channels H, such that\nhl2RH\u0002T\u0002Pfor all 1< l < L . Throughout our ex-\nperiments we used L= 64 layers andH= 128 chan-\nnels. After each convolution we apply batch normaliza-\ntion [21] (denoted by BN(\u0001)) with statistics tied across\ntime and pitch. Batch normalization rescales activations\nat each layer to have mean \fand standard deviation \r,\nwhich greatly improves optimization. After every second\nconvolution, we introduce a skip connection from the hid-\nden state two levels below to reap the beneﬁts of residual\nlearning [15].\nal= BN( Wl\u0003hl\u00001;\rl;\fl) (6)\nhl=8\n<\n:ReLU( al+hl\u00002)\nif3<l<L\u00001andlmod 2 = 0\nReLU( al)otherwise\nhL=aL(7)\nThe ﬁnal activations hL2RI\u0002T\u0002Pare passed through\nthe softmax function to obtain predictions for the pitch at\neach instrument/time pair:\np\u0012(xi;t;pjxC;C) =exp(hL\ni;t;p)P\npexp(hL\ni;t;p)(8)\nThe loss function from Equation 3 is then given by\nL(x;C;\u0012) =\u0000X\n(i;t)=2Clogp\u0012(xi;tjxC;C) (9)\n=\u0000X\n(i;t)=2CX\npxi;t;plogp\u0012(xi;t;pjxC;C)\nwherep\u0012denotes the probability under the model with pa-\nrameters\u0012=W1;\r1;\f1;:::; WL\u00001;\rL\u00001;\fL\u00001. We\ntrain the model by minimizing\nEx\u0018p(x)EC\u0018p(C)1\nj:CjL(x;C;\u0012) (10)\nwith respect to \u0012using stochastic gradient descent with\nstep size determined by Adam [22]. The expectations are\nestimated by sampling piano rolls xfrom the training set\nand drawing a single context Cper sample.\n4. EVALUATION\nThe log-likelihood of a given example is computed ac-\ncording to Algorithm 1 by repeated application of Equa-\ntion 8. Evaluation occurs one frame at a time, within which\nthe model conditions on its own predictions and does not\nsee the ground truth. Unlike notewise teacher-forcing,\nwhere the ground truth is injected after each prediction,\nthe framewise evaluation is thus sensitive to accumulationof error. This gives a more representative measure of qual-\nity of the generative model. For each example, we repeat\nthe evaluation process a number of times to average over\nmultiple orderings, and ﬁnally average across frames and\nexamples. For chronological evaluation, we draw only or-\nderings that have the tls in increasing order.\nAlgorithm 1 Framewise log-likelihood evaluation\nGiven a piano roll x\nLm;i;t 0for allm;i;t\nformultiple orderings m= 0:::M do\nC ;,bx x\nSample an ordering t1;t2:::tTover frames\nforl= 0:::T do\nSample an ordering i1;i2:::iIover instruments\nfork= 0:::I do\n\u0019p p\u0012(xik;tl;pjbxC;C)for allp\nLm;ik;tl P\npxik;tl;plog\u0019p\nbxik;tl\u0018Cat(P;\u0019)\nC C[(ik;tl)\nend for\nbxC xC\nend for\nend for\nreturn\u00001\nTP\ntlog1\nMP\nmexpP\niLm;i;t\n5. SAMPLING\nWe can sample from the model using the orderless N ADE\nancestral sampling procedure, in which we ﬁrst sample an\nordering and then sample variables one by one according\nto the ordering. However, we ﬁnd that this yields poor\nsamples, and we propose instead to use Gibbs sampling.\n5.1 Orderless N ADESampling\nSampling according to orderless N ADE involves ﬁrst ran-\ndomly choosing an ordering and then sampling variables\none by one according to the chosen ordering. We use an\nequivalent procedure in which we arrive at a random or-\ndering by at each step randomly choosing the next variable\nto sample. We start with an empty (zero everywhere) pi-\nano roll x0and empty context C0and populate them itera-\ntively by the following process. We feed the piano roll xs\nand context Csinto the model to obtain a set of categori-\ncal distributions p\u0012(xi;tjxs\nCs;Cs)for(i;t)=2Cs. As the\nxi;tare not conditionally independent, we cannot simply\nsample from these distributions independently. However,\nif we sample from one of them, we can compute new con-\nditional distributions for the others. Hence we randomly\nchoose one (i;t)s+1=2Csto sample from, and let xs+1\ni;t\nequal the one-hot realization. Augment the context with\nCs+1=Cs[(i;t)s+1and repeat until the piano roll is\npopulated. This procedure is easily generalized to tasks\nsuch as melody harmonization and partial score comple-\ntion by starting with a nonempty piano roll.\nUnfortunately, samples thus generated are of low qual-\nity, which we surmise is due to accumulation of errors.214 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Model Temporal resolution\nquarter eighth sixteenth\nNADE [4] 7.19\nRNN-RBM [4] 6.27\nRNN-N ADE [4] 5.56\nRNN-N ADE (our implementation) 5.03 3.78 2.05\nCOCONET (chronological) 7:79\u00060:09 4:21\u00060:05 2:22\u00060:03\nCOCONET (random) 5:03\u00060:06 1:84\u00060:02 0:57\u00060:01\nTable 1 . Framewise negative log-likelihoods (NLLs) on the Bach corpus. We compare against [4], who used quarter-note\nresolution. We also compare on higher temporal resolutions (eighth notes, sixteenth notes), against our own reimplemen-\ntation of RNN-N ADE. C OCONET is an instance of orderless N ADE, and as such we evaluate it on random orderings.\nHowever, the baselines support only chronological frame ordering, and hence we evaluate our model in this setting as well.\nThis is a well-known weakness of autoregressive mod-\nels. [3, 20, 24, 39] While the model provides condition-\nalsp\u0012(xi;tjxC;C)for all (i;t)=2C, some of these con-\nditionals may be better modeled than others. We sus-\npect in particular those conditionals used early on in the\nprocedure, for which the context Cconsists of very few\nvariables. Moreover, although the model is trained to be\norder-agnostic, different orderings invoke different distri-\nbutions, which is another indication that some conditionals\nare poorly learned. We test this hypothesis in Section 6.2.\n5.2 Gibbs Sampling\nTo remedy this, we allow the model to revisit its choices:\nwe repeatedly mask out some part of the piano roll and\nthen repopulate it. This is a form of blocked Gibbs sam-\npling [28]. Blocked sampling is crucial for mixing, as\nthe high temporal resolution of our representation causes\nstrong correlations between consecutive notes. For in-\nstance, without blocked sampling, it would take many steps\nto snap out of a long-held note. Similar considerations hold\nfor the Ising model from statistical mechanics, leading to\nthe Swendsen-Wang algorithm [33] in which large clusters\nof variables are resampled at once.\nWe consider two strategies for resampling a given\nblock of variables: ancestral sampling and indepen-\ndent sampling. Ancestral sampling invokes the orderless\nNADE sampling procedure described in Section 5.1 on the\nmasked-out portion of the piano roll. Independent sam-\npling simply treats the masked-out variables x:Cas inde-\npendent given the context xC.\nUsing independent blocked Gibbs to sample from a\nNADE model has been studied by [40], who propose to use\nan annealed masking probability \u000bn= max(\u000bmin;\u000bmax\u0000\nn(\u000bmax\u0000\u000bmin)=(\u0011N))for some minimum and maximum\nprobabilities \u000bmin;\u000bmax, total number of Gibbs steps N\nand fraction \u0011of time spent before settling onto the mini-\nmum probability \u000bmin. Initially, when the masking proba-\nbility is high, the chain mixes fast but samples are poor due\nto independent sampling. As \u000bndecreases, the blocked\nGibbs process with independent resampling approaches\nstandard Gibbs where one variable at a time is resampled,\nthus amortizing the effects of independent sampling. Nis\na hyperparameter which as a rule of thumb we set equal toIT; it can be set lower than that to save computation at a\nslight loss of sample quality.\n[40] treat independent blocked Gibbs as a cheap ap-\nproximation to ancestral sampling. Whereas plain an-\ncestral sampling (5.1) requires O(IT)model evaluations,\nancestral blocked Gibbs requires a prohibitive O(ITN )\nmodel evaluations and independent Gibbs requires only\nO(N), whereNcan be chosen to be less than IT. More-\nover, we ﬁnd that independent blocked Gibbs sampling in\nfact yields better samples than plain ancestral sampling.\n6. EXPERIMENTS\nWe evaluate our approach on a popular corpus of four-part\nBach chorales. While the literature features many variants\nof this dataset [1, 4, 14, 27], we report results on that used\nby [4]. As the quarter-note temporal resolution used by [4]\nis frankly too coarse to accurately convey counterpoint, we\nalso evaluate on eighth-note and sixteenth-note quantiza-\ntions of the same data.\nIt should be noted that quantitative evaluation of gener-\native models is fundamentally hard [34]. The gold standard\nfor evaluation is qualitative comparison by humans, and we\ntherefore report human evaluation results as well.\n6.1 Data Log-likelihood\nTable 4 compares the framewise log-likelihood of the test\ndata under variants of our model and those reported in [4].\nWe ﬁnd that the temporal resolution has a dramatic inﬂu-\nence on the performance, which we suspect is an artifact\nof the performance metric. The log-likelihood is evalu-\nated by teacher-forcing, that is, the prediction of a frame is\nconditioned on the ground truth of all previously predicted\nframes. As temporal resolution increases, chord changes\nbecome increasingly rare, and the model is increasingly\nrewarded for simply holding notes over time.\nWe evaluate C OCONET on both chronological and ran-\ndom orderings, in both cases averaging likelihoods across\nan ensemble of M= 5 orderings. The chronological or-\nderings differ only in the ordering of instruments within\neach frame. We see in Table 4 that fully random orderings\nlead to signiﬁcantly better performance. We believe the\nmembers of the more diverse random ensemble are moreProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 215mutually complementary. For example, a forward ordering\nis uncertain at the beginning of a piece and more certain to-\nward the end, whereas a backward ordering is more certain\nat the beginning and less certain toward the end.\n6.2 Sample Quality\nIn Section 5 we conjectured that the low quality of\nNADE samples is due to poorly modeled conditionals\np\u0012(xi;tjxC;C)whereCis small. We test this hypoth-\nesis by evaluating the likelihood under the model of sam-\nples generated by the ancestral blocked Gibbs procedure\nwithCchosen according to independent Bernoulli vari-\nables. When we set the inclusion probability \u001ato 0, we\nobtain N ADE. Increasing \u001aincreases the expected context\nsizejCj, which should yield better samples if our hypothe-\nsis is true. The results shown in Table 6.2 conﬁrm that this\nis the case. For these experiments, we used sample length\nT= 32 time steps and number of Gibbs steps N= 100 .\nSampling scheme Framewise NLL\nAncestral Gibbs, \u001a= 0:00(NADE) 1:09\u00060:06\nAncestral Gibbs, \u001a= 0:05 1 :08\u00060:06\nAncestral Gibbs, \u001a= 0:10 0 :97\u00060:05\nAncestral Gibbs, \u001a= 0:25 0 :80\u00060:04\nAncestral Gibbs, \u001a= 0:50 0 :74\u00060:04\nIndependent Gibbs [40] 0:52\u00060:01\nTable 2 . Mean (\u0006SEM) NLL under model of uncondi-\ntioned samples generated from model by various schemes.\nFigure 2 shows the convergence behavior of the various\nGibbs procedures, averaged over 100 runs. We see that for\nlow values of \u001a(smallC), the chains hardly make progress\nbeyond N ADE in terms of likelihood. Higher values of \u001a\n(largeC) enable the model to get off the ground and reach\nsigniﬁcantly better likelihood.\n0 20 40 60 80 100 120\n# of Gibbs steps0.50.751.01.251.51.752.0Chord-wise NLL\nComparing sample quality\nBernoulli(0.05)\nBernoulli(0.10)\nBernoulli(0.25)Bernoulli(0.50)\nNADE\nFigure 2 . Likelihood under the model for ancestral Gibbs\nsamples obtained with various context distributions p(C).\nNADE (Bernoulli(0:00)) is included for reference.\n0 5 10 15 20 25 30 35\n# of winsNADEIndependent\nGibbsBachSampling schemeFigure 3 . Human evaluations from MTurk on how many\ntimes a sampling procedure or Bach is perceived as more\nBach-like. Error bars show the standard deviation of a bi-\nnomial distribution ﬁtted to each’s binary win/loss counts.\n6.3 Human Evaluations\nTo further compare the sample quality of different sam-\npling procedures, we carried out a listening test on Ama-\nzon’s Mechanical Turk (MTurk). The procedures in-\nclude orderless N ADE ancestral sampling and indepen-\ndent Gibbs [40], with each we generate four uncondi-\ntioned samples of eight-measure lengths from empty piano\nrolls. To have an absolute reference for the quality of sam-\nples, we include ﬁrst eight measures of four random Bach\nchorale pieces from the validation set. Each fragment lasts\nthirty-four seconds after synthesis.\nFor each MTurk hit, participants are asked to rate on a\nLikert scale which of the two random samples they per-\nceive as more Bach-like. A total of 96 ratings were col-\nlected, with each source involved in 64 (=96*2/3) pair-\nwise comparisons. Figure 3 shows the number of times\neach source was perceived as closer to Bach’s style. We\nperform a Kruskal-Wallis H test on the ratings, \u001f2(2) =\n12:23;p < 0:001, showing there are statistically signiﬁ-\ncant differences between models. A post-hoc analysis us-\ning the Wilcoxon signed-rank test with Bonferroni correc-\ntion showed that participants perceived samples from in-\ndependent Gibbs as more Bach-like than ancestral sam-\npling (N ADE),p < 0:05=3. This conﬁrms the loglikeli-\nhood comparisons on sample quality in 6.2 that indepen-\ndent Gibbs produces better samples. There was also a sig-\nniﬁcance difference between Bach and ancestral samples\nbut not between Bach and independent Gibbs.\n7. CONCLUSION\nWe introduced a convolutional approach to modeling mu-\nsical scores based on the orderless N ADE [35] framework.\nOur experiments show that the N ADE ancestral sampling\nprocedure yields poor samples, which we have argued is\nbecause some conditionals are not captured well by the\nmodel. We have shown that sample quality improves sig-\nniﬁcantly when we use blocked Gibbs sampling to itera-\ntively rewrite parts of the score. Moreover, annealed in-\ndependent blocked Gibbs sampling as proposed by [40] is\nnot only faster but in fact produces better samples.216 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Acknowledgments\nWe thank Kyle Kastner, Guillaume Alain, Gabriel Huang,\nCurtis (Fjord) Hawthorne, the Google Brain Magenta\nteam, as well as Jason Freidenfelds for helpful feedback,\ndiscussions, suggestions and support. We also thank Cal-\ncul Qu ´ebec and Compute Canada for computational sup-\nport.\n8. REFERENCES\n[1] Moray Allan and Christopher KI Williams. Harmon-\nising chorales by probabilistic inference. Advances\nin neural information processing systems , 17:25–32,\n2005.\n[2] Leonard E Baum and Ted Petrie. Statistical inference\nfor probabilistic functions of ﬁnite state markov chains.\nThe annals of mathematical statistics , 37(6):1554–\n1563, 1966.\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. Scheduled sampling for sequence pre-\ndiction with recurrent neural networks. In Advances in\nNeural Information Processing Systems , pages 1171–\n1179, 2015.\n[4] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. Interna-\ntional Conference on Machine Learning , 2012.\n[5] Darrell Conklin. Music generation from statistical\nmodels. In Proceedings of the AISB 2003 Symposium\non Artiﬁcial Intelligence and Creativity in the Arts and\nSciences , pages 30–35. Citeseer, 2003.\n[6] David Cope. Computers and musical style. 1991.\n[7] Mary Farbood and Bernd Sch ¨oner. Analysis and syn-\nthesis of palestrina-style counterpoint using markov\nchains. In ICMC , 2001.\n[8] Jose D Fern ´andez and Francisco Vico. Ai methods\nin algorithmic composition: A comprehensive survey.\nJournal of Artiﬁcial Intelligence Research , 48:513–\n582, 2013.\n[9] Johann Joseph Fux. The study of counterpoint from\nJohann Joseph Fux’s Gradus ad Parnassum . Number\n277. WW Norton & Company, 1965.\n[10] Leon A Gatys, Alexander S Ecker, and Matthias\nBethge. A neural algorithm of artistic style. arXiv\npreprint arXiv:1508.06576 , 2015.\n[11] Kratarth Goel, Raunaq V ohra, and JK Sahoo. Poly-\nphonic music generation by modeling temporal de-\npendencies using a rnn-dbn. In International Confer-\nence on Artiﬁcial Neural Networks , pages 217–224.\nSpringer, 2014.[12] Ian Goodfellow, Mehdi Mirza, Aaron Courville, and\nYoshua Bengio. Multi-prediction deep boltzmann ma-\nchines. In Advances in Neural Information Processing\nSystems , pages 548–556, 2013.\n[13] Ga ¨etan Hadjeres and Franc ¸ois Pachet. Deepbach: a\nsteerable model for bach chorales generation. arXiv\npreprint arXiv:1612.01010 , 2016.\n[14] Ga ¨etan Hadjeres, Jason Sakellariou, and Franc ¸ois Pa-\nchet. Style imitation and chord invention in poly-\nphonic music with exponential families. arXiv preprint\narXiv:1609.05152 , 2016.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition.\narXiv preprint arXiv:1512.03385 , 2015.\n[16] Dorien Herremans and Kenneth S ¨orensen. Composing\nﬁrst species counterpoint with a variable neighbour-\nhood search algorithm. Journal of Mathematics and the\nArts, 6(4):169–189, 2012.\n[17] Dorien Herremans and Kenneth S ¨orensen. Composing\nﬁfth species counterpoint music with a variable neigh-\nborhood search algorithm. Expert systems with appli-\ncations , 40(16):6427–6437, 2013.\n[18] Lejaren A Hiller Jr and Leonard M Isaacson. Musi-\ncal composition with a high speed digital computer. In\nAudio Engineering Society Convention 9 . Audio Engi-\nneering Society, 1957.\n[19] Geoffrey E Hinton, Simon Osindero, and Yee-Whye\nTeh. A fast learning algorithm for deep belief nets.\nNeural computation , 18(7):1527–1554, 2006.\n[20] Ferenc Husz ´ar. How (not) to train your generative\nmodel: Scheduled sampling, likelihood, adversary?\narXiv preprint arXiv:1511.05101 , 2015.\n[21] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[22] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in neural information\nprocessing systems , pages 1097–1105, 2012.\n[24] Alex M Lamb, Anirudh Goyal ALIAS PARTH\nGOYAL, Ying Zhang, Saizheng Zhang, Aaron C\nCourville, and Yoshua Bengio. Professor forcing: A\nnew algorithm for training recurrent networks. In Ad-\nvances In Neural Information Processing Systems ,\npages 4601–4609, 2016.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 217[25] Hugo Larochelle and Iain Murray. The neural autore-\ngressive distribution estimator. In AISTATS , volume 1,\npage 2, 2011.\n[26] Stefan Lattner, Maarten Grachten, and Gerhard\nWidmer. Imposing higher-level structure in poly-\nphonic music generation using convolutional restricted\nboltzmann machines and constraints. arXiv preprint\narXiv:1612.04742 , 2016.\n[27] Feynman Liang. Bachbot: Automatic composition in\nthe style of bach chorales. Masters thesis, University\nof Cambridge , 2016.\n[28] Jun S Liu. The collapsed gibbs sampler in bayesian\ncomputations with applications to a gene regulation\nproblem. Journal of the American Statistical Associ-\nation , 89(427):958–966, 1994.\n[29] Alexander Mordvintsev, Christopher Olah, and Mike\nTyka. Inceptionism: Going deeper into neural net-\nworks, 2015.\n[30] Franc ¸ois Pachet and Pierre Roy. Musical harmoniza-\ntion with constraints: A survey. Constraints , 6(1):7–19,\n2001.\n[31] David E Rumelhart, Geoffrey E Hinton, and\nRonald J Williams. Learning representations by back-\npropagating errors. Cognitive modeling , 5(3):1, 1988.\n[32] Paul Smolensky. Information processing in dynamical\nsystems: Foundations of harmony theory. Technical re-\nport, DTIC Document, 1986.\n[33] Robert H Swendsen and Jian-Sheng Wang. Nonuniver-\nsal critical dynamics in monte carlo simulations. Phys-\nical review letters , 58(2):86, 1987.\n[34] Lucas Theis, A ¨aron van den Oord, and Matthias\nBethge. A note on the evaluation of generative mod-\nels.arXiv preprint arXiv:1511.01844 , 2015.\n[35] Benigno Uria, Marc-Alexandre C ˆot´e, Karol Gre-\ngor, Iain Murray, and Hugo Larochelle. Neural au-\ntoregressive distribution estimation. arXiv preprint\narXiv:1605.02226 , 2016.\n[36] Benigno Uria, Iain Murray, and Hugo Larochelle. A\ndeep and tractable density estimator. In ICML , pages\n467–475, 2014.\n[37] Aaron van den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499 , 2016.\n[38] Aaron van den Oord, Nal Kalchbrenner, and Koray\nKavukcuoglu. Pixel recurrent neural networks. In Pro-\nceedings of The 33rd International Conference on Ma-\nchine Learning , pages 1747–1756, 2016.[39] Arun Venkatraman, Martial Hebert, and J Andrew Bag-\nnell. Improving multi-step prediction of learned time\nseries models. In AAAI , pages 3024–3030, 2015.\n[40] Li Yao, Sherjil Ozair, Kyunghyun Cho, and Yoshua\nBengio. On the equivalence between deep nade and\ngenerative stochastic networks. In Joint European Con-\nference on Machine Learning and Knowledge Discov-\nery in Databases , pages 322–336. Springer, 2014.218 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Mining Labeled Data from Web-Scale Collections for Vocal Activity Detection in Music.",
        "author": [
            "Eric J. Humphrey",
            "Nicola Montecchio",
            "Rachel M. Bittner",
            "Andreas Jansson 0001",
            "Tristan Jehan"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417769",
        "url": "https://doi.org/10.5281/zenodo.1417769",
        "ee": "https://zenodo.org/records/1417769/files/HumphreyMBJJ17.pdf",
        "abstract": "This work demonstrates an approach to generating strongly labeled data for vocal activity detection by pairing instru- mental versions of songs with their original mixes. Though such pairs are rare, we find ample instances in a massive music collection for training deep convolutional networks at this task, achieving state of the art performance with a fraction of the human effort required previously. Our error analysis reveals two notable insights: imperfect systems may exhibit better temporal precision than human anno- tators, and should be used to accelerate annotation; and, machine learning from mined data can reveal subtle biases in the data source, leading to a better understanding of the problem itself. We also discuss future directions for the de- sign and evolution of benchmarking datasets to rigorously evaluate AI systems.",
        "zenodo_id": 1417769,
        "dblp_key": "conf/ismir/HumphreyMBJJ17",
        "keywords": [
            "strongly labeled data",
            "vocal activity detection",
            "instrumental versions",
            "original mixes",
            "deep convolutional networks",
            "state of the art performance",
            "massive music collection",
            "human effort",
            "imperfect systems",
            "subtle biases"
        ],
        "content": "MINING LABELED DATA FROM WEB-SCALE COLLECTIONS FOR\nVOCAL ACTIVITY DETECTION IN MUSIC\nEric J. Humphrey1Nicola Montecchio1Rachel Bittner1;2\nAndreas Jansson1;3Tristan Jehan1\n1Spotify, New York, USA\n2Music, Audio & Research Lab (MARL), New York University, USA\n3City University, London, UK\nfejhumphrey, venice, rachelbittner, andreasj, tjehan g@spotify.com\nABSTRACT\nThis work demonstrates an approach to generating strongly\nlabeled data for vocal activity detection by pairing instru-\nmental versions of songs with their original mixes. Though\nsuch pairs are rare, we ﬁnd ample instances in a massive\nmusic collection for training deep convolutional networks\nat this task, achieving state of the art performance with a\nfraction of the human effort required previously. Our error\nanalysis reveals two notable insights: imperfect systems\nmay exhibit better temporal precision than human anno-\ntators, and should be used to accelerate annotation; and,\nmachine learning from mined data can reveal subtle biases\nin the data source, leading to a better understanding of the\nproblem itself. We also discuss future directions for the de-\nsign and evolution of benchmarking datasets to rigorously\nevaluate AI systems.\n1. INTRODUCTION\nOver the last few years, the ubiquity of cheap computa-\ntional power and high quality open-source machine learn-\ning software toolkits has grown considerably. This trend\nunderscores the fact that attaining state-of-the-art solutions\nvia machine learning increasingly depends more on the\navailability of large quantities of data than the sophisti-\ncation of the approach itself. Thus, when tackling less\ntraditional or altogether novel problems, machine learning\npractitioners often choose between two paths to acquiring\ndata: manually create (or curate) a dataset, or attempt to\nleverage existing resources.\nBoth approaches present unique challenges. Curation\nis necessary when precise information is required or in-\nsufﬁcient data are available, but can incur large costs in\nboth time and money. Alternatively, “mining” data – re-\ncovering useful information that occurs serendipitously in\ndifferent contexts – can result in large datasets with far\nc\rEric J. Humphrey, Nicola Montecchio, Rachel Bittner,\nAndreas Jansson, Tristan Jehan. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: Eric\nJ. Humphrey, Nicola Montecchio, Rachel Bittner, Andreas Jansson, Tris-\ntan Jehan. “Mining Labeled Data from Web-Scale Collections for V ocal\nActivity Detection in Music”, 18th International Society for Music Infor-\nmation Retrieval Conference, Suzhou, China, 2017.less effort, e.g., recovering labels from the text around an\nimage. While this information is typically generated as a\nby-product of some other pre-existing human activity and\nprone to both noise and bias, recent machine learning re-\nsearch has managed to use this approach to great effect [5].\nWith the continued growth of digital music services, vo-\ncal activity detection (V AD) is a task of increasing impor-\ntance. Robust V AD is a key foundational technology that\ncould power or simplify a number of end-user applica-\ntions, such as vocalist similarity, music recommendation,\nartist identiﬁcation, source separation, or lyrics transcrip-\ntion. Despite previous research, the state of the art contin-\nues to advance with diminishing returns, rendering V AD\nan unsolved problem with considerable potential.\nGiven the dominance of data-driven methods in ma-\nchine learning, it stands to reason that data scarcity may\nbe contributing to the apparent ceiling in the performance\nof V AD algorithms. Modest progress has been made to-\nward increasing the size of labeled datasets, limiting the\nefﬁcacy of modern approaches, e.g., deep learning. Ef-\nforts to leverage strongly labeled datasets have converged\nto hundreds of observations [1,13,15,16], with which com-\nplex methods have been explored [9, 10, 18]. Recent re-\nsearch succeeded in curating a private dataset of 10 k, 30\nsecond weakly labeled clips, e.g., “completely instrumen-\ntal” or “contains singing voice”, using this dataset to train\nconvolutional neural networks [17].\nIn short, V AD research remains largely dependent on\nsustained human involvement in sourcing labeled data, but\nthis approach struggles to scale. Here, we propose leverag-\ning a huge, untapped resource in modern music to circum-\nvent this challenge: the “instrumental version”, i.e.,a song\nin which the vocals have been omitted. The goal of this\nwork is thus the exploration of this opportunity, achieved\nin four steps: mine original-instrumental pairs from a mas-\nsive catalogue of music content; estimate time-varying vo-\ncal density given corresponding tracks; exploit this signal\nto train deep neural networks to detect singing voice; and\nunderstand the effects of this data source on the resulting\nmodels.7092. DATA GENERATION\nIn Western popular music, a song’s arrangement often re-\nvolves around a lead vocalist, accompanied by instruments\nsuch as guitar, drums, bass, etc. It is not uncommon for an\nartist to also release an “instrumental” version of the same\nsong (to be used for e.g., remixes or karaoke), in which the\nprimary difference between it and the corresponding “orig-\ninal” recording is the absence of vocals.1In principle, the\ndifference between these two sound recordings should be\nhighly correlated with vocal activity, which would provide\na ﬁne-grained signal for training machine learning models.\nHowever, to exploit this property at scale, it is necessary to\nidentify and align pairs of original recordings and match-\ning instrumental versions automatically .\nWe outline a three-step approach toward mining\nstrongly labeled instances of singing voice from a music\ncatalogue: identify original-instrumental pairs from track\nmetadata; estimate a vocal density signal for the original\ntrack, given its instrumental; draw positive observations\nfrom an original track as a function of estimated vocal den-\nsity.\n2.1 Selection of Matching Recordings\nWe search the full Spotify catalogue, a set of tens of mil-\nlions of commercially recorded tracks, for paired versions\nusing a heuristic based on track metadata. A pair of tracks\n(A;B)are marked as (original;instrumental )if:\n\u000fAandBare recorded by the same artist.\n\u000f“instrumental” does not appear in the title of A.\n\u000f“instrumental” does appear in the title of B.\n\u000fThe titles of AandBarefuzzy matches .\n\u000fThe track durations differ by less than 10 seconds.\nFuzzy matching is performed on track titles by ﬁrst\nlatinizing non-ASCII characters, removing parenthesized\ntext, and ﬁnally converting to lower-case; this yields about\n164k instrumental tracks. Note that this is a many-to-many\nmapping, as an original version can point to several differ-\nent instrumentals, and vice versa.\nA tiny subset of this content is manually reviewed to\ncheck for quality, and we ﬁnd roughly 1 in 10 tracks to be\na mismatched pair: the majority of errors are due to instru-\nmental tracks that appear on multiple albums, such as com-\npilations or movie soundtracks, but are only tagged as such\nin some contexts. An open-source audio ﬁngerprinting al-\ngorithm is used to remove suspect pairs from the candidate\nset [6]. Sequences of codes for tracks are extracted, and\ntrack pairs are discarded as a function of Jaccard similarity\nif code sequences do not overlap sufﬁciently (an erroneous\nfuzzy metadata match) or overlap too much (the tracks in\nthe pair being both instrumental or vocal). Finally, re-\ndundant associations from this mapping are removed, so\nthat each original track is linked to only one instrumental\n1Though other differences in signal characteristics may occur due to\nproduction effects, e.g., mastering, compression, equalization, these are\nnot considered here.track. Overall this process yields roughly 24 ktracks, or\n12koriginal-instrumental pairs, totaling some 1500 hours\nof audio.\n2.2 Estimating Vocal Density\nLetTOandTIdenote two recordings, corresponding to\nan “original” and “instrumental” version, respectively. A\nTime-Frequency Representation (TFR) is computed for\nboth tracks, respectively XOandXI. Subsequently, the\nTFRs are aligned to estimate time-varying vocal density.\nIn this work a Constant-Q Transform (CQT) [3] is cho-\nsen for its complementary relationship between convolu-\ntional neural networks and music audio; the CQT uses a\nlogarithmic frequency scale that linearizes pitch, allowing\nnetworks to learn pitch-invariant features as a result [8].\nThe frequency range of the transform is constrained to\nthe human vocal range, i.e.,E2 - E7 (5 octaves, spanning\n82.4-2637 Hz), and a moderately high resolution is em-\nployed, with 36 bins per octave and 32 frames per second.\nLogarithmic compression is applied pointwise to the TFR.\nThe pair of TFRs (XO;XI)undergoes a feature dimen-\nsionality reduction via Principal Component Analysis2,\nproducing (ZO;ZI); based on empirical ﬁndings, k= 20\ncomponents were found to yield good results. This step\nnot only provides an increase in computational efﬁciency\nin subsequent processing steps, but also affords a useful\ndegree of invariance because of the lower dimensionality.\nThe transformed sequences are then aligned using Dy-\nnamic Time Warping (DTW), yielding two sequences,\nnO;nI, of indices over the original and instrumental song,\nrespectively [14]. This allows us to recover points in time\nfrom both a full and instrumental mix where the back-\nground musical content is roughly identical.\nUsing these indices, the CQT spectra (XO;XI)are re-\nsampled to equivalent shapes, and the half-wave rectiﬁed\ndifference between log-magnitude spectra yields the fol-\nlowing residual:\n\u000ej;k= max\u0000\n0;logjXO\nnO\nj;k+ 1j\u0000logjXI\nnI\nj;k+ 1j\u0001\n(1)\nIn the ideal case, where any difference is due entirely\nto vocals, this residual represents the vocal CQT spectra,\nand will behave like a smooth contour through succes-\nsive time-frequency bins. Practically, however, there will\nlikely be other sources of residual energy, due to subopti-\nmal alignment or true signal differences. To best charac-\nterize contour-like residuals, we normalize the spectral en-\nergy in each time frame and apply the Viterbi algorithm to\ndecode the most likely path through the residual spectra;\nthis step is inspired by previous work on tracking funda-\nmental frequency in a time-frequency activation map [12].\nEmpirically we ﬁnd this process far more robust to residual\nnoise than simpler aggregation schemes, such as summing\nenergy over frequency.\n2We are not interested in learning a general transform; the Principal\nComponents of each pair of tracks are computed independently of the\noverall dataset.710 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Original (i)\n Instrumental (ii)\n Residual Trace (iii) Vocal Density (iv)Figure 1 . The intermediate stages in estimating vocal\ndensity from an original-instrumental pair of recordings,\nshowing (i) the original and (ii) instrumental CQT spectra,\n(iii) the residual with a trace of its fundamental, and (iv)\nthe estimated vocal density over time.\nThe amplitude of this time-frequency path, \u001a, provides\nan estimate of vocal density ,\u001e, the likelihood that vocals\nare present in the original recording,TO, as a function\nof time. Finally, we forwards-backwards ﬁlter \u001ewith a\nHanning window ( L= 15 ), to both smooth and dilate the\ndensity signal to encompass vocal onsets and offsets. The\nstages of this process are pictured in Figure 1.\n2.3 Sampling of Positive and Negative Observations\nHaving estimated where vocals likely occur in a piece of\naudio, we turn our attention to how this information is\nutilized for supervised training. We highlight that track-\nlevel metadata presents a multiple-instance learning prob-\nlem, where each recording can be understood as a bagof\nsamples with a single binary label: “vocal” if it contains\nat least one positive sample, or “non-vocal” if all samples\nare negative. In this setting, positively labeled bags are in-\nherently noisy, with some unknown percentage of negative\nsamples effectively mislabeled as a result. To address this\nissue, the vocal density estimate is used to reweight the\ncontributions of samples drawn from positive bags.\nAn estimator is trained by drawing positive ( Y= 1) and\nnegative (Y= 0) samples from original and instrumental\ntracks, with equal frequency. Negative samples are drawn\nuniformly from instrumental tracks, while positive samples\nare drawn as a function of the vocal density \u001e. To smoothlyinterpolate between a uniform distribution and the vocal\ndensity estimate over positive samples, two parameters are\nintroduced, a threshold, \u001c, and a compression factor, \u000f:\nPr(XO\nnjY= 1)/(\n\u001e\u000f\nn\u001en\u0015\u001c\n0 otherwise(2)\nHere, we are interested in exponentials in the range of\n0< \u000f < 1, which ﬂatten the density function. Note that\n\u000f= 0;\u001c= 0 corresponds to uniform sampling over time,\nand is equivalent to the weakly labeled setting, i.e.,a label\napplies equally to all samples.\nAs a ﬁnal consideration, we highlight that the original\nand instrumental recordings are aligned in the course of\ncomputing the vocal density estimate. Therefore, it is pos-\nsible to draw correlated positive-negative pairs from both\nthe original and instrumental tracks corresponding to the\nsame point in time, a sampling condition we refer to as\nentanglement ,\u00102fTrue;Falseg. One would expect that\nthese paired samples live near the decision boundary, being\nnear-neighbors in the input space but belonging to differ-\nent classes, and we are interested in exploring how training\nwith entangled pairs may affect model behavior.\n3. SYSTEM DESIGN\n3.1 Previous Approaches\nThe majority of V AD research follows a similar architec-\nture: short-time observations are fed to a classiﬁer, each\nobservation is assigned to either a vocal or a non-vocal\nclass, and optionally post-processing is applied to elimi-\nnate spurious predictions. Early work uses “the acoustic\nclassiﬁer of a speech recognizer as a detector for speech-\nlike sounds” to feed an Artiﬁcial Neural Network trained\non a speech dataset (NIST Broadcast News) [1], while\n[16] attempts to explicitly exploit vibrato andtremolo , two\ncharacteristics that are speciﬁc of vocal signals. Alternativ-\nley, Support Vector Machines (SVMs) are used for frame\nclassiﬁcation and Hidden Markov Models act as smoothing\nstep [15]; a similar solution is proposed by [13], which ex-\nploits a wider set of features, including ones derived from\na predominant melody extraction step.\nMore recently, increasingly complex classiﬁers are pre-\nferred to feature engineering, given the widespread success\nof deep learning methods and modest increases in available\ntraining data. Much prior research explores the application\nof deep learning to music tagging, which typically encom-\npasses one or more classes for singing voice in the taxon-\nomy considered [7]. Elsewhere, deep networks have been\nused for pinpointing singing voice in source separation\nsystems [19]. Regarding the particular task at hand, [9]\nproposes a sophisticated architecture based on Recurrent\nNeural Networks that does not have a separate smoothing\nstep, while [17] uses a conventional convolutional network\ntopology, further advancing the state of the art.\n3.2 Proposed System\nThe log-magnitude CQT representation described in 2.2\nis processed in 1 second windows, with a dimensional-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 711ity of (32\u0002180) bins in time and frequency, respec-\ntively. We adopt a ﬁve-layer neural network, with three\n(3D) convolutional layers, each followed by max-pooling,\nand two fully-connected layers, with the following pa-\nrameter shapes: w0= (1;64;5;13);p0= (2;3),w1=\n(64;32;3;9);p1= (2;2),w2= (32;24;5;1);p2= (2;1),\nw3= (1540;768) ,w4= (768;2). All layer activations are\nhard rectiﬁed linear units (ReLUs), with the exception of\nthe last (classifer) layer, which uses a softmax.\nThe network is trained using a negative log-likelihood\nloss function and parameters are optimized with minibatch\nstochastic gradient descent. We implement our model in\nTheano3, leveraging the Pescador4package for drawing\nsamples from our dataset, and accelerate training with a\nNVIDIA Titan X GPU. Networks are trained for 500k iter-\nations (\u001920hours) with a learning rate of 0.05 and a batch\nsize of 50. Dropout is used in all but the last layer, with a\nparameter of 0.125. In addition to the weakly labeled case,\nf\u000f= 0:0;\u001c= 0:0;\u0010=Fg, we explore model behavior\nover two sampling parameter settings, with and without en-\ntanglement:f\u000f= 0:3;\u001c= 0:05gandf\u000f= 1:0;\u001c= 0:2g.\nThese values are informed by ﬁrst computing a histogram\nof vocal activation signals over the collection, revealing\nthat a large number of values occur near zero ( \u00140:05),\nwhile the upper bound rolls off smoothly at \u00192:5.\n4. EXPERIMENTAL RESULTS\nWe evaluate our models on two standard datasets in V AD\nresearch: the Jamendo collection, containing 93 manually\nannotated songs [15]; and the RWC-Pop collection, con-\ntaining 100 manually annotated songs [13]. To compare\nwith previously reported results, we consider the area un-\nder the curve (AUC) score and max-accuracy [17]. The\nAUC score provides insight into the rank ordering of class\nlikelihoods, and max-accuracy indicates the performance\nceiling (or error ﬂoor) given an optimal threshold.\n4.1 Quantitative Evaluation\nTable 1 shows the summary statistics over the two datasets\nconsidered as a function of sampling parameters, alongside\npreviously reported results for comparison [17]. The ﬁrst\nthree systems ( \u000b;\f;\r ) are successive boosted versions of\neach other, i.e.,\u000bis trained with weak labels, and its pre-\ndictions on the training set are used to train \f, and so on;\ntheﬁnemodel is trained directly with strongly labeled data,\nand we refer to each by sufﬁx, e.g.,\u000b. Additionally, the au-\nthors train these models with a witheld dataset unavailable\nto our work here.\nThese results provide a few notable insights. First, we\nconﬁrm that our automated approach of mining training\ndata is sufﬁcient to train models that can match state of\nthe art performance. Conﬁguration I, corresponding to the\nweak labeling condition, performs roughly on par with a\ncomparably trained system, \u000b, validating previous results;\nconﬁguration Vachieves the best scores of our models, and\n3https://github.com/Theano/Theano\n4https://github.com/pescadores/pescadorRWC J AMENDO\nAUC ACC+AUC ACC+\nSCHL¨UTER -\u000b 0.879 0.856 0.913 0.865\nSCHL¨UTER -\f 0.890 0.861 0.923 0.875\nSCHL¨UTER -\r 0.939 0.887 0.960 0.901\nSCHL¨UTER -FINE 0.947 0.882 0.951 0.880\n\u001c \u000f \r\nI 0.0 0.0 F 0.891 0.856 0.911 0.856\nII 0.05 0.3 F 0.918 0.879 0.925 0.869\nIII 0.05 0.3 T 0.918 0.879 0.934 0.874\nIV 0.2 1.0 F 0.937 0.887 0.935 0.872\nV 0.2 1.0 T 0.939 0.890 0.939 0.878\nTable 1 . AUC-scores and maximum accuracies across\nmodels on the RWC andJamendo datasets.\n0.0 0.1 0.2 0.3 0.4 0.5\nFalse Positive Rate0.00.10.20.30.40.5False Negative Rate\nrwc\n0.0 0.1 0.2 0.3 0.4 0.5\nFalse Positive Rate0.00.10.20.30.40.5\njamendo\nFigure 2 . Trackwise error rates, plotting false positives\nversus false negatives for IV; one outlier ( fn\u00190:66) in\ntheJamendo set is not shown to maintain aspect ratio.\nis consistent with gains in prior work. That said, the dif-\nference between models is in the range of 0:02-0:05across\nmetrics, which is of limited reliability with datasets of this\nsize. In terms of sampling parameters, we observe a direct\ncorrelation between signal-to-noise ratio in our training\ndata, i.e.,the more non-vocal observations are discarded,\nthe better the models behave on these measures. Training\nwith entangled pairs ( \u0010=T) also seems to have a small\npositive effect. Finally, we note a possible corpus effect be-\ntween these systems and previously reported results, where\nmodels (Vand\r) perform better on different data. Though\nminor, this potential corpus effect serves as a dimension to\nexplore in subsequent analysis.\n4.2 Error Analysis\nAs the systems reported here are high performing, a poten-\ntially more informative path to understanding model be-\nhavior is through analyzing the errors made. Here, false\npositives occur when a different sound source is mistaken\nfor voice; false negatives occur when the energy of a vocal\nsource has fallen below the model’s sensitivity. Observa-\ntions drawn from the same music recording will be highly\ncorrelated, due to the repetitive nature of music, and so we\nexplore the track-wise frequency of errors to identify be-\nhaviors that may reveal broader trends.\nA slight corpus effect is seen in Figure 2 between the\nRWC andJamendo collections. In the former, the majority712 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170 10 20 30 40 50 60\nTime (sec)LikelihoodN062-M04-T14\n100 120 140 160 180 200 220 240\nTime (sec)Likelihood05 - Bucle ParanoidealFigure 3 . Examples from the evaluation dataset, showing the ground truth (black), estimated likelihoods (blue) and thresh-\nolded prediction (green) over time: (top) a track from the RWC corpus demonstrates how a model can operate with greater\ntemporal precision than a human annotator, a common source of false negatives; (bottom) a track from the Jamendo collec-\ntion illustrates different challenges, including imposter sources (a guitar solo), sensitivity to vocals, and annotator error.\nof error is due to false positives, but at a much lower rate\nof occurrence ( fp < 0:1) than false negatives. Addition-\nally, when errors do occur in a track, they tend to be pri-\nmarily of one type, and seldom both. This is less the case\nfor the Jamendo set, comprised of both “worse” tracks and\na (slightly) larger co-occurrence of error types in a given\ntrack.\nUsing this visualization of trackwise errors, an investi-\ngation into the various outliers yields a few observations.\nThere are two primary sources of false negatives: one,\nshown in Figure 3 (top), trained models exhibit a level of\ntemporal precision beyond the annotators’ in either dataset,\npinpointing breaths and pauses in otherwise continuous vo-\ncalizations; and two, nuances of the data used for train-\ning seem to induce a production bias, whereby the model\nunder-predicts singing voice in lower quality mixes. In\nhindsight, it is unsurprising that models trained on pro-\nfessionally produced music might develop a sensitivity to\nmixing quality, and we note this as a topic for future ex-\nploration. A similar bias also appears to account for the\nmajority of all false positives, often corresponding with\nmonophonic instrumental melodies, e.g., guitar riffs or so-\nlos, but less so for polyphonic melodies, i.e.,two or more\nnotes played simultaneously by the same source, consistent\nwith previous ﬁndings [11].\nFigure 3 (bottom) illustrates an interesting example of\nthis behavior. In the ﬁrst 80 seconds shown here, the model\nagrees with the human annotation. The model fails at the\n180 second mark, misclassifying a guitar line, and contin-\nues through 194-210 seconds, where the model struggles\nto detect rap vocals at a softer volume. However, from that\npoint onwards, the human annotation itself is wrong, while\nthe model is correct; vocals are indeed present between\n210-230 seconds and 230-252 contains no voice, which\naccounts for 16% of the track. Coincidentally, this further\nunderscores the need for large, diverse evaluation datasets\nto produce reliable metrics.4.3 Multitrack Analysis\nThe above results conﬁrm the intuition that it can be chal-\nlenging to manually annotate singing voice activity with\nmachine precision. Ideally, though, human annotation ap-\nproximates a smoothed, thresholded version of the vocal\nsignal energy in isolation, and as such, we are interested\nunderstanding the degree to which model estimations cor-\nrespond with the pure vocal signal. Another way of mea-\nsuring our models’ capacity to estimate singing voice from\na “down-mixed” recording is through the use of multitrack\naudio, which provides direct access to the signal of inter-\nest,i.e.,vocals, in isolation.\nWe now turn our attention to MedleyDB, a dataset of\n122 songs containing recordings of individual stems and\ncorresponding mixes [2]. For each of the 47 songs that\nhave vocals in isolation, we create a single vocal track for\nanalysis, and compute the log-magnitude CQT for the full\nmix (the “original” version) XM, and the isolated vocals,\nXV. Whereas previously Viterbi was used to track vocal\ndensity, here the reference vocal density contains no noise\nand can be computed by summing the spectral energy over\nfrequency, i.e.,\u001eV\nn=P\nkXV\nn;k. The trained models are\napplied to the full mix, XM, for inference, producing a\ntime-varying likelihood, LM.\nThe reference vocal density is not a class label but a\ncontinuous value, and the comparison metrics must be ad-\njusted accordingly. Maximum accuracy is generalized to\nthe case where independent thresholds are considered for\n\u001eV;LMover the dataset, providing insight into the best-\ncase agreement between the two signals. We also con-\nsider the Spearman rank-order correlation between the two\nsets, a measure of the relative rank order between distribu-\ntions [20].\nAn exploration of model performance on this dataset\nvalidates earlier observations, summarized in Table 2. On\nmanual inspection of the temporal precision of the model\non the Medley dataset, we conﬁrm that deviations between\nestimated likelihoods and the reference vocal density areProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 713f\u001c,\u000f,\u0010g SPEARMAN -R ACC+\nI 0.0, 0.0, F 0.681 0.812\nII 0.05, 0.3, F 0.779 0.849\nIII 0.05, 0.3, T 0.768 0.854\nIV 0.2, 1.0, F 0.784 0.852\nV 0.2, 1.0, T 0.796 0.862\nTable 2 . Spearman rank-order correlation and maximum\naccuracy scores across models on the MedleyDB vocal\nsubset.\nrepresentative of true model errors, setting a baseline for\nfuture work. As seen previously, false negatives again cor-\nrespond to vocal loudness relative to the mix, and false\npositives are caused by loud melodic contours. Note also\nthat the Spearman rank-order correlation is consistent with\npreviously observed trends across models, while provid-\ning more nuance; the greatest difference between models\nis>0:11, versus\u00190:05for maximum accuracy. Fi-\nnally, we note that the ﬂexibility of multitrack datasets\npresents a great opportunity for rigorously testing future\nwork, whereby the pitch and loudness of a vocal track can\nbe used to synthesize “imposters” with different timbres,\ne.g., a sine wave or ﬂute, mixed with instrumental tracks,\nand used to measure false positives.\n5. DISCUSSION\nThis work presents an approach to mining strongly labeled\ndata from web-scale music collections for detecting vocal\nactivity. Original recordings, containing vocals, are auto-\nmatically paired with their instrumental counterparts, and\ndifferential information is used to estimate vocal activity\nover time. This signal can be used to train convolutional\nneural networks; the strongly labeled training data pro-\nduces superior results to the weakly labeled setting, achiev-\ning state of the art performance. While analyzing errors\nmade by our models, three distinct lessons stood out.\nFirst, in addition to curation and mining, it is valuable\nto recall a third path to acquiring sufﬁciently large datasets:\nactive learning. Imperfect models can be leveraged to\nmake the annotation process more efﬁcient by perform-\ning aspects of annotation that humans may ﬁnd difﬁcult or\ntime-consuming, as well as prioritizing data as a function\nof model uncertainty. Here, for example, we observe that\nregions annotated as vocal tend to include brief pauses, no\ndoubt resulting from the time and effort it would require to\nannotate at that level of detail. Alternatively, a performant\nmodel, like those described here, could segment audio into\nshort, labeled excerpts for a human to verify or correct,\neliminating a huge time cost. This would allow reliable\ndata to be obtained at a faster rate.\nSecond, the application of machine learning to mined\ndatasets can help identify particular challenges of a given\ntask, unlocking new research directions. Here, our model\nidentiﬁes an interesting bias in the dataset that we had not\npreviously considered, being the tight coupling between\nsinging voice (timbre), melody (pitch), and production ef-\nfects (loudness). Often in Western popular music, leadvocals carry the melody and tend to be one of the more\nprominent sources in the mix. Thus, in the dataset mined\nfrom a commercial music catalogue, instrumental versions\nnot only lack vocal timbres, but prominent melodic con-\ntours are missing as well. This complex relationship is less\nobvious at a distance, but our experiments illustrate the\nchallenges faced data-driven approaches to singing voice\ndetection. By the same token, this also identiﬁes an oppor-\ntunity to build systems invariant to these dimensions.\nFinally, these insights serve as a reminder that it is good\npractice to both design and evolve benchmarking datasets\nto encompass challenging test cases and known failure\nmodes as they are identiﬁed. In our analysis, we ﬁnd\nthat the available benchmarking datasets consist mostly of\nmusical content in which the melody is also voice, and\ntherefore more “difﬁcult” signals would help reliably dis-\ncriminate between models. This content could be identi-\nﬁed automatically via incremental evaluation methods, in\nwhich disagreement between machine estimations effec-\ntively prioritizes data to maximize discrimination between\nmodels [4].\n5.1 Future Work\nPerhaps the most logical next step for this work is to bet-\nter augment training data, such that pitch and melodic in-\nformation are well represented in negative examples. One\npossible approach, for example, would be to use the fre-\nquency of the vocal density estimate recovered in Section\n??to synthesize the melody with different timbres to be\nmixed into the instrumental recording. Whereas before\nentangled pairs contrast the presence of vocals, this ap-\nproach would yield pairs that differ only in the timbre of\nthe voice. Alternatively, additional sources could be lever-\naged for building models invariant to less relevant char-\nacteristics, such as instrumental content without a corre-\nsponding “original” version, or multitrack audio.\nAdditionally, more effort is required to advance eval-\nuation methodology for automatic vocal activity detec-\ntion. Multitrack datasets like MedleyDB, are a particularly\npromising route for rigorous benchmarking. The isolated\nvocal signal provides an optimal reference signal, while\nthe other, non-vocal stems can be recombined as needed\nto deeply explore system behavior. We also recognize that\nlarger, more diverse evaluation datasets are a prerequisite\nto advancing the state of the art in this domain. Thus, as\na ﬁrst step toward these ends, we provide machine estima-\ntions from our model over the datasets used here, as well as\na large publicly available dataset (with audio) to facilitate\nthe manual annotation process.5Though human effort is\nnecessary to verify or correct these machine estimations,\nwe share this data in the hope that it can serve as a starting\npoint to accelerate the growth of labeled data for this task\nand facilitate efforts toward incremental evaluation.\n5https://github.com/ejhumphrey/vox-detect-jams714 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. REFERENCES\n[1] Adam L Berenzweig and Daniel PW Ellis. Locating\nsinging voice segments within music signals. In Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 119–122. IEEE, 2001.\n[2] Rachel M Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo\nBello. Medleydb: A multitrack dataset for annotation-\nintensive MIR research. In Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , volume 14, pages 155–160, 2014.\n[3] Judith C Brown. Calculation of a constant q spectral\ntransform. The Journal of the Acoustical Society of\nAmerica , 89(1):425–434, 1991.\n[4] Ben Carterette and James Allan. Incremental test col-\nlections. In Proceedings of the 14th ACM International\nConference on Information and Knowledge Manage-\nment , pages 680–687. ACM, 2005.\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. Imagenet: A large-scale hierarchi-\ncal image database. In Computer Vision and Pattern\nRecognition (CVPR), IEEE Conference on , pages 248–\n255. IEEE, 2009.\n[6] Daniel PW Ellis, Brian Whitman, and Alastair Porter.\nEchoprint: An open music identiﬁcation service. In\nProceedings of the 12th International Society for Mu-\nsic Information Retrieval Conference (ISMIR) . ISMIR,\n2011.\n[7] Philippe Hamel, Simon Lemieux, Yoshua Bengio, and\nDouglas Eck. Temporal pooling and multiscale learn-\ning for automatic annotation and ranking of music au-\ndio. In Proceedings of the 12th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 729–734, 2011.\n[8] Eric J Humphrey and Juan Pablo Bello. Rethinking\nautomatic chord recognition with convolutional neu-\nral networks. In International Conference on Machine\nLearning and Applications (ICMLA) , volume 2, pages\n357–362. IEEE, 2012.\n[9] Simon Leglaive, Romain Hennequin, and Roland\nBadeau. Singing voice detection with deep recur-\nrent neural networks. In International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 121–125. IEEE, 2015.\n[10] Bernhard Lehner, Gerhard Widmer, and Sebastian\nBock. A low-latency, real-time-capable singing voice\ndetection method with LSTM recurrent neural net-\nworks. In Proceedings of the 23rd European Sig-\nnal Processing Conference (EUSIPCO) , pages 21–25.\nIEEE, 2015.\n[11] Bernhard Lehner, Gerhard Widmer, and Reinhard\nSonnleitner. On the reduction of false positives insinging voice detection. In International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\npages 7480–7484. IEEE, 2014.\n[12] Matthias Mauch and Simon Dixon. pYIN: A funda-\nmental frequency estimator using probabilistic thresh-\nold distributions. In International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 659–663. IEEE, 2014.\n[13] Matthias Mauch, Hiromasa Fujihara, Kazuyoshi\nYoshii, and Masataka Goto. Timbre and melody fea-\ntures for the recognition of vocal activity and instru-\nmental solos in polyphonic music. In Proceedings of\nthe 12th International Society for Music Information\nRetrieval Conference (ISMIR) , pages 233–238, 2011.\n[14] Colin Raffel and Daniel PW Ellis. Large-scale content-\nbased matching of MIDI and audio ﬁles. In Proceed-\nings of the 16th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) . ISMIR, 2015.\n[15] Mathieu Ramona, Ga ¨el Richard, and Bertrand David.\nV ocal detection in music with support vector machines.\nInInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 1885–1888. IEEE,\n2008.\n[16] Lise Regnier and Geoffroy Peeters. Singing voice de-\ntection in music tracks using direct voice vibrato detec-\ntion. In International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 1685–1688.\nIEEE, 2009.\n[17] Jan Schl ¨uter. Learning to pinpoint singing voice from\nweakly labeled examples. In Proceedings of the 17th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2016.\n[18] Jan Schl ¨uter and Thomas Grill. Exploring data aug-\nmentation for improved singing voice detection with\nneural networks. In Proceedings of the 16th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 121–126, 2015.\n[19] Andrew JR Simpson, Gerard Roma, and Mark D\nPlumbley. Deep Karaoke: Extracting vocals from mu-\nsical mixtures using a convolutional deep neural net-\nwork. In Latent Variable Analysis and Signal Sepa-\nration, International Conference on , pages 429–436.\nSpringer, 2015.\n[20] D. Zwillinger and S. Kokoska, editors. Probability and\nStatistics Tables and Formulae. Chapman & Hall, New\nYork, NY , 2000.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 715"
    },
    {
        "title": "Intelligibility of Sung Lyrics: A Pilot Study.",
        "author": [
            "Karim M. Ibrahim",
            "David Grunberg",
            "Kat Agres",
            "Chitralekha Gupta",
            "Ye Wang 0007"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1414730",
        "url": "https://doi.org/10.5281/zenodo.1414730",
        "ee": "https://zenodo.org/records/1414730/files/IbrahimGAGW17.pdf",
        "abstract": "We propose a system to automatically assess the intel- ligibility of sung lyrics. We are particularly interested in being able to identify songs which are intelligible to sec- ond language learners, as such individuals often sing along the song to help them learn their second language, but this is only helpful if the song is intelligible enough for them to understand. As no automatic system for identify- ing ‘intelligible’ songs currently exists, songs for second language learners are generally selected by hand, a time- consuming and onerous process. We conducted an exper- iment in which test subjects, all of whom are learning En- glish as a second language, were presented with 100 ex- cerpts of songs drawn from five different genres. The test subjects listened to and transcribed the excerpts and the in- telligibility of each excerpt was assessed based on average transcription accuracy across subjects. Excerpts that were more accurately transcribed on average were considered to be more intelligible than those less accurately transcribed on average. We then tested standard acoustic features to determine which were most strongly correlated with intel- ligibility. Our final system classifies the intelligibility of the excerpts and achieves 66% accuracy for 3 classes of intelligibility.",
        "zenodo_id": 1414730,
        "dblp_key": "conf/ismir/IbrahimGAGW17",
        "keywords": [
            "automatic assessment",
            "sung lyrics",
            "second language learners",
            "identify intelligible songs",
            "time-consuming process",
            "experimenter",
            "test subjects",
            "English as second language",
            "average transcription accuracy",
            "acoustic features"
        ],
        "content": "INTELLIGIBILITY OF SUNG LYRICS: A PILOT STUDY\nKarim M. Ibrahim1David Grunberg1Kat Agres2\nChitralekha Gupta1Ye Wang1\n1Department of Computer Science, National University of Singapore, Singapore\n2Institute of High Performance Computing, A*STAR, Singapore\nkarim.ibrahim@comp.nus.edu.sg, wangye@comp.nus.edu.sg\nABSTRACT\nWe propose a system to automatically assess the intel-\nligibility of sung lyrics. We are particularly interested in\nbeing able to identify songs which are intelligible to sec-\nond language learners, as such individuals often sing along\nthe song to help them learn their second language, but\nthis is only helpful if the song is intelligible enough for\nthem to understand. As no automatic system for identify-\ning ‘intelligible’ songs currently exists, songs for second\nlanguage learners are generally selected by hand, a time-\nconsuming and onerous process. We conducted an exper-\niment in which test subjects, all of whom are learning En-\nglish as a second language, were presented with 100 ex-\ncerpts of songs drawn from ﬁve different genres. The test\nsubjects listened to and transcribed the excerpts and the in-\ntelligibility of each excerpt was assessed based on average\ntranscription accuracy across subjects. Excerpts that were\nmore accurately transcribed on average were considered to\nbe more intelligible than those less accurately transcribed\non average. We then tested standard acoustic features to\ndetermine which were most strongly correlated with intel-\nligibility. Our ﬁnal system classiﬁes the intelligibility of\nthe excerpts and achieves 66% accuracy for 3 classes of\nintelligibility.\n1. INTRODUCTION\nWhile various studies have been conducted on singing\nvoice analysis, one aspect which has not been well-studied\nis the intelligibility of a given set of lyrics. Intelligibility\ndescribes how easily a listener can comprehend the words\nthat a performer sings; the lyrics of very intelligible songs\ncan easily be understood, while the lyrics of less intelligi-\nble songs sound garbled or even incomprehensible to the\naverage listener. People’s impressions of many songs are\nstrongly inﬂuenced by how intelligible the lyrics are, with\none study even ﬁnding that certain songs were perceived\nas ‘happy’ when people could not understand its lyrics,\nbut was perceived as ‘sad’ when the downbeat lyrics were\nc\rKarim M. Ibrahim, David Grunberg, Kat Agres, Chi-\ntralekha Gupta, Ye Wang. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: Karim M.\nIbrahim, David Grunberg, Kat Agres, Chitralekha Gupta, Ye Wang. “ In-\ntelligibility of Sung Lyrics: a Pilot Study”, 18th International Society for\nMusic Information Retrieval Conference, Suzhou, China, 2017.made comprehensible [20]. It would thus be useful to en-\nable systems to automatically determine intelligibility, as\nit is a key factor in people’s perception of a wide variety of\nsongs.\nWe are particularly interested in measuring the intelli-\ngibility of songs with respect to second language learners.\nMany aspects of learning a second language to the point of\nﬂuency have been shown to be difﬁcult, including separat-\ning the phonemes of an unfamiliar language [30], memo-\nrizing a large number of vocabulary words and grammar\nrules [22], and maintaining motivation for the length of\ntime required to learn the language. Consequently, many\nsecond language learners need help, and music has been\nshown to be a useful tool for this purpose. Singing and\nlanguage development have been shown to be closely re-\nlated at the neurological level [24, 32], and experimental\nresults have demonstrated that singing along with music\nin the second language is an effective way of improving\nmemorization and pronunciation [12, 19]. However, spe-\nciﬁc songs are only likely to help these students if they can\nunderstand the content of the lyrics [11]. As second lan-\nguage learners may have difﬁculty understanding certain\nsongs in their second language due to their lack of ﬂuency,\nthey could be helped by a system capable of automatically\ndetermining which songs they are likely to ﬁnd intelligible\nor unintelligible.\nWe therefore seek to design a system which is capable\nof assessing a given song and assigning it an intelligibil-\nity score, with the standard of intelligibility biased towards\npeople who are learning the language of the lyrics but have\nnot yet mastered it. To gather data for this system we com-\npiled excerpts from 50 songs and had volunteering partic-\nipants listen to the song in order to discover how intelligi-\nble they found the lyrics. Rather than simply having the\nparticipants rate the intelligibility of the song, we had the\nparticipants transcribe the lyrics that they heard and then\ncalculated an intelligibility score for each excerpt based on\nthe statistics of how accurately the students transcribed it.\nExcerpts that were transcribed more accurately on average\nwere judged to be more intelligible than those transcribed\nless accurately on average. A variety of acoustic features\nwere then used to build a classiﬁer which could determine\nthe intelligibility of a given piece of music. The classiﬁer\nwas then run on the same excerpts used in the listening\nexperiment, and the results of each were compared.\nThe remaining outline of this paper is as follows: Sec-686tion 2 lists relevant literature in the ﬁeld. Section 3 de-\nscribes the transcription experiment performed to gather\ndata. Section 4 discusses the features and the classiﬁer.\nFinally, Sections 5 and 6 shows the evaluation of our pro-\nposed model and our conclusions, respectively.\n2. LITERATURE REVIEW\nThat sung lyrics could be more difﬁcult to comprehend\nthan spoken words has long been established in the scien-\ntiﬁc community. One study showed that even professional\nvoice teachers and phoneticians had difﬁculty telling vow-\nels apart when sung at high pitch [7]. Seminal work by\nCollister and Huron found listeners to make hearing er-\nrors as much as seven times more frequently when listen-\ning to sung lyrics than spoken ones [3]. Such studies also\nnoted lyric features which could help differentiate intelligi-\nble from unintelligible songs; for instance, one study noted\nthat songs comprised mostly of common words sounded\nmore intelligible than songs with rarer words [9]. How-\never, lyric features alone are not sufﬁcient to assess intelli-\ngibility; the same lyrics can be rendered more or less intel-\nligible depending on, for instance, the speed at which they\nare sung. These other factors must be taken into account to\ntruly assess lyric intelligibility.\nStudies have been conducted on assessing the overall\nquality of singing voice. One acoustic feature which mul-\ntiple studies have found to be useful for this purpose is\nthe power ratio of frequency bands containing energy from\nthe singing voice to other frequency bands; algorithms us-\ning this feature have been shown to reliably distinguish be-\ntween trained and untrained singers [2,23,34]. Calculation\nof pitch intervals and vibrato have also been shown to be\nuseful for this purpose [21]. However, while the quality\nof singing voice may be a factor in assessing intelligibil-\nity, it is not the only such factor. Aspects of the song that\nhave nothing to do with the skill of the singer or the qual-\nity of their performance, such as the presence of loud back-\nground instruments, can contribute, and additional features\nthat take these factors into account are needed for a system\nwhich determines lyric intelligibility.\nAnother related task is that of singing transcription,\nin which a computer must listen to and transcribe sung\nlyrics [18]. It may seem that one could assess intelligibil-\nity by comparing a computer’s transcription of the lyrics\nto a ground truth set of lyrics and determining if the tran-\nscription is accurate. But this too does not really determine\nintelligibility, at least as humans perceive it. A computer\ncan use various ﬁlters and other signal processing or ma-\nchine learning tools to process the audio and make it easier\nto understand, but a human listening to the music will not\nnecessarily have access to such tools. Thus, even if a com-\nputer can understand or accurately transcribe the lyrics of a\npiece of music, this does not indicate whether those lyrics\nwould be intelligible to a human as well.3. BEHA VIORAL EXPERIMENT\nTo build a system that can automatically process a song\nand evaluate the intelligibility of its lyrics, it is essential to\ngather ground truth data that reﬂects this intelligibility on\naverage across different listeners. Hence, we conducted a\nstudy where participants were tasked with listening to short\nexcerpts of music and transcribing the lyrics, a common\ntask for evaluating intelligibility of lyrics [4]. The accuracy\nof their transcription can be used to assess the intelligibility\nof each excerpt.\n3.1 Method\n3.1.1 Participants\nSeventeen participants (seven females and ten males) vol-\nunteered to take part in the experiment. Participants were\nbetween 21 to 41 years (mean = 27.4 years). All partici-\npants indicated no history of hearing impairment and that\nthey spoke some English as a second language. Partic-\nipants were rewarded with a $10 voucher for their time.\nParticipants were recruited through university channels via\nposters and ﬂiers. The majority of the participants were\nuniversity students.\n3.1.2 Materials\nFor the purpose of this study, we focused solely on\nEnglish-language songs. Because one of the main appli-\ncations for such a system is to recommend music for stu-\ndents who are learning foreign languages, we focused on\ngenres that are popular for students. To identify these gen-\nres, we asked 48 university students to choose the 3 genres\nthat they listen to the most, out of the 12 genres introduced\nin [4], as these 12 genres cover a wide variety of singing\nstyles. The twelve genres are: Avante-garde, Blues, Clas-\nsical, Country, Folk, Jazz, Pop/Rock, Rhythm and Blues,\nRap, Reggae, Religious, and Theater. Because the tran-\nscription task is long and tiring for participants, we lim-\nited the number of genres tested to only ﬁve, from which\nwe would draw approximately 45 minutes worth of music\nfor transcription. We selected the ﬁve most popular gen-\nres indicated by the 48 participants: Classical, Folk, Jazz,\nPop/Rock, and Rhythm and Blues.\nAfter selecting the genres, we collected a dataset of 10\nsongs per genre. Because we were interested in evaluat-\ning participants’ ability to transcribe an unfamiliar song,\nas opposed to transcribing a known song from memory,\nwe focused on selecting songs that are not well-known in\neach genre. We approached this by selecting songs that\nhave less than 200 ratings on the website Rate Your Mu-\nsic (rateyourmusic.com). Rate Your Music is a database\nof popular music where users can rate and review different\nsongs, albums and artists. Popular songs have thousands\nof ratings while less known songs have few ratings. We\nused this criteria to collect songs spanning the 5 genres to\nproduce our dataset. The songs were randomly selected,\nwith no control over the vocal range or the singer’s accent,\nas long as they satisﬁed the condition of being in English\nand having few ratings.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 687Because transcribing an entire song, let alone 50 songs,\nwould be an overwhelming process for the participants, we\nselected short excerpts from each song to be transcribed.\nTwo excerpts per song were selected randomly such that\neach excerpt would include a complete utterance (e.g., no\nexcerpts were terminated mid-phrase). Excerpts varied be-\ntween 3 to 16 seconds in length (average = 6.5 seconds),\nand contained 9.5 words on average. The ground-truth\nlyrics for these songs were collected from online sources\nand reviewed by the experimenters to ensure they matched\nthe version of the song used in the experiment. It is im-\nportant to note that selecting short excerpts might affect\nintelligibility, because the context of the song (which may\nhelp in understanding the lyrics) is lost. However, using\nthese short excerpts is essential in making the experiment\nfeasible for the participants, and would still broadly reﬂect\nthe intelligibility of the song. The complete dataset is com-\nposed of 100 excerpts from 50 songs, 2 excerpts per song,\ncovering 5 genres, and 10 songs per genre. Readers who\nare interested in experimenting on the dataset can contact\nthe authors.\n3.1.3 Procedure\nWe conducted the experiment in three group listening ses-\nsions. During each session, the participants were seated\nin a computer lab, and recorded their transcriptions of the\nplayed excerpts on the computer in front of them. The ex-\ncerpts were played in randomized order, and each excerpt\nwas played twice consecutively. Between the two play-\nbacks of each excerpt there was a pause of 5 seconds, and\nbetween different excerpts a pause of 10 seconds, to allow\nthe participants sufﬁcient time to write their transcription.\nThe total duration of the listening session is 46:59 minutes.\nTwo practice trials were presented before the experimental\ntrials began, to familiarize participants with the experimen-\ntal procedure.\n3.2 Results and Discussion\nTo evaluate the accuracy of the participants’ transcription,\nwe counted the number of words correctly transcribed by\nthe participant that match the ground truth lyrics. For each\ntranscription by each student, the ratio between correctly\ntranscribed words to the total number of words in the ex-\ncerpt was calculated. We then calculated the average ratio\nfor each excerpt across all 17 participants to yield an over-\nall score for each excerpt between 0 and 1. This score was\nused to represent the ground-truth transcription accuracy,\norIntelligibility score , for each excerpt. The distribution\nof Intelligibility scores in the dataset is shown in Figure\n1. From the ﬁgure, we can observe that the intelligibility\nscores are biased towards higher values, i.e. there are rel-\natively few excerpts with a low intelligibility score. This\nmay be caused by the restricted set of popular genres in-\ndicated by students, as certain excluded genres would be\nexpected to have low intelligibility, such as Heavy Metal.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 102468101214\nTranscription AccuracyNumber of ExcerptsDistribution of trascription accuacy for 100 excerpts\n  Figure 1 . The distribution of the transcription accuracies\n(Intelligibility score).\n4. COMPUTATIONAL SYSTEM\nThe purpose of this study is to select audio features that\ncan be used to build a system capable of 1) predicting the\nintelligibility of song lyrics, and 2) evaluating the accu-\nracy of these predictions with respect to the ground truth\ngathered from human participants. In the following ap-\nproach, we analyze the input signal and extract expressive\nfeatures that reﬂect the different aspects of an intelligible\nsinging voice. Several properties may contribute to making\nthe singing voice less intelligible than normal speech. One\nsuch aspect is the presence of background music, as ac-\ncompanying music can cover or obscure the voice. There-\nfore, highly intelligible songs would be expected to have a\ndominant singing voice compared with the accompanying\nmusic [4]. Unlike speech, the singing voice has a wider and\nmore dynamic pitch range, often featuring higher pitches\nin soprano vocal range. This has been shown to affect\nthe intelligibility of the songs, especially with respect to\nthe perception of sung vowels [1, 3]. An additional con-\nsideration is that in certain genres, such as Rap, singing\nis faster and has a higher rate of words per minute than\nspeech, which can reduce intelligibility. Furthermore, as\nindicated in [10], the presence of common, frequently oc-\ncurring words helps increase intelligibility, while uncom-\nmon words decrease the likelihood of understanding the\nlyrics. In our model, we aimed to include features that ex-\npress these different aspects to determine the intelligibility\nof song lyrics across different genres. These features are\nthen used to train the model to accurately predict the intel-\nligibility of lyrics in the dataset, based on the ground truth\ncollected in our behavioral experiment.\n4.1 Preprocessing\nTo extract the proposed features from an input song, two\ninitial steps are required: separating the singing voice from\nthe accompaniment, and detecting the segments with vo-\ncals. To address these steps, we selected the following ap-\nproaches based on current state-of-the-art methods:\n4.1.1 Vocals Separation\nSeparating vocals from accompaniment music is a well-\nknown problem that has received considerable attention in\nthe research community. Our approach makes use of the\npopular Adaptive REPET algorithm [16]. This algorithm is688 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017based on detecting the repeating patten in the song, which\nis meant to represent the background music. Separating the\ndetected pattern leaves the non-repeating part of the song,\nmeant to capture the vocals. Adaptive REPET also has\nthe advantage of discovering local repeating patterns in the\nsong over the original REPET algorithm [26]. Choosing\nAdaptive REPET was based on two main advantages: The\nalgorithm is computationally attractive, and it shows com-\npetitive results compared to other separation algorithms, as\nshown in the evaluation of [14].\n4.1.2 Detecting Vocal Segments\nDetecting vocal and non-vocal segments in the song is an\nimportant step in extracting additional information about\nthe intelligibility of the lyrics. Various approaches have\nbeen proposed to perform accurate vocal segmentation,\nhowever, it remains a challenging problem. For our ap-\nproach, we implemented a method based on extracting the\nfeatures proposed in [15], then training a Random Forest\nclassiﬁer using the Jamendo corpus1[27]. The classiﬁer\nwas then used to binary classify each frame of the input ﬁle\nas either vocals or non-vocals.\n4.2 Audio features\nIn this section, we investigate the set of features we used\nin training the model for estimating lyrics intelligibility.\nWe use a mix of features reﬂecting speciﬁc aspects of in-\ntelligibility plus common standard acoustic features. The\nselected features are:\n1.Vocals to Accompaniment Music Ratio (V AR) :\nDeﬁned as the energy of the separated vocals di-\nvided by the energy of the accompaniment music.\nThis ratio is computed only in segments where vo-\ncals are present. This feature reﬂects how strong the\nvocals are compared to the accompaniment. High\nV AR suggests that vocals are relatively loud and less\nlikely to be obscured by the music. Hence, higher\nV AR counts for higher intelligibility. This feature is\nparticularly useful in identifying songs that are un-\nintelligible due to loud background music which ob-\nscures the vocals.\n2.Harmonics-to-residual Ratio (HRR) : Deﬁned as\nthe the energy in a detected fundamental frequency\n(f0) according to the YIN algorithm [5] plus the en-\nergy in its 20 ﬁrst harmonics (a number chosen based\non empirical trials), all divided by the energy of the\nresidual. This ratio is also applied only to segments\nwhere vocals are present. Since harmonics of the de-\ntected f0 in vocal segments are expected to be pro-\nduced by the singing voice, this ratio, like V AR,\nhelps to determine whether the vocals in a given\npiece of music are stronger or weaker than the back-\nground music which might obscure it.\n1http://www.mathieuramona.com/wp/data/jamendo/3.High Frequency Energy (HFE) : Deﬁned as the\nsum of the spectral magnitude above 4kHz,\nHFE n=Nb=2X\nk=f4kan;k (1)\nwhere an;kis the magnitude of block nand FFT in-\ndexkof the short time Fourier transform of the in-\nput signal, f4kis the index corresponding to 4 kHz\nandNbis the FFT size [8]. We calculate the mean\nacross all frames of the separated and segmented vo-\ncals signal, as we are interested in the high energy\ncomponent in vocals and not the accompanying in-\nstruments. We get a scalar value per input ﬁle re-\nﬂecting high frequency energy. Singing in higher\nfrequencies has been proven to be less intelligible\nthan music in low frequencies [3], so detection of\nhigh frequency energy can be a useful clue that such\nvocals might be present and could reduce the intel-\nligibility of the music, such as frequently happens\nwith opera music.\n4.High Frequency Component (HFC) : Deﬁned as\nthe sum of the amplitudes and weighted by the fre-\nquency squared,\nHFC n=Nb=2X\nk=1k2an;k (2)\nwhere an;kis the magnitude of block nand FFT in-\ndexkof the short time Fourier transform of the input\nsignal and Nbis the FFT size [17]. This is another\nmeasure of high frequency content.\n5.Syllable Rate : Singing at a fast pace while pro-\nnouncing several syllables over a short period of\ntime can negatively affect the intelligibility [6]. In\nthe past, Rao et al. used temporal dynamics of tim-\nbral features to separate singing voice from back-\nground music [28]. These features showed more\nvariance over time for singing voice, while being rel-\natively invariant to background instruments. We ex-\npect that these features will also be sensitive to the\nsyllable rate in singing. We use the temporal stan-\ndard deviation of two of their timbral features: sub-\nband energy (SE) in the range of ([300-900 Hz]),\nand sub-band spectral centroid (SSC) in the range\nof ([1.2-4.5 kHz]), deﬁned as\nSSC =Pkhigh\nk=klowf(k)jX(k)j\nPkhigh\nk=klowjX(k)j(3)\nSE =khighX\nk=klowjX(k)j2(4)\nwhere f(k)and jX(k)jare frequency and magnitude\nspectral value of the kthfrequency bin, and klowand\nkhigh are the nearest frequency bins to the lower and\nupper frequency limits on the sub-band respectively.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 689According to [28], SE enhances the ﬂuctuations be-\ntween voiced and unvoiced utterances, while SSC\nenhances the variations in the 2nd,3rdand 4thfor-\nmants across phone transitions in the singing voice.\nHence, it is reasonable to expect high temporal vari-\nance of these features for songs with high syllable\nrate, and vice versa. Thus, this feature is able to dif-\nferentiate songs with high and low syllable rates. We\nwould expect that very high and very low syllable\nrates should lead to low intelligibility score, while\nrates in a similar range to that of speech should re-\nsult in high intelligibility score.\n6.Word-Frequency Score : Songs which use com-\nmon words have been shown to be more intelligi-\nble than those which use unusual or obscure words\n[10]. Hence, we calculate a word-frequency score\nfor the lyrics of the songs as an additional feature.\nThis feature is a non-acoustic feature that is use-\nful in cases where the lyrics of the song are avail-\nable. We calculate the word-frequency score us-\ning the wordfreq open-source toolbox [31] which\nprovides an estimates of the frequencies of words in\nmany languages.\n7.Tempo and Event Density : These two rhythmic\nfeatures reﬂect how fast the beat and rhythm of the\nsong are. Event density is deﬁned as the average\nfrequency of events, i.e., the number of note on-\nsets per second. Songs with very fast beats and\nhigh event density are likely to be less intelligible\nthan slower songs, since the listener has less time to\nprocess each event before the next one begins. We\nused the MIRToolbox [13] to extract these rhyth-\nmic features.\n8.Mel-frequency cepstral coefﬁcients (MFCCs) :\nMFCCs approximates the human auditory system’s\nresponse more closely than the linearly-spaced fre-\nquency bands [25]. MFCCs have been proven to\nbe effective features in problems related to singing\nvoice analysis [29], and so were considered as a po-\ntential feature here as well. For our system, we se-\nlected the 17 ﬁrst coefﬁcients (excluding the 0th) as\nwell as the deltas of those features, which proved\nempirically to be the best number of coefﬁcients.\nThe MFCCs are extracted from the original signal\nwithout separation, as it reﬂects how the whole song\nis perceived.\nBy extracting this set of features for an input ﬁle, we\nend up with a vector of 43 features to be used in estimating\nthe intelligibility of the lyrics in this song.\n4.3 Model training\nWe used the dataset and ground-truth collected in our be-\nhavioral experiment to train a Support Vector Machine\nmodel to estimate the intelligibility of the lyrics. To cat-\negorize the intelligibility to different levels that would\nmatch a language student’s ﬂuency level, we divided our\nHigh Moderate LowHigh\nModerate\nLow33 9 1\n10 30 2\n4 8 3Confusion Matrix of the complete datasetFigure 2 . Confusion Matrix of the SVM output.\ndataset to three classes:\nHigh Intelligibility : excerpts with transcription\naccuracy of greater than 0.66.\nModerate Intelligibility : excerpts with tran-\nscription accuracy between 0.33 and 0.66 inclusive.\nLow Intelligibility : excerpts with transcription\naccuracy of less than 0.33.\nOut of the 100 samples in our dataset, 43 are in the High\nIntelligibility class, 42 are in the Moderate Intelligibility\nclass, and the remaining 15 are in the Low Intelligibility\nclass. For this pilot study, we tried a number of common\nclassiﬁers, including Support Vector Machine (SVM), ran-\ndom forest and k-nearest neighbors. Our trials for ﬁnding\na suitable model led to using SVM with a linear kernel, as\nit is an efﬁcient, fast and simple model which is suitable\nfor this problem. Finally, as a preprocessing step, we nor-\nmalize all the input feature vectors before passing them to\nthe model to be trained.\n5. MODEL EV ALUATION\nBecause this problem has not been addressed before in the\nliterature, and it is not possible to perform evaluation us-\ning other methods, we based our evaluation on classiﬁca-\ntion accuracy from the dataset. Given the relatively small\nnumber of samples in the dataset, we used leave-one-out\ncross-validation for evaluation. To evaluate the perfor-\nmance of our model, we compute overall accuracy, as well\nas the Area Under the ROC Curve (AUC). We scored AUC\nof 0.71 and accuracy of 66% with the aforementioned set\nof features and model. The confusion matrix of validat-\ning our model using leave-one-out cross-validation on our\ncollected dataset is shown in Figure 2. The ﬁgure shows\nthat the classiﬁer has relatively more accuracy in predicting\nhigh and moderate than low intelligibility, which is often\nconfused with the moderate class. Given that our ﬁndings\nare based on a relatively small segment of excerpts with\nlow intelligibility, the classiﬁer was found to be trained to\nwork better on the high and moderate excerpts.\nFollowing model evaluation on the complete dataset, we\nwere interested in investigating how the model performs on\ndifferent genres, speciﬁcally how it performs when tested690 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017High Moderate LowHigh\nModerate\nLow4 2 3\n1 7 0\n1 1 1Confusion Matrix of Rock Genre\nHigh Moderate LowHigh\nModerate\nLow8 3 0\n1 5 0\n1 1 1Confusion Matrix of Classical Genre\nHigh Moderate LowHigh\nModerate\nLow6 3 0\n3 5 0\n1 2 0Confusion Matrix of Folk Genre\nHigh Moderate LowHigh\nModerate\nLow4 0 0\n5 6 1\n2 1 1Confusion Matrix of R&B Genre\nHigh Moderate LowHigh\nModerate\nLow7 3 0\n3 5 0\n0 2 0Confusion Matrix of Jazz GenreFigure 3 . Confusion matrix of the different genres\nGenre Classiﬁcation Accuracy\nPop/Rock 60%\nR&B 55%\nClassical 70%\nFolk 55%\nJazz 60%\nTable 1 . Classiﬁcation accuracy for different genres\nwith a genre that was not included in the training dataset.\nThis would imply how the model generalizes when run-\nning on different genres that was not present during train-\ning, as well as showing how changing genres affect classi-\nﬁcation accuracy. We performed an evaluation where we\ntrained our model using 4 out of the 5 genres in our dataset,\nand tested it on the 5th genre. The classiﬁcation accuracy\nacross different genres is shown in Table 1. The results\nshow variance in classifying different genres. For exam-\nple, Classical music receives higher accuracy, while gen-\nres as Rhythm and Blues and Folk shows less accuracy.\nBy analyzing the confusion matrices of each genre shown\nin Figure 3, we found that the confusion is mainly between\nhigh and moderate classes.\nBy reviewing the impact of the different features on the\nclassiﬁer performance, we looked into what features have\nthe biggest impact using the attribute ranking feature in\nWeka [35]. We found that several MFCCs contribute most\nin differentiating between the three classes, which we in-\nterpret to be due to analyzing the signal in different fre-\nquency sub-bands incorporates perceptual information of\nboth the singing voice and the background music. This\nwas followed by the features reﬂecting the syllable rate in\nthe song, because singing rate can radically affect the intel-\nligibility. V ocals-to-Accompaniment Ratio and High Fre-\nquency Energy followed in their impact on differentiating\nbetween the three classes. The features that had the least\nimpact were the tempo and event density, which does notnecessarily reﬂect the rate of singing.\nFor further studies on the suitability of the features in\nclassifying songs with very low intelligibility, the genres\npool can be extended to include other genres with lower\nintelligibility, rather than being limited to the popular gen-\nres between students. Further studies can also include the\nfeature selection and evaluation process: similar to the\nwork in [33], deep learning methods may be explored to\nselect the features which perform best, rather than hand-\npicking features, to ﬁnd the most suitable set of features\nfor this problem. It is possible to extend the categorical ap-\nproach of intelligibility levels to a regression problem, in\nwhich the system evaluates the song’s intelligibility with a\npercentage. Similarly, certain ranges of the intelligibility\nscore can be used to recommend songs to students based\non their ﬂuency level.\n6. CONCLUSION\nIn this study, we investigated the problem of evaluating the\nintelligibility of song lyrics to provide an aid for language\nlearners who listen to music as part of language immersion.\nWe conducted a behavioral experiment to review how the\nintelligibility of lyrics in different genres of songs are per-\nceived by human participants. We then developed a com-\nputational system to automatically estimate the intelligibil-\nity of lyrics in a given song. In our system, we proposed\nfeatures to reﬂect different factors that affect the intelli-\ngibility of lyrics according to previous empirical studies.\nWe used the proposed features along with standard audio\nfeatures to train a model capable of estimating the intelli-\ngibility of lyrics (as low, moderate, or high intelligibility)\nwith an AUC of 0.71. The study provides evidence that the\nproposed system has promising initial results, and draws\nattention to the problem of lyrics intelligibility, which has\nreceived little attention in terms of computational audio\nanalysis and automatic evaluation.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 6917. REFERENCES\n[1] Martha S Benolken and Charles E Swanson. The effect\nof pitch-related changes on the perception of sung vow-\nels.The Journal of the Acoustical Society of America ,\n87(4):1781–1785, 1990.\n[2] Ugo Cesari, Maurizio Iengo, and Pasqualina Apisa.\nQualitative and quantitative measurement of the\nsinging voice. Folia Phoniatrica et Logopaedica ,\n64(6):304–309, 2013.\n[3] Lauren Collister and David Huron. Comparison of\nword intelligibility in spoken and sung phrases. Em-\npirical Musicology Review , 3(3):109–125, 2–8.\n[4] Nathaniel Condit-Schultz and David Huron. Catch-\ning the lyrics. Music Perception: An Interdisciplinary\nJournal , 32(5):470–483, 2015.\n[5] Alain De Cheveign ´e and Hideki Kawahara. Yin, a\nfundamental frequency estimator for speech and mu-\nsic.The Journal of the Acoustical Society of America ,\n111(4):1917–1930, 2002.\n[6] Aihong Du, Chundan Lin, and Jingjing Wang. Effect\nof speech rate for sentences on speech intelligibility. In\nCommunication Problem-Solving (ICCP), 2014 IEEE\nInternational Conference on , pages 233–236. IEEE,\n2014.\n[7] Harry Hollien, Ana Mendes-Schwartz, and Kenneth\nNielsen. Perceptual confusions of high-pitched sung\nvowels. Journal of Voice , 14(2):287–298, 2000.\n[8] Kristoffer Jensen and Tue Haste Andersen. Real-time\nbeat estimationusing feature extraction. In Interna-\ntional Symposium on Computer Music Modeling and\nRetrieval , pages 13–22. Springer, 2003.\n[9] Randolph Johnson, David Huron, and Lauren Collis-\nter. Music and lyrics interaction and their inﬂuence on\nrecognition of sung words: an investigation of word\nfrequency, rhyme, metric stress, vocal timbre, melisma,\nand repetition priming. Empirical Musicology Review ,\n9(1):2–20, 2014.\n[10] Randolph B Johnson, David Huron, and Lauren Col-\nlister. Music and lyrics interactions and their inﬂuence\non recognition of sung words: an investigation of word\nfrequency, rhyme, metric stress, vocal timbre, melisma,\nand repetition priming. Empirical Musicology Review ,\n9(1):2–20, 2013.\n[11] Tung-an Kao and Rebecca Oxford. Learning language\nthrough music: A strategy for building inspiration and\nmotivation. System , 43:114–120, 2014.\n[12] Anne Kultti. Singing as language learning activity in\nmultilingual toddler groups in preschool. Early Child\nDevelopment and Care , 183(12):1955–1969, 2013.[13] Olivier Lartillot and Petri Toiviainen. A matlab toolbox\nfor musical feature extraction from audio. https://\nwww.jyu.fi/hytk/fi/laitokset/mutku/\nen/research/materials/mirtoolbox ,\n2007.\n[14] Bernhard Lehner and Gerhard Widmer. Monaural blind\nsource separation in the context of vocal detection. In\n16th International Society for Music Information Re-\ntrieval Conference (ISMIR), At Malaga, Spain , 2015.\n[15] Bernhard Lehner, Gerhard Widmer, and Reinhard\nSonnleitner. On the reduction of false positives in\nsinging voice detection. In 2014 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 7480–7484. IEEE, 2014.\n[16] Antoine Liutkus, Zafar Raﬁi, Roland Badeau, Bryan\nPardo, and Ga ¨el Richard. Adaptive ﬁltering for mu-\nsic/voice separation exploiting the repeating musical\nstructure. In 2012 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 53–56. IEEE, 2012.\n[17] Paul Masri and Andrew Bateman. Improved modelling\nof attack transients in music analysis-resynthesis. In\nICMC , 1996.\n[18] Annamaria Mesaros and Tuomas Virtanen. Automatic\nrecognition of lyrics in singing. EURASIP Journal on\nAudio, Speech, and Music Processing , 2010.\n[19] Carmen Mora. Foreign language acquisition and\nmelody singing. ELT journal , 54(2):146–152, 2000.\n[20] Kazuma Mori and Makoto Iwanaga. Pleasure gener-\nated by sadness: Effect of sad lyrics on the emotions\ninduced by happy music. Psychology of Music , 42(5),\n2014.\n[21] Tomoyasu Nakano. An automatic singing skill evalua-\ntion method for unknown melodies using pitch interval\naccuracy and vibrato features. In Proceedings of IN-\nTERSPEECH2006 , 2006.\n[22] Joan Netten and Claude Germain. A new paradigm for\nthe learning of a second or foreign language: the neu-\nrolinguistic approach. Neuroeducation , 1(1), 2012.\n[23] Koichi Omori, Ashutosh Kacker, Linda Carroll,\nWilliam Riley, and Stanley Blaugrund. Singing power\nratio: quantitative evaluation of singing voice quality.\nJournal of Voice , 10(3):228–235, 1996.\n[24] Aniruddh Patel. Language, music, syntax and the\nbrain. Nature Neuroscience , 6(7):674–681, 2003.\n[25] Lawrence R Rabiner and Biing-Hwang Juang. Fun-\ndamentals of speech recognition . PTR Prentice Hall,\n1993.\n[26] Zafar Raﬁi and Bryan Pardo. Repeating pattern ex-\ntraction technique (repet): A simple method for\nmusic/voice separation. IEEE transactions on audio,\nspeech, and language processing , 21(1):73–84, 2013.692 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[27] Mathieu Ramona, Ga ¨el Richard, and Bertrand David.\nV ocal detection in music with support vector machines.\nInProc. ICASSP ’08 , pages 1885–1888, March 31 -\nApril 4 2008.\n[28] Vishweshwara Rao, Chitralekha Gupta, and Preeti\nRao. Context-aware features for singing voice de-\ntection in polyphonic music. In International Work-\nshop on Adaptive Multimedia Retrieval , pages 43–57.\nSpringer, 2011.\n[29] Martın Rocamora and Perfecto Herrera. Comparing au-\ndio descriptors for singing voice detection in music au-\ndio ﬁles. In Brazilian symposium on computer music,\n11th. san pablo, brazil , volume 26, page 27, 2007.\n[30] Daniele Schon, Sylvain Moreno, Mireille Besson, Is-\nabelle Peretz, and Regine Kolinsky. Songs as an aid\nfor language acquisition. Cognition , 106(2):975–983,\n2008.\n[31] Robert Speer, Joshua Chin, Andrew Lin, Lance\nNathan, and Sara Jewett. wordfreq: v1.5.1. https:\n//doi.org/10.5281/zenodo.61937 , Septem-\nber 2016.\n[32] Valerie Trollinger. The brain in singing and language.\nGeneral Music Today , 23(2), 2010.\n[33] Xinxi Wang and Ye Wang. Improving content-based\nand hybrid music recommendation using deep learn-\ning. In Proceedings of the 22nd ACM international\nconference on Multimedia , pages 627–636. ACM,\n2014.\n[34] Christopher Watts, Kathryn Barnes-Burroughs, Julie\nEstis, and Debra Blanton. The singing power ratio as\nan objective measure of singing voice quality in un-\ntrained talented and nontalented singers. Journal of\nVoice , 20(1):82–88, 2006.\n[35] Ian H Witten, Eibe Frank, Mark A Hall, and Christo-\npher J Pal. Data Mining: Practical machine learning\ntools and techniques . Morgan Kaufmann, 2016.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 693"
    },
    {
        "title": "Singing Voice Separation with Deep U-Net Convolutional Networks.",
        "author": [
            "Andreas Jansson 0001",
            "Eric J. Humphrey",
            "Nicola Montecchio",
            "Rachel M. Bittner",
            "Aparna Kumar",
            "Tillman Weyde"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1414934",
        "url": "https://doi.org/10.5281/zenodo.1414934",
        "ee": "https://zenodo.org/records/1414934/files/JanssonHMBKW17.pdf",
        "abstract": "The decomposition of a music audio signal into its vocal and backing track components is analogous to image-to- image translation, where a mixed spectrogram is trans- formed into its constituent sources. We propose a novel application of the U-Net architecture — initially devel- oped for medical imaging — for the task of source sep- aration, given its proven capacity for recreating the fine, low-level detail required for high-quality audio reproduc- tion. Through both quantitative evaluation and subjective assessment, experiments demonstrate that the proposed al- gorithm achieves state-of-the-art performance.",
        "zenodo_id": 1414934,
        "dblp_key": "conf/ismir/JanssonHMBKW17",
        "keywords": [
            "decomposition",
            "vocal",
            "backing track",
            "U-Net",
            "source separation",
            "mixed spectrogram",
            "fine",
            "low-level detail",
            "audio reproduction",
            "state-of-the-art performance"
        ],
        "content": "SINGING VOICE SEPARATION WITH DEEP U-NET CONVOLUTIONAL\nNETWORKS\nAndreas Jansson1, 2,Eric Humphrey2,Nicola Montecchio2,\nRachel Bittner2,Aparna Kumar2,Tillman Weyde1\n1City, University of London,2Spotify\nfandreas.jansson.1, t.e.weyde g@city.ac.uk\nfejhumphrey, venice, rachelbittner, aparna g@spotify.com\nABSTRACT\nThe decomposition of a music audio signal into its vocal\nand backing track components is analogous to image-to-\nimage translation, where a mixed spectrogram is trans-\nformed into its constituent sources. We propose a novel\napplication of the U-Net architecture — initially devel-\noped for medical imaging — for the task of source sep-\naration, given its proven capacity for recreating the ﬁne,\nlow-level detail required for high-quality audio reproduc-\ntion. Through both quantitative evaluation and subjective\nassessment, experiments demonstrate that the proposed al-\ngorithm achieves state-of-the-art performance.\n1. INTRODUCTION\nThe ﬁeld of Music Information Retrieval (MIR) concerns\nitself, among other things, with the analysis of music in\nits many facets, such as melody, timbre or rhythm [20].\nAmong those aspects, popular western commercial mu-\nsic (“pop” music) is arguably characterized by emphasiz-\ning mainly the Melody and Accompaniment aspects; while\nthis is certainly an oversimpliﬁcation in the context of the\nwhole genre, we restrict the focus of this paper to the\nanalysis of music that lends itself well to be described in\nterms of a main melodic line (foreground) and accompa-\nniment (background) [27]. Normally the melody is sung,\nwhereas the accompaniment is performed by one or more\ninstrumentalists; a singer delivers the lyrics, and the back-\ning musicians provide harmony as well as genre and style\ncues [29].\nThe task of automatic singing voice separation consists\nof estimating what the sung melody and accompaniment\nwould sound like in isolation. A clean vocal signal is help-\nful for other related MIR tasks, such as singer identiﬁca-\ntion [18] and lyric transcription [17]. As for commercial\napplications, it is evident that the karaoke industry, esti-\nmated to be worth billions of dollars globally [4], would\nc\rAndreas Jansson, Eric Humphrey, Nicola Montecchio,\nRachel Bittner, Aparna Kumar, Tillman Weyde. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Andreas Jansson, Eric Humphrey, Nicola Montecchio, Rachel\nBittner, Aparna Kumar, Tillman Weyde. “Singing V oice Separation with\nDeep U-Net Convolutional Networks”, 18th International Society for Mu-\nsic Information Retrieval Conference, Suzhou, China, 2017.directly beneﬁt from such technology.\n2. RELATED WORK\nSeveral techniques have been proposed for blind source\nseparation of musical audio. Successful results have been\nachieved with non-negative matrix factorization [26, 30,\n32], Bayesian methods [21], and the analysis of repeating\nstructures [23].\nDeep learning models have recently emerged as power-\nful alternatives to traditional methods. Notable examples\ninclude [25] where a deep feed-forward network learns to\nestimate an ideal binary spectrogram mask that represents\nthe spectrogram bins in which the vocal is more prominent\nthan the accompaniment. In [9] the authors employ a deep\nrecurrent architecture to predict soft masks that are multi-\nplied with the original signal to obtain the desired isolated\nsource.\nConvolutional encoder-decoder architectures have been\nexplored in the context of singing voice separation in [6]\nand [8]. In both of these works, spectrograms are com-\npressed through a bottleneck layer and re-expanded to the\nsize of the target spectrogram. While this “hourglass” ar-\nchitecture is undoubtedly successful in discovering global\npatterns, it is unclear how much local detail is lost during\ncontraction.\nOne potential weakness shared by the papers cited\nabove is the lack of large training datasets. Existing mod-\nels are usually trained on hundreds of tracks of lower-than-\ncommercial quality, and may therefore suffer from poor\ngeneralization. In this work we aim to mitigate this prob-\nlem using weakly labeled professionally produced music\ntracks.\nOver the last few years, considerable improvements\nhave occurred in the family of machine learning algorithms\nknown as image-to-image translation [11] — pixel-level\nclassiﬁcation [2], automatic colorization [33], image seg-\nmentation [1] — largely driven by advances in the design\nof novel neural network architectures.\nThis paper formulates the voice separation task, whose\ndomain is often considered from a time-frequency perspec-\ntive, as the translation of a mixed spectrogram into vocal\nand instrumental spectrograms. By using this framework\nwe aim to make use of some of the advances in image-to-\nimage translation — especially in regard to the reproduc-745tion of ﬁne-grained details — to advance the state-of-the-\nart of blind source separation for music.\n3. METHODOLOGY\nThis work adapts the U-Net [24] architecture to the task\nof vocal separation. The architecture was introduced in\nbiomedical imaging, to improve precision and localization\nof microscopic images of neuronal structures. The archi-\ntecture builds upon the fully convolutional network [14]\nand is similar to the deconvolutional network [19]. In a de-\nconvolutional network, a stack of convolutional layers —\nwhere each layer halves the size of the image but doubles\nthe number of channels — encodes the image into a small\nand deep representation. That encoding is then decoded\nto the original size of the image by a stack of upsampling\nlayers.\nIn the reproduction of a natural image, displacements\nby just one pixel are usually not perceived as major dis-\ntortions. In the frequency domain however, even a mi-\nnor linear shift in the spectrogram has disastrous effects\non perception: this is particularly relevant in music sig-\nnals, because of the logarithmic perception of frequency;\nmoreover, a shift in the time dimension can become audi-\nble as jitter and other artifacts. Therefore, it is crucial that\nthe reproduction preserves a high level of detail. The U-\nNet adds additional skip connections between layers at the\nsame hierarchical level in the encoder and decoder. This al-\nlows low-level information to ﬂow directly from the high-\nresolution input to the high-resolution output.\n3.1 Architecture\nThe goal of the neural network architecture is to predict the\nvocal and instrumental components of its input indirectly:\nthe output of the ﬁnal decoder layer is a soft mask that is\nmultiplied element-wise with the mixed spectrogram to ob-\ntain the ﬁnal estimate. Figure 1 outlines the network archi-\ntecture. In this work, we choose to train two separate mod-\nels for the extraction of the instrumental and vocal com-\nponents of a signal, to allow for more divergent training\nschemes for the two models in the future.\n3.1.1 Training\nLetXdenote the magnitude of the spectrogram of the orig-\ninal, mixed signal, that is, of the audio containing both vo-\ncal and instrumental components. Let Ydenote the mag-\nnitude of the spectrograms of the target audio; the latter\nrefers to either the vocal ( Yv) or the instrumental ( Yi) com-\nponent of the input signal.\nThe loss function used to train the model is the L1;1\nnorm1of the difference of the target spectrogram and the\nmasked input spectrogram:\nL(X; Y ; \u0002) =jjf(X;\u0002)\fX\u0000Yjj1;1 (1)\nwhere f(X;\u0002)is the output of the network model applied\nto the input Xwith parameters \u0002– that is the mask gener-\nated by the model.\n1TheL1;1norm of a matrix is simply the sum of the absolute values\nof its elements.Two U-Nets, \u0002vand\u0002i, are trained to predict vocal and\ninstrumental spectrogram masks, respectively.\n3.1.2 Network Architecture Details\nOur implementation of U-Net is similar to that of [11].\nEach encoder layer consists of a strided 2D convolution\nof stride 2 and kernel size 5x5, batch normalization, and\nleaky rectiﬁed linear units (ReLU) with leakiness 0.2. In\nthe decoder we use strided deconvolution (sometimes re-\nferred to as transposed convolution) with stride 2 and ker-\nnel size 5x5, batch normalization, plain ReLU, and use\n50% dropout to the ﬁrst three layers, as in [11]. In the ﬁnal\nlayer we use a sigmoid activation function. The model is\ntrained using the ADAM [12] optimizer.\nGiven the heavy computational requirements of train-\ning such a model, we ﬁrst downsample the input audio to\n8192 Hz in order to speed up processing. We then com-\npute the Short Time Fourier Transform with a window size\nof 1024 and hop length of 768 frames, and extract patches\nof 128 frames (roughly 11 seconds) that we feed as input\nand targets to the network. The magnitude spectrograms\nare normalized to the range [0;1].\n3.1.3 Audio Signal Reconstruction\nThe neural network model operates exclusively on the\nmagnitude of audio spectrograms. The audio signal for an\nindividual (vocal/instrumental) component is rendered by\nconstructing a spectrogram: the output magnitude is given\nby applying the mask predicted by the U-Net to the magni-\ntude of the original spectrum, while the output phase is that\nof the original spectrum, unaltered. Experimental results\npresented below indicate that such a simple methodology\nproves effective.\n3.2 Dataset\nAs stated above, the description of the model architec-\nture assumes that training data was available in the form\nof a triplet (original signal, vocal component, instrumental\ncomponent). Unless one is in the extremely fortunate po-\nsition as to have access to vast amounts of unmixed multi-\ntrack recordings, an alternative strategy has to be found in\norder to train a model like the one described.\nA solution to the issue was found by exploiting a spe-\nciﬁc but large set of commercially available recordings in\norder to “construct” training data: instrumental versions of\nrecordings.\nIt is not uncommon for artists to release instrumental\nversions of tracks along with the original mix. We lever-\nage this fact by retrieving pairs of (original, instrumental)\ntracks from a large commercial music database. Candi-\ndates are found by examining the metadata for tracks with\nmatching duration and artist information, where the track\ntitle (fuzzily) matches except for the string “Instrumen-\ntal” occurring in exactly one title in the pair. The pool\nof tracks is pruned by excluding exact content matches.\nDetails about the construction of this dataset can be found\nin [10].746 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 1 . Network Architecture\nGenre Percentage\nPop 26.0%\nRap 21.3%\nDance & House 14.2%\nElectronica 7.4%\nR&B 3.9%\nRock 3.6%\nAlternative 3.1%\nChildren’s 2.5%\nMetal 2.5%\nLatin 2.3%\nIndie Rock 2.2%\nOther 10.9%\nTable 1 . Training data genre distribution\nThe above approach provides a large source of X\n(mixed) and Yi(instrumental) magnitude spectrogram\npairs. The vocal magnitude spectrogram Yvis obtained\nfrom their half-wave rectiﬁed difference. A qualitative\nanalysis of a large handful of examples showed that this\ntechnique produced reasonably isolated vocals.\nThe ﬁnal dataset contains approximately 20,000 track\npairs, resulting in almost two months worth of continuous\naudio. To the best of our knowledge, this is the largest\ntraining data set ever applied to musical source separation.\nTable 1 shows the relative distribution of the most frequentgenres in the dataset, obtained from the catalog metadata.\n4. EVALUATION\nWe compare the proposed model to the Chimera model\n[15] that produced the highest evaluation scores in the 2016\nMIREX Source Separation campaign2; we make use of\ntheir web interface3to process audio clips. It should be\nnoted that the Chimera web server is running an improved\nversion of the algorithm that participated in MIREX, using\na hybrid “multiple heads” architecture that combines deep\nclustering with a conventional neural network [16].\nFor evaluation purposes we built an additional baseline\nmodel; it resembles the U-Net model but without the skip\nconnections, essentially creating a convolutional encoder-\ndecoder, similar to the “Deconvnet” [19].\nWe evaluate the three models on the standard iKala [5]\nand MedleyDB dataset [3]. The iKala dataset has been\nused as a standardized evaluation for the annual MIREX\ncampaign for several years, so there are many existing\nresults that can be used for comparison. MedleyDB on\nthe other hand was recently proposed as a higher-quality,\ncommercial-grade set of multi-track stems. We generate\nisolated instrumental and vocal tracks by weighting sums\nof instrumental/vocal stems by their respective mixing co-\n2www.music-ir.org/mirex/wiki/2016:Singing_\nVoice_Separation_Results\n3danetapi.com/chimeraProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 747U-Net Baseline Chimera\nNSDR V ocal 11.094 8.549 8.749\nNSDR Instrumental 14.435 10.906 11.626\nSIR V ocal 23.960 20.402 21.301\nSIR Instrumental 21.832 14.304 20.481\nSAR V ocal 17.715 15.481 15.642\nSAR Instrumental 14.120 12.002 11.539\nTable 2 . iKala mean scores\nU-Net Baseline Chimera\nNSDR V ocal 8.681 7.877 6.793\nNSDR Instrumental 7.945 6.370 5.477\nSIR V ocal 15.308 14.336 12.382\nSIR Instrumental 21.975 16.928 20.880\nSAR V ocal 11.301 10.632 10.033\nSAR Instrumental 15.462 15.332 12.530\nTable 3 . MedleyDB mean scores\nefﬁcients as supplied by the MedleyDB Python API4. We\nlimit our evaluation to clips that are known to contain\nvocals, using the melody transcriptions provided in both\niKala and MedleyDB.\nThe following functions are used to measure per-\nformance: Signal-To-Distortion Ratio (SDR), Signal-to-\nInterference Ratio (SIR), and Signal-to-Artifact Ratio\n(SAR) [31]. Normalized SDR (NSDR) is deﬁned as\nNSDR (Se; Sr; Sm) =SDR(Se; Sr)\u0000SDR(Sm; Sr)(2)\nwhere Seis the estimated isolated signal, Sris the refer-\nence isolated signal, and Smis the mixed signal. We com-\npute performance measures using the mirevaltoolkit [22].\nTable 2 and Table 3 show that the U-Net signiﬁcantly\noutperforms both the baseline model and Chimera on all\nthree performance measures for both datasets. In Figure 2\nwe show an overview of the distributions for the different\nevaluation measures.\nAssuming that the distribution of tracks in the iKala\nhold-out set used for MIREX evaluations matches those\nin the public iKala set, we can compare our results to the\nparticipants in the 2016 MIREX Singing V oice Separation\ntask.5Table 4 and Table 5 show NSDR scores for our\nmodels compared to the best performing algorithms of the\n2016 MIREX campaign.\nIn order to assess the effect of the U-Net’s skip connec-\ntions, we can visualize the masks generated by the U-Net\nand baseline models. From Figure 3 it is clear that while\nthe baseline model captures the overall structure, there is a\nlack of ﬁne-grained detail observable.\n4.1 Subjective Evaluation\nEmiya et al. introduced a protocol for the subjective eval-\nuation of source separation algorithms [7]. They suggest\n4github.com/marl/medleyDB\n5http://www.music-ir.org/mirex/wiki/2016:\nSinging_Voice_Separation_ResultsModel Mean SD Min Max Median\nU-Net 14.435 3.583 4.165 21.716 14.525\nBaseline 10.906 3.247 1.846 19.641 10.869\nChimera 11.626 4.151 -0.368 20.812 12.045\nLCP2 11.188 3.626 2.508 19.875 11.000\nLCP1 10.926 3.835 0.742 19.960 10.800\nMC2 9.668 3.676 -7.875 22.734 9.900\nTable 4 . iKala NSDR Instrumental, MIREX 2016\nModel Mean SD Min Max Median\nU-Net 11.094 3.566 2.392 20.720 10.804\nBaseline 8.549 3.428 -0.696 18.530 8.746\nChimera 8.749 4.001 -1.850 18.701 8.868\nLCP2 6.341 3.370 -1.958 17.240 5.997\nLCP1 6.073 3.462 -1.658 17.170 5.649\nMC2 5.289 2.914 -1.302 12.571 4.945\nTable 5 . iKala NSDR V ocal, MIREX 2016\nasking human subjects four questions that broadly corre-\nspond to the SDR/SIR/SAR measures, plus an additional\nquestion regarding the overall sound quality.\nAs we asked these four questions to subjects without\nmusic training, our subjects found them ambiguous, e.g.,\nthey had problems discerning between the absence of arti-\nfacts and general sound quality. For better clarity, we dis-\ntilled the survey into the following two questions in the\nvocal extraction case:\n\u000fQuality: “Rate the vocal quality in the examples be-\nlow.”\n\u000fInterference: “How well have the instruments in the\nclip above been removed in the examples below?”\nFor instrumental extraction we asked similar questions:\n\u000fQuality: “Rate the sound quality of the examples be-\nlow relative to the reference above.”\n\u000fExtracting instruments: “Rate how well the instru-\nments are isolated in the examples below relative to\nthe full mix above.”\nData was collected using CrowdFlower6, an online\nplatform where humans carry out micro-tasks, such as im-\nage classiﬁcation, simple web searches, etc., in return for\nsmall per-task payments.\nIn our survey, CrowdFlower users were asked to listen\nto three clips of isolated audio, generated by U-Net, the\nbaseline model, and Chimera. The order of the three clips\nwas randomized. Each question asked one of the Quality\nand Interference questions. In the Interference question\nwe also included a reference clip. The answers were given\naccording to a 7 step Likert scale [13], ranging from “Poor”\nto “Perfect”. Figure 4 is a screen capture of a CrowdFlower\nquestion.\n6www.crowdflower.com748 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017U-Net NSDR\nBaseline NSDRChimera NSDRU-Net SIR\nBaseline SIRChimera SIRU-Net SAR\nBaseline SARChimera SAR5\n0510152025303540iKala Vocal\nU-Net NSDR\nBaseline NSDRChimera NSDRU-Net SIR\nBaseline SIRChimera SIRU-Net SAR\nBaseline SARChimera SAR5\n0510152025303540iKala InstrumentalFigure 2 . iKala vocal and instrumental scores\nFigure 3 . U-Net and baseline masks\nFigure 4 . CrowdFlower example question\nTo ensure the quality of the collected responses, we in-\nterspersed the survey with “control questions” that the user\nhad to answer correctly according to a predeﬁned set of ac-\nceptable answers on the Likert scale. Users of the platform\nare unaware of which questions are control questions. If\nthey are answered incorrectly, the user is disqualiﬁed from\nthe task. A music expert external to our research group\nwas asked to provide acceptable answers to a number of\nrandom clips that were designated as control questions.\nFor the survey we used 25 clips from the iKala dataset\nand 42 clips from MedleyDB. We had 44 respondents and\n724 total responses for the instrumental test, and 55 re-spondents supplied 779 responses for the voice test7.\nFigure 5 shows mean and standard deviation for an-\nswers provided on CrowdFlower. The U-Net algorithm\noutperforms the other two models on all questions.\n5. CONCLUSION AND FUTURE WORK\nWe have explored the U-Net architecture in the context of\nsinging voice separation, and found that it brings clear im-\nprovements over the state-of-the-art. The beneﬁts of low-\nlevel skip connections were demonstrated by comparison\nto plain convolutional encoder-decoders.\nA factor that we feel should be investigated further is\nthe impact of large training data: work remains to be done\nto correlate the effects of the size of the training dataset to\nthe quality of source separation.\nWe have observed some examples of poor separation on\ntracks where the vocals are mixed at lower-than-average\nvolume, uncompressed, suffer from extreme application of\naudio effects, or otherwise unconventionally mixed. Since\nthe training data consisted exclusively of commercially\nproduced recordings, we hypothesize that our model has\nlearned to distinguish the kind of voice typically found in\ncommercial pop music. We plan to investigate this further\nby systematically analyzing the dependence of model per-\nformance on the mixing conditions.\nFinally, subjective evaluation of source separation al-\ngorithms is an open research question. Several alternatives\nexist to 7-step Likert scale, e.g. the ITU-R scale [28]. Tools\nlike CrowdFlower allow us to quickly roll out surveys, but\ncare is required in the design of question statements.\n6. REFERENCES\n[1] Vijay Badrinarayanan, Alex Kendall, and Roberto\nCipolla. Segnet: A deep convolutional encoder-\ndecoder architecture for scene segmentation. IEEE\nTransactions on Pattern Analysis and Machine Intel-\nligence , 2017.\n7Some of the audio clips we used for evaluation can\nbe found on http://mirg.city.ac.uk/codeapps/\nvocal-source-separation-ismir2017Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 749U-Net Quality\nBaseline Quality Chimera Quality\nU-Net Interference\nBaseline Interference Chimera Interference1234567MedleyDB vocal\nU-Net Quality\nBaseline Quality Chimera Quality\nU-Net Interference\nBaseline Interference Chimera Interference1234567iKala vocal\nU-Net Quality\nBaseline Quality Chimera Quality\nU-Net Interference\nBaseline Interference Chimera Interference1234567MedleyDB instrumental\nU-Net Quality\nBaseline Quality Chimera Quality\nU-Net Interference\nBaseline Interference Chimera Interference1234567iKala instrumentalFigure 5 . CrowdFlower evaluation results (mean/std)\n[2] Aayush Bansal, Xinlei Chen, Bryan Russell, Ab-\nhinav Gupta, and Deva Ramanan. Pixelnet: To-\nwards a general pixel-level architecture. arXiv preprint\narXiv:1609.06694 , 2016.\n[3] Rachel M. Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo\nBello. MedleyDB: A multitrack dataset for annotation-\nintensive MIR research. In Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference, ISMIR 2014, Taipei, Taiwan, October 27-\n31, 2014 , pages 155–160, 2014.\n[4] Kevin Brown. Karaoke Idols: Popular Music and the\nPerformance of Identity . Intellect Books, 2015.\n[5] Tak-Shing Chan, Tzu-Chun Yeh, Zhe-Cheng Fan,\nHung-Wei Chen, Li Su, Yi-Hsuan Yang, and Roger\nJang. V ocal activity informed singing voice separation\nwith the iKala dataset. In Acoustics, Speech and Signal\nProcessing (ICASSP), 2015 IEEE International Con-\nference on , pages 718–722. IEEE, 2015.\n[6] Pritish Chandna, Marius Miron, Jordi Janer, and Emilia\nG´omez. Monoaural audio source separation using deep\nconvolutional neural networks. In International Con-\nference on Latent Variable Analysis and Signal Sepa-\nration , pages 258–266. Springer, 2017.\n[7] Valentin Emiya, Emmanuel Vincent, Niklas Harlander,\nand V olker Hohmann. Subjective and objective qual-\nity assessment of audio source separation. IEEE Trans-actions on Audio, Speech, and Language Processing ,\n19(7):2046–2057, 2011.\n[8] Emad M Grais and Mark D Plumbley. Single channel\naudio source separation using convolutional denoising\nautoencoders. arXiv preprint arXiv:1703.08019 , 2017.\n[9] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis. Singing-voice separation from\nmonaural recordings using deep recurrent neural net-\nworks. In Proceedings of the 15th International So-\nciety for Music Information Retrieval Conference, IS-\nMIR 2014, Taipei, Taiwan, October 27-31, 2014 , pages\n477–482, 2014.\n[10] Eric Humphrey, Nicola Montecchio, Rachel Bittner,\nAndreas Jansson, and Tristan Jehan. Mining labeled\ndata from web-scale collections for vocal activity de-\ntection in music. In Proceedings of the 18th ISMIR\nConference , 2017.\n[11] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and\nAlexei A Efros. Image-to-image translation with\nconditional adversarial networks. arXiv preprint\narXiv:1611.07004 , 2016.\n[12] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[13] Rensis Likert. A technique for the measurement of at-\ntitudes. Archives of psychology , 1932.750 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[14] Jonathan Long, Evan Shelhamer, and Trevor Darrell.\nFully convolutional networks for semantic segmenta-\ntion. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 3431–\n3440, 2015.\n[15] Yi Luo, Zhuo Chen, and Daniel PW Ellis. Deep clus-\ntering for singing voice separation. 2016.\n[16] Yi Luo, Zhuo Chen, John R Hershey, Jonathan Le\nRoux, and Nima Mesgarani. Deep clustering and con-\nventional networks for music separation: Stronger to-\ngether. arXiv preprint arXiv:1611.06265 , 2016.\n[17] Annamaria Mesaros and Tuomas Virtanen. Auto-\nmatic recognition of lyrics in singing. EURASIP\nJournal on Audio, Speech, and Music Processing ,\n2010(1):546047, 2010.\n[18] Annamaria Mesaros, Tuomas Virtanen, and Anssi Kla-\npuri. Singer identiﬁcation in polyphonic music using\nvocal separation and pattern recognition methods. In\nProceedings of the 8th International Conference on\nMusic Information Retrieval, ISMIR 2007, Vienna,\nAustria, September 23-27, 2007 , pages 375–378, 2007.\n[19] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.\nLearning deconvolution network for semantic segmen-\ntation. In Proceedings of the IEEE International Con-\nference on Computer Vision , pages 1520–1528, 2015.\n[20] Nicola Orio et al. Music retrieval: A tutorial and re-\nview. Foundations and Trends R\rin Information Re-\ntrieval , 1(1):1–90, 2006.\n[21] Alexey Ozerov, Pierrick Philippe, Frdric Bimbot, and\nRmi Gribonval. Adaptation of bayesian models for\nsingle-channel source separation and its application to\nvoice/music separation in popular songs. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n15(5):1564–1578, 2007.\n[22] Colin Raffel, Brian McFee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel P. W.\nEllis. Mir eval: A transparent implementation of com-\nmon MIR metrics. In Proceedings of the 15th Inter-\nnational Society for Music Information Retrieval Con-\nference, ISMIR 2014, Taipei, Taiwan, October 27-31,\n2014 , pages 367–372, 2014.\n[23] Zafar Raﬁi and Bryan Pardo. Repeating pattern ex-\ntraction technique (REPET): A simple method for\nmusic/voice separation. IEEE transactions on audio,\nspeech, and language processing , 21(1):73–84, 2013.\n[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks for biomedical image\nsegmentation. In International Conference on Medi-\ncal Image Computing and Computer-Assisted Inter-\nvention , pages 234–241. Springer, 2015.[25] Andrew JR Simpson, Gerard Roma, and Mark D\nPlumbley. Deep karaoke: Extracting vocals from mu-\nsical mixtures using a convolutional deep neural net-\nwork. In International Conference on Latent Vari-\nable Analysis and Signal Separation , pages 429–436.\nSpringer, 2015.\n[26] Paris Smaragdis, Cedric Fevotte, Gautham J Mysore,\nNasser Mohammadiha, and Matthew Hoffman. Static\nand dynamic source separation using nonnegative fac-\ntorizations: A uniﬁed view. IEEE Signal Processing\nMagazine , 31(3):66–75, 2014.\n[27] Philip Tagg. Analysing popular music: theory, method\nand practice. Popular music , 2:37–67, 1982.\n[28] Thilo Thiede, William C Treurniet, Roland Bitto,\nChristian Schmidmer, Thomas Sporer, John G\nBeerends, and Catherine Colomes. Peaq-the itu stan-\ndard for objective measurement of perceived audio\nquality. Journal of the Audio Engineering Society ,\n48(1/2):3–29, 2000.\n[29] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nspeech and audio processing , 10(5):293–302, 2002.\n[30] Shankar Vembu and Stephan Baumann. Separation of\nvocals from polyphonic audio recordings. In ISMIR\n2005, 6th International Conference on Music Informa-\ntion Retrieval, London, UK, 11-15 September 2005,\nProceedings , pages 337–344, 2005.\n[31] Emmanuel Vincent, R ´emi Gribonval, and C ´edric\nF´evotte. Performance measurement in blind audio\nsource separation. IEEE transactions on audio, speech,\nand language processing , 14(4):1462–1469, 2006.\n[32] Tuomas Virtanen. Monaural sound source separation\nby nonnegative matrix factorization with temporal con-\ntinuity and sparseness criteria. IEEE transactions on\naudio, speech, and language processing , 15(3):1066–\n1074, 2007.\n[33] Richard Zhang, Phillip Isola, and Alexei A Efros. Col-\norful image colorization. In European Conference on\nComputer Vision , pages 649–666. Springer, 2016.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 751"
    },
    {
        "title": "Geographical Origin Prediction of Folk Music Recordings from the United Kingdom.",
        "author": [
            "Vytaute Kedyte",
            "Maria Panteli",
            "Tillman Weyde",
            "Simon Dixon"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417991",
        "url": "https://doi.org/10.5281/zenodo.1417991",
        "ee": "https://zenodo.org/records/1417991/files/KedytePWD17.pdf",
        "abstract": "Field recordings from ethnomusicological research since the beginning of the 20th century are available today in large digitised music archives. The application of music information retrieval and data mining technologies can aid large-scale data processing leading to a better understand- ing of the history of cultural exchange. In this paper we fo- cus on folk and traditional music from the United Kingdom and study the correlation between spatial origins and mu- sical characteristics. In particular, we investigate whether the geographical location of music recordings can be pre- dicted solely from the content of the audio signal. We build a neural network that takes as input a feature vector cap- turing musical aspects of the audio signal and predicts the latitude and longitude of the origins of the music record- ing. We explore the performance of the model for different sets of features and compare the prediction accuracy be- tween geographical regions of the UK. Our model predicts the geographical coordinates of music recordings with an average error of less than 120 km. The model can be used in a similar manner to identify the origins of recordings in large unlabelled music collections and reveal patterns of similarity in music from around the world.",
        "zenodo_id": 1417991,
        "dblp_key": "conf/ismir/KedytePWD17",
        "keywords": [
            "field recordings",
            "ethnomusicological research",
            "large digitised music archives",
            "music information retrieval",
            "data mining technologies",
            "cultural exchange",
            "folk and traditional music",
            "United Kingdom",
            "geographical location",
            "audio signal"
        ],
        "content": "GEOGRAPHICAL ORIGIN PREDICTION OF FOLK MUSIC\nRECORDINGS FROM THE UNITED KINGDOM\nVytaute Kedyte1Maria Panteli2Tillman Weyde1Simon Dixon2\n1Department of Computer Science, City University of London, United Kingdom\n2Centre for Digital Music, Queen Mary University of London, United Kingdom\nfVytaute.Kedyte, T.E.Weyde g@city.ac.uk, fm.panteli, s.e.dixon g@qmul.ac.uk\nABSTRACT\nField recordings from ethnomusicological research since\nthe beginning of the 20th century are available today in\nlarge digitised music archives. The application of music\ninformation retrieval and data mining technologies can aid\nlarge-scale data processing leading to a better understand-\ning of the history of cultural exchange. In this paper we fo-\ncus on folk and traditional music from the United Kingdom\nand study the correlation between spatial origins and mu-\nsical characteristics. In particular, we investigate whether\nthe geographical location of music recordings can be pre-\ndicted solely from the content of the audio signal. We build\na neural network that takes as input a feature vector cap-\nturing musical aspects of the audio signal and predicts the\nlatitude and longitude of the origins of the music record-\ning. We explore the performance of the model for different\nsets of features and compare the prediction accuracy be-\ntween geographical regions of the UK. Our model predicts\nthe geographical coordinates of music recordings with an\naverage error of less than 120km. The model can be used\nin a similar manner to identify the origins of recordings in\nlarge unlabelled music collections and reveal patterns of\nsimilarity in music from around the world.\n1. INTRODUCTION\nSince the beginning of the 20th century ethnomusicolog-\nical research has contributed signiﬁcantly to the collec-\ntion of recorded music from around the world. Collections\nof ﬁeld recordings are preserved today in digital archives\nsuch as the British Library Sound Archive. The advances\nof Music Information Retrieval (MIR) technologies make\nit possible to process large numbers of music recordings.\nWe are interested in applying these computational tools to\nstudy a large collection of folk and traditional music from\nthe United Kingdom (UK). We focus on exploring music\nattributes with respect to geographical regions of the UK\nand investigate patterns of music similarity.\nc\rVytaute Kedyte, Maria Panteli, Tillman Weyde, Simon\nDixon. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Vytaute Kedyte, Maria Panteli,\nTillman Weyde, Simon Dixon. “Geographical origin prediction of folk\nmusic recordings from the United Kingdom”, 18th International Society\nfor Music Information Retrieval Conference, Suzhou, China, 2017.The comparison of music from different geographical\nregions has been the topic of several studies from the ﬁeld\nof ethnomusicology and in particular the branch of com-\nparative musicology [13]. Savage et al. [17] studied stylis-\ntic similarity within music cultures of Taiwan. In particu-\nlar, they formed music clusters for a collection of 259tra-\nditional songs from twelve indigenous populations of Tai-\nwan and studied the distribution of these clusters across ge-\nographical regions of Taiwan. They showed that songs of\nTaiwan can be grouped into 5clusters correlated with geo-\ngraphical factors and repertoire diversity. Savage et al. [18]\nanalysed 304recordings contained in the ‘Garland Ency-\nclopedia of World Music’ [14] and investigated the dis-\ntribution of music attributes across music recordings from\naround the world. They proposed 18music features that\nare shared amongst many music cultures of the world and\na network of 10features that often occur together.\nThe aforementioned studies incorporated knowledge\nfrom human experts in order to annotate music characteris-\ntics for each recording. While expert knowledge provides\nreliable and in-depth insights into the music, the amount of\nhuman labour involved in the process makes it impractical\nfor large-scale music corpora. Computational tools on the\nother hand provide an efﬁcient solution to processing large\nnumbers of music recordings. In the ﬁeld of MIR several\nstudies have used computational tools to study large music\ncorpora. For example, Mauch et al. [10] studied the evo-\nlution of popular music in the USA in a collection of ap-\nproximately 17000 recordings. They concluded that popu-\nlar music in the US evolved with particular rapidity during\nthree stylistic revolutions, around 1964 ,1983 and1991 .\nWith respect to non-Western music repertoires Moelants et\nal. [12] studied pitch distributions in 901recordings from\nCentral Africa from the beginning until the end of the 20th\ncentury. They observed that recent recordings tend to use\nmore equally-tempered scales than older recordings.\nComputational studies have also focused on predict-\ning the geographic location of recordings from their music\ncontent. Gomez et al. [3] approached prediction of musical\ncultures as a classiﬁcation problem, and classiﬁed music\ntracks into Western and non-Western. They identiﬁed cor-\nrelations between the latitude and tonal features, and the\nlongitude and rhythmic descriptors. Their work illustrates\nthe complexity of using regression to predict the geograph-\nical coordinates of music origin. Zhou et al. [23] also ap-\nproached this as a regression problem, predicting latitudes664and longitudes of the capital city of the music’s country of\norigin, for pieces of music from 73countries. They used\nK-nearest neighbours and Random Forest regression tech-\nniques, and achieved a mean distance error between pre-\ndicted and target coordinates of 3113 kilometres (km). The\nadvantage of treating geographic origin prediction as a re-\ngression problem is that it allows the latitude and longitude\ncorrelations found by Gomez et al. [3] to be considered as\nwell as the topology of the Earth. The disadvantage is not\naccounting for latitudes getting distorted towards the poles,\nand longitudes diverging at \u0006180degrees. Location is usu-\nally used as an input feature in regression models, however\nsome studies have explored prediction of geographical ori-\ngin in a continuous space in the domains of linguistics [2],\ncriminology [22], and genetics [15, 21].\nIn this paper we study the correlation between spatial\norigins and musical characteristics of ﬁeld recordings from\nthe UK. We investigate whether the geographical location\nof a music recording can be predicted solely based on its\naudio content. We extract features capturing musical as-\npects of the audio signal and train a neural network to pre-\ndict the latitude and longitude of the origins of the record-\ning. We investigate the model’s performance for different\nnetwork architectures and learning parameters. We also\ncompare the performance accuracy for several feature sets\nas well as the accuracy across different geographical re-\ngions of the UK.\nOur developments contribute to the evaluation of ex-\nisting audio features and their applicability to folk music\nanalysis. Our results provide insights for music patterns\nacross the UK, but the model can be expanded to process\nmusic recordings from all around the world. This could\ncontribute to identifying the location of recordings in large\nunlabelled music collections as well as studying patterns\nof music similarity in world music.\nThis paper is organised as follows: Section 2 provides\nan overview of the music collection and Section 3 de-\nscribes the different sets of audio features considered in\nthis study. Section 4 provides a detailed description of the\nneural network architecture as well as the training and test-\ning procedures. Section 5 presents the results of the model\nfor different learning parameters, audio features, and geo-\ngraphical areas. We conclude with a discussion and direc-\ntions for future work.\n2. DATASET\nOur music dataset is drawn from the World &Traditional\nmusic collection of the British Library Sound Archive1\nwhich includes thousands of music recordings collected\nover decades of ethnomusicological research. In particu-\nlar, we use a subset of the World &Traditional music col-\nlection curated for the Digital Music Lab project [1]. This\nsubset consists of more than 29000 audio recordings with a\nlarge representation ( 17000 ) from the UK. We focus solely\non recordings from the UK and process information on the\nrecording’s location (if available) to extract the latitude and\n1http://sounds.bl.uk/World-and-traditional-music\n(a) Geographical spread\n0200400600\n1890 1920 1950 1980 2010\nYearCount (b) Year distribution\nFigure 1 : Geographical spread and year distribution in our\ndataset of 10055 traditional music recordings from the UK.\nlongitude coordinates. We keep only those tracks whose\nextracted coordinates lie within the spatial boundaries of\nthe UK.\nThe ﬁnal dataset consists of a total of 10055 recordings.\nThe recordings span the years between 1904 and2002 with\nmedian year 1983 and standard deviation 12:3years. See\nFigure 1 for an overview of the geographical and temporal\ndistribution of the dataset. The origins of the recordings\nspan a range of maximum 1222 km. From the origins of all\n10055 recordings we compute the average latitude and av-\nerage longitude coordinates and estimate the distance be-\ntween each recording’s location and the average latitude,\nlongitude. This results in a mean distance of 167with stan-\ndard deviation of 85km. A similar estimate is computed\nfrom recordings in the training set and used as the random\nbaseline for our regression predictions (Section 5).\n3. AUDIO FEATURES\nWe aim to process music recordings to extract audio fea-\ntures that capture relevant music characteristics. We use\na speech/music segmentation algorithm as a preprocessing\nstep and extract features from the music segments using\navailable V AMP plugins2. We post-process the output of\nthe V AMP plugins to compute musical descriptors based\non state of the art MIR research. Additional dimensional-\nity reduction and scaling is considered as a ﬁnal step. The\nmethodology is summarised in Figure 2 and details are ex-\nplained below.\nSeveral recordings in our dataset consist of compila-\ntions of multiple songs or a mixture of speech and mu-\nsic segments. The ﬁrst step in our methodology is to use\na speech/music segmentation algorithm to extract relevant\nmusic segments from which the rest of the analysis is de-\nrived. We choose the best performing segmentation algo-\nrithm [9] based on the results of the Music/Speech Detec-\ntion task of the MIREX 2015 evaluation3. We apply the\nsegmentation algorithm to extract music segments from\n2http://www.vamp-plugins.org\n3http://www.music-ir.org/mirex/wiki/2015:\nMusic/Speech_Classification_and_DetectionProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 665British \nLibrary IOI Ratio Histogram \nMin, Max, Mean, Std \nMin, Max, Mean, Std Pitch Histogram \nContour Features Onset Times \nMel-Freq. Cepstral Coeff. Melody \nChromagram PCALat  Lon \nMusic \nSpeech Speech Figure 2 : Summary of the methodology: UK folk music recordings are processed with a speech/music segmentation\nalgorithm and V AMP plugins are applied to music segments. Audio features are derived from the output of the V AMP\nplugins, PCA is applied, and output is fed to a neural network that predicts the latitude and longitude of the recording.\neach recording in our dataset. We require a minimum of\n10seconds of music for each recording and discard any\nrecordings with total duration of music segments less than\nthis threshold.\nOur analysis aims to capture relevant musical charac-\nteristics which are informative for the spatial origins of the\nmusic. We focus on aspects of rhythm, melody, timbre,\nand harmony. We derive audio features from the following\nV AMP plugins: MELODIA - Melody Extraction4, Queen\nMary - Chromagram5, Queen Mary - Mel-Frequency Cep-\nstral Coefﬁcients6, and Queen Mary - Note Onset Detec-\ntor7. We apply these plugins for each recording in our\ndataset and omit frames that correspond to non-music seg-\nments as annotated by the previous step of speech/music\nsegmentation.\nThe raw output of the V AMP plugins cannot be directly\nincorporated in our regression model. We post-process the\noutput to low-dimensional and musically meaningful de-\nscriptors as explained below.\nRhythm. We post-process the output of the Queen\nMary - Note Onset Detector plugin to derive histograms of\ninter-onset interval (IOI) ratios [4]. Let O=fo1;:::;o ng\ndenote a sequence of nonset locations (in seconds) as\noutput by the V AMP plugin. The IOIs are deﬁned as\nIOI=foi+1\u0000oigfor indexi= 1;:::;n\u00001. The IOI ratios\nare deﬁned as IOIR =fIOI j+1\nIOI jgfor indexj= 1;:::;n\u00002.\nThe IOI ratios denote tempo-independent descriptors be-\ncause the tempo information carried with the magnitude\nof IOIs vanishes with the ratio estimation. We compute\na histogram for the IOIR values with 100bins uniformly\ndistributed between [0;10).\nTimbre. We extract summary statistics from the output\nof the Queen Mary - Mel-Frequency Cepstral Coefﬁcients\n(MFCC) plugin [8] with the default values of frame and\nhop size. In particular, we remove the ﬁrst coefﬁcient (DC\ncomponent) and extract the min, max, mean, and standard\ndeviation of the remaining 19MFCCs over time.\nMelody. The output of the MELODIA - Melody Ex-\ntraction plugin denotes the frequency estimates over time\n4http://mtg.upf.edu/technologies/melodia\n5http://vamp-plugins.org/plugin-doc/\nqm-vamp-plugins.html#qm-chromagram\n6http://vamp-plugins.org/plugin-doc/\nqm-vamp-plugins.html#qm-mfcc\n7http://vamp-plugins.org/plugin-doc/\nqm-vamp-plugins.html#qm-onsetdetectorof the lead melody. We extract a set of features captur-\ning characteristics of the pitch contour shape and melodic\nembellishments [16]. In particular, we extract statistics\nof the pitch range and duration, ﬁt a polynomial curve\nto model the overall shape and turning points of the con-\ntour, and estimate the vibrato range and extent of melodic\nembellishments. Each recording may consist of multiple\nshorter pitch contours. We keep the mean and standard\ndeviation of features across all pitch contours extracted\nfrom the audio recording. We also post-process the out-\nput from MELODIA to compute an octave-wrapped pitch\nhistogram [20] with 1200 -cent resolution.\nHarmony. The output of the Queen Mary - Chroma-\ngram plugin is an octave-wrapped chromagram with 100-\ncent resolution [5]. We use the default frame and hop\nsize and extract summary statistics denoting the min, max,\nmean, and standard deviation of chroma vectors over time.\nThe above process results in a total of 1484 features\nper recording. Before further processing, the features were\nstandardised with z-scores. Dimensionality reduction was\nalso applied with Principal Component Analysis (PCA) in-\ncluding whitening and keeping enough components to rep-\nresent 99% of the variance.\n4. REGRESSION MODEL\nThe prediction of spatial coordinates from music data has\nbeen treated as a regression problem in previous research\nusing K-nearest neighbours and Random Forest Regres-\nsion methods [23]. We explore the application of a neu-\nral network method. Neural networks have been shown to\noutperform existing methods in supervised tasks of music\nsimilarity [7, 11, 19]. We evaluate the performance of a\nneural network under different parameters for the regres-\nsion problem of predicting latitude and longitudes from\nmusic features.\nA neural network with two continuous value outputs,\nlatitude and longitude predictions, was built in Tensorﬂow.\nWe used the Adaptive Moment Estimation (Adam) algo-\nrithm for optimisation, Rectiﬁed Linear Unit (ReLU) as\nactivation function, and drop-out rate of 0:5for regularisa-\ntion. The evaluation of the model performance was based\non the mean distance error in km, calculated using the\nHaversine formula [6]. The Haversine distance dbetween\ntwo points in km is given by666 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Parameters Values\nTarget Scaling True or False\nNumber of hidden layers f3;4g\nCost function Haversine or MSE\nLearning Rate f0:005;0:01;0:05g\nL1 regularisation f0;0:05;0:5g\nL2 regularisation f0;0:05;0:5g\nTable 1 : The hyper-parameters and their range of values\nfor optimisation.\nd= 2rarcsin([sin2(\u001e2\u0000\u001e1\n2)+\ncos(\u001e1) cos(\u001e2) sin2(\u00152\u0000\u00151\n2)]1\n2)(1)\nwhere\u001erepresents the latitude, \u0015longitude, and rthe ra-\ndius of the sphere (with rﬁxed to 6367 km in this study).\nWe further explored the performance of the model under\narchitectures with different numbers of hidden layers, two\ndifferent cost functions, and a range of regularisation pa-\nrameters as explained below.\n4.1 Parameter Optimisation\nA grid-search of model hyper-parameters was performed\nto identify the combination that achieves best performance\nin cross-validation. The following hyper-parameters were\nconsidered for optimisation: whether or not to scale the\ntargets (i.e., z-score standardisation of the ground truth lat-\nitude/longitude coordinates of each recording), the num-\nber of hidden layers, two possible cost functions, namely,\nthe Haversine distance in km and the Mean Squared Error\n(MSE), and a range of values for learning rate, L1andL2\nregularisation parameters. The parameter optimisation is\nsummarised in Table 1. We tested in total 216combina-\ntions of hyper-parameters and selected the best performing\ncombination to tune parameters and retrain the model for\nthe ﬁnal results.\n4.2 Train-test splits\nThe training of the model was done in two phases. First\nthe model was trained using the full set of features (Sec-\ntion 3) and the different hyper-parameters as deﬁned in Ta-\nble 1. The hyper-parameters were tuned based on the opti-\nmal performance obtained through cross-validation. In the\nsecond phase, the hyper-parameters were ﬁxed to their op-\ntimal values and the model was retrained for different sets\nof features. Each new model’s performance was assessed\non a test set unique to that model.\nIn the ﬁrst training phase, we sampled at random 70%\nfrom the total number of 10055 recordings for training.\nThis resulted in a total of 7038 samples in the training set,\nof which 30% (2111 ) was set aside for validation. Follow-\ning PCA, the feature dimensionality of the dataset was 368.Target Hidden Cost Training Validation\nScaling Layers Function Error (km) Error (km)\nTrue 3 Haversine 72.68 119.36\nTrue 3 MSE 166:21 166 :27\nTrue 4 Haversine 98:03 128 :44\nTrue 4 MSE 166:19 166 :24\nFalse 3 Haversine 165:34 166 :79\nFalse 3 MSE 169:91 169 :30\nFalse 4 Haversine 170:91 171 :26\nFalse 4 MSE 181:44 180 :10\nTable 2 : Results for parameter optimisation. Learning\nrate,L1, andL2regularisation parameters are ﬁxed to\n0:005;0;0:5respectively. Best performance is obtained\nwhen target scaling is combined with 3hidden layers and\nHaversine distance as cost function.\nWe used cross-validation with K= 5 folds and tuned pa-\nrameters based on the mean of the distance error on the val-\nidation set (Equation 1). In the second phase we retrained\nthe model for different feature sets. For each feature set,\nthe dataset was split into training (random 70%) and test\n(remaining 30%) and the performance of the model was\nassessed on the test set.\n5. RESULTS\n5.1 Parameter Optimisation\nThe model that produced the lowest mean error on the val-\nidation set ( 119km) used the following hyper parameters:\ntarget scaling, 3hidden layers, Haversine distance as cost\nfunction, learning rate of 0:005, andL1,L2regularisation\nparameters of 0and0:5, respectively. The main hyper-\nparameters that determined the accuracy of the model were\nthe use of Haversine distance as the cost function, and\nthe application of target scaling. The performance of the\nmodel for different parameter values is shown in Table 2.\n5.2 Results for different feature sets\nThe second set of experiments explored the performance of\nthe model when trained for different sets of features. We\nestimated the random baseline from the origins of record-\nings in the training set. In particular, we computed the av-\nerage latitude and average longitude coordinates of record-\nings and estimated the distance between each recording’s\nlocation and the average latitude, longitude. Based on this\nestimate the mean distance error of the baseline approach\nwas167:4km. Each model was compared to the baseline\napproach (i.e., the mean distance error of its test targets)\nwith a Wilcoxon signed-rank test. The performances of\nthe models trained on different sets of features and evalu-\nated on separate test sets were compared with a pairwise\nWilcoxon rank sum test (also known as Mann-Whitney)\nwith Bonferroni correction for multiple comparisons. We\nconsider a signiﬁcance level of \u000b= 0:05and denote the\nBonferroni corrected level by ^\u000b.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 667Model Feature Set Error\nNo. Name (km)\n1 All features 149:8\n2 Rhythm: IOIR histogram 160:0\n3 Harmony: Chromagram statistics 152:5\n4 Timbre: MFCC statistics 129:0\n5 Pitch histogram 160:1\n6 Contour features mean 159:8\n7 Contour features standard deviation 162:3\n8 Melody: Pitch hist., contour features 152:6\n9 Rhythm and Harmony 149:1\n10 Rhythm and Timbre 120:1\n11 Rhythm and Melody 150:5\n12 Melody and Harmony 139:4\n13 Melody and Timbre 117:1\n14 Timbre and Harmony 114.0\n15 Rhythm, Harmony, and Timbre 118:3\n16 Rhythm, Harmony, and Melody 142:8\n17 Rhythm, Timbre, and Melody 119:8\n18 Harmony, Timbre, and Melody 140:3\n– Baseline 167:4\nTable 3 : The mean distance error (in km) of the test set for\n18models trained on different sets of features.\n12345678910121416180 100 200 300 400\nf\nh\na\nbc\nh\nh\nh\naf\nf\ng\nf\nce\ndg\nd\ndg\nce\nbg\ne\nFeature setsDistance error (km)\nFigure 3 : Distance error of predictions for different sets of\nfeatures (see Table 3 for the feature set used to train each\nmodel). Labels a\u0000lin indicate features sets that have non-\nsigniﬁcantly different results ( p>^\u000b) where they share the\nsame letter. For example, feature set 3shares the label a\nwith feature set 8but shares no label with any other feature\nset, indicating that results from model 3are signiﬁcantly\ndifferent from all other models except for model 8.\n01002003004005006007008009001000(a) Ground truth\n01002003004005006007008009001000 (b) Predictions\nFigure 4 : (a) Ground truth and (b) predicted music record-\ning origins, coloured by the distance error (in km) for the\nbest performing model (no. 14).\nAll models achieved results signiﬁcantly different from\nthe baseline approach ( p < : 0001 ). The best performance\n(lowest error of 114:0km) was achieved when combin-\ning the timbral and harmonic descriptors (model 14). This\ncombines the summary statistics of the chromagram and\nthe summary statistics of the MFCCs. The performance\nof this model was signiﬁcantly different ( p < ^\u000b) from all\nother models except models 13and15trained on melodic\nand timbral, and rhythmic, harmonic and timbral descrip-\ntors, respectively. The model achieved a mean error of\n149:8km on the test set when all features (Section 3) were\nused. The results from model 3trained on harmonic de-\nscriptors were signiﬁcantly different from all other models\nexcept model 8trained on melodic features. The model\ntrained on rhythmic descriptors (model 2) is amongst the\nweakest predictors. However, adding rhythmic features to\nany of melodic, harmonic, or timbral features, for example\nmodels 9;10;11, signiﬁcantly improves the performance\nof the model ( p < ^\u000bfor pairwise comparisons between\nmodels 3and9,4and10,8and11). Models 5;6;7trained\non pitch histograms, contour features mean, and contour\nfeatures standard deviation, respectively, are also amongst\nthe weakest predictors but when all these features are com-\nbined together as in model 8, the performance is improved.\nSee Table 3 for an overview of the prediction accuracy of\nmodels trained on different feature sets. Figure 3 provides\na box-plot visualisation of the results from different feature\nsets and marks statistical signiﬁcance between results.\n5.3 Results for different regions\nThe last analyses aim to study the prediction accuracy with\nrespect to the geographical origins of recordings. Figure 4\nshows the ground truth and predicted coordinates for the\nbest performing model (model no. 14as denoted in Table 3)\ncoloured by the distance error in km. We observe that data\npoints with the lowest predictive accuracy originate from\nthe north-eastern and the south-western areas of the UK\n(Figure 4a). Predictions are mostly concentrated in the\nsouthern part of the UK. Data points predicted towards the668 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 201701002003004005006007008009001000(a) Rhythm\n01002003004005006007008009001000 (b) Harmony\n01002003004005006007008009001000 (c) Timbre\n01002003004005006007008009001000 (d) Melody\nFigure 5 : Music recording origins coloured by the distance error (in km) for models trained on (a) rhythmic, (b) harmonic,\n(c) timbral, and (d) melodic features (models no. 2;3;4;8respectively as deﬁned in Table 3).\neastern areas indicate a larger distance error (Figure 4b).\nIn Figure 5 we visualise the prediction accuracy of mod-\nels trained on different feature sets with respect to geogra-\nphy. We observe that for all models the northern areas of\nthe UK (i.e., in the region of Scotland) are predicted with\na relatively large distance error (lowest accuracy). For the\nmodel trained on timbral features (Figure 5c) we also ob-\nserve the south west of England predicted with lower ac-\ncuracy than the models trained on harmonic and melodic\nfeatures (Figures 5b and 5d).\n6. DISCUSSION\nOur results provide insights on the contribution of different\nfeature sets and suggest patterns of music similarity across\ngeographical regions. The methodology can be improved\nin various ways.\nThe initial corpus of folk and traditional music from the\nUK consisted of a total of 17000 of which only 10055\nwere processed in this study. The ﬁnal dataset had a\nskewed geographical distribution with over-representation\nof the south-eastern and south-western UK regions, e.g.,\nDevon and Suffolk, and under-representation of the North-\nEastern, North-Western areas, e.g., Scotland and Northern\nIreland. Effects from the skewness of the dataset could be\nobserved in the distribution of predicted latitude and longi-\ntude coordinates (Figure 4b). A larger and more represen-\ntative corpus can be used in future work.\nWe used features derived from the output of V AMP plu-\ngins to describe musical content of audio recordings. Some\nof these plugins were designed for different music styles\nand their application to folk music might not give robust\nresults. A thorough evaluation of the suitability of the\nfeatures can give valuable insights for improving their ro-\nbustness to different corpora such as the one used in this\nstudy. We used feature representations averaged over time\nbut in future work preserving temporal information in the\nfeatures could provide better music content description.\nWe observed that results from models trained on indi-\nvidual features showed on average larger distance errors.\nWhen however combinations of features were considered,\nthe model achieved on average higher accuracies. An ex-\nception is the case when all features were considered but\nthe performance of the model had a relatively large dis-tance error. This could be due to limitations of the model\nespecially with regards to over-ﬁtting or the lack of ade-\nquate music information captured by the features. Inte-\ngrating additional audio features could help capture more\nof the variance of the data and improve the model.\nThe model was validated for a range of parameters and\nseveral approaches were considered to avoid over-ﬁtting.\nHowever, evidence of over-ﬁtting could still be observed\nin the ﬁnal results. Training with more data could help\nmake the model more generalisable in future work. What is\nmore, oversampling techniques could be explored to over-\ncome the problem of under-represented geographical re-\ngions in our dataset.\nNeural networks in combination with audio features as\nproposed in this study, can provide good predictions of the\norigins of the music. This can aid musicological research\nas well as improve spatial metadata associated with large\nmusic collections.\n7. CONCLUSION\nWe studied a collection of ﬁeld recordings from the\nUK and investigated whether the geographical origins of\nrecordings can be predicted from the music attributes of\nthe audio signal. We treated this as a regression prob-\nlem and trained a neural network to take as input audio\nfeatures and predict the latitude and longitude of the mu-\nsic’s origin. We trained the model under different hyper-\nparameters and tested its performance for different feature\nsets. Highest accuracy was achieved for the model trained\non timbral and harmonic features but no signiﬁcant differ-\nences were found to the same model with rhythm features\nadded or with melody replacing harmony. The southern\nregions of the UK were predicted with a relatively high ac-\ncuracy whereas northern regions were predicted with low\naccuracy. Effects of the skewness of the dataset and the re-\nliability of audio features were discussed. The corpus and\nmethodology can be improved in future work and the ap-\nplicability of the model could be extended to music from\naround the world.\n8. ACKNOWLEDGEMENTS\nMP is supported by a Queen Mary research studentship.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 6699. REFERENCES\n[1] S. Abdallah, E. Benetos, N. Gold, S. Hargreaves,\nT. Weyde, and D. Wolff. The Digital Music Lab: A\nBig Data Infrastructure for Digital Musicology. ACM\nJournal on Computing and Cultural Heritage , 10(1),\n2017.\n[2] J. Eisenstein, B. O’Connor, N.A Smith, and E.P. Xing.\nA Latent Variable Model for Geographic Lexical Varia-\ntion. In Proceedings of the 2010 Conference on Empir-\nical Methods in Natural Language Processing , pages\n1277–1287, 2010.\n[3] E. G ´omez, M. Haro, and P. Herrera. Music and geog-\nraphy: Content description of musical audio from dif-\nferent parts of the world. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 753–758, 2009.\n[4] F. Gouyon, S. Dixon, E. Pampalk, and G. Widmer.\nEvaluating rhythmic descriptors for musical genre clas-\nsiﬁcation. In Proceedings of the AES 25th International\nConference , pages 196–204, 2004.\n[5] C. Harte and M. Sandler. Automatic chord identifca-\ntion using a quantised chromagram. In 118th Audio En-\ngineering Society Convention , 2005.\n[6] J. Inman. Navigation and Nautical Astronomy: For the\nUse of British Seamen . F. & J. Rivington, 1849.\n[7] I. Karydis, K. Kermanidis, S. Sioutas, and L. Iliadis.\nComparing content and context based similarity for\nmusical data. Neurocomputing , 107:69–76, 2013.\n[8] B. Logan. Mel-Frequency Cepstral Coefﬁcients for\nMusic Modeling. In Proceedings of the International\nSymposium on Music Information Retrieval , 2000.\n[9] M. Marolt. Music/speech classiﬁcation and detection\nsubmission for MIREX 2015. In MIREX , 2015.\n[10] M. Mauch, R. M. MacCallum, M. Levy, and A. M.\nLeroi. The evolution of popular music: USA 1960-\n2010. Royal Society Open Science , 2(5):150081, 2015.\n[11] C. McKay and I. Fujinaga. Automatic genre classiﬁ-\ncation using large high-level musical feature sets. In\nProceedings of the International Society for Music In-\nformation Retrieval Conference , pages 525–530, 2004.\n[12] D. Moelants, O. Cornelis, and M. Leman. Exploring\nAfrican Tone Scales. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 489–494, 2009.\n[13] B. Nettl. The Study of Ethnomusicology: Thirty-one Is-\nsues and Concepts . University of Illinois Press, Urbana\nand Chicago, 2nd edition, 2005.\n[14] B. Nettl, R. M. Stone, J. Porter, and T. Rice, editors.\nThe Garland Encyclopedia of World Music. Garland\nPub, New York, 1998-2002 edition, 1998.[15] J. Novembre, K. Bryc, S. Bergmann, A.R. Boyko,\nC.D. Bustamante, A. Auton, M. Stephens, Z. Kuta-\nlik, A. Indap, T. Johnson, M.R. Nelson, and K.S.\nKing. Genes mirror geography within Europe. Nature ,\n456(7218):98–101, 2008.\n[16] M. Panteli, R. Bittner, J. P. Bello, and S. Dixon. To-\nwards the characterization of singing styles in world\nmusic. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing , pages 636–640, 2017.\n[17] P. E. Savage and S. Brown. Mapping Music: Clus-\nter Analysis Of Song-Type Frequencies Within and\nBetween Cultures. Ethnomusicology , 58(1):133–155,\n2014.\n[18] P. E. Savage, S. Brown, E. Sakai, and T. E. Currie.\nStatistical universals reveal the structures and func-\ntions of human music. Proceedings of the National\nAcademy of Sciences of the United States of America ,\n112(29):8987–8992, 2015.\n[19] D. Turnbull and C. Elkan. Fast recognition of musi-\ncal genres using RBF networks. IEEE Transactions\non Knowledge and Data Engineering , 17(4):580–584,\n2005.\n[20] G. Tzanetakis, A. Ermolinskyi, and P. Cook. Pitch his-\ntograms in audio and symbolic music information re-\ntrieval. Journal of New Music Research , 32(2):143–\n152, 2003.\n[21] W. Yang, J. Novembre, E. Eskin, and E. Halperin. A\nmodel-based approach for analysis of spatial structure\nin genetic data. Nature Genetics , 44(6):725–731, 2012.\n[22] J. M. Young, L. S. Weyrich, J. Breen, L. M. Mac-\ndonald, and A. Cooper. Predicting the origin of soil\nevidence: High throughput eukaryote sequencing and\nMIR spectroscopy applied to a crime scene scenario.\nForensic Science International , 251:22–31, 2015.\n[23] F. Zhou, Q. Claire, and R. D. King. Predicting the Geo-\ngraphical Origin of Music. In IEEE International Con-\nference on Data Mining , pages 1115–1120, 2014.670 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Examining Musical Meaning in Similarity Thresholds.",
        "author": [
            "Katherine M. Kinnaird"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417721",
        "url": "https://doi.org/10.5281/zenodo.1417721",
        "ee": "https://zenodo.org/records/1417721/files/Kinnaird17.pdf",
        "abstract": "Many approaches to Music Information Retrieval tasks rely on correctly determining if two segments of a given musical recording are repeats of each other. Repetitions in recordings are rarely exact, and identifying the appropriate threshold for these pairwise decisions is crucial for tuning MIR algorithms. However, current approaches for deter- mining and reporting this threshold parameter are devoid of contextual meaning and interpretations, which makes comparing previous results difficult and which requires ac- cess to specific datasets. This paper highlights weaknesses in current approaches to choosing similarity thresholds, provides a framework using the proportion of orthogonal musical change to tie thresholds back to feature spaces with the cosine dissimilarity measure, and introduces new research possibilities given a music-centered approach for selecting similarity thresholds.",
        "zenodo_id": 1417721,
        "dblp_key": "conf/ismir/Kinnaird17",
        "keywords": [
            "Music Information Retrieval",
            "repetitions",
            "threshold determination",
            "pairwise decisions",
            "cosine dissimilarity measure",
            "orthogonal musical change",
            "feature spaces",
            "research possibilities",
            "music-centered approach",
            "comparison difficulty"
        ],
        "content": "EXAMINING MUSICAL MEANING IN SIMILARITY THRESHOLDS\nKatherine M. Kinnaird\nBrown University\nkatherine kinnaird@brown.edu\nABSTRACT\nMany approaches to Music Information Retrieval tasks\nrely on correctly determining if two segments of a given\nmusical recording are repeats of each other. Repetitions in\nrecordings are rarely exact, and identifying the appropriate\nthreshold for these pairwise decisions is crucial for tuning\nMIR algorithms. However, current approaches for deter-\nmining and reporting this threshold parameter are devoid\nof contextual meaning and interpretations, which makes\ncomparing previous results difﬁcult and which requires ac-\ncess to speciﬁc datasets. This paper highlights weaknesses\nin current approaches to choosing similarity thresholds,\nprovides a framework using the proportion of orthogonal\nmusical change to tie thresholds back to feature spaces\nwith the cosine dissimilarity measure, and introduces new\nresearch possibilities given a music-centered approach for\nselecting similarity thresholds.\n1. INTRODUCTION\nSince Foote introduced the self-similarity matrix as a tech-\nnique for visualizing and representing audio data [7], ma-\ntrix representations have been widely used to represent\nmusic-based data, such as songs or musical scores, when\naddressing different kinds of tasks in Music Information\nRetrieval [6,11,13,17]. Recordings of music often contain\nslight variations between repeated sections either due to\nartistic interpretations or noise introduced by the record-\ning environment. Addressing these MIR tasks often re-\nquires grouping time steps together using a threshold on\nthe self-(dis)similarity matrix representation to determine\nwhich pairs of time steps are similar enough to be classi-\nﬁed as repetitions of each other. There are two issues at\nplay when choosing this similarity threshold: 1) selecting\nthe best value given the task and data, and 2) using the\nvalue with the best musical interpretation.\nSimilarity thresholds are currently determined in ways\nthat prioritize computational successes and ignore tangi-\nble musical interpretations. These thresholds are usually\ndependent on the data at hand and reported as a selection\nmethod (say a ﬁxed percentage) instead of as a particu-\nlar threshold value. These data-dependent thresholds, re-\nc\rKatherine M. Kinnaird. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nKatherine M. Kinnaird. “Examining Musical Meaning in Similarity\nThresholds”, 18th International Society for Music Information Retrieval\nConference, Suzhou, China, 2017.ported as methods, require access to common datasets in\norder to compare previous and current research. Further-\nmore, many of the processes for determining this crucial\nthreshold do not have a mechanism for connecting that\nthreshold back to the original feature space. For example,\ncurrent methods give little understanding to what a “small-\nvalue” cosine dissimilarity measurement corresponds to in\nterms of musical sounds such as notes and chords.\nInstead of only justifying similarity thresholds based on\nstatistical theory or computational success, we argue mu-\nsical meaning should be included in the selection and dis-\ncussion of a similarity threshold. In Section 2, examples\nbased on a jazz lead sheet offer motivation for similarity\nthresholds with musical context. In Section 3, we model\na framework for tying a chosen threshold to a particular\nfeature space via the concept of the maximum proportion\nof orthogonal musical change. In Section 4, we introduce\nhow music-centered thresholds can enhance MIR research.\n2. MOTIV ATION WITH EXAMPLES\nIn MIR literature, there are a variety of methods for set-\nting the similarity threshold used to decide when sections\nof a song are similar. Current methods have been based\non statistical ideas combined with concise algorithmic ex-\nplanations. In [1, 11, 15], for a given recording of a per-\nformance of a piece of music, the threshold was speciﬁed\nso that a ﬁxed percentage of a matrix representation (either\nself-similarity matrix or self-dissimilarity matrix - SDM)\nwould be selected. The method in [18–20] sets the mean-\ning of “similar” for each time step by ﬁrst looking for the\n\u0014nearest neighbors of a given time step and then by en-\nforcing a mutual condition; that is that time steps iand\njare determined to be similar if both time step iis a\u0014\nnearest neighbor of time step jand vice versa. In [3], the\nthreshold was set using statistical techniques on a set of\nsample data. In [8–10], Goto determined a threshold us-\ning the automatic threshold selection method developed by\nOtsu [16] which selects a threshold using statistics of the\ngrey-level histogram of a particular image. In the case of\nGoto’s work [8–10], the image is a matrix representation\nfor a song.\nWhile the above methods are efﬁcient and have satisfy-\ning connections to our intuition about similarity, a crucial\nweakness of these methods is a lack of a musical connec-\ntion for the similarity threshold. For example, the ﬁxed\npercentage thresholds in [1, 11, 15] are easy to set and of-\nfer clear methods for reproducing those workﬂows, but no\nmusical intuition is offered for these methods. Underlying635ﬁxed percentage thresholds is the assumption that all mu-\nsic has the same proportion of similarity, which certainly\nwould not be the case in a collection with both classical and\njazz recordings. The method in [18–20] to some degree ad-\ndresses the ﬂaws in this assumption, but this method leaves\nunanswered what it means musically to be mutual \u0014near-\nest neighbors.\nThe following four examples based off a jazz lead sheet\nuse the bottom-10% paradigm for similarity threshold se-\nlection and highlight some of the issues with this method.\nThe ﬁrst example is just the chords by beat as described in\nthe lead sheet. The second example adds absolute Gaus-\nsian noise, while the third example adds notes in a re-\nstricted manner, seeking to mimic the spontaneous com-\nposition of jazz music. The ﬁnal example adds both abso-\nlute Gaussian noise and restricted “note” noise. These ex-\namples are constructed from a human coded .jazz ﬁle1of\nAisha by McCoy Tyner from 1961 found in the iRb Corpus\nin **jazz format dataset [2]. Beat tracking was not used\nsince these examples are based on a version of the piece’s\nlead sheet. Each time step represents 8 continuous beats\nby concatenating adjacent 8 feature vectors (one per beat).\nFor each example, the distribution of dissimilarity val-\nues and the thresholded SDM are shown. All results are\nfrom single runs of the associated random processes, but\nsimilar results occur with repeated trials. For the thresh-\nolded SDM, the original SDM values are retained to fur-\nther highlight contrast between examples.\nExample 1 - Jazz Lead Sheet\nThis example is the ground truth for the true repeated\nstructure of the lead sheet. We assume that there is neither\nnoise nor spontaneous composition on the track.\nFigure 1 . Complete SDM for Aisha lead sheet. Values\nnear 0 are dark.\nUsing the bottom-10% paradigm, the threshold Tis\n0:375, meaning that two feature vectors with the angle be-\ntween them no greater than 51.318 degrees will be deemed\nsimilar enough to be repeats of each other. This is quite\na generous threshold; for example, a feature vector repre-\nsenting a C chord (held for 8 beats) and a feature vector\nrepresenting C-minor chord (also held for 8 beats) would\nbe deemed as repeats of each other.\n1The .jazz ﬁle was converted to a .txt ﬁle using code by Yuri Broze [2].\nChromagrams were then extracted using a new converter ﬁle, available at\nhttps://github.com/kmkinnaird/MusicalThresh\n(a)Thresholded SDM under\nbottom-10% paradigm\n(b)Histogram of all dissimilar-\nity values\nFigure 2 .Aisha lead sheet without additions\nExample 2 - Jazz Lead Sheet with Gaussian Noise\nIn this example, we add proxy for general noise (such as\nfeedback in the recording environment) to the lead sheet.\nTo each note-beat entry of the chroma matrix for the lead\nsheet, we add the absolute value of a random sample from\nthe Gaussian centered at 0 with standard deviation 0.5.\n(a)Thresholded SDM under\nbottom-10% paradigm\n(b)Histogram of all dissimilar-\nity values\nFigure 3 .Aisha lead sheet with added track noise\nIn addition to most of the similarity from Example 1,\nadditional segments were classiﬁed as repeats using the\nbottom-10% paradigm, meaning that “similarity” is being\ncreated under this threshold selection method. However,\nthis example’s threshold value is lower, so two audio shin-\ngles must be more similar to be considered repeats than\nin Example 1. The threshold Tis approximately 0:232,\nmeaning that two feature vectors with the angle between\nthem no greater than 39.818 degrees will be deemed sim-\nilar enough to be repeats of each other. This shifted (and\npossibly contradictory) deﬁnition of similarity may be ap-\npropriate given the data but there is no musical interpre-\ntation of the threshold to support this choice. The lower\nthreshold does reﬂect the compression of the distribution\nof dissimilarity values, shown in Figure 3(b).\nExample 3 - Jazz Lead Sheet with “Note” Noise\nIn this third example, we add a proxy for spontaneous\ncomposition. This added “note” noise is restricted to the\nnotes within the chord speciﬁed on the lead sheet and has\nits note weight randomly selected from the distribution of\nnote values, shown in Figure 4.\nThe threshold Tfor this example is approximately\n0:417, meaning that two feature vectors with the angle be-\ntween them no greater than 54.357 degrees will be deemed636 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 4 . Histogram of note values for “note” noise\nsimilar enough to be repeats of each other. As expected\ngiven this example’s construction, this threshold is similar\nto the one in Example 1. However, while much of the sim-\nilarity from Example 1 was found using the bottom-10%\nparadigm, it is clear that not all of it was. As with Exam-\nple 2, this threshold may be appropriate, but there is no\nmusical interpretation to support this choice.\n(a) Thresholded SDM under\nbottom-10% paradigm\n(b)Histogram of all dissimilar-\nity values\nFigure 5 .Aisha lead sheet with added “note” noise\nExample 4 - Jazz Lead Sheet with “Note” Noise and with\nGaussian Noise\nIn this example, we add proxies for both track noise (as\nin Example 2) and “note” noise (as in Example 3). Since\nwe are assuming that there is both spontaneous composi-\ntion and additional noise on the track, it is tempting to sim-\nply add the thresholds from Examples 2 and 3. However,\nwe cannot, given the construction of the proxies and that\ncosine dissimilarity measure does not observe the triangle\ninequality.\n(a)Thresholded SDM under\nbottom-10% paradigm\n(b)Histogram of all dissimilar-\nity values\nFigure 6 . Track and “note” noise added to Aisha lead sheetSimilar to Example 2, we have a possibly contradictory\ndeﬁnition of similarity. In this example, the bottom-10%\nparadigm captures most of the similarity from Example 1\nbut also incorrectly matches additional repeated “similar-\nity.” However, the value of this threshold Tis lower, at ap-\nproximately 0:293, which translates to an angle no greater\nthan45:026degrees between two feature vectors deemed\nsimilar enough to be repeats.\nComparing Four Examples\nThese four examples highlight some of the weaknesses\nin the commonly used ﬁxed percentage threshold selec-\ntion paradigm. First, the generous thresholds in Exam-\nples 1 and 3 allow for major and minor chords (such as C-\nmajor and C-minor) to be deemed as repeats of each other.\nHowever, the histogram from Example 3 is quite similar\nto Example 1, which signals that an appropriate choice of\nthreshold for a lead sheet would also be appropriate to ap-\nply to a lead sheet with spontaneous composition.\nSecond, when a proxy for random track noise is intro-\nduced, as in Examples 2 and 4, major and minor chords\nwould no longer be matched. However, a passing glance on\nthe resulting thresholded SDMs in Examples 2 and 4 show\nsections of the lead sheet designated as repeats when they\nperhaps should not be. Additionally, the histograms for\nExamples 2 and 4 are much more compressed than those\nin Examples 1 and 3, which further signals a need in in-\ncorporate musical context into the selection of similarity\nthresholds.\nEven though these four examples are based on a lead\nsheet, of which three employ random processes as prox-\nies for track noise and spontaneous compositions, these\ncontrolled and constructed examples demonstrate the need\nfor careful examination of the meaning and limitations of\nthresholds used in MIR tasks and approaches.\n3. RELATING TTO MAXIMUM PROPORTION\nOF ORTHOGONAL MUSICAL CHANGE\nIn this section, we establish a framework for linking a simi-\nlarity threshold Tto the space of audio shingles composed\nof chroma feature vectors under the cosine dissimilarity\nmeasure. We deﬁne the proportion of orthogonal musical\nchange (or POMC) for this feature space and prove a rela-\ntionship between a given threshold Tto POMC. Although\nwe ground our discussion in one particular feature space, a\nsimilar procedure can be used to tie similarity thresholds to\nany feature space using the cosine dissimilarity measure.\n3.1 Preliminary Deﬁnitions and Notation\nWe create overlapping audio shingles fromkconcatenated\nfeature vectors, where kis a ﬁxed integer [3–5]. For a time-\nstepi, the chroma feature vector \u001fiis the column vector of\n12 non-negative entries, where each entry corresponds to\none of the Western pitch classes fC;C#;:::;Bgencoding\nthe amount of that pitch class in the ithobservation [14].Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 637For time-step i, the audio shingle of length k, incorporating\nlocal information, is the column vector \u000bi:\n\u000bi=h\n\u001ft\ni;\u001ft\n(i+1);\u001ft\n(i+2);:::;\u001ft\n(i+k\u00001)it\n(1)\nEach audio shingle is an element of R(k\u000212)\n\u00150 , the non-\nnegative closed orthant of R(k\u000212)and can be regarded\nas vectors that start at the origin. Let \u0012\u000bi;\u000bjbe the an-\ngle between \u000biand\u000bj. Since\u000bi;\u000bj2R(k\u000212)\n\u00150 , then\n\u0012\u000bi;\u000bj2[0;\u0019\n2]. The pairwise cosine dissimilarity between\ntwo audio shingles \u000biand\u000bjis deﬁned as:\nDi;j= 1\u0000cos\u0012\u000bi;\u000bj (2)\nIt is natural to ask: Given the valueDi;j, what are the\nmusical differences between those two time steps? We\nintroduce the notion of proportion of orthogonal musical\nchange (POMC), or rather the amount an audio shingle \u000bi\nmust change orthogonally (before scaling) in order to be-\ncome\u000bj. POMC encodes of how much one audio shingle\ncan be comprised of elements perpendicular to another au-\ndio shingle before we say these two audio shingles are no\nlonger considered to be “similar” of one another.\nConsider Figure 7; the vector ~ \ris orthogonal to \u000biand\nwhen added to \u000biwill meet\u000bj. We can scale the vector\n(\u000bi+~ \r)to match\u000bj. Similarly ~\u001eis orthogonal to \u000bj\nand when added to \u000bjmeets\u000bi. We can scale (\u000bj+~\u001e)\nto match\u000bi. The length of ~ \risjj\u000bijj\u0001tan\u0012\u000bi;\u000bj, and the\nlength of~\u001eisjj\u000bjjj\u0001tan\u0012\u000bi;\u000bj. Sotan\u0012\u000bi;\u000bjis the amount\nof orthogonal change for \u000bito become a scalar multiple of\n\u000bjand vice versa.\ny\nx\u0012\u000bi;\u000bj\u000bj\u000bi~ \r~\u001e\nFigure 7 . Visualization of orthogonal musical change of\n\u000bionto\u000bjand of\u000bjonto\u000bi, represented respectively by\nthe vectors~ \rand~\u001e.\nDeﬁnition 3.1. For a pair of audio shingles \u000biand\u000bj, the\nproportion of orthogonal musical change (POMC) is given\nbytan\u0012ai;aj.\n3.2 Maximum POMC Given T\nSuppose that we have one audio shingle, denoted ~\u0018ter-\nminating at point \u0018, and that we want to classify all audio\nshingles that are repetitions of ~\u0018. LetTbe the threshold de-\ntermining whether pairs of audio shingles are close enough\nto be repeats. We deﬁne \u0012Tascos\u00001(1\u0000T).Let\u0004be the set of audio shingles that are less than T\ncosine dissimilar from ~\u0018. So~ v2\u0004, iff1\u0000cos\u0012~ v;~\u0018\u0014T,\nfor\u0012~ v;~\u0018. Additionally, for each vector ~ v2\u0004, we have:\ncos\u0012~ v;~\u0018\u00151\u0000T= cos\u0012T (3)\nDeﬁnition 3.2. GivenT, the maximum POMC , denoted\u001a,\nistan(\u0012T), where\u0012T= cos\u00001(1\u0000T).\nWe begin establishing the comparison between the au-\ndio shingles in \u0004and~\u0018using just POMC. We ﬁrst note\nthat the set of audio shingles orthogonal to ~\u0018is comprised\nof the audio shingles representing silence (i.e. those with-\nout any notes) and the audio shingles that do not have notes\nin common time with ~\u0018. For example, if ~\u0018represented a C\nchord followed by a F chord, then an audio shingle that is\northogonal to it could be one representing a C# chord fol-\nlowed by an E chord. Given the importance of note and\nchord order in music generally, the audio shingle repre-\nsenting an F note followed by a C chord is orthogonal to a\nsecond representing a C chord followed by an F note. Nei-\nther of the above pairs would be mistaken as similar, and\nso we restrict \u0012T2[0;\u0019\n2), since including \u0012T=\u0019\n2would\nimply that orthogonal pairs of audio shingles are similar.\ny\nx~\u0018\n\u0004\n\u0012T\nFigure 8 . Visualization of \u0004, the set of audio shingles that\nare less than Tcosine dissimilar from ~\u0018. The gray area\nﬂanked in dotted arrows is the set \u0004. The dashed line con-\ntinuing from ~\u0018is the subspace deﬁned by ~\u0018.\nMore often, we want to compare pairs like a C chord\nfollowed by a second C chord with a C chord followed\nby a C7 chord, and determine if these two audio shingles\nare close enough to be deemed similar. These two audio\nshingles are the same save for the B [note in the second\nchord. Clearly a lone B [note is orthogonal to a C chord\nbut is not orthogonal to the C7 chord. However, the C7\nchord can be decomposed into the sum of a C chord and a\nB[note. In other words, the C7 chord is the C chord plus\na vector orthogonal to it. Such a decomposition is at the\nheart of the concept of POMC.\nReturning to our general case with audio shingle ~\u0018,\nwe make the following deﬁnitions generalizing the above\ncomparison of the C and C7 chords:\nDeﬁnition 3.3. Let\u0018?denote the hyperplane that is or-\nthogonal to the vector ~\u0018with the point \u00182\u0018?.\nWe note that \u0018?does not require that vectors in \u0018?to\nbe within \u0004. The following deﬁnition adds this restriction:638 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Deﬁnition 3.4. LetV+be the set of vectors originating at\nthe point\u0018and terminating at a point in \u0018?such that for\n~ v+2V+, we have that the cosine of the angle between\n(~\u0018+~ v+)and~\u0018is greater than or equal to cos\u0012T.\nFor any vector ~ v+2V+, we have that the angle be-\ntween~\u0018and~ v+is the right angle in a right triangle with one\nleg along~\u0018with lengthjj~\u0018jj2and with another leg along\n~ v+with lengthjj~ v+jj2. The tangent of the angle between\n(~\u0018+~ v+)and~\u0018is equal tojj~ v+jj2\njj~\u0018jj2, which must be less than\nor equal to tan\u0012T. Sojj~ v+jj2\u0014jj~\u0018jj2\u0001tan(\u0012T).\ny\nx~\u0018\u0004\n\u0012T~ v+\n~\u0018+~ v+V+\n\u0018?\nFigure 9 . Visualization of the right triangle formed by ~\u0018\nand~ v+2V+inR2. The solid line perpendicular to ~\u0018\nrepresentsV+and is 2jj~\u0018jj2\u0001tan(\u0012T)long. The dashed\nline continuing from V+combined with V+represents\u0018?.\nThe setV+represents the set of audio shingles that are\ncreated by adding to ~\u0018an orthogonal vector of length no\nlonger thanjj~\u0018jj2\u0001tan(\u0012T). For example, if T= 0:1and\nif~\u0018represented one C chord, then the audio shingle repre-\nsenting a C7-chord would terminate in the hyperplane \u0018?,\nbut would not be included in \u0004; however, if T= 0:14, then\nit would be included in \u0004.\n3.3 Decomposition of Elements in \u0004\nThus far we have established the relationship between our\naudio shingle ~\u0018and the audio shingles ending in \u0018?\\\u0004.\nWe are interested in understanding the relationship of T\nwith all of \u0004. We offer the following decomposition for\nthe elements of \u0004, which connects the deﬁnitions of the\nprevious section to Deﬁnition 3.1 shown in Figure 7.\nProposition 3.1. The set \u0004is equal to the setS, given by\nS=n\nw(~\u0018+~ v+)j~ v+2V+;w2R\u00150o\n:\nProof : First, we show that S\u0012 \u0004. We begin by letting ~ v\nbe an element ofS. So~ v=w(~\u0018+~ v+)for some~ v+2V+\nand somew2R\u00150. Let\fdenote the angle between ~ vand\n~\u0018, which is also the angle between (~\u0018+~ v+)and~\u0018since\n~ vand(~\u0018+~ v+)are scalar multiples of each other. By the\ndeﬁnition of V+, we have that cos\f\u0015cos\u0012T. So~ v2\u0004\nand thusS\u0012\u0004.\nNext, we show that \u0004\u0012S. Let~ v2\u0004and let\u0012~ v;~\u0018be\nthe angle between ~ vand~\u0018. Then:\n1\u0000cos\u0010\n\u0012~ v;~\u0018\u0011\n\u0014T (4)Letv+be the point where ~ vintersects\u0018?(noting that\nit may be necessary to continue in the direction of ~ vto\nintersect with \u0018?). We have a right triangle with one leg in\nthe direction of ~\u0018with lengthjj~\u0018jj2and another leg from ~\u0018\ntov+that is lengthjjv+\u0000\u0018jj2. Let~ v+be the vector from ~\u0018\ntov+. By deﬁnition of v+, thenjj~ v+jj2=jjv+\u0000\u0018jj2and\nthus:\ntan\u0010\n\u0012~ v;~\u0018\u0011\n=jjv+\u0000\u0018jj2\njj~\u0018jj2: (5)\nLeveraging Eqn (5), we have that\njj~ v+jj2=jjv+\u0000\u0018jj2=jjv+\u0000\u0018jj2\u0001jj~\u0018jj2\njj~\u0018jj2\n=jj~\u0018jj2\u0001tan\u0010\n\u0012v;~\u0018\u0011\n\u0014jj~\u0018jj2\u0001tan(\u0012T):\nThe last inequality is due to the angle between two vectors\nin\u0004is less than\u0012T, that\u0012T2[0;\u0019\n2), and that tan(x)is a\nmonotonically increasing function on the interval [0;\u0019\n2).\nSo~ v+2V+. Letwbe the positive scalar that we multiply\n(~\u0018+~ v+)by to get~ v. Then~ v=w(~\u0018+~ v+)for some\n~ v+2V+and somew2R\u00150. So~ v2S and\u0004\u0012S.\u0003\nThis proposition gives us a decomposition for all au-\ndio shingles that are less than Tcosine dissimilar from\n~\u0018, regardless of how Tis set. Using standard orthogo-\nnal projections, we can decompose any audio shingle into\nthe formw(~\u0018+~ \u000b), where~ \u000bis audio shingle orthogonal\nto~\u0018. To check if ~ \u000b2V+, we computejj~ \u000bjj2and see if\njj~ \u000bjj2\u0014jj~\u0018jj2\u0001tan(\u0012T). Proposition 3.1 gives contextual\nmeaning to our thresholds, that is the maximum proportion\nof orthogonal notes allowed between two audio shingles of\nat mostTcosine dissimilarity.\n3.4 Relating Choice of Tto Audio Shingles via\nMaximum POMC\nThe above decomposition for vectors within Tcosine dis-\nsimilarity measure from ~\u0018provides us an avenue for re-\nlating our chosen thresholds directly to musical building\nblocks such as notes and chords, when they are represented\nas chroma feature vectors. This means that we can set\na threshold by directly encoding acceptable musical vari-\nation for a small segment instead of setting the thresh-\nold using parameters free from musical context, such as\na ﬁxed percentage of entries from a matrix representation\nor a ﬁxed-number of nearest neighbors.\nWe can set a threshold in one of three ways: 1) choos-\ningTusing existing methods, 2) setting the largest allow-\nable\u0012Tbetween two audio shingles classiﬁed as similar\nenough, or 3) by setting \u001a, the maximum POMC. Since\nT;\u0012T, and\u001aare functions of each other, ﬁxing one inher-\nently ﬁxes the other two, and so we have an interpretation\nfor that threshold in the space of audio shingles (under the\ncosine dissimilarity measure), returning musical context to\nwhat we mean by “similar structure.”\n\u000fIf we ﬁxT, then we have \u0012T= cos\u00001(1\u0000T)and\n\u001a=p\n1\u0000(1\u0000T)2\n(1\u0000T)Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 639\u000fIf we ﬁx\u0012T, thenT= 1\u0000cos\u0012Tand\u001a= tan\u0012T.\n\u000fIf we instead ﬁx \u001a, then we have \u0012T= tan\u00001(\u001a)and\nT= 1\u00001p\n\u001a2+ 1\n3.5 Returning to Motivating Examples\nIn Section 2, we described the thresholds for each exam-\nple in terms of \u0012T, which is still unsatisfactory in terms of\nmusical intuition. Now we will interpret each Tusing\u001a.\nThe thresholds in Examples 1 and 3 in Section 2 have\nsimilar interpretations, which makes sense given their con-\nstructions. In Example 1, we have T= 0:375. So\n\u001a= 1:249, meaning that for each note in a given audio\nshingle~\u0018, we can add an orthogonal vector of notes with\n1:249times the magnitude of ~\u0018and have the result be con-\nsidered similar to ~\u0018. This is quite a generous threshold. For\nexample, an audio shingle representing a C chord whole\nnote is considered similar to a second shingle representing\na C chord whole note plus a D-minor6 chord whole note\nand a B[dotted-half note. Example 3 has a similarly gen-\nerous threshold with T= 0:417. So\u001a= 1:395, and we\ncan add a few more orthogonal notes to ~\u0018than in Example\n1 and still have the result be considered similar to ~\u0018.\nExamples 2 and 4 in Section 2 both include the incor-\nporation of Gaussian noise, and their associated thresh-\nolds have similar interpretations. In Example 2, we have\nT= 0:232; so\u001a= 0:834. In Example 4, we have\nT= 0:293; so\u001a= 1:001. These thresholds are less gen-\nerous than those in Examples 1 and 3. In Example 4, an\naudio shingle representing a C chord whole note is con-\nsidered similar to a second shingle representing a C chord\nwhole note plus a D-minor chord.\nWhile the above interpretations offer a musical context\nfor our similarity thresholds, these interpretations only re-\ngard the worst case (and less likely) scenario for comparing\na given audio shingle ~\u0018to another one; that is comparing ~\u0018\nto one comprised of ~\u0018added to an audio shingle orthogonal\nto~\u0018. In addition to this interpretation, we would also advo-\ncate that when setting the similarity threshold, researchers\nalso explore comparisons of ~\u0018to audio shingles comprised\nof~\u0018with audio shingles that are not orthogonal to ~\u0018.\n4. EXPANDING USES OF MAXIMUM POMC\nBuilding off the examples in Section 2 and the methodol-\nogy in Section 3, we propose research directions that could\nbeneﬁt from using a musically relevant threshold.\nWithin the song comparison tasks, we can use the max-\nimum POMC to explore less well deﬁned variants of the\nversion detection task. For example, we can explore how\nmuch spontaneous composition is added to a jazz lead\nsheet, while also detecting the repeated sections in the lead\nsheet given a maximum POMC. In another direction, we\ncould use the relationship between a threshold and maxi-\nmum POMC to create a lower bound threshold for detect-\ning recordings using auto-tune compared to those without.Maximum POMC can be used beyond the song com-\nparison tasks. We can leverage the maximum POMC to\nperform comparisons between genres, perhaps, by quanti-\nfying the amount of expected structure in a song from one\ngenre, and comparing that to the expected value of another\ngenre. Using ideas from topological data analysis, we can\ncreate diagrams quantifying the amount of structure in a\ngiven piece as we increase the maximum POMC. We could\nalso use a dynamically set maximum POMC in generative\nmusic tasks to enforce musical style constraints given the\ntarget genre for the generated musical work.\n5. CONCLUSION\nPrevious work in MIR determined and reported similarity\nthresholds as a speciﬁc method for a speciﬁc dataset pre-\nprocessed in a speciﬁc manner for a speciﬁc task, and thus\nit is hard to compare previous results. However we can\nmore easily compare future work on both new and current\nsong datasets if we choose a similarity threshold for our\nmatrix representations that includes a tangible interpreta-\ntion within the feature space.\nThis paper offered three contributions to the study of\nsimilarity thresholds used in MIR on the self-similarity (or\ndissimilarity) matrices, like those introduced in [7]. First,\nwe demonstrated weaknesses in the current ﬁxed percent-\nage paradigm, using four examples based off one jazz lead\nsheet to show inconsistencies between the interpretations\nof the musical differences between sections of music that\nare regarded as similar.\nNext we demonstrated that it is possible to link a thresh-\nold to the feature space of the original data, by providing\na theoretical framework relating a given threshold to the\nspace of audio shingles comprised of chroma vectors under\nthe cosine dissimilarity measure. Crucial to this framework\nis the notion of proportion of orthogonal musical change\n(POMC), introduced here. This paper provides an avenue\nfor interpreting and exploring the musical context of sim-\nilarity thresholds (regardless of how they are determined)\nfor self-dissimilarity matrices built from the space of audio\nshingles through the maximum POMC. Since the theoret-\nical work in this paper only relied on facts of the cosine\ndissimilarity measure, the present framework could easily\nbe adjusted to accommodate another feature space using\nthe cosine dissimilarity measure.\nFinally we brieﬂy proposed new MIR research direc-\ntions where contextually meaningful thresholds could pro-\nvide insight. We also discussed how a contextually mean-\ningful threshold could enhance current research directions.\nSetting the similarity threshold can take into account\nboth success on a particular task, given particular data, as\nwell as a tangible musical interpretation of that threshold.\nUnderstanding that continuing to use current methods for\ndetermining the similarity threshold may be the best for\ncontinued computational success, this paper advocates for\nthe inclusion of musical context into, at least, the discus-\nsion of the similarity threshold, if not the selection.640 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Acknowledgements\nPart of this work is a portion of the author’s doctoral thesis\n[12], which was partially funded by the GK-12 Program\nat Dartmouth College (NSF award #0947790). The author\nthanks Yuri Broze for his assistance installing his code [2]\nto create the .jazz ﬁles. The author also thanks Scott Pauls,\nMichael Casey, Dan Ellis, and Jessica Thompson for their\nfeedback on the early versions of this work.\n6. REFERENCES\n[1] J. Bello. Measuring structural similarity in music.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 19(7):2013–2025, 2011.\n[2] Y . Broze and D. Shanahan. The iRb Corpus in**jazz\nformat. http://musiccog.ohio-state.edu/\nhome/index.php/iRb_Jazz_Corpus , 2012.\n[Online; accessed 28-September-2016].\n[3] M. Casey, C. Rhodes, and M. Slaney. Analysis of min-\nimum distances in high-dimensional musical spaces.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 16(5):1015 – 1028, 2008.\n[4] M. Casey and M. Slaney. Song intersection by approx-\nimate nearest neighbor search. Proc. of 7thISMIR Con-\nference , pages 144–149, 2006.\n[5] M. Casey and M. Slaney. Fast recognition of remixed\naudio. 2007 IEEE International Conference on Audio,\nSpeech and Signal Processing , pages IV – 1425 – IV–\n1428, 2007.\n[6] M. Cooper and J. Foote. Summarizing popular music\nvia structural similarity analysis. IEEE Workshop on\nApplications of Signal Processing to Audio and Acous-\ntics, pages 127 –130, 2003.\n[7] J. Foote. Visualizing music and audio using self-\nsimilarity. Proc. ACM Multimedia 99 , pages 77–80,\n1999.\n[8] M. Goto. A chorus-section detecting method for musi-\ncal audio signals. Proc. of ICASSP , 2003.\n[9] M. Goto. SmartMusicKIOSK: Music listening station\nwith chorus-search function. Proc. of 16thACM Sym-\nposium on User Interface Software and Technology\n(UIST 2003) , pages 31–40, 2003.\n[10] M. Goto. A chorus-section detection method for musi-\ncal audio signals and its application to a music listen-\ning station. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 14(5):1783–1794, 2006.\n[11] P. Grosche, J. Serr `a, M. M ¨uller, and J.Ll. Arcos.\nStructure-based audio ﬁngerprinting for music re-\ntrieval. Proc. of 13thISMIR Conference , pages 55–60,\n2012.\n[12] K. M. Kinnaird. Aligned Hierarchies for Sequential\nData . PhD thesis, Dartmouth College, 2014.[13] M. M ¨uller. Information Retrieval for Music and Mo-\ntion. Springer Verlag, 2007.\n[14] M. M ¨uller and S. Ewert. Chroma Toolbox: MATLAB\nimplementations for extracting variants of chroma-\nbased audio features. Proc. of 12thISMIR Conference ,\npages 215–220, 2011.\n[15] M. M ¨uller, P. Grosche, and N. Jiang. A segment-based\nﬁtness measure for capturing repetitive structures of\nmusic recordings. Proc. of 12thISMIR Conference ,\npages 615–620, 2011.\n[16] N. Otsu. A threshold selection method from gray-level\nhistograms. Advances in Neural Information Process-\ning Systems 14: Proceedings of the 2002 Conference ,\nSMC-9(1):62–66, 1979.\n[17] J. Paulus and A. Klapuri. Music structure analysis us-\ning probabilistic ﬁtness measure and a greedy search\nalgorithm. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 17(6):1159–1170, 2009.\n[18] J. Serr `a, M. M ¨uller, P. Grosche, and J.Ll. Arcos. Unsu-\npervised detection of music boundaries by time series\nstructure features. Proc. of the Twenty-Sixth AAAI Con-\nference on Artiﬁcial Intelligence , 2012.\n[19] J. Serr `a, M. M ¨uller, P. Grosche, and J.Ll. Arcos. Un-\nsupervised music structure annotation by time series\nstructure features and segment similarity. IEEE Trans-\nactions on Multimedia , 16(5), 2014.\n[20] J. Serr `a, X. Serra, and R.G. Andrzejak. Cross recur-\nrence quantiﬁcation for cover song identiﬁcation. New\nJournal of Physics , 11(093017), 2009.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 641"
    },
    {
        "title": "Comparing Offertory Melodies of Five Medieval Christian Chant Traditions.",
        "author": [
            "Peter van Kranenburg",
            "Geert Maessen"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415508",
        "url": "https://doi.org/10.5281/zenodo.1415508",
        "ee": "https://zenodo.org/records/1415508/files/KranenburgM17.pdf",
        "abstract": "In this study, we compare the melodies of five medieval chant traditions: Gregorian, Old Roman, Milanese, Ben- eventan, and Mozarabic. We present a newly created dataset containing several hundreds of offertory melodies, which are the longest and most complex within the total body of chant melodies. For each tradition, we train n- gram language models on a representation of the chants as sequence of chromatic intervals. By computing perplexi- ties of the melodies, we get an indication of the relations between the traditions, revealing the melodies of the Gre- gorian tradition as most diverse. Next, we perform a classi- fication experiment using global features of the melodies. The choice of features is informed by expert knowledge. We use properties of the intervallic content of the melodies, and properties of the melismas, revealing that significant differences exist between the traditions. For example, the Gregorian melodies contain less step-wise intervals com- pared to the other repertoires. Finally, we train a classifier on the perplexities as computed with the n-gram models, resulting in a very reliable classifier.",
        "zenodo_id": 1415508,
        "dblp_key": "conf/ismir/KranenburgM17",
        "keywords": [
            "melodies",
            "Gregorian",
            "Old Roman",
            "Milanese",
            "Ben-eventan",
            "Mozarabic",
            "n-gram language models",
            "chromatic intervals",
            "perplexities",
            "melismas"
        ],
        "content": "COMPARING OFFERTORY MELODIES OF FIVE MEDIEV AL\nCHRISTIAN CHANT TRADITIONS\nPeter van Kranenburg\nUtrecht University, Meertens Institute\npeter.van.kranenburg@meertens.knaw.nlGeert Maessen\nindependent scholar\ngmaessen@xs4all.nl\nABSTRACT\nIn this study, we compare the melodies of ﬁve medieval\nchant traditions: Gregorian, Old Roman, Milanese, Ben-\neventan, and Mozarabic. We present a newly created\ndataset containing several hundreds of offertory melodies,\nwhich are the longest and most complex within the total\nbody of chant melodies. For each tradition, we train n-\ngram language models on a representation of the chants as\nsequence of chromatic intervals. By computing perplexi-\nties of the melodies, we get an indication of the relations\nbetween the traditions, revealing the melodies of the Gre-\ngorian tradition as most diverse. Next, we perform a classi-\nﬁcation experiment using global features of the melodies.\nThe choice of features is informed by expert knowledge.\nWe use properties of the intervallic content of the melodies,\nand properties of the melismas, revealing that signiﬁcant\ndifferences exist between the traditions. For example, the\nGregorian melodies contain less step-wise intervals com-\npared to the other repertoires. Finally, we train a classiﬁer\non the perplexities as computed with the n-gram models,\nresulting in a very reliable classiﬁer.\n1. INTRODUCTION\nIn 789 Charlemagne ordained the Roman rite normative\nfor Christian worship throughout his Empire. The chant of\nthis rite became widely known as Gregorian chant (GRE).\nThe earliest manuscripts with pitch-readable notation date\nfrom the beginning of the eleventh-century, increasing in\nnumber until the Renaissance. Manuscripts with neumatic\ncontour notation go back to the end of the ninth century,\nand manuscripts with only the texts of the chants to al-\nmost 800. Basically all these manuscripts exhibit the same\nchants for speciﬁc liturgical occasions [13].\nSince the invention of book printing and the Reforma-\ntion, this uninterrupted and almost omnipresent European\nchant tradition came to an end. The Council of Trent\n(1545–1563) seems the beginning of many emended and\nsometimes drastically refashioned traditions of Gregorian\nchant. Since the restoration of Gregorian chant in the late\nc\rPeter van Kranenburg, Geert Maessen. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Peter van Kranenburg, Geert Maessen. “Comparing Of-\nfertory Melodies of Five Medieval Christian Chant Traditions”, 18th In-\nternational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.nineteenth century, remnants of non-Gregorian chant tradi-\ntions have continued to intrigue scholars. By the thirteenth\ncentury most of these traditions had already been abolished\nand replaced by Gregorian chant.\nTo this day the only surviving non-Gregorian tradi-\ntion is the Milanese chant (MIL) of the Ambrosian rite\nin Northern Italy. The earliest notated manuscripts date\nfrom the twelfth century. Several hundreds of MIL chants\nare melodically related to GRE chants [2]. The Old Ro-\nman chant (ROM) that once existed in Rome itself is pre-\nserved in three graduals, several antiphoners and fragments\nfrom the eleventh till thirteenth centuries. Nearly all ROM\nchants are melodically related to GRE chants, with simi-\nlar liturgical assignments. ROM was abolished in the thir-\nteenth century [14]. Nearly 200 chants of the Beneven-\ntan rite of Southern Italy survive in eleventh and twelfth-\ncentury manuscripts among the regular GRE chants. Old\nBeneventan chant (BEN) was abolished in 1058 [15].\nOn the Iberian Peninsula and Southern France the\nMozarabic rite was dominant from the sixth till the\neleventh century. Its chant is called Old Hispanic chant.\nIt was abolished in 1085 and replaced by the Roman rite\nwith its GRE. Six parishes in Toledo were allowed to con-\ntinue the tradition. The oral Mozarabic tradition was no-\ntated in early sixteenth century musical notation (MOZ).\nHowever, we also have over 5,000 Old Hispanic chants\npreserved in neumatic contour notation from the tenth till\nthirteenth centuries. Unfortunately, the vast majority of\nthese chants do not correspond with MOZ and remain pitch\nunreadable [19, 27].\nSince the 1950s the central question in chant scholar-\nship concerned the relationship between GRE and ROM.\nWhich of these traditions was the earliest? Was there per-\nhaps another tradition preceding both? Many hypotheses\nhave been put forward, but hardly any conclusive positions\nhave been reached. Most scholars, however, believe that\nboth GRE and ROM are later developments of the Roman\ntradition that was known to the Carolingians in the second\nhalf of the eight century. So the question became: Which\nwas closer to eight century Rome, GRE or ROM? Some\nscholars believe the formulaic character of ROM to hold\nthe earliest evidence, although the surviving manuscripts\nare of later date than the earliest GRE sources [7]. Some\nbelieve GRE reﬂects the earlier tradition, having adjusted\nthe Roman chant only slightly to the speciﬁc needs of the\nCarolingian world [22]. Some still believe a third, Gallican\nor Hispanic, tradition played a major role in the creation of204tradition century chants offertories parts avg. notes/part std. notes/part\nGRE: Gregorian Chant XI–XII 1,000 115 344 162.54 62.65\nROM: Old Roman Chant XI–XIII 700 94 285 170.14 69.04\nMIL: Milanese Chant XII–XIII 800 104 147 177.63 98.50\nBEN: Beneventan chant XI–XII 100 39 41 152.98 64.00\nMOZ: Mozarabic chant XI–XVI 400 71 139 127.94 52.74\nTable 1 . Estimation of date and number of mass proper chants in the main sources of ﬁve traditions, number of offertory-\nchants in our data set, number of offertory-parts, and average and standard deviation of the lengths of the parts.\nthe differences between GRE and ROM [17, 18]. A mat-\nter of debate also is the date when the chants were created.\nMcKinnon [22] argues, primarily based on the liturgical\nassignment of the chant texts, that the Roman repertoire\nwas composed according to a plan in the last decades of\nseventh-century Rome. Pﬁsterer [26] on the other hand\nargues, primarily based on the comparison of Latin Bible\ntranslations, that the repertoire has grown in accordance\nwith the solemnity of the feasts between the ﬁfth and early\nseventh centuries.\nAn important contribution to the discussion has been\nmade by Rebecca Maloy in her 2010 monograph on the\nmost complex of all chant genres in both traditions: the\noffertory [20]. Her book (including a digital edition of\n94 cognate pairs of GRE and ROM offertories) provides\na fascinating insight in modern scholarship and a highly\nsophisticated analysis of the offertory genre in both tradi-\ntions. Basically, however, she does not reach conclusive\narguments for a best hypothesis. In this paper, also, we\ndo not pretend to present a conclusive position. Instead,\nwe present the ﬁrst results of a computational analysis of\nmelodic similarities and differences between chant tradi-\ntions, illustrating directions of research that may give new\ninput to the longstanding questions. To this end we im-\nproved the musicological approach of traditional styles in\nterms of melismas, intervallic steps and leaps [13] to per-\nplexities. We also veriﬁed and transformed Maloy’s edi-\ntion into a data set, and expanded the set with all offerto-\nries from the main sources of GRE, ROM, MIL, BEN and\nMOZ. Table 1 provides an overview with estimated num-\nbers of mass proper chants in each tradition and the number\nof offertory chants included in our data set.\nImportant chant studies using computational techniques\nwere published by several authors. However, most of the\ndata are no longer available [10], represent only part of a\nsingle tradition [9], or a genre not easily available in ﬁve\ntraditions [6,12], or were not meant as exact data sets [30].\nSome of the procedures used, however, need further inves-\ntigation. Hansen’s [10] distinction of different tonalities\nfor different layers in GRE is one of these, as is the seg-\nmentation procedure used by Halperin [9] and Haas [8].\nIn fact this last approach can be seen as a precursor of the\nn-gram method we use in the current paper.\nMaloy [20] does not use computational techniques, but\nshe does with the offertory present a genre that is clearly\navailable in ﬁve different traditions.\nIn this paper we demonstrate the importance of a com-putational approach for two longstanding and complemen-\ntary questions in chant research. Based on local melodic\nstructure, our n-gram method presents relations between\ndifferent traditions (Section 3). Given a set of traditions,\nit shows which tradition has most characteristics in com-\nmon with all (or most) traditions. This clearly relates to the\nmusicological question of “origin”. Based on global fea-\ntures of the chants, our decision-tree based classiﬁcation\nmethod shows differences between the traditions, and is\nable to identify with high reliability the traditional “home”\nof single chants (Section 4). This can be helpful in identi-\nfying chants not corresponding to the catalogues in use.\n2. DATA SET\nThe contents of our data set is summarized in Table 1,\nshowing the number of offertory chants included in our set.\nThe ﬁrst column lists codes and names of the separate tra-\nditions. The second and third columns give an estimate\nof period and number of the total preserved mass proper\nchants to which our offertories belong. In most cases, one\noffertory is divided in parts, the ﬁrst part being the an-\ntiphon, and the subsequent parts the verses. Throughout\nthis paper, we take the parts as basic units for analysis and\nclassiﬁcation. We include the number of parts per tradition\nin the table. We also include basic statistics on the length\nof chant-parts in number of notes.\nFor the GRE and ROM offertories we could have used\nthe data set of Haas [8]. However, we preferred the criti-\ncal edition of Maloy, because the Offertoriale Triplex [23]\nused by Haas is notably unreliable. One of the prob-\nlems with the offertory concerns the many transpositions\nto avoid non-diatonical pitches. In selecting the best sin-\ngle manuscript for each separate chant Maloy chose, in our\nview, the best option. We converted Maloy’s Finale scores\nto V olpiano strings and again carefully checked all de-\ntails. We manually encoded the remaining offertories from\nthe facsimile of Maloy’s most important manuscript, Ben\n34 [1] and one, GRE-115, Audi Israhel , from her book.\nThe V olpiano truetype font was developed by David Hi-\nley and Fabian Weber at Regensburg University.1It is\na typeface for note heads on the ﬁve line staff for mono-\nphonic music. It is perfectly suitable for our data set. It\naffords an encoding of each score as a string of characters.\nCharacters atoprepresent the notes A till a”, while the\n1Downloadable from: http://www.uni-regensburg.de/\nFakultaeten/phil_Fak_I/Musikwissenschaft/cantus/Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 205irepresents the ﬂat sign for b’. Small and capital w,x,y\nandzrepresenting other alterations; some as deﬁned in the\nfont, some by new convention. Three dashes --- separate\nthe notes for the different words, two dashes --, for dif-\nferent syllables, and one dash marks a new neume within a\nsyllable. Numerals indicate clefs and breaks. For clariﬁca-\ntion, Figure 1 shows an example of a V olpiano string and\nthe rendering of it with the V olpiano font.\n1---a---c-cd-cdcd-f-fe---  1---a---c-cd-cdcd-f-fe---\t\t\nFigure 1 . Example of a string in volpiano encoding and its\nrendering in the font.\nWe manually encoded the MIL, BEN and MOZ offer-\ntories from the best available sources; the Milanese mass\nbook [29], the recent critical edition of Beneventan chant\n[16], and the facsimile of Mozarabic chant books [5].\n3. COMPARING TRADITIONS USING N-GRAM\nMODELS\nTo examine the interconnections between the chant tradi-\ntions concerning small-scale melodic fragments, we take\nann-gram approach. n-gram modeling has been devel-\noped in computational linguistics [21]. It employs repet-\nitive structures of a language to construct a probabilistic\nmodel allowing to compute the probability of occurrence\nof a word in its local context within a sentence. Con-\ncretely, let wbe a word in vocabulary Vbelonging to a\nlanguage L, and let s=w1; w2; : : : ; w lbe a sentence of\nlwords, also belonging to language L. Then, for a word\nwiins, ann-gram model allows to compute the condi-\ntional probability of wigiven the preceding context of n\u00001\nwords: p(wijwi\u0000(n\u00001); : : : ; w i\u00001). Previous application\nofn-gram modeling of music, notably include the IDyOM\nmodel [24], which combines long and short term models.\nFor our purpose the basic n-gram approach sufﬁces.\nTypically, an n-gram model is derived from a large col-\nlection of training sentences belonging to the language of\ninterest. In the most basic approach, for each unique con-\ntextwi\u0000(n\u00001); : : : ; w i\u00001in these training sentences, an in-\nventory is made of all possible continuations wi. This re-\nsults in a distribution over the vocabulary, indicating the\nprobability of each possible continuation of the context.\nThe full model consists of the collection of distributions\nfor the continuations of all unique contexts.\nOne of the uses of an n-gram model is to evaluate to\nwhat extent a given sentence ﬁts in a given language. This\nis the way in which we employ n-gram models of the chant\ntraditions. Since sentences are of variable length, it is not\npossible to simply compute the probability of a sentence\nas product of the probabilities of each word. Therefore,\nwe will use the measure of perplexity , which indicates thedegree to which the sentence ‘ﬁts’ in the language:\nPP=p(w1; w2; : : : ; w l)\u00001\nl: (1)\nOne particular problem in computing\np(wijwi\u0000(n\u00001); : : : ; w i\u00001)occurs if the n-gram\nwi\u0000(n\u00001); : : : ; w ihas no occurrences in the training\ndata. In that case, the probability of wiis evaluated as\nzero, rendering the probability of the entire sentence zero.\nTo circumvent this problem, several approaches exist. We\nuse modiﬁed Kneser-Ney smoothing [4] as implemented\nin the KenLM Language Model Toolkit [11]. This method\nis widely accepted as the preferred method to deal with\nzero-counts.\n3.1 Application on Chant Data\nTo derive an n-gram model from our chant data, we need\nto redeﬁne some linguistic terms. We consider the tradi-\ntions as languages. We consider each part of a chant as\nsentence, and we consider the intervals between the notes\nas words, where an interval is represented by a signed inte-\nger number indicating the direction (pos/neg) and the size\nof the interval in semi-tones. Because each of the chant tra-\nditions uses the same melodic intervals, the traditions have\nthe same vocabulary, which allows us to compute the per-\nplexity of a given chant for all ﬁve traditions. Also, since\nthe vocabulary is very small compared to the vocabulary of\nany natural language, we need much less training data than\ntypically is needed for natural language modeling.\n3.2 Choosing n\nAn important question is which value to choose for n. For\neachn2f2;3; : : : ; 10g, and for each tradition we com-\npute for each chant in that tradition the perplexity given\nits own tradition. To avoid overﬁtting, we follow a 10-\nfold evaluation, successively taking one subset of the data\nto compute the perplexities and taking the other 9 subsets\nfor training, making sure that all parts of the same chant\nalways are in the same subset. By visual inspection of\nthe distributions of perplexities, we observe that for GRE,\nMIL, and MOZ, no further decrease of average perplexity\nis noticeable for n > 5, while for BEN and ROM slight\nimprovement is achieved for respectively 7-gram and 8-\ngram models. Based on these ﬁndings, we choose n= 5\nthroughout this paper.\n3.3 Comparing Chant Traditions using n-gram\nmodels\n3.3.1 Method\nAs we are interested in the differences and commonalities\nof the ﬁve chant traditions, we perform an exhaustive eval-\nuation in which we compute for each chant-part ﬁve per-\nplexity values, one for each of the ﬁve traditions. In the\ncase of the perplexity of a chant-part given its own tradi-\ntion, we need to derive an n-gram model from all other\nchants of that tradition, excluding the chant that contains\nthe chant-part. To include this chant in deriving the n-gram206 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017model would result in a too optimistic value for the per-\nplexity. The chant must be ‘unseen’ by the model. The re-\nsulting perplexity reﬂects the extent to which the chant-part\nﬁts in its own tradition. For the perplexities given the other\nfour traditions, we take n-gram models that have been de-\nrived from the entire sets of chants from those traditions.\nThese resulting four perplexity values reﬂect to what extent\nthe chant-part ﬁts in the respective other four traditions.\nAfter obtaining all perplexity values, we visualize the\ndistributions for the various conditions as box-and-whisker\nplots [31]. The median is indicated with the red horizontal\nline. The box extends from the ﬁrst to the third quartile,\nwhich is the interquartile range (IQR). The lower vertical\nwhisker extends to the lowest data point still within 1.5\nIQR from the ﬁrst quartile and the upper whisker extends\nto the highest data point still within 1.5 IQR from the third\nquartile. The data points past the whiskers are considered\noutliers and are individually plotted as circles.\nWe evaluate whether two distributions differ signiﬁ-\ncantly by performing a Kolmogorov-Smirnov test [28], and\nwe evaluate the magnitude of the difference by computing\nthe effect-size according to\ne=\u0016x1\u0000\u0016x2\nmax(s1; s2)(2)\nin which x1andx2are the averages of the perplexities,\nands1ands2are the standard deviations. By taking the\nmax of s1ands2, the resulting value for the effect size is a\npessimistic estimation.\ngre | gre rom | rom mil | mil ben | ben moz | moz02468101214perplexity\nPerplexities\nFigure 2 . The distributions of perplexities of the chant-\nparts given their own respective tradition, represented as\nbox-plots.\n3.3.2 Results and Interpretation\nThe differences between the traditions are noticeable in\nFigure 2. The higher the perplexities, the higher the in-\nternal diversity of the repertoire. GRE is most divers.\nThe outliers show speciﬁc chants of a single tradition most\nalien to this tradition. In GRE the two verses of GRE-63,\nOravi Deum meum, are most extreme. This conforms to\nthe fact that this is the most “chromatic” GRE chant. Its\nproblematic pitches were already discussed by John of Af-\nﬂighem (early twelfth century; [20]). The next GRE outlier\nis the antiphon of GRE-95, Elegerunt apostoli .Oravi andElegerunt are considered two of only ﬁve offertories with\npossible Gallican origin, since they have cognate pairs in\nOld Hispanic chant. In ROM both antiphon and verse of\nROM-92, Domine Jesu Christe, are most extreme. This\nconforms to the fact that this chant is almost identical to\nits GRE counterpart, GRE-92, Domine Jesu Christe. As\nMaloy demonstrates on textual evidence this chant in fact\nshould be considered a GRE chant. As she puts it, “it is one\nof the few demonstrable instances of reverse, Frankish-to-\nRoman transmission in the offertory repertory” [20]. MIL\nand BEN hardly show outliers. However, in BEN the most\nextreme outlier, BEN-70, Tunc imperator, is the most syl-\nlabic chant of BEN. In MOZ, ﬁnally, the most extreme out-\nlier is MOZ-13, Offerte Domino, the only MOZ chant in\nthe fourth church mode.\nFigure 3 shows the interrelations between the traditions.\nComparing the ﬁve traditions to the ﬁve models we have\n25 comparisons, resulting in 25 distributions of perplexity\nvalues. The Kolmogorov-Smirnov test for independence\nshows that only six out of the 300 possible pairs of distri-\nbutions do not differ signiﬁcantly ( p > 0:028). Only 15\npairs of distributions have an effect size less than medium\n(e <0:5). This indicates that the vast majority of the dif-\nferences we see in the diagrams, are of signiﬁcance.\nMost striking is the top ﬁgure, showing the perplexities\ngiven the GRE model. The ﬁve box-plots there show that\nall ﬁve traditions are pretty close to the GRE model. BEN\nbeing most alien. However, compared to the four other\nﬁgures, we see BEN being even more alien to ROM and\nMOZ. As is apparent from the diagrams, GRE gives the\nbest overall model for all traditions. Second best is MIL.\nThe worst model for all is ROM, followed by MOZ.\nThese ﬁndings can be related to the longstanding ques-\ntion about origin. Assuming that the process of oral trans-\nmission generally results in decreasing complexity, it is\nwell conceivable that all traditions stem from GRE, while\nit seems inconceivable that ROM was the root of all.\n4. CLASSIFICATION WITH GLOBAL FEATURES\n4.1 Feature Set\nWe also examine the differences between the traditions\nin terms of a set of global features. A global feature\nsummarizes the entire melody in one value. The feature\nset we use relates to earlier musicological approaches\nto characterize the traditions. There are two groups of\nfeatures: features that describe the intervallic contents\nof a melody, and features that are related to the length\nof melismas. We measure the following features: the\nfrequencies of occurrence of each of the melodic intervals\nfrom -12 to 12 semitones, where the sign indicates the\ndirection; aleaps ,asteps ,dleaps , and dsteps ,\nwhich measure the fraction of intervals that respec-\ntively are ascending leaps, ascending steps, descending\nleaps, and descending steps; leaps andsteps are\nthe fractions of leaps and steps disregarding direction;\nunison is the fraction of note repetitions; melis1-1 ,\nmelis2-2 ,melis3-5 ,melis6-10 ,melis11-20 ,Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 207Figure 3 . Distributions of the perplexities given the vari-\nous traditions.tradition precision recall F1-score support\nBEN 0.38 0.22 0.28 41\nGRE 0.90 0.89 0.89 344\nMIL 0.64 0.65 0.65 147\nMOZ 0.77 0.74 0.76 139\nROM 0.80 0.86 0.83 285\navg/total 0.79 0.79 0.79 956\nTable 2 . Classiﬁcation results using the Decision Tree\nlearner on the data set with global features.\nmelis21-50 ,melis51-100 ,melis100-inf ,\n(melisx-y in general), represent the fraction of lyric\nsyllables that have between xandy(yincluded) notes\nin the melody. melis mode is the most common\nnumber of notes per syllable. melis longest ,\nmelis secondlongest ,melis thirdlongest ,\nandmelis fourthlongest are the lengths of the\nfour longest melismas. Finally, melis skewness , and\nmelis kurtosis are the skewness and kurtosis of the\ndistribution of melisma lengths.\nWe measure the values of these features in each of the\n956 chant-parts. With the resulting dataset we perform\na classiﬁcation experiment to examine whether these fea-\ntures contain information for distinguishing between the\nﬁve traditions.\n4.2 Decision Tree Classiﬁcation\nSince we are not solely interested in classiﬁcation accu-\nracy, but we also want to understand the differences be-\ntween the traditions, we prefer a learning algorithm that\nresults in an interpretable model. Therefore, we learn a\ndecision tree from our dataset with global features. We\nuse the implementation of the tree learning algorithm as\nprovided by the Python Scikit-learn library [25]. To pre-\nvent overﬁtting, and to obtain a relatively small tree, we\nset the minimum number of chant-parts per leaf to 10 and\nthe maximum depth of the tree to 3.\nTo estimate the generalization of the learned tree, we\nperform 10-fold cross-validation, successively using one\nsubset for testing and the other 9 subsets for learning a tree.\nFor each chant in the current test-set, we record whether\nthe classiﬁcation was right. Again, we make sure to keep\nall parts from the same chant in either the test or the train\nset. After this procedure, we have a classiﬁcation result for\neach of the chant-parts. Table 2 summarizes the resulting\nclassiﬁcation performance. While the overall-performance\nis not bad, discerning the chant-parts from BEN and MIL\nappears to be less successful.\nThere is no clear sign of overﬁtting. Therefore, we train\na tree on the entire data set, which represents the differ-\nences between the traditions. The tree is depicted in Figure\n4. It is apparent from the tree that the amount of step-wise\nmotion in the melodies is one of the most important charac-\nteristics to isolate the GRE chants. These chants show the\nlowest amount of steps. Furthermore, the number of sylla-\nbles with only one note, the amount of descending minor\nthirds, and the amount of descending minor seconds are of208 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017steps <= 0.6187samples = 956counts = [41, 344, 147, 139, 285]class = gredsteps <= 0.3321samples = 360counts = [15, 312, 19, 9, 5]class = greTruemelis_1-1 <= 0.3239samples = 596counts = [26, 32, 128, 130, 280]class = romFalse\nmelis_1-1 <= 0.5704samples = 332counts = [4, 308, 15, 5, 0]class = gre-1 <= 0.0961samples = 28counts = [11, 4, 4, 4, 5]class = bensamples = 321counts = [3, 306, 12, 0, 0]class = gresamples = 11counts = [1, 2, 3, 5, 0]class = mozsamples = 11counts = [0, 4, 4, 3, 0]class = gresamples = 17counts = [11, 0, 0, 1, 5]class = bendsteps <= 0.4086samples = 327counts = [16, 21, 25, 13, 252]class = rom-3 <= 0.0161samples = 269counts = [10, 11, 103, 117, 28]class = mozsamples = 115counts = [15, 21, 25, 3, 51]class = romsamples = 212counts = [1, 0, 0, 10, 201]class = romsamples = 121counts = [1, 3, 6, 100, 11]class = mozsamples = 148counts = [9, 8, 97, 17, 17]class = milFigure 4 . Decision tree as learned from the data set with global feature values. The order of the classes in the ‘counts’ ﬁeld\nis: [BEN, GRE, MIL, MOZ, ROM]. The values indicate the number of chant-parts from the respective tradition that are\n‘in’ the leave of tree.\ntradition precision recall F1-score support\nBEN 0.71 0.59 0.64 41\nGRE 0.90 0.92 0.91 344\nMIL 0.73 0.73 0.73 147\nMOZ 0.91 0.88 0.90 139\nROM 0.93 0.94 0.94 285\navg/total 0.88 0.88 0.88 956\nTable 3 . Classiﬁcation results using the Random Forest\nclassiﬁer on the data set with global features.\nimportance. With just these features, it appears possible to\nseparate the traditions to a moderately high degree.\n4.3 Random Forest Classiﬁcation\nTo examine whether it is possible to get higher classiﬁ-\ncation accuracy, we also train a Random Forest Classi-\nﬁer, which trains a number of trees on random subsets\nof the data [3]. This does not lead to an easily inter-\npretable model, but this procedure is known to typically\nshow higher performance than a single decision tree. We\nset the number of trees to 10 and we follow the same pro-\ncedure using 10-fold cross validation. The results are pre-\nsented in Table 3. The results show signiﬁcant improve-\nment, but still with weaknesses for BEN and MIL.\n4.4 Classiﬁcation with Perplexity values\nSince the perplexity of a chant-part given the n-gram\nmodel of a tradition also can be considered a global fea-\nture, we assemble another data set with for each chant-part\nthe ﬁve perplexities for the ﬁve traditions, as computed in\nSection 3, as features. The classiﬁcation results for a Ran-\ndom Forest Classiﬁer are shown in Table 4.\nBased on the perplexity values, we obtain a very accu-\nrate classiﬁer with a F1-score as high as 0.97. Even for\nthe minority class BEN we obtain very good results. Such\na classiﬁer can be of particular interest in tracing chants\nwhose origins are unclear.tradition precision recall F1-score support\nBEN 0.93 0.98 0.95 41\nGRE 0.97 0.98 0.98 344\nMIL 0.96 0.95 0.96 147\nMOZ 0.95 0.94 0.95 139\nROM 1.00 0.98 0.99 285\navg/total 0.97 0.97 0.97 956\nTable 4 . Classiﬁcation results using the Random Forest\nclassiﬁer on the perplexity data.\nWe performed an analysis of the chant-parts that are\nmis-classiﬁed by our best-performing classiﬁer, the ran-\ndom forest trained on the perplexity data. Due to space\nconstraints, it is not possible to give a full account of the\nanalysis here, but in general we can state that many of\nthe mis-classiﬁed parts are remarkable cases, including the\noutliers that have been discussed in Section 3.3.2, but also\nsome other chants with debatable origin.\n5. CONCLUSION AND FUTURE WORK\nWe presented an n-gram method to examine relations\nbetween medieval chant repertories, touching on central\nquestions in chant scholarship. Our method shows in a\nquantitatively precise way that the body of Gregorian offer-\ntory melodies is characterized by a higher internal diversity\nthan the offertories from the other four traditions. We also\npresented a highly accurate classiﬁcation method. Outliers\nand misclassiﬁcations in both cases pointed at known prob-\nlems in chant scholarship. Future work will concentrate on\nthe reﬁnement of our approaches for separate chant genres\nwithin traditions.\n6. REFERENCES\n[1]Le Codex VI 34 de la Biblioth `eque capitulaire de\nB´en´event, Graduel avec Prosaire & Tropaire . Num-\nber XV in Pal ´eographie Musicale. Abbaye Saint-Pierre\nde Solesmes, Solesmes, 1992.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 209[2] Terence Bailey. Ambrosian chant. http://www.\noxfordmusiconline.com/subscriber/\narticle/grove/music/00754 .\n[3] L. Breiman. Random forests. Machine Learning ,\n45(1):5–32, 2001.\n[4] Stanley F. Chen and Joshua Goodman. An empirical\nstudy of smoothing techniques for language modeling.\nComputer Speech and Language , 13:359–394, 1999.\n[5]´Angel Fern ´andez Collado, Alfredo Rodr ´ıguez\nGonz ´alez, and Isidoro Casta ˜neda Tordera, editors.\nLos Cantorales Moz ´arabes de Cisneros, Catedral de\nToledo . Cabildo de la Catedral Primada, 2011.\n[6] Ike de Loos. Duitse en Nederlandse Muzieknotaties in\nde 12e en 13e eeuw . PhD thesis, Universiteit Utrecht,\nUtrecht, 1996.\n[7] Joseph Dyer. The roman offertory, an introduction and\nsome hypotheses. In The Offertory and its Verses:\nResearch, Past, Present and Future . Tapir Academic\nPress, Trondheim, 2007.\n[8] Max Haas. M¨undliche ¨Uberlieferung und altr ¨omischer\nChoral, Historische und analytische computergest ¨utzte\nUntersuchungen . Peter Lang, Bern, 1997.\n[9] David Halperin. Contribution to a Morphology of Am-\nbrosian Chant, A computer-aided analysis of the pars\nhiemalis according to the British Museum manuscript\nadd. 34209 together with a package of computer pro-\ngrams for the analysis of monophonic music . PhD the-\nsis, Tel Aviv University, Tel Aviv, 1986.\n[10] Finn Egeland Hansen. The Grammar of Gregorian\nTonality, An Investigation Based on the Repertory in\nCodex H 159, Montpellier . Dan Fog Musikforlag,\nCopenhagen, 1979.\n[11] Kenneth Heaﬁeld. Kenlm language model toolkit.\nhttp://kheafield.com/code/kenlm/ .\n[12] Katherine Eve Helsen. The Great Responsories of\nthe Divine Ofﬁce, Aspects of Structure and Transmis-\nsion. PhD thesis, Universit ¨at Regensburg, Regensburg,\n2008.\n[13] David Hiley. Gregorian Chant . Cambridge University\nPress, Cambridge, 2009.\n[14] Helmuth Hucke and Joseph Dyer. Old roman\nchant. http://www.oxfordmusiconline.\ncom/subscriber/article/grove/music/\n11725 .\n[15] Thomas Forrest Kelly. Beneventan chant.\nhttp://www.oxfordmusiconline.com/\nsubscriber/article/grove/music/\n02678 .\n[16] Thomas Forrest Kelly and Matthew Peattie, editors.\nMonumenta Monodica Medii Aevi IX, The Music of the\nBeneventan Rite . B¨arenreiter, Kassel, 2016.[17] Kenneth Levy. A new look at old roman chant. Early\nMusic History , 19:81–104, 2000.\n[18] Kenneth Levy. A new look at old roman chant - ii.\nEarly Music History , 20:173–198, 2001.\n[19] Geert Maessen and Peter Van Kranenburg. A semi-\nautomatic method to produce singable melodies for the\nlost chant of the mozarabic rite. In Proceedings of the\n7th International Workshop on Folk Music Analysis ,\nM´alaga, 2017.\n[20] Rebecca Maloy. Inside the Offertory, Aspects of\nChronology and Transmission . Oxford University\nPress, Oxford, 2010.\n[21] Christopher D. Manning and Hinrich Sch ¨utze. Founda-\ntions of Statistical Natural Language Processing . MIT\nPress, Cambridge, Massachusetts, 1999.\n[22] James McKinnon. The Advent Project, The Later-\nSeventh-Century Creation of the Roman Mass Proper .\nUniversity of California Press, Berkeley, 2001.\n[23] Karl Ott and Rupert Fischer, editors. Offertori-\nale Triplex cum Versiculis . Abbaye Saint-Pierre de\nSolesmes, Solesmes, 1985.\n[24] Marcus Thomas Pearce. The Construction and Evalua-\ntion of Statistical Models of Melodic Structure in Music\nPerception and Composition . PhD thesis, City Univer-\nsity, London, London, 2005.\n[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay. Scikit-learn: Machine learning in Python. Journal\nof Machine Learning Research , 12:2825–2830, 2011.\n[26] Andreas Pﬁsterer. Cantilena Romana, Untersuchungen\nzur¨Uberlieferung des gregorianischen Chorals . Ferdi-\nnand Sch ¨oningh, Paderborn, 2002.\n[27] Don Michael Randel. Mozarabic chant.\nhttp://www.oxfordmusiconline.com/\nsubscriber/article/grove/music/\n19269 .\n[28] Nikola ˘ı Vasilevich Smirnov. Tables for estimating the\ngoodness of ﬁt of empirical distributions. Annals of\nMathematical Statistics , 19(2):279–281, 1948.\n[29] Gregorio Maria Su ˜nol, editor. Antiphonale Missarum\njuxta ritum Sanctae Ecclesiae Mediolanensis . Descl ´ee\net Socii, Rome, 1935.\n[30] Jessica Thompson, Andrew Hankinson, and Ichiro Fu-\njinaga. Searching the liber usualis: using couchdb and\nelasticsearch to query graphical musical documents. In\nProceedings of the 12th International Society for Mu-\nsic Information Retrieval Conference , Miami, 2011.\n[31] John Wilder Tukey. Exploratory Data Analysis .\nAddison-Wesley, Readin, PA, 1977.210 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Feature Discovery for Sequential Prediction of Monophonic Music.",
        "author": [
            "Jonas Langhabel",
            "Robert Lieck",
            "Marc Toussaint",
            "Martin Rohrmeier"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1418249",
        "url": "https://doi.org/10.5281/zenodo.1418249",
        "ee": "https://zenodo.org/records/1418249/files/LanghabelLTR17.pdf",
        "abstract": "Learning a model for sequential prediction of symbolic music remains an open challenge. An important special case is the prediction of pitch sequences based on a corpus of monophonic music. We contribute to this line of re- search in two respects: (1) Our models improve the state- of-the-art performance. (2) Our method affords learning interpretable models by discovering an explicit set of rele- vant features. We discover features using the PULSE learn- ing framework, which repetitively suggests new candi- date features using a generative operation and selects fea- tures while optimizing the underlying model. Defining a domain-specific generative operation allows to combine multiple music-theoretically motivated features in a uni- fied model and to control their interaction on a fine-grained level. We evaluate our models on a set of benchmark cor- pora of monophonic chorales and folk songs, outperform- ing previous work. Finally, we discuss the characteristics of the discovered features from a musicological perspec- tive, giving concrete examples.",
        "zenodo_id": 1418249,
        "dblp_key": "conf/ismir/LanghabelLTR17",
        "keywords": [
            "sequential prediction",
            "symbolic music",
            "pitch sequences",
            "state-of-the-art performance",
            "interpretable models",
            "explicit features",
            "PULSE learning framework",
            "domain-specific generative operation",
            "monophonic chorales",
            "folk songs"
        ],
        "content": "FEATURE DISCOVERY FOR\nSEQUENTIAL PREDICTION OF MONOPHONIC MUSIC\nJonas Langhabel1Robert Lieck2;3Marc Toussaint2Martin Rohrmeier3;4\n1Department of Computer Science, TU Berlin, Germany\n2Machine Learning and Robotics Lab, University of Stuttgart, Germany\n3Systematic Musicology and Music Cognition, TU Dresden, Germany\n4Digital and Cognitive Musicology Lab, EPFL, Switzerland\nlanghabel@campus.tu-berlin.de, robert.lieck@ipvs.uni-stuttgart.de,\nmarc.toussaint@ipvs.uni-stuttgart.de, martin.rohrmeier@epfl.ch\nABSTRACT\nLearning a model for sequential prediction of symbolic\nmusic remains an open challenge. An important special\ncase is the prediction of pitch sequences based on a corpus\nof monophonic music. We contribute to this line of re-\nsearch in two respects: (1) Our models improve the state-\nof-the-art performance. (2) Our method affords learning\ninterpretable models by discovering an explicit set of rele-\nvant features. We discover features using the PULSE learn-\ning framework, which repetitively suggests new candi-\ndate features using a generative operation and selects fea-\ntures while optimizing the underlying model. Deﬁning a\ndomain-speciﬁc generative operation allows to combine\nmultiple music-theoretically motivated features in a uni-\nﬁed model and to control their interaction on a ﬁne-grained\nlevel. We evaluate our models on a set of benchmark cor-\npora of monophonic chorales and folk songs, outperform-\ning previous work. Finally, we discuss the characteristics\nof the discovered features from a musicological perspec-\ntive, giving concrete examples.\n1. INTRODUCTION\nPredictive processing and the formation of expectancies\nare core capacities of human cognition, that are closely\ntied to the perception and interaction with our environment\nand to survival and ﬁtness in an evolutionary perspective.\nApart from its role in most cognitive domains, predictive\nprocessing has also been understood to play a fundamental\nrole in music perception [22, 29]. The formation of musi-\ncal expectancies is essential for goal directed processes at\ndifferent musical time-scales, for musical interaction and\nsynchronization as well as for the play with emotional ef-\nfects in music [10, 17], and particularly, musical tension\n[8, 13]. Musical expectancy has also been understood to be\nc\rJonas Langhabel, Robert Lieck, Marc Toussaint, Martin\nRohrmeier. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Jonas Langhabel, Robert\nLieck, Marc Toussaint, Martin Rohrmeier. “Feature Discovery for Se-\nquential Prediction of Monophonic Music”, 18th International Society\nfor Music Information Retrieval Conference, Suzhou, China, 2017.culture- and style-dependent and to be grounded in musical\nknowledge that is acquired through processes of implicit or\nstatistical learning [10, 31, 32]. Modeling of human pre-\ndictive processing in music is thus fundamental for com-\nputational cognitive models of music as well as for models\nof musical interaction or generation.\nMusical expectancy has been studied in terms of\nmelody, harmony and rhythm. While the task involves the\nprediction of the next note, onset, chord or combinations\nthereof given the past events in the sequence, there are\nmany different types of underlying models one can posit\nto compute predictions and event probabilities. While the\nsetting of predicting the next event is difﬁcult to deﬁne ac-\ncurately and to tackle in the general case of polyphonic\nmusic, many past approaches have simpliﬁed the problem\nto tackle a single stream of events, such as melodic notes\nor chord events. Because this context is similar to the lin-\nguistic case, one frequent approach has been to take into\naccount language models as commonly used in computa-\ntional linguistics, particularly, n-gram models and derived\nmodels, which are discussed in Sec. 1.1. More recently,\nconnectionist models have also become increasingly pop-\nular, as discussed in Sec. 1.3.\nApart from n-gram and connectionist models musical\nexpectancy and melodic structure have been modelled by\nother kinds of approaches that we do not discuss in de-\ntail. These involve, most notably, hidden Markov models\nfor melodic structure (e.g. [15, 16]) and dynamic Bayesian\nnetworks (e.g. [19, 20, 27]). More generally, a large variety\nof latent structure models beyond Markovian approaches\nmay be adequate to characterize prediction and to compute\nsequential event probabilities.\n1.1n-gram Models\nMarkovian and similar approaches have been applied since\ndecades for the modeling of music (see [22] for a review).\nn-gram models track the number of times a particular con-\ntiguous sub-sequence of events occurs in the data. In sim-\nplen-gram models the predictive distribution is computed\nby normalizing these frequency counts for a ﬁxed context\nlengthn. Suchn-gram models have been applied particu-\nlarly in the modelling of melody [5, 21, 24] and harmony649[26, 30, 33, 36, 37], classiﬁcation [4, 9], or applications\nsuch as style description or identiﬁcation [18, 28].\nA major problem with this approach is that short context\nlengths are unable to capture longer patterns while longer\ncontext lengths overﬁt on the training data by assigning\nzero probability to unseen sequences. Two common meth-\nods to overcome this problem are (1) escaping strategies,\nwhich explicitly assign non-zero weights to unseen se-\nquences and (2) smoothing methods, which combine mul-\ntiplen-gram models of different, possibly unbounded con-\ntext length (see [24] for an extensive overview).\nExtending common n-gram models, Conklin and Wit-\nten [5] proposed the notion of multiple viewpoint sys-\ntems, which combine n-gram models over different basic\nand derived features in order to improve melodic predic-\ntion taking into account correlations between different fea-\ntures. Pearce [21] extended this idea forming the basis\nof IDyOM, a cognitive model of melodic prediction and\ngeneration, which was evaluated with human psychologi-\ncal data [23].\n1.2 Long-Term and Short-Term Model\nConklin and Witten [5] proposed the distinction between\na long-term model (LTM) and a short-term model (STM).\nThe LTM is trained on a corpus of data and is supposed\nto capture piece-independent characteristics of the corre-\nsponding style, epoch, or genre. The STM, on the other\nhand, is supposed to capture properties of a single piece\nlike motives or repetitions and is trained online at predic-\ntion time for each piece separately. LTM and STM are\ncombined at prediction time (see Sec. 2.6 for details).\n1.3 RTDRBM (Connectionist Models)\nRecently, Cherla et al. [3] used a recurrent temporal dis-\ncriminative restricted Boltzmann machine (RTDRBM) as\nLTM, improving over the to-date best performing n-gram\nLTMs. RTDRBM are state-full connectionist models sim-\nilar to recurrent neural networks (RNNs), which have the\npotential to capture long-term dependencies in time series\ndata. This renders them particularly suited for the LTM.\n2. THE PULSE FRAMEWORK FOR MUSIC\nWe employ the PULSE learning framework [14] for discov-\nering relevant musical features. PULSE performs a guided\nsearch through an inﬁnitely large feature space and thereby\nallows to discover features in spaces that are too large\nfor classical feature selection approaches. In doing so,\nPULSE iteratively performs (forward) feature expansion and\n(backward) feature selection, resulting in a framework sim-\nilar to evolutionary algorithms. We will ﬁrst describe the\ngeneral principles of discovering features with PULSE in\nSec. 2.1 before going into details about our speciﬁc im-\nplementation.\n2.1 Discovering Features with PULSE\nPULSE addresses the ubiquitous machine learning problem\nthat, on the one hand, we need to include task-speciﬁc priorknowledge to efﬁciently solve a learning task but, on the\nother hand, explicitly specifying a set of features might\nneither be possible nor desirable for a number of reasons:\n(a) We may lack the explicit knowledge required to spec-\nify good features. (b) The speciﬁed features may be too\nspeciﬁc and “overﬁt” on a single problem instance. (c) Ex-\nplicitly specifying features is tedious work to be done by\nexperts, which we might want to automate.\nMore precisely, instead of explicitly specifying features,\ninPULSE we specify a generative operation N+that sug-\ngests new candidate features based on the current feature\nset.N+may inject new features as well as mutate and\nrecombine existing features, analogous to an evolutionary\nalgorithm. After expanding the feature set by including all\ncandidate features suggested by the N+operation, PULSE\nshrinks the feature set by optimizing the underlying model\nand selecting features based on the model performance.\nAgain this is akin to evolutionary algorithms with the dif-\nference that PULSE deﬁnes an objective based on the whole\nfeature set and features are thus not scored individually but\nselected based on how much they contribute to the ﬁtness\nof the whole population. For learning an optimal set of fea-\ntures and parameters, PULSE repetitively expands the fea-\nture set by applying N+and selects a subset of features by\noptimizing the model parameters \u0002and removing features\nwith zero weight.\nAs the underlying model, PULSE uses a conditional ran-\ndom ﬁeld [12], which deﬁnes a conditional probability dis-\ntributionp(xjy)as\np(xjy) =1\nZ(y)expX\nf2F\u0012ff(x;y) (1)\nZ(y) =X\nx02XexpX\nf2F\u0012ff(x0;y); (2)\nwherey2Y is known at prediction time, x2X is to be\npredicted,Fis the set of features with weights \u0002 =f\u0012f2\nRjf2Fg , and the partition function Z(y)ensures correct\nnormalization of the conditional distribution. The features\nf2F may be arbitrary real-valued functions of xandy,\nf:X\u0002Y! R. When modeling sequential data, x2X\nis the next event, y2Y \u0011X\u0003is the sequence of past\nevents, andXis called the symbol space or the alphabet.\nThe parameters are optimized by performing (stochastic)\ngradient descent on the negative log-likelihood of the data\n`(\u0002;D) =\u0000X\n(x;y)2Dlogp(xjy) +\u001a(\u0002); (3)\nwhere\u001a(\u0002)comprises any regularization terms, most no-\ntably anL1-regularization to enforce a sparse feature set.\nNote that since \u001aimplements a prior over the model pa-\nrameters, specifying additional regularization terms is an-\nother means to inject task-speciﬁc knowledge in addition\ntoN+(also see Sec. 2.5). For optimization we use Ada-\nGrad [7] combined with the approach described by Tsu-\nruoka, Jun’ichi Tsujii, and Ananiadou [35] for implement-\ning theL1-regularization. We will interchangeably speak\nof maximizing the data likelihood or minimizing the model\ncross-entropy as both objectives are equivalent.650 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Pitch Contour Interval\nTrue FalseBasis Features:\nFeature Values:+1 -5 79\n+1 -5 7479 79 77 76 67 7274Figure 1 . Visualization of generalized n-gram features\nconstructed from three different viewpoints with basis fea-\ntures (top row), a classical n-gram feature (middle row),\nand a generalized n-gram feature (bottom row). See text\nfor detailed explanation.\nWe use PULSE to discover two kinds of features: View-\npoint features (Sec. 2.2), which generalize the concept of\nclassicaln-grams, and Anchor features (Sec. 2.3), which\nincorporate the concepts of tonic and mode (key) that is\ncommon in tonal music.\n2.2 Viewpoint Features\nViewpoint or n-gram features indicate the presence of a\nspeciﬁc sequence of events. Compared to classical n-\ngrams, the approach described in the following deﬁnes a\nmore general set of features, namely ones that also in-\nclude partial sequences (i.e. sequences with “holes”) and\nsequences that mix different viewpoints. We sometimes\ndifferentiate between these generalized and classical non-\ngeneralizedn-gram features.\nAny viewpoint \u0018implies a particular alphabet X\u0018. We\ndeﬁne an associated set of basis features B\u0018such that fea-\ntureb(x;t)(s)2B\u0018indicates whether symbol x2X\u0018oc-\ncurred at time T\u0000tin the given sequence s2X\u0003\n\u0018of length\nT. The feature b(x;t)thus lookststeps into the past, indi-\ncating the occurrence of event xat that time, with b(x;0)\nreferring to the next event to be predicted. In the upper\npart of Tab. 1 we list all viewpoints that we used for con-\nstructingn-gram features.\nPicking up on the construction method suggested in [14]\nwe construct more complex features via logical conjunc-\ntion of multiple basis features. Features constructed in this\nway are more general than classical n-grams in two re-\nspects: (1) They do not necessarily contain a contiguous\nsequence of basis features, which allows to generalize over\nevents at a speciﬁc time by leaving a “hole”. (2) They may\nbe composed of basis features derived from different al-\nphabets/viewpoints, which allows to deﬁne a speciﬁc com-\nbination of viewpoints at one point in a sequence and ig-\nnore some of the viewpoints at others. In Sec. 2.4 we de-\nscribe the speciﬁc generative N+operations that we use in\ndetail.\nAn illustration of possible features constructed in this\nway is given in Fig. 1. Here, we used pitch, interval and\ncontour (P, I, C in Tab. 1) as viewpoints. The last tone\nin the sequence, the rightmost G5, is to be predicted andhas time index t= 0. As indicated by the colored bor-\nder, features evaluate to true (or1) if they match the\ndata and to false (or0) otherwise. In the top row, the\nbasis features b(I=\u00005;t=4),b(C=+1;t=2),b(P=79;t=1)and\nb(P=74;t=0)are depicted. Note that interval and contour\nfeatures not only depend on the pitch at time tbut addi-\ntionally on the previous pitch at time t\u00001. By concatenat-\ning basis features from a single viewpoint we can construct\nclassicaln-gram features, as illustrated for a pitch n-gram\nfeature in the middle row, which indicates the pitch se-\nquence 72;67;76;77;79;79. If we combine basis features\nfrom different viewpoints in a non-contiguous way, we end\nup with a generalized n-gram feature, as shown in the bot-\ntom row. This feature indicates a sequence that starts with\na 5-semitone step down, followed by an arbitrary tone from\nwhich it rises by an arbitrary interval, again followed by an\narbitrary tone, and ﬁnally terminates on a D5. As the ac-\ntual sequence terminates on a G5 this feature evaluates to\nfalse /0in this speciﬁc case.\n2.3 Anchor Features\nAnchor features allow to incorporate the concept of tonic\nand mode, that is key, into our model. They essentially are\ninterval features where the value is not deﬁned with respect\nto the previous tone but with respect to an anchor tone that\nmay be computed based on any information available at\nprediction time.\nWe use three kinds of anchor features that introduce an\nincreasing amount of prior knowledge about tonal music,\nas listed in the bottom part of Tab. 1. The F ifeatures use\ntheithtone of the current piece as reference tone, which\nis trivial to compute, does not change during the piece and\nignores the mode. In many cases the tonic is among the\nﬁrst tones of a piece. A more sophisticated approach is to\nestimate the tonic based on all tones heard so far using the\nkey-ﬁnding algorithm by Krumhansl [11] with parameters\nfrom [34]. This is realized in the T features, which may\nthus change during a piece (even though a change usually\nonly occurs within the ﬁrst couple of tones) but still ignore\nthe mode. As the employed key-ﬁnding algorithm also es-\ntimates whether the piece is in major or minor mode it is\nstraightforward to include this distinction, which is real-\nized in the K features. It is interesting to note that in con-\ntrast ton-gram features, which have an arbitrary yet ﬁxed\nlength, anchor features incorporate information from the\nentire history dating back to the very ﬁrst tone in a piece.\nJust as with viewpoint features it is possible to form log-\nical conjunctions of anchor features (anchored generalized\nn-gram features), however, in this work we conﬁne our-\nselves to including only single (unigram) anchor features.\n2.4N+Operation\nIn this section we describe the different generative N+op-\nerations we use to search through the space of generalized\nn-gram features. The role of the N+operation is to sug-\ngest new candidate features that are included in the feature\nset if they improve the model (see Sec. 2.1). Our N+op-\nerations will inject new basis features (unigrams) and sug-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 651Abbrev. Name Value Range DescriptionViewpoint\nFeaturesP pitch P MIDI pitch of the current note\nI interval I pitch difference between current and previous note\nC contour f\u00001;0;1g sign of the interval\nX extended contour f\u00002;\u00001;0;1;2g like C but\u00062 for intervals larger than \u00065 stepsAnchor\nFeaturesFiithin pieceI pitch differences to the ithtone in the current piece\nT tonic f0;:::; 11g octave invariant pitch difference to the tonic\nK key fmaj;ming\u0002f 0;:::; 11glike T but separate for major and minor keys\nTable 1 . Set of employed basis features. Pcorresponds to the prediction alphabet, that is, the set of possible MIDI pitches.\nI=fa\u0000bja;b2Pg is the set of all possible intervals in P.\ngest new candidates by taking existing features and adding\na new basis feature via logical conjunction. In general,\nwe will apply a combination of multiple N+operations to\nlearn our models.\nMore precisely, we write \u0018to refer to an N+operation\nthat adds all basis features b(x;0)2B\u0018of the correspond-\ning viewpoint for time zero to the feature set. Whether\nthe symbol\u0018refers to the viewpoint or the corresponding\nN+operation should be clear from context or is explicitly\nstated otherwise. Likewise, the operators F i, T, and K add\nthe corresponding anchor features to the feature set. We\nwrite\u0018\u0003for anN+operation that expands existing fea-\ntures by adding another basis feature from the correspond-\ning viewpoint. The \u0018\u0003operation ignores features corre-\nsponding to viewpoints other than \u0018. When applying the\n\u0018\u0003operation we implicitly assume that the \u0018operation is\nalso applied (after \u0018\u0003) without explicitly stating it.\nOur\u0018\u0003operations come in two versions, as backwards\nexpansion for the LTM and as forwards expansion for the\nSTM.\nForbackwards expansion (LTM), in the ithiteration of\nfeature expansion, \u0018\u0003expands all existing features with all\nbasis features b(x;i)2B\u0018with timei. That is, if we were\nnot to remove any features from the set, after niterations of\nbackwards expansion ,\u0018\u0003would have constructed all possi-\nble generalized n-gram features (with and without “holes”)\nfor alphabetX\u0018.\nForforwards expansion (STM),\u0018\u0003ﬁrst shifts all exist-\ning features by one time step to the past and then expands\nthem with all basis features b(x;0)2BXfor time zero. If\nwe were not to remove any features, after niterations of\nforwards expansion ,\u0018\u0003would have constructed all possi-\nblenon-generalized n-gram features (only those without\n“holes”) for alphabet X\u0018.\nBackwards and forwards expansion take into account\nthe different learning scenarios for LTM and STM. For\nthe LTM all data is known from the beginning and back-\nwards expansion successively suggests n-gram features of\nincreasing context length until the model stops improving.\nIn contrast, for the STM new data keeps coming in and\nwe construct n-gram features on-the-ﬂy by performing for-\nward expansion once per time step (see Sec. 3). This en-\nsures that (1) short n-gram features can be rebuilt from\nscratch to account for new data and (2) if an existing n-\ngram feature captures a motive in the piece, all possiblecontinuations are considered as new candidate features in\nthe next time step.\n2.5 Regularization\nThe purpose of the regularization terms \u001a(\u0002)in the PULSE\nobjective is twofold: (1) It limits growth of the feature set\nviaL1-regularization. (2) It implements a prior/bias, which\nshapes the model characteristics and is a means to prevent\noverﬁtting. We use a regularization of the form\n\u001a(\u0002) =X\nf2Fh\nj\u0012fj\u001aL1(f) +j\u0012fj2\u001aL2(f)i\n; (4)\nwhere\u001aL1(f)and\u001aL2(f)compute the L1andL2regu-\nlarization independently for each feature f. For theL1-\nregularization in our LTM we follow the rationale that the\nfurther back a note lies in time, the less impact it has on\nthe prediction of the current note. This means that longer\ncontext lengths risk to overﬁt on the training data and\nshould be regularized more strongly. We did not observe\na signiﬁcant improvement from applying an additional L2-\nregularization in the LTM and use\nLTM:\u001aL1(f) =\u00151e\u001cf=\u000f(5)\n\u001aL2(f) = 0; (6)\nwhere\u001cfis the temporal extent of feature f(i.e. the max-\nimum time index of the basis features), \u000fdetermines how\nquickly the regularization kicks in for increasing tempo-\nral extent, and \u00151determines the overall regularization\nstrength. For the STM we use\nSTM:\u001aL1(f) =\u00151e\u0000t=r1e\u001cf=\u000f(7)\n\u001aL2(f) =\u00152e\u0000t=r2; (8)\nwhich implements the same idea with two crucial modiﬁ-\ncations: (1) The overall regularization strength decays ex-\nponentially as more data becomes available, where r1=2\nare the decay rates and tis the current time index in the\nsong during online training of the STM. (2) We use an ad-\nditionalL2-regularization, which impedes sparsity but was\nfound to improve the STM performance especially in the\ninitial phase.\nThe structure of these regularization functions was the\nresult of preliminary runs. Parameters are chosen as de-\nscribed in Sec. 3.652 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20172.6 Combining LTM and STM\nLTM and STM are combined by computing a weighted\narithmetic mean of their predictive distributions [25]\np(xjy)/P\nm2Mwmpm(xjy)P\nm2Mwm(9)\nwhereMis the set of available models, which in princi-\nple may contain more than just two models. The weights\nare computed based on how “certain” a given model is, as\nmeasured by the entropy of its predictive distribution\nwm=\u0014\u0000P\nx2Xlogpm(xjy)\nlogjXj\u0015\u0000b\n; (10)\nwhere division by logjXj(the maximum possible entropy)\nensures that weights are in [0;1], andb\u00150is a bias pa-\nrameter that allows to shift weights towards models with\nlower entropy.\n3. EXPERIMENTS\nWe evaluate our models on a corpus of eight symbolic mu-\nsic datasets, as used in [1–3, 24]. The corpus consists\nof 1009 monophonic folk melodies from different coun-\ntries and styles as well as the soprano lines of 185 Bach\nchorales. The data is parsed from the **kern format using\nthe Python toolkit music21 [6]. As input for our model, the\nmelodies are represented as monophonic chromatic pitch\nsequences with ties being merged. Performance is indi-\ncated by the model cross-entropy measured in bits. We\nperform 10-fold cross-validation on each corpus separately\nusing the same folds as [2, 24]. The overall model perfor-\nmance is computed by ﬁrst computing the cross-entropy\nfor the test set in each cross-validation fold, then averag-\ning over the folds within one corpus, and ﬁnally averaging\nover the different corpora (this is the same approach as in\nprevious work).\nOptimization of the feature weights is done using Ada-\nGrad [7] with a constant learning learning rate of \u0011= 1\nand initial gradient squared accumulators of g= 10\u000010.\nLTM: We evaluate our LTM using the different N+op-\nerations listed in Tab. 2. Our best performing LTM is com-\npared to the state-of-the-art (see Tab. 3). The feature set is\nexpanded until less than 1% of the features change. The\nLTM hyperparameter \u000fwas ﬁxed to \u000f= 1=ln(2)\u00191:44\nin preliminary runs while \u00151is optimized for every cross-\nvalidation fold by leaving out 10% of the training data as\nvalidation set and performing a Gaussian process based\noptimization for \u001512[10\u00009;10\u00006]using the framework\nScikit-Optimize1.\nSTM: For the STM we combine the N+operations P, I\u0003\nand F 1. The feature set is expanded once per time step fol-\nlowed by an optimization of the feature weights until con-\nvergence. The hyperparameters were set to the following\nvalues based on preliminary runs: \u00151= 10\u00005,r1= 100 ,\n\u000f= 1=ln(1:2)\u00195:48,\u00152= 10\u00002,r2= 8.\n1https://scikit-optimize.github.ioPULSE-LTM\nPI\u0003C – 2.701\nX\u0003– 2.692\nC\u0003– 2.692\nF1 2.620\nF1F2F32.602\nT 2.586\nK 2.547\nTable 2 . Performance of PULSE-LTM for different conﬁgu-\nrations.\nLTM STM Hybrid\nPULSE 2.547 3.094 2.395\nRTDRBM [3] 2.712 3.363 2.421\nn-gram [24] 2.878 3.139 2.479\nTable 3 . Comparison of best performing PULSE, RTDRBM,\nandn-gram models.\nHybrid: For the hybrid model we combine our\n(PI\u0003C\u0003K)-LTM with our (PI\u0003F1)-STM. We also test the\ncombination of our LTM with an (C\u0003I)n-gram STM model\nfrom the IDyOM-framework [21]. The bias parameter b\nwas determined over the grid b2f0;1;2;3;4;5;6;16;32g\non the training set of each cross-validation fold.\n4. RESULTS\nThe chief results are that\n1. Our PULSE-LTM outperforms the current state-of-\nthe-art, RTDRBM [2].\n2. Our PULSE-STM outperforms the current state-of-\nthe-art, X\u0003UIn-gram [24].\n3. Our LTM/STM-hybrid model outperforms the cur-\nrent state-of-the-art, RTDRBM/ n-gram [3].\n4. The discovered features and learned weights provide\nmusically interpretable insights into the model.\nWe will now discuss these results in more detail.\n4.1 LTM Conﬁgurations\nIn Tab. 2 we list the results for our different LTM con-\nﬁgurations. In preliminary runs we identiﬁed PI\u0003C to\nbe the minimum setup for outperforming previous work.\nPerformance is improved by expanding contour features\n(C!C\u0003) in addition to intervals, enabling the model to\nlearn melody contours in addition to transposition invariant\nmotifs. Interestingly, the distinction of small and large in-\ntervals using extended contour features, X\u0003, which is con-\nsidered relevant in music theory, did not result in further\nimprovement.\nAs expected, incorporating an increasing amount of\nprior knowledge about tonic and key via F 1, F1F2F3, T,Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 653and K, respectively, led to signiﬁcant improvements. Our\nbest LTM conﬁguration, PI\u0003C\u0003K, signiﬁcantly improves\nover the current state-of-the-art, with the performance gain\nbeing of the same magnitude as was achieved by the cur-\nrent state-of-the-art RTDRBM [2] versus n-grams [24].\nIt is interesting to note that the algorithm for comput-\ning T and K involves a combination of music-theoretical\ninsights and empirical tone proﬁles. An important future\nresearch question is how this can be generalized and made\naccessible to learning from corpus data.\n4.2 STM Performance\nIn Tab. 3 we compare the PI\u0003F1PULSE-STM with the best\nRTDRBM [3] and n-gram STM [24]. To our knowl-\nedge we present the ﬁrst model that outperforms the well-\nestablishedn-gram STM for the task of sequential pre-\ndiction of symbolic monophonic music. We assume that\nan even better performance is achievable by performing\na more thorough (yet very expensive) optimization of the\nSTM hyperparameters.\n4.3 Hybrid\nCombining the PULSE-LTM with the PULSE-STM gives a\nperformance boost, outperforming current state-of-the-art\nhybrids (see Tab. 3). The combination of our PULSE-LTM\nwith an C\u0003In-gram STM from the IDyOM-framework\nyields an interesting result: While the n-gram STM alone\nperforms worse (3.152 bits) than our PULSE STM the\ncorresponding hybrid displays a better performance of\n2.365 bits. We conjecture that the n-gram STM has com-\nplementary properties to our PULSE-based model and there-\nfore is able to contribute valuable missing information.\nThe best performing LTM/STM-hybrid on this corpus thus\nis the combination of our PI\u0003C\u0003KPULSE-LTM and the C\u0003I\nn-gram STM.\n4.4 Discussion of Features\nWhile an extensive discussion of all features for the dif-\nferent corpora is beyond the scope of this paper, we show\na qualitative plot of the weights for a subset of features\nlearned by our PI\u0003C\u0003KPULSE-LTM from the Bach chorale\ncorpus in Fig. 2. First, we see that the pitch features (P)\ndescribe a general preference for tones in the middle reg-\nister. For the interval features (I) we restrict ourselves to\nlength-one features, which show a preference of small (es-\npecially descending) steps over large steps. This is in ac-\ncordance with general music-theoretic principles of voice\nleading. Note that a tritone step ( \u00066 semitones) is partic-\nularly discouraged. The anchor features (K) model sep-\narate tone proﬁles for major (M) and minor (m) modes.\nWe empirically conﬁrm a preference for relevant in-scale\ntones: tonic (0), major third (4)/minor third (3) and ﬁfth\n(7), whereas the out-of-scale tones minor second (1), mi-\nnor third(3)/major third (4), tritone (6), and minor seventh\n(10)/major seventh (11) are discouraged.\nDuring training of the model, a total of 5851 features\nwas temporally included from which 322 remained in the\nFigure 2 . Qualitative plot of the feature weights for P and I\nfeatures of length one as well as K features, learned based\non the Bach chorales. For I and K features middle C is\nchosen as reference tone as marked by the circles.\nﬁnal model. This underlines the relevance of performing\nboth feature expansion andselection, which allows PULSE\nto scale to very large feature spaces.\n5. CONCLUSION\nWe applied the PULSE framework to the problem of learning\na model for sequential prediction of symbolic monophonic\nmusic. Our models outperform the current state-of-the-art\nfor long-term, short-term and hybrid models on a standard\nbenchmark corpus of folk melodies and Bach chorales. At\nthe same time our approach affords interpretable models\nthat use an explicit set of musically relevant features. The\nsize of the processed feature spaces are challenging for\nclassical feature expansion methods and our method has\nthe potential to scale to even larger spaces. This becomes\nparticularly relevant for an application to polyphonic mu-\nsic and modeling of harmony as well as for including more\ncomplex viewpoints.\nThis is the ﬁrst application of the PULSE framework\nfor modelling music, which provides excellent results and\nopens up a number of possible directions for further inves-\ntigation. We therefore consider PULSE to be a promising\nframework for the development of a uniﬁed architecture\nfor modelling music.\n6. REFERENCES\n[1] Srikanth Cherla et al. “A Distributed Model For\nMultiple-Viewpoint Melodic Prediction”. In: Inter-\nnational Society for Music Information Retrieval\n(ISMIR) . 2013, pp. 15–20.654 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[2] Srikanth Cherla et al. “Discriminative learning and\ninference in the Recurrent Temporal RBM for\nmelody modelling”. In: Neural Networks (IJCNN),\nInternational Joint Conference on . IEEE. 2015,\npp. 1–8.\n[3] Srikanth Cherla et al. “Hybrid Long-and Short-Term\nModels of Folk Melodies.” In: International Soci-\nety for Music Information Retrieval . 2015, pp. 584–\n590.\n[4] Darrell Conklin. “Multiple viewpoint systems for\nmusic classiﬁcation”. In: Journal of New Music Re-\nsearch 42.1 (2013), pp. 19–26.\n[5] Darrell Conklin and Ian H. Witten. “Multiple View-\npoint Systems for Music Prediction”. In: Journal of\nNew Music Research 24.1 (1995), pp. 51–73.\n[6] Michael Scott Cuthbert and Christopher Ariza. “mu-\nsic21: A toolkit for computer-aided musicology\nand symbolic music data”. In: International Soci-\nety for Music Information Retrieval (ISMIR) . 2010,\npp. 637–642.\n[7] John Duchi, Elad Hazan, and Yoram Singer. “Adap-\ntive Subgradient Methods for Online Learning and\nStochastic Optimization”. In: Journal of Machine\nLearning Research 12.Jul (2011), pp. 2121–2159.\n[8] Morwaread M Farbood. “A parametric, temporal\nmodel of musical tension”. In: Music Perception: An\nInterdisciplinary Journal 29.4 (2012), pp. 387–428.\n[9] Ruben Hillewaere, Bernard Manderick, and Darrell\nConklin. “Global Feature Versus Event Models for\nFolk Song Classiﬁcation.” In: International Society\nfor Music Information Retrieval (ISMIR) . 2009.\n[10] David Huron. Sweet Anticipation: Music and\nthe Psychology of Expectation . Cambridge, Mas-\nsachusetts: MIT Press, 2006.\n[11] Carol L. Krumhansl. Cognitive Foundations of Mu-\nsical Pitch . Oxford Psychology 17. New York / Ox-\nford: Oxford University Press, 1990.\n[12] John D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. “Conditional Random Fields: Prob-\nabilistic Models for Segmenting and Labeling Se-\nquence Data”. In: Proceedings of the Eighteenth In-\nternational Conference on Machine Learning . San\nFrancisco, CA, USA: Morgan Kaufmann Publishers\nInc., 2001, pp. 282–289.\n[13] Moritz Lehne et al. “The inﬂuence of different struc-\ntural features on felt musical tension in two piano\npieces by Mozart and Mendelssohn”. In: Music Per-\nception: An Interdisciplinary Journal 31.2 (2013),\npp. 171–185.\n[14] Robert Lieck and Marc Toussaint. “Temporally\nExtended Features in Model-Based Reinforcement\nLearning with Partial Observability”. en. In: Neuro-\ncomputing 192 (2016), pp. 49–60.[15] Panayotis Mavromatis. “A hidden Markov model\nof melody production in Greek church chant”. In:\nComputing in Musicology 14 (2005), pp. 93–112.\n[16] Panayotis Mavromatis. “Minimum description\nlength modelling of musical structure”. In: Journal\nof Mathematics and Music 3.3 (2009), pp. 117–136.\n[17] L B Meyer. Emotion and Meaning in Music . Lon-\ndon: University of Chicago Press, 1956.\n[18] M. Ogihara and T Li. “ N-Gram chord proﬁles\nfor composer style identiﬁcation”. In: International\nSociety for Music Information Retrieval (ISMIR) .\n2008, pp. 671–676.\n[19] Jean-Francois Paiement, Samy Bengio, and Douglas\nEck. “Probabilistic models for melodic prediction”.\nIn:Artiﬁcial Intelligence 173.14 (2009), pp. 1266–\n1274.\n[20] Jean-Francois Paiement, Yves Grandvalet, and\nSamy Bengio. “Predictive models for music”. In:\nConnection Science 21.2-3 (2009), pp. 253–272.\n[21] Marcus T. Pearce. “The Construction and Evalua-\ntion of Statistical Models of Melodic Structure in\nMusic Perception and Composition”. PhD thesis.\nDepartment of Computing, City University, Lon-\ndon, UK, 2005.\n[22] Marcus T Pearce and Geraint A Wiggins. “Audi-\ntory expectation: The information dynamics of mu-\nsic perception and cognition”. In: Topics in cogni-\ntive science 4.4 (2012), pp. 625–652.\n[23] Marcus T Pearce and Geraint A Wiggins. “Expecta-\ntion in melody: The inﬂuence of context and learn-\ning”. In: Music Perception: An Interdisciplinary\nJournal 23.5 (2006), pp. 377–405.\n[24] Marcus T. Pearce and Geraint A. Wiggins. “Im-\nproved Methods for Statistical Modelling of Mono-\nphonic Music”. In: Journal of New Music Research\n33.4 (2004), pp. 367–385.\n[25] Marcus Pearce, Darrell Conklin, and Geraint Wig-\ngins. “Methods for Combining Statistical Models of\nMusic”. In: International Symposium on Computer\nMusic Modeling and Retrieval . Springer, 2004,\npp. 295–312.\n[26] Dan Ponsford, Geraint Wiggins, and Chris Mel-\nlish. “Statistical learning of harmonic movement”.\nIn:Journal of New Music Research 28.2 (1999),\npp. 150–177.\n[27] Stanislaw Raczynski et al. “Multiple pitch tran-\nscription using DBN-based musicological models”.\nIn:International Society for Music Information Re-\ntrieval (ISMIR) . 2010, pp. 363–368.\n[28] M. Rohrmeier and I. Cross. “Statistical Properties\nof Harmony in Bach’s Chorales”. In: 10th Interna-\ntional Conference on Music Perception and Cogni-\ntion (ICMPC) . 2008, pp. 619–627.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 655[29] Martin A Rohrmeier and Stefan Koelsch. “Predic-\ntive information processing in music cognition. A\ncritical review”. In: International Journal of Psy-\nchophysiology 83.2 (2012), pp. 164–175.\n[30] Martin Rohrmeier and Thore Graepel. “Compar-\ning feature-based models of harmony”. In: Pro-\nceedings of the 9th International Symposium on\nComputer Music Modelling and Retrieval . Citeseer.\n2012, pp. 357–370.\n[31] Martin Rohrmeier and Patrick Rebuschat. “Implicit\nlearning and acquisition of music”. In: Topics in\ncognitive science 4.4 (2012), pp. 525–553.\n[32] Jenny R Saffran et al. “Statistical learning of tone\nsequences by human infants and adults”. In: Cogni-\ntion70.1 (1999), pp. 27–52.\n[33] R. Scholz, E. Vincent, and F. Bimbot. “Robust\nmodelling of musical chord sequences using prob-\nabilistic n-grams”. In: International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE. 2009, pp. 53–56.\n[34] David Temperley. “What’s Key for Key? The\nKrumhansl-Schmuckler Key-Finding Algorithm\nReconsidered”. In: Music Perception: An Interdis-\nciplinary Journal 17.1 (1999), pp. 65–100.\n[35] Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia\nAnaniadou. “Stochastic Gradient Descent Training\nfor L1-Regularized Log-Linear Models with Cumu-\nlative Penalty”. In: Proceedings of the Joint Con-\nference of the 47th Annual Meeting of the ACL and\nthe 4th International Joint Conference on Natural\nLanguage Processing of the AFNLP . Suntec, Sin-\ngapore: Association for Computational Linguistics,\n2009, pp. 477–485.\n[36] Raymond P Whorley and Darrell Conklin. “Mu-\nsic generation from statistical models of harmony”.\nIn:Journal of New Music Research 45.2 (2016),\npp. 160–183.\n[37] R. Whorley et al. “Multiple Viewpoint Systems:\nTime Complexity and the Construction of Domains\nfor Complex Musical Viewpoints in the Harmonisa-\ntion Problem”. In: Journal of New Music Research\n42 (2013), pp. 237–266.656 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "&quot;I&apos;m at #Osheaga!&quot;: Listening to the Backchannel of a Music Festival on Twitter.",
        "author": [
            "Audrey Laplante",
            "Timothy D. Bowman",
            "Nawel Aamar"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416352",
        "url": "https://doi.org/10.5281/zenodo.1416352",
        "ee": "https://zenodo.org/records/1416352/files/LaplanteBA17.pdf",
        "abstract": "It has become common practice for audience members to use social media to connect, share, and communicate with each other during events (e.g., sport events, elections, award ceremonies). But how is this backchannel used during a musical event and what does it say about how people engage with the music and the artists performing it? In this paper, we present the result of a study of a da- taset composed of 31,140 tweets posted during and around the 10th edition of Osheaga, an important music festival held annually in Montreal. A combination of sta- tistics and qualitative content analysis is used to examine the postings. This allows us to describe the content of these postings (i.e., topics, shared media), the type of message being shared (i.e., opinion, expression, infor- mation), and who the authors of these tweets are.",
        "zenodo_id": 1416352,
        "dblp_key": "conf/ismir/LaplanteBA17",
        "keywords": [
            "social media",
            "backchannel",
            "musical event",
            "Osheaga",
            "Montreal",
            "dataset",
            "tweets",
            "content analysis",
            "topics",
            "shared media"
        ],
        "content": "“I’M AT #OSHEAGA!”: LISTENING TO THE BACKCHANNEL OF A MUSIC FESTIVAL ON TWITTER Audrey Laplante Timothy D. Bowman Nawel Aamar Université de Montréal audrey.laplante @umontreal.ca Wayne State University timothy.d.bowman@wayne.edu Université de Montréal nawel.aammar @umontreal.ca ABSTRACT It has become common practice for audience members to use social media to connect, share, and communicate with each other during events (e.g., sport events, elections, award ceremonies). But how is this backchannel used during a musical event and what does it say about how people engage with the music and the artists performing it? In this paper, we present the result of a study of a da-taset composed of 31,140 tweets posted during and around the 10th edition of Osheaga, an important music festival held annually in Montreal. A combination of sta-tistics and qualitative content analysis is used to examine the postings. This allows us to describe the content of these postings (i.e., topics, shared media), the type of message being shared (i.e., opinion, expression, infor-mation), and who the authors of these tweets are.   1. INTRODUCTION Music tastes are for many an important dimension of their sense of self, particularly during adolescence and young adulthood. People often use their music tastes as a ‘social badge’ of their identity [1, 2], which tells others who they are or who they aspire to be. Disclosing our music preferences is therefore an exercise of taste and discrimination. This is certainly one of the reasons music recordings have not displaced live musical performances; attending a music show is one of the strongest ways of showing others our love of music and/or of a particular music artist [3]. It is also the occasion to buy T-shirts, posters, or other mementos to testify that we were there. Music tastes also play an important role in the construc-tion of group identity. Concert going, as a social outing, is therefore also an opportunity to share an experience that could reinforce a friendship or a romantic relation-ship.  Social media have further amplified the role of music tastes in identity formation. By providing tools that allow their users to share their cultural preferences in various ways, users can now display their ‘social badge’ to a broader audience composed of friends, relatives, co-workers, acquaintances, or even unknown people. By do-ing so, they make a ‘taste statement’ that is used for ‘taste performance’, as an expression of prestige [4]. Further-more, social media afford users a means of connecting with other concert-goers and potentially even with the performing music artists. It has become common for the organizers of important events to provide an official hashtag so that audience members can connect and partic-ipate in a shared conversation about the event. But how do people use these affordances?   Music appears to be a common topic on Twitter; the hashtag #nowplaying, used to indicate the music a user is currently listening to, was the 6th most popular hashtag from 26 March to 25 April 20171. Several musical events, from televised music award ceremonies and contests to music festivals now propose their own official hashtag. However, very few studies have examined the content of these tweets.  This study focuses on the use of Twitter during an important musical event, the 10th edition of Montreal’s Osheaga festival. Using both quantitative and qualitative methods, we analyzed 31,140 tweets with the aim of ex-ploring the following research questions: RQ1. Who tweeted during the event and who were they speaking to? RQ2. What is the content of these messages (i.e., top-ic, media)?  RQ3. Are these messages objective or subjective? RQ4. Which events, shows, or artists during the festi-val generated the most tweets?  Garnering more information about the content and the authors of these tweets could provide some insights into how people engage with music and what they have to say about it, about the artists performing it, and about the fans. Since our reception of music depends not only on the inherent characteristics of the music itself but also on its social and cultural context, it seems relevant to exam-ine what type of information user-generated content relat-ed to music could provide and how it could help us better understand how music tastes are shaped. Moreover, ac-cording to surveys conducted by The Nielsen Company [5, 6], large music festivals have been gaining in popular-ity in Canada and in the United States. American music festival-goers were 98% more likely than the average American to discover new music on Spotify, the music streaming service, and nearly half of them shared photos and/or texted friends while attending a concert. This sug-gests that having a better understanding of the music con-                                                             1Hashtagify. http://hashtagify.me \n © Audrey Laplante, Timothy D. Bowman, Nawel Aammar. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Audrey Laplante, Timothy D. Bowman, Nawel Aammar. “ ‘I’m at #Osheaga!’: Listening to the Backchannel of a Music Festival on Twitter”, 18th International Society for Music Information Retrieval Conference, Suzhou, China, 2017. 585   sumption and perception of this growing user group could be particularly relevant for the design of music recom-mender systems.    2. RELATED WORK 2.1 Twitter As a Backchannel Social media provide a fertile ground for research. These online contexts offer a public or semi-public space where people can connect and share a conversation in real time. Therefore, despite the recency of these platforms, there are already numerous studies that have focused on the use of Twitter for various purposes, including its use for con-necting with other audience members during major events. Researchers have looked at Twitter use during televised events [7-9]. Wohn and Na [8] examined the messages posted on Twitter during two televised events, a talent show and a political speech. They manually coded the postings into four categories: emotion, attention, in-formation, and opinion. Their analysis revealed that the most popular category was Opinion, in which more than 30% of the postings were coded for both programs. Also using a qualitative approach to content analysis, Giglietto and Selva [7] looked at Twitter activity during a full sea-son of a political talk show. Again, opinion expression was the most important tweet category: it accounted for 59% of the postings. Their study also revealed that Twit-ter could be useful in identifying the most engaging mo-ments of such shows. Bruns and Stieglitz [9] employed statistical methods to examine the audience activity on Twitter, minute per minute, during the television broad-cast of the British Royal Wedding. This allowed them to determine that there was a strong correlation between Twitter activity and key moments during the ceremony.  These studies suggest that, thanks to the affordances of Twitter and other social media platforms, the audience has taken a more active role. Twitter serves as a back-channel—or as a ‘second screen’ [7]—that complements the broadcasting media and allows the broadcasters to receive audience feedback in real time [10].   2.2 Music-Related Twitter Studies In the same line of research, Highfield et al. [11] used a combination of quantitative and qualitative methods to examine Twitter activity during a major musical event that is broadcasted internationally: the Eurovision Song Contest. They found that broadcasters encouraged the use of Twitter, for example by promoting an official hashtag for the event and, in some cases, by selectively showing user tweets on screen. This indicates that there is a real interest from broadcasters and the organizers to receive live feedback from the audience and to facilitate audience engagement. Their study also showed the potential of Twitter for establishing and supporting fan communities.  A few studies have also been conducted within the MIR community. Hauger et al. [12] presented the ‘Mil-lion Musical Tweets Dataset’ (MMTD), a dataset com-posed of tweets collected using music-related hashtags. Since all tweets have geo-location data, the researchers used the dataset to geographically represent listening preferences. The MMTD has been used other researchers. Moore et al. [13] employed probabilistic embedding methods to uncover geographic and cultural patterns in it, and Farrahi et al. [14] explored the potential of Twitter data for improving the collaborative filtering approaches used by music recommender systems. Zangerle et al. [15] presented another dataset, the ‘#nowplaying Music Da-taset’. Kim et al. [16] used this dataset to examine the re-lationship between the Billboard rank and play counts extracted from Twitter postings. A strong correlation be-tween the two was found. Finally, Iren et al. [17] released the ‘Top 2000 Dataset’ composed of tweets posted in connection with the Top 2000, a yearly event broadcasted on the radio in the Netherlands for which the public is invited to vote for the greatest 2000 songs of all times.  The interest the MIR community has already demon-strated for Twitter data is an indication of the potential it has in helping us better understand users’ music behav-iour and music tastes, with the objective of improving music recommender systems.   3. OSHEAGA Created in 2006 by Evenko, the Festival Musique et Arts Osheaga is one of the most important music festival in Canada. Held annually in Montreal during the summer, the festival hosts more than 100 music artists across three days each year. While it focused on local underground music artists in the beginning, Osheaga has been hosting international artists for several years now. The festival offers a varied programme that covers different music genres, including rap, indie, and electronic music. In ad-dition to the concerts, the festival offers on-site activities as well as visual art installations. Gaining in popularity, Osheaga attracts visitors from all over the world each year, most of whom are between 20 and 25 years old. In 2016, 65% of the 135,000 festival-goers came from out-side Quebec [18, 19]. 4. METHODS 4.1 The Dataset To examine how people used Twitter during and around the Osheaga music festival, we collected the tweets relat-ed to the 2015 edition of the festival, which was held from July 31 to August 2, 2015. Although the festival it-self did not promote the use of any official hashtag on its website, the hashtag #Osheaga2015 was included in many postings made by the festival on Twitter. People also used the more generic #Osheaga hashtag. Therefore, from July 24, 2015 to August 13, 2015, we collected the tweets that contained at least one of these two hashtags, as well as tweets that contained the Twitter handle of the festival, @osheaga (i.e., the username of the official ac-count of Osheaga on Twitter). The final dataset was com-posed of 31,140 tweets.  4.2 Data Analysis A mixed-methods approach was used to analyse the data. With our research questions in mind, we calculated de-586 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   scriptive statistics, to which we added the activity, visibil-ity, and temporal metrics defined in [9].  To capture the richness of the postings, we employed a grounded theory approach to content analysis, which means that we let the categories emerge from the data, without imposing any preconceived model on it [20]. For this part, we focused on the tweets posted during the fes-tival (from July 31 to August 2). We also limited our analysis to original tweets, which means that retweets were excluded. These will be analysed separately but, due to the limited length of this paper, this analysis is not in-cluded here. Since manual coding is time consuming, we chose to focus on a random sample stratified by date. More specifically, we randomly selected 5% of the post-ings published on each of the three days of the festival. In total, 712 postings were manually coded (see Table 1).   Posting date No. of original tweets No of postings analyzed July 31 3,377 169 Aug. 01 4,778 239 Aug. 02 6,084 304 Total: 14,239 712 Table 1. Description of the dataset that was manually coded. Qualitative content analysis is a multi-step and itera-tive process. The first step consisted in developing the codebook, which was done by coding 100 postings that were not included in the final sample [21]. In the next step, two researchers used the codebook to independently code the first 100 postings of the sample in order to test it. The analyses of the two coders were then compared and discussed. This led to a revised and final version of the codebook, which was composed of 66 categories. The coding of the first 100 postings was revised and the 612 remaining postings were coded. Coding each posting took time. For each posting, the coder accessed the user profile to determine what type of user it was (e.g., individual, broadcaster, promoter). If the tweet contained URLs, the coder had to follow them to see where they led. Moreo-ver, the coder had to make sense of the content of the text. Multiple codes could be applied to one message.  5. RESULTS 5.1 Who Participates in the Conversation? As mentioned before, Twitter affordances invite users to connect and converse with other people attending the concert, with people who could not or did not want to be there, and even with the performing artists. But in reality, who participates in this shared conversation?  Visibility. Our dataset was composed of 31,140 tweets. These tweets were posted by 12,294 distinct us-ers, for an average of 2.5 tweets by user. However, a closer look shows an uneven distribution: a very small number of users accounted for a large proportion of the postings. More specifically, the top 1% of most active users accounted for 17.5% of the tweets, and the top 10%, for 44.8%. Conversely, we find a long tail of users with little activity. Indeed, 7,202 (58.6%) of users had posted only one message during the festival. Categories of users. The coding process for content analysis included accessing the Twitter account of the author of each message in order to categorize it (see Ta-ble 2). Individuals accounted for 74.2% of the postings. The next two most important categories were reporters, bloggers, TV/radio hosts, and photographers, who au-thored 10.8% of the tweets, and magazines, newspapers, blogs, and TV/radio stations, who posted 4.9% of the tweets. Different types of societies (e.g., restaurants, clothing companies) posted some tweets, usually for promotional purposes. The festival itself posted 2.5% of the messages of our sample.  Category of users No. of tweets % (n=712) Individuals 530 74.4% Reporters, bloggers, TV/radio hosts, and pho-tographers 77 10.8% Magazines, newspapers, blogs, and TV/radio sta-tions 35 4.9% Societies 23 3.2% Osheaga 18 2.5% Music artists (performing during the festival or not) 13 1.8% Promoters 9 1.3% Music producers and la-bels 7 1.0% Total 712 100.0% Table 2. Tweets by user category. 5.2 Who Are They Speaking To? Mentions. In the language of Twitter, a mention is a ref-erence to a user in a tweet using his or her Twitter handle (e.g., @osheaga). Of the 31,140 tweets in our dataset, 16,773 (53.9%) included at least one mention. There was a total of 25,746 mentions. Postings included between 0 and 9 mentions, for an average of 0.83 mention and a median of 1 mention per posting.  Mentions were used in different ways, sometimes for addressing a tweet to a specific user: Hey @b### are you at #OSHEAGA2015 this week-end? to tag someone in a photo or in a posting: #day1 @osheaga with my main girl @L###### #stayhydratedfolks #osheaga #ootd @ Parc Jean-Drapeau [followed by a link to a photo of the two friends] or to tag the performing artists of the concerts they are attending: #OSHEAGA2015 Day 3 Wrap-up @GaryClarkJr @Bobmosesmusic @SylvanEsso @sanferminband @charli_xcx @TheWarOnDrugs @Hot_Chip @alt_J @theblackkeys The number of mentions a user receives is an indication of his or her visibility. In our dataset, 3,023 distinct users received at least one mention. A few users received a Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 587   high number of mentions. With 6,360 mentions (24.7%), the Twitter account of the festival received the most men-tions, which is not surprising considering that this was one of the criteria for collecting the tweets. If we put the festival mentions aside and examine the remaining 19,286 mentions, we notice that the top 1% of most men-tioned users accounted for 32.7% of the tweets. They had received between 109 and 701 mentions each. Half of these top users were artists who performed during the fes-tival (e.g., Kendrick Lamar, James Bay, Of Monsters and Men). Among these users were also online magazines and blogs (e.g., Sidewalk Hustle, Much), and music streaming services (e.g., Stingray Music, Spotify Canada). We also find two celebrities who attended the festival but had no official role to play in it: a local pop signer (i.e., Marie-Mai) and an international top model (i.e., Cara Delevingne) whose agency had posted several photos of her at the festival on Twitter.  Artists. But what role did the performing artists take in the conversation? Users regularly mentioned the names of the artists that were performing in their tweets. Among the postings that were manually coded, 231 or 32.4% contained a reference to a performing artist. However, the users did not always used the Twitter handle of the artist to do so. More precisely, they used the Twitter handle 40.7% of the time. This suggests that most of the time, the user was not expecting any reaction from the artist. But even when a Twitter handle was used, the user did not always explicitly address his/her message to the artist. Indeed, if some users talked directly to the artist, as in this message: @MarinasDiamonds we loved your set at #OSHEAGA2015 and we would love to hug you and wish you well :) you're the best most talked about the artist at the 3rd person, as in this message: @flo_tweet has the most amazing voice. I’m in awe of this woman #OSHEAGA2015 [followed by a link to a photo] Among the 123 singers and groups who performed during the festival, two had no Twitter account. Accord-ing to our sample, a large majority (81 or 66.9%) of those who were Twitter users had not posted any tweets about Osheaga during the data collection period; they may have tweeted about the festival, but they did not use the hashtags or mentions we queried in the collection pro-cess. The remaining 40 artists (33.1%) had posted be-tween 1 and 14 tweets each, for an average of 3.2 and a median of 2 postings per artist. In total, our dataset in-cluded 129 postings from performing artists. These tweets were all manually coded. Eighty-five (65.9%) of these messages were retweets. As we can expect, the original tweet often consisted of a positive review of the artist’s performance. The tweets came mostly from blogs, radio/TV stations, newspapers, music services, report-ers/bloggers, or the festival itself, but there were a few cases (12) where the artists retweeted a fan’s posting.  Among the 44 postings that were not retweets were 13 thank you notes to the festival or the fans in general, such as: That was one of our favourite shows this summer @osheaga Thank you! #OSHEAGA2015 Some (10) used Twitter to announce that they were per-forming at the festival or doing their sound check. One music group shared a photo of its set list for the concert. There were only three postings that showed a direct inter-action between an artist and a fan. For example, a fan had asked a music group (using its Twitter handle) to play a specific song, a request to which the band drily replied: Not gonna happen In another case, the tweet was a personal thank you to a fan. And in the final case, the singer shared a fan’s video showing a blooper from his show and commented on it: Hahaha that was such a fail [followed by the link to the fan’s tweet with the video] This particular tweet was then retweeted 128 times by other users.  The low number of tweets that show a direct interac-tion between the artists and their fans should however be interpreted with caution: our dataset was composed of tweets containing two specific hashtags and one Twitter handle. It is possible that some artists replied to their fans without including those in their reply.   5.3 What Do They Tweet About? Topics. The qualitative content analysis allowed us to closely look at the content of the messages that were posted on Twitter. The main topics are presented in Table 3. By far, the most common message was to announce that one was going to Osheaga. However, we must stress that many of these messages were not posted at the initia-tive of their author. The festival was encouraging festival-goers to register their bracelet online in order to win priz-es and be able to take part in some activities on site. They could create a new account to sign up, or they could use Facebook or Twitter. Using Twitter apparently resulted in the application posting the following message on Twitter: I’m at #Osheaga2015 Day 1 - powered by Sam-sung Galaxy S6 Some changed it slightly. It could apparently also be done on site since many added to the message a photo taken in a dedicated space. These messages accounted for 29.6% of the dataset and 31.1% of the sample used for content analysis. Although these messages may appear to be spam, the fact that many users added a photo and/or did not make the effort to create a new account for the festi-val suggest that perhaps they wanted to share these tweets. Moreover, these messages were part of the con-versation about the festival on Twitter: people reacted to and commented on these tweets, and they certainly creat-ed a ‘hype’ on Twitter considering the volume. For this reason, we decided to keep them for the analysis.  Registering the bracelet online was not the only in-centive for sharing that one was attending the festival. Many did that on their own initiative, oftentimes adding a photo of their bracelet: Off to osheaga #Osheaga #OSH15 [followed by a link to a photo of the Osheaga bracelet] A few users (25) also explicitly announced attending a concert: 588 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   I’m so excited today I’m gonna see two of my favorite artists live! @MarinasDiamonds and @twentyonepilots <3 #OSHEAGA2015 Questions, comments, and complaints were addressed to the festival, who would then reply to the users. Some people also shared personal experiences during the festi-val, like hurting themselves or stumbling upon a singer or musician: Casually met the band of Florence & The Ma-chine in the lobby of my hotel tonight #OSHEAGA2015 In 12 cases, people commented on or complained about other festival-goers, about their appearance or their be-haviour, as in: Festival etiquette breach number one. #osheaga #get #down #now @ parc jean drapeau [accompanied by a photo of a person sitting on someone else’s shoulders]   all I see at #Osheaga is fake Kylie-Jenner-styled people Other topics, such as fashion, food, weather, and even books were also occasionally discussed, sometimes in combination: Tacos in the rain? Why not! #osheaga #tacos #festival #food [accompanied by a photo of the tacos] Topics No. of tweets  % (n=712) Presence at festival 271  38.1% Performing artists and their music 231  32.4% Festival (e.g., schedule, lo-gistic, transport) 50  7.0% Promotion of work, products, or services 46  6.5% Presence at concert 25  3.5% Fashion 17  2.4% Personal experience 17  2.4% Other festival-goers’ behaviour 12  1.7% Food 11  1.5% Weather 10  1.4% On-site activities 8  1.1% Table 3. Main topics discussed in tweets posted dur-ing the festival As seen in Section 5.1, festival-goers were not the on-ly ones to take part in the conversation. Various societies used Twitter to promote their work, products, or services. For instance, some on-site restaurants and shops used Twitter as an advertising venue: We are at #Osheaga! Come and see us @C####### near the Scène des Arbres [accom-panied by a photo of the food truck] (trans-lated from French) Some other retailers who were not on site, such as cloth-ing companies, tailored their promotional message for the Osheaga festival-goers: Dress it up or dress it down! This look is easy to take from day to night. #ootd #toms #friday #osheaga [accompanied by a photo of an outfit from the clothing company] Reporters, bloggers, photographers, and radio and TV hosts promoted their work differently, some by directly sharing the link to the result of their work—be it an newspaper article, a blog post, or a photo—others by an-nouncing that they were covering the festival, such as in the following tweet posted by a TV reporter: #C##### backstage at #osheaga with #patrick-watson and string quartet #mommasontherun [accompanied by a photo of the members of the string quartet] Media. Close to half (42.7%) of the 712 tweets that were manually coded contained or pointed to a non-textual resource (i.e., photo, video) (see Table 4). By far, the most often shared media type was the photo: 34% of the postings analyzed contained a photo taken by the au-thor. Amongst the main categories of photos shared by users, whether their own or someone else’s, were the fol-lowing: photos of concert (36.4% of photos), selfies with others (26.3%), photos of festival site (12.3%), other fes-tival-goers (7.2%), and selfies alone (5.5%). The vast ma-jority of the videos shared were videos of a live perfor-mance taken during the festival.  Type of media shared No. of tweets % (n=712) Personal photo 242 34.0% Personal video  41 5.8% Someone else’s with photo 15 2.1% Shares someone else’s video  6 0.8% Tweets with media in total: 304 42.7% Table 4. Types of media shared in tweets posted dur-ing the festival 5.4 Are the Messages Objective or Subjective? As mentioned in the introduction, research shows that people used their music tastes as a social badge that tells other people who they are, a phenomenon that has been exacerbated by social media who provide the sounding-box for such messages. Therefore, it seems reasonable to expect a large number of people using Twitter to express an opinion about the music they are listening to.  Of the 712 messages that were manually coded, 153 (21.5%) were explicit expression of an opinion, which is quite high considering the large proportion of tweets that were automatically generated when participants regis-tered their bracelet online (see Section 5.3). Moreover, when people used Twitter to announce publically that they were attending a concert, although they were not ex-plicitly expressing an opinion about the artist and his/her music, it seems very likely that for many, this was a form of implicit expression of their love for the artist. Howev-er, since it was impossible to know with certainty what the user had in mind while posting these tweets, they were not included in the Opinion Expression category.  In addition to expressing their opinion through their tweets, subjectivity also took the form of emotion expres-sion. A small proportion of the tweets (45 or 6.3%) fell in that category. Most of the time, the emotion conveyed was excitement:  Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 589   So excited to see @runjewels today at #Osheaga today, it's gonna be hype! Mikey & brian of #Weezer. #VIP I feel like a 16 year old Asian girl #OSHEAGA [accompanied by a photo of self with friends]    Sometimes, the emotion was not named but it transpired from the interjections, the emojis, or the repetition of some letters in a word: KENDRICKKKKKKKK #WEGONBEALRIGHT #OSHEAGA2015 [accompanied by a photo of Kendrick Lamar on stage]  @youngthegiant 50 mins untill you guys play!!! @youngthegiant #osheaga!  Sharing pure information, as in the tweet below, was not common.  New adult 'play' area complete with jumping castles and swings #osheaga2015 #cbcmtl [ac-companied by a photo of the area]  More often, information and opinion or emotion were combined in one tweet:  The charming George Ezra is playing on the Mountain Stage#OSHEAGA2015 [accompanied by a photo of the singer on stage]  No. of tweets  % (n=712) Opinion expression (all) 153  21.5% About concerts or artists 96  13.5% About festival 37  5.2% About on-site activities 3  0.4% Other 17  2.4% Emotion expression 45  6.3% Subjective tweets in total: 207 29.1% Table 5. Opinion and emotion expression in tweets post-ed during the festival 5.5 How Do the Festival Events Influence Twitter Ac-tivity?  To identify how the activity on Twitter relates to the fes-tival events, we looked at the number of tweets per hour during the three days of the festival, from 7 AM to 11 PM. Figure 1 shows the distribution of the tweets per hour for these days.  The program started at 1 PM to finish at 11 PM. We notice a first peak on each day at 12 PM, which certainly corresponds to the time at which people would arrive on site. Two other peaks are noted on July 31, which coin-cide with the beginning of shows by headliners artists of the festival. The first peak occurred at 3 PM, the time at which the Run The Jewels show started, and the second occurred at 8 PM, the time at which the show of two im-portant artists simultaneously started: FKA Twigs and Of Monsters and Men. On August 2, two clear bursts of activity on Twitter are observed, one at 4 PM, during the Father John Misty show, and another at 6 PM. This last peak is harder to ex-plain. It might be due to the fact that it corresponds with the end of the The War On Drugs show and the beginning of the Hot Chip’s. The activity on Twitter is more stable on August 1st, which is surprising since this was the day the most awaited show—Kendrick Lamar’s—was sched-uled. This concert, which started at 9:20 PM, only trig-gered a modest burst. A closer look at the tweets posted during this show could help better explain why it did not led to more activity on Twitter.  \n Figure 1. Number of tweets per hour during the festival 6. CONCLUSION In this paper, we presented a study on the tweets posted during and around a major music festival, Osheaga 2015. A combination of quantitative and qualitative methods allowed us to better understand how Twitter was used by festival-goers, broadcasters, other societies, and perform-ing artists. The analysis confirmed the results of previous studies, which revealed that Twitter [22] and other social media platforms [4] are used for taste performance or for what Papacharissi calls ‘performances of the self’. In-deed, the high proportion of opinion expression tweets and the even higher number of tweets users wrote to an-nounce that they were going to the festival or attending a specific concert suggest a desire to perform in this semi-public space. The content analysis also indicated that some users wanted the music artists they loved to take part in the conversation. Many users included the Twitter handle of the artists they were talking about in their tweets; some even spoke directly to them, even though we found little evidence that such interactions were common. This echoes the work of Litt and Hargittai [23] on the ‘imagined audience’ of Twitter users. In addition to the personal, communal, and professional ties people envision as their audience when posting a tweet, some people imagine ‘phantasmal ties’, which represent the famous people they hope to reach with their tweets and with whom they have an ‘illusionary relationship’. This study shows how rich the backchannel conversa-tion of a music festival can be on Twitter. This conversa-tion could provide interesting avenues for the refinement of music recommender systems. Since people use Twitter to express their opinion about music artists, this channel could be used to better understand the temporal dynamics of individuals’ music tastes. Also, since Twitter allows us to follow the music reception of festival-goers in real-time, music recommender systems could potentially use hashtags of musical events to retrieve tweets that could allow them to identify music trends in a specific location.   \n590 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   7. REFERENCES [1] S. Frith: Sound Effects: Youth, Leisure, and the Politics of Rock 'n' Roll, Pantheon Books, New York, 1981. [2] A. C. North, and D. J. Hargreaves: “Music and Adolescent Identity,” Music Education Research, Vol. 1, No. 1, pp. 75-92, 1999. [3] S. Frith: “Live Music Matters,” Scottish Music Review, Vol. 1, No. 1, 2007. [4] H. Liu: “Social Network Profiles as Taste Performances,” Journal of Computer-Mediated Communication, vol. 13, no. 1, pp. 252-275, 2007. [5] The Nielsen Company: Audience Insights Report on Music Festivals, 2015. [6] The Nielsen Company: That’s the Ticket: Music Fest Goers Are Ready for Summer in Canada, 2015. [7] F. Giglietto, and D. Selva: “Second Screen and Participation: A Content Analysis on a Full Season Dataset of Tweets,” Journal of Communication, Vol. 64, No. 2, pp. 260-277, 2014. [8] D. Y. Wohn, and E.-K. Na: “Tweeting about TV: Sharing Television Viewing Experiences Via Social Media Message Streams,” First Monday, Vol. 16, No. 3, 2011. [9] A. Bruns, and S. Stieglitz: “Towards More Systematic Twitter Analysis: Metrics for Tweeting Activities,” International Journal of Social Research Methodology, Vol. 16, no. 2, pp. 91-108, 2013. [10] S. Harrington, T. Highfield, and A. Bruns: “More than a Backchannel: Twitter and television,” Participations: Journal of Audience and Reception Studies, Vol. 10, No. 1, pp. 405-409, 2013. [11] T. Highfield, S. Harrington, and A. Bruns: “Twitter as a Technology for Audiencing and Fandom” Information, Communication & Society, Vol. 16, No. 3, pp. 315-339, 2013. [12] D. Hauger: “The Million Musical Tweets Dataset: What Can We Learn from Microblogs,” Proceedings of the 14th International Society for Music Information Retrieval Conference, 2013. [13] J. L. Moore, T. Joachims, and D. Turnbull: “Taste Space Versus the World: An Embedding Analysis of Listening Habits and Geography,” Proceedings of the 15th International Society for Music Information Retrieval Conference, pp. 439-444, 2014. [14] K. Farrahi, M. Schedl, A. Vall, D. Hauger, and M. Tkalcic: “Impact of Listening Behavior on Music Recommendation,” 15th International Society for Music Information Retrieval Conference, pp. 484-486, 2014. [15] E. Zangerle, M. Pichl, W. Gassler, and G. Specht: “#nowplaying Music Dataset: Extracting Listening Behavior from Twitter,” Proceedings of the First International Workshop on Internet-Scale Multimedia Management, pp. 21-26, 2014. [16] Y. Kim, B. Suh, and K. Lee: “#nowplaying the Future Billboard: Mining Music Listening Behaviors of Twitter Users for Hit Song Prediction,” Proceedings of the first international workshop on Social media retrieval and analysis, pp. 51-56, 2014. [17] D. Iren, C. C. S. Liem, J. Yang, and A. Bozzon: “Using Social Media to Reveal Social and Collective Perspectives on Music,” Proceedings of the 8th ACM Conference on Web Science, pp. 296-300, 2016. [18] S. Chartier: “Dix ans dans les oreilles.,” Le Devoir, 2015. [19] É. Côté: “Bilan positif pour Osheaga,” La Presse, 2016. [20] H.-F. Hsieh, and S. E. Shannon: “Three Approaches to Qualitative Content Analysis,” Qualitative Health Research, Vol. 15, No. 9, pp. 1277-1288, 2005. [21] A. Marwick: “Ethnographic and Qualitative Research on Twitter,” Twitter and Society, K. Weller, A. Bruns, C. Puschmann, J. Burgess and M. Mahrt, eds., pp. 109-122, Peter Lang, New York, 2013. [22] Z. Papacharissi: “Without You, I’m nothing: Performances of the Self on Twitter,” International Journal of Communication, Vol. 6, No. 2, 2012. [23] E. Litt, and E. Hargittai: “The Imagined Audience on Social Network Sites,” Social Media + Society, January-March, pp. 1-12, 2016.  Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 591"
    },
    {
        "title": "Clustering Expressive Timing with Regressed Polynomial Coefficients Demonstrated by a Model Selection Test.",
        "author": [
            "Shengchen Li",
            "Simon Dixon",
            "Mark D. Plumbley"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417101",
        "url": "https://doi.org/10.5281/zenodo.1417101",
        "ee": "https://zenodo.org/records/1417101/files/LiDP17.pdf",
        "abstract": "Though many past works have tried to cluster expressive timing within a phrase, there have been few attempts to cluster features of expressive timing with constant dimen- sions regardless of phrase lengths. For example, used as a way to represent expressive timing, tempo curves can be regressed by a polynomial function such that the num- ber of regressed polynomial coefficients remains constant with a given order regardless of phrase lengths. In this paper, clustering the regressed polynomial coefficients is proposed for expressive timing analysis. A model selec- tion test is presented to compare Gaussian Mixture Models (GMMs) fitting regressed polynomial coefficients and fit- ting expressive timing directly. As there are no expect- ed results of clustering expressive timing, the proposed method is demonstrated by how well the expressive tim- ing are approximated by the centroids of GMMs. The re- sults show that GMMs fitting the regressed polynomial co- efficients outperform GMMs fitting expressive timing di- rectly. This conclusion suggests that it is possible to use regressed polynomial coefficients to represent expressive timing within a phrase and cluster expressive timing with- in phrases of different lengths.",
        "zenodo_id": 1417101,
        "dblp_key": "conf/ismir/LiDP17",
        "keywords": [
            "expressive timing",
            "tempo curves",
            "regression",
            "Gaussian Mixture Models (GMMs)",
            "centroid",
            "phrase lengths",
            "model selection",
            "polynomial coefficients",
            "representation",
            "clustering"
        ],
        "content": "CLUSTERING EXPRESSIVE TIMING WITH REGRESSED POLYNOMIAL\nCOEFFICIENTS DEMONSTRATED BY A MODEL SELECTION TEST\nShengchen Li\nBeijing University of\nPosts and Telecommunications\nshengchen.li@bupt.edu.cnSimon Dixon\nQueen Mary University of London\ns.e.dixon@qmul.ac.ukMark D. Plumbley\nUniversity of Surrey\nm.plumbley@surrey.ac.uk\nABSTRACT\nThough many past works have tried to cluster expressive\ntiming within a phrase, there have been few attempts to\ncluster features of expressive timing with constant dimen-\nsions regardless of phrase lengths. For example, used as\na way to represent expressive timing, tempo curves can\nbe regressed by a polynomial function such that the num-\nber of regressed polynomial coefﬁcients remains constant\nwith a given order regardless of phrase lengths. In this\npaper, clustering the regressed polynomial coefﬁcients is\nproposed for expressive timing analysis. A model selec-\ntion test is presented to compare Gaussian Mixture Models\n(GMMs) ﬁtting regressed polynomial coefﬁcients and ﬁt-\nting expressive timing directly. As there are no expect-\ned results of clustering expressive timing, the proposed\nmethod is demonstrated by how well the expressive tim-\ning are approximated by the centroids of GMMs. The re-\nsults show that GMMs ﬁtting the regressed polynomial co-\nefﬁcients outperform GMMs ﬁtting expressive timing di-\nrectly. This conclusion suggests that it is possible to use\nregressed polynomial coefﬁcients to represent expressive\ntiming within a phrase and cluster expressive timing with-\nin phrases of different lengths.\n1. INTRODUCTION\nIn performed classical piano music, small variations of the\nbeat length serving music expression is known as expres-\nsive timing . Expressive timing can be represented by tempo\ncurves that connects the value of tempo on each beat to for-\nm a curve. A common method [3,5,8,11] of analysing ex-\npressive timing within a phrase in performed classical pi-\nano music is to cluster expressive timing. One of the possi-\nble unit used for clustering expressive timing is phrase [5]\nthat contains a certain beats forming a sensible music struc-\nture. The length of phrase, or phrase length (deﬁned as\nthe number of beats contained in a phrase), is expected\nto be identical throughout a piece of music by most algo-\nrithms such as Li et al. [5]. Such strong restrictions make\nc⃝Shengchen Li, Simon Dixon, Mark D. Plumbley. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Shengchen Li, Simon Dixon, Mark D.\nPlumbley. “Clustering expressive timing with regressed polynomial co-\nefﬁcients demonstrated by a model selection test”, 18th International So-\nciety for Music Information Retrieval Conference, Suzhou, China, 2017.the large-scale applications of existing algorithms almost\nimpossible because the phrase lengths are not constant in\nmost pieces. This paper proposes a way to cluster expres-\nsive timing regardless of phrase length.\nIn past research [14], polynomial functions, especial-\nly parabolic functions, are used to regress tempo curves.\nRegressing a tempo curve, the coefﬁcients of the resulting\npolynomial function is called regressed polynomial coef-\nﬁcients for a tempo curve. Given an order of polynomial\nfunction to be regressed to, each tempo curve can be rep-\nresented by a ﬁxed number of regressed coefﬁcients. In\nthis paper, we propose to cluster regressed polynomial co-\nefﬁcients instead of clustering expressive timing directly\nin order to enable the clustering of expressive timing with-\nout a pre-deﬁned unit possible. A model selection test is\nshown in this paper demonstrating the Gaussian Mixture\nModels (GMMs) ﬁtting regressed polynomial coefﬁcients\noutperform the GMMs ﬁtting expressive timing directly.\nFor simplicity, the GMMs ﬁtting the expressive tim-\ning are represented by GMM o, whereas the GMMs ﬁtting\nthe regressed polynomial coefﬁcients are represented as\nGMM r. There are multiple ways to compare two GMMs.\nBecause the two types of GMMs ﬁtting two different sets\nof data in this paper, the traditional model selection criteria\n(such as Bayesian Information Criterion [1, Ch. 3] used by\nLi et al. [5]) based on model likelihood cannot be used. Al-\nthough comparing the clustering results with a ground truth\nis a more general way to evaluate model performance, the\nclustering of expressive timing has no consensus or well-\nrecognised “ground truth” by the musicologists. The per-\nformance of GMMs is evaluated by the approximation of\neach tempo curve by their corresponding centroids as this\nprinciple is a general evaluation for clustering algorithms.\nTo make the clustering of expressive timing and the\nregressed polynomial coefﬁcients comparable, the pieces\nwe selected in this paper still have constant phrase length-\ns. However, the expressive timing in various phrases can\nbe regressed to the polynomial function of a single order\nregardless of phrase lengths. The three pieces of music\nare two pieces of Chopin’s Mazurkas (Op. 24, No. 2 and\nOp. 30, No. 2) used in the previous works [10, 11] and\nBerekrev’s Islamey dataset, which Li et al [5] used. Al-\nthough the music analysed is classical music, the proposed\nalgorithm for clustering expressive timing may be poten-\ntially used for other forms of music such as jazz music.\nThis paper is organised as follows: relevant literatures457are reviewed ﬁrst, then we describe how the standardised\ntempo curves and the regressed polynomial coefﬁcients are\nclustered. Next, we will show how the performance of\nmodels are represented and the results are presented. A\ndiscussion comparing the differences of the GMMs pre-\ncedes the conclusion of the paper.\n2. BACKGROUNDS\nClustering is a widely used methodology for analysing ex-\npressive timing. As demonstrated by Li et al. [5], the stan-\ndardised tempo curves within a phrase can be clustered.\nRepp [8] used Principle Component Analysis (PCA) to\nanalyse expressive timing and found a certain number of\ncommon patterns. Spiro et al. [11] used self-organising\nmaps to cluster expressive timing and expressive dynamic-\ns patterns within a bar and asserted that expressive timing\nand dynamics are affected by music structure. All these\nworks requested a pre-selected unit of analysis with an i-\ndentical length, such as bars, phrases or the entire piece of\nmusic. Such requirements, on the other hand, limit the us-\nability of the methods of analysis because the choice of a u-\nniﬁed unit for analysis is hard to ﬁnd. Clustering regressed\npolynomial coefﬁcients instead of expressive timing di-\nrectly relaxes the restriction of a constant phrase length in\nthe testing pieces; thus, more pieces of music can be anal-\nysed using different methodologies of clustering.\nUsing second-order polynomial function, or parabolic\nfunction, to regress expressive timing tempo curves rep-\nresenting expressive timing is a traditional way to model\nexpressive timing [14]. This method was widely used in\na range of past works [8, 9, 12, 15, 16]. Repp [8, 9] used\nPCA to analyse expressive dynamics and timing in cer-\ntain numbers of performances of a Chopin’s ´etude. Rep-\np asserted that the parabolic curves are particularly good\nat modelling the expressive timing within a longer phrase\nunit [8] and that parabolic curves are useful for regress-\ning the expressive dynamics [9]. Tobudic and Widmer\n[13] used multi-level parabolic curves to learn how a con-\ncert pianist varied both dynamics and tempo when play-\ning several Mozart pieces. The learned methods were then\nused to automatically render performances of other pieces\nwith success. Timmers [12] suggested that using parabolic\ncurves to regress expressive parameters in performances is\nuseful in vocal performances. Despite the wide usage of\nparabolic curves for modelling, it is rare to cluster expres-\nsive timing with the regressed parabolic coefﬁcients or re-\ngressed polynomial coefﬁcients. This paper intends to use\na model selection test to demonstrate that regressed poly-\nnomial coefﬁcients are a valid representation of expressive\ntiming for clustering.\nGMMs are used to ﬁt the distribution of expressive tim-\ning within a phrase and regressed polynomial coefﬁcients.\nThe resulting GMM oand GMM rare compared in the pro-\nposed model selection test. A model selection test is a\ncommon method in machine learning research to test the\nﬁtness of data with a mathematical model [1, Ch. 1]. Li\net al. [5] used this method to analyse expressive timing.\nModel selection tests were used to demonstrate expressivetiming can be modelled by a clustered model [5] and to\ndetermine the factors that affect the selection of cluster-\ns of expressive timing [6]. Because GMM oand GMM r\nmodel two different datasets, the approximation of expres-\nsive timing by their corresponding centroids of GMM oand\nGMM ois used for evaluation in this paper.\nWe adapt the dataset used by Li et al. [5] in which\neach piece has a constant length phrase to make GMM o\nand GMM rcomparable. The three testing pieces of mu-\nsic are Chopin’s Mazurkas (Op. 24, No. 2 and Op. 30,\nNo. 2) and Islamey , whose lengths of phrases are twelve\nbeats, twenty-four beats and eight beats throughout the en-\ntire piece, respectively. For each testing piece, there are\nsixty-four, thirty-four and twenty-ﬁve performances.\nIn each performance, the timing of each beat is record-\ned asft1, t2, . . .gand the tempo value on each beat τican\nbe calculated as the reciprocal of inter-beat interval, name-\nlyτi=1\nti+1\u0000ti. The tempo value is then smoothed\nby the method of moving window average with a window\nsize of 3 (i.e. \u0016τi=τi\u00001+τi+τi+1\n3) to approximate hu-\nman perception of tempo [2]. The expressive timing with-\nin a phrase can then be represented as a vector of tempi\nor tempo curve: T=f\u0016τ1,\u0016τ2, . . . , \u0016τng, where nrepre-\nsents the total number of beats. The resulting standard-\nised tempo curves Tare obtained by setting the mean of\neach tempo curve to 1, e.g. T=f^τ1,^τ2, . . . , ^τng, where\n^τi=n\u0016τi∑n\nj=1\u0016τj. After the standardisation process, in each\ndata set there are msamples of n-dimensional data to be\nclustered, where mrepresents the number of phrases in the\ntesting piece of music. These samples are the raw data for\nclustering and regression.\n3. MODEL EVALUATION\nIn this paper, a method to cluster expressive timing regard-\nless of the length of phrase is proposed. As there are no\nmusicological ground truth available, the candidate model-\ns are evaluated by a traditional way to assess unsupervised\nmachine learning algorithms: how well the original data\ncan be approximated by the centroids of clusters.\nBefore discussing how GMM oand GMM rare com-\npared in details, we will ﬁrstly brief how the GMMs are\ntrained to ﬁt data with ndimensions. A traditional way to\ntrain a GMM distribution is to use the Expectation Max-\nimisation (EM) algorithm [7, Ch. 11] which attempts to\nraise the model likelihood of the training data by adjusting\nthe parameters in GMM. Particularly in this paper, all the\nGMMs are trained for ten times with random initialisation\nand the best GMM is selected as the resulting GMM.\nNext we will show how GMM oand GMM rare com-\npared. As these two types of GMMs ﬁt different data, the\ntraditional measurements based on model likelihood such\nas BIC (Bayesian Information Criterion) are not valid. As\na result, we evaluate how well the expressive timing within\na phrase is approximated by the centroids of the resulting\nGMM oand GMM r. We will discuss how the approxima-\ntion is measured in this section.458 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20173.1 Evaluation of the Clustered Standardised Tempo\nCurves\nSuppose the expressive timing in the ith phrase can be rep-\nresented as Ti=f^τ1,^τ2, . . . , ^τng. The distribution of Ti\ncan be ﬁtted to an A-component GMM (GMM o) as [5]\np(Ti) =A∑\nk=1πkN(Tij⃗ µk,\u0006k). (1)\nThe centroids of the resulting GMM ocan be represent-\ned as ⃗ µ1, ⃗ µ2, . . . , ⃗ µ A. If⃗ µi= (µ1, µ2, . . . , µ n)is used to\nrepresent the centroids of the cluster that tempo curve of\ntheith phrase ( Ti=f\u0016τ1,\u0016τ2, . . . , \u0016τng) belongs to (where\n⃗ µi2 f⃗ µ1, ⃗ µ2, . . . , ⃗ µ Ag), the correlation coefﬁcient ( ρ) and\nEuclidean distance ( D) between the corresponding cen-\ntroids µiand the expressive timing within a phrase Tiare\ngiven by\nρ(Ti, ⃗ µi) =∑n\nj=1(\u0016τj\u0000Ti)(µj\u0000⃗ µi)\n√∑n\nj=1(\u0016τj\u0000Ti)2√∑n\nj=1(µj\u0000⃗ µi)2(2)\nD(Ti, ⃗ µi) =vuutn∑\nj=1(\u0016τj\u0000µj)2 (3)\nwhere Ti=1\nn∑n\nk=1^τkand⃗ µi=1\nn∑n\nk=1µk.\n3.2 Evaluation of the Regressed Polynomial\nCoefﬁcients\nWith the least square algorithm, the standardised tempo\ncurves can be regressed to a oth order polynomial func-\ntion fo(x) =∑o\ni=0bixi. Thus the standardised tempo\ncurve representing expressive timing in phrase i(Ti=\nf\u0016τ1,\u0016τ2, . . . , \u0016τng) can be represented by a vector of re-\ngressed polynomial coefﬁcients ⃗Bi= (b0, b1, b2, . . . , b o).\nA GMM (GMM g) ﬁtting the oth order of polynomial co-\nefﬁcients is represented by GMMo\ng. For clarity, GMM g\nrepresents the GMMs ﬁtting the regressed parabolic coef-\nﬁcient of any orders.\nTo prevent overﬁtting (e.g. the function used for regres-\nsion is too complex to generalise the distribution of data),\nthe order of polynomial function oshould be smaller than\nthe length of phrase n(o < n ). The GMMo\ngcan be trained\nto ﬁt the distribution of ⃗Bisuch that\np(⃗Bi) =A∑\nk=1πkN(⃗Bij⃗ mk,\u0006k). (4)\nIf the expressive timing of phrase i(Ti) belongs to\ncluster kwhose centroid can be represented as ⃗ mk=\n(bm0, bm1, . . . , b mo), the regressed polynomial curve of\nthe expressive timing within phrase i(Ti) can be repre-\nsented as fo(xj⃗ mi) =∑o\nj=0bmjxj= (x1, x2, . . . , x n).\nThus the correlation coefﬁcient ( ρ) and Euclidean distance\n(D) between the regressed polynomial curve fo(xj⃗ mi)and\nthe expressive timing Tiare given byρ(Ti, fo(xj⃗ mi)) =\n∑n\nj=1(\u0016τj\u0000Ti)(fj\u0000fo(xj⃗ mi))\n√∑n\nj=1(\u0016τj\u0000Ti)2√∑n\nj=1(fj\u0000fo(xj⃗ mi))2(5)\nD(Ti, fo(xj⃗ mi)) =vuutn∑\nj=1(\u0016τj\u0000xj)2 (6)\nwhere Ti=1\nn∑n\nk=1^τkandfo(xj⃗ mi) =1\nn∑n\nk=1xk.\n4. RESULTS\nIn this section, we will compare how the centroids of\nGMM oand GMM gapproximate expressive timing within\na phrase by showing the correlation coefﬁcients and Eu-\nclidean distance discussed in Section 3. To train a GMM\nwith the dataset selected, an important parameter should\nbe decided: the intended number of clusters. Follow-\ning the detailed discussion by Li et al. [5], we train G-\nMMs with two Gaussian components for Islamey , eight\nGaussian components for Chopin Mazurka Op.24/2 and\nfour Gaussian components for Chopin Mazurka Op.30/2.\nMoreover, the order of polynomial function for regression\nis chosen between the second order and the tenth order\nfor both Chopin’s Mazurkas, whereas for Islamey whose\nphrase length is 8 beats the chosen order of polynomial\nfunction is between second order and the eighth order to\nprevent overﬁtting.\nCompared with the complexity of the proposed GMM o\nand GMM g, the data we have is fairly limited. To preven-\nt overﬁtting, cross validation is used in this experiment.\nRather than using the entire dataset to train the GMM oand\nGMM g, only four-ﬁfths of the performances form a train-\ning dataset, and the remaining performances form a testing\ndataset. Speciﬁcally for the candidate pieces, there are 5,\n13, 7 performances used for testing and the numbers of\nperformances for training are 20, 51, 27 for the candidate\npieces Islamey , Chopin’s Mazurka Op.24/2 and Chopin’s\nMazurka Op.30/2 respectively. To even out the possible\nbias caused by the randomness of the formation of the test-\ning and training sets, cross validation tests are repeated for\n100 times with the performances in the testing and train-\ning sets randomly selected. The performance of candidate\nmodels are evaluated by the average performance in the\n100 cross validation tests.\nWith the EM algorithm, a GMM oand a GMM gare\ntrained with each training dataset engaged. The re-\nsulting GMM oand GMM gare used to cluster the test-\ning dataset. The centroids of the resulting clusters are\nused to calculate ρ(Ti, ⃗ µi),D(Ti, ⃗ µi),ρ(Ti, fo(xj⃗ mi))\nandD(Ti, fo(xj⃗ mi))according to equations (2), (3), (5)\nand (6) where Tiis in the testing dataset. To remove\nthe possible bias caused by the randomness of perfor-\nmance selection, the experiment is repeated 100 times.\nThe resulting ρ(Ti, ⃗ µi),ρ(Ti, fo(xj⃗ mi)),D(Ti, ⃗ µi)and\nD(Ti, fo(xj⃗ mi))are compared pairwisely.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 459(a)Islamey\n(b) Mazurka Op.24/2\n(c) Mazurka Op.30/2\nFigure 1 . Box plots of the resulting correlation coefﬁcients\nand Euclidean distance between the standardised tempo\ncurves and their corresponding cluster centroids. The box\nshows 25th and 75th percentiles. The line in the box shows\nthe mean. Outliers are shown by a ‘+’ sign. A higher cor-\nrelation coefﬁcients and a smaller Euclidean distance indi-\ncates a better approximation of expressive timing by corre-\nsponding centroids. The label ‘Org’ represents the results\nof clustering expressive timing directly.\nIn Figure 1, box plots of the resulting ρ(Ti, ⃗ µi),\nD(Ti, ⃗ µi),ρ(Ti, fo(xj⃗ mi))andD(Ti, fo(xj⃗ mi))in the\n100 cross-validation tests for each testing piece are shown.\nIn the diagram, the label ‘Org’ represents the perfor-\nmance of the centroids in GMM o(namely ρ(Ti, ⃗ µi)and\nD(Ti, ⃗ µi),). The numbered labels represent the value of o\ninρ(Ti, fo(xj⃗ mi))andD(Ti, fo(xj⃗ mi)). In each boxing\nplot, the box indicates the 25th and 75th percentiles and\nthe line in the box shows the mean. The ‘+’ signs show\nthe outliers. A higher correlation coefﬁcient and a smaller\nEuclidean distance means better approximation of the ex-\npressive timing by the corresponding centroids in GMM o\nand GMM g.\nIn the resulting diagram, GMMo\ngoutperforms GMM g\nregardless of the value of o. As seen in Figure 1(b) and Fig-\nure 1(c), GMMo\ngoutperforms GMM raccording to both the\ncorrelation coefﬁcients and Euclidean distance. In Figure\n1(a), although the correlation coefﬁcients does not show\nthat GMMo\ngis better than GMM r, the Euclidean distance\nshows that GMMo\ngoutperforms GMM r. This result con-ﬁrms that using polynomial functions to regress expres-\nsive timing within a phrase can help to improve the per-\nformance of clustering expressive timing.\nNext, we discuss about which value of omakes the\nbest performed GMMo\ng. With an one-way ANOV A test [7,\nCh.8], we ﬁnd that a higher value of odoes not always\nintroduce a better performance of GMMo\ng. To show the\nsigniﬁcance of the difference between the means and cor-\nrelation coefﬁcients of GMMo\ngand GMM r, we perform a\nTukey’s Honest Signiﬁcant Difference (HSD) test.\nWith a preference of a simpler model, the results of\nTukey’s HSD show the following facts. If two GMMo\ng\nhave different values of obut no differences of perfor-\nmance, the GMMo\ngwith lower ovalues will be preferred\ndue to lower complexity of GMMo\ng. For Islamey , the\nperformance of GMM2\ngto GMM8\ngdoes not make signif-\nicant differences thus GMM2\ngis preferred. As a result,\nthe second order of polynomial function is the most suit-\nable method regressing expressive timing in Islamey . For\nChopin Mazurka Op.24/2, GMM7\ngto GMM10\ngmake no\nsigniﬁcant differences according to correlation coefﬁcients\nwhereas according to Euclidean distance, GMM10\ngis worse\nthan GMM7\ngto GMM9\ng. So in general GMM7\ngis the best\nmodel amongst the candidate models and the seventh order\nof polynomial function is the best function to regress ex-\npressive timing within a phrase for Mazurka Op.24/2. For\nChopin Mazurka Op.30/2, the best performed models are\nGMM7\ngto GMM10\ngaccording to Euclidean distance where-\nas GMM10\ngis marginally better than other models accord-\ning to correlation coefﬁcients. As a result, amongst the\ncandidate models, the tenth order of polynomial function\nis the best model to regress the expressive timing within a\nphrase.\nConsidering the fact the phrase length of Islamey ,\nMazurka Op.24/2 and Mazurka Op.30/2 are 8 beats, 12\nbeats and 24 beats respectively and the most suitable\npolynomial function to regress expressive timing within a\nphrase is the second, the seventh and the tenth order, there\nmay be a potential relationship between the most suitable\norder of polynomial function for regression and the phrase\nlength. Demonstrating this hypothesis is beyond the scope\nof this paper but is possibly a future work.\n5. DISCUSSION\n5.1 Centroid Pairing\nFrom the results of the model selection test, GMM rout-\nperforms GMM o. However, the resulting GMM rmay not\nbe necessary to make musical sense. As GMM omakes\nmusical sense [4], the centroids of GMM rand GMM oare\ncompared. If the regressed polynomial curves recovered\nfrom GMM rare correlated with the centroids of GMM o,\nthe GMM rwill also make musical sense.\nRecall that in Section 3, ⃗ µirepresented the centroids of\nthe GMMs for expressive timing within a phrase and ⃗ mj\nrepresented the centroids of the GMMs for regressed poly-\nnomial coefﬁcients that can be recovered as a polynomial\ncurve fo(xj⃗ mj). The similarity between centroids can be460 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017deﬁned by the correlation coefﬁcients ( ρ) between ⃗ µiand\nfo(xj⃗ mj), namely\nρ(⃗ µi, fo(xj⃗ mj))\n=∑n\nk=1(µk\u0000⃗ µi)(xk\u0000fo(xj⃗ mj))√∑n\nk=1(µk\u0000⃗ µi)2√∑n\nk=1(xk\u0000fo(xj⃗ mj))2(7)\nwhere ⃗ µi=1\nn∑n\nk=1µkandfo(xj⃗ mi) =1\nn∑n\nk=1xk.\nSuppose there are AGaussian components in the\nGMM rand GMM o. The centroids in GMM rand GMM o\ncan be paired according to Algorithm 1. In Table 1, the re-\nsults of pairing the centroids of the GMMs for the oth order\npolynomial coefﬁcients are shown. In Figure 2, we demon-\nstrate how the centroids of GMM ocompared with the re-\ngressed polynomial curves with the centroids of GMM2\nr\nand GMM7\nrin Chopin’s Mazurka Op.24 No.2. From the\nresults we can see that the regressed polynomial curves re-\ncovered from the centroids of GMM2\nrare highly correlated\nwith GMM owhile the regressed polynomial curves recov-\nered from the centroids of GMM7\nrare even more similar\nto GMM odue to the higher model complexity. Thus the\nresults demonstrate that the GMMs for the regressed poly-\nnomial coefﬁcients are musically valid.\nAlgorithm 1 Pair centroids\nRequire: fo(xj⃗ m)i,i2[1, A]\nRequire: ⃗ µj,i2[1, A]\nC(i, j) =ρ(⃗ µi, fo(xj⃗ mj))\nwhile max(C)\u0015 \u00001do\n(r,c)=arg max ( Crc)\npairs:=pairs [ fr,cg\nAssociate locrwithlocc\nCri= -2\nCjc= -2\nend while\nIslamey Op.24/2 Op.30/2\nGMM2\nr 0.99 0.83 0.75\nGMM3\nr 0.99 0.82 0.74\nGMM4\nr 0.99 0.84 0.72\nGMM5\nr 0.99 0.89 0.80\nGMM6\nr 1.00 0.90 0.78\nGMM7\nr 1.00 0.90 0.82\nGMM8\nr 1.00 0.90 0.76\nGMM9\nr N/A 0.90 0.79\nGMM10\nr N/A 0.90 0.86\nTable 1 . The correlation coefﬁcients between the polyno-\nmial curves recovered from the centroids of GMMo\nrand\nthe centroids of GMM o.\n5.2 GMM rwith more clusters\nWith the results presented, we can conclude that with the\nsame Gaussian components in the model, GMM goutper-forms GMM owhen the intended number of Gaussian com-\nponents is decided by the GMM oprovided by Li et al. [5].\nIn this section, we observe whether GMM r, which has\nmore Gaussian components, has a better performance. As\nan example, we compare the performance of GMM2\nr, mea-\nsured by correlation coefﬁcients with multiple Gaussian\ncomponents. In Table 2, we show how well the regressed\nparabolic curves approximate the centroids of GMM2\ngby\nshowing ρ(Ti, fo(xj⃗ mi))calculated by equation (5).\nClusters Islamey Op.24/2 Op.30/2\n2 0.5315 0.5872 0.7339\n4 0.5411 0.6181 0.7399\n8 0.5718 0.6877 0.7442\n16 0.6261 0.6930 0.7635\n32 0.6722 0.7165 0.7677\n64 0.6857 0.7324 0.7585\n128 0.6978 0.7298 0.7413\n256 0.6940 0.7282 N/A\n512 0.6467 0.7109 N/A\nTable 2 . The average value of ρ(Ti, fo(xj⃗ mi))resulting\nfrom GMM2\ngwith different numbers of Gaussian compo-\nnents (labelled as clusters in the table). A larger number\nmeans a better approximation and a better performance\n(bold). The number of clusters we set in the previous\nexperiments are in italics. The training set of Mazurka\nOp.30/2 has less than 256 samples because it is impossi-\nble to set 256 and 512 clusters in the experiments.\nFrom the table, we can see that the numbers of Gaussian\ncomponents we engaged in the experiments in section 4 for\nGMM2\ngdo not have the best performance. Thus the GMM2\ng\nwith more Gaussian components can improve the model\nperformance further.\n6. CONCLUSIONS\nIn this paper, we demonstrate whether regressing standard-\nised tempo curves within a phrase by a polynomial func-\ntion is a valid method to analyse expressive timing by com-\nparing Gaussian Mixture Models (GMMs) ﬁtting expres-\nsive timing (GMM o) and ﬁtting regressed polynomial co-\nefﬁcients (GMM g). As the candidate models ﬁt different\nsets of data and there are no musicological ground truth for\nthe clustering of expressive timing, the approximation of\nexpressive timing by the centroids of GMM oand GMM r\nis used to evaluate model performance.\nMeasured by correlation coefﬁcients and Euclidean dis-\ntance, the experiment shows that GMM goutperforms\nGMM rwhen the same numbers of Gaussian components\nare engaged. With more Gaussian components engaged,\nGMM rperforms even better. The distribution of regressed\npolynomial coefﬁcients has a lower degree of freedom\ncompared with the tempo curves representing expressive\ntiming within a phrase hence the regression of expressive\ntiming with polynomial function reduces data dimension.\nThe results demonstrate that regressing expressive timing\nwith polynomial functions may help the clustering process.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 461Figure 2 . The centroids of GMM ocompared with the regressed polynomial curves with the centroids of GMM2\nrand\nGMM7\nrin Chopin’s Mazurka Op.24 No.2.\nWhen comparing the regressed polynomial curves re-\ncovered from the centroids of GMM gwith the centroids\nof GMM o, the two sets of centroids are highly correlated\nwith each other, which demonstrates that the centroids of\nGMM gmake similar musical sense with GMM o. As a re-\nsult, the polynomial functions can be used to help cluster\nexpressive timing, which makes clustering expressive tim-\ning across phrases with various lengths possible.\n7. ACKNOWLEDGEMENT\nAs a study based on my PhD research, we would like to\nthank Dr. Dawn A. A. Black and Prof. Elaine Chew for\ninitial discussions. Chinese Scholarship Council partially\nfounded the initial stage of the research.\n8. REFERENCES\n[1]Kenneth P. Burnham and David R. Anderson. Mod-\nel Selection and Multimodel Inference — A Practical\nInformation-Theoretic Approach . Springer, 2nd edi-\ntion, 2002.\n[2]Emilios Cambouropoulos, Simon Dixon, Werner Goe-\nbl, and Gerhard Widmer. Human preferences for tem-\npo smoothness. In Proceedings of the VII International\nSymposium on Systematic and Comparative Musicolo-\ngy and III International Conference on Cognitive Mu-\nsicology , pages 18–26, 2001.\n[3]Maarten Grachten, Werner Goebl, Sebastian Floss-\nmann, and Gerhard Widmer. Phase-plane represen-\ntation and visualization of gestural structure in ex-\npressive timing. Journal of New Music Research ,\n38(2):183–195, 2009.[4]Shengchen Li, Dawn A. A. Black, Elaine Chew, and\nMark D. Plumbley. Evidence that phrase-level tempo\nvariation may be represented using a limited dictionary.\nInProceedings of International Conference on Music\nPerception and Cognition (ICMPC’14) , 2014.\n[5]Shengchen Li, Dawn A. A. Black, and Mark D.\nPlumbley. The clustering of expressive timing within\na phrase in classical piano performances by Gaussian\nmixture models. Lecture Notes in Computer Science,\nPost Conference Proceeding of International Sympo-\nsium on Computer Music Multidisciplinary Research ,\n9617:322–345, 2016.\n[6]Shengchen Li, Simon Dixon, Dawn A. A. Black, and\nMark D. Plumbley. A model selection test for factors\naffecting the choice of expressive timing clusters for a\nphrase. In Proceedings of 13th Sound and Music Com-\nputing Conference , 2016.\n[7]Kevin P. Murphy. Machine Learning: A Probabilistic\nPerspective . The MIT Press, 2012.\n[8]Bruno H. Repp. A microcosm of musical expression.\nI. Quantitave analysis of pianists’ timing in the initial\nmeasures of Chopin’s Etude in E major. The Journal\nof the Acoustical Society of America , 104:1085–1100,\n1998.\n[9]Bruno H. Repp. A microcosm of musical expression: I-\nI. Quantitative analysis of pianists’ dynamics in the ini-\ntial measures of Chopin’s Etude in E major. The Jour-\nnal of the Acoustical Society of America , 105:1972–\n1988, 1999.462 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[10] Craig Sapp. Hybrid numeric/rank similarity metrics for\nmusical performance analysis. In Proceedings of the\nInternational Conference on Music Information Re-\ntrieval (ISMIR) , pages 501–506, 2008.\n[11] Neta Spiro, Nicolas Gold, and John Rink. The form of\nperformance: Analyzing pattern distribution in select\nrecordings of Chopin’s Mazurka op. 24 no. 2. Musicae\nScientiae , 14(2):23–55, 2010.\n[12] Renee Timmers. V ocal expression in recorded per-\nformances of Schubert songs. Musicae Scientiae ,\n11(2):237–268, 2007.\n[13] Asmir Tobudic and Gerhard Widmer. Playing Mozart\nphrase by phrase. In Proceedings of the 5th Inter-\nnational Conference on Case-based Reasoning (IC-\nCBR’03) , pages 552–566. Springer, 2003.\n[14] Neil P. Mcangus Todd. The dynamics of dynamics: A\nmodel of musical expression. Journal of the Acoustical\nSociety of America , 91:3540–3550, 1992.\n[15] Gerhard Widmer. Discovering simple rules in complex\ndata: A meta-learning algorithm and some surprising\nmusical discoveries. Artiﬁcial Intelligence , 146:129–\n148, 2003.\n[16] Gerhard Widmer and Asmir Tobudic. Playing Mozart\nby analogy: Learning multi-level timing and dynamic-\ns strategies. Journal of New Music Research , 32:259–\n268, 2003.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 463"
    },
    {
        "title": "Video-Based Vibrato Detection and Analysis for Polyphonic String Music.",
        "author": [
            "Bochen Li",
            "Karthik Dinesh",
            "Gaurav Sharma 0001",
            "Zhiyao Duan"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417249",
        "url": "https://doi.org/10.5281/zenodo.1417249",
        "ee": "https://zenodo.org/records/1417249/files/LiDSD17.pdf",
        "abstract": "In music performance, vibrato is an important artistic ef- fect, where slight variations in pitch are introduced to add expressiveness and warmth. Automatic vibrato detection and analysis, although well studied for monophonic mu- sic, has rarely been explored for polyphonic music, be- cause of the challenge in multi-pitch analysis. We propose a video-based approach for detecting and analyzing vibrato in polyphonic string music. Specifically, we capture the fine motion of the left hand of string players through opti- cal flow analysis of video frames. We explore two meth- ods. The first uses a feature extraction and SVM classifica- tion pipeline, and the second is an unsupervised technique based on autocorrelation analysis of the principal motion component. The proposed methods are compared with audio-only methods applied to individual instrument tracks separated from original audio mixture using the score. Ex- periments show that the proposed video-based methods achieve a significantly higher vibrato detection accuracy than the audio-based methods especially in high polyphony cases. Further experiments also demonstrate the utility of the approach in vibrato rate and extent analysis.",
        "zenodo_id": 1417249,
        "dblp_key": "conf/ismir/LiDSD17",
        "keywords": [
            "vibrato",
            "artistic effect",
            "slight variations",
            "pitch",
            "add expressiveness",
            "warmth",
            "polyphonic music",
            "multi-pitch analysis",
            "video-based approach",
            "optical flow analysis"
        ],
        "content": "VIDEO-BASED VIBRATO DETECTION AND ANALYSIS FOR\nPOLYPHONIC STRING MUSIC\nBochen Li Karthik Dinesh Gaurav Sharma Zhiyao Duan\nDept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY , USA\nfbochen.li, kdinesh, gaurav.sharma, zhiyao.duan g@rochester.edu\nABSTRACT\nIn music performance, vibrato is an important artistic ef-\nfect, where slight variations in pitch are introduced to add\nexpressiveness and warmth. Automatic vibrato detection\nand analysis, although well studied for monophonic mu-\nsic, has rarely been explored for polyphonic music, be-\ncause of the challenge in multi-pitch analysis. We propose\na video-based approach for detecting and analyzing vibrato\nin polyphonic string music. Speciﬁcally, we capture the\nﬁne motion of the left hand of string players through opti-\ncal ﬂow analysis of video frames. We explore two meth-\nods. The ﬁrst uses a feature extraction and SVM classiﬁca-\ntion pipeline, and the second is an unsupervised technique\nbased on autocorrelation analysis of the principal motion\ncomponent. The proposed methods are compared with\naudio-only methods applied to individual instrument tracks\nseparated from original audio mixture using the score. Ex-\nperiments show that the proposed video-based methods\nachieve a signiﬁcantly higher vibrato detection accuracy\nthan the audio-based methods especially in high polyphony\ncases. Further experiments also demonstrate the utility of\nthe approach in vibrato rate and extent analysis.\n1. INTRODUCTION\nVibrato is an important artistic effect in musical perfor-\nmance. Instrument players use vibrato to color a tone and\nexpress emotions. Physically, vibrato is generated by pitch\nmodulation of a note in a periodic fashion [23]. Important\ncharacteristics of vibrato include rateandextent of this pe-\nriodic modulation [8]. These characteristics vary signif-\nicantly across instruments, cultures, and personal styles.\nCompared to woodwind and brass instruments, vibrato is\nmore pronounced in strings.\nAutomatic vibrato detection and analysis is an impor-\ntant topic in music information retrieval (MIR) with broad\nimpacts. It is useful in musicological studies to compare\ndifferent articulation styles of different performers and in-\nstruments [2]. It is critical in expressive performance ped-\nagogy for singing [19] and violin [28]. It also facilitates\nc\rBochen Li, Karthik Dinesh, Gaurav Sharma, Zhiyao\nDuan. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Bochen Li, Karthik Dinesh, Gaurav\nSharma, Zhiyao Duan. “Video-based vibrato detection and analysis for\npolyphonic string music”, 18th International Society for Music Informa-\ntion Retrieval Conference, Suzhou, China, 2017.\n20 40 60 80 100 12053.854.054.254.454.6MIDI Number\n20 40 60 80 100 12053.854.054.254.454.6MIDI Number\n20 40 60 80 100 120\nFrame Number-0.3-0.2-0.10.00.10.20.3Pitch\tContour\t\n(Ground -truth)\nPitch\tContour\t\n(Estimated\tfrom\t\nseparated\ttracks)\nMotion\t\nFluctuations\t\nfrom\tLeft\tHand(a)\n(b)\n(c)Figure 1 . The proposed method tackles the challenging\nproblem of vibrato analysis for polyphonic music by ex-\nploiting information from the video to augment audio anal-\nysis. (a) The ground-truth pitch contour of a cello vibrato\nnote in a violin-cello duet performance showing a clear\nvibrato pattern, (b) The estimated pitch contour of this\nnote from the audio mixture using a state-of-the-art score-\ninformed pitch detection method showing corruption due\nto the interference from the other source, (c) The left hand\nmotion along the ﬁngerboard of the cello player extracted\nfrom video analysis is clean and well correlated with the\nground-truth pitch contour. The hand motion proﬁle ex-\ntracted from video is used for vibrato analysis in this paper.\nother MIR tasks such as singing voice extraction [12],\nharmonic-percussive decomposition [21], and audio-visual\nsource association [16]. Vibrato analysis also provides\nthe statistical basis for vibrato synthesis of musical instru-\nments [13], singing voices [11], and bird songs [4], through\nwhich the synthesized sounds are more realistic and ex-\npressive.\nMost of the existing methods for automatic vibrato de-\ntection and analysis are audio-based with a focus on mono-\nphonic sources, where vibrato can be easily characterized\nfrom the pitch trajectory estimated through a monophonic\npitch detection algorithm. Methods include thresholding\nthe pitch drift within each note [3], calculating the median\ndistance of the neighboring peaks/troughs of the pitch con-\ntour [9], analyzing the spectral peak after a Fourier trans-\nform of the pitch contour [25], cross-correlation analysis\nof frequency/amplitude modulation [26], and a nonlinear\nsinusoidal decomposition method [27].\nFew approaches have focused on polyphonic music, and\nwhen they do, they only characterize vibrato of a single\nsource (usually the solo instrument) in the mixture. This123is mainly due to the difﬁculty of reliably estimating simul-\ntaneous pitches in polyphonic music [5]. Abeßer et al. [1]\nproposed a score-informed approach to ﬁrst estimate the\npitch contour of the solo instrument from the audio mix-\nture and then perform vibrato detection and analysis on the\npitch contour through autocorrelation. The performance of\nthis approach, however, depends heavily on the pitch esti-\nmation performance. Spectrogram-based approaches such\nas harmonic partial tracking [12] and template convolu-\ntion [6] reduce the dependency on pitch estimation. How-\never, these operations are still error-prone when harmonics\nof different sources overlap. To our best knowledge, there\nis no existing approach for vibrato detection and analysis\nof multiple simultaneous sources of a polyphonic music\nmixture, such as a string ensemble. Existing polyphonic\naudio analysis techniques are not yet sufﬁcient.\nFigure 1 shows the limitation of audio-based analy-\nsis and motivates the video-based analysis proposed in\nthis paper. In Figure 1 (a), the ground-truth pitch con-\ntour of a cello vibrato note in a violin-cello duet perfor-\nmance is shown. This pitch contour is estimated using\na monophonic pitch detection algorithm [17] on the iso-\nlated (ground truth) signal of the cello note prior to mix-\ning. Vibrato characteristics are clearly observable in this\npitch contour. Figure 1 (b) shows the estimated pitch\ncontour of this cello note obtained from a state-of-the-art\nscore-informed source separation and pitch estimation al-\ngorithm [7]. Due to the interference from the violin, the es-\ntimated pitch contour is corrupted and the vibrato patterns\nare obscured, especially toward the later time instants rep-\nresented on the right side of the plot. Note that this exam-\nple is just a duet of instruments with distinct pitch ranges.\nFor music with higher polyphony using instruments with\nsimilar pitch ranges, the estimated pitch contours are fur-\nther corrupted, making audio-based vibrato detection and\nanalysis unsatisfactory.\nFor some instruments such as strings, vibrato is often\nvisible from the left hand motion, and this visual infor-\nmation does not degrade as audio information does when\npolyphony increases. This motivates our proposed ap-\nproach of vibrato detection and analysis through video-\nbased analysis of the ﬁne motion of the left hand. Figure\n1 (c) shows the left hand rolling motion along the prin-\ncipal motion direction (i.e., the ﬁngerboard) of the cello\nplayer playing the note. We can see that this motion curve\nis smooth and it aligns with the ground-truth pitch contour\nin Figure 1 (a) very well.\nThe overview of our proposed approach is illustrated in\nFigure 2. This approach integrates audio, visual and score\ninformation, and assumes that the players in the video are\nwell associated with score tracks. Our previous work has\naddressed the association problem accurately [14]. For\neach string player, we track the left hand, and then esti-\nmate optical ﬂow motion vectors at the pixel level around\nthe left hand. We use audio-score alignment to identify the\nonset and offset of each note, and perform vibrato detec-\ntion and analysis on each note from the motion vectors. We\ndevelop two approaches for vibrato detection. One uses\nMotion \nFeatures\nScore\t\nAlignment\nAudioMIDI\nNote \nOnset/Offset\nFine\tMotion\t\nEstimation\nHand\t\nTracking Vibrato \nRate/Extent\nTrack\t\nAssociation\nSource\t\nSeparationVideo\nVibrato\t\nDetection\nVibrato\t\nAnalysis\nPitch \nContour\nPitch\t\nEstimationFigure 2 . System overview of the proposed video-based\nvibrato detection and analysis framework.\na Support Vector Machine (SVM) to classify motion fea-\ntures extracted from the pixel-level motion vectors, and the\nother is based on autocorrelation analysis of the left hand\nmotion along the principal direction (i.e., ﬁngerboard). We\nfurther propose a framework to analyze vibrato character-\nistics: rate and extent. The vibrato rate is estimated from\nthe period of the hand motion curve, and the vibrato extent\nis estimated from the amplitude of the motion curve after it\nis scaled to match the estimated noisy pitch contour from\nscore-informed audio analysis.\nExperiments are carried on 19 pieces of polyphonic\nstring music from an audio-visual music performance\ndataset, and the proposed video-based approach is com-\npared with two audio-based baseline methods for vibrato\ndetection. Results show a signiﬁcant improvement for\nvideo-based vibrato detection over the audio-based meth-\nods. Further analysis reveals that video-based vibrato de-\ntection is robust irrespective of polyphony and instrument\ntypes. We further show that the video-based approach is\nable to estimate the vibrato rate and extent with a deviation\nfrom the ground-truth smaller than 1 Hz and 10 musical\ncents for 90% of the notes, respectively.\n2. AUDIO-BASED METHOD\nIn this section, we introduce an audio-based framework\nto detect vibrato in polyphonic music to serve as a base-\nline method. Vibrato can be detected from the pitch con-\ntour of each source using either autocorrelation or Fourier\ntransform. However, estimating the pitch contour of each\nsource from the audio mixture is challenging. Inspired\nby [1], score information can be utilized to alleviate the\ndifﬁculty of pitch estimation and its assignment to sources.\n2.1 Score-informed Pitch Estimation\nTo utilize the score information for pitch estimation of\neach source, robust audio-score alignment is required to\nguarantee the temporal synchronization between the score\nevents and audio articulations. We apply the Dynamic\nTime Warping (DTW) framework with chroma feature to\nrepresent audio and score, as described in [14]. Then the\naudio mixture is separated using harmonic masking as de-\nscribed in [7]: Pitches of each source are ﬁrst estimated\nwithin two semitones around the quantized score-notated124 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Audio MIDI\n5678910116364656667686970717273 Pitch\n(MIDI\t\nnumber)\n5678910114546474849505152535455\nTime (sec)\nInputFigure 3 . Audio-based vibrato detection. Detected vibrato\nnotes are marked with green rectangles in the pitch trajec-\ntories estimated by score-informed pitch estimation.\npitches; Sound sources are then separated by harmonic\nmasking of the pitches in each frame, where the soft masks\ntake into account the harmonic indexes when distributing\nthe mixture signal’s energy to overlapping harmonics.\nWe then re-estimate the pitch contour of each source\nfrom its separated signal for vibrato analysis. We again ap-\nply the above-mentioned score-informed pitch reﬁnement\nstep to further reduce interference from other sources. The\noutput pitch contour is segmented into notes using the on-\nset/offset information provided by the aligned score. Note\nthat although we can reﬁne the pitches directly from the au-\ndio mixture without source separation, it is reported in [16]\nthat the result is more robust on the separated sources. Be-\nsides, the availability of separated audio sources is advan-\ntageous for other vibrato detection methods that do not rely\non pitch contours.\n2.2 Vibrato Detection from the Pitch Contour\nAfter obtaining the pitch contour, vibrato detection can be\nachieved by analyzing the periodic pattern for each note.\nThe pitch contour is analyzed in the MIDI scale, and its DC\ncomponent is removed by subtracting the average value\nover the contour. Then we implement two methods to de-\ntect the ﬂuctuation rate of the pitch contour: autocorrela-\ntion [1] and spectral analysis [25]. For the autocorrelation\nmethod, prominent peaks are detected from the autocorre-\nlation function, and the median value of all the neighboring\npeak distance is used to calculate the ﬂuctuation rate. If the\nrate is within the range of 3-9 Hz (considering a typical vi-\nbrato rate range of [4, 7.5] Hz for strings [10]), the note is\ndetected as vibrato. For the spectral analysis method, we\nﬁrst calculate the magnitude spectrum of the pitch contour\nof a note through Fourier transform. We then check if the\nfrequency of the maximum peak lies in the rang of 3-9 Hz.\nQuadratic interpolation is applied in both methods to get a\nmore precise peak location estimation.\nThe audio-based methods are simple, yet sufﬁcient to\ndetect vibrato in the score-informed fashion. Figure 3 re-\nviews this process and illustrated the detected vibrato notes\nin green boxes. This approach achieves high detection ac-\ncuracy in low polyphony settings, but the performance de-\ngrades rapidly with increasing polyphony.3. PROPOSED METHOD\nMotivated by the fact that the motion features from the\nvideo are correlated with the pitch ﬂuctuations, we propose\na video-based vibrato detection and analysis framework.\nA string instrument player exhibits three kinds of motions:\nbowing motion to articulate notes, ﬁngering motion to con-\ntrol pitches, and the whole body motion to express musical\nintentions. Fine periodic ﬁngering motion on the left hand\nalong the ﬁngerboard which changes the length and tension\nof the string results in vibrato articulations. In this section,\nwe will present the method to extract this ﬁne motion for\nvibrato detection and analysis.\n3.1 Motion Capture\ny\nx\n-1.0\t\t\t\t\t\t\t\t\t\t -0.5\t\t\t\t\t\t\t\t\t\t\t0 0.5\t\t\t\t\t\t\t\t\t\t\t1.01.0\n0.5\n0\n-0.5\n-1.0\nFigure 4 . Motion capture results from left hand tracking\n(left), color encoded pixel velocities (middle), and scatter\nplot of frame-wise reﬁned motion velocities (right).\nThe ﬁrst step is to detect and track the left hand for\neach player, where the vibrato motions come from. The\nhand tracking is based on the Kanade-Lucas-Tomasi (KLT)\ntracker [24] and implemented using the same parameters\nas presented in [16]. The KLT tracker results in a dy-\nnamic region of tracked hand location where we apply the\noptical ﬂow estimation [22] to obtain the raw motion ve-\nlocities for each pixel in xandydirections within that\nregion. The motion velocities are spatially averaged as\nu(t) = [ux(t);uy(t)], whereuxanduyrepresents the\nmean motion velocities in xandydirections respectively,\nandtis the time index. Notice that the motion velocities in\nthe hand region contain not only the player’s ﬁne motion\ncorresponding to vibrato playing, but also his/her large-\nscale body motions during the performance. In order to\neliminate the body movement and obtain a reﬁned motion\nvelocities for vibrato observation, we subtract a moving\naverage of the signal u(t)from itself, to obtain\nv(t) =u(t)\u0000\u0016u(t); (1)\nwhere \u0016u(t)is the moving average of u(t)over a 10 frame\nwindow. Figure 4 illustrates the original video frame with\nthe tracked hand position, the raw motion velocities from\noptical ﬂow estimation, and the reﬁned motion velocities\nv(t)across all the frames.\n3.2 Vibrato Detection from Motion Features\nThe proposed vibrato detection methods are score in-\nformed, where the note onset/offset information from the\nscore is utilized to temporally segment the reﬁned mean\nmotion velocities into vi(t), whereiis the note index.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 125To achieve this, each score track needs to be temporally\naligned with the video frames, and spatially associated\nwith the players. The ﬁrst issue is resolved using audio-\nscore alignment, assuming video and audio frames are nat-\nurally synchronized. The second issue is addressed as\nin [14], where player locations are segmented and asso-\nciated with the score tracks by correlating the bow mo-\ntions with note events. By utilizing the mean motion ve-\nlocities and the extracted features, we introduce two meth-\nods for vibrato detection. The ﬁrst method is based on a\nSVM framework, where each vi(t)is classiﬁed as vibrato\nor non-vibrato. The second method is analogous to the\naudio-based technique, where we perform auto-correlation\non the extracted 1-D motion curve along the ﬁngerboard\nafter principal component analysis.\n3.2.1 SVM\nWe train a Support Vector Machine (SVM) as a classiﬁ-\ncation framework for vibrato/non-vibrato detection. We\nutilize the reﬁned motion velocity segments vi(t) =\n[vi\nx(t);vi\ny(t)]obtained from the procedure explained in\nSection 3.1. From each vi(t), we have velocity compo-\nnents inxandydirections from which 8 dimensional fea-\ntures are extracted. The features are\n(a)Zero crossing rate (4-D): Vibrato has inherent peri-\nodicity when compared to non-vibrato regions. Hence we\nutilize the zero crossing rate, which is the ratio of total zero\ncrossings to total frame length for vi\nx(t),vi\ny(t)and their\nauto-correlations, respectively.\n(b)Frequency (2-D): Vibrato has a typical frequency in\nthe range of 3-9 Hz. Hence we calculate the sum of the ab-\nsolute value of Fourier coefﬁcients in the 3-9 Hz frequency\nrange forvi\nx(t)andvi\ny(t).\n(c)Auto-correlation peaks (2-D): Auto-correlation of\nvi\nx(t)andvi\ny(t)is calculated within a ﬁxed lag of 10 video\nframes, where total number of local maximum values is\nutilized as one of the features.\nThe SVM is trained on tracks which are distinct from\nthe test set using the leave-one-out training strategy. The\nground truth vibrato/non-vibrato labels are obtained from\nground-truth audio tracks and associated with the corre-\nsponding player. For the SVM training algorithm we set\nthe kernel function and scale parameters as radial basis\nfunction and automatic scaling, respectively.\n3.2.2 PCA\nWe also propose an unsupervised framework for vibrato\ndetection. From Figure 4, we ﬁnd that the distribution\nof the reﬁned motion velocities for vibrato motions are\nalong the ﬁngerboard. So we perform Principal Compo-\nnent Analysis (PCA) on v(t)across all frames to identify\nthis principal motion direction, and project the motion ve-\nlocity vectors to this principal direction to obtain a 1-D\nmotion velocity curve V(t)as\nV(t) =v(t)T~ v\nk~ vk; (2)\nwhere ~ vis the eigenvector corresponding to the largest\neigenvalue of the PCA of v(t). We then perform an inte-gration of the motion velocity curve over time to calculate\namotion displacement curve as\nX(t) =Zt\n0V(\u001c)d\u001c: (3)\nThis displacement curve corresponds to the ﬂuctuation of\nthe vibrating length of the string and hence the pitch ﬂuctu-\nation of the note. Figure 1 (c) shows the motion displace-\nment curve for one vibrato note, which is matched with\nthe ground-truth pitch contour. Similar to the audio-based\napproach, vibrato is detected through peak picking on the\nautocorrelation function of the motion displacement curve.\nNote that different thresholds on the peak picking will af-\nfect the sensitivity of the vibrato detection, and we use the\nuniform threshold for all the notes which yields the best\noverall results.\n3.3 Vibrato Analysis\nThe video-based method also enables new techniques for\nanalyzing the vibrato features, i.e., vibrato rate and vibrato\nextent, which describe the speed and the amount by which\nthe pitch is varied. Here extent is deﬁned as the dynamic\nrange of the pitch contour, i.e., the peak-trough difference.\nVibrato rate can be directly extracted from video by ob-\nserving how fast the left hand is rolling along the ﬁnger-\nboard. Again this is solved by analyzing the autocorrela-\ntion on the motion displacement curve X(t). Quadratic\ninterpolation is required for peak picking due to the low\nframe rate of videos. Vibrato extent, however, cannot be\nestimated by capturing the motion extent, which varies\nupon different camera distance and angles. Besides, to\ngenerate the same vibrato extent, the extent of motion also\ndepends on the vibrato articulation style, the hand position\non the ﬁngerboard, and the instrument type. Therefore, we\ncombine the audio analysis together with the extracted mo-\ntion displacement curve for vibrato extent estimation.\nWe ﬁrst estimate the vibration extent of the motion dis-\nplacement curve as ^weby calculating the median of the\ndistance between all the peaks and troughs within each\nnote. We then scale the displacement curve to ﬁt the pitch\ncontour, and the vibrato extent can be calculated from the\nscaling factor. Speciﬁcally, assuming F(t)is the estimated\npitch contour (in MIDI number) of the detected vibrato\nnote from audio analysis after subtracting the DC compo-\nnent of itself, the vibrato extent ve(in musical cents) is\nestimated as ^veas:\n^ve= arg min\nvetoffX\nt=ton\f\f\f\f100\u0001F(t)\u0000veX(t)\n^we\f\f\f\f2\n: (4)\nwhere 100\u0001F(t)is the pitch contour measured in musi-\ncal cents;X(t)\n^weis the normalized hand displacement curve.\nSinceX(t)is calculated from the video modality, temporal\ninterpolation is applied beforehand to guarantee the same\nframe rate as the audio, i.e., the hop size for Short-Time\nFourier Transform. Note that temporal shift may be ap-\nplied toX(t)to maximize the cross correlation between\nX(t)andF(t)to compensate the slight asynchrony be-\ntween the two modalities (usually within 20ms).126 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20174. EXPERIMENTS\n4.1 Dataset and Evaluation Measures\nThe vibrato detection and analysis system is tested on the\nURMP dataset [15]. The dataset contains individually\nrecorded audio-visual tracks of various instruments, which\nare synchronized and assembled to form 44 classical en-\nsemble pieces ranging from duets to quintets. Ground-\ntruth audio tracks and pitch/note annotations are provided\nin the dataset. The ground-truth annotation of the vibrato\nrate/extent is acquired by the autocorrelation method as\ndescribed in Section 2.2 on ground-truth individual audio\ntracks, and the presence of vibrato is manually examined.\nFor our experiments, we use the 19 ensemble pieces that\ncontains at most one non-string instrument, including 5\nduets, 4 trios, 7 quartets, and 3 quintets. Audio is sam-\npled at 48 KHz, and processed with a frame length of 42.7\nms and a hop size of 10 ms for the STFT. Video resolution\nis 1080P, and the frame rate is 29.97 frames per second.\nIn the experiments, we evaluate the two proposed video-\nbased methods, i.e., the classiﬁcation method using SVM\nframework (Vid-SVM) and autocorrelation analysis on the\nprincipal motion component (Vid-PCA). Two audio-based\nmethods described in Section 2.2 are also compared as\nbaseline methods, i.e., peak-picking of the autocorrela-\ntion (Aud-AC), and Fourier transform of the pitch contour\n(Aud-FT). Since the vibrato detection can be viewed as a\nretrieval task, we compute the note-level precision (P), re-\ncall (R), and F-measure (F) using the number of true posi-\ntives, false positives and false negatives on each track. For\nthe two audio-based methods and the Vid-PCA method, we\nadjust the peak-picking threshold for a balanced value of\nprecision and recall and ﬁx it for all the tracks. For vibrato\nrate and extent estimation, we calculate the error between\nthe estimated and ground-truth values on the true positive\ndetections from the Vid-PCA method.\n4.2 Results\n4.2.1 Overall Evaluation on Vibrato Detection\nWe ﬁrst evaluate the vibrato detection results using preci-\nsion, recall and F-measure for the four methods on all of\nthe 57 tracks from the 19 pieces excluding non-string in-\nstrument ones, as plotted in Figure 5. Each bar is the aver-\nage of the 57 tracks. We ﬁnd that in polyphonic music,\nboth audio-based methods achieve limited performance;\nlower than 75% for the F-measure. Video-based meth-\nods can get a pronounced improvement on the F-measure,\nwhich is as high as 90%. The supervised classiﬁcation\nmethod based on SVM further outperforms the unsuper-\nvised method, because of the richer features.\n4.2.2 Vibrato Detection Evaluation on Different Cases\nWe further investigate how the vibrato detection per-\nformance changes along with polyphony and instrument\ntypes. Figure 6 illustrates the scatter plot of the vibrato\ndetection F-measure for the four methods (with different\ncolors) in four different polyphony levels corresponding\nto duets, trios, quartets, and quintets. Each sample point\nrepresents the evaluation on one track, and the average\n73.1 73.3 92.0 89.0\nAud-AC Aud-FT Vid-SVM Vid-PCA6065707580859095Accuracy (%)Precision\nRecall\nF-measureFigure 5 . Overall vibrato detection results showing the\nprecision, recall, and F-measure (shown on top) accuracies\nfor 2 audio-based methods and 2 video-based methods.\nvalue in each subset is marked as the red line. We see that\nthe two audio-based methods can reach performance com-\nparable with the video-based methods in low-polyphony\npieces, but their performance drops when polyphony in-\ncreases. This is because of the decreased quality of the\npitch contour that is extracted from high-polyphony audio.\nHowever, polyphony does not affect the vibrato detection\nperformance for the two video-based methods, since the\nleft hands are always directly observable from visual scene\nin this dataset. Note that there are several extremely low\nF-measure values for video-based methods. These come\nfrom tracks with plucking-vibrato articulations, where the\nvibrato is captured from hand motion but is not annotated\nin the ground truth as its duration and extent are different\nfrom the bowing-vibrato articulations.\n2345Aud-ACAud-FTVid-SVMVid-PCAPolyphony\tNumberF-measure\t(%)20 30 40 50 60 70 80 90 100\nFigure 6 . Vibrato detection performance decreases as\npolyphony increases for audio-based methods, while it\nstays the same for video-based methods.\nFigure 7 further reveals how the vibrato detection re-\nsults vary for different instruments: violin, viola, cello,\nand double bass. Again, the audio-based methods are sen-\nsitive to instrument types while video-based methods are\nnot. The reason is that the separated track of the low-\npitch instrument (such as double bass) is likely to get con-\ntaminated by other higher-pitch voices using the harmonic\nmask method for source separation. In contrast, the vibrato\nmotions for the four different instruments have similar pat-\nterns, thus easy to capture by our proposed methods.\n4.2.3 Evaluation of Vibrato Characteristics\nDue to the unsatisfactory performance of audio-based vi-\nbrato detection, we evaluate the accuracy of vibrato rateProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 127F-measure\t(%)VnVaVcDbAud-ACAud-FTVid-SVMVid-PCAInstrument\tType20 30 40 50 60 70 80 90 100Figure 7 . Vibrato detection performance decreases when\nthe fundamental frequency decreases for audio-based\nmethods, while it stays the same for video-based methods.\n0.00.51.01.52.02.53.0HzVibrato Rate Error\n0.02.55.07.510.012.515.017.520.0CentsVibrato Extent Error\nFigure 8 . Distribution of vibrato rate and extent estimation\nerror on all notes of all tracks.\nand extent estimation only based on the video modality.\nWe conduct this analysis on the true positive detections\nfrom the Vid-PCA method, totaling 2290 vibrato notes\nfrom the 57 tracks. We calculate the absolute deviation\nof the estimated value from the ground-truth value for all\nthe notes, and get an average vibrato rate estimation er-\nror of 0.38 Hz and median of 0.23 Hz. For vibrato extent,\nwe have an average estimation error of 3.47 cents and a\nmedian of 2.29 cents. Figure 8 plots the vibrato rate and\nextent error distribution for all the notes. We ﬁnd that for\n90% of the vibrato notes, the proposed approach estimates\nthe vibrato rate and extent within an error of 1 Hz and 10\ncents, respectively.\nVn Va Vc Db3.03.54.04.55.05.56.06.57.07.58.0Vibrato Rate (Hz)\nVn Va Vc Db051015202530354045505560Vibrato Extent (Cents)\nFigure 9 . Distributions of vibrato rate and extent for dif-\nferent instruments.\nIn order to further demonstrate the potential applica-\ntions of our approach in musicology studies, we analyze\nhow the vibrato rate and extent vary on different instru-\nments and players in this dataset. Figure 9 plots the distri-\nP1 P2 P3 P43.03.54.04.55.05.56.06.57.07.58.0Vibrato Rate (Hz)\nP1 P2 P3 P4051015202530354045505560Vibrato Extent (Cents)Figure 10 . Distributions of vibrato rate and extent of four\ndifferent violin players.\nbutions of rate and extent for the four string instruments,\nwhere each sample point represents one track. Similar vi-\nbrato rate and extent can be observed for violin and vi-\nola whereas, in contrast, we observe a signiﬁcant drop for\nthe double bass, where a slower rate and subtler extent\nis inferred. This was explained in [18]; to produce audi-\nble pitch ﬂuctuations on the thicker and longer strings on\ndouble bass requires more effort to overcome the strength,\nﬂexibility, and coordination than other string instruments.\nThus vibrato rates of double bass players (4-5 Hz [20]) are\ntypically slower than other string instrumentalists.\nWe also analyze the vibrato patterns of the four different\nviolinists among the 31 violin tracks, as plotted in Figure\n10. Vibrato rate is more dispersed among players than vi-\nbrato extent, and both rate and extent show a similar trend\namong the players. For example, the second player ex-\nhibits a slower vibrato rate with a subtler vibrato extent,\nwhile the forth player exhibits a faster vibrato rate with a\npronounced vibrato extent. This may be because of differ-\nent players’ articulation styles, or different characteristics\nof the pieces. Detailed discussion is not included in this pa-\nper, but our proposed system can provide a powerful tool\nfor further analyses on the musicology side.\n5. CONCLUSION\nWe proposed a video-based vibrato detection and analy-\nsis framework for polyphonic string music. Speciﬁcally,\nwe developed two methods that utilize the motion features\nfrom the video for vibrato detection based on the observed\ncorrelation between the motion vibrations and the vibrato\npitch ﬂuctuations. We also extended the framework to\nestimate the vibrato rate and extent. Experiments show\nthat the proposed method is successful and offers much\nbetter performance than audio-based methods, particularly\non pieces with high polyphony, where the strong interfer-\nence between sources severely degrades the performance\nof audio-based methods. In future work, it would be help-\nful to develop a non-score-informed framework for vibrato\ndetection and analysis.\n6. ACKNOWLEDGMENT\nWe thank the Center for Integrated Research Computing\n(CIRC), University of Rochester for providing computa-\ntional resources for the project.128 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20177. REFERENCES\n[1] Jakob Abeßer, Estefan ´ıa Cano, Klaus Frieler, Martin\nPﬂeiderer, and Wolf-Georg Zaddach. Score-informed\nanalysis of intonation and pitch modulation in jazz so-\nlos. In Proc. Intl. Society for Music Information Re-\ntrieval (ISMIR) , pages 823–829, 2015.\n[2] Jakob Abeßer, Klaus Frieler, Estefan ´ıa Cano, Martin\nPﬂeiderer, and Wolf-Georg Zaddach. Score-informed\nanalysis of tuning, intonation, pitch modulation, and\ndynamics in jazz solos. IEEE/ACM Trans. Audio,\nSpeech, and Language Process. , 25(1):168–177, 2017.\n[3] Isabel Barbancho, Cristina de la Bandera, Ana M\nBarbancho, and Lorenzo J Tardon. Transcription and\nexpressiveness detection system for violin music. In\nProc. IEEE Intl. Conf. Acoustics, Speech, and Signal\nProcess. (ICASSP) , pages 189–192. IEEE, 2009.\n[4] Jordi Bonada, Robert Lachlan, and Merlijn Blaauw.\nBird song synthesis based on hidden markov models.\nInProc. InterSpeech , volume 2016, 2016.\n[5] Karthik Dinesh, Bochen Li, Xinzhao Liu, Zhiyao\nDuan, and Gaurav Sharma. Visually informed multi-\npitch analysis of string ensembles. In Proc. IEEE Intl.\nConf. Acoust., Speech, and Signal Process. (ICASSP) ,\n2017.\n[6] Jonathan Driedger, Stefan Balke, Sebastian Ewert, and\nMeinard M ¨uller. Template-based vibrato analysis of\nmusic signals. In Proc. Intl. Society for Music Infor-\nmation Retrieval (ISMIR) , 2016.\n[7] Zhiyao Duan and Bryan Pardo. Soundprism: An online\nsystem for score-informed source separation of music\naudio. IEEE J. Sel. Topics Signal Process. , 5(6):1205–\n1215, 2011.\n[8] Harvey Fletcher and Larry C Sanders. Quality of violin\nvibrato tones. The Journal of the Acoustical Society of\nAmerica , 41(6):1534–1544, 1967.\n[9] Anders Friberg, Erwin Schoonderwaldt, and Patrik N\nJuslin. Cuex: An algorithm for automatic extraction of\nexpressive tone parameters in music performance from\nacoustic signals. Acta acustica united with acustica ,\n93(3):411–420, 2007.\n[10] John M Geringer, Rebecca B MacLeod, and Michael L\nAllen. Perceived pitch of violin and cello vibrato tones\namong music majors. Journal of Research in Music Ed-\nucation , 57(4):351–363, 2010.\n[11] Hung-Yan Gu and Zheng-Fu Lin. Singing-voice syn-\nthesis using ann vibrato-parameter models. J. Inf. Sci.\nEng., 30(2):425–442, 2014.\n[12] Chao-Ling Hsu and Jyh-Shing Roger Jang. Singing\npitch extraction by voice vibrato/tremolo estimation\nand instrument partial deletion. In Proc. Intl. Society\nfor Music Information Retrieval (ISMIR) , pages 525–\n530, 2010.[13] Hanna J ¨arvel ¨ainen. Perception-based control of vibrato\nparameters in string instrument synthesis. In Proc.\nInternational Computer Music Conference (ICMC) ,\n2002.\n[14] Bochen Li, Karthik Dinesh, Zhiyao Duan, and Gaurav\nSharma. See and listen: Score-informed association of\nsound tracks to players in chamber music performance\nvideos. In Proc. IEEE Intl. Conf. Acoust., Speech, and\nSignal Process. (ICASSP) , 2017.\n[15] Bochen Li, Xinzhao Liu, Karthik Dinesh, Zhiyao\nDuan, and Gaurav Sharma. Creating a classical mu-\nsical performance dataset for multimodal music anal-\nysis: Challenges, insights, and applications. IEEE\nTrans. Multimedia . submitted. Available: https://\narxiv.org/abs/1612.08727 .\n[16] Bochen Li, Chenliang Xu, and Zhiyao Duan. Audio-\nvisual source association for string ensembles through\nmulti-modal vibrato analysis. In Proc. Sound and Mu-\nsic Computing (SMC) , 2017.\n[17] Matthias Mauch and Simon Dixon. PYIN: a funda-\nmental frequency estimator using probabilistic thresh-\nold distributions. In Proc. IEEE Intl. Conf. Acoustics,\nSpeech, and Signal Process. (ICASSP) , pages 659–663.\nIEEE, 2014.\n[18] James Paul Mick. An analysis of double bass vi-\nbrato: Rates, widths, and pitches as inﬂuenced by\npitch height, ﬁngers used, and tempo . PhD thesis, The\nFlorida State University, 2012.\n[19] Tomoyasu Nakano, Masataka Goto, and Yuzuru Hi-\nraga. An automatic singing skill evaluation method for\nunknown melodies using pitch interval accuracy and\nvibrato features. In Proc. InterSpeech , 2006.\n[20] George Papich and Edward Rainbow. A pilot study of\nperformance practices of twentieth-century musicians.\nJournal of Research in Music Education , 22(1):24–34,\n1974.\n[21] Jeongsoo Park and Kyogu Lee. Harmonic-percussive\nsource separation using harmonicity and sparsity con-\nstraints. In Proc. Intl. Society for Music Information\nRetrieval (ISMIR) , pages 148–154, 2015.\n[22] Deqing Sun, Stefan Roth, and Michael J Black. Secrets\nof optical ﬂow estimation and their principles. In Proc.\nIEEE Conf. Computer Vision and Pattern Recognition\n(CVPR) , 2010.\n[23] Johan Sundberg. Acoustic and psychoacoustic aspects\nof vocal vibrato. STL-QPSR , pages 35–62, 1995.\n[24] Carlo Tomasi and Takeo Kanade. Detection and track-\ning of point features. Technical Report CMU-CS-91-\n132, School of Computer Science, Carnegie Mellon\nUniversity, Apr. 1991.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 129[25] Jos ´e Ventura, Ricardo Sousa, and Anibal Ferreira.\nAccurate analysis and visual feedback of vibrato in\nsinging. In Proc. Intl. Symposium on Communications\nControl and Signal Process. (ISCCSP) , pages 1–6.\nIEEE, 2012.\n[26] Henrik V on Coler and Axel Roebel. Vibrato detection\nusing cross correlation between temporal energy and\nfundamental frequency. In Proc. 131st Audio Engineer-\ning Society Convention , 2011.\n[27] Luwei Yang, Khalid Z Rajab, and Elaine Chew. The\nﬁlter diagonalisation method for music signal analysis:\nframe-wise vibrato detection and estimation. Journal\nof Mathematics and Music , pages 1–19, 2017.\n[28] Jun Yin, Ye Wang, and David Hsu. Digital violin tu-\ntor: an integrated system for beginning violin learners.\nInProc. ACM Intl. Conf. Multimedia , pages 976–985.\nACM, 2005.130 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Automatic Stylistic Composition of Bach Chorales with Deep LSTM.",
        "author": [
            "Feynman T. Liang",
            "Mark Gotham",
            "Matthew Johnson 0003",
            "Jamie Shotton"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416208",
        "url": "https://doi.org/10.5281/zenodo.1416208",
        "ee": "https://zenodo.org/records/1416208/files/LiangG0S17.pdf",
        "abstract": "This paper presents “BachBot”: an end-to-end automatic composition system for composing and completing mu- sic in the style of Bach’s chorales using a deep long short-term memory (LSTM) generative model. We pro- pose a new sequential encoding scheme for polyphonic music and a model for both composition and harmoniza- tion which can be efficiently sampled without expensive Markov Chain Monte Carlo (MCMC). Analysis of the trained model provides evidence of neurons specializing without prior knowledge or explicit supervision to detect common music-theoretic concepts such as tonics, chords, and cadences. To assess BachBot’s success, we conducted one of the largest musical discrimination tests on 2336 par- ticipants. Among the results, the proportion of responses correctly differentiating BachBot from Bach was only 1% better than random guessing.",
        "zenodo_id": 1416208,
        "dblp_key": "conf/ismir/LiangG0S17",
        "keywords": [
            "BachBot",
            "end-to-end automatic composition",
            "Bach’s chorales",
            "deep long short-term memory (LSTM)",
            "generative model",
            "polyphonic music",
            "sequential encoding scheme",
            "composition and harmonization",
            "efficient sampling",
            "Markov Chain Monte Carlo (MCMC)"
        ],
        "content": "AUTOMATIC STYLISTIC COMPOSITION OF BACH CHORALES WITH\nDEEP LSTM\nFeynman Liang\nDepartment of Engineering\nUniversity of Cambridge\nfl350@cam.ac.ukMark Gotham\nFaculty of Music\nUniversity of Cambridge\nmrhg2@cam.ac.ukMatthew Johnson\nMicrosoftJamie Shotton\nMicrosoft\nABSTRACT\nThis paper presents “BachBot”: an end-to-end automatic\ncomposition system for composing and completing mu-\nsic in the style of Bach’s chorales using a deep long\nshort-term memory (LSTM) generative model. We pro-\npose a new sequential encoding scheme for polyphonic\nmusic and a model for both composition and harmoniza-\ntion which can be efﬁciently sampled without expensive\nMarkov Chain Monte Carlo (MCMC). Analysis of the\ntrained model provides evidence of neurons specializing\nwithout prior knowledge or explicit supervision to detect\ncommon music-theoretic concepts such as tonics, chords,\nand cadences. To assess BachBot’s success, we conducted\none of the largest musical discrimination tests on 2336 par-\nticipants. Among the results, the proportion of responses\ncorrectly differentiating BachBot from Bach was only 1%\nbetter than random guessing.\n1. INTRODUCTION\nRecent advances have enabled computational modeling to\nprovide novel insights into a range of musical phenomena.\nOne use case is automatic stylistic composition : the algo-\nrithmic generation of music in a style similar to a particular\ncomposer or repertoire. This study explores that goal, re-\nstricting its attention to generative probabilistic sequence\nmodels which are learned from data . This model is desir-\nable because it can be applied to a variety of tasks, includ-\ning: harmonizing a melody (by conditioning the model on\nthe melody) and automatic composition (by sampling a se-\nquence from the model).\nThe aim is to build a system capable of generating mu-\nsic in the style of Bach chorales such that an average lis-\ntener cannot distinguish it from original Bach . While the\nmethod we develop is capable of modeling any multi-part\nmusic, we limit the scope of this work to Bach’s chorales\nbecause: they provide a relatively large corpus, by a single\ncomposer, are well understood by music theorists, and are\nroutinely used in the teaching of music theory.\nc\rFeynman Liang, Mark Gotham, Matthew Johnson,\nJamie Shotton. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Feynman Liang, Mark\nGotham, Matthew Johnson, Jamie Shotton. “Automatic stylistic compo-\nsition of Bach chorales with deep LSTM”, 18th International Society for\nMusic Information Retrieval Conference, Suzhou, China, 2017.1.1 Related Work\nTwo well-known difﬁculties in automatic composition are\n1) learning the long-term dependencies required for plau-\nsible phrasing structure and motif distribution [31], and 2)\nevaluating the model’s performance rigorously [34]. Ad-\ndressing the ﬁrst difﬁculty, more recent work has reported\nimprovements in learning long-term dependencies by us-\ning LSTM [14, 13, 18]. Eck and Schmidhuber [14] used\nLSTM to model blues music and found that LSTM can in-\ndeed learn long-term aspects of musical structure such as\nrepeated motifs without explicit modelling.\nEvaluating model performance has proven to be more\nproblematic. In recent work, researchers have begun con-\nducting larger-scale human evaluations. Quick [35] evalu-\nated her rule-based system’s outputs on 237 human partici-\npants from Amazon’s MTurk. Perhaps most relevant to the\npresent study is Collins et al. [6]: a Markov chain expert\nsystem for automatic composition. The authors evaluated\non25participants with a mean of 8:56years of formal mu-\nsic training and found that only 20% of participants (5 out\nof 25) performed signiﬁcantly better than chance. While\nthese prior results are strong, both of these systems relied\nupon a large amount of expert domain knowledge encoded\ninto the models. In contrast, BachBot leverages minimal\nprior knowledge and is evaluated on a signiﬁcantly larger\nparticipant pool.\nBach chorales have been a popular corpus for previous\nwork on automatic composition. Early deterministic sys-\ntems included rule-based symbolic methods [7, 8, 12, 36],\ngrammatical inference [9], and constraint logic program-\nming [39]. Probabilistic models learned from data include\nthe effective Boltzmann machine [3] as well as various\nconnectionist models [37, 38, 24, 31, 15, 27].\nAllan and Williams [1] used hidden Markov models\nto generate Bach chorale harmonizations and is one of\nthe ﬁrst studies to evaluate model performance quantita-\ntively using cross-entropy on held-out data. They intro-\nduce the JSB Chorales dataset which has since become\na standard benchmark routinely used to evaluate the per-\nformance of generative models on polyphonic music mod-\nelling [4, 33, 2, 21, 41]. However, JSB Chorales quan-\ntizes time to eighth notes, distorting 2816 notes ( 2:85% of\nthe corpus). In contrast, BachBot eliminates this problem\nwith 2\u0002the time resolution (distorting no notes). Unfor-\ntunately, the higher resolution time quantization of Bach-449Bot’s data as well as BachBot’s sequential encoding format\nmake direct comparison of cross-entropies against studies\nusing this dataset difﬁcult. On this dataset, the current\nstate-of-the-art (as measured by cross-entropy validation\nloss) by Goel and V ohra [20] uses a deep belief network\n(DBN) which uses a LSTM to propagate temporal dynam-\nics. While BachBot also utilizes a LSTM for capturing\nlong range dependencies, BachBot uses a softmax distri-\nbution rather than a DBN to parameterize the probability\ndistribution and hence does not require Monte Carlo sam-\npling at each time step of training and inference.\nA recent approach developed concurrent to BachBot\nwas by Hadjeres and Pachet [23]. Their approach also uses\nan encoding which accounts for note articulations and fer-\nmatas and is similarly capable of harmonization under ar-\nbitrary constraints (e.g. a given Alto and Tenor part). How-\never, their model utilizes LSTMs to summarize both past\nand future context within \u000616time steps, limiting context\nto a temporally local region and inhibiting the learning of\nlong-term structures such as motifs. Since future context\nis not always available, to generate samples the authors\nﬁrst randomly initialize a predetermined number of time\nsteps followed by multiple iterations of MCMC. In con-\ntrast, BachBot’s ancestral sampling method requires only\na single forward pass and does not require the number of\ntimestamps in the sample to be known in advance. The\nauthors also evaluate their model using an online discrimi-\nnation test, but on a smaller participant pool of 1272.\n2. THE BACHBOT SYSTEM\n2.1 Corpus Construction and Preprocessing\nWe took the full set of Bach chorales in MusicXML format\nas provided by Cuthbert and Ariza [10]. Following prior\nwork [31, 14, 16, 17] preprocessing transposed all scores to\nC-major / A-minor and quantized time into sixteenth notes.\nTime quantization at this resolution does not distort any\nnotes in the corpus.\n2.2 Sequential Encoding of Polyphonic Music Scores\nWe encode the scores into sequences of tokens amenable\nfor sequential processing by recurrent neural networks\n(RNNs). We limit the symbolic representation to pitch\nand rhythm. This is consistent with previous work [4, 33]\nand the practice of music theoretic pedagogy. Unlike some\nprior work [15, 14, 1], we avoid explicitly encoding music-\ntheoretic concepts such as motifs, phrases, and chords /\ninversions, instead tasking the model to learn musically\nmeaningful features with minimal prior knowledge (see\nsection 3.4).\nOur encoding represents polyphonic scores with\nsixteenth-note frames , encoding duration implicitly by the\nnumber of frames processed. Such an encoding requires\nthe network to leverage memory to account for longer du-\nrations notes, a counting and timing task which LSTM is\nknown to be capable of [19]. Consecutive frames are sep-\narated by a unique delimiter ( ||| in ﬁg. 1).Within each frame, we represent individual notes rather\nthan entire chords, reducing the vocabulary size from\nO(1284)down toO(128) . Prior work modeling charac-\nters versus words in language modeling tasks suggests that\nthis has negligible impact [22]. Each frame consists of\nfour (Soprano, Alto, Tenor, and Bass) hPitch;Tieitu-\nples where Pitch2f0;1;\u0001\u0001\u0001;127grepresents the MIDI\npitch of a note and Tie2fTrue;Falsegdistinguishes\nwhether a note is tied with a note at the same pitch from the\nprevious frame or is articulated at the current timestep. We\norder notes within a frame in descending MIDI pitch and\nneglects crossing voices ; potential consequences of doing\nso are discussed in section 3.2.\nFor each score, a unique START symbol and END sym-\nbol are added. This enables initialization of the trained\nmodel prior to ancestral sampling of a token sequence by\nproviding a START token and also allows us to determine\nwhen a sampled composition ends. In addition, our encod-\ning also includes fermatas (represented by (.)), which\nBach used to denote ends of phrases. Signiﬁcantly, we\nfound that adding this additional notation to the input re-\nsulted in more realistic phrase lengths in generated output.\n2.3 Model Architecture, Training, and Sampling\nWe use a RNN with LSTM memory cells and the following\nhyperparameters:\n1.numlayers – the number of memory cell layers\n2.rnnsize – the number of hidden units per mem-\nory cell ( i.e. hidden state dimension)\n3.wordvec – dimension of vector embeddings\n4.seqlength – number of frames before truncating\nback-propagation through time (BPTT) gradient\n5.dropout – the dropout probability\nOur model ﬁrst embeds the inputs xtinto a wordvec -\ndimensional vector-space, compressing the dimensionality\ndown fromjVj \u0019 140 towordvec dimensions. Next,\nnumlayers layers of memory cells followed by batch\nnormalization [28] and dropout [26] with dropout proba-\nbility dropout are stacked. The outputs y(numlayers )\nt are\nfollowed by a fully-connected layer mapping to jVj= 108\nunits, which are passed through a softmax to yield a pre-\ndictive distribution P(xt+1jht\u00001;xt): the probability dis-\ntribution over the next token xt+1given the current token\nxtand the previous RNN memory cell state ht\u00001.\nModels were trained using the Adam optimizer [29]\nwith a minibatch size of 50and an initial learning rate\nof2\u000210\u00003decayed by 0:5every 5epochs. The back-\npropagation through time gradients were clipped at \u00065:0\n[32] and truncated after seqlength frames.\nWe minimize cross-entropy loss between the predicted\ndistributions P(xt+1jxt;ht\u00001)and the actual target dis-\ntribution\u000ext+1. During training, the correct token xt+1is\ntreated as the model output even if the most likely predic-\ntionargmaxP(xt+1jht;xt)differs. Williams and Zipser450 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017(a) Three musical chords in traditional\nmusic notation. The red arrows indicate\nthe order in which notes are sequentially\nencoded.START\n(65, False)\n(59, False)\n(55, False)\n(43, False)\n|||\n(64, False)(59, True)\n(55, True)\n(43, True)\n|||\n(.)\n(64, False)\n(60, False)(55, False)\n(48, False)\n|||\nEND\n(b) A corresponding sequential encoding of the three chords in an eighth-note time-\nquantization (for illustration, broken over three columns). Each line within a column\ncorresponds to an individual token in the encoded sequence. ||| delimit frames and\n(.) indicate a fermata is present within the corresponding frame.\nFigure 1 : Example encoding of three musical chords ending with a fermata (“pause”) chord.\n[40] refers to this as teacher forcing , which is performed\nto aid convergence because the model’s predictions may\nnot be reliable early in training. During inference, we per-\nform ancestral sampling and reuse the actual token ^xtsam-\npled fromP(xtjht\u00001;xt\u00001)to compute P(xt+1jht;xt)\nfor sampling ^xt+1. Unlike MCMC, which requires run-\nning multiple iterations to obtain a single sample, ancestral\nsampling requires only a single forward pass.\n2.4 Harmonization with Greedy 1-best Search\nChorale harmonization involves providing accompaniment\nparts to an existing melody. This is a musical task with eco-\nlogical validity undertaken by many composers including\nBach himself. Many of Bach’s chorales are harmoniza-\ntions by Bach of pre-existing melodies (not by Bach) and\ncertain melodies (by Bach or otherwise) form the basis of\nmultiple chorales with different harmonizations.\nWe extend this harmonization task to the completion of\nchorales for a wider number and type of given parts. Let\nx(1:T)be a sequence of tokens representing an encoded\nmusical score, \u000b\u001af1;2;:::;Tga multi-index, and sup-\nposebx\u000bcorrespond to some ﬁxed token values to be har-\nmonized (e.g. a provided Soprano line).\nWe are interested in solving the following optimization:\nx\u0003\n(1:T)= argmax\nx(1:T)P(x(1:T)jx\u000b=bx\u000b) (1)\nFirst, any proposed solution ~x1:Tmust satisfy ~x\u000b=bx\u000b,\nso the decision variables are ~x(1:T)n\u000b. Hinton and Se-\njnowski [25] refer to this constraint as “clamping” the gen-\nerative model. We propose a simple greedy strategy for\nchoosing ~x(1:T)n\u000b:\n~xt=(\nbxt ift2\u000b\nargmaxxtP(xtj~x1:t\u00001)otherwise(2)\nwhere the tilde on the previous tokens ~x1:t\u00001indicate that\nthey are equal to the actual previous argmax choices. This\ncorresponds to a greedy 1-best search at each time twith-\nout any accounting of future constraints (e.g. x\u001cif\u001c > t\nand\u001c2\u000b). This is sub-optimal, and we leave more so-\nphisticated search strategies such as beam search [30] for\nfuture work.3. EXPERIMENTS\n3.1 Sequence Modelling\nWith the BachBot model, we performed a grid search\nthrough the parameter grid in table 1 and found\nnumlayers = 3,rnnsize = 256 ,wordvec = 32 ,\nseqlength = 128 dropout = 0:3achieves the low-\nest cross-entropy loss of 0:477bits on a 10% held-out val-\nidation corpus.\nParameter Values Searched\nnumlayers f1;2;3;4g\nrnnsizef128;256;384;512g\nwordvec f16;32;64g\nseqlengthf64;128;256g\ndropoutf0:0;0:1;0:2;0:3;0:4;0:5g\nTable 1 : The grid of hyperparameters searched over while\noptimizing RNN structure\n3.2 Harmonization\nS A T B AT ATB\nTER 0.532 0.442 0.235 0.241 0.686 0.718\nFER 0.532 0.442 0.235 0.241 0.787 0.8780:00:10:20:30:40:50:60:70:80:9Error RateHarmonization model error rates\nTER\nFER\nFigure 2 : Token error rates (TER) and frame error rates\n(FER) for various harmonization tasks\nFor the parts to harmonize (i.e. x(1:T)n\u000b), we consid-\nered the following test cases:\n1. One part: Soprano (S), Alto (A), Tenor (T), or Bass\n(B).Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4512. The inner parts (AT). Completion of the inner\nparts corresponds to a musically-valid exercise com-\nmon in Baroque composition (including some Bach\nchorales) where only the outer voices are speciﬁed\n(with or without ﬁgured bass to indicate the chord\ntypes).\n3. All parts except Soprano (ATB): the most common\nform of harmonization exercise.\nIt is widely accepted that these tasks successively increase\nin terms of difﬁculty [11].\nWe deleted the different subsets of parts from a valida-\ntion corpus and used eq. (2) to ﬁll in the missing parts. Our\nmodel’s error rates for predicting individual tokens (token\nerror rate, TER, % of errors in individual token predic-\ntions) as well as all tokens within frames (frame error rate,\nFER, % of errors in frame predictions where any token pre-\ndiction errors within a frame counts as a frame error) are\nreported in ﬁg. 2.\nSurprisingly, error rates were higher for S/A than for\nT/B. One possible explanation for this result is our design\ndecision in section 2.2 to order notes within a frame in\nSATB order. As a result, the model must predict the So-\nprano part for each frame without any knowledge of the\nother parts. When predicting the Bass part, however, it has\nalready seen all of the other parts and can leverage this\nharmonic context. To assess this idea, we propose as fu-\nture work an investigation of different part orderings in the\nencoding.\n3.3 Musical Discrimination Test\nTo measure BachBot’s success in this task, we devel-\noped a publicly accessible musical discrimination test at\nbachbot.com . Unlike prior studies which leverage paid\nservices like Amazon MTurk for human feedback [35],\nwe offered no such incentive and promoted the study only\nthrough social media.\nParticipants were ﬁrst surveyed for their age group and\nprior music experience (ﬁg. 3a). Next, they are presented\nﬁve discrimination tasks which presented two audio tracks\n(an original Bach composition and a synthetic composition\nby BachBot) and ask them to identify the Bach original.\nEach audio track contains an entire composition from start\nto end. The music score for the audio was not provided.\nParticipants were granted an unlimited amount of time and\nallowed to replay each track an arbitrary number of times.\nParticipants could only see the next question after submit-\nting the current one and were not allowed to modify their\nresponses after submitting.\nThe ﬁve questions comprised of three harmonizations\n(S/A/T/B, one AT, one ATB), and two original composi-\ntions. To construct the questions, harmonizations were\npaired along with the original Bach chorales the ﬁxed parts\nwere taken from. No such direct comparison is possible\nfor the SATB case, so these synthetic compositions were\npaired with a randomly selected Bach chorale in a some-\nwhat different comparative listening task. Harmonizationsunder18 18to25 26to45 46to60 over60\nnovice 34 181 244 66 16\nintermediate 36 387 565 85 18\nadvanced 17 176 233 23 5\nexpert 5 34 81 21 902505007501000CountParticipant demographics\nMusic experience\nnovice\nintermediate\nadvanced\nexpert\n(a) Demographics of respondents; self-reported music experience\ndeﬁned as follows — Novice : casual listener, Intermediate : plays\nan instrument, Advanced : formally studied music composition,\nExpert : music teacher/researcher.\nS A T B AT ATB SATB\nProportion 0.82 0.58 0.49 0.39 0.73 0.65 0.510:00:20:40:60:8Proportion correctPerformance by question type\n(b) Proportion of responses correctly discriminating BachBot\nfrom Bach for different question types. The SATB column shows\nthat BachBot’s generated compositions can be differentiated from\nBach only 1%better than random guessing.\nS A T B AT ATB SATB\nnovice 0.7 0.6 0.42 0.44 0.62 0.65 0.46\nintermediate 0.85 0.57 0.53 0.28 0.78 0.66 0.52\nadvanced 0.85 0.69 0.43 0.56 0.74 0.61 0.52\nexpert 0.92 0.44 0.57 0.6 0.79 0.72 0.610:00:20:40:60:81:0Proportion correctPerformance by question type and music experience\nMusic experience\nnovice\nintermediate\nadvanced\nexpert\n(c) Figure 3b segmented by self-reported music experience. As\nexpected, more experienced listeners generally produced more\ncorrect responses, though not for the ‘B’ condition.\nFigure 3 : Results collected from a web-based musical dis-\ncrimination test.452 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 4 : Activation proﬁles suggesting that neurons have specialized to become detectors of musically relevant features.\nLayer 1, neuron 64 : strongly correlates with the use of dominant seventh chords in the main, tonic key (C major, originally\nD major). These are the main non-triadic harmony, are strongly key deﬁning, and have a important function in the harmonic\nclosure of phrases in this style. Layer 1, neuron 151 : ﬁres with the equivalent dominant seventh chord for the two cadences\nin the relative minor (a minor, originally b minor) that end phrases 2 and 4. These are the only two appearances in the\nchorale of the pitch G# which is foreign to C major, and strongly key deﬁning in a minor.\nwere synthesized by extracting part(s) from a randomly se-\nlected Bach chorale and ﬁlling in the remaining parts of the\ncomposition using the method previously described in sec-\ntion 2.4. Original compositions (questions labelled SATB)\nwere generated by providing a START symbol followed by\nancestral sampling as previously described in section 2.3\nuntil an END symbol is reached. The ﬁnal audio provided\nin the questions were obtained by rendering the composi-\ntions using the Piano instrument from the Fluid R3 GM\nSoundFont.\nWe only considered the ﬁrst response per IP address of\nparticipants who had played both choices in every question\nat least once and completed all ﬁve questions. This totaled\n2336 participants at the time of writing, making our study\none of the largest subjective listening evaluation of an au-\ntomatic composition system to date.\nFigure 3b shows the performance of BachBot on vari-\nous question types. The SATB column shows that, for the\nnovel synthetic compositions, participants on average suc-\ncessfully discriminated Bach from BachBot only 51%:av-\nerage human listeners could only perform 1%better than\nrandom guessing . To assess statistical signiﬁcance, we\nchoose signiﬁcance level \u000b= 0:05and conducted a one-\ntailed binomial test ( 446successes in 874trials) to ﬁnd that\nthe probability of a discrimination rate higher than 51%\nhasp-value 0:282> \u000b . Thus, we conclude that there\ndoes not exist sufﬁcient evidence that the discrimination\nrate between Bach and BachBot is signiﬁcantly different\n(at\u000b= 0:05) than the rate achieved by random guessingrandom guessing .\nThe weaker performance of BachBot’s outputs on most\nharmonization questions (ﬁg. 3b other than SATB) com-\npared to automatic composition questions (SATB) is coun-\nterintuitive: one would expect the provided parts to aid the\nmodel in creating more Bach-like music. This result may\nbe explained by the shortcomings of our greedy 1-best har-\nmonization method (discussed above) and/or by the pos-\nsible beneﬁt of consistent origins, with all-Bach and all-\nBachBot being preferred over hybrid solutions.\nAcross the S/A/T/B and AT/ATB conditions, the results\nvary signiﬁcantly. The ease of discrimination appears to\ncorrelate with the position in the texture from highest (S,\neasiest) to lowest (B, hardest). This may be due to the S\npart’s importance in carrying the melody in chorale style,\nor (more likely) due once again to the BachBot’s lower er-\nror rates for completing bass parts as compared with other\nparts (ﬁg. 2), which in turn is probably due to the sequen-\ntial encoding (ﬁg. 1) of bass notes last within each frame,\ngiving it a harmonic context to work with. Another possi-\nbility is that most listeners focus more on the top melody,\nneglecting the bass part and any potential deviations there.\nIn any case, the relatively poor performance of expert lis-\nteners for the B-only condition (see ﬁg. 3c) is noteworthy,\nand not explained by any aspect of the process.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4533.4 Do Neurons Specialize to Music-Theoretic\nConcepts?\nResearch in convolutional networks has shown that neu-\nrons within computer vision models specialize to detect\nhigh-level visual features [42]. Similarly, convolutional\nnetworks trained on audio spectrograms have been shown\nto possess neurons which detect high-level aural features\n[5]. Following these results, one might expect the Bach-\nBot model to possess neurons which detect features within\nsymbolic music which have music theoretic relevance.\nTo investigate this further, one could look at the acti-\nvations over time of individual neurons within the LSTM\nmemory cells to see if neuron activity correlates with rec-\nognized musical processes. An informal analysis sug-\ngests that while some neurons are ambiguous to interpreta-\ntion, other neurons correlate signiﬁcantly with recognized\nmusic-theoretic objects, particularly chords (see ﬁg. 4).\nTo our knowledge, this is the ﬁrst reported evidence for\nan LSTM optimized for automatic composition learning\nmusic-theoretic concepts without explicit prior informa-\ntion. This invites a follow-up study testing the statistical\nsigniﬁcance of these observations.\n4. DISCUSSION\nThe data generated by bachbot.com shows that subjects\ndistinguished BachBot from Bach only 51% of the time,\nsuggesting that BachBot successfully composes and com-\npletes music that cannot be distinguished from Bach sig-\nniﬁcantly above the chance level. Additionally, BachBot’s\ndesign involves no explicit encoding of musical parame-\nters beyond the notation, so the results reﬂects its ability to\nacquire music knowledge independently from data.\nAs discussed, the higher time resolution of our custom\nencoding scheme enabled the model to learn about Bach’s\nuse of sixteenth notes, which is not possible for models\ntrained on JSB Chorales . Unfortunately, this improved en-\ncoding means that we are unable to compare quantitative\nperformance metrics such as log likelihood against other\nliterature values reported for polyphonic modeling on the\nJSB Chorales [1] dataset.\nUsing this sequential encoding scheme, we train a deep\nLSTM sequential prediction model and discover that it\nlearns music theoretic concepts without prior knowledge or\nexplicit supervision. We then propose a method to utilize\nthe sequential prediction model for harmonization tasks.\nWe acknowledge that our method is not ideal and discuss\nbetter alternatives in future work. Our harmonization re-\nsults reveal that this issue is signiﬁcant and should be a\npriority for any follow-up work.\nFinally, we leveraged our model to generate harmoniza-\ntions as well as novel compositions and used the generated\nmusic in a web-based music discrimination test. Our re-\nsults here conﬁrm the success of our project.\nWhile many opportunities for extension are highlighted,\nwe conclude that our stated research aims have been\nreached. In other words, generating stylistically successful\nBach chorales is now a more closed (as a result of Bach-Bot) than open problem.\n5. CONCLUSION\nIn this paper, we:\n\u000fintroduce a sequential encoding scheme for music\nwhich achieves time-resolution 2\u0002that of the com-\nmonly used JSB Chorales [1] dataset.\n\u000fperformed the largest (to the best of our knowledge\nat time of publication) musical discrimination test\nof an automatic composition system, which demon-\nstrated that high quality data can be collected from\nvoluntary internet surveys.\n\u000fdemonstrate that a deep LSTM sequential prediction\nmodel trained on our encoding scheme is capable of\ncomposing music that can be distinguished only 1%\nbetter than random guessing, a statistically insignif-\nicant difference\n\u000fprovide the ﬁrst evidence that neurons in the LSTM\nmodel appear to model common music-theoretic\nconcepts without prior knowledge or supervision.\nIn addition, we have open sourced the code for Bach-\nBot1as well as our music discrimination test frame-\nwork2. The Magenta project of Google Brain has re-\ncently implemented the BachBot model for their poly-\nphonic RNN model3.\n6. REFERENCES\n[1] Moray Allan and Christopher KI Williams. al-\nlan2005. Advances in Neural Information Processing\nSystems , 17:25–32, 2005.\n[2] Justin Bayer, Christian Osendorfer, Daniela Korham-\nmer, Nutan Chen, Sebastian Urban, and Patrick\nvan der Smagt. On fast dropout and its ap-\nplicability to recurrent networks. arXiv preprint\narXiv:1311.0701 , 2013.\n[3] Matthew I Bellgard and Chi-Ping Tsang. Harmoniz-\ning music the boltzmann way. Connection Science , 6\n(2-3):281–297, 1994.\n[4] Nicolas Boulanger-Lewandowski, Pascal Vincent,\nand Yoshua Bengio. Modeling Temporal Dependen-\ncies in High-Dimensional Sequences: Application\nto Polyphonic Music Generation and Transcription.\nProc. of the 29th International Conference on Ma-\nchine Learning (ICML-12) , (Cd):1159–1166, 2012.\n1https://github.com/feynmanliang/bachbot\n2https://github.com/feynmanliang/\nsubjective-evaluation-server and https://github.\ncom/feynmanliang/subjective-evaluation-client\n3https://github.com/tensorflow/magenta/tree/\nmaster/magenta/models/polyphony_rnn454 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[5] Keunwoo Choi, George Fazekas, Mark Sandler, and\nJeonghee Kim. Auralisation of deep convolutional\nneural networks: Listening to learned features. In\nProceedings of the 16th International Society for Mu-\nsic Information Retrieval Conference, ISMIR , pages\n26–30, 2015.\n[6] Tom Collins, Robin Laney, Alistair Willis, and\nPaul H Garthwaite. Developing and evaluating com-\nputational models of musical style. Artiﬁcial Intelli-\ngence for Engineering Design, Analysis and Manu-\nfacturing , 30(01):16–43, 2016.\n[7] David Cope. Experiments in music intelligence. In\nProc. of the International Computer Music Confer-\nence, 1987.\n[8] David Cope. Computer modeling of musical intelli-\ngence in emi. Computer Music Journal , 16(2):69–83,\n1992.\n[9] Pedro P Cruz-Alc ´azar and Enrique Vidal-Ruiz.\nLearning regular grammars to model musical style:\nComparing different coding schemes. In Interna-\ntional Colloquium on Grammatical Inference , pages\n211–222. Springer, 1998.\n[10] Michael Scott Cuthbert and Christopher Ariza. mu-\nsic21: A toolkit for computer-aided musicology and\nsymbolic music data. 2010.\n[11] James Denny. The Oxford school harmony course ,\nvolume 1. Oxford University Press, 1960.\n[12] Kemal Ebcio ˘glu. An expert system for harmonizing\nfour-part chorales. Computer Music Journal , 12(3):\n43–51, 1988.\n[13] D. Eck and J. Schmidhuber. Finding temporal struc-\nture in music: Blues improvisation with LSTM re-\ncurrent networks. Neural Networks for Signal Pro-\ncessing - Proc. of the IEEE Workshop , 2002-Janua:\n747–756, 2002. ISSN 0780376161. doi: 10.1109/\nNNSP.2002.1030094.\n[14] Douglas Eck and J ¨urgen Schmidhuber. A 1st\nLook at Music Composition using LSTM Re-\ncurrent Neural Networks. Idsia , 2002. URL\nhttp://www.idsia.ch/{ ˜}juergen/\nblues/IDSIA-07-02.pdf .\n[15] Johannes Feulner and Dominik H ¨ornel. Melonet:\nNeural networks that learn harmony-based melodic\nvariations. In Proc. of the International Com-\nputer Music Conference , pages 121–121. INTER-\nNATIONAL COMPUTER MUSIC ACCOCIATION,\n1994.\n[16] Judy A Franklin. Recurrent neural networks and pitch\nrepresentations for music tasks. In FLAIRS Confer-\nence, pages 33–37, 2004.[17] Judy A Franklin. Jazz melody generation from recur-\nrent network learning of several human melodies. In\nFLAIRS Conference , pages 57–62, 2005.\n[18] Judy A Franklin. Recurrent neural networks for mu-\nsic computation. INFORMS Journal on Computing ,\n18(3):321–338, 2006.\n[19] Felix A Gers, Nicol N Schraudolph, and J ¨urgen\nSchmidhuber. Learning precise timing with lstm re-\ncurrent networks. Journal of machine learning re-\nsearch , 3(Aug):115–143, 2002.\n[20] Kratarth Goel and Raunaq V ohra. Learning tempo-\nral dependencies in data using a dbn-blstm. arXiv\npreprint arXiv:1412.6093 , 2014.\n[21] Kratarth Goel, Raunaq V ohra, and JK Sahoo. Poly-\nphonic music generation by modeling temporal de-\npendencies using a rnn-dbn. In International Confer-\nence on Artiﬁcial Neural Networks , pages 217–224.\nSpringer, 2014.\n[22] Alex Graves. Generating sequences with recurrent\nneural networks. arXiv preprint arXiv:1308.0850 ,\n2013.\n[23] Ga ¨etan Hadjeres and Franc ¸ois Pachet. Deepbach: a\nsteerable model for bach chorales generation. arXiv\npreprint arXiv:1612.01010 , 2016.\n[24] Hermann Hild, Johannes Feulner, and Wolfram Men-\nzel. Harmonet: A neural net for harmonizing chorales\nin the style of js bach. In NIPS , pages 267–274, 1991.\n[25] Geoffrey E Hinton and Terrence J Sejnowski. Learn-\ning and releaming in boltzmann machines. Paral-\nlel distributed processing: Explorations in the mi-\ncrostructure of cognition , 1:282–317, 1986.\n[26] Geoffrey E Hinton, Nitish Srivastava, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan R Salakhut-\ndinov. Improving neural networks by preventing\nco-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580 , 2012.\n[27] Dominik H ¨ornel. Melonet i: Neural nets for invent-\ning baroque-style chorale variations. In NIPS , pages\n887–893, 1997.\n[28] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[29] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[30] Xunying Liu, Yongqiang Wang, Xie Chen, Mark JF\nGales, and Philip C Woodland. Efﬁcient lattice\nrescoring using recurrent neural network language\nmodels. In 2014 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 4908–4912. IEEE, 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 455[31] Michael C Mozer. Neural network music composi-\ntion by prediction: Exploring the beneﬁts of psychoa-\ncoustic constraints and multi-scale processing. Con-\nnection Science , 6(2-3):247–280, 1994.\n[32] Razvan Pascanu, Tomas Mikolov, and Yoshua\nBengio. On the difﬁculty of training recurrent neural\nnetworks. Proc. of The 30th International Confer-\nence on Machine Learning , (2):1310–1318, 2012.\nISSN 1045-9227. doi: 10.1109/72.279181. URL\nhttp://jmlr.org/proceedings/papers/\nv28/pascanu13.pdf .\n[33] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho,\nand Yoshua Bengio. How to construct deep recurrent\nneural networks. arXiv preprint arXiv:1312.6026 ,\n2013.\n[34] Marcus Pearce and Geraint Wiggins. Towards a\nframework for the evaluation of machine composi-\ntions. In Proc. of the AISB’01 Symp. on Artiﬁcial\nIntelligence and Creativity in the Arts and Sciences ,\npages 22–32. Citeseer, 2001.\n[35] Donya Quick. Kulitta: A Framework for Automated\nMusic Composition . PhD thesis, YALE UNIVER-\nSITY , 2014.\n[36] Randall R Spangler, Rodney M Goodman, and Jim\nHawkins. Bach in a box-real-time harmony. 1998.\n[37] Peter Todd. A sequential network design for musi-\ncal applications. In Proc. of the 1988 connectionist\nmodels summer school , pages 76–84, 1988.\n[38] Peter M Todd. A connectionist approach to algorith-\nmic composition. Computer Music Journal , 13(4):\n27–43, 1989.\n[39] Chi Ping Tsang and Melanie Aitken. Harmoniz-\ning music as a discipline in contraint logic program-\nming. In Proc. of the International Computer Music\nConference , pages 61–61. INTERNATIONAL COM-\nPUTER MUSIC ACCOCIATION, 1991.\n[40] Ronald J Williams and David Zipser. A learning al-\ngorithm for continually running fully recurrent neural\nnetworks. Neural computation , 1(2):270–280, 1989.\n[41] Wojciech Zaremba. An empirical exploration of re-\ncurrent network architectures. 2015.\n[42] Matthew D Zeiler, Dilip Krishnan, Graham W Taylor,\nand Rob Fergus. Deconvolutional networks. In Com-\nputer Vision and Pattern Recognition (CVPR), 2010\nIEEE Conference on , pages 2528–2535. IEEE, 2010.456 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Chord Generation from Symbolic Melody Using BLSTM Networks.",
        "author": [
            "Hyungui Lim",
            "Seungyeon Rhyu",
            "Kyogu Lee"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417327",
        "url": "https://doi.org/10.5281/zenodo.1417327",
        "ee": "https://zenodo.org/records/1417327/files/LimRL17.pdf",
        "abstract": "Generating a chord progression from a monophonic melody is a challenging problem because a chord progression requires a series of layered notes played simultaneously. This paper presents a novel method of generating chord sequences from a symbolic melody using bidirectional long short-term memory (BLSTM) networks trained on a lead sheet database. To this end, a group of feature vectors composed of 12 semitones is extracted from the notes in each bar of monophonic melodies. In order to ensure that the data shares uniform key and duration characteristics, the key and the time signatures of the vectors are normalized. The BLSTM networks then learn from the data to incorporate the temporal dependencies to produce a chord progression. Both quantitative and qualitative evaluations are conducted by comparing the proposed method with the conventional HMM and DNN-HMM based approaches. Proposed model achieves 23.8% and 11.4% performance increase from the other models, respectively. User studies further confirm that the chord sequences generated by the proposed method are preferred by listeners.",
        "zenodo_id": 1417327,
        "dblp_key": "conf/ismir/LimRL17",
        "keywords": [
            "chord progression",
            "monophonic melody",
            "layered notes",
            "bidirectional long short-term memory (BLSTM) networks",
            "lead sheet database",
            "symbolic melody",
            "feature vectors",
            "12 semitones",
            "key and duration characteristics",
            "normalized"
        ],
        "content": "CHORD GENERATION FROM SYMBOLIC MELODY USING BLSTM NETWORKS 𝐇𝐲𝐮𝐧𝐠𝐮𝐢\t𝐋𝐢𝐦𝟏,𝟐, 𝐒𝐞𝐮𝐧𝐠𝐲𝐞𝐨𝐧\t𝐑𝐡𝐲𝐮𝟏 and 𝐊𝐲𝐨𝐠𝐮\t𝐋𝐞𝐞𝟏,𝟐 \t3Music and Audio Research Group, Graduate School of Convergence Science and Technology \t4Center for Super Intelligence Seoul National University, Korea {goongding7, rsy1026, kglee}@snu.ac.kr  ABSTRACT Generating a chord progression from a monophonic melody is a challenging problem because a chord progression requires a series of layered notes played simultaneously. This paper presents a novel method of generating chord sequences from a symbolic melody using bidirectional long short-term memory (BLSTM) networks trained on a lead sheet database. To this end, a group of feature vectors composed of 12 semitones is extracted from the notes in each bar of monophonic melodies. In order to ensure that the data shares uniform key and duration characteristics, the key and the time signatures of the vectors are normalized. The BLSTM networks then learn from the data to incorporate the temporal dependencies to produce a chord progression. Both quantitative and qualitative evaluations are conducted by comparing the proposed method with the conventional HMM and DNN-HMM based approaches. Proposed model achieves 23.8% and 11.4% performance increase from the other models, respectively. User studies further confirm that the chord sequences generated by the proposed method are preferred by listeners.  1. INTRODUCTION Generating chords from melodies is an artistic process for musicians, which requires knowledge of chord progression and tonal harmony. While it plays an important role in music composition studies, the implementation of its process can be difficult especially for individuals who do not have prior experience or domain knowledge in musical studies. For this reason, the chord generation process often serves as an obstacle for novices who try to compose music based on a melody.     To overcome this limitation, automatic chord generation systems have been implemented based on machine learning methods [1, 2]. One of the most popular approaches for this task is probabilistic modelling, which commonly applies the hidden Markov model (HMM). A single-HMM is used with 12-semitone vectors of melody as observations and corresponding chords as hidden states [3, 4]. Allan and Williams trained a first-order HMM which learns from pieces composed by Bach, to generate chorale harmonies [5]. A more complex method is presented by Raczyński et al. [6], using time-varying tonalities and bigrams as observations with melody variables. In addition, a multi-level graphical model using tree structures and HMM is proposed by Paiement et al. [7]. Their model generates chord progressions based on the root note progression predicted from a melodic sequence. Forsyth and Bello [8] also introduced a MIDI based harmonic accompaniment system using a finite state transducer (FST).      Although the HMM has been successfully used for various tasks, it has several drawbacks. According to one of the assumptions of the Markov model, observations occur independently of their neighbors, depending only on the current state. Moreover, the current state of a Markov chain is only affected by its previous state. These drawbacks are also observable in chord generation from melody tasks because long-term dependencies exist in chord progressions and melodic sequences of Western tonal music [6].     Meanwhile, deep learning based approaches have recently shown great improvements in machine learning tasks of large datasets. Especially for temporal sequences, recurrent neural networks (RNN) and long short term memory (LSTM) networks have proven to be more powerful models than HMM in the field of handwriting recognition [9], speech recognition [10], and emotion recognition [11]. Nowadays, even music generation researches have increasingly adapted RNN/LSTM models in two major stream – one that aims to generate complete music sequences [12, 13], and the other which concentrates on generating music components such as melody, chord and drum sequence [14, 15]. We attempt an extended approach to the latter stream by implementing a chord generation system with a melody input.      In this paper, we implement a chord generation algorithm based on bidirectional LSTM (BLSTM) and evaluate its performance on reflecting temporal dependencies on melody/chord progressions by comparing with two HMM-based methods: a simple HMM, and deep neural networks-HMM (DNN-HMM). We then present the quantitative analysis and the accuracy results of the three models. We also describe the qualitative results based on subjective ratings provided by 25 non-musicians. \n © Hyungui Lim, Seungyeon Ryu and Kyogu Lee. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Hyungui Lim, Seungyeon Ryu and Kyogu Lee. “Chord Generation from Symbolic Melody Using BLSTM Networks”, 18th International Society for Music Information Retrieval Conference, 2017. 621   \n    The remainder of the paper is organized as follow. In Section 2, we explain the preprocessing step and the details of the machine learning methods we apply. Section 3 describes the experimental setup for evaluating the proposed approach. The experimental results are presented in Section 4, with additional discussions. Finally, we draw a conclusion followed by limitations and future works in Section 5.  2. METHODOLOGY The method proposed in this paper can be divided into two main parts. The first part is a preprocessing procedure to extract input/output features from lead sheets. The other part consists of model training and a chord generation processes. We apply BLSTM networks for the proposed model and two types of HMM for the comparable models. The overall framework of our proposed method is shown in Figure 1. 2.1 Preprocessing To extract appropriate features for this task, we first collect musical features such as time signature, measure (bar), key {fifths, mode}, chord {root, type} and note {root, octave, duration} from the lead sheets. These features are then represented in a matrix by concatenating rows, which respectively represent the musical features of a single note as shown in Figure 2.     The generated data is then preprocessed in order to make an acceptable relation between melody input and chord output. All songs are in major key in the database and are transposed to C major key for data consistency. In other words, all roots of chords and notes are shifted to C major key to normalize different characteristics of melodies and chords in different songs.     Each song contains a time signature, which has a variety of meters such as 4/4, 3/4, 6/8, etc. The variety in time signature causes the imbalance of total note durations in a bar among different songs, so note durations are normalized by multiplying them with the reciprocal number of each time signature. After that, every note in a bar is stored into 12 semitone classes, without the octave information. Each class consists of a single value that accumulates the duration of the corresponding semitone in the bar.     Since the total number of chord types is quite large, if all of these chord types exist as independent classes, then each chord may not have enough samples. For such reason, all types of chords are mapped into one of two primary triads: major and minor. Each chord is represented with a binary 24-dimensional class to indicate the 24 major/minor chords.  2.2 BLSTM Networks Recurrent neural networks (RNN) is a deep learning model, which learns complex networks not only by reconstructing the input features in a nonlinear process, but also by using the parameters of previous states in its hidden layer. A concept of “time step” exists in RNN, which is able to control the number of feedbacks on a recurrent process. This property enables the model to incorporate temporal dependencies by storing the past information in its internal memory, in contrast to a simple feedforward deep neural networks (DNN).     Despite such advantages of RNN models, there still exist problems regarding the long-term dependency. This is caused by vanishing gradient during the back propagation through time (BPTT) [16]. In the process of calculating the gradient of the loss function, the error between the estimated value and the actual value diminishes as the number of hidden layers increases. Thus, we instead use long short-term memory (LSTM) layers, \nFigure 2. An example of extracted data from a single bar.  \nFigure 1. The overview of proposed system \n622 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   which improve the limitation of storing long-term history with three multiplicative gates [17].      Generally, chords and melodies are formed in a sequential order, which is affected by both the previous and next order. Based on this, we can predict that if we reverse the lead sheet and train the musical progressions, a meaningful sequential context similar to the originals will appear. Hence, we apply a BLSTM so that the network can reflect musical context not only in forward but also in backward directions.     As shown in Figure 1, the input semitone vectors from each bar enter the network sequentially during the time step (i.e. a fixed number of bars) and emit the corresponding output chord classes in the same order. This is possible because the hidden layer in the network returns the output for each input. In order to train this sequence of multiple bars, we reconstruct our dataset by applying the window with the size of the time step and overlapping the window with the hop size of one bar. Each window, composed of multiple bars, is then used as a sample to train the network.     For our model, we build a time distributed input layer with 12 units, which represents the sequence of semitone vectors, 2 hidden layers with 128 BLSTM units, and a time distributed output layer with 24 units, which represents the sequence of chord classes. We empirically choose the number of hidden layers and units that yield the best result. We use hyperbolic tangent activation function for the hidden layers to reconstruct the features in a nonlinear process. We then apply the softmax function for the output layer to generate values corresponding to the probability of each class. Dropout is also employed with a rate of 0.2 on all hidden layers to prevent overfitting. We use mini-batch gradient descent with categorical cross entropy as the cost function and Adam as the optimizer. In addition, for the model training process, we use a batch size of 512 and early stopping for 10 epoch patience.  2.3 Hidden Markov Model We apply two types of supervised HMM as baseline models. First is a simple HMM which is a generative model and the other is hybrid deep neural network–HMM (DNN-HMM) which is a sequence-discriminative model [18].  2.3.1 Simple HMM The simple HMM consists of three parameters: initial state distribution, transition probability and emission probability. In our case, the initial state distribution is the histogram of each chord in our train set. The transition probability is computed using the bigram of chord transition and it is assumed to follow the rule of general first-order Markov chains. A higher-order transition probability is not taken into account because the fixed length of an input bar in our task is not long enough. The emission probability is determined by a multinomial distribution of semitone observations from each chord class.  Once the parameters are learned, the model can generate a sequence of hidden chord states from a melody with three steps. First, the probabilities of 24 chord classes in each bar are determined by the melody distribution in each bar. As mentioned above, the simple HMM is a generative model. Hence, it uses not only the emission probability but also a class prior to calculate posterior probability with the Bayes rule. We define the class prior same as the initial probability, which is the histogram of each chord. Secondly, in order to reflect sequential effects, transition probability is applied to adjust the probabilities of the chord classes. In case of the first chord state, since there is no previous state to consider the transition, the initial probability is applied instead. After that, a Viterbi decoding algorithm is implemented to find the optimal chord sequence that is most likely to match along with the observed melody sequence [19]. 2.3.2 DNN-HMM The hybrid DNN-HMM is a popular model in the field of speech recognition [20]. It is a sequence-discriminative model, which adapts the advantage of sequential modeling method of HMM, but does not require the class prior and the emission probability to get posterior probability. DNN makes it possible because the probability result from a softmax output layer can be assumed as a posterior probability. Then the two of HMM parameters - initial state distribution and transition probability – are applied identically with the simple HMM to employ the Viterbi decoding algorithm.     We build an input layer with 12 units, 3 hidden layers with 128 units that are all identical and an output layer with 24 units. We use hyperbolic tangent activation function for the hidden layer and softmax for the output layer. Other features such as dropout, loss function, optimizer and batch size are applied in the same settings of BLSTM.  3. EXPERIMENTS In this section, we first introduce our dataset, which is parsed from digital lead sheets. Then we present the experimental setup for evaluating the performance of chord generation models. We conduct both quantitative and qualitative evaluations for this task. 3.1 Dataset We use the lead sheet database provided by Wikifonia.org, which was a public lead sheet repository. The site unfortu-nately stopped service in 2013, but some of the data, which consists of 5,533 Western music lead sheets in MusicXML format, including rock, pop, country, jazz, folk, R&B, chil-dren’s song, etc., was obtained before the termination and we extracted features from the data for only academic pur-pose. From the obtained database, we collect 2,252 lead sheets, which are all in major key, and the majority of the Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 623   bars in the lead sheets have a single chord per bar. If a bar consists of two or more chords, we choose the first chord in the bar. Then we extract musical features and convert them to a CSV format (see Section 2.1). The set is split into two sets – a training set of 1802 songs, which consists of 72,418 bars and a test set of 450 songs, which consists of 17,768 bars. Since musical features in this dataset can be useful for not only chord generation but also for other kinds of symbolic music tasks, the dataset is shared on our website (http://marg.snu.ac.kr/chord_generation/) for public access.  3.2 Quantitative Evaluation We perform a quantitative analysis by comparing the accuracies of chord estimation from each model using the test set. The accuracy is calculated by counting the number of matching samples between the predicted and the true chords and by dividing it by the total number of samples. We mainly apply a 4-bar melody input for our task, but also experiment with 8-, 12- and 16-bar inputs to analyze the influence on the length of a melody sequence.     Determining the “right” chord is a difficult process because chord selection can vary among people based on their musical styles and tastes. However, the aforementioned accuracy calculation is often used to evaluate the capability of incorporating the long-term dependency in the musical progression [6, 8]. Therefore, we use it for measuring which model reflects the relationship between chord and melody most adequately.  3.3 Qualitative Evaluation As mentioned above, there is a limit to evaluate the model performance only by a quantitative analysis. Thus, we also conduct qualitative evaluation based on subjective rating from actual user. This assessment allows us to determine the validity of each model by comparing how the chords generated from different models are perceived by actual users. For the experiment, we collect eighteen 4-bar-length melodies from lead sheets of thirteen K-pop songs and five Western pop songs. Every melodic sequence is converted into a vector of 12 semitones as described in Section 2.1. HMM, DNN-HMM, and BLSTM then generate chord sequences from each vector. Those sequences are evaluated by 25 musically untrained participants (13 males and 12 females) through a web-based survey.  The participants complete 18 sets of surveys in their own pace. At the beginning of each set, participants listen to a melody. After that, participants listen to the four types of chord progressions, including the one from the original song, along with the melody. Participants are asked to rate each chord progression on a five-point scale (1 – ‘not appropriate’; 5 – ‘very appropriate’). At the end of each set, participants also are asked to answer a question whether they have pre-existing familiarity with the original songs. The audio samples used for experiment are available on our website. 4. RESULTS 4.1 Chord Prediction Performance Table 1 presents the accuracy results of three models for four instances of different bar lengths. The results show that the BLSTM method achieves the best performance on the test set followed by DNN-HMM and HMM. According to the average scores of models, BLSTM has 23.8% and 11.4% performance increase from the HMM and DNN-HMM, respectively. The results also demonstrate that the number of input bars is not an important factor affecting the accuracy for all models since they don’t show obvious linear variations. To examine the quality of predicted chords from each model more in depth, we compute the results of each model into a confusion matrix. This allows us to easily analyze the results through visualization. We normalize the matrix with the number of samples in respective chords so that each row represents the distribution of predicted chords on each true chord class. In Figure 3, we display this normalized confusion matrix of each model.      A number of noteworthy findings from each matrix are observed. First, HMM yields a skewed result that shows severe misclassification of chords especially on C, F and G as shown in Figure 3(a). We hypothesize this is resulted from the lack of complexity of the model. Emission probability, one of the parameters of the model, does not properly capture the accurate correlation between the chords and corresponding melodies. Moreover, the fact that the training data contains more frequent occurrences of C, F and G chords (over 60% in total samples) reduced the accuracy of the HMM model which uses the prior probability to obtain the posterior as mentioned in Section 2.3.1. Lastly, a noticeable bias in transition matrix moving to C chord also seems to lower the precision of the model.  The result of DNN-HMM is similar to HMM but the skewness on C chord spreads out little bit to F and G chords. Despite our initial expectation that the DNN would perform better since it is a discriminative model that calculates posterior directly, still many misclassifications on three chords exist as shown in Figure 3(b). To find the reasoning behind this observation, we test simple DNN with 1-bar input without the sequential parameter of HMM. The accuracy is higher than DNN-HMM (46.93%) and the confusion matrix produces more diagonal elements as shown in Figure 4. This finding supports that the transition \nTable 1. Chord prediction performance using different number of input bar. \n624 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   \nprobability of HMM forces the model to generate limited classes and also that the model is not adequate to train various chord progressions.      In contrast to the HMM based method, the confusion matrix of the BLSTM shows a less skewed distribution and clearer diagonal elements as shown in Figure 3(c). BLSTM has much more complex parameters in hidden layers, which train the sequential information of both melodies and chords. We believe this property makes the performance better compared to the others.   4.2 User Preference In the user subjective test, evaluation scores are obtained from 450 sets (18 sets x 25 participants). Each set contains chord sequences from HMM, DNN-HMM, and BLSTM. An original chord sequence is also included for relative comparison of the generated results to the original. These four chord sequences are evaluated as described in Section 3.3. Figure 5 shows the example of melody and chord sequences which is used in the user test and more examples are available to listen on our website. The average score of each model is shown in Figure 6. The original chord progression is preferred the most followed by BLSTM, DNN-HMM, and HMM. To investigate whether differences on scores between the results are critical, we conduct one-way repeated measure ANOVA setting each model as a variable. The result shows that at least one out of four scores is significantly different from the others. (F(3, 1772) = 310, p < 0.001). We then conduct a pairwise t-test with Bonferroni correction on the mean scores between each pair of models for a post-hoc analysis. As a result, differences between all pairs are proven to be significant (p < 0.01). Therefore, it can be concluded that the BLSTM produces the most satisfying chord sequences among the other computational models but it produces less satisfying results than the original. Moreover, since the difference between BLSTM and DNN-HMM is bigger than other pairs, it seems there is a big quality difference between them.      To verify our hypothesis that having familiarity with the original song affects the result we perform a further analysis. We separate 450 evaluation sets into two, 248 sets marked as known and the rest as unknown, and conduct further analysis. A simple comparison of those two sets based on the evaluation scores shows that awareness of the songs does not affect the preference rank of the models. We also perform one-way repeated measure \nFigure 3. Normalized confusion matrix of HMM(a), DNN-HMM(b), and BLSTM(c) using 4-bar melody input. \nFigure 4. Normalized confusion matrix of simple DNN using single bar melody input. \nFigure 5. An example of generated chord progressions from three different models and the original progression. Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 625   \nANOVA for each group of awareness (group of known songs: F3, 964\t=\t286, p\t<\t0.001; group of unknown songs: F3, 780\t=\t72, p\t<\t0.001) and pairwise t-test with Bonferroni correction. The results are presented in Figure 7. As shown in the figure, when songs are unknown, the preference for HMM based models increases while it decreases for BLSTM generated and original chords. A plausible explanation for this observation can be that when the listener knows the song, he/she is more perceptive of the monotonous chord sequences generated from HMM and DNN-HMM which tend to produce more of C, F and G than other chords. However, when the listener does not know the song, he/she is less aware of the monotonous progression of the chords and tend to give more generous scores to those two models. For BLSTM, the result is the opposite. Listeners who are more used to the dynamic chord progression of the original song tend to give relatively higher scores to BLSTM than to HMM based methods probably because BLSTM often generates a more diverse chord sequences. On the other hand, when the songs are unknown, relative preference towards both BLSTM and the original chords is less strong. The reduced gap among four different options when the songs are unknown may be explained by the assumption that when the songs are not familiar, all four options are relatively equally acceptable to the listeners. Regardless of the difference in the results, however, BLSTM is preferred over the other two models in both cases.  5. CONCLUSIONS We have introduced a novel approach for generating a chord sequence from symbolic melody using neural network models. The result shows that BLSTM achieves the best performance followed by DNN-HMM and HMM. Therefore, the recurrent layer of BLSTM is more appropriate to model the relationship between melody and chord than HMM based sequential methods.      Our work can be further improved by modifying data extracting and preprocessing steps. First, since the lead sheets used in this study have one chord in each bar, the task is constrained to one-chord generation for each bar. Since actual music usually contains a lot of bars with multiple chords, additional extraction process is needed to allow the model to generate multiple chords per bar. Secondly, in the preprocessing step, all chords are mapped into only 24 classes of major and minor. Thus, further chord classes such as maj7 and min7 need to be included for performance improvement. Lastly, our input feature vectors consist of 12 semitones by accumulating the melody notes in each bar, so the sequential information of melodies in each bar disappears in this step. Thus, another feature-preprocessing step may be needed not to omit the information, which can be a crucial factor in the future work. We hope that more researches will be done through our published data to overcome the limitations as well as to further develop of this task.  6. ACKNOWLEDGEMENTS This work was supported by Kakao Corp. and Kakao Brain Corp. \nFigure 6. Mean score of subjective evaluation of each model. \nFigure 7. Mean score of subjective evaluation for a group of known songs (a), and of unknown songs (b). \n626 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017   7. REFERENCES [1] E. C. Lee and M. W. Park: “Music Chord Recommendation of Self Composed Melodic Lines for Making Instrumental Sound,” Multimedia Tools and Applications, pp. 1-17, 2016. [2] S. D. You and P. Liu: “Automatic Chord Generation System Using Basic Music Theory and Genetic Algorithm,” Proceedings of the IEEE Conference on Consumer Electronics (ICCE), pp. 1-2, 2016. [3] I. Simon, D. Morris, and S. Basu: \"MySong: Automatic Accompaniment Generation for Vocal Melodies,\" Proceedings of the Special Interest Group on Computer-Human Interaction (SIGCHI) Conference on Human Factors in Computing Systems, pp. 725-734, 2008. [4] H. Lee and J. Jang: \"i-Ring: A System for Humming Transcription and Chord Generation,\" Proceedings of the IEEE International Conference on Multimedia and Expo (ICME), Vol. 2, pp. 1031–1034, 2004. [5] M. Allan and C. Williams: “Harmonizing Chorales by Probabilistic Inference,” Advances in Neural Information Processing Systems, Vol. 17, pp. 25-32, 2005. [6] S. A. Raczyński, S. Fukayama, and E. Vincent: “Melody Harmonization with Interpolated Probabilistic Models,” Journal of New Music Research, Vol. 42, No. 3, pp.223-235, 2013.  [7] J. Paiement, D. Eck, and S. Bengio:  \"Probabilistic Melodic Harmonization,\" Proceedings of the 19th Canadian Conference on Artificial Intelligence, pp. 218-229, 2006. [8] J. P. Forsyth and J. P. Bello: \"Generating Musical Accompaniment Using Finite State Transducers,\" Proceedings of the 16th International Conference on Digital Audio Effects (DAFx-13), 2013. [9] A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidlhuber: “A Novel Connectionist System for Unconstrained Handwriting Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 5, pp. 855-868, 2009. [10] H. Sak, A. Senior, and F. Beaufays: “Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition,” CoRR arXiv: 1402.1128, 2014. [11] M. Wöllmer, A. Metallinou, F. Eyben, B. Schuller, and S. Narayanan: “Context-Sensitive Multimodal Emotion Recognition from Speech and Facial Expression Using Bidirectional LSTM Modeling,” Interspeech, pp. 2362-2365, 2010.  [12] I. Liu and B. Ramakrishnan: “Bach in 2014: Music Composition with Recurrent Neural Network,” CoRR arXiv: 1412.3191, 2014. [13] D. D. Johnson: “Generating Polyphonic Music Using Tied Parallel Networks,” International Conference on Evolutionary and Biologically Inspired Music and Art, pp. 128-143, 2017. [14] A. E. Coca, D. C. Corrêa, and L. Zhao: “Computer-aided Music Composition with LSTM Neural Network and Chaotic Inspiration,” Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN), pp. 1-7, 2013.  [15] K. Choi, G. Fazekas, and M. Sandler: “Text-based LSTM Networks for Automatic Music Composition,” CoRR arXiv: 1604.05358, 2016.  [16] F. A. Gers, J. Schmidhuber, and F. Cummins: “Learn-ing to Forget: Continual Prediction with LSTM,” Neural Computation, Vol. 12, pp. 2451-2471, 2000. [17] S. Hochreiter and J. Schmidhuber: “Long Short-Term Memory,” Neural Computation, Vol. 9, No. 8, pp. 1735-1780, 1997. [18] K. Veselý, A. Ghoshal, L. Burget, and D. Povey: “Sequence-Discriminative Training of Deep Neural Networks,” Interspeech, pp. 2345-2349, 2013.  [19] K. Lee and M. Slaney: “Automatic Chord Recogni-tion from Audio Using a Hmm with Supervised Learning,” Proceedings of the 7th International Con-ference on Music Information Retrieval, pp. 133–137, 2006. [20] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury: “Deep Neural Networks for Acoustic Modeling in Speech Recognition,” IEEE Signal Processing Magazine, Vol. 29, No. 6, pp. 82-97, 2012.  Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 627"
    },
    {
        "title": "Artist Preferences and Cultural, Socio-Economic Distances Across Countries: A Big Data Perspective.",
        "author": [
            "Meijun Liu",
            "Xiao Hu 0001",
            "Markus Schedl"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417193",
        "url": "https://doi.org/10.5281/zenodo.1417193",
        "ee": "https://zenodo.org/records/1417193/files/LiuHS17.pdf",
        "abstract": "Users in different countries may have different music pref- erences, possibly due to geographical, economic, linguis- tic, and cultural factors. Revealing the relationship be- tween music preference and cultural socio-economic dif- ferences across countries is of great importance for music information retrieval in a cross-country or cross-cultural context. Existing works are usually based on small sam- ples in one or several countries or take only one or two socio-economic aspects into account. To bridge the gap, this study makes use of a large-scale music listening dataset, LFM-1b with more than one billion music listen- ing logs, to explore possible associations between a variety of cultural and socio-economic measurements and artist preferences in 20 countries. From a big data perspective, the results reveal: 1) there is a highly uneven distribution of preferred artists across countries; 2) the linguistic differ- ences among these countries are positively associated with the distances in artist preferences; 3) country differences in three of the six cultural dimensions considered in this study have positive influences on the difference of artist preferences among the countries; and 4) geographical and economic distances among the countries have no signifi- cant relationship with their artist preferences.",
        "zenodo_id": 1417193,
        "dblp_key": "conf/ismir/LiuHS17",
        "keywords": [
            "music preference",
            "cultural socio-economic differences",
            "cross-country or cross-cultural context",
            "large-scale music listening dataset",
            "artist preferences",
            "20 countries",
            "linguistic differences",
            "country differences",
            "cultural dimensions",
            "geographical and economic distances"
        ],
        "content": "ARTIST PREFERENCES AND CULTURAL, SOCIO-ECONOMIC\nDISTANCES ACROSS COUNTRIES: A BIG DATA PERSPECTIVE\nMeijun Liu\nThe University of Hong Kong\nmeijun@connect.hku.hkXiao Hu\nThe University of Hong Kong\nxiaoxhu@hku.hkMarkus Schedl\nJohannes Kepler University Linz\nmarkus.schedl@jku.at\nABSTRACT\nUsers in different countries may have different music pref-\nerences, possibly due to geographical, economic, linguis-\ntic, and cultural factors. Revealing the relationship be-\ntween music preference and cultural socio-economic dif-\nferences across countries is of great importance for music\ninformation retrieval in a cross-country or cross-cultural\ncontext. Existing works are usually based on small sam-\nples in one or several countries or take only one or two\nsocio-economic aspects into account. To bridge the gap,\nthis study makes use of a large-scale music listening\ndataset, LFM-1b with more than one billion music listen-\ning logs, to explore possible associations between a variety\nof cultural and socio-economic measurements and artist\npreferences in 20 countries. From a big data perspective,\nthe results reveal: 1) there is a highly uneven distribution\nof preferred artists across countries; 2) the linguistic differ-\nences among these countries are positively associated with\nthe distances in artist preferences; 3) country differences\nin three of the six cultural dimensions considered in this\nstudy have positive inﬂuences on the difference of artist\npreferences among the countries; and 4) geographical and\neconomic distances among the countries have no signiﬁ-\ncant relationship with their artist preferences.\n1. INTRODUCTION\nProbing the relationship between cultural and socio-\neconomic difference and the cross-country difference in\nmusic preferences not only matters in music information\nretrieval (MIR), but also brings important cues to under-\nstand the difference in cultural and socio-economic aspects\namong countries. Against the background of the world’s\ndiversity in many cultural and socio-economic aspects, re-\nsearch aiming to uncover cross-country differences in the\nﬁeld of music recommendation and retrieval is seeing in-\ncreasing attention [22, 30]. It is widely acknowledged\nthat music information behavior is inherently a kind of\ncultural behavior, shaped by the culture and other socio-\neconomic factors [32, 56]. A growing body of literature\nc\rMeijun Liu, Xiao Hu, Markus Schedl. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Meijun Liu, Xiao Hu, Markus Schedl. “Artist Preferences\nand Cultural, Socio-economic Distances across Countries: A Big Data\nPerspective”, 18th International Society for Music Information Retrieval\nConference, Suzhou, China, 2017.demonstrated that different cultures or different countries\nhave disparity in music information behaviors, e.g. music\nretrieval, management and consumption, and music mood\njudgment [30, 39, 44]. This is also true in music prefer-\nences [25, 56]. In this case, a question naturally arises:\nwhich kind of cultural and socio-economic background\nmight possibly be responsible for the difference of mu-\nsic preferences among countries? It is thus necessary to\nhave an in-depth understanding of the differences in mu-\nsic preferences across different countries and of how these\ndifferences are mirrored by cultural and socio-economic\nfactors. Answers to these questions can facilitate con-\nstructing cross-cultural MIR systems, and promoting mu-\nsic recommendation and retrieval results by taking into ac-\ncount cultural and the socio-economic background of users\n[56]. Furthermore, this paper also contributes to improv-\ning the knowledge of the differences in customs, traditions,\ncultural values, and other socio-economic factors among\ncountries.\nExisting literature provides little evidence of the exact\nrelationships between the cross-country differences in a va-\nriety of cultural and socio-economic factors and those in\nartist preference. Furthermore, limited literature investi-\ngated which cultural dimension reﬂects the inter-country\ndifference in artist preferences. Even fewer previous stud-\nies were based on large-scale user-generated datasets. This\nsituation calls for more studies in this regard. Therefore,\nwe investigate in this paper the following research ques-\ntions:\nRQ1: How do artist preferences differ across countries?\nRQ2: Does the inter-country difference in artist prefer-\nences depend on the geographic, economic, linguistic, and\ncultural distances among countries?\nRQ3: Which cultural dimension can reﬂect the differ-\nence in artist preferences across countries?\nInspired by this research gap and the scientiﬁc impor-\ntance, this paper seeks to probe whether the differences\namong countries in music taste rely on any factors in the\ncultural and socio-economic dimensions, through applying\ndescriptive analysis, Kruskal-Wallis variance analysis and\nQuadratic Assignment Procedure (QAP) on a large dataset\nwith more than one billion listening records, the LFM-1b\ndataset [49]. To our knowledge, this is a ﬁrst work that\nexplores relationship between the inter-country difference\nin artist preferences and a variety of cultural and socio-\neconomic differences among countries.1032. RELATED WORK\nRelated work can be categorized into research that investi-\ngates the connection between music preferences and socio-\neconomic factors, and that between music preferences and\ncultural dimensions. A recent study analyzed the country-\nspeciﬁc music preferences. However, it did not investigate\nthe inﬂuential factors of music preferences [50].\nIn recent years, due to the availability of large-scale mu-\nsic listening data, users geospatial context for music rec-\nommendation has received increasing attention [53]. How-\never, there is limited literature on directly exploring the re-\nlationship between music preferences and geographic lo-\ncations. Before large-scale music listening datasets have\nbeen published, several works involved location-related in-\nformation. Researchers proposed a mobile music recom-\nmender system, Lifetrack, that enables a playlist based on\nthe location and other information in the users environ-\nment [47]. More recently, researchers found that draw-\ning on the information of listeners geographic location help\npromote music recommendation [43, 52]. Although some\nworks have been done, the authors suggested that combing\ncultural regions with geographical distances may better ex-\nplain differences in music taste [53].\nEconomic status seems to have a potential inﬂuence on\nmusical preferences. Cultural consumption is closely re-\nlated to individuals social status that in turn is directly in-\nterrelated with the amount of income. According to Bour-\ndieu’s class theory, high-status groups have more cultural\ncapital which is deﬁned as knowledge and appreciation of\nhighbrow culture, and the possession of high or low in-\ncome in people’s childhood tend to shape their taste [7].\nEmpirical evidence showed that the cultural taste of the\nhigh-status group is distinct from people in other classes.\nFor example, people belonging to the high-status group\nfrequently visit museums, classical concerts, the theater\nand so forth [15, 31, 33]. In the ﬁeld of music research,\nthere exist some evidence that support the connection be-\ntween income and music preferences as well. Cutler found\nthat preference for classical music tended to grow with\nincome [13]. Duncan, Herrington and Capella also sug-\ngested that the music taste of upper-income individuals is\ndifferent from their counterparts with low income or/and\nwith only high school education [18]. It has been found\nthat high socio-economic status positively impacts musi-\ncal openness that is related to the acceptance for diverse\nmusic [57].\nCulture is a well-discussed factor in music informa-\ntion research, compared to other socio-economic aspects.\nFrom the perspectives of sociology, psychology and behav-\nior science, researchers believe that general behaviors and\npreferences are shaped by culture [32]. In the ﬁeld of MIR,\nretrieval methods that consider cultural differences in mu-\nsic perception and consumption are highly desirable [39].\nIn recent years, taking cultural factors into account has be-\ncome a frequently-used strategy in MIR research to explore\nusers music need at the country level [23]. Researchers\nfound that preference for music mood altered signiﬁcantly\nbetween countries, implying that utilizing geographic in-formation of users could facilitate further studies [48].\nMore recently, it has been suggested that the country-based\ndiversity pattern of music listening is associated with some\ncultural dimensions presented in Hofstede’s theory on cul-\ntural dimensions [23]. Speciﬁcally, researchers found that\nusers in countries with high scores in the culture dimen-\nsion of power distance tended to show less diversity in the\nartists and genres they listened to. Oppositely, individu-\nalism dimension was negatively correlated with music di-\nversity. Furthermore, the correlation between long-term\norientation and artist diversity was considered negative.\nBased on small-sample data obtained from surveys, pre-\nvious studies provided more direct evidence to show the\ninﬂuence of language on listeners reactions to and com-\nments on music. Empirical results presented that there\nwas a signiﬁcantly positive correlation between familiar-\nity with a language and attitude toward the language in\nsongs [1]. Speciﬁcally, it was reported that some children\nresponded to foreign-language music with negative judg-\nment [38]. In a study which focused on language in the\ncontext of songs, it was observed that English-speaking\nstudents preferred pop songs performed in English to those\nwith Spanish or Chinese lyrics [2]. By examining under-\ngraduate non-music majors world music preferences, re-\nsearchers found that the breadth and length of studying\nforeign languages were related to a high degree of world\nmusic preferences [26].\nIn a nutshell, the cultural and socio-economic variables\nwe selected are thought of as potentially correlative factors\nof music preferences. However, the exact relationship be-\ntween these factors and music preferences under a cross-\ncountry context still remains unclear. First, current stud-\nies are limited to small samples collected from surveys or\nquestionnaires. Besides, such self-reported responses can\nbe subjective. In other words, there is scarce literature that\ninvestigates this research question using objective datasets\nin a large scale. Second, most existing studies simply in-\nclude one or two socio-economic factors, leaving many po-\ntentially relevant aspects unexplored. Third, extant studies\nignored the discussion of the association between socio-\neconomic factors and music taste in a context of multiple\ncountries or multiple cultures, since a majority of them\npaid attention to individuals in a single cultural environ-\nment. Fourth, among the few studies on relationship be-\ntween music preferences and socio-economic factors (e.g.\ngeographical location), the conclusions are often ambigu-\nous and indecisive. To bridge the gaps, this study aims\nto uncover the relationship between music preferences and\ncultural and socio-economic factors at the country level us-\ning a large-scale and user-generated dataset.\n3. DATA AND METHODOLOGY\n3.1 LFM-1b Dataset\nThis study uses the open dataset LFM-1b1[49]. This\ndataset includes more than one billion music listening\n1www.cp.jku.at/datasets/LFM-1b. The period during which the data\nwas collected ranges from January 2013 to August 2014.104 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 1 : Number of users in the 20 sample countries\n(top) and the continents where countries are located (bot-\ntom).(Country code: US: United States, RU: Russia, DE:\nGermany, UK: United Kingdom, PL:Poland, BR: Brazil,\nFI: Finland, NL: Netherlands, ES: Spain, SE: Sweden, UA:\nUkraine, CA: Canada, FR: France, AU: Australia, IT: Italy,\nJP: Japan, NO: Norway, MX: Mexico, CZ: Czech Repub-\nlish, BY: Belarus)\nevents created by 120,322 users and enables us to con-\nduct a large-scale analysis in music listening behaviors.\nIt is noteworthy that only 54.13% of users in the LFM-\n1b dataset provide information on their nationality and the\ndistribution of users across countries is very unbalanced.\nTo avoid possible negative effects on our analysis, we elim-\ninate countries with less than 1% of users in LFM-1b and\nonly use the remaining 20 countries in this study. Fi-\nnally, we obtained a dataset including 46,619 users with\n678,640,512 listening events that cover 2,259,103 unique\nartists.\nThe distribution of users in the sampling countries and\nwhich continent these countries belong to are shown in\nFigure 1. It indicates that most of them are located in\nEurope and America, with one country in Asia, Oceania,\nand South America respectively.\n3.2 Modeling Country-speciﬁc Diversity in Artist\nPreferences\nIn this study, we used the coefﬁcient of variation (CV) and\nGini coefﬁcient to measure and compare the diversity of\nartist preferences across the countries. Coefﬁcient of vari-\nation is a standardized measure of dispersion of the fre-\nquency distribution, which is deﬁned as the ratio of the\nstandard deviation to the average of a variable [20]. CV has\nbeen frequently used for comparing diversity or inequality\nin groups [3, 5]. The Gini index enables us to examine\nthe inequality of artist listening frequency in each coun-\ntry [46, 60]. We adapt the deﬁnition of the Gini coefﬁcient\nfor a country cto our task and calculate it as shown inEquation 1,\nGc=\f\f\f\f\f1 +1\nN\u00002\nm\u0002N2X\ni(N\u0000Oi+ 1)\u0001yi\f\f\f\f\f(1)\nwhereNis the number of artists listened to by users in\ncountryc;yiis the listening count of artist iin country\nc;Oiis the inverse rank of yiwhen sorting the values yi\nfor all artists iin countryc, andmis arithmetic mean of\nlistening counts across the Nartists.\nWe adopted the Kruskal-Wallis (KW) non-parametric\nanalysis of variance as the primary tool to probe whether\nthere is a signiﬁcant difference among countries in the fre-\nquency of artist listening. After performing the Shapiro-\nWilk test, it was observed that the data exhibited non-\nnormal distribution, and thus non-parametric analysis of\nvariance was adopted [19]. A follow-up test was carried\nout to ﬁnd out which pairs of countries have signiﬁcant dif-\nferences [9, 29, 54].\nTo avoid possible bias caused by the disequilibrium of\nlistening counts across countries, we also normalized the\nlistening frequency of each artist in a country against the\ntotal listening count of that country. In other words, we\nlook into not only the raw listening counts but also the nor-\nmalized listening count of each artist.\n3.3 Modeling Country Distances in terms of Artist\nPreferences, Cultural and Socio-economic Dimensions\nThedistance of artist preference among countries is the de-\npendent variable in this study. Based on the data of listen-\ning events in LFM-1b, we calculated the cosine distance\nof artist preferences among countries. Speciﬁcally, each\ncountry is represented by a vector of artists, with each di-\nmension of the vector being the number of times the cor-\nresponding artist was listened to by users in this country.\nThen, the cosine distance between each pair of vectors was\ncalculated. The results are shown in Figure 2. Notably, the\ndistances between Japan and all other countries are sub-\nstantially higher ( >0:5) than those between other pairs of\ncountries, making Japan an outlier, which is in line with\nprevious studies [51].\nIn this study, the cultural and socio-economic distance\nbetween countries is measured by the following aspects:\ngeographic, economic, linguistic, and cultural distance.\nGeographic distance is the geodetic distance between the\ncapital cities calculated by Vincenty’s equations and on the\nbasis of the latitude and longitude, i.e., the length of the\nshortest curve between two points along the surface of the\nEarth [58]. We deﬁne economic distance as the difference\nof gross domestic product (GDP) per capita between coun-\ntries, calculated based on the data obtained from World\nBank2. For linguistic distance between countries, we re-\ngard the language used by the largest population in a coun-\ntry as the main language in that country. On the website of\nthe Central Intelligence Agency (CIA),3the language and\n2http://databank.worldbank.org/data/home.aspx\n3https://www.cia.gov/library/publications/\nthe-world-factbook/fields/2098.htmlProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 105Figure 2 : Heat map of cosine distances among countries based on artist listening frequency\nthe size of its speakers in a country are obtained to identify\nthe main language in each of the sampling countries. Eth-\nnologue4provides the information of the global language\nfamily tree, based on which we calculated the linguistic\ndistance between two countries. Consistent with existing\nliterature [21, 37], the linguistic distance between two lan-\nguagesiandjis deﬁned in Equation 2,\nDij= 1\u0000\u0012jNi\\NJj\n1\n2\u0001(Ni+Nj)\u0013\f\n(2)\nwhereNidenotes the number of nodes in country i’s lan-\nguage tree,Njanalogously. The relative distance between\nlanguages which are both included in the same family\nhinges on the value of \f. According to the experience in\nother studies [21, 37], we set \f= 0:5. For example, in\nthe language family tree, English belongs to the follow-\ning branch: Indo-European >Germanic>West>En-\nglish while Swedish is classiﬁed into this branch: Indo-\nEuropean>Germanic>North Germanic >East Scan-\ndinavian>Continental Scandinavian >Swedish. The\ndistance between these two languages is approximately\n0:368in that they have four and six nodes separately, shar-\ning two common nodes. In order to quantify the cultural\ndistance between countries, we calculate the distance of\nscores between countries in each of Hofstede’s cultural di-\nmensions5[28]: Power distance index (PDI) refers to\nthe extent to which the less powerful members accept and\nexpect that power is distributed unequally; Individualism\n(IDV) deﬁnes the degree of preference for a loosely-knit or\ntightly-knit social framework; Masculinity (MAS) refers\nto the degree of preference for achievement, heroism, as-\nsertiveness, and material rewards for success; Uncertainty\navoidance (UAI) expresses the attitude of individuals to-\nwards uncertainty and ambiguity; Long-term orientation\n4https://www.ethnologue.com/\n5https://geert-hofstede.com/national-culture.\nhtml(LTO) describes to which degree a society ties the past with\nthe present and future actions or challenges; Indulgence\n(IND) measures the happiness of a society.\n3.4 Quadratic Assignment Procedure\nIn this study, we applied the Quadratic Assignment Proce-\ndure (QAP) [36, 55] via Double Dekker Semi-partialling\n[4, 14] to examine the relationship between distance of\nartist preference across countries, and geographic, eco-\nnomic, linguistic and cultural proximities across countries.\nIn other words, we explore whether the difference among\ncountries in artist listening has a relationship with their dif-\nferences in the aspects of geographic location, economy,\nlanguages and culture. The primary reason for using QAP\nin this study is to avoid biases caused by autocorrelation of\nerror in the dyadic dataset [55]. In this study, each obser-\nvation is a pair of countries (i.e., a dyad in network analysis\nterminologies). Dyads are non-independent because each\nnode in a dyad is connected to other dyads. Therefore, re-\ngression methods that assume independent distribution of\ndata such as ordinary least squares (OLS) regression would\nlead to biased estimators [4, 6, 41]. In contrast, QAP ex-\nplicitly takes into account dependence between dyads as\nwell as autocorrelation of errors in the dyadic dataset. It\nis frequently used in regression analyses on network and\nrelationship datasets [8, 10, 11, 16, 36]. The independent\nvariables are the six matrices of the between-country dis-\ntances in the six cultural dimensions, whereas the depen-\ndent variable is the matrix of inter-country distance on the\nartist preferences. We control the geographical distance\n(GEO), economic distance (ECO) and linguistic distance\n(LAN) among the countries. Besides, we calculated the\nmean variance inﬂation factor score (1.79), which is far\nlower than critical point 10, implying that multicolinearity\ncan be ignored in this study.106 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20174. RESULTS\n4.1 Differences among Countries in terms of Artist\nPreferences\nTable 1 presents statistics of the artist listening histories\nacross countries, including the average number of listening\nevents to an artist, the standard deviation (SD), coefﬁcient\nof variation (CV), the number of unique artists (Uniq.#)\nlistened to by listeners in each country and Gini coefﬁcient\n(Gini).\nAs can be seen from Table 1, users from the US and\nRussia listen to a large number of unique artists, far ex-\nceeding other countries. Furthermore, the high CV val-\nues for US, BR, RU, PL and UK imply that listeners from\nthese countries listen to a wider range of artists, compared\nto users from other countries.\nCountry Mean SD CV Uniq.# Gini\nUS 182.16 2930.63 16.09 747004 96.44%\nRU 114.46 1795.28 15.69 632460 96.38%\nDE 134.46 1746.64 12.99 474874 96.35%\nUK 127.68 1709.56 13.39 456456 95.39%\nPL 209.95 3280.39 15.62 362155 95.21%\nBR 203.11 3263.22 16.07 267186 95.15%\nNL 83.57 833.97 9.98 256895 94.53%\nUA 68.38 743.48 10.87 249287 93.98%\nSE 102.94 1095.51 10.64 229714 93.38%\nFI 114.41 1230.23 10.75 213645 93.10%\nFR 71.79 595.26 8.29 207878 93.05%\nCA 93.83 817 8.71 191728 92.97%\nES 82.3 717.39 8.72 190671 92.96%\nJP 63.44 548.04 8.64 185128 92.95%\nBY 51.78 469.26 9.06 166465 92.74%\nNO 78.52 669.54 8.53 165663 92.66%\nAU 89.02 759.82 8.54 164145 92.52%\nIT 81.28 783.94 9.64 156599 92.03%\nMX 73.17 753.87 10.3 144930 91.51%\nCZ 87.8 743.38 8.47 127726 91.15%\nMean 105.70 1274.32 11.05 279530 93.72%\nTable 1 : Statistics of artist listening frequency across sam-\nple countries ranked by the number of unique artists\nIn general, Gini indices are high for all countries, mean-\ning users’ preferences for an artist varied a lot. In partic-\nular, the inequality of artist listening is most noticeable in\nthe US, Brazil, Poland, and the UK, which is consistent\nwith the CV results.\nWhen comparing frequency of artist listening across\ncountries, the result of the Kruskal-Wallis test shows a\nstatistically signiﬁcant difference ( p < 0:01) among the\nsampling countries. After conducting a follow-up pairwise\ncomparison, we ﬁnd that signiﬁcant differences on artist\npreferences exist between all 190 country pairs, except for\nBR and AU, CZ and AU, CZ and BR, JP and CZ, MX and\nFR, PL and CA, RU and NO, SE and FI, SE and FR.\n4.2 QAP Correlation and Regression Results\nWe run two models to test the relationship between artist\npreference (as represented by artist listening frequencies)\ndistance among countries, and the geographical, economic,linguistic, and cultural distances among them. The QAP\ncorrelation coefﬁcients among the variables are reported in\nTable 2, and the regression results are in Table 3. For com-\nparison, only the control variables are included in model 1\nand we added the independent variables to model 2. The\nadjustedR2in the two models are signiﬁcant: 0.594 and\n0.643 in model 1 and 2, respectively. In other words, nearly\n59.4% of the variance in the matrix of the artist preference\ndistances among countries can be explained by their dis-\ntances in the geographic, economic and linguistic aspects;\nand 64.3% of the variance can be explained in model 2\nwith the addition of cultural distances.\nThe distance among countries in term of main lan-\nguages is positively associated with their distance in artist\npreferences ( r= 0.745 in Table 2). In model 2, the coef-\nﬁcient of linguistic distance among countries is signiﬁcant\nand positive ( \f= 0:68;p < 0:001). Furthermore, three\ndimensions of cultural distance among countries have pos-\nitive effects on their artist preference distance: masculin-\nity (\f= 0:13;p < 0:05), long-term orientation ( \f=0.12;\np< 0:01), and indulgence ( \f=0.14;p< 0:05).\nBesides, the regression results in both model 1 and\nmodel 2 reveal that economic distance has no signiﬁcant\nimpact on the artist preference distance on the country\nlevel. Geographic distance has a signiﬁcant impact on the\ndependent variable in model 1, but becomes insigniﬁcant\nwhen cultural distances are included in model 2.\n5. DISCUSSION\nWe summarize our main ﬁndings in the following. The\ndistribution of music listening behavior across artists is\nhighly uneven . In particular, substantial inequality of artist\npreferences is found in the US, Brazil, Poland, Russia, and\nthe UK. In comparing across the countries, there are signif-\nicant distinctions in artist preferences within most of coun-\ntry pairs.\nThedistance between the main languages used in coun-\ntries is positively associated with the distance in their artist\npreferences . This result could be attributed to the fact\nthat familiarity is a key factor that inﬂuences music prefer-\nence [17, 27]. Familiarity not only refers to having heard a\nmusic piece somewhere before, but can also be reﬂected by\nthe degree of familiarity with the language in the songs [1].\nListeners may be less familiar with music sung in lan-\nguages they know little about, and thus they may be less\nlikely to listen to that kind of music.\nAmong the six cultural dimensions, masculinity, long-\nterm orientation, and indulgence distances between coun-\ntries have positive correlations with their distances in\nartist preferences. First, masculinity indicates the degree\nto which a culture delineates gender roles, and a mascu-\nline culture clearly differentiates the social expectations on\nmales and females [42]. Previous literature pointed out that\na huge gender difference in both the expression and percep-\ntion of mood could be found in cultures high in masculin-\nity. Other researchers also demonstrated that masculinity\ncan explain the gender difference in personality traits [12].\nIt is generally agreed that music listening behavior andProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 107ARTIST GEO ECO LAN PDI IDV MAS UAI LTO\nARTIST 1\nGEO 0.248 1\nECO 0.122 -0.017 1\nLAN 0.745*** 0.066 0.118 1\nPDI 0.149 -0.034 0.516*** 0.211 1\nIDV 0.215 0.354* 0.37** 0.136 0.458** 1\nMAS 0.34* -0.056 0.102 0.317* 0.005 -0.106 1\nUAI 0.144 -0.101 0.352** 0.241* 0.566** 0.266* 0.12 1\nLTO 0.267** 0.266** 0.016 0.081 0.019 0.112 0.01 0.025 1\nIND 0.269* 0.112 0.334** 0.14 0.416** 0.326** -0.037 0.398** 0.346** 1\nTable 2 : QAP correlation coefﬁcients (Note: signiﬁcance levels: *: p< 0:05; **:p< 0:01; ***:p< 0:001)\nVariable Model 1 Model 2\nGEO 0.200* 0.133\nECO 0.040 0.003\nLAN 0.727*** 0.683***\nPDI -0.035\nIDV 0.063\nMAS 0.131*\nUAI -0.074\nLTO 0.122**\nIND 0.140*\nAdjusted R20.594 *** 0.642***\nN of Obs 380 380\nTable 3 : The QAP regression result. (Note that all coefﬁ-\ncients presented are standardized coefﬁcients. Signiﬁcance\nlevels: ***p< 0:001,**p< 0:01, *p< 0:05)\nemotion strongly interact with each other [34, 35]. More-\nover, the correlation between personality and music behav-\nior is documented in empirical studies [24, 45]. Conse-\nquently, it is possible that on the country level, the music\npreference difference and the cultural difference in mas-\nculinity interacted through the gender differences in terms\nof emotion and personality traits.\nSecond, prior studies offer evidence that people in coun-\ntries scoring low in long-term orientation have a lower\npreference for listening to diverse artists since they value\nsteadfastness and believe that traditions are to be honored\nand kept [23]. In other words, people in short-term ori-\nented cultures may prefer to listen to more traditional mu-\nsic, and their music listening behavior is possibly more\nconservative. Furthermore, it is recently found that indi-\nviduals in countries with long-term orientation tend to be\nmore patient [59]. This characteristics may not only inﬂu-\nence business activities, but also generate different listen-\ning behaviors across countries. For instance, in long-term\noriented countries, people may be more likely to have the\npatience to listen to slow and long music. Future studies\ncan further explore and test these hypotheses.\nThird, in countries scoring high on indulgence , people\ntend to have more freedom in controlling their daily lives\nand in choosing the way to enjoy life. Given that listening\nto music is often regarded as an important entertainmentactivity, the cultural difference in the dimension of indul-\ngence can possibly affect people’s choices of music, and\nthus bringing about the differences in music preferences\nacross countries.\nIn the ﬁnal regression model (model 2 in Table 3), there\nis no signiﬁcant association between geographical and eco-\nnomic distance on the music preference distance across\ncountries. Perhaps geographical distance is no longer\na barrier for people to access various music in today’s\nhighly connected information society. Therefore geoloca-\ntion plays a less signiﬁcant role in music preference com-\npared to linguistic and cultural differences among coun-\ntries. In terms of economic distance, although on the indi-\nvidual level, it is conﬁrmed in the literature [40] that mu-\nsic preferences vary by the income level, this seems ques-\ntionable on the country level. This discrepancy might be\nrelated to the correlation between people’s cultural behav-\niors and social status [40]. On an individual level, income\nis related to social status which in turn can inﬂuence one’s\nmusic preference. However, on the country level, people’s\nsocial status ranges a lot in any single country and has vir-\ntually no relationship with the GDP per capita of a country.\nConsequently, economic distance among countries cannot\nexplain differences in music preferences.\n6. CONCLUSION AND FUTURE WORK\nIn this study, we applied descriptive statistical analysis,\nKrusal-Wallis variance analysis, and Quadratic Assign-\nment Procedure on the LFM-1b dataset, to reveal the as-\nsociation between the distance of a variety of cultural and\nsocio-economic aspects among countries, and the cross-\ncountry difference in artist preference.\nFindings of this study contribute to the literature of mu-\nsic listening behaviors and preferences, particularly from\nthe cross-country perspective. By analyzing one of the\nlargest datasets in the ﬁeld, we aim to draw conclusions\nthat are representative and generalizable. Multiple factors\nin the cultural, linguistic, geographic, and economic as-\npects were analyzed, and the results can potentially help\ndesign new strategies of MIR systems in the cross-country\nand cross-cultural context. Future studies can compare\ncross-country differences on other facets of music such as\ngenre and mood.108 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20177. ACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their helpful com-\nments. This study is partially supported by the Austrian\nScience Fund (FWF): P25655 and by a seed research grant\nfunded by University of Hong Kong.\n8. REFERENCES\n[1] Carlos R Abril. Multicultural dimensions and their ef-\nfect on children’s responses to pop songs performed in\nvarious languages. Bulletin of the Council for Research\nin Music Education , pages 37–51, 2005.\n[2] Carlos R Abril and Patricia J Flowers. Attention,\npreference, and identity in music listening by mid-\ndle school students of different linguistic backgrounds.\nJournal of Research in Music Education , 55(3):204–\n219, 2007.\n[3] Arthur G Bedeian and Kevin W Mossholder. On the\nuse of the coefﬁcient of variation as a measure of diver-\nsity.Organizational Research Methods , 3(3):285–297,\n2000.\n[4] Geoffrey G Bell and Akbar Zaheer. Geography, net-\nworks, and knowledge ﬂow. Organization Science ,\n18(6):955–972, 2007.\n[5] RB Bendel, SS Higgins, JE Teberg, and DA Pyke.\nComparison of skewness coefﬁcient, coefﬁcient of\nvariation, and gini coefﬁcient as inequality measures\nwithin populations. Oecologia , 78(3):394–400, 1989.\n[6] Stephen P Borgatti, Martin G Everett, and Linton C\nFreeman. Ucinet for windows: Software for social net-\nwork analysis. 2002.\n[7] Pierre Bourdieu. Distinction: A social critique of the\njudgement of taste . Harvard University Press, 1984.\n[8] Val Burris. Interlocking directorates and political co-\nhesion among corporate elites 1. American Journal of\nSociology , 111(1):249–283, 2005.\n[9] Herve M Caci et al. Kwallis2: Stata module to perform\nkruskal-wallis test for equality of populations. Statisti-\ncal Software Components , 1999.\n[10] Junho H Choi, George A Barnett, and BUM-SOO\nCHON. Comparing world city networks: a network\nanalysis of internet backbone and air transport intercity\nlinkages. Global Networks , 6(1):81–99, 2006.\n[11] David T Connolly. An improved annealing scheme for\nthe qap. European Journal of Operational Research ,\n46(1):93–100, 1990.\n[12] Paul Costa Jr, Antonio Terracciano, and Robert R Mc-\nCrae. Gender differences in personality traits across\ncultures: robust and surprising ﬁndings., 2001.\n[13] B Cutler. North american demographics. American De-\nmographics , 1992.[14] David Dekker, David Krackhardt, and Tom AB Sni-\njders. Sensitivity of mrqap tests to collinearity and\nautocorrelation conditions. Psychometrika , 72(4):563–\n581, 2007.\n[15] Paul DiMaggio and John Mohr. Cultural capital, ed-\nucational attainment, and marital selection. American\njournal of sociology , 90(6):1231–1261, 1985.\n[16] Michael Dreiling and Derek Darves. Corporate unity\nin american trade policy: A network analysis of\ncorporate-dyad political action 1. American Journal of\nSociology , 116(5):1514–63, 2011.\n[17] Kevin Droe. Music preference and music education: A\nreview of literature. Update: Applications of Research\nin Music Education , 24(2):23–32, 2006.\n[18] J Duncan Herrington and Louis M Capella. Practical\napplications of music in service settings. Journal of\nServices Marketing , 8(3):50–65, 1994.\n[19] Alan C Elliott and Linda S Hynan. A sas R\rmacro im-\nplementation of a multiple comparison post hoc test for\na kruskal–wallis analysis. Computer methods and pro-\ngrams in biomedicine , 102(1):75–80, 2011.\n[20] BS Everitt. The cambridge dictionary of statistics\ncambridge university press. Cambridge, UK Google\nScholar , 1998.\n[21] James D Fearon. Ethnic and cultural diversity by coun-\ntry.Journal of Economic Growth , 8(2):195–222, 2003.\n[22] Bruce Ferwerda and Markus Schedl. Investigating the\nrelationship between diversity in music consumption\nbehavior and cultural dimensions: A cross-country\nanalysis. In Proc. of the 1st Workshop on SOAP , 2016.\n[23] Bruce Ferwerda, Andreu Vall, Marko Tkalcic, and\nMarkus Schedl. Exploring music diversity needs across\ncountries. In Proceedings of the 2016 Conference on\nUser Modeling Adaptation and Personalization , pages\n287–288. ACM, 2016.\n[24] Bruce Ferwerda, Emily Yang, Markus Schedl, and\nMarko Tkalcic. Personality traits predict music taxon-\nomy preferences. In Proceedings of the 33rd Annual\nACM Conference Extended Abstracts on Human Fac-\ntors in Computing Systems , pages 2241–2246. ACM,\n2015.\n[25] Flavio Figueiredo, Bruno Ribeiro, Christos Faloutsos,\nNazareno Andrade, and Jussara M Almeida. Mining\nonline music listening trajectories. In ISMIR , pages\n688–694, 2016.\n[26] C Victor Fung. Undergraduate nonmusic majors’ world\nmusic preference and multicultural attitudes. Journal\nof Research in Music Education , 42(1):45–57, 1994.\n[27] Russell P Getz. The effects of repetition on listen-\ning response. Journal of Research in Music Education ,\n14(3):178–192, 1966.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 109[28] Geert Hofstede. Cultures and organizations. intercul-\ntural cooperation and its importance for survival. soft-\nware of the mind. London: Mc Iraw-Hill , 1991.\n[29] DG Housley, KA Berube, TP Jones, S Anderson,\nFD Pooley, and RJ Richards. Pulmonary epithelial re-\nsponse in the rat lung to instilled montserrat respirable\ndusts and their major mineral components. Occupa-\ntional and environmental medicine , 59(7):466–472,\n2002.\n[30] Xiao Hu and Jin Ha Lee. A cross-cultural study of mu-\nsic mood perception between american and chinese lis-\nteners. In ISMIR , pages 535–540. Citeseer, 2012.\n[31] V olker Kirchberg. Museum visitors and non-visitors in\ngermany: A representative survey. Poetics , 24(2):239–\n258, 1996.\n[32] Shinobu Kitayama and Hyekyung Park. Cultural shap-\ning of self, emotion, and well-being: How does it\nwork? Social and Personality Psychology Compass ,\n1(1):202–222, 2007.\n[33] Wim Knulst and Gerbert Kraaykamp. Trends in leisure\nreading: Forty years of research on reading in the\nnetherlands. Poetics , 26(1):21–41, 1998.\n[34] Stefan Koelsch. Brain correlates of music-evoked emo-\ntions. Nature Reviews Neuroscience , 15(3):170–180,\n2014.\n[35] Stefan Koelsch, Thomas Fritz, Karsten M ¨uller, An-\ngela D Friederici, et al. Investigating emotion with mu-\nsic: an fmri study. Human brain mapping , 27(3):239–\n250, 2006.\n[36] David Krackardt. Qap partialling as a test of spurious-\nness. Social networks , 9(2):171–186, 1987.\n[37] David D. Laitin and Rajesh Ramachandran. Language\npolicy and human development. American Political\nScience Review , 110(3):457–480, 2016.\n[38] Albert LeBlanc. Effects of style, tempo, and perform-\ning medium on children’s music preference. Journal of\nResearch in Music Education , 29(2):143–156, 1981.\n[39] Jin Ha Lee, J Stephen Downie, and Sally Jo Cunning-\nham. Challenges in cross-cultural/multilingual music\ninformation seeking. 2005.\n[40] Lawrence W Levine. Highbrow/lowbrow , volume\n1986. Harvard University Press, 1988.\n[41] Bing Liu, Songshan Sam Huang, and Hui Fu. An ap-\nplication of network analysis on tourist attractions: The\ncase of xinjiang, china. Tourism Management , 58:132–\n141, 2017.\n[42] David Matsumoto. Cultural inﬂuences on the percep-\ntion of emotion. Journal of Cross-Cultural Psychology ,\n20(1):92–105, 1989.[43] Joshua L Moore, Thorsten Joachims, and Douglas\nTurnbull. Taste space versus the world: an embedding\nanalysis of listening habits and geography. In ISMIR ,\npages 439–444, 2014.\n[44] Esa Nettamo, Mikko Nirhamo, and Jonna H ¨akkil ¨a. A\ncross-cultural study of mobile music: retrieval, man-\nagement and consumption. In Proceedings of the 18th\nAustralia conference on Computer-Human Interaction:\nDesign: Activities, Artefacts and Environments , pages\n87–94. ACM, 2006.\n[45] Jodi L Pearson and Stephen J Dollinger. Music prefer-\nence correlates of jungian types. Personality and indi-\nvidual differences , 36(5):1005–1008, 2004.\n[46] Martin Ravallion. Income inequality in the developing\nworld. Science , 344(6186):851–855, 2014.\n[47] Sasank Reddy and Jeff Mascia. Lifetrak: music in\ntune with your life. In Proceedings of the 1st ACM in-\nternational workshop on Human-centered multimedia ,\npages 25–34. ACM, 2006.\n[48] Markus Schedl. Leveraging microblogs for spatiotem-\nporal music information retrieval. In European Con-\nference on Information Retrieval , pages 796–799.\nSpringer, 2013.\n[49] Markus Schedl. The LFM-1b Dataset for Music Re-\ntrieval and Recommendation. In Proceedings of the\nACM International Conference on Multimedia Re-\ntrieval (ICMR) , New York, USA, April 2016.\n[50] Markus Schedl. Investigating country-speciﬁc music\npreferences and music recommendation algorithms\nwith the lfm-1b dataset. International Journal of Mul-\ntimedia Information Retrieval , 6(1):71–84, 2017.\n[51] Markus Schedl. Investigating country-speciﬁc music\npreferences and music recommendation algorithms\nwith the LFM-1b dataset. International Journal of\nMultimedia Information Retrieval , 6(1):71–84, 2017.\n[52] Markus Schedl and Dominik Schnitzer. Hybrid re-\ntrieval approaches to geospatial music recommenda-\ntion. In Proceedings of the 36th international ACM SI-\nGIR conference on Research and development in infor-\nmation retrieval , pages 793–796. ACM, 2013.\n[53] Markus Schedl, Andreu Vall, and Katayoun Farrahi.\nUser geospatial context for music recommendation in\nmicroblogs. In Proceedings of the 37th international\nACM SIGIR conference on Research & development in\ninformation retrieval , pages 987–990. ACM, 2014.\n[54] Sidney Siegel. Castellan. nonparametric statistics for\nthe social sciences, 1988.\n[55] William B. Simpson. Qap: The quadratic assignment\nprocedure. North American Stata Users Group Meet-\ning, 2001.110 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[56] Marcin Skowron, Florian Lemmerich, Bruce Ferw-\nerda, and Markus Schedl. Predicting genre preferences\nfrom cultural and socio-economic factors for music re-\ntrieval. In European Conference on Information Re-\ntrieval , pages 561–567. Springer, 2017.\n[57] Koen Van Eijck. Social differentiation in musical taste\npatterns. Social forces , 79(3):1163–1185, 2001.\n[58] Thaddeus Vincenty. Direct and inverse solutions of\ngeodesics on the ellipsoid with application of nested\nequations. Survey review , 23(176):88–93, 1975.\n[59] Mei Wang, Marc Oliver Rieger, and Thorsten Hens.\nHow time preferences differ: Evidence from 53 coun-\ntries. Journal of Economic Psychology , 52:115–135,\n2016.\n[60] Sean D Young, Caitlin Rivers, and Bryan Lewis. Meth-\nods of using real-time social media technologies for\ndetection and remote monitoring of hiv outcomes. Pre-\nventive medicine , 63:112–115, 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 111"
    },
    {
        "title": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music.",
        "author": [
            "Steven Losorelli",
            "Duc T. Nguyen",
            "Jacek P. Dmochowski",
            "Blair Kaneshiro"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417917",
        "url": "https://doi.org/10.5281/zenodo.1417917",
        "ee": "https://zenodo.org/records/1417917/files/LosorelliNDK17.pdf",
        "abstract": "Understanding human perception of music is foundational to many research topics in Music Information Retrieval (MIR). While the field of MIR has shown a rising interest in the study of brain responses, access to data remains an obstacle. Here we introduce the Naturalistic Music EEG Dataset—Tempo (NMED-T), an open dataset of electro- physiological and behavioral responses collected from 20 participants who heard a set of 10 commercially available musical works. Song stimuli span various genres and tem- pos, and all contain electronically produced beats in du- ple meter. Preprocessed and aggregated responses include dense-array EEG and sensorimotor synchronization (tap- ping) responses, behavioral ratings of the songs, and basic demographic information. These data, along with illustra- tive analysis code, are published in Matlab format. Raw EEG and tapping data are also made available. In this pa- per we describe the construction of the dataset, present re- sults from illustrative analyses, and document the format and attributes of the published data. This dataset facilitates reproducible research in neuroscience and cognitive MIR, and points to several possible avenues for future studies on human processing of naturalistic music.",
        "zenodo_id": 1417917,
        "dblp_key": "conf/ismir/LosorelliNDK17",
        "keywords": [
            "Understanding human perception of music",
            "Music Information Retrieval (MIR)",
            "Brain responses",
            "Naturalistic Music EEG Dataset—Tempo (NMED-T)",
            "Electronically produced beats",
            "Commercially available musical works",
            "Song stimuli",
            "Behavioral ratings",
            "Demographic information",
            "Preprocessed and aggregated responses"
        ],
        "content": "NMED-T: A TEMPO-FOCUSED DATASET OF CORTICAL AND\nBEHA VIORAL RESPONSES TO NATURALISTIC MUSIC\nSteven Losorelli1,2Duc T. Nguyen1,2Jacek P. Dmochowski3Blair Kaneshiro1,2\n1Center for the Study of Language and Information, Stanford University, USA\n2Center for Computer Research in Music and Acoustics, Stanford University, USA\n3Department of Biomedical Engineering, City College of New York, USA\nfslosorelli, dtn006, blairbo g@stanford.edu jdmochowski@ccny.cuny.edu\nABSTRACT\nUnderstanding human perception of music is foundational\nto many research topics in Music Information Retrieval\n(MIR). While the ﬁeld of MIR has shown a rising interest\nin the study of brain responses, access to data remains an\nobstacle. Here we introduce the Naturalistic Music EEG\nDataset—Tempo (NMED-T), an open dataset of electro-\nphysiological and behavioral responses collected from 20\nparticipants who heard a set of 10 commercially available\nmusical works. Song stimuli span various genres and tem-\npos, and all contain electronically produced beats in du-\nple meter. Preprocessed and aggregated responses include\ndense-array EEG and sensorimotor synchronization (tap-\nping) responses, behavioral ratings of the songs, and basic\ndemographic information. These data, along with illustra-\ntive analysis code, are published in Matlab format. Raw\nEEG and tapping data are also made available. In this pa-\nper we describe the construction of the dataset, present re-\nsults from illustrative analyses, and document the format\nand attributes of the published data. This dataset facilitates\nreproducible research in neuroscience and cognitive MIR,\nand points to several possible avenues for future studies on\nhuman processing of naturalistic music.\n1. INTRODUCTION\nHumans possess a unique ability to process music, and\nmany topics in Music Information Retrieval (MIR) involve\ncomputational modeling of human perception. Tasks that\nhumans often perform with ease—such as melody extrac-\ntion, beat detection, and artist identiﬁcation—remain open\ntopics in MIR. At the same time, a full understanding of\nthe cognitive and perceptual processes underlying human\nprocessing of music has yet to be reached.\nGreater cross-disciplinary collaboration between MIR\nand neuroscience has been proposed [14], and a number\nof studies have incorporated approaches from both ﬁelds.\nc\rSteven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski,\nand Blair Kaneshiro. Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Steven Losorelli,\nDuc T. Nguyen, Jacek P. Dmochowski, and Blair Kaneshiro. “NMED-T:\nA Tempo-Focused Dataset of Cortical and Behavioral Responses to Nat-\nuralistic Music”, 18th International Society for Music Information Re-\ntrieval Conference, Suzhou, China, 2017.For example, neural correlates of short- and long-term fea-\ntures introduced in MIR for genre classiﬁcation [34] have\nbeen sought [1, 6, 10, 20], and brain responses have been\nused in MIR-related applications including tempo estima-\ntion [29, 30] and emotion recognition [5, 21]. Yet even as\nbrain data become more prevalent in MIR research, ex-\nperimental design, data collection, and data cleaning can\npresent challenges [14]. Therefore, the research com-\nmunity can arguably beneﬁt from curated, ready-to-use\ndatasets of brain responses to real-world musical works.\nAiming to provide an open dataset with which sev-\neral MIR and neuroscience topics can be explored, we\nintroduce the Naturalistic Music EEG Dataset—Tempo\n(NMED-T), a dataset of EEG and behavioral responses to\ncommercially available musical works. The dataset con-\ntains dense-array EEG responses from 20 adult participants\nwho listened to 10 full-length songs, as well as tapped re-\nsponses to the beat of shorter excerpts (collected in a sep-\narate listen). These responses have been cleaned and ag-\ngregated, and are ready to use in Matlab format along with\nratings of familiarity and enjoyment, as well as basic de-\nmographic information about the participants.\nNMED-T contributes to a growing body of publicly\navailable music-related EEG repositories, including the\nDEAP [17], Music BCI [32], NMED-H [15], and Open-\nMIIR [31] datasets. It is well suited for MIR research in\nthat the data are cleaned and ready to use but are also made\navailable in raw form; stimuli are complete, naturalistic1\nmusical works spanning a wide range of tempos; metadata\nlinks to stimulus audio are provided; and behavioral data\nare included. Moreover, as EEG was recorded while par-\nticipants listened attentively but did not focus on any par-\nticular dimension of the songs, these data are suitable for\nstudying many aspects of music processing.\nThe remainder of the paper is structured as follows. In\nx2 we describe stimulus selection, study design, data col-\nlection, and data preprocessing. Illustrative analyses of the\npreprocessed data, which build upon past music perception\nand MIR approaches and reveal cortical and behavioral ev-\nidence of entrainment to musical beat, are presented in x3.\nInx4 we document the dataset itself. We conclude and\ndiscuss potential future uses of the data in x5.\n1Denoting real-world music—i.e., music that was created to be con-\nsumed in everyday life, as opposed to controlled stimuli created for ex-\nperimental research.3392. METHODS\n2.1 Stimuli\n2.1.1 Stimulus Selection\nAs the present dataset is focused on naturalistic music and\ntempo, stimuli were ecologically valid, real-world musical\nworks containing steady, electronically produced beats in\nduple meter at a variety of tempos. The 10 selected songs\nare all 4:30–5:00 in length, contain vocals (all but one in\nEnglish), and are in the Western musical tradition. Song\ninformation is summarized in Table 1.\nTo aid in song selection, we computed objective mea-\nsures of tempo using publicly available Matlab code [8].\nThe computed tempos were then validated perceptually by\nfour trained musicians. The ﬁnal set of selected songs\nrange in tempo from 56–150 BPM—a wide enough range\nto potentially explore octave errors [11, 35]. To facilitate\nfurther research on the audio as well as the responses, we\npurchased digital versions of all songs from Amazon, and\ninclude in Table 1 each song’s Amazon Standard Identiﬁ-\ncation Number (ASIN).\nThese real-world stimuli are complex and contain en-\nergy at various frequencies—not just those directly re-\nlated to the beat. We followed the approach of Nozaradan\net al. [27] and visualized low-frequency spectra of the stim-\nuli. We extracted the amplitude envelope of each song us-\ning the MIR Toolbox, version 1.5 [18] at a sampling rate of\n125 Hz (the sampling rate of the preprocessed EEG), and\nplotted magnitude spectra up to 15 Hz. As can be seen in\nFig. 1, spectral peaks often occur at harmonics and subhar-\nmonics of the beat—implicating the hierarchical timescale\nof music—as well as at other frequencies.\n2.1.2 Stimulus Preparation\nTo prepare the stimuli for the EEG experiment, full-length\nsongs were ﬁrst converted to mono using Audacity, ver-\nsion 2.1.2.2We then embedded the second audio channel\nwith an intermittent click that was transmitted directly to\nthe EEG ampliﬁer (not played to participants) to ensure\nprecise time stamping of the stimuli. For the behavioral\nexperiment, we created 35-second excerpts of each song.\nUsing Audacity, we selected the audio from 1:00–1:34 and\napplied a linear fade-in and fade-out to the ﬁrst and last\n2 seconds, respectively. We then appended 1 second of si-\nlence to make the conclusion of each excerpt more obvious\nto the participant.\n2.2 Participants\nTwenty right-handed participants, aged 18–29 years (mean\nage 23 years, 6 female) participated in the experiment. All\nreported normal hearing, ﬂuency in English, and no cogni-\ntive or decisional impairments. We imposed no eligibility\ncriteria related to formal musical training; 17 participants\nreported having received training (mean 8.4 years among\nthose with training). Participants reported listening to mu-\nsic for 14.5 hours per week on average.\n2http://www.audacityteam.org2.3 Experimental Speciﬁcations & Data Collection\nThis study was approved by the Stanford University In-\nstitutional Review Board. All participants provided writ-\nten informed consent before participating. Each partici-\npant ﬁlled out a general demographic and musical back-\nground questionnaire, after which the EEG and tapping\nblocks were completed, with the EEG block always oc-\ncurring ﬁrst.\n2.3.1 EEG Experiment\nFirst, each participant was informed that the general pur-\npose of the experiment was to study human processing of\nmusic, and that he or she would be completing an EEG ses-\nsion and a behavioral test. As the EEG data were collected\nfor the general study of music processing (not limited to\nbeat perception), no explicit mention of beat or tempo was\ngiven at this stage of the experiment. Rather, participants\nwere instructed simply to listen attentively to the songs as\nthey played, and to avoid movement of any kind (includ-\ning stretching, yawning, and tapping or moving to the beat)\nduring the trials. Songs were presented in random order.\nFollowing each trial, participants delivered ratings of fa-\nmiliarity and enjoyment for the song just presented, on a\nscale of 1–9. The EEG experiment was split into two con-\nsecutive recording blocks in order to mitigate participant\nfatigue, limit data size of the EEG recordings, and allow\nfor veriﬁcation of electrode impedances between record-\nings. Therefore, a total of 40 EEG recordings were col-\nlected across the 20 participants.\nThe EEG experiment was programmed in Matlab ver-\nsion 2013b3with a custom template built on the Psy-\nchophysics Toolbox, version 3 [4]. Each participant sat\ncomfortably in a chair at a desk for the duration of the\nexperiment. Stimuli were presented through magnetically\nshielded Genelec 1030A speakers at a measured loudness\nlevel between 73–78 dB. During the trials, the participant\nviewed a ﬁxation image presented on a computer monitor\nlocated 57 cm in front of him or her.\nDense-array EEG was recorded using the Electrical\nGeodesics, Inc. (EGI) GES300 system [33]. Data were\nrecorded from 128 electrodes with vertex reference using\nan EGI Net Amps 300 ampliﬁer and Net Station 4.5.7\nacquisition software, sampled at 1 kHz with a range of\n24 bits. Electrode impedances were veriﬁed to be no\ngreater than 50k\n—an appropriate level for this system—\nat the start of each recording.\n2.3.2 Behavioral Experiment\nFollowing the EEG recordings, the electrode net was re-\nmoved from the participant, and the behavioral test began.\nHere, each participant listened to the 35-second song ex-\ncerpts, after receiving instructions to “tap to the steady beat\nof the song as you perceive it.” If the participant had ques-\ntions about tapping to multiple tempos for a given song, he\nor she was instructed to tap to the steady beat that best re-\nﬂected his or her perception of it in the moment. Excerpts\nwere presented in random order.\n3https://www.mathworks.com340 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017#Song Title Artist ASIN Tempo (BPM) Tempo (Hz) min:sec\n1“First Fires” Bonobo B00CJE73J6 55.97 0.9328 4:38\n2“Oino” LA Priest B00T4NHS2W 69.44 1.1574 4:31\n3“Tiptoes” Daedelus B011SAZRLC 74.26 1.2376 4:36\n4“Careless Love” Croquet Club B06X9736NJ 82.42 1.3736 4:54\n5“Lebanese Blonde” Thievery Corporation B000SF16MI 91.46 1.5244 4:49\n6“Canop ´ee” Polo & Pan B01GOL4IB0 96.15 1.6026 4:36\n7“Doing Yoga” Kazy Lambist B01JDDVIQ4 108.70 1.8116 4:52\n8“Until the Sun Needs to Rise” R ¨uf¨us du Sol B01APT6JKA 120.00 2.0000 4:52\n9“Silent Shout” The Knife B00IMN40O4 128.21 2.1368 4:54\n10 “The Last Thing You Should Do” David Bowie B018GS2A46 150.00 2.5000 4:58\nTable 1 . Stimulus set. Songs were selected on the basis of vocals, electronically produced beats, genre, tempo, and length.\nFigure 1 . Low-frequency magnitude spectra of stimulus amplitude envelopes. Frequencies related to the musical beat\nhierarchy, from 1=4xthe tempo (whole notes) to 8xthe tempo (32nd notes) are denoted with vertical dashed lines.\nTapping responses were collected using Tap-It, an iOS\napplication that plays audio while simultaneously record-\ning responses tapped on the touchscreen [16]. We note\na tap-to-timestamp latency of approximately 15 msec\n(st. dev. 5 msec) [16]. An Apple iPad 2 was used for this\nexperiment, with stimuli delivered at a comfortable listen-\ning level using over-ear Sony MDR-V6 headphones.\n2.4 Data Preprocessing\nAll data preprocessing and analysis was conducted using\nMatlab, versions 2013b and 2016b.\n2.4.1 EEG Preprocessing\nThe following preprocessing steps were performed on in-\ndividual EEG recordings that had been exported from\nNet Station to Matlab cell arrays. First, data from\neach electrode in the electrodes-by-time data matrix were\nzero-phase ﬁltered using 8th-order Butterworth highpass\n(0.3 Hz) and notch (59–61 Hz) ﬁlters, and a 16th-order\nChebyshev Type I lowpass (50 Hz) ﬁlter. Following this,\nthe ﬁltered data were temporally downsampled by a factor\nof 8 to a ﬁnal sampling rate of 125 Hz.\nWe extracted trial labels, onsets, and behavioral rat-\nings, and corrected the stimulus onset times using the click\nevents sent directly from the audio to the EEG ampliﬁer.\nThe data for each trial were epoched, concatenated, and\nDC corrected (subtracting from each electrode its median\nvalue). Bad electrodes were removed from the data ma-\ntrix, resulting in a reduction in the number of rows. We\ncomputed EOG components for tracking vertical and hori-\nzontal eye movements, and retained electrodes 1–124 for\nfurther analysis, excluding electrodes on the face. Weapplied a validated approach using Independent Compo-\nnents Analysis (ICA) to remove ocular and cardiac artifacts\nfrom the data [2, 13] using the runica function from the\nEEGLAB toolbox [7].\nAs ﬁnal preprocessing steps, transients exceeding 4\nstandard deviations of each electrode’s mean power were\nidentiﬁed in an iterative fashion and replaced with NaNs.\nWe then reconstituted missing rows corresponding to pre-\nviously identiﬁed bad electrodes with rows of NaNs, en-\nsuring that each data matrix contained the same number\nof rows. We appended a row of zeros—representing the\nvertex reference—and converted the data frame to average\nreference (subtracting from each electrode the mean of all\nelectrodes). All missing values (NaNs) were imputed with\nthe spatial average of data from neighboring electrodes,\nand a ﬁnal DC correction was performed. Finally, the\nepochs were separated once again into single trials. There-\nfore, after preprocessing, each recording produced a cell\narray of EEG data, each element of which contained an\nelectrodes-by-time matrix of size 125\u0002T, where Tvaried\naccording to the length of the stimulus.\nAfter preprocessing all recordings, we aggregated the\ndata on a per-song basis. The data frame for each song is\nthus a 3D electrodes-by-time-by-participant matrix of size\n125\u0002T\u000220.\n2.4.2 Preprocessing of Tapping Responses\nThe Tap-It application stores the timestamps of taps, in\nseconds, measured from the device touchscreen on a per-\ntrial basis, as well as each participant’s randomized stim-\nulus ordering array [16]. We aggregated the tapping re-\nsponses in a cell array and the ordering arrays in a matrix.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 3413. ILLUSTRATIVE ANALYSES\nThe following analyses are presented to illustrate basic\nproperties of the dataset.\n3.1 EEG Responses\nOne approach to studying beat processing using EEG in-\nvolves low-frequency ( \u001420Hz) steady-state evoked po-\ntentials (SS-EPs). In an SS-EP paradigm, stimuli presented\n(e.g., ﬂashed or sounded) at a particular frequency elicit\nbrain responses at that same frequency. While SS-EPs are\nmore often used to study vision processing [25], the ap-\nproach has in recent years been used to study responses to\nauditory rhythms. Here, SS-EPs have shown evidence of\nentrainment to musical beat, peaking at beat- and meter-\nrelated frequencies even when metrical accents are imag-\nined [26] or when beat frequencies do not dominate low-\nfrequency stimulus spectra [27]. To our knowledge, mu-\nsic SS-EP studies have to date used simple, synthesized\nrhythmic patterns as stimuli. Our ﬁrst illustrative analysis\nextends this approach to complex, naturalistic music.\nSpatial ﬁltering is a technique for EEG analysis\nwhereby a weighted sum of electrodes is computed sub-\nject to some criterion [3]. Advantages of concentrating ac-\ntivity of interest from many electrodes to a few spatially\nﬁltered components include dimensionality reduction, im-\nproved SNR, and a reduction in multiple comparisons. For\nthe present analysis we consider two simple spatial ﬁlters.\nThe ﬁrst is simply the mean across all electrodes (ME),\nwhich can be thought of as a constant weight applied to\neach electrode. For the second, we perform Principal Com-\nponents Analysis (PCA), and analyze the ﬁrst PC of data.\nWe ﬁrst averaged each song’s 3D electrodes-by-time-\nby-participant matrix across participants, producing an\nelectrodes-by-time matrix for each song. Then, so that we\nanalyzed the same amount of data for each song and to\naccount for the time course of listener entrainment to the\nbeat [9], we retained 4 minutes of data from each song,\nstarting 15 seconds into the song.\nTo compute the spatial ﬁlters, we concatenated the\nparticipant-averaged data frames across all songs, produc-\ning an electrodes-by-aggregated-time matrix. Then, for the\nME spatial ﬁlter, we computed the mean across electrodes,\nwhile for the PCA ﬁlter we computed electrode weightings\nfor PC1 using Singular Value Decomposition (SVD). Fi-\nnally, we reshaped each resulting song-concatenated com-\nponent vector into a songs-by-time matrix. As our current\ninterest is on SS-EPs, we present the magnitude spectrum\nof each component on a per-song basis.\nThe SS-EPs are shown in Fig. 2; y-axis scaling is con-\nsistent within each spatial ﬁltering technique. By inspec-\ntion of the plots, low frequencies ( <15 Hz) of ME spectra\noccasionally contain peaks at frequencies in the musical\nbeat hierarchy (e.g., Song 5). PC1 performs better, elic-\niting more robust spectral peaks at beat-related frequen-\ncies. Moreover, EEG PC1 appears to peak at frequencies\ndirectly related to musical beat, while suppressing many of\nthe other spectral peaks that were observed in the magni-\ntude spectra of stimulus amplitude envelopes (Fig. 1).Spatial ﬁlters can be visualized by projecting the ﬁlter\nweights on a 2D scalp topography. While it is common\nto convert the spatial ﬁlter weights to a so-called “forward\nmodel,” which captures the projection of ﬁltered activity\non the scalp, for PCA the spatial ﬁlter is equivalent to the\nforward model [28]. The ME ﬁlter, applying a constant\nweight to all electrodes, would reveal no spatial variation.\nHowever, the PC1 ﬁlter topography (Fig. 2, bottom right)\napplies a range of positive and negative weights to the elec-\ntrodes, which may help to explain why this ﬁlter produces\nmore prominent spectral peaks at beat frequencies.\n3.2 Behavioral Ratings\nParticipant ratings of familiarity and enjoyment are shown\nin Fig. 3. Familiarity with the songs was low overall; rat-\nings of enjoyment tended to be higher, and also varied\nmore across participants.\n3.3 Tapped Responses\nFor each trial of tapping data, we ﬁrst converted each inter-\ntap interval to an instantaneous measure of tempo in Hz,\nmapped it to the midpoint of the interval, and then linearly\ninterpolated the result to a consistent timing grid with a\ntemporal resolution of 2 Hz. We analyze and plot data from\na 17-second interval starting 15 seconds into the excerpt\n(i.e., starting at time 1:15 in the complete song).\nThe aggregate tapping responses are shown in Fig. 4.\nWe present two visualizations of these results. First, the\ntop ﬁgure for each song shows instantaneous tempo over\nthe time of the excerpt for individual participants (gray\ncurves), with the median across participants plotted in\nblack. In bottom ﬁgures, we compute the median tempo\nacross time for each individual participant, and summa-\nrize with histograms. Beat-related frequencies are shown\nin the orange (1/2 xtempo frequency), green (tempo fre-\nquency), and red (2 xtempo frequency) lines. To a large\nextent, participants tended to tap at what we had previously\ndetermined to be the tempo frequency. However, there\nare cases of lower agreement, particularly for the slowest\nsongs (Song 1 and Song 2). Here, the histograms suggest\na nearly bimodal distribution of tapped tempos, split be-\ntween the computational measure and twice that, with the\nhigher measure lying closer to what is considered the pre-\nferred tempo region for humans [23].\n4. PUBLISHED DATASET\nWe publish the cleaned EEG data, aggregated behavioral\nratings, aggregated tapped responses, and basic demo-\ngraphic data about the participants in Matlab .mat for-\nmat. Example code and helper functions for the illustrative\nanalyses are provided, also in Matlab format. Finally, we\npublish raw EEG recordings (for researchers who wish to\napply their own preprocessing pipelines) as well as individ-\nual ﬁles of the tapped responses. The dataset is available\nfor download from the Stanford Digital Repository [22],4\npublished under a Creative Commons CC-BY license.\n4https://purl.stanford.edu/jn859kj8079342 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 2 . Low-frequency EEG spectra using a mean-electrode spatial ﬁlter (top) and PC1 spatial ﬁlter (bottom) for each\nsong. Beat-related frequencies are shown with dashed vertical lines. Bottom right: PC1 spatial ﬁlter weights.\nFigure 3 . Participant ratings of familiarity and enjoyment.\n4.1 Cleaned EEG Data\nThe.mat ﬁlesongSS Imputed.mat contains the\ncleaned EEG records, aggregated across participants, for\nsongSS(x2.4.1). There are 10 such ﬁles, one per song.\nEach.mat ﬁle contains the following variables:\n\u000fdataSS : 3D electrodes-by-time-by-participant data\nframe. The size is 125\u0002T\u000220, with Tvarying\naccording to the song.\n\u000fsubsSS : Cell array of participant ids. Contents are\nthe same for all songs, but are included in order to\nlink these data to raw EEG ﬁles, raw tapping re-\nsponses, and participant demographics.\n\u000ffs: Sampling rate, in Hz (always 125).\n4.2 Raw EEG Data\nWe provide the raw EEG records in their exported state\nbefore preprocessing. No ﬁltering, epoching, or cleaning\nhas been performed. As each participant underwent two\nrecordings, there are a total of 40 raw EEG ﬁles. The ﬁle\nPPRraw.mat refers to recording R21;2from partici-\npantPP. Each ﬁle contains the following variables:\n\u000fX: Raw data frame. Size is electrodes-by-time,\n129\u0002T, where Tis the total length of the recording,\nincluding time periods not related to the experimen-\ntal trials. The vertex reference electrode is row 129.\u000fDIN 1: Cell array containing all event labels (trig-\ngers) and times. We provide the helper function\nparseDIN.m to extract the labels and onsets into\nnumeric vectors. Full speciﬁcation on labels is pro-\nvided in the README ﬁle accompanying the dataset.\n\u000ffs: Sampling rate, in Hz (always 1000).\n4.3 Behavioral Ratings\nParticipants delivered ratings of familiarity (Q1) and en-\njoyment (Q2) of each song during the EEG session. The\nﬁlebehavioralRatings.mat contains a single vari-\nable behavioralRatings , which is a 3D participant-by-\nsong-by-question ( 20\u000210\u00022) matrix.\n4.4 Tapping Responses\nAggregated and raw tapping responses are stored in the ﬁle\nTapIt.zip . This archive contains the ﬁle TapIt.mat ,\nwhich comprises the following variables:\n\u000fallTappedResponses : Aggregated tapped response\ntimes across all participants and songs. This is a\nparticipants-by-song ( 20\u000210) cell array. Each entry\nis a column vector of tap times in seconds, recorded\nfrom the device touchscreen.\n\u000fallSongOrders : Song-order vectors, aggregated\nacross all participants. This is a participants-by-trial\n(20\u000210) matrix, where each row contains the stimu-\nlus presentation order for the respective participant.\nNumbering starts at 1.\nIndividual response ﬁles are also included in the .zip ﬁle:\n\u000fPPP SS.txt : Single trial of tapped responses, in sec-\nonds, for participant PPP and song SS.\n\u000fPPP play order.txt : Stimulus presentation ordering\nfor participant PPP. Numbering starts at 0.\n4.5 Participant Demographics\nThe ﬁle participantInfo.mat contains a struct ar-\nrayparticipantInfo with participant demographics. FieldsProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 343Figure 4 . Tapping responses. Top: Instantaneous tempo over time for individual participants (gray), with median across\nparticipants in black. Bottom: Histograms of median tempo, over time, for individual participants. Ground-truth tempos\nare shown with orange ( 1=2xtempo frequency), green (tempo frequency), and red ( 2xtempo frequency) lines.\ninclude age,nYearsTraining ,weeklyListening (hours), and\nid(participant identiﬁer link to raw ﬁlenames).\n4.6 Code\nThe ﬁle Code.zip contains the Matlab scripts for the\nanalyses performed in x3. A variety of helper functions\nand ﬁles (e.g., electrode location map, script to parse the\nDIN 1variable in raw EEG ﬁles) are also provided here.\n5. DISCUSSION\nThis paper introduces NMED-T, an open dataset of elec-\ntrophysiological and behavioral responses collected from\n20 participants listening to real-world musical excerpts.\nThe published data include both raw and preprocessed\ndense-array EEG and tapping responses, behavioral ratings\nof the songs, and basic demographic information.\nOur illustrative analyses validate the frequency-tagging,\nSS-EP approach [26, 27] with responses to complex, nat-\nuralistic music (Fig. 2). Even a simple PCA ﬁlter com-\nputed from trial-averaged responses highlights beat-related\nfrequencies in the EEG spectra. Many PC1 spectra show\nprominent peaks between 5–10 Hz, regardless of tempo;\nfuture research could use this dataset to investigate further\nthe stimulus and response attributes contributing to this\nphenomenon. The variability in tapping responses (Fig. 4)\nhighlights the challenge of deﬁning a ‘ground truth’ for\ntempo and beat identiﬁcation, particularly for complex mu-\nsic [24]. Here we see various, sometimes conﬂicting re-\nsults across and within participants’ tapped responses. Past\nresearch has suggested that humans inherently prefer cer-\ntain frequencies related to natural movement [23,35]. This\nmay help to explain why some participants tapped at twice\nthe tempo for the slowest songs, tending toward the postu-\nlated 2-Hz natural resonant frequency.\nWe faced several trade-offs when designing the study.\nCollection of EEG data, while relatively inexpensive [14],\nstill incurs costs of equipment and time. Participant fa-\ntigue must also be taken into account when planning the\noverall duration of an experiment. As we wished to col-\nlect EEG responses to a set of full-length songs from ev-\nery participant, we were limited in the number of songswe could use, and relegated the secondary tapping task to\nshorter excerpts. Stimulus selection, too, is often a com-\npromise of breadth and depth. For example, the OpenMIIR\ndataset [31] uses shorter stimuli from a variety of genres,\nbut at the expense of depth within any one genre; while the\nNMED-H [15] includes various stimulus manipulations of\ncomplete songs, but only four songs from a single genre.\nOur focus on full-length songs with a steady beat and a va-\nriety of tempos limited the range of genres somewhat. We\nalso deliberately avoided massively popular songs in or-\nder to minimize possible effects, on the brain responses, of\nvarying familiarity, established personal preferences, and\nautobiographical associations with the songs [12].\nThere are shortfalls to the dataset. One potential con-\nfound is that the EEG session always preceded the behav-\nioral task; thus, participants were more familiar with the\nmusic during the tapping task. As a result, the tapping\ndata may not be suitable for studying the time course of\nbeat entrainment. However, we chose this arrangement so\nthat participants would not be focused speciﬁcally on beat\nwhile EEG responses were recorded. Second, the tapping\ndata show variations in tapped tempo across participants\nand within-participant over time. Whether this reﬂects our\nparticipant pool (not all trained musicians), inadequate in-\nstruction for the task, or is merely characteristic of this re-\nsponse is not addressed in the present illustrative analyses.\nFinally, listeners are known to exhibit variations in tempo\noctave during tapping while largely agreeing on whether\na song is fast or slow [19], but we unfortunately did not\ncollect data here to explore this distinction.\nGenerally speaking, this dataset facilitates research on\nencoding and decoding of naturalistic music. While the\nstudy design and initial analyses focused primarily on beat\nand tempo, the EEG responses can be analyzed in conjunc-\ntion with various other stimulus features as well. Investiga-\ntion of individual differences is also possible (e.g., predict-\ning a particular participant’s tapping tempo or preference\nrating from his or her own EEG). Other researchers might\nconsider augmenting the dataset with complementary re-\nsponses to the same songs. Ideally, the dataset will ﬁnd ap-\nplications in MIR and neuroscience research beyond those\nenvisioned by the authors of this study.344 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. ACKNOWLEDGMENTS\nThis research was supported by the Patrick Suppes Gift\nFund and the Wallenberg Network Initiative: Culture,\nBrain, Learning. The authors thank Anthony Norcia, Dan\nEllis, and the anonymous ISMIR reviewers for helpful\nfeedback on the manuscript.\n7. REFERENCES\n[1] V . Alluri, P. Toiviainen, I. P. J ¨a¨askel ¨ainen, E. Glerean,\nM. Sams, and E. Brattico. Large-scale brain networks\nemerge from dynamic processing of musical timbre,\nkey and rhythm. NeuroImage , 59(4):3677–3689, 2012.\n[2] A. J. Bell and T. J. Sejnowski. An information-\nmaximization approach to blind separation and blind\ndeconvolution. Neural Computation , 7(6):1129–1159,\n1995.\n[3] B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe,\nand K. R. Muller. Optimizing spatial ﬁlters for ro-\nbust EEG single-trial analysis. IEEE Signal Processing\nMagazine , 25(1):41–56, 2008.\n[4] D. H. Brainard. The psychophysics toolbox. Spatial Vi-\nsion, 10(4):433–436, 1997.\n[5] R. Cabredo, R. S. Legaspi, P. S. Inventado, and M. Nu-\nmao. An emotion model for music using brain waves.\nInISMIR , pages 265–270, 2012.\n[6] F. Cong, V . Alluri, A. K. Nandi, P. Toiviainen, R. Fa,\nB. Abu-Jamous, L. Gong, B. G. W. Craenen, H. Poiko-\nnen, M. Huotilainen, and T. Ristaniemi. Linking brain\nresponses to naturalistic music through analysis of on-\ngoing EEG and stimulus features. IEEE Trans. Multi-\nmedia , 15(5):1060–1069, 2013.\n[7] A. Delorme and S. Makeig. EEGLAB: An open source\ntoolbox for analysis of single-trial EEG dynamics in-\ncluding independent component analysis. Journal of\nNeuroscience Methods , 134(1):9–21, 2004.\n[8] D. P. W. Ellis. Beat tracking by dynamic programming.\nJournal of New Music Research , 36(1):51–60, 2007.\n[9] P. Fraisse and B. H. Repp. Anticipation of rhythmic\nstimuli: Speed of establishment and precision of syn-\nchronization (1966). Psychomusicology: Music, Mind,\nand Brain , 22(1):84, 2012.\n[10] N. Gang, B. Kaneshiro, J. Berger, and J. P. Dmo-\nchowski. Decoding neurally relevant musical features\nusing Canonical Correlation Analysis. In ISMIR , 2017.\n[11] J. Hockman and I. Fujinaga. Fast vs slow: Learning\ntempo octaves from user data. In ISMIR , pages 231–\n236, 2010.\n[12] P. Janata. The neural architecture of music-evoked\nautobiographical memories. Cerebral Cortex ,\n19(11):2579–2594, 2009.\n[13] T.-P. Jung, C. Humphries, T.-W. Lee, S. Makeig, M. J.\nMcKeown, V . Iragui, and T. J. Sejnowski. Extended\nICA removes artifacts from electroencephalographic\nrecordings. In NIPS , pages 894–900, 1998.[14] B. Kaneshiro and J. P. Dmochowski. Neuroimaging\nmethods for music information retrieval: Current ﬁnd-\nings and future prospects. In ISMIR , pages 538–544,\n2015.\n[15] B. Kaneshiro, D. T. Nguyen, J. P. Dmochowski,\nA. M. Norcia, and J. Berger. Naturalistic music EEG\ndataset—Hindi (NMED-H). In Stanford Digital Repos-\nitory , 2016.\n[16] H.-S. Kim, B. Kaneshiro, and J. Berger. Tap-It: An iOS\napp for sensori-motor synchronization experiments. In\nICMPC12 , 2012.\n[17] S. Koelstra, C. Muhl, M. Soleymani, J. S. Lee, A. Yaz-\ndani, T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras.\nDEAP: A database for emotion analysis using phys-\niological signals. IEEE Trans. Affective Computing ,\n3(1):18–31, 2012.\n[18] O. Lartillot and P. Toiviainen. A Matlab toolbox for\nmusical feature extraction from audio. In DAFx , pages\n237–244, 2007.\n[19] M. Levy. Improving perceptual tempo estimation with\ncrowd-sourced annotations. In ISMIR , pages 317–322,\n2011.\n[20] Y . P. Lin, J. R. Duann, W. Feng, J. H. Chen, and\nT. P. Jung. Revealing spatio-spectral electroencephalo-\ngraphic dynamics of musical mode and tempo percep-\ntion by independent component analysis. Journal of\nNeuroEngineering and Rehabilitation , 11(1), 2014.\n[21] Y . P. Lin, C. H. Wang, T. P. Jung, T. L. Wu, S. K.\nJeng, J. R. Duann, and J. H. Chen. EEG-based emo-\ntion recognition in music listening. IEEE Transactions\non Biomedical Engineering , 57(7):1798–1806, 2010.\n[22] S. Losorelli, D. T. Nguyen, J. P. Dmochowski,\nand B. Kaneshiro. Naturalistic music EEG dataset—\nTempo (NMED-T). In Stanford Digital Repository ,\n2017.\n[23] D. Moelants. Preferred tempo reconsidered. In\nICMPC7 , pages 1–4, 2002.\n[24] D. Moelants and M. F. McKinney. Tempo perception\nand musical content: What makes a piece fast, slow, or\ntemporally ambiguous? In ICMPC8 , pages 558–562,\n2004.\n[25] A. M. Norcia, L. G. Appelbaum, J. M. Ales, B. R. Cot-\ntereau, and B. Rossion. The steady-state visual evoked\npotential in vision research: A review. Journal of Vi-\nsion, 15(6):4, 2015.\n[26] S. Nozaradan, I. Peretz, M. Missal, and A. Mouraux.\nTagging the neuronal entrainment to beat and meter.\nThe Journal of Neuroscience , 31(28):10234–10240,\n2011.\n[27] S. Nozaradan, I. Peretz, and A. Mouraux. Selective\nneuronal entrainment to the beat and meter embed-\nded in a musical rhythm. The Journal of Neuroscience ,\n32(49):17572–17581, 2012.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 345[28] L. C. Parra, C. D. Spence, A. D. Gerson, and P. Sajda.\nRecipes for the linear analysis of EEG. NeuroImage ,\n28(2):326–341, 2005.\n[29] A. Sternin, S. Stober, J. A. Grahn, and A. M. Owen.\nTempo estimation from the EEG signal during percep-\ntion and imagination of music. In BCMI/CMMR , 2015.\n[30] S. Stober, T. Pr ¨atzlich, and M. Meinard. Brain beats:\nTempo extraction from EEG data. In ISMIR , pages\n276–282, 2016.\n[31] S. Stober, A. Sternin, A. M. Owen, and J. A. Grahn.\nTowards music imagery information retrieval: Intro-\nducing the OpenMIIR dataset of EEG recordings from\nmusic perception and imagination. In ISMIR , 2015.\n[32] M. S. Treder, H. Purwins, D. Miklody, I. Sturm,\nand B. Blankertz. Decoding auditory attention to\ninstruments in polyphonic music using single-trial\nEEG classiﬁcation. Journal of Neural Engineering ,\n11(2):026009, 2014.\n[33] D. M. Tucker. Spatial sampling of head electrical\nﬁelds: The geodesic sensor net. Electroencephalogra-\nphy and Clinical Neurophysiol. , 87(3):154–163, 1993.\n[34] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Trans. Speech and Audio\nProcessing , 10(5):293–302, 2002.\n[35] L. van Noorden and D. Moelants. Resonance in the per-\nception of musical pulse. Journal of New Music Re-\nsearch , 28(1):43–66, 1999.346 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Modeling the Multiscale Structure of Chord Sequences Using Polytopic Graphs.",
        "author": [
            "Corentin Louboutin",
            "Frédéric Bimbot"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415186",
        "url": "https://doi.org/10.5281/zenodo.1415186",
        "ee": "https://zenodo.org/records/1415186/files/LouboutinB17.pdf",
        "abstract": "Chord sequences are an essential source of information in a number of MIR tasks. However, beyond the sequen- tial nature of musical content, relations and dependencies within a music segment can be more efficiently modeled as a graph. Polytopic Graphs have been recently introduced to model music structure so as to account for multiscale rela- tionships between events located at metrically homologous instants. In this paper, we focus on the description of chord se- quences and we study a specific set of graph configura- tions, called Primer Preserving Permutations (PPP). For sequences of 16 chords, PPPs account for 6 different la- tent systems of relations, corresponding to 6 main struc- tural patterns (Prototypical Carrier Sequences or PCS). Observed chord sequences can be viewed as distorted ver- sions of these PCS and the corresponding optimal PPP is estimated by minimizing a description cost over the latent relations. After presenting the main concepts of this approach, the article provides a detailed study of PPPs across a cor- pus of 727 chord sequences annotated from the RWC POP database (100 pop songs). Our results illustrate both qual- itatively and quantitatively the potential of the proposed model for capturing long-term multiscale structure in mu- sical data, which remains a challenge in computational mu- sic modeling and in Music Information Retrieval.",
        "zenodo_id": 1415186,
        "dblp_key": "conf/ismir/LouboutinB17",
        "keywords": [
            "Chord sequences",
            "MIR tasks",
            "Polytopic Graphs",
            "Multiscale relationships",
            "Primer Preserving Permutations (PPP)",
            "Prototypical Carrier Sequences (PCS)",
            "Description cost",
            "Optimal PPP estimation",
            "Corpus of 727 chord sequences",
            "RWC POP database"
        ],
        "content": "MODELING THE MULTISCALE STRUCTURE\nOF CHORD SEQUENCES USING POLYTOPIC GRAPHS\nCorentin Louboutin\nUniversit ´e Rennes 1 / IRISA, France\ncorentin.louboutin@irisa.frFr´ed´eric Bimbot\nCNRS - UMR 6074 / IRISA, France\nfrederic.bimbot@irisa.fr\nABSTRACT\nChord sequences are an essential source of information in\na number of MIR tasks. However, beyond the sequen-\ntial nature of musical content, relations and dependencies\nwithin a music segment can be more efﬁciently modeled as\na graph.\nPolytopic Graphs have been recently introduced to\nmodel music structure so as to account for multiscale rela-\ntionships between events located at metrically homologous\ninstants.\nIn this paper, we focus on the description of chord se-\nquences and we study a speciﬁc set of graph conﬁgura-\ntions, called Primer Preserving Permutations (PPP). For\nsequences of 16 chords, PPPs account for 6different la-\ntent systems of relations, corresponding to 6main struc-\ntural patterns (Prototypical Carrier Sequences or PCS).\nObserved chord sequences can be viewed as distorted ver-\nsions of these PCS and the corresponding optimal PPP is\nestimated by minimizing a description cost over the latent\nrelations.\nAfter presenting the main concepts of this approach,\nthe article provides a detailed study of PPPs across a cor-\npus of 727chord sequences annotated from the RWC POP\ndatabase ( 100pop songs). Our results illustrate both qual-\nitatively and quantitatively the potential of the proposed\nmodel for capturing long-term multiscale structure in mu-\nsical data, which remains a challenge in computational mu-\nsic modeling and in Music Information Retrieval.\n1. INTRODUCTION\nOne of the essential properties of music structure is the\nmultiscale nature of the inner organization of musical seg-\nments, i.e. the existence of relationships between musical\nelements at different time-scales simultaneously.\nGiven its important role in supporting the local har-\nmonic ground-plan of the music in a signiﬁcant number of\nmusic genres, chord sequences are commonly considered\nas an essential source of information in a variety of MIR\ntasks (see for instance [13, 17, 22]).\nc\rCorentin Louboutin, Fr ´ed´eric Bimbot. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Corentin Louboutin, Fr ´ed´eric Bimbot. “Modeling the Mul-\ntiscale Structure of Chord Sequences Using Polytopic Graphs”, 18th In-\nternational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.\nFigure 1 . A chord sequence represented on a tesseract.\nHowever, beyond the sequential order of chords along\nthe timeline, relations and dependencies between chords\nwithin a music segment tend to be more efﬁciently mod-\neled as a graph.\nPolytopic Graphs of Latent Relations (PGLR) [11] have\nbeen recently introduced to model music structure, so as\nto account for multiscale relationships between events lo-\ncated at metrically homologous instants, by means of an\noriented graph supported by a n-dimensional polytope.\nThis class of models is assumed to be particularly well\nsuited for strongly ”patterned” music, such as pop music,\nwhere recurrence and regularity tend to play a central part\nin the structure of the musical content.\nPGLR also relax the adjacency hypothesis of the GTTM\nmodel [10], by which the grouping of elements into higher\nlevel objects is strictly limited to neighbouring units. This\nis particularly useful to account for period-like abac pat-\nterns, where the similarity relationship between the two a\nsegments spans above (and irrespective of) the bsegment.\nIn this paper, we focus on the description of metric-\nsynchronous chord sequences of 16 elements, resting on\nregular phrasal structures or carrures . In that case, the sup-\nporting polytope is a tesseract (i.e. a 4-cube) as illustrated\nby Fig. 1, and the graph description lives on this tesseract\n(as represented on Fig. 4).\nAfter providing, in Section 2, the main concepts and\nformalisms related to the approach, we study in detail a\nparticular variant of the model, where the graph structure181is restricted to a set of 6conﬁgurations, called Primer Pre-\nserving Permutations (PPP). We show in Section 3.1 that\nPPPs relate to prototypical multi-scale structural patterns\nwhich we call Prototypical Carrier Sequences (PCS) and\nwe explain how observed chord sequences can be viewed\nas distorted versions of these prototypical patterns. In the\nlast part of the article (Section 4), we provide an exper-\nimental study of PPPs across a corpus of 727 chord se-\nquences annotated from the RWC POP database ( 100pop\nsongs) with qualitative and quantitative results illustrating\nthe potential of the model. We conclude with perspectives\noutlined by the proposed approached.\n2. CONCEPTS AND FORMALISM\n2.1 The PGLR Framework\nAs mentioned in the introduction, the PGLR approach\nviews a sequence of musical elements within a structural\nsegment as exhibiting privileged relationships with other\nelements located at similar metrical positions across dif-\nferent timescales.\nFor metrically regular segments of 2nevents, the cor-\nresponding PGLR conveniently lives on an n-dimensional\ncube (square, cube, tesseract, etc...)1,nbeing the num-\nber of scales considered simultaneously in the multiscale\nmodel. Each vertex in the polytope corresponds to a musi-\ncal element of the lowest scale, each edge represents a la-\ntent relationship between two vertices and each face forms\nan elementary system of relationships between (typically)\n4 elements. In addition, the proposed model views the last\nvertex in each elementary system as the denied realization\nof a (virtual) expected element, itself resulting from the\nimplication triggered by the combination of former rela-\ntionships within the system (see Section 2.3).\nFor a given support polytope, the estimated PGLR\nstructure results from the joint estimation of (i) the conﬁgu-\nration of an oriented graph resting on the polytope, with the\nconstraint that it reﬂects causal time-order preserving de-\npendencies and interactions between the elements within\nthe musical segment, and (ii) the inference of the corre-\nsponding relations between the nodes of the graph, these\nrelations being termed as latent, as they are not explicitly\nobserved (and may even not be uniquely deﬁned).\n2.2 Chord Representation and Relations\nStrictly speaking, a chord is deﬁned as any harmonic set of\npitches that are heard as if sounding simultaneously. How-\never, in tonal western music, chords are more speciﬁcally\nunderstood as sets of pitch classes which play a strong role\nin the accompaniment of the melody (in particular, in pop\nsongs).\nA number of formalisms exist for describing chord rela-\ntions, either in the context of classical musicology or in the\nframework of more recent theories, for instance, the neo-\nRiemannian theory and voice-leading models [5,6,20], or\ncomputational criteria such as Minimal Transport [10].\n1and more generally speaking, on an n-polytope\nFigure 2 . Circles of thirds (inner) and phase-shifts (outer).\nWhile chords may contain combinations of four pitch\nclasses or even more, they are frequently reduced to triads\n(i.e. sets of three pitch classes), with a predominance of\nmajor and minor triads. A minimal representation of triads\nboils down to 24distinct triads ( 12major and 12minor).\nIn the rest of this article, we restrict ourselves to this case,\nin spite of its simpliﬁed nature.\nIn order to model relations between triads, we consider\ntriadic circles, i.e. circular arrangements of chords aimed\nat reﬂecting some proximity relationship between triads\nalong their circumference.\nThe circle of thirds is formed by alternating major and\nminor triads with neighbouring triads sharing two common\npitch classes, which is a way to model some kind of prox-\nimity between chords. In particular, chords belonging to a\ngiven key lie in a same sector of the circle of thirds. As\nan alternative, we also consider the circle of phase-shifts,\nwhich consists of a chord progression resulting from a min-\nimal displacement on the 3-5phase torus of triads as de-\nﬁned in [1]. Both circles are shown together on Fig. 2.\nEach circle provides a way to express (in a unique way),\nthe relationship between two triads, as the angular dis-\nplacement along the circle. Note that a ”chromatic” circle\n(...BmBCmCDb\nmDb...) could also be considered, but\nit is not represented on Fig. 2, for reasons explained later.\n2.3 Systemic Organization\nBased on the hypothesis that the relations between musi-\ncal elements form a system of projective implications, the\nSystem & Contrast (S&C) model [2] has been recently for-\nmalized [3] as a generalization and an extension of Nar-\nmour’s Implication-Realization model [16]. Its applicabil-\nity to various music genres for multidimensional and mul-\ntiscale music analysis has been explored in [7] and algo-\nrithmically implemented in an early version as ”Minimal\nTransport Graphs” [10].\nThe S&C model primarily assumes that relations be-\ntween 4elements in a musical segment x0x1x2x3can be\nviewed as based on a two-scale system of relations rooted\non the ﬁrst element x0(theprimer ), which thus plays the\nrole of a common antecedent to all other elements in the182 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 3 . Tesseract where vertices at a same depth (or\ngeodesic distance) from vertex # 0are aligned vertically.\nThe resulting partial order between vertices is causal.\nsystem. This is the basic principle that enables the joint\nmodeling of several timescales simultaneously.\nIn the S&C approach, it is further assumed that latent\nsystemic relations x1=f(x0)andx2=g(x0)trigger a\nprocess of projective implication:\nx0f(x0)g(x0)implies=)g(f(x0)) = ^x3 (1)\nThevirtual element ^x3may then be more or less strongly\ndenied by a contrast :x3=\r(^x3)6= ^x3, which creates\nsome sense of closure to the segment.\nIn this work, the S&C model is used as the basic scheme\nto describe systems of music elements forming the faces of\nthe tesseract.\n3. GRAPH-BASED DESCRIPTION\n3.1 Nested Systems\nElementary systems of four elements, as introduced in Sec-\ntion 2.3, can be used to describe longer sequences of mu-\nsical events. In particular, sequences of 2nelements ar-\nranged on an n-cube, provide a layout of the data where\neach face potentially forms a S&C, involving time instants\nthat share speciﬁc relationships in the metrical grid.\nAs opposed to the sequential viewpoint which assumes\na total order of elements along the timeline, the systemic\norganization on the tesseract leads to a partial order (il-\nlustrated on Fig. 3), where elements of the same depth are\naligned vertically and where, in the framework of the S&C,\nthe fourth element of each face can be deﬁned in reference\nto the virtual element resulting from the projective impli-\ncation of the three others. In the most general case, valid\nsystemic organizations can be characterized by a graph of\nnested systems, the ﬂow of which respects the partial or-\ndering of Fig. 3. Note however that there is a possible\nconﬂict between three implications systems for elements\n7,11,13and14(each possible implication corresponding\nto a face of the tesseract2), and six for element 15.\n2for instance, node 7can be viewed as resulting from 3 implication\nsystems: [1;3;5;7],[2;3;6;7]and[4;5;6;7].P00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nA A A A B B B B C C C C D D D D\nP10 1 4 5 2 3 6 7 8 9 12 13 10 11 14 15\nA A B B A A B B C C D D C C D D\nP20 2 4 6 1 3 5 7 8 10 12 14 9 11 13 15\nA B A B A B A B C D C D C D C D\nP30 1 8 9 2 3 10 11 4 5 12 13 6 7 14 15\nA A B B C C D D A A B B C C D D\nP40 2 8 10 1 3 9 11 4 6 12 14 5 7 13 15\nA B A B C D C D A B A B C D C D\nP50 4 8 12 1 5 9 13 2 6 10 14 3 7 11 15\nA B C D A B C D A B C D A B C D\nTable 1 . List of the 6PPPs, together with their correspond-\ning Prototypical Carrier Sequences (PCS).\n3.2 Primer Preserving Permutations (PPP)\nOne way to handle these conﬂicts is to constrain the graph\nto preserve systemic properties at higher scales. This can\nbe achieved by forcing lower-scale systems to be supported\nby parallel faces on the tesseract, while the ﬁrst elements\nof each of the 4 lower-scale systems are used to form an\nupper-scale system. This approach drastically brings down\nthe number of possible graphs to 6, which corresponds to\nspeciﬁc permutations of the initial index sequence (see Ta-\nble 1), termed here as PPP (Primer Preserving Permuta-\ntions).\nTo illustrate a PPP, let’s consider the subdivision of a\nsequence of 16chords into four sub-sequences of four suc-\ncessive chords. Each sub-sequence can be described as\na separate Lower-Scale S&C (LS): [0;1;2;3],[4;5;6;7],\n[8;9;10;11]and[12;13;14;15]. Then, these four S&Cs\ncan be related to one another by forming the Upper-Scale\nS&C (US) [0;4;8;12], linking the four primers of the 4LS.\nThis conﬁguration ( P0) turns out to be particularly eco-\nnomical for describing chord sequences such as SEQ 1:\nCm Cm Cm Bb Ab Ab Ab Gm F F F F Cm Cm Bb Bb\nas most similarities develop between neighbouring ele-\nments.\nIf we now consider the following example ( SEQ 2):\nBm Bm A A G Em Bm Bm Bm Bm A A G Em Bm Bm\na different conﬁguration appears to be more efﬁcient to\nexplain this sequence. In fact, grouping chords into the\nfollowing 4 LS: [0;1;8;9],[2;3;10;11],[4;5;12;13]and\n[6;7;14;15], and then relating these four faces of the\ntesseract by a US [0;2;4;6](conﬁguration P3) leads to a\nless complex (and therefore more economical) description\nof the relations between the data within the segment. Fig. 4\nillustrates these two conﬁgurations.\n3.3 Prototypical Carrier Sequences (PCS)\nEach of the 6PPPs can be related to a prototypical struc-\ntural pattern which turns out to be the one that is the most\nconcisely described in the framework of this particular\nconﬁguration. These 6 patterns, identiﬁed in Table 1, can\nbe interpreted as ”Prototypical Carrier Sequences” (PCS)\nover which the actual chord sequence appears as partially\n”modulated” information.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 183Figure 4 . Representations of two PPP-based PGLRs on a tesseract: P0(left),P3(right). In blue, the Upper-Scale S&C – in\nblack, the 4 Lower-Scale (LS) S&Cs. Dotted nodes indicate the virtual elements ( ^x) in the implication scheme (Section 2.3).\nFor instance, SEQ 1appears merely as a sequence of\ntypeP0, which has been altered in positions 3,7,14and\n15from the following carrier system:\nCm Cm Cm Cm Ab Ab Ab Ab F F F F Cm Cm Cm Cm\nConversely, SEQ 2exhibits a pattern that strongly relates\ntoP3, with scattered deviations from:\nBm Bm A A G G Bm Bm Bm Bm A A G G Bm Bm\nlocated in positions 5and13.\nInferring the PCS shows interesting analogies with a de-\nmodulation operation and/or an error correcting code pro-\ncess, by concentrating the redundancy on the carrier se-\nquence. It can of course happen that a sequence has several\npossible descriptions of equivalent plausibility, i.e. mul-\ntiple coexisting interpretations w.r.t. its prototypical PPP\nstructure.\nIn summary, PPP provide a limited set of baseline mul-\ntiscale structural patterns which can be used to characterize\nactual chord sequences, via a minimum deviation criterion.\n3.4 Algorithmical Considerations\nIn practice, given a (chord) sequence, X=x0:::x l\u00001, its\noptimal description ( DX) within a class of PGLRs, can be\nobtained by minimizing a criterion Fwritten as:\nDX= [\tX;RX] =argmin [\t;R]F(\t;RjX)(2)\nwhere \tis a PGLR and Ris a set of latent relations com-\npatible with the connections of \t.\nIn the general case, both quantities are optimized\njointly, considering all possible relations between each set\nof elements associated to each possible \t, and minimizing\nthe cost over the whole sequence X.\nAssuming thatFis measuring the complexity of the\nsequence structure, DXcan be deﬁned as the shortest de-\nscription of the sequence. Therefore, searching for DXcan\nbe seen as a Minimum Description Length (MDL) prob-\nlem [21] andFcan be understood as a function that eval-\nuates the size of the ”shortest” program needed to recon-\nstruct the data [9]. This is strongly related to the concept\nof Kolmogorov complexity, which has received increasing\ninterest in the music computing community over the past\nyears [12, 14, 15, 19].In the general case, the above optimization problem\nmay turn out to be of a relatively high combinatorial com-\nplexity (see [10, 11]). But when considering triads over a\ncircular arrangement, and limiting the set of possible \tto\n6PPPs, the optimization of Dbecomes easily tractable: all\nsix PPPs can be tested exhaustively and for each of them,\nthe setRcomprises 16 relations (15 displacements over the\ntriadic circle + the initialization of x0) which are uniquely\ndeﬁned. Therefore, each cost can be readily computed as\nthe sum of 16 terms, and the minimal PPP is easily found.\n4. EXPERIMENTS\nIn order to study the ability of the PGLR model to capture\nstructural information in chord sequences, we have carried\nout a set of experiments on the RWC POP dataset [8] on\na corpus of 727\u000216beat-synchronous chords sequences\nannotated manually as triads3.\nAs there exists no ground truth as of the actual struc-\nture of a chord sequence, we compare different models\nas regards their ability to predict and compress chord se-\nquences: in other words, how much side information is\nbrought by the structure model, that saves information\nneeded to describe of the content.\n4.1 Distribution of PPPs\nFor each chord sequence X, the polytopic S&C graph PX,\ncorresponding to the PPP with minimal cost can be esti-\nmated by the optimization algorithm of Section 3.4. This\nyields the histogram depicted on Figure 54.\nPermutation P3appears as the dominant one ( \u001933%)\nand this may be related to the fact that its prototypical car-\nrier sequence corresponds to a rather common ”antecedent-\nconsequent” form in music (especially, in pop music).\nConversely, the least frequent PPP ( P2), displays a fre-\nquency of occurrence below 5%. Somewhere in between,\nthe 4 other permutations see their frequencies ranging\nloosely within 10% to20%).\n3Data are available on musicdata.gforge.inria.fr/RWCAnnotations.html\n4About 2=3of test sequences correspond to a unique optimal PPP\nbut when k >1permutations provide equally optimal solutions, each of\nthem is counted as 1=k.184 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 5 . Histogram of PPPs across the test data.\n4.2 Prediction and Compression\nIn order to compare the prediction and compression capa-\nbilities provided by multiscale polytopic graphs, we con-\nsider 4structure models:\n\u000fS, a purely sequential graph where each element is\nrelated to its immediate predecessor5,\n\u000fP0, the polytopic S&C graph corresponding to PPP\nP0for all sequences,\n\u000fP3, the polytopic S&C graph corresponding to PPP\nP3for all sequences,\n\u000fPX, the polytopic S&C graph corresponding to the\nPPP with minimal cost optimised a posteriori for\neach 16-chord sequence, as described in section 3.4.\nAll models are ﬁrst-order models, in the sense that any\ngiven element within the sequence is related to a single\nantecedent (its time predecessor for the sequential graph, a\nprimer or a virtual element, in the case of the S&C model).\nPerformance for each model is obtained by calculating\naperplexity [4]B\u0003, derived from the entropyH\u0003.\nGiven a model M, the computation of perplexity re-\nquires the deﬁnition of a probability density function\n(pdf) for all observable events which underlie the model.\nIn our case, this means assigning a probability value\nPM(xij\b(xi))to any pair (xi;\b(xi)), where \b(xi)is\nthe antecedent of xiin the graph. This is equivalent to\ndeﬁningPM(r(\b(xi);xi)), wherer(\b(xi);xi)is the re-\nlation which turns \b(xi)intoxi. Considering a simple\nrotation angle \u0012(x2jx1) =\u00122\u0000\u00121on the triadic circle,\nPM(r(x1;x2))is a pdf that takes z= 24 distinct values.\nThe entropy of model Mcan be computed as:\nH\u0003(M) =\u0000zX\nk=0PM(rk) log2PM(rk) (3)\nB\u0003= 2H\u0003can be interpreted as a branching factor, that\nis the equivalent number of distinct relations between two\nchords, if these relations were equiprobable. It measures\nthe compression capacity of the model and is smaller for\nmodels which capture more information in the data.\n5This corresponds to a sequential bi-gram model, a very common ap-\nproach in MIR [18].Triad Circle\nThird Phase Random\nB(S) 8:00 7:67 9:32\nB(P0) 6:68 6:77 7:84\nB(P3) 5:35 5:35 6:02\nB(PX) 4:63 4:63 5:21\nBtot(PX)5:18 5:18 5:83\nTable 2 . Average cross-perplexity obtained for the vari-\nous models on RWC-Pop data with 2-fold cross-validation\n(training on even songs + testing on odd songs and vice-versa).\nIn this work, we consider speciﬁcally the cross-\nperplexityBderived from the negative log likeli-\nhood (NLL) ^H, computed on a test-set (of Lobservations).\nIn that case, the capacity of the model to catch relevant in-\nformation from an unseen musical segment is measured by\nmeans of a cross-entropy score, which quantiﬁes the ability\nof the model to predict unknown sequences from a similar\n(yet different) population.\nFor a given model M,^His deﬁned as:\n^H(M) =\u00001\nLL\u00001X\ni=0log2PM(xij\b(xi)) (4)\nwith the convention P(x0j\b(x0)) = 1=24.\nIn that context, the cross-perplexity B= 2Hcan be\nunderstood as an estimation of the (per symbol) average\nbranching factor in predicting the sequence knowing its\nstructure, on the basis of probabilities learnt on other se-\nquences, assumed to be of the same sort.\nAdditionnally, for model PX, we also compute the to-\ntal entropy ^Htot(PX) = ^H(PX) +Q, which includes the\nnumber of bits needed to encode the optimal conﬁguration\nof the PPP (1 among 6) for each sequence of 16 chords,\nnamely:\nQ= log2(6)=16\u00190:16bits/symbol, (5)\nthis term being equal to 0for the other models.\nThe ﬁrst column in Table 2 compares cross-perplexity\nﬁgures obtained with the 4structure models and consid-\nering the circle of thirds for modeling relations between\nchords. These results show that all tested polytopic models\noutperform the sequential model, with an additional advan-\ntage for the PXapproach, even when taking into account\nthe overhead cost required for PPP conﬁguration encoding.\n4.3 Impact of the Triadic Circle\nIn the rest of Table 2, cross-perplexity values are provided\nfor two other circles of triads: the circle of phase-shifts as\ndeﬁned on Fig. 2 and a randomized circle, where triads are\npositioned at random. Results show that the phase circle\nperforms quite the same as the circle of thirds, whereas the\nrandomized circle clearly performs less well. All outper-\nform their counterpart in the sequential model, as for all\npolytopic models, the identity relation is of zero cost and\nhigher probability. We do not report perplexities on the\nchromatic circle, given that it is congruent to the circle of\nthirds, thus yielding strictly identical results.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 185USLS1LS2LS3LS4\n44:4 % 8:0 % 15:7 % 19:7 % 22:4 %\nTable 3 . Proportion of sequences with contrastive US\n(Upper-Scale system) and LSk(kthLower-Scale system).\nFigure 6 . Proportion of contrastive systems within US sys-\ntems (left) and the 4 LS systems (right)\n4.4 Distribution and Density of Contrasts\nTo study the speciﬁc relationship between the virtual and\nthe contrastive element in the PXscheme, we investigated\non the location and the number of contrastive vs. non-\ncontrastive elements in potentially contrastive positions\ndeﬁned by the PPP framework.\nTable 3 presents the distribution of actual contrasts for\nthe Upper-Scale (US) and the 4 Lower-Scale (LS) in con-\ntrastive positions. While 44.4% of Upper-Scale Systems\nare contrastive, it can also be noted that the frequency of\nLS contrasts (or, so to speak, the occurrence of surprises\nat the lower-scale span) increases with the index of the LS\nsystem (i.e., its depth in the tesseract).\nFigure 6 depicts the proportion of sequences as a func-\ntion of the number of actual contrasts observed in different\ncontrastive positions. It can be observed that the number\nof contrastive Lower-Scale systems decays (roughly ex-\nponentially) from over 60.4% of sequences with no con-\ntrastive Lower-Scale system down to only 2.3% with all 4\nLS systems being contrastive.\nIt would surely be interesting to compare these proﬁles\nacross different music genres and a variety of musical di-\nmensions, in order to study possible correlations.\n4.5 Contrast Intensity\nTable 4 reports the perplexity obtained when considering\nseparately the systemic positions and the contrastive posi-\ntions. Keeping in mind that they may be speciﬁc to the cor-\npus, results show nevertheless two very interesting trends.\nPerplexity is higher in systemic positions ( 5:6) as op-\nposed to constrastive positions ( 3:5), implying that the ac-\ntual observations in contrastive positions often correspond\n(or are close) to the projective implication. This can be re-\nlated to the results observed in the previous section, w.r.t.\nthe relatively low density of actual contrasts.\nHowever, when different from identity (column Diff),\nthese relations show a lower perplexity for systemic re-\nlations ( 14:6vs18:7) indicating that, when a relation is\nnot identity, the contrast is more unpredictable and/or moreAll Diff\nSystemic position 5:614:6\nContrastive position 3:518:7\nTable 4 . Perplexity of relations for systemic relations and\ncontrastive relations, including (All) or excluding (Diff)\nthe identity relation.\nFigure 7 . Proportion of chord sequences showing ndistor-\ntions relative to their Prototypical Carrier Sequence (PCS).\ndistant on the circle of thirds, than it is for systemic rela-\ntions.\nIn summary, strictly contrastive relations tend to be less\nfrequent but more intense than systemic relations. This cer-\ntainly relates to the presumed role of contrasts as carrying\na strong quantity of surprise. These observations may be\na motivation for a different treatment of systemic relations\nvs. contrastive ones.\n4.6 Distortion of Prototypical Carrier Sequences\nUltimately, we considered the distribution of the number\nof distortions between observed chord sequences and their\nPCS, as deﬁned in section 3.3. Figure 7 shows a domi-\nnance of 4 deviations, with an overall prevalence of even\nvalues, suggesting that modelling systems of relations (i.e.\nedges) within the tesseract could be useful to further im-\nprove the compression capabilities of the PGLR model.\n5. CONCLUSIONS\nBoth from the conceptual and experimental viewpoints, the\npolytopic approach presented in this article appears as an\nefﬁcient way to model multiscale relations in chord se-\nquences.\nWhile still at an early stage of development, the PGLR\nmodel provides a potentially useful and powerful frame-\nwork for a number of tasks in MIR, as well as interesting\ntracks for music analysis and generation. Indeed, the core\nprinciples of the PGLR scheme are not speciﬁc to chord\nsequences: its application to other types of musical ob-\njects, such as melodic motives and rhythmic patterns are\ncurrently being explored.\nOngoing work also includes the extension of the poly-\ntopic model to a wider range of timescales, and the han-\ndling of segments of irregular size.186 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. REFERENCES\n[1] Emmanuel Amiot. The Torii of Phases , pages 1–18.\nSpringer, Mathematics and Computation in Music: 4th\nInternational Conference, MCM 2013, Montreal, QC,\nCanada, June 12-14, 2013. Proceedings, Berlin, Hei-\ndelberg, 2013.\n[2] Fr ´ed´eric Bimbot, Emmanuel Deruty, Gabriel Sargent,\nand Emmanuel Vincent. Semiotic structure labeling of\nmusic pieces: concepts, methods and annotation con-\nventions. In Proc. ISMIR , 2012.\n[3] Fr ´ed´eric Bimbot, Emmanuel Deruty, Gabriel Sargent,\nand Emmanuel Vincent. System & Contrast : A Poly-\nmorphous Model of the Inner Organization of Struc-\ntural Segments within Music Pieces. Music Percep-\ntion, 33:631–661, June 2016. Former version pub-\nlished in 2012 as Research Report IRISA PI-1999, hal-\n01188244.\n[4] Peter F Brown, Vincent J Della Pietra, Robert L Mer-\ncer, Stephen A Della Pietra, and Jennifer C Lai. An Es-\ntimate of an Upper Bound for the Entropy of English.\nComputational Linguistics , 18(1):31–40, 1992.\n[5] Clifton Callender, Ian Quinn, and Dmitri Ty-\nmoczko. Generalized voice-leading spaces. Science ,\n320(5874):346–348, 2008.\n[6] Richard Cohn. Audacious Euphony: Chromatic Har-\nmony and the Triad’s Second Nature . Oxford Univer-\nsity Press, 2011.\n[7] Emmanuel Deruty, Fr ´ed´eric Bimbot, and Brigitte\nVan Wymeersch. Methodological and Musicological\nInvestigation of the System & Contrast Model for Mu-\nsical Form Description. Research Report RR-8510, IN-\nRIA, 2013. hal-00965914.\n[8] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC Music Database:\nPopular, Classical and Jazz Music Databases. In\nISMIR , volume 2, pages 287–288, 2002.\n[9] Peter Grunwald and Paul Vit ´anyi. Shannon Infor-\nmation and Kolmogorov Complexity. arXiv preprint\ncs/0410002 , 2004.\n[10] Corentin Louboutin and Fr ´ed´eric Bimbot. Description\nof Chord Progressions by Minimal Transport Graphs\nUsing the System & Contrast Model. In ICMC 2016\n- 42nd International Computer Music Conference ,\nUtrecht, Netherlands, September 2016.\n[11] Corentin Louboutin and Fr ´ed´eric Bimbot. Polytopic\nGraph of Latent Relations: A Multiscale Structure\nModel for Music Segments. In MCM 2017 - 6th In-\nternational Conference of the Society of Mathematics\nand Computation in Music , Mexico City, Mexico, June\n2017.[12] Corentin Louboutin and David Meredith. Using\nGeneral-Purpose Compression Algorithms for Music\nAnalysis. Journal of New Music Research , 45(1):1–16,\n2016.\n[13] Matthias Mauch, Katy Noland, and Simon Dixon. Us-\ning Musical Structure to Enhance Automatic Chord\nTranscription. In ISMIR , pages 231–236, 2009.\n[14] Panayotis Mavromatis. Minimum Description Length\nModelling of Musical Structure. Journal of Mathemat-\nics and Music , 3(3):117–136, 2009.\n[15] David Meredith. Music analysis and Kolmogorov\ncomplexity. Proceedings of the 19th Colloquio\nd’Informatica Musicale (XIX CIM) , 2012.\n[16] Eugene Narmour. The Analysis and Cognition of\nMelodic Complexity: The Implication-Realization\nModel . University of Chicago Press, 1992.\n[17] H ´el`ene Papadopoulos and George Tzanetakis. Model-\ning chord and key structure with markov logic. In IS-\nMIR, pages 127–132, 2012.\n[18] Marcus Thomas Pearce. The Construction and Evalua-\ntion of Statistical Models of Melodic Structure in Music\nPerception and Composition . City University London,\n2005.\n[19] David Temperley. Probabilistic Models of Melodic In-\nterval. Music Perception , 32(1):85–99, 2014.\n[20] Dmitri Tymoczko. Scale Theory, Serial Theory and\nV oice Leading. Music Analysis , 27(1):1–49, 2008.\n[21] Paul MB Vit ´anyi and Ming Li. Minimum description\nlength induction, Bayesianism, and Kolmogorov com-\nplexity. IEEE Trans. Information Theory , 46(2):446–\n464, 2000.\n[22] Xinquan Zhou and Alexander Lerch. Chord Detection\nUsing Deep Learning. In Proceedings of the 16th IS-\nMIR Conference , volume 53, 2015.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 187"
    },
    {
        "title": "Fast and Accurate: Improving a Simple Beat Tracker with a Selectively-Applied Deep Beat Identification.",
        "author": [
            "Akira Maezawa"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415520",
        "url": "https://doi.org/10.5281/zenodo.1415520",
        "ee": "https://zenodo.org/records/1415520/files/Maezawa17.pdf",
        "abstract": "In music applications, audio beat tracking is a central com- ponent that requires both speed and accuracy, but a fast beat tracker typically has many beat phase errors, while an accurate one typically requires more computation. This paper achieves a fast tracking speed and a low beat phase error by applying a slow but accurate beat phase detector at only the most informative spots in a given song, and inter- polating the rest by a fast tatum-level tracker. We present (1) a framework for selecting a small subset of the tatum in- dices that information-theoretically best describes the beat phases of the song, (2) a fast HMM-based beat tracker for tatum tracking, and (3) an accurate but slow beat detec- tor using a deep neural network (DNN). The evaluations demonstrate that the proposed DNN beat phase detection halves the beat phase error of the HMM-based tracker and enables a 98% decrease in the required number of DNN invocations without dropping the accuracy.",
        "zenodo_id": 1415520,
        "dblp_key": "conf/ismir/Maezawa17",
        "keywords": [
            "beat tracking",
            "audio beat tracking",
            "beat phase errors",
            "fast beat tracker",
            "accurate beat tracker",
            "information-theoretic selection",
            "HMM-based tracker",
            "DNN-based detector",
            "beat phase detection",
            "DNN invocations"
        ],
        "content": "FAST AND ACCURATE: IMPROVING A SIMPLE BEAT TRACKER WITH\nA SELECTIVELY-APPLIED DEEP BEAT IDENTIFICATION\nAkira Maezawa\nYamaha Corporation\nABSTRACT\nIn music applications, audio beat tracking is a central com-\nponent that requires both speed and accuracy, but a fast\nbeat tracker typically has many beat phase errors, while\nan accurate one typically requires more computation. This\npaper achieves a fast tracking speed and a low beat phase\nerror by applying a slow but accurate beat phase detector at\nonly the most informative spots in a given song, and inter-\npolating the rest by a fast tatum-level tracker. We present\n(1) a framework for selecting a small subset of the tatum in-\ndices that information-theoretically best describes the beat\nphases of the song, (2) a fast HMM-based beat tracker for\ntatum tracking, and (3) an accurate but slow beat detec-\ntor using a deep neural network (DNN). The evaluations\ndemonstrate that the proposed DNN beat phase detection\nhalves the beat phase error of the HMM-based tracker and\nenables a 98% decrease in the required number of DNN\ninvocations without dropping the accuracy.\n1. INTRODUCTION\nOfﬂine audio beat tracking, the task of identifying beats in\na music audio signal, is now a critical component in mu-\nsic applications, having uses in digital audio workstations,\nsynthesizers, music recommendation and many others. In\nbeat tracking, it is important to both estimate a reasonable\ntempo (inversely proportional to the period between two\nbeats), as well as the timings of beat occurrence, or the\nbeat phase. In these applications, a beat tracker must be\nfast and accurate for common types of musical pieces such\nas popular music and electronic dance music. The capabil-\nity to analyze one song in a few seconds is often desirable\nin end-user products, while being satisfactorily accurate.\nThere is a trade-off, however, between the speed and\nthe accuracy of a beat tracker. On the one hand, a fast beat\ntracker tends to make mistakes due to some simplifying\nassumptions. On the other hand, an accurate beat tracker\nthat employs a more elaborate model like deep neural net-\nworks (DNN) tends to require more computation, which is\nproportional to the song duration.\nWe observe two points in these extrema. First, many\nof the errors in a fast tracker are attributed to incorrect beat\nc\rAkira Maezawa. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: Akira\nMaezawa. “Fast and Accurate: Improving a Simple Beat Tracker With a\nSelectively-applied Deep Beat Identiﬁcation ”, 18th International Society\nfor Music Information Retrieval Conference, Suzhou, China, 2017.\nBeat position \ninterpolationAudio signal\nFigure 1 . The overview of our method. Our system im-\nproves a simple beat tracker with little computational over-\nhead by adding a slow but accurate beat phase estimator.\nComputation is reduced by only using a subset of the de-\ntected tatums for phase estimation, and interpolating the\nestimated phase output.\nphase estimation, especially for current popular music. For\nexample, a simple beat tracker often mistakenly tracks the\noff-beat. This means that a simple method is already quite\ncapable of tracking the tatum, i.e.,some integer subdivi-\nsion of the beat, but poorly identiﬁes which of them are the\nbeats.\nSecond, using an accurate but slow beat tracker to\nsweep through an entire song is often wasteful. For many\npieces where tatum tracking is possible with a simple\nmethod, the primary role of an elaborate method is in beat\nphase identiﬁcation. In many musical pieces, however, the\nmeter is mostly stationary, so the beat phase identiﬁcation\nneeds to be done only sparingly. If the beats are identiﬁed\nat the most informative spots in the music for beat phase\nidentiﬁcation, the rest may be interpolated by exploiting\nthe stationarity of the meter.\nIn this paper, we combine the best of both worlds – a\nfast tracking of beats with a moderate amount of beat phase\nerrors, and an accurate identiﬁcation of the beats through\nthe use of elaborate methods. Our key idea, as shown in\nFigure 1, is to efﬁciently ﬁx the beat phase estimation er-\nrors of a simple but fast beat tracker, by sparingly applying\nan accurate but slow beat phase identiﬁer, only at the most\ninformative spots in the song. To elaborate, we (1) detect\nthe tatum reliably with a simple beat tracker, (2) select a\nsmall disjoint subset of the tatums that best describes the\nbeat phases of the entire song, (3) apply an elaborate beat309identiﬁer only at the selected tatum subset, and (4) interpo-\nlate the beat identiﬁcation for the remaining tatums. Such\na framework is enabled by exploiting a strong tatum-level\ncorrelation of the beat phase: it allows us to select a small\nset of tatums that best describes, in the sense of mutual in-\nformation, the rest of the beats, and to interpolate the rest.\nOur contributions are (1) a low-overhead framework for\nimproving an existing simple beat tracker by cascading a\nmore elaborate beat phase detector, achieved by identify-\ning the most informative tatums in a song for beat phase\nidentiﬁcation; (2) a beat phase identiﬁcation method us-\ning a DNN that accurately identiﬁes the beat phase of a\ntatum-sliced data; and (3) a simple and fast HMM-based\nbeat detector that jointly decodes the BPM and beat phase.\n2. RELATED WORK\n2.1 Beat Tracking and Downbeat Estimation\nBeat tracking is the task of identifying the beats in a mu-\nsic audio signal, a task that has been studied extensively.\nEarlier methods match hand-crafted onset features while\nassuming the evolution of the tempo, through soft rules [9]\ndynamic programming [8] or hidden Markov models [20],\noften with an explicit tempo induction step [5, 6]. One of\nthe key design issues is the choice of the hand-crafted fea-\ntures, such as changes of harmony [9] or variants of spec-\ntral ﬂux [8, 13, 18], and features indicating the salient beat\ninterval [13].\nBeat phase error is a common failure mode in beat\ntrackers [4, 5, 18]. For example, it is often common for\na beat tracker to track half a beat behind an acceptable beat\nposition, or to track the off-beats ( e.g., tracking second and\nfourth beats in a 4=4time) when tracking at half the under-\nlying tempo. The frequency of such a failure mode occurs\nsuggests that tatum tracking is relatively easily done with a\nsimple and fast method, but identifying the beat within the\ndetected tatums is a more delicate problem.\nTo estimate the beat phase, and more generally down-\nbeats, modeling of the rhythmic patterns [9,16], or extract-\ning the features indicating the spectral change at multiple\ntemporal level [13] have been shown to be useful. More\nrecently, signiﬁcant improvements in downbeat estimation\nhave been achieved through the use of DNN, which dele-\ngates the delicate task of designing the relevant features to\nmachine learning. For example, convolutional neural net-\nworks [7] or recurrent neural networks [2, 15] have shown\nsigniﬁcant improvements, at a cost of more computation.\n2.2 Sensor Placement and Submodular Optimization\nThe core idea of our paper is to ﬁnd the “best” tatum po-\nsitions to apply the computationally-heavy DNN output so\nthat the most information may be extracted with each invo-\ncation of the DNN. In a related problem of acquiring data\nwith costly sensors, the problem of determining the “best”\nway to place each sensor as to get the most information\nout is known as the sensor placement problem [14]. Sen-\nsor placement problem is often tackled by exploiting the\nspatial correlation. For example, if the spatial distribution\nFigure 2 . The state transition for the simple beat estimator.\nThe beat phase counts down deterministically, and the next\nbeat duration is chosen according to a beat period transition\nprobability.\nof the temperature needs to be acquired, it is better to place\nthe temperature sensors far apart with than near each other,\nsince the sensor readings at two nearby points, as opposed\nto far-away points, are more strongly correlated and thus\nare less revealing.\nThe sensor placement problem can be formulated as to\nmaximize the mutual information between the placed sen-\nsors and some other points of interest for which the sensors\nare not placed. While this problem is NP-hard, the sub-\nmodularity of the mutual information may be exploited to\narrive at a greedy near-optimal algorithm [19]. Submodu-\nlarity amounts to concavity for sets, and means that a given\nfunctionfover a setAsatisﬁesf(X[fig)\u0015f(Y[fig)\nfor allX\u0012Y\u0012Aandi2AnY.\n3. PROPOSED METHOD\nOur method consists of (1) a simple tatum tracker that\nquickly tracks the tatum in an audio signal, (2) a slow but\naccurate beat identiﬁer that identiﬁes which of the tracked\ntatums are the beats, and (3) a tatum index selector, that\nselects a few tatum indices for applying the beat identiﬁer,\nas to extract the most information regarding the presence\nof the beat.\n3.1 Tracking Tatums with a Fast HMM Beat Tracker\nWe ﬁrst track the tatum in a given audio signal. This is\nachieved by ﬁrst using a simple beat tracker to extract the\nbeat positions. Then, the tracked beat positions are sub-\ndivided equidistantly by a given factor \u0001nto obtain the\ntatums ( \u0001n= 4 in this paper). Notice that while the beat\ndetector may often track the wrong beat phase, it usually\ntracks the tatum properly.\nTo track the beat, we use an HMM-based beat detec-\ntor, which uses onset and tempo features to jointly decode\nthe beat position and the tempo. For the onset feature at\nframet, we compute the ﬁrst-order difference of the log-\nmagnitude spectrum ﬂux ot, and for the BPM feature, we\ncompute a comb ﬁlter-bank with the onset feature rt, sim-\nilar to [13].\nWe assume that the observed feature sequence is gener-\nated from an underlying sequence of discretized beat du-310 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017ration (related to tempo) !, and a “count-down” timer in-\ndicating the number of feature frames until the next beat.\nWe assume that each normalized \u001eis associated to a unique\nonset feature observation likelihood p(oj\u001e), and each value\nof the beat duration !is associated with a unique tempo\nfeature observation likelihood p(rj!). Based on these as-\nsumptions, the observation likelihood is given as follows:\np(ot;rtj\u001et;!t) =p(otj\u001et)p(rtj!t): (1)\nWe choosep(oj\u001e)to be a Normal distribution whose mean\nand the variance are selected based on the value of \u001enor-\nmalized by the beat duration. We choose three sets of the\nmean and the variance, based on whether the normalized\nbeat phase is 0, 0.5 and others; the parameters are trained\nwith maximum likelihood. Furthermore, p(rj!)is chosen\nto be a von Mises-Fisher distribution1, whose parameters\nswitches for each value of !.\nThe time sequence of the beat duration !tand the\ncount-down timer \u001etevolves such that (1) the \u001etdecreases\ndeterministically until reaching zero while !tremains con-\nstant and (2) when \u001etis zero, the beat duration !tswitches\nto a new value according to a tempo transition matrix, and\n\u001et+1is set to!t+1. This amounts to the following genera-\ntive process, also illustrated in Figure 2:\n(\u001et;!t)j(\u001et\u00001;!t\u00001)\n\u0018(\n\u000e(\u001et;\u001et\u00001\u00001)\u000e(!t;!t\u00001)\u001et\u00001>0\nR!t\u00001;!t\u000e(\u001et;\b!t)\u001et\u00001= 0;(2)\nwhere \b!is the number of audio frames corresponding to\nthe beat duration for the beat duration !, andR!1;!2is\na transition matrix that describes the probability of tran-\nsitioning from beat duration !1to!2. We reduce the\nsearch space by pruning negligible values of Rand lim-\niting the set of beat durations !to consider, similar to\n[17]. The beat positions are decoded using the Viterbi al-\ngorithm to arrive at a set of Nestimated tatum positions\nf\u001cnjn2T=f1;2:::Ngg.\nThis model is quite similar to the bar pointer model\n[24], except (1) we apply the bar pointer model only to\ndecode the beats and not the underlying meter or rhythm\nand (2) we use both the tempo and the onset likelihoods to\ndecode the beats and the beat durations. The inference is\nmore efﬁcient compared to the bar-pointer model because\nthe search space is much smaller – i.e., the state space is at\nthe beat level instead of the bar level, the beat duration is\ndiscretized, and the allowed transition is pruned.\nDespite the efﬁcient processing, this method, like many\nbeat trackers, suffers from beat phase estimation errors, oc-\ncurring approximately once every ten songs, for example,\nfor songs with strong syncopations. Thus, we consider us-\ning a more elaborate beat phase detection that is capable\nof directly modeling the kind of long-term characteristics\nrequired for beat phase identiﬁcation.\n1The likelihood is given by p(x;\u0016;\u0014)/exp(\u0014\u0016Tx)for some\u0016,x\nin(D\u00001)-sphere,Dbeing the dimension of the comb-ﬁlter output, and\n\u0014is a scalar parameter.\nTatum-sliced \nMSLSBeat \nExistenceFully-connected\n+BatchNorm+lReLU2D Conv. + MaxPool\n+ lReLU\n30ch \n(3x3)100ch \n(3x10)30ch \n(3x3)97x80 14250x200 200x2 On/O ﬀFigure 3 . The architecture for predicting the Beat phase.\n3.2 Identifying the Beats with Deep Neural Networks\nTo detect the presence of a beat at some tatum index, we\nuse a DNN-based classiﬁer of beats given tatum-sliced fea-\ntures. A DNN-based model is preferable because the no-\ntion of a beat depends on many factors like the rhythm and\nthe harmony, and a manual feature design on such a prob-\nlem is difﬁcult.\nTo identify the beat position using a DNN, we use as\nthe input the mel-scale log spectrogram (MSLS) that has\nbeen computed at each tatum. For each tatum, an 80-\ndimensional MSLS is extracted over a tatum window of\n48 tatums before and after the current tatum, creating an\ninput of R96\u000280.\nThe network consists of three convolutional layers, each\nwith a leaky ReLU activation followed by max-pooling.\nThe number of channels and the kernel sizes, in increas-\ning order of layers, are 30, 100, 30 and (3\u00023),(3\u000210)\nand(3\u00023), respectively. It is followed by dropout regu-\nlarization [23] during learning and a fully-connected layer\nwith 200-dimensional output with a batch-normalization\nlayer [11] and a leaky ReLU activation. Finally, a fully\nconnected layer with a softmax activation extracts the pos-\nterior probability of the beat presence. Thus, for some\ntatum index n, the DNN outputs bn2 f0;1g, which is\n1 if tatum index nis a beat and 0otherwise. Note that the\nmodel has no recurrent connections, allowing a random ac-\ncess to the tatum index.\nGiven a ground-truth annotation of the beat presence ^bn,\nwe minimize the cross-entropy loss L(\u0002):\nL(\u0002) =X\nn^bnlogbn(\u0002)+(1\u0000^bn) log(1\u0000bn(\u0002));(3)\nwithnindexed over the training dataset. The optimiza-\ntion is done stochastically, using ADAM [12] with weight\ndecay regularization. The mini-batch is shufﬂed randomly\nand we augment the data by pitch-shifting the input audio\nby -7 to +7 semitones, similar to [22].\nThis model is similar to the network in [7] in that we\nalso use tatum-level features, but we (1) use the full MSLS\ninstead of a band-passed input, allowing simultaneous ex-\ntraction of both harmonic and rhythmic features that con-\ntribute to beats, and (2) use both convolutional and fully\nconnected layers.\n3.2.1 On the Choice of the Tatum Window Size\nTo justify the use of MSLS evaluated over a windows of 48\ntatums before and after the current tatum, we have tested\nthe accuracy of our beat identiﬁcation method when chang-\ning the window radius. The accuracy on the validation data\nby changing the number of beats (assuming 4 tatums perProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 311L[beats] 1 2 4 8 10 12\nAccuracy[%] 83 87 90 93 93 93\nTable 1 . Validation accuracy of the beat estimator when\nchanging the tatum window radius L, in beats.\nbeat) is shown in Table 1. Since the performance saturates\nat 10 beats, use of 48 tatums provides sufﬁcient perfor-\nmance. Such a change in accuracy shows that a long-term\nanalysis spanning a few measures is indeed required for\nproperly identifying the beat.\n3.3 Choosing Tatums for Beat Identiﬁcation\nSince the identiﬁcation of the beats from a single frame is\nprone to errors, it is typical to identify the beat presence bn\nfor allNframes to arrive at the ﬁnal beat position estimate.\nHowever, identifying the beat using all the detected tatums\nis computationally expensive. Therefore we consider us-\ning a small subset of the detected tatums with K\u001cN\nelements for identifying the beats at the selected tatums\nand interpolating the rest.\nLet us formalize the problem. Let bnbe the beat phase\nestimation at some tatum index n.bncan be evaluated by\ninvoking an accurate but computationally expensive beat\nphase estimator as was discussed. Let T=f1;2;\u0001\u0001\u0001Ng\nbe the set of tatum indices. The ultimate goal is to obtain\nanestimate ofbnfor allninTreliably while evaluating\nbnonly at a few spots. Thus, the goal is to ﬁnd some small\nsubsetDK\u001aT ofKelements for which we do invoke the\nbeat phase estimation algorithm. For the remaining tatums\n\u0016DK, we interpolate the beat phase estimate by evaluating\nthe expectation of b\u0016DKgivenbDK.\nTo both identify the small subset DKand interpolate\nthe beat phase output, we exploit the strong tatum-level\ncorrelation of the beat identiﬁcation output. To illustrate,\nFigure 4 shows the auto-correlation riof the beat identiﬁ-\ncation output bn. Notice that a non-negligible correlation\nexists not only nearby but also far away, up to about 100\ntatums. This means that the existence of a beat at some\ntatum index provides information about the beat existence\nof the neighboring tatums.\nTo exploit such a covariance we assume that bnis a\nGaussian process [21]. That is, for some disjoint ordered\nsets of indicesD;U\u0012T , the joint pdf of bD=fbiji2Dg\nandbU=fbiji2Ug is expressed as follows:\n\u0012bD\nbU\u0013\n\u0018N\u0012\n\u0016;\u0012\u0006DD \u0006DU\n\u0006UD \u0006UU\u0013\u0013\n: (4)\nHere, \u0006ABis the cross-covariance matrix 2RjAj\u0002jBjbe-\ntweenfbiji2Agandfbiji2Bg, such that element (i;j)\nof\u0006A;B is the covariance between the ith element of A\nand thejth element of B.\u0016is the expectation of bithat is\ncomputed from a training dataset.\n3.3.1 Index Selection for Beat Phase Identiﬁcation\nAssuming the underlying Gaussian process, we seek to\nidentify a set of tatum indices DK2T subject tojDKj=\n4E-4\n2E-4\n0E-4\n-2E-4\n0 40 80 120 160\nLag (tatum)AutocorrelationFigure 4 . The auto-correlation of the estimated beat phase.\nK, as to maximize the mutual information between beat\nphase output bDKand the unobserved beat phase output\nb\u0016DK. This problem is NP-hard [14], but thanks to the sub-\nmodularity of mutual information, a near-optimal greedy\nalgorithm exists [19]. To solve the problem with a greedy\nnear-optimal algorithm, we iteratively add a new index ito\nthe set of indicesDkthat maximizes the increase in mutual\ninformation, i.e.,Dk=Dk\u00001[figwhere:\ni= arg max\ni02\u0016Dk\u00001MI(Dk\u00001[fi0g)\u0000MI(Dk\u00001);(5)\nwhere MI(D)denotes the mutual information between D\nand \u0016D. This can be seen as a special case of the sen-\nsor placement problem based on mutual information maxi-\nmization, where we ignore any points for which the sensor\nmay not be placed.\nEquation 5 for a Gaussian process amounts to setting\ni= arg maxi02\u0016Dk\u000ek\u00001;i0at stepk, with the following \u000ei;k:\n\u000ek;i=\u0006i;i\u0000\u0006fig;Dk\u0006\u00001\nDk;Dk\u0006Dk;fig\n\u0006i;i\u0000\u0006fig;Cki\u0006\u00001\nCki;Cki\u0006Cki;fig; (6)\nwhereCki=Dk[fig. Here, the numerator amounts to\nthe conditional variance of bigivenbDkand the denomi-\nnator amounts to the conditional variance of bigiven the\nremaining elements. Intuitively, therefore, this objective\nseeks to ﬁnd an index ithat is unpredictable based on the\nalready-observed data bDk, while being representative of\nthe non-selected indices, i.e., easily predictable from see-\ning the data of the non-selected indices.\nIt is important to notice that the computation of Equa-\ntion 6 is independent of the actual values of bn. Thus,DK\ncan be evaluated without invoking the computationally ex-\npensive DNN beat identiﬁer . Furthermore, given the auto-\ncorrelation computed beforehand, the set DKdepends only\non the number of tatums in a given song and not the ac-\ntual observations. Thus, DKmay be pre-computed for all\npractical values of the number of tatums, incurring zero\nruntime overhead.\n3.3.2 Analysis of the Selected Indices\nTo see how the indices are chosen with our method, Fig-\nure 5 shows the index chosen as the index selection algo-\nrithm proceeds. Notice how in the initial stage ( K= 2and\n4; red and yellow boxes), the algorithm selects the middle\nand the edges of the song while selecting pairs of indices\nthat are 2tatums apart, congruent mod 4. This shows that312 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Relative position in the songT atum indexmodulo 4K=2 K=32 K=16 K=8 K=4Figure 5 . The indices chosen by the the mutual infor-\nmation maximization criterion as the algorithm progresses\nwithK= 2,4, up to 32(best viewed in color).\nthe method tries to disambiguate the beat versus the off-\nbeat, while staking out the entire song. As the algorithm\nprogresses it selects tatum indices so that it (1) is more-or-\nless uniformly sampled throughout the song and (2) sam-\nples indices two tatums apart due to the weak correlation\nwith lag 2 as was shown in Figure 4.\n3.3.3 Interpolation of the Beat Phase Outputs\nFor interpolation, we evaluate the conditional expectation\nofbTgivenbDK:\nE[bijbDK] = \u0006i;DK\u0006\u00001\nDK;DKbDK: (7)\nThe evaluation of this function over all tatum indices re-\nquires a total of Kinvocations to the beat phase estimator.\nFinally, given the interpolated beat phase estimates\ncomputed from Equation 7, we use in this paper a simple\nheuristic to decide the beat positions. For the given level\nof beat subdivision \u0001nused to compute the tatums, we\nﬁnd the beat phase given a beat subdivision. For each beat\nphase hypothesis \u001a, we compute the following quantity:\nR\u001a=N=\u0001nX\nk=0E[bk\u0001n+\u001ajbDK] (8)\nThen, the beats are estimated as all tatum positions with\nindices ^\u001a+k\u0001nwithk2Nand^\u001a= arg max \u001aR\u001a. This\nheuristic is valid if the tatum tracking is successful and the\nnumber of tatums per beat remains ﬁxed at \u0001n.\n4. EXPERIMENTAL EV ALUATION\n4.1 Dataset\nFor the training and the validation dataset of the DNN beat\nidentiﬁcation and for estimating the auto-correlation of the\nbeat identiﬁer output at the tatum level, we used 100 popu-\nlar songs from the RWC Popular Music Database [10]. For\nthe test dataset, we prepared an in-house dataset consisting\nof 410 popular music in the United States and Japan. The\nmedian duration of the songs was four minutes. The tatum\nwas extracted with the proposed method and the beat phase\nwas hand-annotated based on a music score data created by\nprofessional musicians.Method Beat phase\nerrorReal-time\nfactor\nBOCK15 14.0% 0.149\nBaseline 12.2% 0.012\nProposed (Full) 6.1% 0.328\nProposed (K= 4) 16.2% 0.016\nProposed (K= 8) 10.1% 0.017\nProposed (K= 16 ) 7.6% 0.020\nProposed (K= 32 ) 6.1% 0.026\nTable 2 . The beat phase estimation error for songs that\nsucceeded at tatum tracking, and the mean real-time factor.\n4.2 Experiment 1: Beat Detection Improvement\nFirst, we evaluated the beat phase estimation error of the\nproposed DNN beat identiﬁcation method.\n4.2.1 Experimental Condition\nWe extracted the beats using four methods: (1) an imple-\nmentation2of the DNN-based beat detector in [3] with the\ntempo estimation method of [1], denoted “BOCK15,” (2)\nthe tatum detector used as a beat tracker (denoted “Base-\nline”), (3) the tatum detector with the DNN beat identiﬁer\nevaluated over the entire data (denoted “Proposed (full)”),\n(4) the tatum detector with the DNN beat identiﬁer evalu-\nated over a subset of the data that has been selected with\nthe proposed method (denoted “Proposed ( K=n)” when\nusingntatum indices to evaluate the DNN). Since our fo-\ncus is on ﬁxing beat detection that succeeds at tatum track-\ning but fails at beat phase identiﬁcation, we compared the\nmethods for songs for which tatum extraction was success-\nful (393 songs out of 410).\nIn addition, we computed the real-time factor, mea-\nsured on a machine with Intel Core i5 processor running\nat 2.6 GHz with 4 GB of RAM with no GPU, utilizing one\nCPU core. Notice that the condition “Proposed (full)” is\nthe baseline, in terms of computational speed, for most pre-\nvious DNN-based downbeat detectors such as [7], as the\nprevious method applies DNN to the entire audio data. The\nbaseline method was implemented in C++ using SSE2 for\nSIMD instructions. The DNN was implemented in Python\nusing the Chainer3library (an SSE2-optimized implemen-\ntation of the DNN in C++ yielded in a similar benchmark\nand thus is omitted).\n4.2.2 Results and Discussion\nThe results are shown in Table 2. It can be seen that by\nchoosingK= 32 , the method performs identically to\nwhen using the entire data for beat estimation, while being\ntwelve times faster (38x faster than real-time). The modest\nincrease in computation time over the Baseline suggests\nthat there is only a marginal overhead, especially when the\nbeat phase identiﬁcation is executed in tandem with the\nbeat extraction.\n2Obtained from https://github.com/CPJKU/madmom , com-\nmitde906fb\n3https://github.com/chainer/chainerProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 31375%80%85%90%95%100%\nK=4 K=8 K=16 K=32Head Mid Random\nLinear ProposedFigure 6 . The match to the DNN beat detector when sam-\nplingKtatums in a song using different methods. Greater\nvalue indicates that the estimated beat phase better matches\nthat estimated by the DNN.\nTo get a better idea of the computational costs, the beat\ndetector takes 740 msec to process a minute of audio using\na single core. The selection of indices incurred virtually no\noverhead, since we pre-computed the indices for all possi-\nble number of tatums. Note that this kind of index pre-\ncomputation is viable for K= 32 ; even when evaluating\nthe indices for songs with 65000 tatums, the total memory\nfor storing the indices is at most only 4 MB.\nEach call to the DNN used about 100 msec. Of this, the\nconvolutional layer comprised about 60% of the total time,\nand the fully-connected layer comprised about 40%. In our\nimplementation, the convolution layer is performed fully\nfor each call to the DNN, but this is redundant because\nportions of MSLS spectrograms may be shared across dif-\nferent audio frames. For songs with fewer than K\u000296\ntatums ( e.g., song of six and a half minutes with a BPM\nof120andK= 32 ), there are redundant outputs of the\nconvolutive layers. Such a redundancy potentially enables\nus to further speed up the convolutive layers.\n4.3 Experiment 2: Tatum Selection Strategy\nSecond, we evaluated the capability for the proposed tatum\nindex selection method to approximate the DNN evaluated\nover all tatums, by comparing the index selection method\nto other possible ways of selecting the tatum index subset.\n4.3.1 Experimental Condition\nFor each song in the test dataset, including those for which\ntatum tracking had failed, we selected Kindices with K=\n[4;8;16;32], with ﬁve different strategies: (1) select the\nﬁrstKtatums (denoted ”Head”), (2) select the middle K\ntatums (denoted ”Mid”), (3) select Kuniformly-sampled\ntatums (denoted ”Random”), (4) select Klinearly-spaced\ntatums (denoted ”Linear”), and (5) select Ktatums with\nthe proposed method. Then we evaluated, for each strat-\negy andK, the agreement rate of the beat detection out-\nput, evaluated with Equation 8, between that obtained us-\ning (1) the indices obtained by each strategy and (2) all\nindex. Notice that a high agreement for a given index se-\nlection scheme suggests its capability to approximate the\nbeat identiﬁcation using all indices.4.3.2 Results and Discussion\nFigure 6 shows the agreement rate between each tatum se-\nlection strategy and the full DNN.\nWhen trying to identify the beat with only more than\nfour tatums, the proposed method consistently outper-\nformed the baselines. When choosing only four tatums\n(K= 4), the strategy of choosing the middle performs the\nbest, perhaps because it is better to focus on one region to\nestimate the beat phase instead of dispersing the selection\nthroughout the piece. The result nonetheless demonstrates\nthe capability of our method to select a “good” set of in-\ndices for beat identiﬁcations compared to other intuitively-\narrived methods.\nComparing the result with the previous experiment,\nwithK= 32 the agreement of the proposed method does\nnot reach 100% even though the beat detection accuracy\nforK= 32 is identical to those using the entire DNN\noutput. This means that interpolated beats disagree for\nsongs for which tatum tracking has failed. Such a disagree-\nment occurs because (1) if the tatum tracking fails, the as-\nsumed covariance of the DNN output \u0006poorly describes\nthe underlying DNN output and (2) our beat position de-\ncoding method relies on a proper tracking of tatums with\nno change of meter.\n5. CONCLUSION\nThis paper presented a method to improve a fast and simple\nbeat tracker with little computatinal overhead by using an\nelaborate DNN-based beat identiﬁer to ﬁx the error in the\nsimple beat tracker at carefully-selected tatums.\nWe addressed the critical issue of achieving both the\naccuracy enjoyed by a DNN-based beat identiﬁcation of\nslow and elaborate methods, and the fast speed enjoyed by\na simple but erroneous beat tracking methods. This was\ntackled by applying a DNN beat identiﬁcation sparingly ,\nonly at the most informative tatum indices given by a sim-\nple beat-tracker. The selection was done as to maximize\nthe mutual information between the selected and the non-\nselected indices for invoking the DNN.\nEvaluation demonstrated that the DNN halved the beat\nphase error, and the tatum selection strategy provided the\nsame performance as when sweeping through the entire\naudio signal, resulting in a twelve-fold speed improve-\nment for a typical song. Furthermore, the subset selec-\ntion method was also shown to be consistently efﬁcient at\napproximating the DNN output compared to other index\nselection methods.\nFuture work includes (1) application of the index selec-\ntion framework to other tasks in MIR such as downbeat\nestimation, (2) relaxing the assumptions made for the in-\ndex selection, such as assuming a known and a ﬁxed co-\nvariance of the output, (3) allowing the parameter Kto be\ndetermined automatically, (4) improving the heuristics for\ndeciding the beat positions.314 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. REFERENCES\n[1] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nAccurate tempo estimation based on recurrent neural\nnetworks and resonating comb ﬁlters. In Proc. Inter-\nnational Conference on Music Information Retrieval ,\npages 625–631, 2015.\n[2] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nJoint beat and downbeat tracking with recurrent neural\nnetworks. In Proc. International Conference on Music\nInformation Retrieval , 2016.\n[3] Sebastian B ¨ock and Markus Schedl. Enhanced beat\ntracking with context-aware neural networks. In Proc.\nInternational Conference on Digital Audio Effects ,\npages 135–139, 2011.\n[4] Matthew E.P. Davies, Norberto Degara, and Mark D.\nPlumbley. Evaluation methods for musical audio beat\ntracking algorithms. Queen Mary University of London\nCentre for Digital Music, Tech. Rep. C4DM-TR-09-06 ,\n2009.\n[5] Matthew E.P. Davies and Mark D. Plumbley. Context-\ndependent beat tracking of musical audio. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n15(3):1009–1020, 2007.\n[6] Simon Dixon. Evaluation of the audio beat track-\ning system Beatroot. Journal of New Music Research ,\n36(1):39–50, 2007.\n[7] Simon Durand, Juan P. Bello, Bertrand David, and\nGa¨el Richard. Feature adapted convolutional neural\nnetworks for downbeat tracking. In Proc. International\nConference on Acoustics, Speech and Signal Process-\ning, pages 296–300, 2016.\n[8] Daniel P.W. Ellis. Beat tracking by dynamic program-\nming. Journal of New Music Research , 36(1):51–60,\n2007.\n[9] Masataka Goto. An audio-based real-time beat tracking\nsystem for music with or without drum-sounds. Jour-\nnal of New Music Research , 30(2):159–171, 2001.\n[10] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nPopular, classical, and jazz music databases. In\nProc. International Conference on Music Information\nRetrieval , pages 287–288, 2002.\n[11] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[12] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.[13] Anssi P. Klapuri, Antti J. Eronen, and Jaakko T. As-\ntola. Analysis of the meter of acoustic musical signals.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 14(1):342–355, 2006.\n[14] Andreas Krause, Ajit Singh, and Carlos Guestrin.\nNear-optimal sensor placements in gaussian processes:\nTheory, efﬁcient algorithms and empirical studies.\nJournal of Machine Learning Research , 9:235–284,\n2008.\n[15] Florian Krebs, Sebastian B ¨ock, Matthias Dorfer,\nand Gerhard Widmer. Downbeat tracking using beat-\nsynchronous features and recurrent neural networks. In\nProc. International Conference on Music Information\nRetrieval , pages 129–135, 2016.\n[16] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nRhythmic pattern modeling for beat and downbeat\ntracking in musical audio. In Proc. International Con-\nference on Music Information Retrieval , pages 227–\n232, 2013.\n[17] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nAn efﬁcient state-space model for joint tempo and me-\nter tracking. In Proc. International Conference on Mu-\nsic Information Retrieval , pages 72–78, 2015.\n[18] Jean Laroche. Efﬁcient tempo and beat tracking in au-\ndio recordings. Journal of the Audio Engineering Soci-\nety, 51(4):226–233, 2003.\n[19] George L. Nemhauser, Laurence A. Wolsey, and Mar-\nshall L. Fisher. An analysis of approximations for max-\nimizing submodular set functionsi. Mathematical Pro-\ngramming , 14(1):265–294, 1978.\n[20] Geoffroy Peeters and Helene Papadopoulos. Simulta-\nneous beat and downbeat-tracking using a probabilistic\nframework: Theory and large-scale evaluation. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 19(6):1754–1769, 2011.\n[21] Carl E. Rasmussen. Gaussian processes for machine\nlearning. 2006.\n[22] Jan Schl ¨uter and Thomas Grill. Exploring data aug-\nmentation for improved singing voice detection with\nneural networks. In Proc. International Conference on\nMusic Information Retrieval , pages 121–126, 2015.\n[23] Nitish Srivastava, Geoffrey E. Hinton, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. Journal of Machine\nLearning Research , 15(1):1929–1958, 2014.\n[24] Nick Whiteley, Ali Taylan Cemgil, and Simon J God-\nsill. Bayesian modelling of temporal structure in musi-\ncal audio. In Proc. International Conference on Music\nInformation Retrieval , pages 29–34, 2006.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 315"
    },
    {
        "title": "Chord Recognition in Symbolic Music Using Semi-Markov Conditional Random Fields.",
        "author": [
            "Kristen Masada",
            "Razvan C. Bunescu"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1418343",
        "url": "https://doi.org/10.5281/zenodo.1418343",
        "ee": "https://zenodo.org/records/1418343/files/MasadaB17.pdf",
        "abstract": "Chord recognition is a fundamental task in the harmonic analysis of tonal music, in which music is processed into a sequence of segments such that the notes in each seg- ment are consistent with a corresponding chord label. We propose a machine learning model for chord recognition that uses semi-Markov Conditional Random Fields (semi- CRFs) to perform a joint segmentation and labeling of symbolic music. One benefit of using a semi-Markov model is that it enables the utilization of segment-level fea- tures, such as segment purity and chord coverage, that cap- ture the extent to which the events in an entire segment of music are compatible with a candidate chord label. Corre- spondingly, we develop a rich set of segment-level features for a semi-CRF model that also incorporates the likelihood of a large number of chord-to-chord transitions. Evalua- tions on a dataset of Bach chorales and a corpus of theme and variations for piano by Beethoven and Mozart show that the proposed semi-CRF model outperforms a discrim- inatively trained Hidden Markov Model (HMM) that does sequential labeling of sounding events, thus demonstrating the suitability of semi-Markov models for joint segmenta- tion and labeling of music.",
        "zenodo_id": 1418343,
        "dblp_key": "conf/ismir/MasadaB17",
        "keywords": [
            "Chord recognition",
            "fundamental task",
            "symbolic music",
            "joint segmentation",
            "labeling",
            "semi-Markov Conditional Random Fields",
            "segment-level features",
            "corpus of theme and variations",
            "HMM",
            "suitability of semi-Markov models"
        ],
        "content": "CHORD RECOGNITION IN SYMBOLIC MUSIC USING SEMI-MARKOV\nCONDITIONAL RANDOM FIELDS\nKristen Masada\nSchool of EECS\nOhio University, Athens, OH\nkm942412@ohio.eduRazvan Bunescu\nSchool of EECS\nOhio University, Athens, OH\nbunescu@ohio.edu\nABSTRACT\nChord recognition is a fundamental task in the harmonic\nanalysis of tonal music, in which music is processed into\na sequence of segments such that the notes in each seg-\nment are consistent with a corresponding chord label. We\npropose a machine learning model for chord recognition\nthat uses semi-Markov Conditional Random Fields (semi-\nCRFs) to perform a joint segmentation and labeling of\nsymbolic music. One beneﬁt of using a semi-Markov\nmodel is that it enables the utilization of segment-level fea-\ntures, such as segment purity and chord coverage, that cap-\nture the extent to which the events in an entire segment of\nmusic are compatible with a candidate chord label. Corre-\nspondingly, we develop a rich set of segment-level features\nfor a semi-CRF model that also incorporates the likelihood\nof a large number of chord-to-chord transitions. Evalua-\ntions on a dataset of Bach chorales and a corpus of theme\nand variations for piano by Beethoven and Mozart show\nthat the proposed semi-CRF model outperforms a discrim-\ninatively trained Hidden Markov Model (HMM) that does\nsequential labeling of sounding events, thus demonstrating\nthe suitability of semi-Markov models for joint segmenta-\ntion and labeling of music.\n1. INTRODUCTION AND MOTIVATION\nHarmonic analysis is an important step towards creating\nhigh level representations of tonal music. High level struc-\ntural relationships form an essential component of music\nanalysis, whose aim is to achieve a deep understanding of\nhow music works. At its most basic level, harmonic anal-\nysis requires the partitioning of music into segments along\nthe time dimension, such that the notes in each segment\ncorrespond to a musical chord. This chord recognition\ntask can often be time consuming and cognitively demand-\ning, hence the utility of computer-based implementations.\nReﬂecting historical trends in artiﬁcial intelligence, auto-\nmatic approaches to harmonic analysis have evolved from\npurely grammar-based and rule-based systems [6, 15], toc\u0000Kristen Masada, Razvan Bunescu. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Kristen Masada, Razvan Bunescu. “Chord Recognition\nin Symbolic Music using semi-Markov Conditional Random Fields”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.\nFigure 1 . Segment-based recognition (top) vs. event-\nbased recognition (bottom), on measures 11 and 12 from\nBeethoven WoO68.\nsystems employing weighted rules and optimization algo-\nrithms [8, 11, 13, 14], to data driven approaches based on\nsupervised machine learning (ML) [9,10]. Due to their re-\nquirements for annotated data, ML approaches have also\nled to the development of music analysis datasets contain-\ning a large number of manually annotated harmonic struc-\ntures, such as the 60 Bach Chorales introduced in [9], and\nthe 27 theme and variations of TA VERN [2].\nA relatively common strategy in ML approaches to\nchord recognition is to break the musical input into a se-\nquence of short duration spans and then train sequence\ntagging algorithms such as HMMs to assign a chord label\nto each span in the sequence (at the bottom in Figure 1).\nThe spans can result from quantization using a ﬁxed mu-\nsical period such as half a measure [10] or constructed\nfrom consecutive note onsets and offsets [9]. Variable-\nlength chord segments are then created by joining consec-\nutive spans labeled with the same chord symbol (at the top\nin Figure 1). A signiﬁcant drawback of these short-span\ntagging approaches is that segments are not known during\ntraining and inference, therefore the ML model cannot use\nfeatures that capture properties of segments that are known\nto be relevant with respect to their harmonic content. The\nchordal analysis system of Pardo and Birmingham [8] is\nan example where the assignment of chords to segments\ntakes into account segment-based features, however it uses\na processing pipeline where segmentation is done indepen-\ndently of the subsequent chord labeling.\nIn this paper, we propose a machine learning approach\nto chord recognition in which a semi-Markov CRF model\nis trained to do joint segmentation and labeling of sym-\nbolic music. Also called segmental CRFs, this class of\nmodels can exploit features that look at all the notes in-272side a segment. Correspondingly, we deﬁne a rich set of\nfeatures that capture the extent to which the events in an\nentire segment of music are compatible with a candidate\nchord label. When evaluated on a dataset of Bach chorales,\nthe semi-CRF approach obtains a 15% error reduction over\nan event-tagging system. Substantially larger improve-\nments in event-level and segment-level performance are\nobserved on a more difﬁcult corpus of theme and variations\nby Beethoven and Mozart, thus validating empirically the\nmodeling advantage of joint segmentation and labeling.\n2. SEMI-MARKOV CRF MODEL FOR CHORD\nRECOGNITION\nSince harmonic changes may occur only when notes be-\ngin or end, we ﬁrst create a sorted list of all the note on-\nsets and offsets in the input music, i.e. the list of partition\npoints [8]. A basic music event [9] is then deﬁned as the\nset of notes sounding in the time interval between two con-\nsecutive partition points. Let s=hs1,s2,. . . ,s Kidenote\na segmentation of the musical input x, where a segment\nsk=hsk.f, s k.liis identiﬁed by the indices sk.fandsk.l\nof its ﬁrst and last events, respectively.\nLety=hy1,y2,. . . ,y Kibe the vector of chord labels\ncorresponding to the segmentation s. The set of labels\ncan range from coarse grained labels that indicate only the\nchord root [14] to ﬁne grained labels that capture mode,\ninversions, added and missing notes [3], and even chord\nfunction [2]. Here we follow the middle ground proposed\nby Radicioni and Esposito [9] and deﬁne a set of labels that\nencode the chord root (12 pitch classes), the mode (ma-\njor, minor, diminished), and the added note (none, fourth,\nsixth, seventh), for a total of 144 different labels. Since\nthe labels do not encode for function, the model does not\nrequire knowing the key in which the input was written.\nA semi-Markov CRF [12] deﬁnes a probability distribu-\ntion over segmentations and their labels as shown in Equa-\ntions 1 and 2. Here, the global segmentation feature vec-\ntorFdecomposes as a sum of local segment feature vec-\ntorsf(sk,yk,yk\u00001,x), with label y0set to a constant “no\nchord” value. The ensuing factorization of the distribution\nenables an efﬁcient computation of the most likely seg-\nmentation arg maxs,yP(s,y|x,w)using a semi-Markov\nanalogue of the Viterbi algorithm [12].\nP(s,y|x,w)=ewTF(s,y,x)\nZ(x)(1)\nZ(x)=X\ns,yewTF(s,y,x)\nF(s,y,x)=KX\nk=1f(sk,yk,yk\u00001,x) (2)\nFollowing Muis and Lu [7], for faster inference, we further\nrestrict the local segment features to two types: segment-\nlabel features f(sk,yk,x)that depend on the segment and\nits label, and label transition features g(yk,yk\u00001,x)that\ndepend on the labels of the current and previous segments.\nThe corresponding probability distribution over segmenta-\ntions is shown in Equations 3 to 5 below.Given an arbitrary segment sand a label y, the vector\nof segment-label features can be written as f(s, y,x)=\n[f1(s, y),. . . ,f |f|(s, y)], where the input xis left implicit\nin order to compress the notation. Similarly, given arbi-\ntrary labels yandy0, the vector of label transition features\ncan be written as g(y,y0,x)=[ g1(y,y0),. . . ,g |g|(y,y0)].\nIn Section 3 we describe the set of segment-label features\nfi(s, y)and label transition features gj(y,y0)that are used\nin our semi-CRF chord recognition system.\nP(s,y|x,w)=ewTF(s,y,x)+uTG(s,y,x)\nZ(x)(3)\nF(s,y,x)=KX\nk=1f(sk,yk,x) (4)\nG(s,y,x)=KX\nk=1g(yk,yk\u00001,x) (5)\n3. CHORD RECOGNITION FEATURES\nGiven a segment sand chord y, we will use the following\nnotation:\n•s.Notes ,s.N= the set of notes in the segment s.\n•s.Events ,s.E= the sequence of events in s.\n•e.len ,n.len = the length of event eor note n, in\nquarters.\n•e.acc ,n.acc = the accent value of event eor note n,\nas computed by the beatStrength() function in\nMusic211. This is a value that is determined based\non the metrical position of n, e.g. in a song written\nin a 4/4 time signature, the ﬁrst beat position would\nhave a value of 1.0, the third beat 0.5, and the sec-\nond and fourth beats 0.25. Any other eighth note\nposition within a beat would have a value of 0.125,\nany sixteenth note position strictly within the beat\nwould have a value of 0.0625, and so on.\n•y.root,y.third , and y.ﬁfth = the triad tones of the\nchord y.\n•y.added = the added note of chord y, ifyis an added\ntone chord.\nWe use the following heuristics to determine whether a\nnote nfrom a segment sis a ﬁguration note with respect to\na candidate chord label y:\n1.Passing : There are two anchor notes n1andn2such\nthat: n1’s offset coincides with n’s onset; n2’s onset\ncoincides with n’s offset; n1is one scale step below\nnandn2is one step above n, orn1is one step above\nnandn2one step below; nis not longer than either\nn1orn2; the accent value of nis strictly smaller than\nthe accent value of n1; at least one of the two anchor\nnotes belongs to segment s;nis non-harmonic with\nrespect to chord y, i.e. nis not equivalent to the root,\n1http://web.mit.edu/music21Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 273third, ﬁfth, or added note of y; both n1andn2are\nharmonic with respect to the segments they belong\nto.\n2.Neighbor : There are two anchor notes n1andn2\nsuch that: n1’s offset coincides with n’s onset; n2’s\nonset coincides with n’s offset; n1andn2are both\neither one step below or one step above n;nis not\nlonger than either n1orn2; the accent value of nis\nstrictly smaller than the accent value of n1; at least\none of the two anchor notes belongs to segment s;n\nis non-harmonic with respect to chord y; both anchor\nnotes are harmonic with respect to the segments they\nbelong to.\n3.Suspension : Note nbelongs to the ﬁrst event of seg-\nment s. There is an anchor note min the previous\nevent (last event in the previous segment) such that:\nmandnhave the same pitch; nis either tied with\nm(i.e. held over) or m’s offset coincides with n’s\nonset (i.e. restruck); nis not longer than m;nis\nnon-harmonic with respect to chord y, while mis\nharmonic with respect to the previous chord.\n4.Anticipation : Note nbelongs to the last event of\nsegment s. There is an anchor note min the next\nevent (ﬁrst event in the next segment) such that: n\nandmhave the same pitch; mis either tied with\nn(i.e. held over) or n’s offset coincides with m’s\nonset (i.e. restruck); nis not longer than m;nis\nnon-harmonic with respect to chord y, while mis\nharmonic relative to all other notes in its event.\nFuthermore, because we are using the weak semi-CRF fea-\ntures shown in Equation 4, we need a heuristic to determine\nwhether an anchor note is harmonic whenever the anchor\nnote precedes the current segment. The heuristic simply\nlooks at the other notes in the event containing the anchor\nnote: if the event contains 2 or more other notes, at least 2\nof them need to be consonant with the anchor, i.e. intervals\nof octaves, ﬁfths, thirds, and their inversions; if the event\ncontains just one other note, it has to be consonant with the\nanchor.\nWe emphasize that the rules mentioned above for de-\ntecting ﬁguration notes are only approximations. We rec-\nognize that correctly identifying ﬁguration notes can also\ndepend on subtler stylistic and contextual cues, thus allow-\ning for exceptions to each of these rules.\nEquipped with this heuristic deﬁnition of ﬁguration\nnotes, we augment the notation as follows:\n•s.Fig(y)= the set of notes in sthat are ﬁguration\nwith respect to chord y.\n•s.NonFig (y)= s.Notes \u0000s.Fig(y)= the set of\nnotes in sthat are not ﬁguration with respect to y.\nSome of the segment-label features introduced in this\nsection have real values. Given a real-valued feature\nf(s, y)that takes values in [0,1], we discretize it into K+2\nBoolean features by partitioning the [0,1]interval into a setofKsubinterval bins B={(bk\u00001,bk]|1kK}. For\neach bin, the corresponding Boolean feature determines\nwhether f(s, y)2(bk\u00001,bk]. Additionally, two Boolean\nfeatures are deﬁned for the boundary cases f(s, y)=0 and\nf(s, y)=1 . For each real-valued feature, unless speciﬁed\notherwise, we use the bin set B=[ 0,0.1,. . . ,0.9,1.0].\n3.1 Segment Purity\nThe segment purity feature f1(s, y)computes the fraction\nof the notes in segment sthat are harmonic, i.e. belong to\nchord y:\nf1(s, y)=X\nn2s.Notes1[n2y]\n|s.Notes |\nThe duration-weighted version f2(s, y)of the purity fea-\nture weighs each note nby its length n.len :\nf2(s, y)=X\nn2s.Notes1[n2y]⇤n.len\nX\nn2s.Notesn.len\nThe accent-weighted version f3(s, y)of the purity feature\nweighs each note nby its accent weight n.acc :\nf3(s, y)=X\nn2s.Notes1[n2y]⇤n.acc\nX\nn2s.Notesn.acc\nThe 3 real-valued features are discretized using the default\nbin set B.\n3.1.1 Figuration-Controlled Segment Purity\nFor each segment purity feature, we create a ﬁguration-\ncontrolled version that ignores notes that were heuristi-\ncally detected as ﬁguration, i.e. replace s.Notes with\ns.NonFig (y)in each feature formula.\n3.2 Chord Coverage\nThe chord coverage features determine which of the chord\nnotes belong to the segment. The ﬁrst 3 features refer to\nthe triad notes:\nf4(s, y)= 1[y.root 2s.Notes ]\nf5(s, y)= 1[y.third 2s.Notes ]\nf6(s, y)= 1[y.ﬁfth 2s.Notes ]\nA separate feature determines if the segment contains all\nthe notes in the chord:\nf7(s, y)=Y\nn2y1[n2s.Notes ]\nA chord may have an added tone y.added , such as a 4th,\na 6th, or a 7th. If a chord has an added tone, we deﬁne two\nfeatures that determine whether the segment contains the\nadded note:\nf8(s, y)= 1[9y.added ^y.added 2s.Notes ]\nf9(s, y)= 1[9y.added ^y.added /2s.Notes ]274 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Through the ﬁrst feature, the system can learn to prefer the\nadded tone version of the chord when the segment con-\ntains it, while the second feature enables the system to\nlearn to prefer the triad-only version if no added tone is\nin the segment. To prevent the system from recognizing\nadded chords too liberally, we add a feature that is trig-\ngered whenever the total length of the added note in the\nsegment is greater than the total length of the root:\nalen(s, y)=X\nn2s.Notes1[n=y.added ]⇤n.len\nrlen(s, y)=X\nn2s.Notes1[n=y.root]⇤n.len\nf10(s, y)=1[9y.added ]⇤1[alen(s, y)>r l e n (s, y)]\nThe duration-weighted versions of the chord coverage fea-\ntures weigh each chord tone by its total duration in the\nsegment. For the root, the feature would be computed as\nshown below:\nf11(s, y)=X\nn2s.Notes1[n=y.root]⇤n.len\nX\nn2s.Notesn.len\nSimilar features f12andf13are computed for the third and\nthe ﬁfth. The corresponding accent-weighted features f14,\nf15, and f16are computed in a similar way, by replacing\nthe note duration n.len in the duration-weighted formulas\nwith the note accent value n.acc .\nThe duration-weighted feature for the added tone is\ncomputed similarly:\nf17(s, y)=1[9y.added ]⇤X\nn2s.Notes1[n=y.added ]⇤n.len\nX\nn2s.Notesn.len\nFurthermore, by replacing n.len with n.acc , we also ob-\ntain the accent-weighted version f18.\nAn alternative deﬁnition of duration-weighted features\nis based on the proportion of the segment time that is\ncovered by a particular chord note. The corresponding\nduration-weighted feature for the chord root is shown be-\nlow:\nf19(s, y)=X\ne2s.Events1[y.root 2e]⇤e.len\nX\ne2s.Eventse.len\nSimilar duration-weighted features normalized by the seg-\nment length are deﬁned for thirds, ﬁfths, and added notes.\nAll duration-weighted and accent-weighted features are\ndiscretized using the default bin set B.\n3.2.1 Figuration-Controlled Chord Coverage\nFor each chord coverage feature, we create a ﬁguration-\ncontrolled version that ignores notes that were heuristi-\ncally detected as ﬁguration, i.e. replace s.Notes with\ns.NonFig (y)in each feature formula.3.3 Bass\nThe bass note provides the foundation for the harmony of\na musical segment. For a correct segment, its bass note of-\nten matches the root of its chord label. If the bass note in-\nstead matches the chord’s third, ﬁfth, or added dissonance,\nthis may indicate that the chord is inverted. Thus, compar-\ning the bass note with the chord tones can provide useful\nfeatures for determining whether a segment is compatible\nwith a chord label.\nThere are multiple ways to deﬁne the bass note of a seg-\nment s. One possible deﬁnition is the lowest note of the\nﬁrst event in the segment, i.e. s.e1.bass. Comparing it with\nthe root, third, ﬁfth, and added tones of a chord results in\nthe following features:\nf20(s, y)= 1[s.e1.bass =y.root]\nf21(s, y)= 1[s.e1.bass =y.third ]\nf22(s, y)= 1[s.e1.bass =y.ﬁfth]\nf23(s, y)= 1[9y.added ^s.e1.bass =y.added ]\nAn alternative deﬁnition of the bass note of a segment is\nthe lowest note in the entire segment, i.e. min e2s.Ee.bass.\nThe corresponding features will be:\nf24(s, y)= 1[y.root =m i n\ne2s.Ee.bass]\nf25(s, y)= 1[y.third =m i n\ne2s.Ee.bass]\nf26(s, y)= 1[y.ﬁfth =m i n\ne2s.Ee.bass]\nf27(s, y)= 1[9y.added ^y.added =m i n\ne2s.Ee.bass]\nThe duration-weighted version of the bass features\nweigh each chord tone by the time it is used as the low-\nest note in each segment event, normalized by the duration\nof the bass notes in all the events. For the root, the feature\nis computed as shown below:\nf28(s, y)=X\ne2s.Events1[e.bass =y.root]⇤e.len\nX\ne2s.Eventse.len\nSimilar features f29andf30are computed for the third\nand the ﬁfth. The duration-weighted feature for the added\ntone is computed as follows:\nf31(s, y)=1[9y.added ]⇤X\ne2s.E1[e.bass =y.root]⇤e.len\nX\ne2s.Ee.len\nThe corresponding accent-weighted features f31,f32,f33,\nandf34are computed in a similar way, by replacing the\nbass duration e.bass.lenin the duration-weighted formulas\nwith the note accent value e.bass.acc.\nAll duration-weighted and accent-weighted features are\ndiscretized using the default bin set B.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 2753.3.1 Figuration-Controlled Bass\nFor each bass feature, we create a ﬁguration-controlled\nversion that ignores event bass notes that were heuristi-\ncally detected as ﬁguration, i.e. replace e2s.Events with\ne2s.Events ^e.bass / 2s.Fig(y)in each feature formula.\n3.4 Chord Bigrams\nThe arrangement of chords in chord progressions is an im-\nportant component of harmonic syntax [1]. A ﬁrst order\nsemi-Markov CRF model can capture chord sequencing in-\nformation only through the chord labels yandy0of the cur-\nrent and previous segment. To obtain features that gener-\nalize to unseen chord sequences, we follow Radicioni and\nEsposito [9] and create chord bigram features using only\na) the mode : major (M), minor (m), or diminished (d); b)\ntheadded note: none ( ;), fourth (4), sixth (6), or seventh\n(7); and c) the interval in semitones between the roots of\nthe two chords.\ng1(y,y0)=1[y.mode ={M,m,d }^y.added ={;,4,6,7}^\ny0.mode ={M,m,d }^y0.added ={;,4,6,7}^\u0000\u0000y.root \u0000y0.root\u0000\u0000={0,1,. . . ,11}]\nNote that g1(y,y0)is a feature template that can gen-\nerate (3 modes ⇥4 added)2⇥12 intervals = 1,728 dis-\ntinct features. To reduce the number of features, we use\nonly the (mode.added)–(mode.added)’–interval combina-\ntions that appear in the manually annotated chord bigrams\nfrom the training data.\n4. CHORD RECOGNITION DATASETS\nFor evaluation, we used two chord recognition datasets:\n1.BCHD: this is the Bach Choral Harmony Dataset, a\ncorpus of 60 four-part Bach chorales that contains\n5,664 events and 3,090 segments in total [9].\n2.TA VERN: this is a corpus of 27 complete sets of\ntheme and variations for piano, composed by Mozart\nand Beethoven. It consists of 63,876 events and\n12,802 segments overall [2].\nThe BCHD corpus has been annotated by a human expert\nwith chord labels, using the set of labels described in Sec-\ntion 2. Of the 144 possible labels, 102 appear in the dataset\nand of these only 68 appear 5 times or more. Some of the\nchord labels used in the manual annotation are enharmonic,\ne.g. C-sharp major and D-ﬂat major, or D-sharp major and\nE-ﬂat major. Reliably producing one of two enharmonic\nchords cannot be expected from a system that is agnostic\nof the key context. Therefore, we normalize the chord la-\nbels and for each mode we deﬁne a set of 12 canonical\nroots, one for each scale degree. When two enharmonic\nchords are available for a given scale degree, we selected\nthe one with the fewest sharps or ﬂats in the correspond-\ning key signature. Consequently, for the major mode we\nuse the canonical root set {C, Db, D, Eb, F, Gb, G, Ab,\nA, Bb, B }, whereas for the minor and diminished modeswe used the root set {C, C#, D, D#, F, F#, G, G#, A, Bb,\nB}. Thus, if a chord is manually labeled as C-sharp ma-\njor, the label is automatically changed to the enharmonic\nD-ﬂat major. The actual chord notes used in the music are\nleft unchanged. Whether they are spelled with sharps or\nﬂats is immaterial, as long as they are enharmonic with the\nroot, third, ﬁfth, or added note of the labeled chord.\nThe TA VERN dataset2currently contains 17 works by\nBeethoven (181 variations) and 10 by Mozart (100 vari-\nations). The themes and variations are divided into a\ntotal of 1,060 phrases, 939 in major and 121 in minor.\nThe pieces have two levels of segmentations: chords and\nphrases. The chords are annotated with Roman numer-\nals, using the Humdrum representation for functional har-\nmony3. When ﬁnished, each phrase will have annotations\nfrom two different experts, with a third expert adjudicat-\ning cases of disagreement between the two. After adjudi-\ncation, a unique annotation of each phrase is created and\njoined with the note data into a combined ﬁle encoded in\nstandard **kern format. However, many pieces do not cur-\nrently have the second annotation or the adjudicated ver-\nsion. Consequently, we only used the ﬁrst annotation for\neach of the 27 sets. Furthermore, since our chord recog-\nnition approach is key agnostic, we developed a script that\nautomatically translated the Roman numeral notation into\nthe key-independent canonical set of labels used in BCHD.\nBecause the TA VERN annotation does not mark added\nfourth or sixth notes, the only added chords that were gen-\nerated by the translation script were those containing sev-\nenths. This results in a set of 72 possible labels, of which\n69 appear in the dataset.\nFew other annotated chord recognition datasets exist.\nOne of these is the Kostka-Payne corpus4, a dataset of 46\nbrief excerpts compiled by David Temperley from Kostka\nand Payne’s music theory textbook [4]. Several chord\nrecognition systems have used this dataset in the past [8,9].\nHowever, it is smaller than both BCHD and TA VERN,\nwith 3,964 events and only 779 segments. Another dataset\nis Chris Harte’s Beatles collection [3], containing anno-\ntations for 12 complete albums. Though this dataset is\nmuch larger in size, the chord labels are mapped to audio.\nWe considered re-mapping these labels to MIDI ﬁles, but\nhad difﬁculty ﬁnding accurate MIDI ﬁles for most Beatles\nsongs.\n5. EXPERIMENTAL EVALUATION\nWe implemented the semi-Markov CRF chord recognition\nsystem using a multi-threaded package5that has been pre-\nviously used for noun-phrase chunking of informal text [7].\nFollowing the experimental setting from [9], we evaluated\nthe semi-CRF model on BCHD using 10-fold cross val-\nidation: the 60 Bach Chorales were split into 10 folds,\nand each fold was used as test data, with the other nine\nfolds being used for training. We used the same parti-\n2https://github.com/jcdevaney/TA VERN\n3http://www.humdrum.org/Humdrum/representations/harm.rep.html\n4http://www.cs.northwestern.edu/ pardo/kpcorpus.zip\n5http://statnlp.org/research/ie/276 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017tion into folds as that employed by Radicioni and Espos-\nito [9], to enable comparison with their perceptron-trained\nHMM system, henceforth referred to as HMPerceptron.\nWe also used their set of labels, consisting of the 102\nchords observed in the dataset, which corresponds to 90\ncanonical chords. For each feature we computed its fre-\nquency of occurrence in the training data, using only the\ntrue segment boundaries and their labels. To speedup train-\ning and reduce overﬁtting, we only used features whose\ncounts were at least 5. The performance measures were\ncomputed by pooling together the results from the 10 test\nfolds. Table 1 shows the event-level and segment-level\nperformance of the semi-CRF model, together with two\nversions of the HMPerceptron: HMPerceptron 1, for which\nwe do enharmonic normalization both on training and test\ndata, similar to the normalization done for semi-CRF; and\nHMPerceptron 2, which is the original system from [9] that\ndoes enharmonic normalization only on test data. When\ncomputing the segment-level performance, a predicted seg-\nment is considered correct only if both its boundaries and\nits label match those of the true segment.\nSystem Acc E PS RS FS\nsemi-CRF 83.16 % 77.60 % 73.48 % 75.48 %\nHMP-tron 180.30% 74.18% 69.76% 71.90%\nHMP-tron 280.16% 70.24% 65.78% 67.94%\nTable 1 . Comparative results on the BCHD dataset, using\nEvent-level accuracy (Acc E) and Segment-level precision\n(PS), recall (R S), and F-measure (F S).\nThe semi-CRF model achieves a 3% improvement in\naccuracy over the original HMPerceptron model, which\ncorresponds to a 15% relative error reduction. The im-\nprovement in accuracy over HMPerceptron 1is statistically\nsigniﬁcant at a p-value of 0.01, using a one-tailed Welch’s\nt-test over the sample of 60 chorale results. The improve-\nment in segment-level performance is even more substan-\ntial, with a 7.5% absolute improvement in F-measure over\nthe original HMPerceptron model, and a 3.6% improve-\nment in F-measure over the HMPerceptron 1version, which\nis statistically signiﬁcant at a p-value of 0.04, using a one-\ntailed Welch’s t-test.\nError analysis revealed wrong predictions being made\non chords that contained dissonances that spanned the du-\nration of the entire segment (e.g. a second above the root of\nthe annotated chord), likely due to an insufﬁcient number\nof such examples during training. Manual inspection also\nrevealed a non-trivial number of cases in which the authors\ndisagreed with the manually annotated chords, e.g. some\nchord labels were clear mistakes, as they did not contain\nany of the notes in the chord. This further illustrates the\nnecessity of building music analysis datasets that are an-\nnotated by multiple experts, with adjudication steps akin\nto the ones followed by TA VERN.\nTo evaluate on the TA VERN corpus, we created a test\ndataset from 6 Beethoven sets ( B063 ,B064 ,B065 ,B066 ,\nB068 ,B069 ) and 4 Mozart sets ( K025 ,K179 ,K265 ,K353 ).The remaining 11 Beethoven sets and 6 Mozart sets were\nused for training. All sets were normalized enharmoni-\ncally before being used for training or testing. Table 2\nshows the event-level and segment-level performance of\nthe semi-CRF and HMPerceptron model on the TA VERN\ndataset. Despite the smaller number of chord labels (69 in\nSystem Acc E PS RS FS\nsemi-CRF 77.47 % 66.86 % 60.35 % 63.44 %\nHMP-tron 60.55% 27.83% 23.21% 25.31%\nTable 2 . Comparative results on the TA VERN dataset, us-\ning Event-level accuracy (Acc E) and Segment-level preci-\nsion (P S), recall (R S), and F-measure (F S).\nTA VERN vs. 90 in BCHD), the results in Tables 1 and 2\nshow that chord recognition is substantially more difﬁcult\nin the TA VERN dataset. The comparatively lower perfor-\nmance on TA VERN is likely due to the substantially larger\nnumber of ﬁgurations and higher rhythmic diversity of the\nvariations compared to the easier, mostly note-for-note tex-\nture of the chorales. Error analysis on TA VERN revealed\nmany segments where the ﬁrst event did not contain the\nroot of the chord. For such segments, HMPerceptron in-\ncorrectly assigned chord labels whose root matched the\nbass of this ﬁrst event. Since a single wrongly labeled\nevent invalidates the entire segment, this can explain the\nlarger discrepancy between the event-level accuracy and\nthe segment-level performance. In contrast, semi-CRF as-\nsigned the correct labels in these cases, likely due to its\nability to exploit context through segment-level features,\nsuch as the chord root coverage feature f4and its duration-\nweighted version f11.\n6. CONCLUSION AND FUTURE WORK\nWe presented a semi-Markov CRF approach to chord\nrecognition that does joint segmentation and labeling of\ntonal music in symbolic form. Compared to event-level\ntagging approaches based on HMMs or linear CRFs, the\nsegmental CRF approach has the advantage that it can ac-\ncommodate features that consider all the notes in a candi-\ndate segment. This capability was shown to be especially\nuseful for music with complex textures that diverge from\nthe simpler note-for-note structures of the Bach chorales.\nOn the more difﬁcult TA VERN corpus, the semi-CRF sub-\nstantially outperformed a previous system based on event-\nlevel tagging, thus validating empirically the suitability of\njoint segmentation and labeling for chord recognition.\nManually engineering good features for chord recogni-\ntion is a cognitively demanding and time consuming pro-\ncess that requires music theoretical knowledge and that\nis unlikely to lead to optimal sets of features, especially\nwhen complex features are involved. In future work we\nplan to investigate automatic feature extraction using re-\ncurrent neural networks (RNN) that preserve the semi-\nMarkov property, such as the recently proposed segmental\nRNNs [5].Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 2777. ACKNOWLEDGMENTS\nWe would like to thank Patrick Gray for his help with pre-\nprocessing the TA VERN corpus. We also greatly appreci-\nate the prompt help provided by Daniele P. Radicioni and\nRoberto Esposito. Their detailed responses to our ques-\ntions and willingness to share the original BCHD dataset\nand the HMPerceptron code enabled us to conduct a more\nthorough experimental comparison with their system. Fi-\nnally, we would like to thank the anonymous reviewers for\ntheir insightful comments on the initial draft of this paper.\n8. REFERENCES\n[1]E. Aldwell, C. Schachter, and A. Cadwallader. Har-\nmony and Voice Leading . Schirmer, 4th edition, 2011.\n[2]Johanna Devaney, Claire Arthur, Nathaniel Condit-\nSchultz, and Kirsten Nisula. Theme and variation en-\ncodings with roman numerals (TA VERN): A new data\nset for symbolic music analysis. In International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2015.\n[3]Christopher Harte. Towards Automatic Extraction of\nHarmony Information from Music Signals . PhD thesis,\nQueen Mary University of London, August 2010.\n[4]Stefan Kostka and Dorothy Payne. Tonal Harmony .\nMcGraw-Hill, 1984.\n[5]Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith,\nand Steve Renals. Segmental recurrent neural networks\nfor end-to-end speech recognition. In INTERSPEECH ,\nSan Francisco, CA, September 2016.\n[6]H. John Maxwell. An expert system for harmoniz-\ning analysis of tonal music. In Mira Balaban, Kermal\nEbcio ˘glu, and Otto Laske, editors, Understanding Mu-\nsic with AI , pages 334–353. MIT Press, Cambridge,\nMA, USA, 1992.\n[7]Aldrian Obaja Muis and Wei Lu. Weak semi-Markov\nCRFs for noun phrase chunking in informal text. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 714–719, San Diego, California, June 2016. As-\nsociation for Computational Linguistics.\n[8]Bryan Pardo and William P. Birmingham. Algo-\nrithms for chordal analysis. Computer Music Journal ,\n26(2):27–49, July 2002.\n[9]Daniele P. Radicioni and Roberto Esposito. BREVE:\nan HMPerceptron-based chord recognition system. In\nAdvances in Music Information Retrieval , pages 143–\n164. Springer Berlin Heidelberg, 2010.\n[10] Christopher Raphael and Josh Stoddard. Harmonic\nanalysis with probabilistic graphical models. In Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , 2003.[11] Thomas Rocher, Matthias Robine, Pierre Hanna, and\nRobert Strandh. Dynamic chord analysis for symbolic\nmusic. In International Computer Music Conference,\nICMC , 2009.\n[12] Sunita Sarawagi and William W. Cohen. Semi-Markov\nConditional Random Fields for information extraction.\nInProceedings of the 17th International Conference\non Neural Information Processing Systems , NIPS’04,\npages 1185–1192, Cambridge, MA, USA, 2004. MIT\nPress.\n[13] Ricardo Scholz and Geber Ramalho. Cochonut: Rec-\nognizing complex chords from midi guitar sequences.\nInInternational Conference on Music Information\nRetrieval (ISMIR) , pages 27–32, Philadelphia, USA,\nSeptember 14-18 2008.\n[14] David Temperley and Daniel Sleator. Modeling meter\nand harmony: A preference-rule approach. Computer\nMusic Journal , 23(1):10–27, January 1999.\n[15] Terry Winograd. Linguistics and the computer analysis\nof tonal harmony. Journal of Music Theory , 12(1):2–\n49, 1968.278 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Structured Training for Large-Vocabulary Chord Recognition.",
        "author": [
            "Brian McFee",
            "Juan Pablo Bello"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1414880",
        "url": "https://doi.org/10.5281/zenodo.1414880",
        "ee": "https://zenodo.org/records/1414880/files/McFeeB17.pdf",
        "abstract": "Automatic chord recognition systems operating in the large-vocabulary regime must overcome data scarcity: cer- tain classes occur much less frequently than others, and this presents a significant challenge when estimating model parameters. While most systems model the chord recog- nition task as a (multi-class) classification problem, few attempts have been made to directly exploit the intrinsic structural similarities between chord classes. In this work, we develop a deep convolutional-recurrent model for automatic chord recognition over a vocabulary of 170 classes. To exploit structural relationships between chord classes, the model is trained to produce both the time-varying chord label sequence as well as binary en- codings of chord roots and qualities. This binary encod- ing directly exposes similarities between related classes, allowing the model to learn a more coherent representa- tion of simultaneous pitch content. Evaluations on a cor- pus of 1217 annotated recordings demonstrate substantial improvements compared to previous models.",
        "zenodo_id": 1414880,
        "dblp_key": "conf/ismir/McFeeB17",
        "keywords": [
            "data scarcity",
            "large-vocabulary regime",
            "model parameters",
            "structural similarities",
            "deep convolutional-recurrent model",
            "chord recognition task",
            "binary encodings",
            "similarity between chord classes",
            "coherent representation",
            "pitch content"
        ],
        "content": "STRUCTURED TRAINING FOR LARGE-VOCABULARY CHORD\nRECOGNITION\nBrian McFee1,2\n1Center for Data Science\nNew York University\nbrian.mcfee@nyu.eduJuan Pablo Bello2\n2Music and Audio Research Laboratory\nNew York University\njpbello@nyu.edu\nABSTRACT\nAutomatic chord recognition systems operating in the\nlarge-vocabulary regime must overcome data scarcity: cer-\ntain classes occur much less frequently than others, and\nthis presents a signiﬁcant challenge when estimating model\nparameters. While most systems model the chord recog-\nnition task as a (multi-class) classiﬁcation problem, few\nattempts have been made to directly exploit the intrinsic\nstructural similarities between chord classes.\nIn this work, we develop a deep convolutional-recurrent\nmodel for automatic chord recognition over a vocabulary\nof 170 classes. To exploit structural relationships between\nchord classes, the model is trained to produce both the\ntime-varying chord label sequence as well as binary en-\ncodings of chord roots and qualities. This binary encod-\ning directly exposes similarities between related classes,\nallowing the model to learn a more coherent representa-\ntion of simultaneous pitch content. Evaluations on a cor-\npus of 1217 annotated recordings demonstrate substantial\nimprovements compared to previous models.\n1. INTRODUCTION\nAutomatic chord recognition has been an active area of re-\nsearch within music informatics for nearly two decades [8].\nChord recognition systems take as input an audio signal,\nand produce a time-varying symbolic representation of the\nsignal in terms of chord labels , which encode simultaneous\npitch class content, such as C:maj orG:hdim7 . Many\nsystems focus on simpliﬁed versions of this task, by pre-\ndicting only the root note and major orminor qualities,\norno-chord (N). Recently, interest has shifted toward the\nlarge-vocabulary regime, where a broader class of chord\nqualities must be estimated, such as triads, sixths, sevenths,\nand suspended chords.\nTypical chord recognition systems model the task as a\ntime-varying multi-class classiﬁcation problem. This ap-\nproach may be reasonable for the small-vocabulary regime,\nwhere the classes are sufﬁciently distinct to be modeled\nc\rBrian McFee1,2, Juan Pablo Bello2. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Brian McFee1,2, Juan Pablo Bello2. “Structured training\nfor large-vocabulary chord recognition”, 18th International Society for\nMusic Information Retrieval Conference, Suzhou, China, 2017.as unrelated, and each class may be observed with ap-\nproximately uniform probability. However, in the large-\nvocabulary setting, the multi-class formulation ignores the\nstructural similarity between related chords, such as the\nshared notes between C:min andC:min7 . Moreover,\nthe distribution of classes becomes highly skewed, thereby\nmaking it difﬁcult to model these relationships from purely\nsymbolic representations with no additional structure. We\nhypothesize that leveraging known relationships between\nchord classes in terms of common roots and shared pitch\nclasses can help mitigate the problem of observation bias,\nresulting in more accurate models of rare classes.\n1.1 Our Contributions\nWe address the problem of large-vocabulary chord recog-\nnition by introducing a structured representation of chord\nqualities, which decouples the problem of detecting roots\nand pitch classes from the problem of mapping these prop-\nerties onto symbolic labels. We integrate this represen-\ntation with deep, convolutional-recurrent neural networks,\nwhich are trained end-to-end to predict time-varying chord\nsequences from spectral audio representations. The pro-\nposed models achieve substantially higher accuracy than\nprevious models based on convolutional networks and hid-\nden Markov models, resulting in absolute gains of 4–5% in\nthe most difﬁcult categories (sevenths and tetrads).\n2. RELATED WORK\nChord recognition has received a substantial amount of\nattention in the MIR literature, and a comprehensive sur-\nvey of existing methods is beyond the scope of this paper.\nHere, we highlight the work that is most closely related to\nthe proposed methods in this paper.\nHidden Markov models (HMMs) have been a popular\nmethod for designing chord recognition systems, and pro-\nvide a ﬂexible framework in which to integrate musical\ndomain knowledge. The general HMM approach models\nchord identities as latent state variables to be inferred from\nobserved time-series features ( e.g., chroma vectors). Sys-\ntems like Chordino [17] and HPA [20] extend this idea by\nintroducing additional latent variables to model key, bass,\nand metrical position. In these systems, bass is modeled\nby weighting or partitioning the frequency range to pro-\nduce distinct bass andtreble chroma observations. The K-\nstream HMM takes this idea a step further by modeling K188distinct frequency sub-bands, though it does not explicitly\ninfer bass [5]. The structured representation we describe\nin Section 3 differs in that root, bass, and chord quality are\njointly inferred from the entire spectrum, and it makes no\nassumptions about absolute height. Weller et al. [23] also\nadapted structured training techniques for chord recogni-\ntion, but at the level of dynamics rather than the chord vo-\ncabulary.\nIn recent years, deep learning methods have been in-\ncreasingly popular for chord recognition. The majority of\nexisting systems are trained in two stages. First, a model\nis built ﬁrst to encode short patches of audio, e.g., as an\nidealized chroma vector [2, 16] or likelihood distribution\nover chord categories [7, 11, 22, 24]. Second, a dynamics\nmodel integrates the time-series of learned representations\nto produce a sequence of predicted labels, e.g., using an\nHMM [11, 24], recurrent neural network (RNN) [2, 22],\nor simple point-wise prediction [16]. The models pro-\nposed here differ in that they are jointly trained end-to-end\nfrom spectral features, and learn the internal representation\nalong with the dynamics using multiple recurrent layers.\nRegardless of the model architecture, it is common to\nexploit some structural properties of chords, e.g., by ty-\ning model parameters for the same chord quality across\nroots [11], or rotating chroma vectors through all possible\nroot positions during training [5]. Although the methods\nwe propose do not model quality independent of root, they\ndo model active pitch class sets independently. Chroma\nrotation can be viewed as a form of data augmentation,\nand the models we develop beneﬁt substantially from a\nslightly more general form of augmentation described in\nSection 3.4. To the best of our knowledge, the proposed\nmethod is the ﬁrst to exploit similarities between chords\nby jointly modeling labels and structured encodings.\n3. METHODS\nThis section outlines the data preparation, architectures,\nand training strategies for the models under comparison.\nWe consider three independent design choices: convolu-\ntional or recurrent decoding, the inclusion of structured\noutput training, and the use of data augmentation. This\nresults in eight model conﬁgurations.\n3.1 Encoder-decoder models\nThe models we investigate fall under the umbrella of\nencoder-decoder architectures [3]. The encoder compo-\nnent maps time-varying input (audio) into a latent feature\nspace, while the decoder component maps from the latent\nfeature space to the output space (chord labels).\n3.1.1 The encoder architecture\nThe encoder, and depicted in Figure 1, is common to all\nmodels considered in this paper. Input audio is represented\nas aT\u0002Ftime-series of log-power constant-Q transform\n(CQT) spectra (for Tframes andFfrequency bands). Af-\nter batch normalization [13], the ﬁrst convolutional layer\nconsists of a single two-dimensional 5\u00025ﬁlter, followed\nConv2D\n5x5 [1]\nReLUConv1D\n1 [36]\nReLUBi-GRUBatch-\nnormFigure 1 . The encoder module uses a convolutional-\nrecurrent network architecture to map the input (CQT\nframes) to a sequence of hidden state vectors h(t)2RD.\nby a bank of 36single-frame, one-dimensional convolu-\ntional ﬁlters, resulting in a T\u000236feature map. Both lay-\ners use rectiﬁed linear (ReLU) activations. The ﬁrst layer\ncan be interpreted as a harmonic saliency enhancer, as it\ntends to learn to suppress transients and vibrato while em-\nphasizing sustained tones. The second layer summarizes\nthe pitch content of each frame, and can be interpreted as\na local feature extractor.\nFinally, the local features are encoded by a bi-\ndirectional gated recurrent unit (GRU) model [4]. The\nGRU model is similar to the long-short-term memory\n(LSTM) model [10], but has fewer parameters and per-\nforms comparably in practice [14]. For a sequence of d-\ndimensional input vectors x(t)2Rd, a GRU layer pro-\nduces a sequence of D-dimensional hidden state vectors\nh(t)2[\u00001;+1]Das follows:\nr(t) =\u001b(Wrx(t) +Trh(t\u00001) +br) (1)\nu(t) =\u001b(Wux(t) +Tuh(t\u00001) +bu) (2)\n^h(t) =\u001a(Whx(t) +Th(r(t)\fh(t\u00001)) +bh)(3)\nh(t) =u(t)\fh(t\u00001) + (1\u0000u(t))\f^h(t);(4)\nwherer(t);u(t)2[0;1]Dare the reset andupdate vec-\ntors, each of which are controlled by RNN dynamics\ndepending on the input x(t)and previous hidden state\nh(t\u00001),\u001b(x) = (1 + e\u0000x)\u00001denotes the logistic func-\ntion, and\u001a= tanh . The parameters are the input map-\npingsW\u00032RD\u0002d, transition operators T\u00032RD\u0002D, and\nbias vectors b\u00032RD.\nWhen an element jof the update vector uj(t)\u00191,\nthe corresponding element of the previous hidden state is\ncopied directly to the current state hj(t) hj(t\u00001).\nOtherwise, if r(t)\u00191, thenh(t)evolves according to stan-\ndard RNN dynamics. However, when both u(t);r(t)\u00190,\nthehterm in (3) goes to 0 and the update resets , depend-\ning only on the input x(t). This allows the GRU model\nto persist a hidden state across arbitrarily long spans of\ntime, and capture variable-length temporal dependencies.\nThese properties make the GRU model appealing for chord\nrecognition, where dependencies may span long ranges\n(compared to frames), and are subject to sudden changes\nrather than gradual evolution.\nThe bi-directional variant consists of two independent\nGRUs, one running in each temporal direction, whoseProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 189hidden state vectors are concatenated to produce the bi-\ndirectional hidden state vector h(t). This layer integrates\nover the entire input signal, and provides temporal smooth-\ning and context for the encoded feature representation.\n3.1.2 Decoder architectures\nWe investigate two models, depicted in Figure 3, for de-\ncodingh(t)to the sequence of chord labels ^y(t). The ﬁrst\nmodel, denoted CR1, decodes each frame independently:\n^y(t) := softmax (Wyh(t) +by); (5)\nwhere the soft-max operates over the chord vocabulary V,\nproducing a likelihood vector ^y(t)2[0;1]jVj. For the CR1\narchitecture, we set the dimensionality of the hidden state\nvector to 512 (256 for each temporal direction).\nThe second model, denoted CR2, uses a bi-GRU layer\nto maph(t)to an intermediate representation h2(t)prior\nto frame-wise decoding by eq. (5). To keep the number\nof parameters roughly comparable between CR1 andCR2,\nwe set the dimensionality CR2’s recurrent layers to 256.\nFor each conﬁguration, all model parameters \u0002are\njointly trained to maximize the empirical log-likelihood:\nargmax\n\u0002X\ntX\nc2Vyc(t) log ^yc(t); (6)\nwhere the reference labels are one-hot encoded vectors\ny(t)2 f0;1gjVj. While both architectures have access\nto the entire observation sequence, CR2 may be better at\ncapturing long-range interactions. This should allow the\nencoder to focus on short-term smoothing and local con-\ntext, while the decoder can model chord progressions and\nglobal context. In the CR1 model, the encoder is responsi-\nble for both short- and long-range interactions.\nAt test time, the maximum likelihood label is selected\nfor each frame, and the series of chord labels is run-length\nencoded to form the estimated annotation for the track.\n3.2 Chord vocabulary simpliﬁcation\nTo formulate chord recognition as a classiﬁcation task, we\ndeﬁne a mapping of all valid chord descriptions to a ﬁnite\nvocabularyV.1First, inversions and suppressed or addi-\ntional notes are discarded, e.g.:\nD[:maj(9)/37! D[:maj/37! D[:maj:\nNext, labels are decomposed into root andpitch classes\n(relative to the root) using mireval [21]:\nD[:maj7!(\n1 root\n(0;4;7) pitch classes:\nThe set of active pitch classes is matched against 14\ntemplates: min, maj, dim, aug, min6, maj6,\nmin7, minmaj7, maj7, 7, dim7, hdim7,\nsus2, sus4 . The root and matched template are\n1Avalid chord is any string belonging to the formal language of\nHarte et al. [9], or the extended grammar implemented by JAMS [12].combined, and mapped to a canonical form to resolve\nenharmonic equivalences:\n(1;(0;4;7))7!C]:maj:\nIf the pitch class set does not match one of the templates,\nit is mapped to the unknown chord symbol X; the no-chord\nsymbol is represented distinctly as N. The ﬁnal vocabulary\ncontains 170 classes: 2 special symbols ( N, X ), and 12\u0002\n14 = 168 combinations of root and quality.\n3.3 Structured training\nThe CR models described above map each hidden state\nvectorh(t)to a ﬁxed vocabulary produced described in\nSection 3.2. They can be optimized in the usual way to\nmaximize (6), but this approach has some clear drawbacks.\nFirst, it does not leverage the inherent structure of the\nspace of chords. If the model predicts B:maj instead of\nB:7, it is penalized just as badly as if it had predicted\nC:maj . This is at odds with evaluation, where predictions\nare evaluated along multiple dimensions, such as capturing\nthe root, third, or ﬁfth. More generally, some mistakes are\nsimply more severe than others, and this is not reﬂected in\na 1-of-K classiﬁcation formulation.\nSecond, the chord simpliﬁcation strategy is lossy in that\nit discards information such as suppressed or additional\nnotes. This can render certain chords ambiguous, and can\nintroduce discrepancies between the (simpliﬁed) annota-\ntion and the corresponding acoustic content. Continuing\ntheD[:maj(9)/3 example, the simpliﬁcation C]:maj\nimplies the absence of D], although it was explicitly in-\ncluded in the original annotation and should be expected\nin the signal. This introduces label noise to the model, and\nmay negatively impact accuracy.\nThird, out-of-gamut chords all map to a common class\nX, despite having disparate roots and tonal content. This\nclass provides little useful information to the model while\ntraining. At test time, it would be beneﬁcial if the model\ncould predict “nearby” chords, but multi-class training pro-\nvides little incentive to learn this behavior.\nTo counteract these effects, we introduce a structured\nrepresentation, depicted in Figure 2. This is inspired by the\nstandard evaluation criteria for chord recognition, which\noperate over a decomposed representation of ( root,pitch\nclasses ,bass) [21]. This representation can be computed\nfor any valid chord label, and provided as supervision\nto the model, thereby helping it learn common features\nshared by similar chords. At prediction time, the struc-\ntured representation is used as an intermediate representa-\ntion which contributes to the chord label prediction, which\ncan now be interpreted as a human-readable decoding of\nthe structured representation.\nThe structured models (denoted as CR1/2+S ), depicted\nin Figure 3, predict for each frame tthe root pitch class\n(C–B, plus Nfor no-root), the bass pitch class, and the ac-\ntive pitch classes from the hidden state vector h(t). Root\nand bass estimation are modeled as a multi-class predic-\ntion with a soft-max non-linearity. Pitch class prediction190 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Db:maj(9)/3 C#:maj\nCD E F GA B NSimplification\nEncodingroot\nbasspitchesFigure 2 . Target chords are represented in both simpliﬁed\ncanonical form (Section 3.2), and as binary vectors encod-\ning the root, bass, and pitch classes (Section 3.3). The spe-\ncial symbols N,X map to an extra root/bass class N, and\nthe all-zeros pitch vector.\nChordsRoot\nPitches\nBassBi-GRU\n[256]Encoder\n[256]InputChordsRoot\nPitches\nBassEncoder\n[512]InputChordsBi-GRU\n[256]Encoder\n[256]InputChordsEncoder\n[512]Input\nCR1\nCR2\nCR1+S\nCR2+S\nFigure 3 . Block diagrams of all architectures described in\nSection 3. The encoder block is depicted in Figure 1.\nis modeled as a multi-label prediction, and uses a logis-\ntic (sigmoid) non-linearity. This results in an idealized\nchroma representation similar to that of Korzeniowski and\nWidmer [16], but estimated from the full input observation\nrather than a ﬁxed spectrogram patch. An illustrative ex-\nample of this predicted encoding is provided in Figure 4.\nIt is generally non-trivial to invert the root-pitch-bass\nrepresentation to a unique chord label. Therefore, these\nthree layers are concatenated, along with the hidden state\nh(t), to produce the structured representation from which\nthe chord label is predicted. During training, the struc-\ntured models learn to minimize the sum of losses across\nall outputs: root, pitches, bass, and label ^y(t). Minimizing\ntheroot andpitches losses corresponds to maximizing the\nroot andtetrads recall scores during training, while eq. (6)\nlearns the decoding into the human-readable chord vocab-\nulary. This formulation effectively decouples the problems\nof root and pitch class identiﬁcation from chord annotation,\nwhich is known to be subjective [11].\n3.4 Data augmentation\nTo increase training set variability, we apply pitch-shifting\ndata augmentation using MUDA [18]. For each training\nexample, 12 deformations are generated by shifting up or\ndown by between 1–6 semitones. Because each observa-\ntion exists in all twelve root classes, this provides a brute-\nforce, approximate root invariance to the model. Models\ntrained with data augmentation are denoted by +A.\n10 15 20 25 30\nTimeA#:majA#:minC:majF:7F:majG#:majG:majG:minNA#:min6C:7D#:majF:minG:7G:min7Chords\nReference\nEstimateFigure 4 . The predicted chord encodings and labels for\nThe Beatles — Hold Me Tight by model CR2+S+A .\n4. EV ALUATION\nFor evaluation, we used the dataset provided by Humphrey\nand Bello [11], which includes 1217 tracks from the Iso-\nphonics, Billboard, RWC Pop, and MARL collections. To\nfacilitate comparison with previous work, we retain the\nsame 5-fold cross-validation splits, and randomly hold out\n1/4 of each training set for validation. We compare to two\nstrong baselines: a deep convolutional network [11] (de-\nnoted DNN ), and the K-stream HMM [5] ( KHMM ).2\n4.1 Pre-processing\nFeature extraction was performed with librosa 0.5.0 [19].\nEach track was represented as a log-power constant-Q\nspectrogram with 36 bins per octave, spanning 6 octaves\nstarting at C1, and clipped at 80dB below the peak. Signals\nwere analyzed at 44.1KHz with a hop length of 4096 sam-\nples, resulting in a frame rate of approximately 10.8Hz.\n4.2 Training\nAll models are trained on 8-second patches (86 frames),\nthough they readily support input of arbitrary length. For\ntracks with multiple reference annotations, the output is\nselected uniformly at random from all references for the\npatch, which reduces sampling bias toward speciﬁc anno-\ntators. Models are trained using mini-batches of 32 patches\nper batch, and 512 batches per epoch. We use the ADAM\noptimizer [15], and reduce the learning rate if there is no\nimprovement in validation score after 10 epochs. Train-\ning is stopped early if there is no improvement in valida-\ntion score after 20 epochs, and limited to a maximum of\n100 epochs total. For all models, validation score is deter-\nmined solely by label likelihood (eq. (6)). All models were\nimplemented with Keras 2.0 and Tensorﬂow 1.0 [1, 6].3\n4.3 Results\nThe main results of the evaluation are listed in Fig-\nure 5, which illustrates the median weighted recall scores\nachieved by each model.4Each subplot reports the recall\n2Comparisons were facilitated using the pre-computed outputs pro-\nvided at https://github.com/ejhumphrey/ace-lessons .\n3Our implementation is available at https://github.com/\nbmcfee/ismir2017_chords .\n4The trends for the mean scores are qualitatively similar, but the scores\nare lower for all models. We report the median here to reduce the in-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 1910.80 0.85CR2+S+A\nCR2+S\nCR2+A\nCR2\nCR1+S+A\nCR1+S\nCR1+A\nCR1\nKHMM\nDNN\nroot\n0.775 0.800 0.825\nthirds\n0.75 0.80\ntriads\n0.65 0.70\nsevenths\n0.60 0.65CR2+S+A\nCR2+S\nCR2+A\nCR2\nCR1+S+A\nCR1+S\nCR1+A\nCR1\nKHMM\nDNN\ntetrads\n0.80 0.85\nmajmin\n0.80 0.85\nmirexFigure 5 . Weighted recall scores for all methods under comparison. Each dot represents the median score across all test\npoints, with error bars covering the 95% conﬁdence interval estimated by bootstrap sampling. KHMM denotes the K-stream\nHMM of Cho [5]; DNN denotes the convolutional network of Humphrey and Bello [11].\nscores computed by mireval : 1. root; 2.thirds : root\nand third; 3. triads : root, third, and ﬁfth; 4. sevenths : root,\nthird, ﬁfth, and seventh; 5. tetrads : all intervals; 6. ma-\nj-min : 12 major, 12 minor, and Nclass; and 7. MIREX : at\nleast three correct notes.\nFrom Figure 5, several trends can be observed. First,\ndata augmentation ( +Avariants) provides a consistent and\nsubstantial improvement for all models. This is to be ex-\npected, since the CRmodels do not separate root from\nquality. Note that DNN models these independently, and\nKHMM was trained with chroma-rotation data augmenta-\ntion, so it is unsurprising that augmentation is necessary to\nmatch performance of these methods.\nSecond, structured training ( +Svariants) provides a\nmodest, but consistent improvement, for both the shallow\nCR1 and deep CR2 decoder models. The difference is most\npronounced in the root evaluation, which is expected due\nto the explicit objective to correctly identify the root.\nThird, the deep decoder models CR2 provide another\nsmall, but consistent improvement over the shallow de-\ncoders CR1. The aggregate scores are reported in Table 1;\nfor brevity, only the models with data augmentation are in-\ncluded. The combined effect of structured training, deep\ndecoder, and data augmentation ( CR2+S+A ) results in the\nhighest scoring model across all metrics.\n4.4 Error analysis\nTo get some more insight about the mistakes made by the\nmodel at test time, we illustrate the frame-wise, within-\nﬂuence of the erroneous or otherwise spurious reference annotations re-\nported by Humphrey and Bello [11].\nmin\nmaj\ndim\naug\nmin6\nmaj6\nmin7\nminmaj7\nmaj7\n7\ndim7\nhdim7\nsus2\nsus4\nN\nX\nEstimatemin\nmaj\ndim\naug\nmin6\nmaj6\nmin7\nminmaj7\nmaj7\n7\ndim7\nhdim7\nsus2\nsus4\nN\nXReference\n0.00.20.40.60.81.0\nFigure 6 . Within-root, frame-wise quality confusions for\nthe best performing model CR2+S+A . The value at row i,\ncolumnjcorresponds to the fraction of frames labeled as\nclassibut predicted as class j.\nroot quality confusion matrix for the CR2+S+A model in\nFigure 6. For each frame of a test track, its (simpliﬁed)\nreference label is compared to the label estimated by the\nmodel if they match at the root. Results are then aggre-\ngated across all test tracks, and normalized by (reference\nquality) frequency to produce the confusion matrix. Under\nthis evaluation, the CR2+S+A achieves 63.6% accuracy of\ncorrectly identifying the simpliﬁed chord label (root and\nquality) at the frame level.192 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Method Root Thirds Triads Sevenths Tetrads Maj-Min MIREX\nCR2+S+A 0.861 0.836 0.812 0.729 0.671 0.855 0.852\nCR2+A 0.850 0.828 0.801 0.719 0.659 0.845 0.837\nCR1+S+A 0.850 0.824 0.801 0.716 0.648 0.842 0.832\nCR1+A 0.841 0.815 0.791 0.702 0.647 0.834 0.829\nKHMM [5] 0.849 0.822 0.785 0.674 0.629 0.817 0.827\nDNN [11] 0.838 0.809 0.766 0.654 0.605 0.803 0.812\nTable 1 . Median weighted recall scores for methods under comparison.\nmin\nmaj\ndim\naug\nmin6\nmaj6\nmin7\nminmaj7\nmaj7\n7\ndim7\nhdim7\nsus2\nsus4\nN\nX\nEstimatemin\nmaj\ndim\naug\nmin6\nmaj6\nmin7\nminmaj7\nmaj7\n7\ndim7\nhdim7\nsus2\nsus4\nN\nXReference\n0.15\n0.10\n0.05\n0.000.050.100.15\nFigure 7 . The difference between confusion matrices for\nCR2+S+A and the unstructured CR2+A (best viewed in\ncolor). Positive values along the diagonal indicate in-\ncreased accuracy for CR2+S+A .\nIn Figure 6, the ﬁrst obvious trend is a bias to-\nward min andmaj, in accordance with the natural bias\nin the training set (13.6% and 52.5% of the data, re-\nspectively, by duration). Note, however, that the con-\nfusions are generally understandable as simpliﬁcations:\ne.g., (min7 ,minmaj7 )!min and (maj7 ,7)!maj. The\nmodel still appears to struggle with 6th and suspended\nchords, which account for 1.5% and 2.5% of the data, re-\nspectively. The bottom row corresponds to out-of-gamut\nX-chords, which map overwhelmingly to maj andmin.\nThis can be explained by examining which labels map to\nXduring simpliﬁcation. There are 4557 instances of such\nchords in the corpus (2.2% of the data), and of these, 2091\nare 1-chords (only the root) and 2365 are power chords\n(root+ﬁfth), neither of which map unambiguously onto the\nsimpliﬁed vocabulary. The model appears to resolve these\ntoward the more commonly used min andmaj qualities.\nTo understand the inﬂuence of structured training, Fig-\nure 7 illustrates the difference between the confusion ma-\ntrices of the structured model CR2+S+A and the unstruc-\ntured model CR2+A . Positive values (red) along the diago-\nnal indicate increased accuracy for the structured model,\nwhile negative values along the diagonal (blue) indicate\ndecreased accuracy. The net effect is positive, increasing\naccuracy by +0.8% over CR2+A (62.8%).\nDespite a slight degradation for maj7 , there are sub-stantial improvements for aug,dim7 ,hdim7 , and mod-\nest improvement for sus4 . Moreover, the negative values\nin the second column reveal a consistent reduction of con-\nfusions to maj. This indicates that the structured model is\nmore robust to quality bias in the training set. Compared\nto the unstructured model, the structured model reduces\nconfusions from aug to (maj,7), and dim7 to (min,7,\nN). The CR2+S+A still performs poorly on the rarest class\nminmaj7 (0.03% of the data), but compared to CR2+A , it\nresolves toward min more often and min7 less often. The\nstructured model appears to be better at abstaining from\npredicting a seventh if it appears unlikely, rather than pre-\ndict the wrong seventh.\n5. CONCLUSION\nThis work developed deep architectures and a structured\ntraining framework for chord recognition in large vocab-\nularies. Although the proposed models improve over the\nbaseline methods, there are clear directions forward in ex-\ntending the ideas presented here. First, although the pro-\nposed model predicts the bass note, this feature is only\nused for establishing context in decoding, and the model\ndoes not predict inversions. Supporting inversion predic-\ntion would be a simple extension of the method described\nhere, and would not require creating special vocabulary en-\ntries for each potential inversion. Second, the structured\nrepresentation facilitates modeling infrequently observed,\ncomplex chords, and could readily be extended to support\nextended chords by using a multi-octave pitch class repre-\nsentation. However, doing so effectively—and evaluating\nthe resulting predictions—would require larger annotated\ncorpora for these classes than are presently available.\n6. ACKNOWLEDGMENTS\nBM acknowledges support from the Moore-Sloan data sci-\nence environment at NYU. We thank the NVIDIA Corpo-\nration for the donation of a Tesla K40 GPU.\n7. REFERENCES\n[1] Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Ian Goodfellow, Andrew Harp, Geoffrey\nIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-\nicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-\nberg, Dan Man ´e, Rajat Monga, Sherry Moore, DerekProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 193Murray, Chris Olah, Mike Schuster, Jonathon Shlens,\nBenoit Steiner, Ilya Sutskever, Kunal Talwar, Paul\nTucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-\nnanda Vi ´egas, Oriol Vinyals, Pete Warden, Martin\nWattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang\nZheng. TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. Software available from\ntensorﬂow.org.\n[2] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Audio chord recognition with recurrent\nneural networks. In ISMIR , pages 335–340, 2013.\n[3] Kyunghyun Cho, Aaron Courville, and Yoshua Bengio.\nDescribing multimedia content using attention-based\nencoder-decoder networks. IEEE Transactions on Mul-\ntimedia , 17(11):1875–1886, 2015.\n[4] Kyunghyun Cho, Bart van Merrienboer, C ¸ aglar\nG¨ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning phrase repre-\nsentations using RNN encoder-decoder for statistical\nmachine translation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP , pages 1724–1734, 2014.\n[5] Taemin Cho. Improved techniques for automatic chord\nrecognition from music audio signals . PhD thesis, New\nYork University, 2014.\n[6] Franc ¸ois Chollet. Keras. https://github.com/\nfchollet/keras , 2015.\n[7] Junqi Deng and Yu-Kwong Kwok. A hybrid gaussian-\nhmm-deep-learning approach for automatic chord esti-\nmation with very large vocabulary. ISMIR, 2016.\n[8] Takuya Fujishima. Realtime chord recognition of mu-\nsical sound: a system using common lisp music. In\nICMC , pages 464–467, 1999.\n[9] Christopher Harte, Mark B Sandler, Samer A Abdal-\nlah, and Emilia G ´omez. Symbolic representation of\nmusical chords: A proposed syntax for text annota-\ntions. In ISMIR , pages 66–71, 2005.\n[10] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\n[11] Eric J Humphrey and Juan Pablo Bello. Four timely in-\nsights on automatic chord estimation. In ISMIR , pages\n673–679, 2015.\n[12] Eric J Humphrey, Justin Salamon, Oriol Nieto, Jon\nForsyth, Rachel M Bittner, and Juan Pablo Bello.\nJAMS: A JSON annotated music speciﬁcation for re-\nproducible MIR research. In ISMIR , pages 591–596,\n2014.\n[13] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducinginternal covariate shift. In Proceedings of the 32nd In-\nternational Conference on Machine Learning (ICML-\n15), pages 448–456, 2015.\n[14] Rafal J ´ozefowicz, Wojciech Zaremba, and Ilya\nSutskever. An empirical exploration of recurrent net-\nwork architectures. In Proceedings of the 32nd Interna-\ntional Conference on Machine Learning, ICML 2015,\nLille, France, 6-11 July 2015 , pages 2342–2350, 2015.\n[15] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[16] Filip Korzeniowski and Gerhard Widmer. Feature\nlearning for chord recognition: the deep chroma ex-\ntractor. In ISMIR , 2016.\n[17] Matthias Mauch and Simon Dixon. Approximate note\ntranscription for the improved identiﬁcation of difﬁcult\nchords. In ISMIR , 2010.\n[18] Brian McFee, Eric J Humphrey, and Juan Pablo Bello.\nA software framework for musical data augmentation.\nInISMIR , pages 248–254, 2015.\n[19] Brian McFee, Matt McVicar, Oriol Nieto, Stefan\nBalke, Carl Thome, Dawen Liang, Eric Battenberg,\nJosh Moore, Rachel Bittner, Ryuichi Yamamoto, Dan\nEllis, Fabian-Robert Stoter, Douglas Repetto, Simon\nWaloschek, CJ Carr, Seth Kranzler, Keunwoo Choi,\nPetr Viktorin, Joao Felipe Santos, Adrian Holovaty,\nWaldir Pimenta, and Hojin Lee. librosa 0.5.0, Febru-\nary 2017.\n[20] Yizhao Ni, Matt McVicar, Raul Santos-Rodriguez, and\nTijl De Bie. An end-to-end machine learning system\nfor harmonic analysis of music. IEEE Transactions on\nAudio, Speech, and Language Processing , 20(6):1771–\n1783, 2012.\n[21] Colin Raffel, Brian McFee, Eric J Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel PW\nEllis. mir eval: A transparent implementation of com-\nmon mir metrics. In ISMIR , 2014.\n[22] Siddharth Sigtia, Nicolas Boulanger-Lewandowski,\nand Simon Dixon. Audio chord recognition with a hy-\nbrid recurrent neural network. In ISMIR , pages 127–\n133, 2015.\n[23] Adrian Weller, Daniel Ellis, and Tony Jebara. Struc-\ntured prediction models for chord transcription of mu-\nsic audio. In Machine Learning and Applications,\n2009. ICMLA’09. International Conference on , pages\n590–595. IEEE, 2009.\n[24] Xinquan Zhou and Alexander Lerch. Chord detection\nusing deep learning. In ISMIR , 2015.194 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Monaural Score-Informed Source Separation for Classical Music Using Convolutional Neural Networks.",
        "author": [
            "Marius Miron",
            "Jordi Janer",
            "Emilia Gómez"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416498",
        "url": "https://doi.org/10.5281/zenodo.1416498",
        "ee": "https://zenodo.org/records/1416498/files/MironJG17.pdf",
        "abstract": "Score information has been shown to improve music source separation when included into non-negative matrix factorization (NMF) frameworks. Recently, deep learning approaches have outperformed NMF methods in terms of separation quality and processing time, and there is scope to extend them with score information. In this paper, we propose a score-informed separation system for classical music that is based on deep learning. We propose a method to derive training features from audio files and the corre- sponding coarsely aligned scores for a set of classical mu- sic pieces. Additionally, we introduce a convolutional neu- ral network architecture (CNN) with the goal of estimating time-frequency masks for source separation. Our system is trained with synthetic renditions derived from the original scores and can be used to separate real-life performances based on the same scores, provided a coarse audio-to-score alignment. The proposed system achieves better perfor- mance (SDR and SIR) and is less computationally inten- sive than a score-informed NMF system on a dataset com- prising Bach chorales.",
        "zenodo_id": 1416498,
        "dblp_key": "conf/ismir/MironJG17",
        "keywords": [
            "score information",
            "non-negative matrix factorization (NMF)",
            "deep learning approaches",
            "classical music",
            "convolutional neural network (CNN)",
            "time-frequency masks",
            "source separation",
            "real-life performances",
            "coarse audio-to-score alignment",
            "synthetic renditions"
        ],
        "content": "MONAURAL SCORE-INFORMED SOURCE SEPARATION FOR\nCLASSICAL MUSIC USING CONVOLUTIONAL NEURAL NETWORKS\nMarius Miron, Jordi Janer, Emilia G ´omez\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona\nfirstname.lastname@upf.edu\nABSTRACT\nScore information has been shown to improve music\nsource separation when included into non-negative matrix\nfactorization (NMF) frameworks. Recently, deep learning\napproaches have outperformed NMF methods in terms of\nseparation quality and processing time, and there is scope\nto extend them with score information. In this paper, we\npropose a score-informed separation system for classical\nmusic that is based on deep learning. We propose a method\nto derive training features from audio ﬁles and the corre-\nsponding coarsely aligned scores for a set of classical mu-\nsic pieces. Additionally, we introduce a convolutional neu-\nral network architecture (CNN) with the goal of estimating\ntime-frequency masks for source separation. Our system is\ntrained with synthetic renditions derived from the original\nscores and can be used to separate real-life performances\nbased on the same scores, provided a coarse audio-to-score\nalignment. The proposed system achieves better perfor-\nmance (SDR and SIR) and is less computationally inten-\nsive than a score-informed NMF system on a dataset com-\nprising Bach chorales.\n1. INTRODUCTION\nAs a special case of audio source separation, music source\nseparation has gained signiﬁcant attention during the past\nyears. Recovering the sources corresponding to the instru-\nments from an audio mixture allows for interesting appli-\ncations such as music upmixing [9] or virtual-reality con-\ncerts [16], and it is useful in music information retrieval\ntasks [11, 30].\nIn contrast to speech separation, music source separa-\ntion poses different challenges due to the variety of sources\nwhich are correlated in time and frequency [7]. Because of\nthe multitude of harmonic instruments, often related tim-\nbres, variations in dynamics, Western classical music is\na challenging case [21]. On the other hand, results can\nimprove if prior knowledge about the nature of sources\n[5, 29] and their timbre [2] informs the separation frame-\nwork. Considerable improvements are obtained in the case\nc\rMarius Miron, Jordi Janer, Emilia G ´omez. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Marius Miron, Jordi Janer, Emilia G ´omez. “Monau-\nral score-informed source separation for classical music using convolu-\ntional neural networks”, 18th International Society for Music Information\nRetrieval Conference, Suzhou, China, 2017.of parametric models, such as NMF, which are restricted\nusing coarsely aligned scores [4, 7, 10, 14].\nRecently, neural network approaches have outper-\nformed NMF in audio source separation challenges [18].\nDeep learning systems estimate soft masks for speciﬁc in-\nstrument classes [3, 13, 15] or computing the instrument\nspectra directly [27]. In contrast to NMF methods, a deep\nlearning framework is less computationally expensive [3]\nat the separation stage, as estimating the sources involves\na single feed forward pass through the network rather than\nan iterative procedure. Thus, it can be used in a low la-\ntency scenario. Furthermore, recurrent [15] and convolu-\ntional [3, 13] networks have the advantage of modeling a\nlarger time context.\nNovel deep learning source separation systems propose\nspecialized models which propose building an NMF logic\ninto an autoenconder [26] or cluster components over large\ntime spans [19]. Including score information into the deep\nlearning separation frameworks can yield further improve-\nments [8].\nIn this paper we introduce a monaural score-informed\nsource separation framework for Western classical music\nusing convolutional neural networks (CNN). We assume\nthat for a given classical music piece the instruments are\nknown and the score is available. Thus, for a set of given\nscores we generate renditions which are used to train a\nCNN. The trained model is used to separate real-life per-\nformances based on these scores [22].\nA global alignment of the score with the audio of a per-\nformance can be obtained by a score following system [4].\nThen, the resulting coarsely aligned score, with errors up\nto0:2seconds, is used to derive score-based soft masks for\neach of the sources. From these masks we generate score-\nﬁltered spectrograms as input features for the CNN.\nTraining neural networks for source separation requires\nisolated audio tracks which are difﬁcult to obtain. There-\nfore, we use the data generation method in [22]. Accord-\ningly, we synthesize renditions of original scores with vari-\nations in timbre, dynamics and local timing deviations.\nThe remainder of the paper is structured as follows. In\nSection 2 we state our contributions in relation with the\nprevious work. In Section 3 we introduce the proposed\nmethod including the feature computation, the architecture\nof the network and the training procedure. In Section 4 we\ndiscuss the evaluation of the proposed method. We present\nour conclusions in Section 5.552. RELATION TO PREVIOUS WORK\nScore-informed constraints [7,10] are imposed to the NMF\nframework through restricting the activations of the note\ntemplates. In a similar manner, we use the score to gener-\nate sparse training features which are used as input to the\nCNN. Furthermore, since score following errors inﬂuence\nthe quality of separation [4, 20], we compensate for local\nmisalignments in a similar manner to [7, 10], by allowing\na tolerance window around note onsets and offsets while\ncomputing our training features. For our experiments and\nthe dataset we used the size of this window is 0:2seconds.\nDeep learning systems can become more robust to real-\nlife cases by increasing the size and variability of the train-\ning dataset through data generation [22] or augmentation\n[24, 25]. In this sense, the difference in performance be-\ntween two similar deep learning methods can be largely ex-\nplained by the difference between training datasets rather\nthan new features or methods [1]. We are motivated by re-\ncent advance in deep learning which go beyond the black-\nbox model and try to integrate musically meaningful fea-\ntures [19,26]. Thus, we aim at improving source separation\nfor classical music with a context-driven method which in-\ncludes score information.\nThe CNN architecture in this paper is adapted from the\nconvolutional autoencoder proposed in [3, 22]. In com-\nparison to [3] our CNN architecture has different ﬁlter\nand layer sizes. Moreover, the original scores from which\ntraining data is generated are further used to derive score-\ninformed features which are given as input to the CNN in a\nrepresentation analogous to multi-channel images. To that\nextent, our approach contrasts with [8] which uses score\nrestrictions inside the deep learning framework. Further-\nmore, to our best knowledge, deep learning audio process-\ning methods do not use a multi-channel input as in im-\nage processing applications. Thus, we analyze whether\nthe convolutional autoencoder introduced in [3,22] learns a\nbetter representation from a multi-channel input than from\na single channel input, given that the feature maps are\nshared between all channels. In addition, we use bootstrap-\nping with replacement to train such an architecture when\nworking with big datasets.\n3. METHOD\nThe diagram of the separation framework with the two\nstages, training and separation, can be seen in Figure 1.\nFor the training stage, we start from the original scores\nfrom which we derive synthetic audio renditions with the\nmethod in [22]. The same scores are used to derive fea-\ntures for training the CNN in form of score-based soft\nmasks, explained in Section 3.1.1, and score-ﬁltered spec-\ntrograms, explained in Section 3.1.2. For the separation\nstage, our framework takes as input an audio mixture and\nthe corresponding coarsely aligned score. Similar to the\ntraining stage, we compute the score-based soft masks and\nthe score-ﬁltered spectrograms which are feed-forwarded\nthrough the CNN model to obtain the magnitude spectro-\ngrams of the separated sources.\nFigure 1 . The overview of the separation system compris-\ning the two stages: training and separation\n3.1 Feature computation\nThe goal of computing score-based soft masks is to de-\nrive additional sparse score-ﬁltered spectrograms which\nare used as an input to the CNN.\n3.1.1 Score-based soft masks\nA score gives the note onsets and offsets time and the MIDI\nnote numbers. Assuming that the source is harmonic and\nwe know the tuning frequency, fq, the MIDI note associ-\nated with A4, mA4, we can compute the fundamental fre-\nquencyf0=fq\u000121\n12\u0001(m\u0000mA4), wheremis the MIDI note\nnumber.\nScore information yields the time-frequency zones\nwhere the notes are played. Correspondingly, for a given\nnotenthat plays between the time frames tbandtewe can\ndeﬁne the time range as:\nUn(t) =u(t\u0000tb\u0000tw)\u0000u(t\u0000te\u0000tw) (1)\nwhere uis the unit step function, and twis tolerance\nwindow set around onset tband offsettewhich compen-\nsates for local misalignments in score-following, similarly\nto [4,7,10,14]. The tolerance window is applied at training\nand separation.\nFurthermore, if we consider the fundamental frequency\nf0of the notenwe deﬁne the frequency range as:\nVn(f) =HX\nh=1u(f\u0000hf0=fi)\u0000u(f\u0000hf0fi)(2)\nwhereh= 1 :Hare the harmonic partials, and fi=\n2fc=1200is the allowed frequency interval below and above\neach harmonic partials, with fcbeing the allowed interval\nin cents, and 1200 is the number of cents per octave.\nFor each source j= 1 :Jand all its notes n= 1 :Nj\nwe can compute score-based binary matrices Kj(t;f)as a\nsum of outer products:\nKj(t;f) =NjX\nn=1Un(t)\nVn(f) (3)56 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 2 . Feature computation for the ﬁrst 4 seconds and\nfrequencies between 0-6500Hz, for the piece Ach Gottund\nHerr of Bach10 dataset [4] comprising four instruments.\nAn example of Kjfor a classical music piece comprising\nfour harmonic sources is shown in the ﬁrst column of Fig-\nure 2.\nThe score-based soft masks for each source, j= 1 :J,\nare given by the equation:\nRj(f;t) =jKjjPJ\nj=1jKjj+\u000f(4)\nwhere\u000f= 1\u000010is a constant to handle division by zero.\nWe illustrate a set of Rjmatrices in the second column of\nFigure 2.\nIn this paper we consider solely combinations between\nharmonic sources, which are reﬂected in the initialization\nofVnusing a series of harmonic partials, as seen in Equa-\ntion 2. However, the proposed solution can be easily ex-\ntended to model non-harmonic sources by initializing the\nvectorVn(f) = 1 along all the frequency range, resulting\nin a less sparse score-ﬁltered spectrogram which is solely\ninformed by onsets and offsets times through Un(t).\n3.1.2 Score-ﬁltered spectrograms\nWe calculate the STFT magnitude spectrogram of the au-\ndio mixture as X(f;t). Then, we derive score-ﬁltered\nspectrograms for each of the sources j= 1 :J, by com-\nputing the element-wise product between the spectrogram\nof the mixture, X, and the score-based soft masks, Rj:\nXj(f;t) =X\u0001Rj (5)\nconv1\nf(1,30)\ns(1,4)conv2\nf(20,1)\ns(1,1)dense1\n256\ninverse\nconv2inverse\nconv1(J,T,F)\n(J,T,F)\n(30,T,F)1 (30,T,F)1 1(30,T,F)1(30,T,F)1 1\ndense2\n  30xTxF1 1\n=\nwith(J,T,F) (J,T,F)\n(T,F)Figure 3 . The CNN architecture used in the separation\nframework for J= 4sources\n3.2 Convolutional Neural Network architecture\nThe convolutional autoencoder architecture can be seen in\nFigure 3. It comprises a convolution stage with two convo-\nlution layers, two dense layers, and a deconvolution stage.\nThe sources are reconstructed using the ﬁlters learned at\nthe convolution stage. In addition, we have two determin-\nistic layers to compute the spectrograms of the sources.\nIn contrast to the CNN architectures in [3, 13, 22], our\nCNN takes as input Jscore-ﬁltered spectrograms for a\ntime context Tand a number of frequency bins F, rather\nthan a single spectrogram of the mixture. The Jscore-\nﬁltered spectrograms share the same feature maps, in a\nsimilar way to image processing deep learning methods\nthat use color RGB channels [1]. Our assumption is that\nthis additional information can better guide the separation\nbetween the sources. Furthermore, as shown in Figure 2,\nthe score-ﬁltered spectrograms are sparser versions of the\noriginal spectrogram, offering a better representation for\nsource separation [23].\nThe ﬁrst layer conv1 is a convolution layer with ﬁlter\nshape (1;30), hence the convolution only happens in fre-\nquency. For this layer we have a stride1of(1;4), which\nreduces dimensionality by keeping into account the spar-\nsity of the input.\nThis layer outputs feature maps of size (30;T;F 1),\nwhereF1= (F\u000030)=4 + 1 , where 30is the length of\nthe ﬁlter and 4is the stride. The second layer conv2 is\na convolution layer with ﬁlter shape (20;1), which learns\ntemporal patterns. The output of this layer has the size\n(30;T1;F1), whereT1= (T\u000020) + 1 with 20being the\nlength of the ﬁlter. This layer has a stride of (1;1), since\nwe are interested in maintaining a good temporal resolu-\ntion at the reconstruction. Note that the convolution layers\nhave a linear activation function.\nWe use a dense bottleneck layer as in [3] with 256units\nand a rectiﬁer linear unit activation function [1], denoted\nasdense1 . The limited number of units and the activa-\ntion function have been proven to better guide the param-\neter learning and prevent overﬁtting in the case of timbre-\ninformed source separation [3].\nTo match dimensions necessary for the deconvolution\n(30;T1;F1)for each of the Jsources, we introduce a layer\n1The stride controls how much a ﬁlter is shifted on the input.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 57dense2 comprisingJdense layers of shape 30\u0001T1\u0001F1. For\neach of these Jlayers we perform the inverse operations\nofconv2 andconv1 and we obtain a set of estimations Ej\nfor each of the separated sources j= 1 :J.\nFollowing [3, 15], we integrate the computation of the\nsoft masks into the architecture of the network as an ad-\nditional deterministic layer. Thus, the soft masks Mj, for\neach source j= 1 :J, are computed from the output of\nthe previous layer, Ej, as:\nMj=jEjjPJ\nj=1jEjj+\u000f(6)\nwhere\u000f= 1\u000010is a constant to handle division by zero.\nThe magnitude spectrogram corresponding to the sources,\n^Xj, are given by the element-wise multiplication between\ninput spectrogram and the soft-masks ^Xj=Mj\u0001X. The\nsoft masks Mjare not to be confounded with the score-\nbased soft-masks Rjintroduced in Section 3.1.1 and used\nto derive input features for the CNN.\n3.3 Training procedure\nThe network is trained according to the mean-squared\nerror between the magnitude spectrograms of the tar-\nget sources, ^Xj, and the magnitude spectrograms of\nthe sources yielded by the network, Xj, as:Loss =PJ\nj=1k^Xj\u0000Xjk2.\nThe parameters of the CNN are updated using mini-\nbatch Stochastic Gradient Descent with the AdaDelta al-\ngorithm [31].\nWith the method in [22] we can generate a high num-\nber of renditions, covering a high number of possibilities,\nwhich makes the framework more robust to real-life data.\nHowever, training on big datasets is an expensive proce-\ndure and we experimented with a faster training method\nsummarized in the Algorithm 1. In this case, we sam-\nple a limited number data points before each epoch rather\nthan having a ﬁxed dataset at the beginning of training. In\nstatistics, this procedure is known as bootstrapping with re-\nplacement [17]. Note that, for this training procedure, the\nconcept of epoch (a single pass through the entire training\nset) does not hold anymore.\nAlgorithm 1 Bootstrapping with replacement\n1repeat\n2 randomly sample a number of data points from the dataset\n3 foreach training batch do\n4 compute weights and bias gradients for the current\nbatch\n5 accumulate the gradients\n6 end for\n7 adjust weights and bias using accumulated gradients\n8until total number of stages is reached\n3.4 Separated source estimation\nWe assume that the individual sources yj(t);j= 1 :J,\nthat compose the audio mixture x(t)are linearly mixed,so thatx(t) =JP\nj=1yj(t). Therefore, from the estimated\nmagnitude spectrograms Xjand using the original phase\nof the audio mixture we can obtain the signals associated to\nthe sources, yj(t), with an inverse overlap-add STFT [10].\nThe neural network yields estimations of shape (T;F )\nfor each of the Jsources. Considering an audio mixture of\nvariable time shape, the estimation is done for overlapping\nsegments of shape (T;F ), with the algorithm described in\n[22].\n4. EV ALUATION\n4.1 Datasets\nFor evaluation purposes we use ten Bach chorales from the\nBach10 dataset [4], played by bassoon, clarinet, tenor sax-\nophone, and violin. The mean duration of a piece is \u001930\nseconds. In addition, each piece is accompanied by the\nscore aligned with the audio, the original score, and an au-\ntomatic alignment obtained with the algorithm in [4]. This\ndataset has been widely used in tasks as source separation,\nalignment, and transcription.\n4.2 Generating training data\nWe generate training data with the method in [22] which\nuses sample-based synthesis with samples from the RWC\ninstrument sound database [12]. The method synthesizes\noriginal scores at different tempos, dynamics, considering\nlocal timing deviations, and using different timbres to gen-\nerate a wide variety of renditions of given pieces.\nIn this case, we have three different timbres and three\nlevel of dynamics. In addition, to account for local\ntiming variations, we circular-shift the audio with s=\nf0;0:1;0:2gseconds. An analogous transformation needs\nto be applied to the associated score by adding sseconds\nto the note onsets and offsets.\nConsidering the variations of the factors above ( 3\u00013\u0001\n3 = 27 ) for the four instruments, we can generate a total\nnumber of 274= 531441 renditions for a single piece.\nBecause it is not feasible to generate such a high number of\naudio ﬁles, we randomly choose 400renditions to build our\ntraining dataset. Samples are uniformly distributed across\nthe dataset. Since we are training a CNN model for all the\n10pieces in Bach10 dataset, we have a total number of\n4000 renditions.\n4.3 Evaluation setup\nWe used the evaluation framework and the metrics de-\nscribed in [28] and [6] : Source to Distortion Ratio (SDR),\nSource to Interference Ratio (SIR), and Source to Artifacts\nRatio (SAR).\nThe STFT is computed using a Blackman-Harris win-\ndow of length 4096 samples, which at a sampling rate of\n44:1KHz corresponds to 93milliseconds (ms), and a hop\nsize of 512samples ( 11ms).\nWhen computing the soft-masks from the score, as de-\nscribed in Section 3.1.1, we consider the tuning frequency,58 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017fq= 440 Hz, the MIDI note associated with A4, mA4=\n69, and we allow fc= 40 cents above and below each\nharmonic partials to account for vibrato. Additionally, be-\ncause we want to train our score-informed system to ac-\ncount for errors in score following, we set the tolerance\nwindow to be tw= 0:2seconds around onsets and offsets.\nThe time context modeled by the CNN is T= 30\nframes. Furthermore, a more robust system is achieved by\ntaking consecutive T-sized frames with an overlap of 25\nframes with the algorithm described in [22].\nThe number of epochs is variable for each training ex-\nperiment. The size of a mini-batch is set to 32.\nThis paper follows the principles of research repro-\nducibility2. The code used in this paper is made available\nonline3. It is built on top of Lasagne, a framework for neu-\nral networks using Theano4. We ran the experiments on a\nUbuntu 16.04 PC with GeForce GTX TITAN X GPU, In-\ntel Core i7-5820K 3.3GHz 6-Core Processor, X99 gaming\n5 x99 ATX DDR44 motherboard.\n4.4 Experiments\nIn a ﬁrst experiment, we compare the proposed framework\nwith an NMF counterpart on the Bach10 dataset. We train\nour CNN framework on the synthetic dataset we described\nin Section 4.2 ( 10\u0002400renditions) and the correspond-\ning scores. Because we want the model to learn to deal\nwith errors in alignment we set a tolerance window around\nnotes’ onsets and offsets. Then, we test the resulting model\non real-life performances in Bach10 dataset and the scores\nyielded by the score-following system in [4].\nBecause we want to isolate the inﬂuence of the score-\nfollowing system, we test our system on the score perfectly\naligned (PA) with the audio. For this case, denoted as CNN\nPA, the tolerance window is not needed, neither for train-\ning nor testing. Furthermore, to assess the inﬂuence of the\nproposed features, we train the CNN architecture without\nany score information, having as input the magnitude spec-\ntrogram of the mixture, similarly to the system in [22]. We\ndenote this experiment as CNN T.\nWe compare our score-informed system to a state of the\nart NMF counterpart [20]. The note templates are trained\non the RWC dataset and are kept ﬁxed during the factor-\nization. Score-information is introduced through the acti-\nvation matrix by setting to zero the activations correspond-\ning to notes which are not played. The activations which\nare set to zero will remain this way during factorization,\nallowing the energy to be distributed between the active\ntemplates.\nFor the NMF system we use as input the score aligned\nwith [4] with a tolerance window of 0:2seconds, and the\nperfectly aligned score, as two separate cases, denoted as\nNMF and NMF PA. Furthermore, for the NMF we kept\nthe default parameters presented in the paper [20]: 50it-\nerations for the factorization, beta-divergence distortion\n2http://soundsoftware.ac.uk/resources/\n3https://github.com/MTG/DeepConvSep\n4http://lasagne.readthedocs.io/en/latest/ Lasagne\nandhttp://deeplearning.net/software/theano/ Theano\f= 1:3, STFT window size 93ms, and hop size 11ms.\nFor this ﬁrst experiment we do not test the bootstrapping\nwith replacement procedure. To that extent, we train the\nCNN with all the 4000 renditions for a maximum number\nof20epochs and we stop training if the loss between two\nepochs drops below 0:2.\nIn a second experiment, we test the effectiveness of the\ntraining procedure based on bootstrapping with replace-\nment, described in Algorithm 1 and compare it with the\nstandard training procedure which maintains the same data\npoints during training. Furthermore, since we want to de-\ntermine the optimal value for the number of renditions used\nat each epoch or stage, we train the CNN successively with\nthe two procedures using different numbers of renditions.\nFor this experiment we train for a number of 50epochs or\nstages.\n4.5 Results\nThe SDR, SIR, and SAR for our system (CNN and CNN\nPA), the timbre informed version CNN T, and the NMF\nframework are presented in the Figure 4. Error bars are\ndrawn for a conﬁdence interval of 95%.\nWe observe that the proposed score-informed frame-\nwork performs better than NMF when working with\ncoarsely aligned scores: 6dB vs 5dB in SDR. Hence, with\nour framework we are able to compensate for local mis-\nalignment errors around 0:2seconds. This results in less\ninterference, since the CNN method has 2dBmore in SIR\nthan the NMF, and can be due to the fact that the CNN\nmodels temporal patterns in the conv2 layer and to the non-\nlinearities in the bottleneck dense1 layer.\nFigure 4 . Results in terms of SDR, SIR, SAR for the pro-\nposed CNN framework and the NMF framework [20]\nHaving score-ﬁltered spectrograms as input (CNN) im-\nproved 2dBin SDR in comparison to giving the magnitude\nspectrograms as input (CNN T), which proves the effec-\ntiveness of the features derived from score.\nWhen the score is perfectly aligned with the audio, there\nis no signiﬁcant difference in SDR between the CNN PA\nand NMF PA. However, the proposed method has 1dB\nhigher SIR and similar SAR values to the NMF PA. Note\nthat CNN PA is trained on the original scores and it is notProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 59targeted for special case. To that extent, as the CNN and\nCNN PA achieve similar results, we believe that having a\nperfect alignment does not improve results for this particu-\nlar type of CNN architecture. This is in line with the results\nobtained in [22].\nFigure 5 . Results for each instrument in terms of SDR for\nthe considered approaches: CNN and NMF [20]\nWe present the results in terms of SDR for each instru-\nment in Figure 5. The CNN framework performs signif-\nicantly better than the NMF for all the instruments, with\nthe exception of bassoon. While experimenting with dif-\nferent STFT window sizes, we observed that the quality\nof the separation for bassoon improved considerably with\nthe increase in the window size, while remaining the same\nfor the other instruments. However, a larger window size\nmeans a higher feature dimensionality, hence more weights\nto be trained and a larger model.\nWe observe that the proposed framework effectively\ncompensates for errors in alignment across all instruments,\nespecially for clarinet.\nThe audio examples for the CNN framework and the\ncomputed metrics for CNN,CNN PA, CNN T, NMF, and\nNMF PA as .mat ﬁles can be accessed online5.\nIn the second experiment we are interested in testing\nthe bootstrapping with replacement training procedure and\nthe standard procedure. The results for various number of\nrenditions can be seen in Figure 6.\nbootstrappingstandard\nFigure 6 . Results in terms of SDR,SIR,SAR when training\nthe proposed CNN with stardard training method vs boot-\nstrapping with replacement with various number of train-\ning samples\nWe observe that bootstrapping with replacement always\n5http://doi.org/10.5281/zenodo.821128improves over the standard training procedure, particu-\nlarly for a small number of training renditions. How-\never, a lower than 50number of renditions, decreases the\nperformance for both of the training methods. In some\ncases ( 50,60,100), using the proposed training procedure\nwith fewer samples is slightly better than training with the\nwhole dataset, as it prevents overﬁtting, in a similar way to\nearly stopping [1]. The optimum number of renditions for\nour experimental scenario is 50samples.\n5. CONCLUSION\nWe proposed a score-informed source separation frame-\nwork targeted at Western classical music. Our framework\nis based on the assumption that classical music pieces are\naccompanied by scores and this information can be lever-\naged. Thus, we proposed a framework which is trained\nwith generated renditions synthesized from the original\nscores. Provided an accurate automatic audio-to-score\nalignment can be obtained by a score-following system,\nour framework separates with low latency any real-life per-\nformances based on those scores, accompanied by a coarse\nalignment.\nWe presented a novel method to derive training fea-\ntures in the form of score-ﬁltered spectrograms, which can\neasily be integrated with CNN architectures. In particu-\nlar, these sorts of homogeneous features are well suited to\nlearning convolutional ﬁlters which are shared between the\ninput channels of the CNN.\nThe proposed system has better SDR and SIR than a\nstate of the art score-informed NMF framework, particu-\nlarly when working with coarsely aligned score, as it is\nthe case of the output of score-following systems. Further-\nmore, we tested a faster training procedure, bootstrapping\nwith replacement, which preserves the performance and in\nsome cases prevents overﬁtting. As future work, we plan\non extending this framework to multi-microphone orches-\ntral music which is a more complex scenario due to in-\ncreased number of instruments. Moreover, reiterating the\nmethod, by inputting the output of the network to another\nsimilar network, could improve results [27].\n6. ACKNOWLEDGMENTS\nThe TITANX used for this research was donated by the\nNVIDIA Corporation. This work is partially supported by\nthe Spanish Ministry of Economy and Competitiveness un-\nder CASAS project (TIN2015-70816-R) and by the Span-\nish Ministry of Economy and Competitiveness under the\nMaria de Maeztu Units of Excellence Programme (MDM-\n2015-0502). We thank Matthew E.P. Davies for his feed-\nback.\n7. REFERENCES\n[1] Y Bengio. Learning Deep Architectures for AI. Foun-\ndations and Trends in Machine Learning , 2(1):1–127,\n2009.60 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[2] JJ Carabias-Orti, T Virtanen, P Vera-Candeas, N Ruiz-\nReyes, and FJ Canadas-Quesada. Musical Instru-\nment Sound Multi-Excitation Model for Non-Negative\nSpectrogram Factorization. IEEE Journal of Selected\nTopics in Signal Processing , 5(6):1144–1158, October\n2011.\n[3] P Chandna, M Miron, J Janer, and E G ´omez. Monoau-\nral audio source separation using deep convolutional\nneural networks. International Conference on Latent\nVariable Analysis and Signal Separation , 2017.\n[4] Z. Duan and B. Pardo. Soundprism: An online system\nfor score-informed source separation of music audio.\nIEEE Journal of Selected Topics in Signal Processing ,\n5(6):1205–1215, 2011.\n[5] J.-L. Durrieu, A. Ozerov, C. F ´evotte, G. Richard, and\nB. David. Main instrument separation from stereo-\nphonic audio signals using a source/ﬁlter model. In\nSignal Processing Conference, 2009 17th European ,\npages 15–19. IEEE, 2009.\n[6] V . Emiya, E. Vincent, N. Harlander, and V . Hohmann.\nSubjective and Objective Quality Assessment of Au-\ndio Source Separation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 19(7):2046–2057,\nSeptember 2011.\n[7] S. Ewert and M. M ¨uller. Using score-informed con-\nstraints for nmf-based source separation. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2012 IEEE\nInternational Conference on , pages 129–132. IEEE,\n2012.\n[8] S Ewert and MB Sandler. Structured dropout for weak\nlabel and multi-instance learning and its application to\nscore-informed source separation. In Acoustics, Speech\nand Signal Processing (ICASSP), 2017 IEEE Interna-\ntional Conference on , pages 2277–2281. IEEE, 2017.\n[9] D Fitzgerald. Upmixing from mono-a source separa-\ntion approach. In Digital Signal Processing (DSP),\n2011 17th International Conference on , pages 1–7.\nIEEE, 2011.\n[10] J. Fritsch and M.D. Plumbley. Score informed audio\nsource separation using constrained nonnegative ma-\ntrix factorization and score synthesis. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2013 IEEE\nInternational Conference on , pages 888–891. IEEE,\n2013.\n[11] E. G ´omez, F. Ca ˜nadas, J. Salamon, J. Bonada,\nP. Vera, and P. Caba ˜nas. Predominant Fundamental\nFrequency Estimation vs Singing V oice Separation\nfor the Automatic Transcription of Accompanied Fla-\nmenco Singing. 13th International Society for Music\nInformation Retrieval Conference , 2012.\n[12] M. Goto. Development of the RWC music database.\nInProceedings of the 18th International Congress on\nAcoustics (ICA 2004) , pages 553–556, 2004.[13] E.M. Grais, M.U. Sen, and H. Erdogan. Deep neu-\nral networks for single channel source separation. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2014 IEEE International Conference on , pages 3734–\n3738. IEEE, may 2014.\n[14] R. Hennequin, B. David, and R. Badeau. Score in-\nformed audio source separation using a paramet-\nric model of non-negative spectrogram. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2011 IEEE\nInternational Conference on , number 1, pages 45–48.\nIEEE, 2011.\n[15] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and\nP. Smaragdis. Deep Learning for Monaural Speech\nSeparation. In Acoustics, Speech and Signal Process-\ning (ICASSP), 2014 IEEE International Conference on ,\npages 1562–1566, 2014.\n[16] J. Janer, E. G ´omez, A. Martorell, M. Miron, and\nB. de Wit. Immersive orchestras: audio processing for\norchestral music VR content. In Games and Virtual\nWorlds for Serious Applications (VS-Games), 2016 8th\nInternational Conference on , pages 1–2. IEEE, 2016.\n[17] R Kohavi et al. A study of cross-validation and boot-\nstrap for accuracy estimation and model selection. In\nIjcai, volume 14, pages 1137–1145. Stanford, CA,\n1995.\n[18] A Liutkus, F-R St ¨oter, Z Raﬁi, D Kitamura, B Rivet,\nN Ito, N Ono, and J Fontecave. The 2016 signal sep-\naration evaluation campaign. In International Confer-\nence on Latent Variable Analysis and Signal Separa-\ntion, pages 323–332. Springer, 2017.\n[19] Y . Luo, Z. Chen, J. R Hershey, J. Le Roux, and N. Mes-\ngarani. Deep clustering and conventional networks for\nmusic separation: Stronger together. arXiv preprint\narXiv:1611.06265 , 2016.\n[20] M Miron, J.J. Carabias, and J Janer. Improving score-\ninformed source separation for classical music through\nnote reﬁnement. 16th International Society for Music\nInformation Retrieval Conference , 2015.\n[21] M Miron, JJ Carabias-Orti, JJ Bosch, E G ´omez, and\nJ Janer. Score-informed source separation for multi-\nchannel orchestral recordings. Journal of Electrical\nand Computer Engineering , 2016, 2016.\n[22] M. Miron, J. Janer, and E. G ´omez. Generating data to\ntrain convolutional neural networks for classical music\nsource separation. In Proceedings of the 14th Sound\nand Music Computing Conference , pages 227–233,\n2017.\n[23] M.D. Plumbley, T. Blumensath, L. Daudet, R. Gribon-\nval, and M.E. Davies. Sparse representations in audio\nand music: from coding to source separation. Proceed-\nings of the IEEE , 98(6):995–1005, 2010.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 61[24] J. Salamon and J.P. Bello. Deep convolutional neu-\nral networks and data augmentation for environmental\nsound classiﬁcation. IEEE Signal Processing Letters ,\n24(3):279–283, 2017.\n[25] J. Schl ¨uter and T. Grill. Exploring data augmentation\nfor improved singing voice detection with neural net-\nworks. In 16th International Society for Music Infor-\nmation Retrieval Conference , pages 121–126, 2015.\n[26] P. Smaragdis and S. Venkataramani. A neural net-\nwork alternative to non-negative audio models. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2017 IEEE International Conference on , pages 86–90.\nIEEE, 2017.\n[27] S. Uhlich, F. Giron, and Y . Mitsufuji. Deep neural\nnetwork based instrument extraction from music. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2015 IEEE International Conference on , pages 2135–\n2139. IEEE, 2015.\n[28] E. Vincent, R. Gribonval, and C. Fevotte. Performance\nmeasurement in blind audio source separation. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 14(4):1462–1469, jul 2006.\n[29] T. Virtanen. Monaural sound source separation by non-\nnegative matrix factorization with temporal continuity\nand sparseness criteria. IEEE Transactions on Audio,\nSpeech, and Language Processing , 15(3):1066–1074,\n2007.\n[30] JR Zapata and E Gomez. Using voice suppression al-\ngorithms to improve beat tracking in the presence of\nhighly predominant vocals. In 2013 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning, pages 51–55. IEEE, may 2013.\n[31] M. D. Zeiler. Adadelta: an adaptive learning rate\nmethod. arXiv preprint arXiv:1212.5701 , 2012.62 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Local Interpretable Model-Agnostic Explanations for Music Content Analysis.",
        "author": [
            "Saumitra Mishra",
            "Bob L. Sturm",
            "Simon Dixon"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417387",
        "url": "https://doi.org/10.5281/zenodo.1417387",
        "ee": "https://zenodo.org/records/1417387/files/MishraSD17.pdf",
        "abstract": "The interpretability of a machine learning model is es- sential for gaining insight into model behaviour. While some machine learning models (e.g., decision trees) are transparent, the majority of models used today are still black-boxes. Recent work in machine learning aims to analyse these models by explaining the basis of their de- cisions. In this work, we extend one such technique, called local interpretable model-agnostic explanations, to music content analysis. We propose three versions of explana- tions: one version is based on temporal segmentation, and the other two are based on frequency and time-frequency segmentation. These explanations provide meaningful ways to understand the factors that influence the classifi- cation of specific input data. We apply our proposed meth- ods to three singing voice detection systems: the first two are designed using decision tree and random forest classi- fiers, respectively; the third system is based on convolu- tional neural network. The explanations we generate pro- vide insights into the model behaviour. We use these in- sights to demonstrate that despite achieving 71.4% classifi- cation accuracy, the decision tree model fails to generalise. We also demonstrate that the model-agnostic explanations for the neural network model agree in many cases with the model-dependent saliency maps. The experimental code and results are available online. 1",
        "zenodo_id": 1417387,
        "dblp_key": "conf/ismir/MishraSD17",
        "keywords": [
            "interpretability",
            "machine learning",
            "black-box",
            "local interpretable model-agnostic explanations",
            "music content analysis",
            "temporal segmentation",
            "frequency segmentation",
            "time-frequency segmentation",
            "singing voice detection",
            "classification accuracy"
        ],
        "content": "LOCAL INTERPRETABLE MODEL-AGNOSTIC EXPLANATIONS FOR\nMUSIC CONTENT ANALYSIS\nSaumitra Mishra, Bob L. Sturm, Simon Dixon\nCentre for Digital Music, Queen Mary University of London, United Kingdom\nfsaumitra.mishra, b.sturm, s.e.dixon g@qmul.ac.uk\nABSTRACT\nThe interpretability of a machine learning model is es-\nsential for gaining insight into model behaviour. While\nsome machine learning models (e.g., decision trees) are\ntransparent, the majority of models used today are still\nblack-boxes. Recent work in machine learning aims to\nanalyse these models by explaining the basis of their de-\ncisions. In this work, we extend one such technique, called\nlocal interpretable model-agnostic explanations, to music\ncontent analysis. We propose three versions of explana-\ntions: one version is based on temporal segmentation, and\nthe other two are based on frequency and time-frequency\nsegmentation. These explanations provide meaningful\nways to understand the factors that inﬂuence the classiﬁ-\ncation of speciﬁc input data. We apply our proposed meth-\nods to three singing voice detection systems: the ﬁrst two\nare designed using decision tree and random forest classi-\nﬁers, respectively; the third system is based on convolu-\ntional neural network. The explanations we generate pro-\nvide insights into the model behaviour. We use these in-\nsights to demonstrate that despite achieving 71.4% classiﬁ-\ncation accuracy, the decision tree model fails to generalise.\nWe also demonstrate that the model-agnostic explanations\nfor the neural network model agree in many cases with the\nmodel-dependent saliency maps. The experimental code\nand results are available online.1\n1. INTRODUCTION\nMusic content analysis (MCA) research aims to build sys-\ntems with the sensitivity and intelligence required to work\nwith information in acoustic environments. Recent ad-\nvances in this domain have been made by leveraging large\namounts of data with statistical machine learning, e.g.,\n[5, 6]. The complexity of the resulting systems, however,\nmakes it extremely difﬁcult to understand their behaviours,\nor to predict their success in the real world.\nRecent work seeks to ascribe certain functions or sen-\nsitivities to architectural elements of a trained system. For\ninstance, analyses of deep computer vision systems ﬁnd\nthe ﬁrst layer to be sensitive to edges, points and colour\n1https://code.soundsoftware.ac.uk/projects/SoundLIME\nc\rSaumitra Mishra, Bob L. Sturm, Simon Dixon. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0). Attribution:\nSaumitra Mishra, Bob L. Sturm, Simon Dixon. “Local Interpretable Model-agnostic\nExplanations for Music Content Analysis”, 18th International Society for Music\nInformation Retrieval Conference, Suzhou, China, 2017.\nMFCC_0 <= X\n Class = A\nMFCC_3 <= Y\n Class = BTrue\nMFCC_5 <= Z\n Class = AFalse\nClass = A Class = B Class = A Class = BFigure 1 : A binary decision tree for classifying audio us-\ning the values of three MFCC feature dimensions.\ngradients, and deeper layers appear sensitive to higher-\nlevel concepts like faces, trees and cars [23,25,26]. Similar\nwork for deep MCA systems has found that the ﬁrst layer\nis sensitive to frequency bands, and deeper layers appear\nsensitive to timbres and temporal patterns, e.g., [3, 6]. In a\ndifferent direction, other research focuses on approaches to\nexplain individual predictions. One approach to explain in-\ndividual predictions substitutes complex black-box models\nwith inherently interpretable models whose predictions can\nbe summarised by simple if-else rules [11,24]. Other meth-\nods use sensitivity analysis [7] or Taylor series expansion\n[15] to analyse the prediction function locally. Sensitiv-\nity analysis aims to capture the local behaviour of the pre-\ndiction function when the input dimensions are perturbed.\nVariants of this approach include saliency maps [20], ex-\nplanation vectors [1], “horse” detection [22] and local in-\nterpretable model-agnostic explanations (LIME) [17]. In\nthis paper, we focus on extending LIME for MCA.\nLIME is an algorithm that provides instance-based ex-\nplanations to predictions of any classiﬁer. These expla-\nnations are locally faithful to the instance, independent of\nthe classiﬁer model type, and are learned over interpretable\nrepresentations of the instance. For example, for an e-mail\nclassiﬁcation system, LIME generates a list of words of an\ne-mail as an explanation for its classiﬁcation to some cate-\ngory. To produce the explanation, LIME approximates the\nclassiﬁer locally with an interpretable model (e.g., sparse\nlinear models, decision trees).\nWe introduce three different versions of explanations\nto apply LIME to MCA. We call this extended frame-\nwork as Sound LIME (SLIME). Each version works in\nthe time, frequency and time-frequency domains, respec-\ntively. SLIME pinpoints the time or time-frequency re-\ngion that contributes most to a decision. This transforms a\nnon-intuitive feature-based classiﬁer decision into a more\nintuitive temporal and spectral description. We demon-537Explanation•𝑅\"•𝑅#•𝑅$(ℰ)Feature extractorClassifier.𝑥)𝐱𝒊∈ℝ..(𝐶)LIME{𝑥)\t, ℰ, C}S𝑦)4∈[0,1].Figure 2 : Schematic representation of LIME explaining\nwhy an MCA system Sapplies label jto instancexiwith\nprobabilityyij.\nstrate SLIME for three trained singing voice detection sys-\ntems, and show how the generated explanations are useful\nin gaining insight into model behaviour, and in identifying\nan untrustworthy model that fails to generalise.\n2. MOTIVATION\nConsider a simple MCA system, the classiﬁcation compo-\nnent of which is the binary decision tree (BDT) shown in\nFig. 1. The input to this system is a T-sec excerpt of audio,\nfrom which the system extracts DMel-frequency cepstral\ncoefﬁcients (MFCC) [4]. This D-dimensional feature vec-\ntor is labeled by the system as either “class A” or “class B”\nbased on the values in speciﬁc dimensions. The particular\ndimensions, and the thresholds of the decisions, are found\nthrough training.\nA binary decision tree is a transparent classiﬁer because\none can trace the reason for a particular outcome - in this\ncase in terms of the MFCC coefﬁcients and thresholds. As\nshown in Fig. 1, if the value of the zeroth MFCC is less\nthan X and that of the third MFCC is less than Y , then this\nsystem classiﬁes the instance as “class A”. What does this\nmean in terms of the qualities of the input sound, however?\nWhy X? Is this a real-world general principle? Or does it\narise from a peculiarity of the training dataset?\nMFCCs were introduced for speech recognition [4], but\nhave been argued as suitable for machine music listen-\ning [9,12]. Extracting MFCC features from audio involves\nwindowing (typically on the order of 10-100 ms), a Mel-\nscale based smoothing of the log magnitude spectrum, and\ndiscrete cosine transform (DCT)-based compression. Al-\nthough MFCC features are pseudo-invertible [2], they are\ndifﬁcult to interpret in terms of the qualities of the underly-\ning sound. This comes in part from frequency bin grouping\nand the log magnitude operations, which destroy the bijec-\ntive mapping between the audio and its spectrum.\nOne might still roughly approximate the meaning of\nparticular MFCCs: low MFCC dimensions relate to broad\nspectral structures (e.g., formants); high MFCC dimen-\nsions relate to ﬁne spectral structures (e.g., pitch and har-\nmonics); and the zeroth MFCC relates to the energy of\na signal. But, as shown in Fig. 1, values along several\nMFCC dimensions and their thresholds jointly contribute\nto a prediction. This combination makes interpretation\neven harder. It is hard to understand what audible qualities\nare captured by the combination of the zeroth MFCC with\neither the third or the ﬁfth MFCC. Thus, though the deci-\nsion tree has clear decision rules, they are not easy to relate\nto audible qualities of inputs. With other machine learningsystems, e.g., deep neural networks or support vector ma-\nchines, this task becomes harder still. This motivates the\nuse of “interpretable representations” for explaining sys-\ntem behaviours for speciﬁc inputs.\n3. INTERPRETABLE EXPLANATIONS FOR\nMUSIC CONTENT ANALYSIS\nWe ﬁrst present the local interpretable model-agnostic ex-\nplanations (LIME) proposed in [17]. We then extend it to\nworking with MCA systems.\n3.1 Summary of LIME [17]\nSection 2 shows how the rules guiding a classiﬁer’s out-\nput can be difﬁcult to interpret in terms of content, even\nfor transparent classiﬁers. This interpretability becomes\nincreasingly difﬁcult when the model becomes complex\n(e.g., support vector machine) or the feature extraction is\nreplaced by feature learning (e.g., convolutional neural net-\nwork). LIME uses an interpretable representation of data\nto maintain interpretability in the generated explanations.\nSuch explanations are easier because they show a more di-\nrect mapping between the input and its prediction.\nLIME is an algorithm that generates interpretable, lo-\ncally faithful and model-agnostic explanations to predic-\ntions of any classiﬁer. Fig. 2 depicts a high-level overview\nof what LIME aims to perform. LIME helps illuminate\nreasons for a system Sapplying label jto instancexiwith\nprobabilityyij. For example, for the input xi, LIME lists\nthree reasons: R1;R2andR3, to explain the prediction.\nR1andR2are positively correlated with the decision and\nR3is negatively correlated.\nLocally faithful explanations refer to capturing the clas-\nsiﬁer behaviour in the neighbourhood of the instance to be\nexplained. To learn a local explanation, LIME approxi-\nmates the classiﬁer’s decision boundary around a speciﬁc\ninstance using an interpretable model. LIME is model-\nagnostic , i.e., it considers the model as a black-box and\nmakes no assumptions about the model behaviour. This\nmakes LIME applicable to any classiﬁer.\nFormally, let C:Rn!Rbe a classiﬁer, mapping\na feature vector to a class label. For a feature vector\nxi=\"(xi), denoteyij=C(xi)as the probability that\nxitakes the class label j. Deﬁne a sequence Xi, which is\ncomposed of elements that are in some sense meaningful\nwith respect to the classiﬁcation of the instance xi. For\nexample, for a text classiﬁcation system, Xicould be the\nsequence of unique words in e-mail. LIME deﬁnes an in-\nterpretable spaceT=f0;1gjXij, where itskth dimension\ncorresponds to the kth element ofXi. Then x0\ni2T is the\ninterpretable representation ofxi. Thus, LIME transforms\nthe input instance xito a binary vector x0\niwhose elements\ncorrespond to presence and absence of elements of Xi.\nLIME deﬁnes an interpretable explanation as a model\ng2G, whereGdenotes a class of interpretable models\n(e.g., linear models, decision trees). LIME learns a model\ngover the interpretable space by the optimisation:\nmin\ng2GL(C;g;\u001a xi) + \u0001(g) (1)538 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017𝐱\"#∈{0,1}|+,|Input instance𝑥\"Generate sequenceGenerate interpretable representationGenerate 𝑁/\tsamplesProject each sample in feature spaceGenerate labels and distancesLearn linear model𝜒\"𝐳\"3#∈0,1|+,|𝐳\"𝒌∈ℝ6𝐶(𝐳\"3)𝜌(𝐱\",𝐳3)𝐰\"ExplanationFigure 3 : Functional block diagram of SLIME depicting\nthe steps in generation of the explanation wifor the pre-\ndiction of the instance xi.\n0.00.51.0Amplitude\na) TEMPORAL SEGMENTATION\n1.0\n0.5\nT1 T2 T3 T4\n0 0.05 0.1 0.15\nTime\n05128192Hz\n0.20B1B2 B4\nB3 B5B6 B8\nB7\nb) TIME-FREQUENCY SEGMENTATION\nFigure 4 : Segmentation based sequence generation for\nSLIME. (a) Temporal segmentation of instance xiinto four\nsuper samples ( Ti), each of duration 50 ms. (b) Time-\nfrequency segmentation of instance xiinto 8 blocks ( Bi).\nwhereL(C;g;\u001a xi)is a locally-weighted loss function that\nfor an instance ximeasures how well the model gapproxi-\nmates the classiﬁer Cin the neighbourhood deﬁned by \u001axi,\nand\u0001(g)is a measure of model complexity (e.g. sparsity\nin linear models). Thus, LIME minimises this function to\nexplain why Cmapsxito class label j.\n3.2 Extending LIME to MCA\nFig. 3 depicts the functional block diagram of SLIME.\nThis consists of two components: the ﬁrst one is our con-\ntribution (dotted box in Fig. 3), which deﬁnes interpretable\nsequences for an input audio. The second one is the LIME\nalgorithm that uses the deﬁned representations to generate\nexplanations.\nThe ﬁrst step in SLIME is to deﬁne a sequence denoted\nXifrom an input instance xi. We deﬁne three kinds of\nsequences: temporal Xt\ni, spectralXf\niand time-frequency\nXtf\ni. We call each element of Xt\niasuper sample , which\nwe generate by temporal partitioning of xi. For example,\nthe instance shown in Fig. 4(a) is uniformly segmented\ninto four super samples each notated Ti. Hence,Xt\ni=\n(T1;T2;T3;T4). Similarly, each element of Xf\ni, notated\nAiis a spectral magnitude in a corresponding frequency\nbin, obtained by the Fourier transform of xi. Hence,\nXf\ni=(A1;A2;A3;:::). Lastly, each element of Xtf\ni, no-\ntatedBiis obtained by segmenting the magnitude spectro-\ngram of the input instance, both along the time and fre-\nquency axes. For example, in Fig. 4(b) the spectrogram of\nthe instance is non-uniformly segmented into eight time-\nfrequency blocks. Hence, Xtf\ni=(B1;B2;:::::::;B 7;B8).\nWe call each element of a sequence as an interpretable\ncomponent . Thus, for a temporal sequence each inter-pretable component is a supersample and for spectral and\ntime-frequency sequences each interpretable component is\na spectral bin and time-frequency block, respectively.\nThe next step is to map the input instance with fea-\nture representation denoted as xi2Rnto its interpretable\nrepresentation denoted as x0\ni2f0;1gjXij. Thus, each of\nthe above mentioned sequences is used to deﬁne an inter-\npretable spaceTand an interpretable representation x0\ni.\nThis creates three interpretable representations for the in-\nput instance xi. We denote temporal, spectral and time-\nfrequency interpretable representations as xt\ni0,xf\ni0andxtf\ni0\nrespectively. These representations provide us three ways\nof understanding a prediction, each highlighting the tem-\nporal, spectral or time-frequency segments of the instance\ninﬂuencing the prediction most.\nTo ﬁnd an explanation, SLIME approximates the classi-\nﬁerC:Rn!Rwith a linear model fg(z0) =wTz0;z02\nTg. To do this SLIME ﬁrst generates Nssamples from\nTin a way that depends on x0\ni, i.e., randomly setting to\nzero the dimensions of x0\ni. Hence, for the interpretable se-\nquenceXt\niin Fig. 4(a), one possible zt\ni0= (1, 0, 1, 0).\nThis synthetic sample indicates the absence of super sam-\nplesT2andT4. Formally, for an instance with Nssuper\nsamples, a total of 2Nssynthetic samples exists. With an\nassumption that there exists a surjective map from Rnto\nT, each synthetic sample is projected to Rn, weighted us-\ning an exponential kernel learned over cosine distance (we\nused the same \u001axias in [17]) and mapped to its correspond-\ning probability C(z). SLIME learns the linear model gtby\nminimising the squared loss and model complexity as in\n(1) over this dataset of synthetic samples and their proba-\nbilities. Formally, denote the kth sample as z0\nkand its pro-\njection zk. Deﬁne a weight function \u001axi:Rn\u0002Rn!R.\nThe locally-weighted loss used by SLIME is given by\nL(C;g;\u001a xi) =X\n(z0\nk;zk)2Z\u001a(xi;zk)[C(zk)\u0000g(z0\nk)]2(2)\nSimilarly, SLIME randomly samples xf\ni0andxtf\ni0to learn\nthe linear models gfandgtf, respectively. Each of these\nmodels provides interpretable explanations in terms of\ntheir learned weights. The magnitude of the coefﬁcients\nrelates to the importance of the temporal segment (super\nsample) or the spectral component (bin frequency) or the\ntime-frequency block in the classiﬁcation of xi. Thus, if\nw1andw2denote the coefﬁcients of super samples T1and\nT2respectively, thenjw1j\u0015jw2jimplies super sample T1\nhas more inﬂuence on a classiﬁcation prediction than T2.\nSimilarly, the polarity of regression weights refers to the\ncorrelation between the segment and the classiﬁer predic-\ntion. For example, if w1<0andw2>0, then the tempo-\nral segments T1andT2are negatively and positively cor-\nrelated with the classiﬁer prediction. The weight function\n\u001axicontrols the contribution each synthetic sample has in\nthe learned model g. Thus, a distant sample in interpretable\nspaceTwill have lower contribution to gfacilitating bet-\nter learning in cases where the random sampling produces\nsamples with highly imbalanced class distributions.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 539Classiﬁer Acc[%] Prec. Recall F- score\nDecision tree 71.4 0.72 0.81 0.75\nRandom forest 76.3 0.75 0.88 0.79\nTable 1 : Singing voice class evaluation results for the two\nselected shallow SVD systems (a) Binary decision tree of\ndepth 8 and information gain as the split criterion (b) Ran-\ndom forest of 64 trees, each with depth 16.\n4. DEMONSTRATION\nWe now use SLIME to explain the predictions of three\nsinging voice detection (SVD) systems that classify an au-\ndio excerpt into two categories: music without singing\nvoice, and music with singing voice. Two systems are\nbased on a shallow architecture proposed in [10]. The other\none is based on hierarchical feature learning [19].\n4.1 Explaining Predictions of Shallow Vocal Detectors\nSeveral shallow vocal detection systems have been pro-\nposed [10, 13, 16, 18]. We adapt the method proposed\nin [10] that uses only MFCC features to reach state of the\nart performance. Our system calculates FFTs on a frame\nsize of 200 ms with 50% overlap at a sampling frequency\nof 22050 Hz. It uses a set of 30 Mel-ﬁlters to extract 30\nMFCC coefﬁcients (including the 0th) and their ﬁrst-order\nderivatives from each audio frame using Librosa [14]. The\nsystem performs classiﬁcation over a 1 sec excerpt, hence\nit calculates the median and standard deviation of the 60\ndimensional vector over ﬁve frames [18], constructing a\nfeature vector of 120 dimensions.\nWe train two systems: the ﬁrst ( S1) combines a binary\ndecision tree (BDT) with the feature vector from above and\nthe second ( S2) replaces the BDT with a random forest\n(RF) classiﬁer. The Jamendo dataset, introduced in [16]\nis used to train, validate and evaluate both the models on\nthree non-overlapping sets. Table 1 reports the results of\nthe evaluation for singing voice class. The vocal detection\nsystems designed using the BDT and RF classiﬁers achieve\nan overall accuracy of 71.4% and 76.3%, respectively. The\nvocal class occupies 57.5% of the test dataset which sug-\ngests that these two systems may have learnt some rep-\nresentation of singing voice that helps to detect vocals.\nWe now apply SLIME to determine if these systems are\ntrustworthy [22]. In other words, are the vocal predictions\ncaused by content where there actually is voice?\nIn order to generate temporal explanations, we segment\nthe instance (1 sec) into ten super samples, each of 100\nms duration. We ﬁrst generate 1000 samples in the inter-\npretable space. We then approximate each classiﬁer’s deci-\nsion boundary in a neighbourhood of the instance by a lin-\near model learnt over the interpretable space. The number\nof interpretable components needed to explain an instance\nmay vary from one instance to the other, but to reduce the\nmodel complexity ( \u0001(g)in (1)), we generate explanations\nwith a ﬁxed number of components. To do this we ﬁrst use\nthe synthetic dataset of perturbed samples and their proba-\nbilities to select the top-3 super samples by forward selec-\ntion, and then learn a linear model [17].Id.Dur.\n(s)Prob-Vocal SS-Pred.SS-True\nBDT RF BDT RF\n41 1.0 0.97 0.85 6,7,9 2,0,7 0-9\n178 1.0 0.86 0.86 9,8,4 9,6,0 0-9\n58 0.4 0.80 0.76 6,5,3 0,2,6 0-3\n124 0.4 0.92 0.84 0,4,6 6,9,8 6-9\nTable 2 : Instance-based temporal explanations generated\nby SLIME. Id: instance index, Dur: vocal duration, SS: su-\nper samples, Prob-V ocal: probability assigned by the SVD\nsystem that the instance contains singing voice, SS-Pred:\nsuper sample indices that are the most inﬂuential upon the\nclassiﬁcation of the input instance to vocal class, SS-True:\nsuper sample indices that actually contain singing voice.\nTable 2 reports the temporal explanations generated by\nSLIME for four instances extracted from the “03 - Say me\nGood Bye.mp3” test ﬁle in the Jamendo dataset. The super\nsamples are arranged in the decreasing order of inﬂuence\non the prediction. The magnitude of the weights learned\nfor each super sample determines the inﬂuence it has on\nthe prediction. This analysis of the temporal explanations\nhelps to gain insight about how the models are forming\ntheir predictions. For example, instance 41 is correctly pre-\ndicted by both the models (true positive). But, the tempo-\nral explanations for both the models are very different. The\nsame is the case with another instance 178. Listening to all\nthe predicted super samples for instances 41 and 178, high-\nlights an interesting observation. For most of the predicted\nsuper samples for the decision tree model there is a pres-\nence of ‘strong’ instrumental onset along with the singing\nvoice. Thus, it might be the case that instead of “listening”\nto the singing voice in the super sample, the decision tree\nmodel is paying attention to instrumental onset.\nTo verify the above hypothesis, we select true positive\ninstances that have instrumental music and singing voice as\nseparate temporal sections. We apply SLIME to two such\ninstances: 58 and 124, which have singing voice in the\nﬁrst and last 400 ms, respectively. The temporal explana-\ntions generated for the BDT highlight that even though the\nprediction score is high for the model, the super samples\nit believes to contain singing voice have only instrumen-\ntal music in most of the explanations. This raises ques-\ntions about the generalisation capability of such a model.\nBased on the explanations generated for the RF model, it\nappears that the model is looking at the right temporal sec-\ntions to form a prediction. Thus, the temporal explanations\nare helpful in identifying an untrustworthy model.\nTemporal explanations help to understand the predic-\ntions but under some limitations. First, for the explana-\ntions to be clearly audible super samples should be at least\n100ms long. Second, for the cases as in instance 41, where\nsinging voice and instrumental music are present for com-\nplete duration, temporal explanations do highlight which\ntemporal sections are useful for prediction but not what in\nthat section is important. One way to solve this problem\nis to use SLIME to generate the spectral or time-frequency\nexplanations as demonstrated below.540 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017512102420484096HzInput Mel-spectrogram\n512102420484096HzTim e-freq segm entation\n0 0.5 1 1.5\nTim e512102420484096HzPos. saliency (grd >  0.5)\n0 0.5 1 1.5\nTim e512102420484096Hza) b)\nc) d)T op-3 interpretable components from SLIMEFigure 5 : Comparing the positive explanation from\nSLIME with the positive saliency map for a 1.6s excerpt.\n4.2 Explaining Predictions of a Deep Vocal Detector\nWe now demonstrate SLIME working with the convolu-\ntional neural network (CNN)-based system proposed in\n[19]. We generate time-frequency explanations for the pre-\ndictions of the system and compare the generated explana-\ntions with the saliency maps [20, 21, 26]. Due to space\nrestriction we have skipped the demonstration of spectral\nexplanations, but such explanations can be easily derived\nfrom time-frequency explanations by expanding the tem-\nporal analysis window to full length of the excerpt.\nThe system proposed in [19] takes in a Mel-spectrogram\nrepresentation of a 1.6 second audio excerpt and returns\nthe probability that it contains singing voice. In order to\nexplain the predictions of the system, we map the Mel-\nspectrogram to the time-frequency interpretable represen-\ntation proposed in subsection 3.2. We segment the time-\nfrequency axis of the input in 6 and 4 segments, respec-\ntively. Thus, the temporal axis of each of the ﬁrst 5 seg-\nments is 266 ms in duration and that of the last segment\nis 280 ms. We aim to keep the temporal axis of the re-\nsulting interpretable components long enough to facili-\ntate audition in the temporal domain. Similarly, segmen-\ntation along the frequency axis results in 4 spectral sec-\ntions, each with 20 spectral bins. Thus, the input Mel-\nspectrogram is mapped to a sequence of time-frequency\nblocksXtf\ni= (B1;:::;B 24), where each block represents\na dimension in the interpretable space. Fig. 5(a), (b) depict\nthe Mel-spectrogram and its time-frequency segmentation,\nrespectively for an input excerpt from “03 - Say me Good\nBye.mp3” ﬁle from the Jamendo test dataset.\nSLIME generates 2000 samples in the neighbourhood\nof the input, approximates the non-linear decision bound-\nary by a linear model, and selects the top-3 interpretable\ncomponents (time-frequency blocks) with the highest pos-\nitive weights. Fig. 5(c) depicts the positive explanation\nfor the prediction of the audio excerpt. We call an expla-\nnation positive if the weights of the interpretable compo-\nnents in the explanation are positive. The input excerptchosen for analysis has singing voice with musical accom-\npaniment for the ﬁrst 900 ms and only musical accompa-\nniment for the last 700 ms. We invert the time-frequency\nblocks in the explanation to temporal domain and on lis-\ntening ﬁnd that all the components in the explanation have\nthe presence of singing voice. This raises conﬁdence in the\npredictions of the model. Moreover, all the components in\nthe negative explanation (not shown due to space restric-\ntion), fall in the temporal sections after 1s. This indicates\nthat the time-frequency segments containing only instru-\nmental music are negatively correlated with the classiﬁer\nprediction. This also seems to be correct behaviour. Thus,\nthe time-frequency explanations help to understand what\nsections in the input are inﬂuencing the prediction most.\nWe now compare SLIME-based explanations with\nsaliency maps. Saliency maps, like time-frequency ex-\nplanations, are tools to analyse black-box neural network\nmodels. They highlight how each input dimension inﬂu-\nences the prediction. The gradient of the output prediction\nwith respect to each input dimension is calculated to com-\npute the saliency maps [20]. Thus, they depict the effect\nof modifying the input along any dimension, on the net-\nwork prediction. Instead of allowing all the gradients to\nﬂow back, techniques proposed in [21, 26] only allow the\npositive gradient to ﬂow back resulting in cleaner visuali-\nsations. Using the technique proposed in [26], we employ\na leaky-ReLU non-linearity [8] in the backward path to re-\nduce the magnitude of the negative gradients ﬂowing back.\nWe compare the positive time-frequency explanations with\nthe positive saliency map. This map will highlight the\ninput dimensions that are positively correlated with the\nclassiﬁer prediction. Not all the dimensions inﬂuence the\npredictions equally, thus we select only those dimensions\nwhose normalised gradient is more than 0.5. We generate\nsuch maps for the output layer of the network. Fig. 5(d)\nshows the thresholded positive saliency map.\nIt is important to note that saliency maps highlight in-\ndividual dimensions in the input while SLIME based ex-\nplanations are time-frequency blocks. One way to com-\npare the two is by visually verifying whether all the di-\nmensions highlighted by the saliency maps are captured\nin the explanations created by SLIME. A visual compar-\nison for the example in Fig. 5 shows that SLIME’s ex-\nplanation includes most of the key dimensions highlighted\nby the saliency map. Numerically we measure how many\ndimensions highlighted by the saliency map are enclosed\nin the explanation generated by SLIME. For the audio ex-\ncerpt shown in Fig. 5, this agreement is 62.5%. We ex-\npand this analysis to a set of 1349 randomly chosen ex-\ncerpts from the Jamendo test dataset. We found that on\nan average SLIME achieves 46.50 % numerical agreement\nwhen compared with the positive saliency maps. Instance-\nbased analysis reveals that in some instances the numerical\nagreement is 100%, but there are cases where this number\nis less than 10%. One possible explanation for this is the\nshape of decision boundary near the instance. If the deci-\nsion boundary is highly non-linear, approximating it with a\nlinear model will result in poor explanations from SLIME.\nWe have not performed an exhaustive comparison (byProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 5412\n4\n6\n8\n10\n12\n14Un(a)\ntop-5\ntop-3\n2000\n4000\n6000\n8000\n10000\nNs\n0\n50\n100\n150\n200\n250\n300\n350Ts (sec)(b)\ntop-5Figure 6 : Plotting the effect of the number of samples ( Ns)\non (a) the stability of generated explanations (b) the time\ntaken to generate them.\nvarying the preset factors, e.g. threshold, number of com-\nponents) between the two techniques. The above analy-\nsis aims to provide an estimate about the performance of\nmodel-agnostic SLIME against a model-dependent tech-\nnique for some preset values. It is obvious that the numer-\nical agreement will be high if the constraints are softer and\nvice versa. Although saliency maps are accurate in high-\nlighting the input dimensions that are inﬂuential to a classi-\nﬁer’s output, they can suffer from lack of temporal context\naround the dimensions. On the other hand, SLIME-based\nexplanations can be readily inverted to an acoustic form for\naudition, which may provide additional insights into how a\nclassiﬁer is forming its prediction for an input.\n4.3 Discussion on the Number of Samples (Ns)\nAs discussed in subsection 3.2, to explain a prediction\nLIME generates Nssamples in the interpretable space\n(T). In [17] there is no discussion about how many sam-\nples should be used to generate each explanation. We\nbelieve that exploring this is important for two reasons.\nFirst, it affects the time taken ( Ts) to generate an expla-\nnation. Second, it affects the stability of the generated ex-\nplanation. Ideally, an explanation should remain the same\n(at least the interpretable components, but their order and\nweights might change) even on multiple iterations of ap-\nplying LIME to the same instance. But, empirically we\nﬁnd that the generated explanations do change on multiple\niterations. This happens because LIME samples randomly\ninT. In this section we seek to understand the effect of\nNson the stability of the explanations and on the time to\ngenerate one explanation.\nFor the experiment, we use the trained model, dataset\nand SLIME set-up from subsection 4.2. We randomly se-\nlect 5 excerpts from each test ﬁle in the Jamendo dataset.\nWe apply SLIME to generate explanation for the predic-\ntion of each excerpt in a batch of 80 and select the top- k\ninterpretable components per explanation (we try k= 3 and\n5). We iterate this process 5 times, each time randomlysampling 80 excerpts, generating explanations and select-\ning the top-kinterpretable components.\nWe deﬁne the stability of an explanation to be inversely\nproportional to the number of unique interpretable com-\nponents (Un) from the sequence Xithat appear in expla-\nnations generated with miterations. For example, if we\napply SLIME m= 2 times to an instance and select\nthe top-3 interpretable components in each iteration. Say\nthe selected time-frequency segments are denoted as sets\n\u00181=fB1;B2;B3gand\u00182=fB2;B6;B5g. ThenUn= 5,\nasB2appears twice in 6 components. To understand the\neffect ofNson the stability of explanations, we generate\n5 explanations for each of the 80 excerpts in the randomly\nsampled batches. We calculate the value of Unin all the\n5 explanations for each excerpt and plot the average result\nover 5 batches for a given value of Ns. Fig. 6(a) reports\nthe results of the experiment. The result shows that Unis\ninversely related to Ns, and thus the stability of the gen-\nerated explanations is proportional to Ns. The result also\nshows that exhaustive search of the interpretable space T\nis not needed to generate stable explanations.\nWe also record the average time taken to generate one\nexplanation for a given value of Ns. Results are gener-\nated by running SLIME on a computer with 1.6 GHz Intel\ncore i5 processor and 8 GB memory and are reported in\nFig. 6(b). Results show that Tsincreases linearly with Ns,\nreaching to a maximum of around 5 mins for an explana-\ntion generated with Ns= 10k. The reported time includes\nthe time taken for prediction by the CNN. These results\nsuggest that selecting a suitable Nsdepends on the trade-\noff between the stability of an explanation and the time-\ntaken to generate it. In our experiment Ns= 1000 seems\nto be a good trade-off.\n5. CONCLUSION\nIn this work we proposed SLIME, an algorithm that ex-\ntends the applicability of LIME [17] to MCA systems.\nWe proposed three versions of SLIME and demonstrated\nthem with three types of singing voice detection systems\nto generate temporal and time-frequency explanations for\nthe predictions of speciﬁc instances. We see that the tem-\nporal explanations generated by SLIME are helpful for re-\nvealing how the BDT is making decisions based on content\nthat does not contain singing voice despite possessing high\nclassiﬁcation accuracy for the selected instances. Such is-\nsues cast doubt on the generalisability of the model. We\nalso demonstrated that the analysis of time-frequency ex-\nplanations is helpful to gain trust in the CNN based SVD\nsystem. We compared SLIME based explanations with\nsaliency maps for the neural network model and the re-\nsults suggest that model-agnostic SLIME based explana-\ntions agree in many cases with saliency maps.\nIn future we would like to apply SLIME to other MCA\nsystems. We also plan to experiment with improved inter-\npretable representations that will be created around audio\n“objects”. We believe that the improved representations\nwill assist in better understanding of the behaviour of the\nunderlying machine learning model.542 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. ACKNOWLEDGEMENTS\nThis work is supported in part by AHRC Grant Agreement\nno. AH/N504531/1. We would like to thank Jan Schl ¨uter\nfor sharing his implementation of the neural network based\nSVD system. We also thank the anonymous reviewers for\ntheir valuable comments and suggestions.\n7. REFERENCES\n[1] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-\nabe, K. Hansen, and K.-R. Muller. How to explain in-\ndividual classiﬁcation decisions. Journal of Machine\nLearning Research , 11:1803–1831, 2010.\n[2] L. E. Boucheron and P. D. Leon. On the inversion\nof Mel-frequency cepstral coefﬁcients for speech en-\nhancement applications. In Proc. of IEEE Intl. Conf.\non Signals and Electronic Systems (ICSES) , 2008.\n[3] K. Choi, G. Fazekas, and M. B. Sandler. Explaining\ndeep convolutional neural networks on music classiﬁ-\ncation. arXiv preprint arXiv:1607.02444 , 2016.\n[4] S. B. Davis and P. Mermelstein. Comparison of para-\nmetric representation for monosyllabic word recogni-\ntion in continously spoken sentences. IEEE Trans. on\nAcoustics, Speech and Signal Processing. , 28(4):357–\n366, 1980.\n[5] L. Deng and D. Yu. Deep Learning: Methods and Ap-\nplications . Now Publisher, 2014.\n[6] S. Dieleman and B. Schrauwen. End-to-end learning\nfor music audio. In Proc. ICASSP , 2014.\n[7] M. Gevrey, I. Dimopoulos, and S. Lek. Review and\ncomparison of methods to study the contribution of\nvariables in artiﬁcial neural network models. Ecolog-\nical Modelling , 160(3):249–264, February 2003.\n[8] A. Jannun, A. Maas, and A. Ng. Rectiﬁer nonlineari-\nties improve neural network acoustic models. In ICML\nWorkshop on Deep Learning for Audio, Speech and\nLanguage Processing , 2013.\n[9] J. H. Jensen, M. G. Christensen, D. P. W. Ellis, and\nS. H. Jensen. Quantitative analysis of a common audio\nsimilarity measure. IEEE Trans. on Audio, Speech, and\nLanguage Processing , 17(4):693–703, 2009.\n[10] B. Lehner, R. Sonnleitner, and G. Widmer. Towards\nlight-weight, real-time-capable singing voice detec-\ntion. In Proc. ISMIR , 2013.\n[11] B. Letham, C. Rudin, T. H. McCormick, and D. Madi-\ngan. Interpretable classiﬁers using rules and Bayesian\nanalysis: Building a better stroke prediction model. An-\nnals of Applied Statistics , 9(3):1350–1371, 2015.\n[12] B. Logan. Mel frequency cepstral coefﬁcients for mu-\nsic modeling. In Proc. ISMIR , 2000.[13] M. Mauch, H. Fujihara, K. Yoshii, and M. Goto. Tim-\nbre and melody features for the recognition of vocal\nactivity and instrumental solos in polyphonic music. In\nProc. ISMIR , 2011.\n[14] B. McFee, C. Raffel, D. Liang, D. P. W. Ellis,\nM. McVicar, E. Battenberg, and O. Nieto. librosa: Au-\ndio and music signal analysis in python. In Proc. of the\n14th Python in Science Conference (SCIPY) , 2015.\n[15] G. Montavon, S. Bach, A. Binder, W. Samek, and K. R.\nM¨uller. Explaining nonlinear classiﬁcation decisions\nwith deep taylor decomposition. Pattern Recognition ,\n65:211–222, 2017.\n[16] M. Ramona, G. Richard, and B. David. V ocal detec-\ntion in music using support vector machines. In Proc.\nICASSP , pages 1885–1888, 2008.\n[17] M. T. Ribeiro, S. Singh, and C. Guestrin. Why should\nI trust you?”: Explaining the predictions of any classi-\nﬁer. In Proc. of the ACM Conf. on Knowledge Discov-\nery and Data Mining (KDD) , 2016.\n[18] M. Rocamora and O. Herrera. Comparing audio de-\nscriptors for singing voice detection in music audio\nﬁles. In Brazilian Symposium on Computer Music , vol-\nume 26, page 27, 2007.\n[19] J. Schl ¨uter and T. Grill. Exploring data augmentation\nfor improved singing voice detection with neural net-\nworks. In Proc. ISMIR , 2015.\n[20] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep In-\nside Convolutional Networks: Visualising Image Clas-\nsiﬁcation Models and Saliency Maps. In Proc. ICLR\nWorkshop , 2014.\n[21] J. T. Springenberg, A. Dosovitskiy, T. Brox, and\nM. Riedmiller. Striving for simplicity: The all convolu-\ntional net. In Proc. of the Intl. Conf. on Learning Rep-\nresentations (ICLR) Workshop , 2015.\n[22] B. L. Sturm. A simple method to determine if a music\ninformation retrieval system is a “horse”. IEEE Trans.\non Multimedia , 16(6):1636–1644, 2014.\n[23] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-\nhan, I. Goodfellow, and R. Fergus. Intriguing proper-\nties of neural networks. In Proc. ICLR , 2014.\n[24] F. Wang and C. Rudin. Falling rule lists. In Proc. AIS-\nTATS , 2015.\n[25] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lip-\nson. Understanding Neural Networks Through Deep\nVisualization. In Proc. of the Intl. Conf. on Machine\nLearning (ICML) Deep Learning Workshop , 2015.\n[26] M. D. Zeiler and R. Fergus. Visualizing and Under-\nstanding Convolutional Networks. In Proc. of the Eu-\nropean Conf. on Computer Vision , 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 543"
    },
    {
        "title": "Performance Error Detection and Post-Processing for Fast and Accurate Symbolic Music Alignment.",
        "author": [
            "Eita Nakamura",
            "Kazuyoshi Yoshii",
            "Haruhiro Katayose"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1414940",
        "url": "https://doi.org/10.5281/zenodo.1414940",
        "ee": "https://zenodo.org/records/1414940/files/NakamuraYK17.pdf",
        "abstract": "This paper presents a fast and accurate alignment method for polyphonic symbolic music signals. It is known that to accurately align piano performances, methods using the voice structure are needed. However, such methods typi- cally have high computational cost and they are applicable only when prior voice information is given. It is pointed out that alignment errors are typically accompanied by per- formance errors in the aligned signal. This suggests the possibility of correcting (or realigning) preliminary results by a fast (but not-so-accurate) alignment method with a refined method applied to limited segments of aligned sig- nals, to save the computational cost. To realise this, we develop a method for detecting performance errors and a realignment method that works fast and accurately in lo- cal regions around performance errors. To remove the de- pendence on prior voice information, voice separation is performed to the reference signal in the local regions. By applying our method to results obtained by previously pro- posed hidden Markov models, the highest accuracies are achieved with short computation time. Our source code is published in the accompanying web page, together with a user interface to examine and correct alignment results.",
        "zenodo_id": 1414940,
        "dblp_key": "conf/ismir/NakamuraYK17",
        "keywords": [
            "polyphonic",
            "symbolic",
            "music",
            "signals",
            "alignment",
            "method",
            "fast",
            "accurate",
            "polyphonic",
            "symbolic"
        ],
        "content": "PERFORMANCE ERROR DETECTION AND POST-PROCESSING FOR\nFAST AND ACCURATE SYMBOLIC MUSIC ALIGNMENT\nEita Nakamura\nKyoto University\nenakamura@sap.ist.i.kyoto-u.ac.jpKazuyoshi Yoshii\nKyoto University/RIKEN AIP\nyoshii@kuis.kyoto-u.ac.jpHaruhiro Katayose\nKwansei Gakuin University\nkatayose@kwansei.ac.jp\nABSTRACT\nThis paper presents a fast and accurate alignment method\nfor polyphonic symbolic music signals. It is known that\nto accurately align piano performances, methods using the\nvoice structure are needed. However, such methods typi-\ncally have high computational cost and they are applicable\nonly when prior voice information is given. It is pointed\nout that alignment errors are typically accompanied by per-\nformance errors in the aligned signal. This suggests the\npossibility of correcting (or realigning ) preliminary results\nby a fast (but not-so-accurate) alignment method with a\nreﬁned method applied to limited segments of aligned sig-\nnals, to save the computational cost. To realise this, we\ndevelop a method for detecting performance errors and a\nrealignment method that works fast and accurately in lo-\ncal regions around performance errors. To remove the de-\npendence on prior voice information, voice separation is\nperformed to the reference signal in the local regions. By\napplying our method to results obtained by previously pro-\nposed hidden Markov models, the highest accuracies are\nachieved with short computation time. Our source code is\npublished in the accompanying web page, together with a\nuser interface to examine and correct alignment results.\n1. INTRODUCTION\nTo computationally analyse music performances or to con-\nstruct performance databases, it is needed to match notes\nin a music performance signal (called an aligned signal )\nto those in a reference musical score or another perfor-\nmance signal ( reference signal ). This process is called mu-\nsic alignment and automating it is a fundamental technique\nfor music information processing and has been a popular\nﬁeld of research [1–16]. This study deals with ofﬂine sym-\nbolic music alignment, with particular focus on piano per-\nformances. Both score-to-MIDI alignment and MIDI-to-\nMIDI alignment are considered in this paper. We consider\nWestern classical music or similar music styles where mu-\nsical scores exist behind the performances.\nc\rEita Nakamura, Kazuyoshi Yoshii, Haruhiro Katayose.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Eita Nakamura, Kazuyoshi Yoshii,\nHaruhiro Katayose. “ Performance Error Detection and Post-Processing\nfor Fast and Accurate Symbolic Music Alignment ”, 18th International\nSociety for Music Information Retrieval Conference, Suzhou, China,\n2017.\n21 22\n21 22\n##\n##*#\n##*#\n#\n##*\n##\n##*#‹ #\n##\n##\n##\n##\n##\n##\n## ##\n### ‹ #\n##\n#\n21 22\n21 22\n##\n## #\n###\n#\n## ##\n##\n#‹ #\n##\n##\n##\n##\n##\n##\n## ##\n### ‹ #\n##\n#&\n?\n&\n?\n&\n?\n&\n?\tTFD\n\tTFD\n3FBMJHONFOU 3FBMJHONFOU*OQVU\u001b\u00013FGFSFODF\u0001TDPSF\u0010.*%*\u0001TJHOBM\u0001\f\u0001BMJHOFE\u0001.*%*\u0001TJHOBM\n&SSPS\u0001SFHJPOT.JTTJOH\u0001OPUF &YUSB\u0001OPUF 1JUDI\u0001FSSPS1SFMJNJOBSZ\u0001BMJHONFOU\u0001 \u0007\u00011FSGPSNBODF\u0001FSSPS\u0001EFUFDUJPO\n3FGFSFODF\nTJHOBM\n\"MJHOFE\nTJHOBM\n3FGFSFODF\nTJHOBM\n\"MJHOFE\nTJHOBMFigure 1 . An outcome of the proposed method. Errors\nin preliminary alignment caused by reordered note pairs in\nthe aligned signal are corrected by the realignment method.\nSince music alignment between two identical perfor-\nmances is trivial, the central issue of automatic music\nalignment is to handle deviations in music performances.\nPossible deviations include tempo changes, performance\nerrors (e.g. pitch errors, note insertions and deletions), or-\nnamentation, and global structural differences (repeats and\nskips). To ﬁnd the optimal alignment, various extensions\nof sequence matching methods such as hidden Markov\nmodels (HMMs) [7–9] and dynamic time warping (DTW)\n[1,2,6] have been studied. In the method using HMMs [7],\nfor example, an HMM is constructed for each reference\nsignal, in which note insertions and deletions, repeats, and\nskips are described by transition probabilities, and pitch\nerrors are described by output probabilities. The aligned\nsignal is considered as an output sequence from the HMM\nand the most probable sequence of latent states is estimated\nwith the Viterbi algorithm for alignment.\nIt has been found that, in the case of polyphonic pi-\nano performances, deviations in performances due to asyn-\nchronies between hands/voices require special treatments\n[4, 5, 7, 9]. Such asynchronies result in reordering of notes\nwith different score times, which is the main cause of\nalignment errors for HMMs or DTWs that are not spe-\ncially designed to handle them. Models with explicit voice\nstructure have been proposed and proved effective to solve\nthis [4,5,9]. However, prior voice information of the refer-\nence signal is needed for applying these methods, which\nimposes limitations on usability since voice information\nis not given in single-channel MIDI signals and in some347score ﬁle formats. Moreover, these methods have high\ncomputational cost compared to standard HMMs or DTWs\n[5–7, 9]. As is empirically known, those reordered notes\nappear only occasionally and in most cases the standard\nalignment methods work as accurately as the reﬁned meth-\nods using voice information. Thus, the high computational\ncost would be reduced if parts of aligned signals, for which\nspecial treatments are necessary, can be selected.\nBecause signiﬁcant deviations in music performances\ncan usually be interpreted as performance errors, align-\nment errors are often connected with performance errors.\nFor example, a pair of extra and missing notes as in Fig. 1\ntypically appear as a result of alignment errors. Based on\nthe authors’ experience, displaying performance errors en-\nables human annotators to easily ﬁnd alignment errors and\ngreatly improves the efﬁciency of examining and correct-\ning automatic alignment results. Likewise, by detecting\nperformance errors in a given result of automatic align-\nment, it would be possible to select limited regions in the\naligned signal that may contain alignment errors.\nBased on these observations, this study aims to develop\nan automatic post-processing method for correcting given\nsymbolic music alignment results. We ﬁrst develop a per-\nformance error detection algorithm that recognises pitch\nerrors, extra notes, and missing notes in a given align-\nment result. Error regions are then deﬁned as segments of\naligned and reference signals around performance errors\nand we investigate how much alignment errors are con-\ntained in these regions with various sizes of the regions.\nNext we develop a post-processing realignment method\nthat can handle hand/voice asynchrony based on a voice-\nstructured model. Since both music alignment and recog-\nnition of performance errors involve searches for an opti-\nmal choice among possible candidate solutions, we formu-\nlate them based on statistical models whose parameters can\nbe optimised from data. To construct a realignment method\nthat does not require prior voice information, we combine\nthe method using merged-output HMMs [9] with a voice\n(hand) separation method [17]. For concreteness, we use\nas a preliminary alignment method the one based on tem-\nporal HMMs [7]. The results of the proposed method are\nevaluated in comparison with the state-of-the-art methods.\nThe contributions of this study are as follows. First,\nour alignment method achieves the highest accuracy and its\ncomputational cost is much smaller than previous methods\nwith comparable accuracies. The method works without\nprior voice information and can be applied for a wide class\nof performance and score data. The source code for our\nalgorithms and a user interface to examine and correct the\nresults is published in the accompanying web page [18]. To\nour knowledge, this is currently the only publicly available\nalignment tool of comparable accuracies. Second, this is\nthe ﬁrst paper that quantitatively investigates the relation\nbetween performance errors and alignment errors, which\ncan be used generally to reduce high computational cost\nthat is typically required in elaborated methods. Lastly, our\nalignment algorithm of the merged-output HMMs yields\nbetter sub-optimisation than a previous one [9].1.1 Current State-of-the-Art Methods\nThe method by Gingras and McAdams [5] ( GM algo-\nrithm ), which takes into account the voice structure and\ntiming information, is regarded as one of the most accu-\nrate methods for symbolic music alignment, with 99.978%\nof accuracy on their data. The method based on the tem-\nporal HMM by Nakamura et al. [7] ( NOSW algorithm )\nalso uses the timing information but not the voice struc-\nture. The method can handle arbitrary repeats and skips,\nbut the accuracy was lower than the GM algorithm in a di-\nrect comparison. For online alignment, the method using\nmerged-output HMMs for incorporating the voice structure\nhad better accuracies than the temporal HMMs [9].\nRecently, Chen et al. [6] reported a signiﬁcant lower ac-\ncuracy (\u001491:93%) for the GM algorithm on other data and\nproposed a method based on DTW ( CJL1 algorithm ) with\na better accuracy ( \u001498:51%)1. Another method ( CJL2\nalgorithm ) is proposed in the paper, which is less accurate\nbut more efﬁcient than the CJL1 algorithm. The CJL al-\ngorithms neither use voice information nor have a special\narchitecture to utilise the voice structure.\n2. PERFORMANCE ERROR DETECTION\n2.1 Problem Statement\nBoth reference and aligned signals can be represented as\na sequence of musical notes (called reference notes and\naligned notes ) with a pitch and an onset time described as\nphysical or score time. For MIDI-to-MIDI alignment, the\nreference signal can be a performed MIDI signal that has\n(almost) continuous onset times. In this case, we cluster\nnotes according to onset times to obtain a reference sig-\nnal with quantised onset times, which enables us to dis-\ncuss score-to-MIDI and MIDI-to-MIDI alignment in a uni-\nﬁed way. Speciﬁcally, we put a threshold of 35 ms, which\nis known to well discriminate chordal notes [19], to form\nclusters of notes and then quantise onset times (e.g. in units\nof ms etc.). An alignment result is a sequence of labels that\nindicates for each aligned note the corresponding reference\nnote. If there is no corresponding note (as is the case for\nextra notes), a distinguished label ‘ EXTRA ’ is given.\nAs performance errors we consider pitch errors, extra\nnotes, and missing notes. Extra notes are aligned notes that\nare not matched to any of the reference notes and missing\nnotes are reference notes that do not appear in the aligned\nsignal. In this study we consider the strict alignment , for\nwhich each reference note can be matched to at most one\naligned note2. For a strict alignment result, performance\nerrors are automatically determined: aligned notes with-\nout corresponding reference notes are extra notes; aligned\nnotes with pitches different from the corresponding refer-\nence notes have pitch errors; reference notes not appearing\n1A different evaluation measure was used in Ref. [6] and these upper\nbounds have been derived as conservative limits.\n2This condition must be relaxed and apply only locally if we allow\nglobal repeats and skips in the aligned signal. In addition, trills and tremo-\nlos are exceptions where multiple aligned notes correspond to each refer-\nence note. For simplicity and for the lack of space, we concentrate on the\ncase without ornaments, repeats, and skips in this paper.348 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017&3FGFSFODF\nTJHOBM\n\"MJHOFE\nTJHOBM&3FGFSFODF\u0001DMVTUFS\n3FTVMU\u0001PG\u0001QSFMJNJOBSZ\nBMJHONFOU\n&YUSB\u0001DMVTUFS\n\tB\n-3\u00017JUFSCJ\u0001BMJHONFOU\nGPS\u0001BMJHOFE\u0001DMVTUFST&\n.JTTJOH\u0001OPUF\n&YUSB\u0001OPUF\n1JUDI\u0001FSSPS3FGFSFODF\nDMVTUFS\"MJHOFE\nDMVTUFS\n-3\u00017JUFSCJ\u0001BMJHONFOU\nGPS\u0001OPUFT\u0001CFUXFFO\nDPSSFDUMZ\u0001QMBZFE\u0001OPUFT\n\tC\nFigure 2 . Steps for performance error detection. After (a)\ncluster-wise LR Viterbi alignment is performed, (b) note-\nwise LR Viterbi alignment is performed for each cluster.\nin the alignment result are missing notes.\nStandard alignment methods such as HMMs and DTWs\noften output alignment results that are not strict (so that\na reference note can appear more than once) and perfor-\nmance error indications are not given. Therefore, the aim\nof performance error detection is to obtain a strict align-\nment result from a non-strict alignment result, which is\nequivalent to identifying extra notes in the latter one. In\nfact, our method uses only the information of matched\nscore times for each note in an input alignment result.\n2.2 Model\nOur approach for the identiﬁcation of extra notes is to carry\nout two left-to-right (LR) Viterbi alignments, ﬁrst in units\nof ‘chords’ and second in units of notes within each ‘chord’\n(Fig. 2). To be precise, we deﬁne a reference cluster as a set\nof all notes with the same score time in the reference sig-\nnal. Aligned notes are clustered so that all successive notes\nform a cluster ( aligned cluster ) as long as their reference\nlabels are in the same onset cluster. The ﬁrst LR Viterbi\nalignment is then performed on the sequence of aligned\nclusters and those clusters assigned a reference cluster dif-\nferent from the original one are identiﬁed as extra clusters.\nNote that after this procedure each non-extra aligned clus-\nter is matched to a unique reference cluster.\nIn the next step, extra notes in each (non-extra) aligned\ncluster are identiﬁed. Based on our intuition that aligned\nnotes with correct pitches play a pivot role and the as-\nsigned reference labels should respect the pitch order, we\nﬁrst identify aligned notes with correct pitches and then\nmatch other notes, which are either extra notes or notes\nwith pitch errors. Since in general there are multiple notes\nwith the same pitch in one aligned cluster, the onset time\ninformation should be used here. As a reference point of\nonset time, the expected onset time ~tof the reference onset\ncluster is computed by local averaging, similarly as tempo\nestimation [19]. If there are multiple candidates with the\ncorrect pitch, the one with an onset time nearest to ~tis cho-\nsen and the other candidates are identiﬁed as extra notes.\nLet(q1;:::;q C)denote an ordered set of notes in the\nconcerned reference cluster, where qcis the integral pitch\nof thec-th note and satisﬁes q1\u0014q2\u0014\u0001\u0001\u0001\u0014qC, and let\nQcorrdenote the set of reference notes matched to aligned\nnotes with correct pitches. Similarly, let us order notes in\nthe concerned aligned cluster according to pitch ﬁrst and\nthen onset time. Denoting the pitch and onset time of theb-th note bypbandtb, we thus have for all b2f1;:::;Bg\n(Bis the number of notes in the aligned cluster) pb\u00001\u0014pb\nandtb\u00001\u0014tbifpb\u00001=pb. Now suppose that a pair\nof pivot notes (c;c0)(c;c02 f1;:::;Cg) satisﬁes that\nqc;qc02Qcorr,qc< qc0, andqj=2Qcorrfor eachqj\nwithc<j <c0. For such a pair we deﬁne Q=fqjjc<\nj < c0gandS=fb2f1;:::;Bgjqc< pb< qc0g. The\nnext step is to match QandSfor each pair (c;c0)of pivot\nnotes. For aligned notes with pitches higher or lower than\nthe highest or lowest pivot note, we can similarly deﬁne Q\nandSas half-bounded sets, and for the case with no pivot\nnotes, we deﬁne Q=f1;:::;CgandS=f1;:::;Bg,\nand carry out the following procedure.\nThe matching is trivial when #Q\u00141and#S\u00141.\nIn other cases, multiple interpretations of pitch errors ex-\nist and some principle must be introduced to ﬁnd the op-\ntimal choice (Fig. 2(b)). We solve this optimisation prob-\nlem with a statistical performance model including tem-\nporal ﬂuctuations and pitch errors, similar to the model in\nRef. [7]. The mapping z:Q3j7!zj2Sis optimised\nby LR Viterbi alignment with the following probability:\nP(zj=bjzj\u00001=b0) =\u0012b0b pitch(pb\u0000qb) time(tb\u0000~t)\nwhere\u0012is a#S\u0002#SLR transition probability matrix,\n\u0012b0b=(\n1=#fl2Sjl>b0g; b0<b;\n0; otherwise;(1)\nand pitch(\u000ep)is the probability of pitch errors in \u000epsemi-\ntones (given by Eq. (30) of Ref. [19]), and  time(\u000et)is the\nprobability of onset time ﬂuctuation given as\n time(\u000et) =N(\u000et; 0;\u001a2): (2)\nHere, N(\u0001;\u0016;\u0006)denotes a normal distribution with mean\n\u0016and variance \u0006. The value of \u001ais taken as 100 ms in our\nimplementation. Aligned notes without matched reference\nnotes are classiﬁed as extra notes.\n2.3 Error Regions and Alignment Errors\nHaving identiﬁed the performance errors, we now deﬁne\nerror regions in the aligned signal around them. To do this,\nwe ﬁrst calculate the synchronised onset time for each ref-\nerence cluster by averaging onset times of corresponding\naligned notes, or if there are no such notes, by interpolat-\ning/extrapolating neighbouring synchronised onset times.\nWe consider, for each extra note nwith onset time tn, a\ntime interval of the form [tn\u0000\u0001;tn+ \u0001) with width \u0001\n(called an extra note region) and construct the set Reof\nsuch time intervals for all extra notes. Likewise, the set of\npitch error regions Rpis constructed. The set of missing\nnote regionsRmis similarly constructed by using the syn-\nchronised onset times to deﬁne each time region. Finally,\nthe error regionRis constructed by combining elements\ninRe,Rp, andRm. If there are overlapping time regions,\nthey are expanded/uniﬁed to one time region at this step\n(Fig. 3). Thus,Ris a setf[tr;t0\nr)gNR\nr=1of non-overlappingProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 3491IZTJDBM\u0001UJNF\u0001JO\nUIF\u0001BMJHOFE\u0001TJHOBM\n.JTTJOH\u0001OPUF\n&YUSB\u0001OPUF\n1JUDI\u0001FSSPS\n&SSPS\u0001SFHJPOT2∆\n2∆\n2∆\n(nm,ne,np): \t\u0012\r\u0012\r\u0012\n \t\u0011\r\u0012\r\u0012\n \t\u0012\r\u0012\r\u0011\nFigure 3 . Examples of error regions and their indices.\ntime regions where we have t0\nr\u00001<trfor allr. The num-\nber of missing notes, extra notes, and pitch errors in each\nregionris denoted by nm(r),ne(r), andnp(r).\nLet us discuss the relation between performance errors\nand alignment errors. An alignment error is deﬁned as a\nreference label in an alignment result that is different from\nthe ground-truth label. We say that an alignment error is\ncontained in the error regions if the onset time of the in-\ncorrectly aligned note is contained in one of the regions in\nR. The proportion of alignment errors contained in the er-\nror regions for varying time width \u0001, calculated for align-\nment results of the temporal HMM on the three datasets\nexplained in Sec. 4, is shown in Fig. 4, together with the\nproportion of aligned notes contained in the error regions.\nMore than 90% of the performance errors are contained in\nthe error regions for \u0001as small as 0:1s, while contained\naligned notes remain less than 20% for\u0001\u00140:3s.\nFor the alignment errors to be corrected by realign-\nment carried out on each error region, not only incor-\nrectly aligned notes but also their corresponding reference\nnotes must be contained in the region. To be precise, for\neach time region [tr;t0\nr)inR, we choose segments of the\naligned and reference signals and use them as the aligned\nand reference signals for realignment. For the segment of\nthe aligned signal, the subsequence of aligned notes whose\nonset times belong to the time region is used. For these\naligned notes, we obtain the maximal and minimal score\ntimes (\u001cmax and\u001cmin) of corresponding reference notes.\nThe subsequence of all reference notes whose onset score\ntimes are in the range [\u001cmin;\u001cmax]is used as the segment of\nthe reference signal. We call an alignment error in an error\nregion correctable if its ground-truth label is ‘ EXTRA ’ or\nis a reference note in the reference signal segment. We see\nin Fig. 4 that the proportion of correctable errors increases\nrapidly for \u0001<0:3s and gradually for \u0001>0:3s.\nAlthough it is not always the case, naively we expect\nthat the number of performance errors is reduced when\nalignment results are corrected, as is evident in the case of\ncorrecting a mismatched pair of missing and extra notes.\nOn the other hand, if only one performance error exists or\nonly missing notes exist in an error region, the number of\nperformance errors cannot be reduced by realigning notes.\nBy expanding this idea, we can impose conditions on er-\nror regions so that most of the alignment errors remain\ncontained in the selected error regions but the contained\nalignment notes are reduced signiﬁcantly. Results in Ta-\nble 1, where error regions were imposed the condition of\ncontaining at least two types of performance errors, show\nan example of this fact. Such conditions can be used to\nincrease the efﬁciency of realignment, as we see in Sec. 4.\n\u0001\u0011\u0001\u0013\u0011\u0001\u0015\u0011\u0001\u0017\u0011\u0001\u0019\u0011\u0001\u0012\u0011\u0011\n\u0001\u0011 \u0001\u0011\u000f\u0016 \u0001\u0012 \u0001\u0012\u000f\u0016 \u0001\u0013 \u0001\u0013\u000f\u0016 \u0001\u0014\nWidth ∆[sec]Proportion (%)$PSSFDUBCMF\u0001FSSPST\"MJHONFOU\u0001FSSPST\n\"MJHOFE\u0001OPUFTFigure 4 . Proportion of alignment errors and aligned notes\ncontained in the error regions.\nConditionsAlignment\nerrorsCorrectableg\nerrorsAligned\nnotes\nNone 98:7% 87:5% 18:5%\nnmne+nenp+npnm>0 88:5% 80:3% 9:2%\nTable 1 . Same as Fig. 4 with and without imposed condi-\ntions on the error regions ( \u0001 = 0:3s).\n3. REALIGNMENT\nHere we develop a realignment method based on merged-\noutput HMMs, which is applied to the error regions to cor-\nrect the preliminary alignment result. The overall proce-\ndure of realignment is illustrated in Fig. 5. We ﬁrst apply\nhand separation for the reference signal segment to esti-\nmate the voice structure and then carry out alignment based\non the merged-output HMM using the estimated voices.\n3.1 Hand Separation\nTo formulate a method that does not require prior voice\ninformation, we apply voice separation to each reference\nsignal segment. Because voice asynchrony in piano per-\nformances usually appears between the left- and right-hand\nparts and a larger number of voices increases the computa-\ntional cost for realignment, we use a technique that sepa-\nrates a performance signal into two hand parts [17].\nV oice information is described with a binary variable\nsmfor each note min the reference signal segment. If\nsm=L(orR), them-th note is in the left-hand (or right-\nhand) part. Let us denote the pitches of the reference sig-\nnal segment by x=x1:M= (x1;:::;x M), where the\nnotes are ordered according to the onset score time. (Sim-\nilar notations appear throughout the paper.) To estimate\nthe sequence s=s1:Mfrom the input x, we construct a\nmerged-output HMM. The Markov model for each voice is\ndescribed with transition probabilities on pitches, denoted\nby\u001fL\nyy0and\u001fR\nyy0. Introducing pitch variables for the two\nvoices,yL\nmandyR\nmfor eachm, the latent state variable for\nthe merged-output HMM is given as Ym= (sm;yL\nm;yR\nm)\nand the transition and output probabilities are given as\nP(Ym=YjYm\u00001=Y0)\n=1\n2(\u000esL\u001fL\ny0LyL\u000ey0RyR+\u000esR\u001fR\ny0RyR\u000ey0LyL);(3)\nP(xmjYm=Y) =\u000esL\u000eyLxm+\u000esR\u000eyRxm; (4)\nwhere\u000eyy0denotes Kronecker’s delta. To complete the350 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017##\n#‹### ‹\n##\n#‹### ‹ &\n?\n&\n?&\n?\n&\n?3FGFSFODF\nTJHOBM\n\"MJHOFE\nTJHOBM.JTTJOH\u0001OPUF\n&YUSB\u0001OPUF### ‹ &\n?\t\u0012\n\u0001)BOE\u0001TFQBSBUJPO\t\u0013\n\u0001\"MJHONFOU\u0001CZ\u0001UIF\nɹ\u0001\u0001NFSHFE\u000ePVUQVU\u0001)..\n-FGU\u0001IBOE 3JHIU\u0001IBOE&SSPS\u0001SFHJPOFigure 5 . The realignment step consists of hand separation\nand alignment by the merged-output HMM.\nstochastic process, we should specify the initial probabil-\nity, which is given similarly as in Eq. (3) with initial pitch\nvalues denoted by yL\n0andyR\n0. We use as yL\n0andyR\n0the\nlowest and highest pitch in the reference signal segment.\nWe can estimate swith the maximal posterior probability\nusing an efﬁcient Viterbi algorithm [17].\n3.2 Realignment Based on Merged-Output HMM\nFor realignment we use the merged-output HMM proposed\npreviously [9]. Modiﬁcations to the model are introduced\nto reduce computational cost and an inference algorithm\nthat is more rigorous than the original one is derived.\nLet us ﬁrst brieﬂy review the temporal HMM [7] for\nmusic alignment. The aligned signal (segment) can be can\nbe described as a sequence (p;t), where p= (p1;:::;p N)\ndenotes the integral pitches and t= (t1;:::;t N)denotes\nthe onset times ( Nis the number of aligned notes). The\nreference signal (segment) is represented as a sequence\nof reference clusters indexed by i2f1;:::;Ig(Iis the\nnumber of reference clusters), and the corresponding onset\nscore time is denoted by \u001ci. Local tempos are denoted by\nv= (v1;:::;v N). The corresponding reference cluster of\nthen-th aligned note is denoted by in2f1;:::;Ig. The\nlatent state of the temporal HMM is indexed by (in;vn)\nfor eachn2f1;:::;Ngand the output symbol is the pair\n(pn;tn). Transition and output probabilities are given as\nP(in;vnjin\u00001;vn\u00001) =\u0019(in\u00001;in)N(vn;vn\u00001;\u001b2\nv);\nP(pnjin) =\u001e(in;pn); (5)\nP(tnjtn\u00001;in\u00001=i0;in=i;vn)\n= (1\u0000\u000eii0)N(tn;tn\u00001+vn(\u001ci\u0000\u001ci0);\u001b2\nt)\n+\u000eii0Exp(tn\u0000tn\u00001;\u0015); (6)\nwhere we have assumed the statistical independence for the\npairsinandvn, andpnandtn. The probability \u0019stochas-\ntically describes how the performance proceeds in the ref-\nerence signal. The standard deviation \u001bvrepresents the\namount of tempo variation during the performance. The\npitch output probability \u001estochastically describes pitch\nerrors (similarly as  pitchin Sec. 2.2); it depends on the\npitch context of reference cluster i. The form of the output\nprobability for onset times reﬂects the fact that inter-onset\nintervals between chordal notes obey an exponential distri-\nbution (denoted by Exp) and those between onset clusters\nare approximately given as the product of the local tempo\nand the score time interval [7]. The scale parameter \u0015and\nthe standard deviations \u001btand\u001bvhave been measured [7].We can now construct the merged-output HMM for mu-\nsic alignment using voice information, by describing each\nvoice by the temporal HMM and merging outputs from the\ntwo HMMs [9]. To reduce computational cost, we intro-\nduce two simpliﬁcations to the model. First, since the error\nregion is considered to span a small time range (less than a\nfew seconds), the variation of tempos should be relatively\nsmall. We therefore assume a constant tempo vin each\nerror region, which can be obtained from the preliminary\nalignment result. This removes the dynamics of tempos\nand reduces the state space of the temporal HMM to that\nindexed only by in. Second, again because of the locality\nof error regions, we can assume LR transition probabilities\nfor\u0019. This reduces the number of possible state transition\npaths and thus reduces the computational cost. With these\nsimpliﬁcations, the state space of the merged-output HMM\nis indexed by k= (s;iL;iR;tL;tR)(s2fL;Rg) and the\ntransition and output probabilities are\nP(kn=kjkn\u00001=k0)\n=1\n2As(is;tsji0s;t0s;v)\n\u0001\u0002\n\u000esL\u000ei0RiR\u000e(t0R\u0000tR) +\u000esR\u000ei0LiL\u000e(t0L\u0000tL)\u0003\n;\nAs(is;tsji0s;t0s;v) =\u0019(i0s;is)P(t0sjts;i0s;is;v);(7)\nP(pnjkn=k) =\u001e(is;pn); (8)\nP(tnjkn=k) =\u000e(tn\u0000ts); (9)\nwhere\u000e(\u0001)is the Dirac delta function. Notating k=k1:N,\nthe complete-data probability is given as\nP(k;p;t) =NY\nn=1P(knjkn\u00001)P(pnjkn)P(tnjkn):(10)\nThe alignment result is obtained by inferring kthat\nmaximisesP(k;p;t). The direct application of the Viterbi\nalgorithm is impossible since the temporal HMM is of\nautoregressive type, i.e. the output probability of onset\ntimes depends on past values. Instead of the rough sub-\noptimisation method used in Ref. [9], we use a trick of in-\ntroducing an auxiliary variable that encodes the historical\npath, as in Ref. [20], which enables almost exact optimi-\nsation. Introduce hn= 1;2;\u0001\u0001\u0001, which is deﬁned as the\nsmallesth\u00151satisfyingsn6=sn\u0000hfor eachn. We have\nhn=(\nhn\u00001+1; sn=sn\u00001;\n1; s n6=sn\u00001;ts\nn=(\ntn; s =sn;\ntn\u0000hn; s6=sn:\nWith a change of variables ( h=h1:N,iL=iL\n1:N, etc.),\nP(k;p;t) =P(s;h;iL;iR;p;t)\n=Y\nn\u001a\n1\n2\u0002\n\u000esnL\u000ei0RiR+\u000esnR\u000ei0LiL\u0003\n\u0001h\n\u000esnsn\u00001\u000ehn(hn\u00001+1)Asame\nn+ (1\u0000\u000esnsn\u00001)\u000ehn1Adi\u000b\nni\u001b\n;\nAsame\nn=Asn(isn\nn;tnjisn\nn\u00001;tn\u00001;v);\nAdi\u000b\nn=Asn(isn\nn;tnjisn\nn\u00001;t~n;v) (11)\nwhere ~n=n\u0000hn\u00001\u00001. It is now possible to derive the\nViterbi algorithm for the state space of (s;h;iL;iR). AProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 351Algorithm GM data CJL data Our data\nProposed 0:18\u00060:08 0:79\u00060:06 0:48\u00060:03\nNOSW+ 1:46\u00060:23 1:81\u00060:08 0:64\u00060:04\nNOSW [7] 1:78\u00060:25 2:33\u00060:10 2:24\u00060:07\nGM [5] 0:28\u00060:10[7]8:07y\u00060:18[6] N/A\nCJL1 [6] N/A 1:49y\u00060:08[6] N/A\nCJL2 [6] N/A 2:20y\u00060:09[6] N/A\nTable 2 . Alignment error rates (%) with 1\u001bstatistical er-\nrors. The best values within 1\u001bsigniﬁcance are displayed\nin bold font. Daggers indicate lower bounds (see Sec. 1.1).\ncutoff (\u001850) on the maximum value of hncan be put to\nreduce the search space with little loss of optimality [20].\nFinally, the performance error detection described in Sec. 2\nis performed separately on each voice (hand part).\nDuring testing the method, we noticed that alignment\nerrors as simple as a pair of missing and extra notes as in\nFig. 1 sometimes remain after applying the described re-\nalignment step. This is often because the result of hand\nseparation is not completely correct. To handle this, we\ncarried out a simple processing step (called pairing step )\nof matching trivially corresponding missing and extra note\npairs. For each missing note, an extra note with the same\npitch is searched within the time region of half width \u0001,\nand if found, they are matched. The pairing step can also\nbe applied before the realignment step to correct trivial\nalignment errors and thus reduce the cost of realignment.\n4. EVALUATION\nAs explained in Sec. 1.1, the state-of-the-art methods are\nthe NOSW algorithm [7], the GM algorithm [5], and the\nCJL algorithms (CJL1 and CJL2) [6]. For comparison, we\nrun the performance error detection on the results of the\nNOSW algorithm ( NOSW+ algorithm ), and the proposed\nrealignment was applied to its results. Since the GM and\nCJL algorithms were not available from their authors but\nthe used data were provided, we run the proposed method\nand temporal HMM on their data and directly compared\nthe accuracies. The GM data consisted of seven perfor-\nmances of two excerpts of Chopin’s piano pieces (total of\n2,815 aligned notes). The CJL data consisted of 21 pairs\nof piano MIDI ﬁles (total of 25,656 aligned notes), most\nof which are synthetic (not human-played) performances.\nWe also tested the proposed method on the human-played\nperformance data that we prepared. Our data consisted of\n60 excerpts of classical piano pieces each played by three\ndifferent pianists (total of 43,608 aligned notes)3. For the\nGM data and our data it was score-to-MIDI alignment and\nfor the CJL data it was MIDI-to-MIDI alignment. For the\nproposed method, \u0001 = 0:3s was used, error regions sat-\nisfying the condition ( nmne+nenp+npnm>0) were\nselected for realignment, and the pairing step was applied\nbefore and after the realignment step.\nThe rates of alignment errors in Table 2 show that for\nall data the realignment method signiﬁcantly improved the\npreliminary alignment results: in total 47% (= 369 aligned\n3The data could be provided upon requests to the authors.GM data CJL data Our data Time (s)\n(a) 0:18 0:79 0:48 5:54\u00060:07\n(b) 0:25 1:17 0:51 6:31\u00060:07\n(c) 0:14 0:85 0:61 6:32\u00060:05\nTable 3 . Alignment error rates (%) and processing time\n(averaged over ﬁve trials) for the proposed method with (a)\nboth paring steps and conditions on error regions, (b) only\nconditions on error regions, and (c) only pairing steps.\nnotes) of alignment errors made by the NOSW+ algorithm\nwere reduced. The proposed method had the highest ac-\ncuracies for all datasets. To evaluate computational ef-\nﬁciency, the processing time was measured. Our algo-\nrithms were implemented in C++ on a computer with 3.1\nGHz CPU and 16 GB memory running Mac OS X 10.11.\nThe measured time for the CJL data was 8:25s for the\nNOSW algorithm, 17:76s for the performance error de-\ntection, and 1:12s for the realignment. Compared to the\nreported values [6], 342:70s and 3535:36s for the CJL1\nand GM algorithm, the computational efﬁciency of the pro-\nposed method is evident, although direct comparison is not\npossible because of different computer environments. Ex-\namples demonstrating the effect of the realignment method\nare shown in the accompanying web page [18].\nTo examine the effect of the paring step and the condi-\ntions imposed on error regions, the proposed method with-\nout these modiﬁcations was compared in terms of accura-\ncies and processing time for all data (Table 3). In addition\nto the expected reduction of computation time, these mod-\niﬁcations were also effective in reducing overall alignment\nerrors. This suggests that the realignment by the merged-\noutput HMM increases alignment errors in some error re-\ngions and these modiﬁcations have effects in avoiding this.\nDetailed analyses are currently being undertaken.\n5. CONCLUSION\nWe have described a realignment method for symbolic mu-\nsic signals based on merged-output HMMs, which can deal\nwith reordered notes due to voice asynchrony. To reduce\nthe high computational cost, performance errors are de-\ntected and the merged-output HMMs are applied to regions\naround the performance errors rather than to the whole\nsignal. In all tested data and for both score-to-MIDI and\nMIDI-to-MIDI alignment cases, the proposed realignment\nmethod combined with an HMM-based method achieved\nthe highest accuracies, with short computation time.\nThe principle of using performance errors to select re-\ngions in the aligned signals that possibly contain alignment\nerrors is generally applicable to save computation time.\nFor example, when a further reﬁned alignment method is\nfound in the future, we can apply it to the error regions\nof the results by the proposed method, instead of doing\nalignment from scratch. In addition, since the realignment\ncan be done locally, it can be applied to performance sig-\nnals with global repeats and skips [7]. For future work,\nreﬁnements for the model for performance error detection\nby examining human-annotated data would be possible to\nfurther improve the accuracy and efﬁciency.352 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. ACKNOWLEDGEMENTS\nWe are grateful to Bruno Gingras and Chun-Ta Chen for\nproviding evaluation data. E. Nakamura is supported by\nthe JSPS Postdoctoral Research Fellowship and this stud-\nied was done when he was a visiting researcher at the\nQueen Mary University of London. This work is in part\nsupported by JSPS KAKENHI Nos. 24220006, 26280089,\n26700020, 15K16054, 16H01744, 16H02917, 16K00501,\nand 16J05486 and JST ACCEL No. JPMJAC1602.\n7. REFERENCES\n[1] R. Dannenberg, “An On-Line Algorithm for Real-Time\nAccompaniment,” Proc. ICMC , pp. 193–198, 1984.\n[2] J. Bloch and R. Dannenberg, “Real-Time Computer\nAccompaniment of Keyboard Performances,” Proc.\nICMC , pp. 279–290, 1985.\n[3] P. Desain, H. Honing, and H. Heijink, “Robust Score-\nPerformance Matching: Taking Advantage of Struc-\ntural Information,” Proc. ICMC , pp. 337–340, 1997.\n[4] H. Heijink, L. Windsor, and P. Desain, “Data Process-\ning in Music Performance Research: Using Structural\nInformation to Improve Score-Performance Match-\ning,” Behavior Research Methods, Instruments, &\nComputers , vol. 32, no. 4, pp. 546–554, 2000.\n[5] B. Gingras and S. McAdams, “Improved Score-\nPerformance Matching Using Both Structural and\nTemporal Information from MIDI Recordings,” J. New\nMusic Res. , vol. 40, no. 1, pp. 43–57, 2011.\n[6] C. Chen, J. R. Jang, and W. Liou, “Improved Score-\nPerformance Alignment Algorithms on Polyphonic\nMusic,” Proc. ICASSP , pp. 1365–1369, 2014.\n[7] E. Nakamura, N. Ono, S. Sagayama, and K. Watanabe,\n“A Stochastic Temporal Model of Polyphonic MIDI\nPerformance with Ornaments,” J. New Music Res. ,\nvol. 44, no. 4, pp. 287–304, 2015.\n[8] B. Pardo and W. Birmingham, “Modeling Form for On-\nline Following of Musical Performances,” Proc. NCAI ,\n2005.\n[9] E. Nakamura, Y . Saito, N. Ono, and S. Sagayama,\n“Merged-Output Hidden Markov Model for Score Fol-\nlowing of MIDI Performance with Ornaments, Desyn-\nchronized V oices, Repeats and Skips,” Proc. Joint\nICMCjSMC 2014 , pp. 1185–1192, 2014.\n[10] C. Raphael, “Automatic Segmentation of Acoustic Mu-\nsical Signals Using Hidden Markov Models,” IEEE\nTPAMI , vol. 21, no. 4, pp. 360–370, 1999.\n[11] A. Cont, “A Coupled Duration-Focused Architec-\nture for Realtime Music to Score Alignment,” IEEE\nTPAMI , vol. 2, no. 6, pp. 974–987, 2010.\n[12] C. Fremerey, M. M ¨uller, and M. Clausen, “Handling\nRepeats and Jumps in Score-Performance Synchro-\nnization,” Proc. ISMIR , pp. 243–248, 2010.\n[13] C. Joder, S. Essid, and G. Richard, “A Conditional\nRandom Field Framework for Robust and Scalable\nAudio-to-Score Matching, IEEE TASLP , vol. 19, no. 8,\npp. 2385–2397, 2011.[14] M. Grachten, M. Gasser, A. Arzt, and G. Widmer, “Au-\ntomatic Alignment of Music Performances with Struc-\ntural Differences,” Proc. ISMIR , pp. 607–612, 2013.\n[15] A. Maezawa, K. Itoyama, K. Yoshii, and H. G. Okuno,\n“Bayesian Audio Alignment Based on a Uniﬁed Model\nof Music Composition and Performance,” Proc. IS-\nMIR, pp. 233–238, 2014.\n[16] W. Siying, S. Ewert, and S. Dixon, “Robust and Ef-\nﬁcient Joint Alignment of Multiple Musical Perfor-\nmances,” IEEE/ACM TASLP , vol. 24, no. 11, pp. 2132–\n2145, 2016.\n[17] E. Nakamura, N. Ono, and S. Sagayama, “Merged-\nOutput HMM for Piano Fingering of Both Hands,”\nProc. ISMIR , pp. 531–536, 2014.\n[18] E. Nakamura, K. Yoshii, and H. Katayose,\nSymbolic Music Alignment Tool , 2017.\nhttp://anonym9382.github.io/demo.html\n[Online]\n[19] E. Nakamura, T. Nakamura, Y . Saito, N. Ono, and\nS. Sagayama, “Outer-Product Hidden Markov Model\nand Polyphonic MIDI Score Following,” J. New Music\nRes., vol. 43, no. 2, pp. 183–201, 2014.\n[20] E. Nakamura, K. Yoshii, and S. Sagayama, “Rhythm\nTranscription of Polyphonic Piano Music Based\non Merged-Output HMM for Multiple V oices,”\nIEEE/ACM TASLP , vol. 25, no. 4, pp. 794–806, 2017.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 353"
    },
    {
        "title": "Acoustic Features for Determining Goodness of Tabla Strokes.",
        "author": [
            "Krish Narang",
            "Preeti Rao"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416274",
        "url": "https://doi.org/10.5281/zenodo.1416274",
        "ee": "https://zenodo.org/records/1416274/files/NarangR17.pdf",
        "abstract": "The tabla is an essential component of the Hindustani clas- sical music ensemble and therefore a popular choice with musical instrument learners. Early lessons typically tar- get the mastering of individual strokes from the inven- tory of bols (spoken syllables corresponding to the distinct strokes) via training in the required articulatory gestures on the right and left drums. Exploiting the close links between the articulation, acoustics and perception of tabla strokes, this paper presents a study of the different timbral quali- ties that correspond to the correct articulation and to iden- tified common misarticulations of the different bols. We present a dataset created out of correctly articulated and distinct categories of misarticulated strokes, all perceptu- ally verified by an expert. We obtain a system that auto- matically labels a recording as a good or bad sound, and additionally identifies the precise nature of the misarticu- lation with a view to providing corrective feedback to the player. We find that acoustic features that are sensitive to the relatively small deviations from the good sound due to poorly articulated strokes are not necessarily the features that have proved successful in the recognition of strokes corresponding to distinct tabla bols as required for music transcription.",
        "zenodo_id": 1416274,
        "dblp_key": "conf/ismir/NarangR17",
        "keywords": [
            "Tabla",
            "Hindustani classical music",
            "musical instrument learners",
            "articulation",
            "acoustics",
            "perception",
            "dataset",
            "correctly articulated",
            "misarticulated strokes",
            "acoustic features"
        ],
        "content": "ACOUSTIC FEATURES FOR DETERMINING GOODNESS OF TABLA\nSTROKES\nKrish Narang Preeti Rao\nDepartment of Electrical Engineering,\nIndian Institute of Technology Bombay, Mumbai, India.\nkrishn@google.com, prao@ee.iitb.ac.in\nABSTRACT\nThe tabla is an essential component of the Hindustani clas-\nsical music ensemble and therefore a popular choice with\nmusical instrument learners. Early lessons typically tar-\nget the mastering of individual strokes from the inven-\ntory of bols (spoken syllables corresponding to the distinct\nstrokes) via training in the required articulatory gestures on\nthe right and left drums. Exploiting the close links between\nthe articulation, acoustics and perception of tabla strokes,\nthis paper presents a study of the different timbral quali-\nties that correspond to the correct articulation and to iden-\ntiﬁed common misarticulations of the different bols. We\npresent a dataset created out of correctly articulated and\ndistinct categories of misarticulated strokes, all perceptu-\nally veriﬁed by an expert. We obtain a system that auto-\nmatically labels a recording as a good or bad sound, and\nadditionally identiﬁes the precise nature of the misarticu-\nlation with a view to providing corrective feedback to the\nplayer. We ﬁnd that acoustic features that are sensitive to\nthe relatively small deviations from the good sound due to\npoorly articulated strokes are not necessarily the features\nthat have proved successful in the recognition of strokes\ncorresponding to distinct tabla bols as required for music\ntranscription.\n1. INTRODUCTION\nTraditionally the art of playing the tabla (Indian hand\ndrums) has been passed down by word of mouth, and docu-\nmentation of the same is rare. Moreover, recent years have\nseen a decline in the popularity of Indian classical mu-\nsic, possibly due to the relatively limited accessibility op-\ntions in todays digital age. While tuners are commonly uti-\nlized with melodic instruments, a digital tool that assesses\nthe timbre of the produced sound can prove invaluable for\nlearners and players of percussion instruments such as the\ntabla, in avoiding deep-seated deﬁciencies that arise from\nerroneous practice.\nBased on the fact that there is an overall consensus\nc\rKrish Narang and Preeti Rao. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Krish Narang and Preeti Rao. “Acoustic Features for Determining\nGoodness of Tabla Strokes”, 18th International Society for Music Infor-\nmation Retrieval Conference, Suzhou, China, 2017.\nBayan Dayan\n(Tabla) rim\n(kinar)\npatch\n(siyahi)\nhead\n(maidan)Figure 1 . Regions of the left (bayan) and right (dayan)\ntabla surfaces, Patel and Iversen [1].\namong experts when it comes to the quality of sound (in\nterms of intonation, dynamics and tone quality) produced\nby an instrumentalist [2], Picas et al. [3] proposed an au-\ntomatic system for measuring perceptual goodness in in-\nstrumental sounds, which was later developed into a com-\nmunity driven framework called good-sounds.org [4]. The\nwebsite worked with a host of string and wind instruments,\nwhose goodness broadly depended on similar acoustic at-\ntributes. We follow the motivation of good-sounds, extend-\ning it to a percussive instrument, the tabla, which has a so-\nphisticated palette of basic sounds, each characterized by a\ndistinct vocalized syllable known as a “bol”. Further, in the\ninterest of creating a system that provides meaningful feed-\nback to a learner, we explicitly take into account the link\nbetween the manner of playing, or articulatory aspects, and\nthe corresponding acoustic attributes.\nThe tabla consists of two sealed membranophones with\nleather heads: the smaller, wooden-shell “dayan’ (treble\ndrum) is played with the right hand, and the larger, metal-\nshell “bayan’ (bass drum) is played with the left. Each\ndrum surface is divided into regions as shown in Figure 1.\nUnlike typical percussion instruments that are played with\nsticks or mallets hit at the ﬁxed place on the drum surface,\na tabla stroke is speciﬁed by the precise hand gesture to be\nemployed (we term this the “manner of articulation”, bor-\nrowing on terminology from speech production) and the\nparticular region of the drum surface to be struck (“place\nof articulation”). Previous work has addressed the recog-\nnition of tabla bols for transcription via the distinct acous-257Figure 2 . Articulation based classiﬁcation of tabla bols.\ntic characteristics associated with each of the strokes [5,6].\nTemporal and spectral features commonly applied to musi-\ncal instrument identiﬁcation were used to achieve the clas-\nsiﬁcation of segmented strokes corresponding to different\nbols. Gillet and Richard [5] performed classiﬁcation of\nindividual bols by ﬁtting Gaussian distributions to the en-\nergies in each of four different frequency bands. Chor-\ndia [6] used descriptors comprised of generic temporal as\nwell as spectral features commonly used in the ﬁeld of Mu-\nsic Information Retrieval for bol classiﬁcation. More re-\ncently, Gupta et al. [15] used traditional spectral features,\nthe mel-frequency cepstral coefﬁcients, for the transcrip-\ntion of strokes in a ryhythm pattern extraction task on audio\nrecordings. While the recognition of well-played strokes\ncan beneﬁt from the contrasting sounds corresponding to\nthe different bols, the difference between a well-played and\nbadly-played version of a bol is likely to be more nuanced\nand require developing bol-speciﬁc acoustic features. In\nfact, Herrera et al. [8] use spectral features for percussion\nclassiﬁcation based on a taxonomy of shape/material of the\nbeaten object, speciﬁcally omitting instruments that drasti-\ncally change timbre depending on how they are struck.\nIn this work, we consider the stroke classiﬁcation prob-\nlem where we wish to distinguish improperly articulated\nstrokes from correct strokes by the analysis of the audio\nrecording, and further provide feedback on the nature of\nthe misarticulation. Based on a training dataset, that con-\nsists of strokes representing various kinds of playing errors\ntypical of learners, as simulated by tabla teachers, we carry\nout a study of acoustic characteristics in relation to artic-\nulation aspects for each stroke. This is used to propose\nacoustic features that are sensitive to the articulation er-\nrors. Traditional features used in tabla bol recognition are\nused as baseline features and eventually we develop and\nevaluate a stroke classiﬁcation system based on the combi-\nnation of proposed and baseline features in a random forest\nclassiﬁer.Type Bol Label Position Manner Pressure\nResonant\nLeftGe Good Maidan Bounce Variable\nBad1 Siyahi Bounce Medium\nBad2 Maidan Press Medium\nBad3 Kinar Bounce Medium\nDamped\nLeftKe Good Siyahi Press Medium\nBad1 Maidan Press Medium\nBad2 Siyahi Bounce Light\nResonant\nRightTa/ Good Kinar Press Medium\nNa Bad1 Kinar(e) Press Heavy\nBad2 Maidan Press Medium\nBad3 Kinar Press Heavy\nTun Good Siyahi Bounce None\nBad1 Siyahi Press Light\nBad2 Maidan Bounce None\nTin Good Maidan Bounce Light\nBad1 Siyahi Bounce Light\nBad2 Maidan Bounce Heavy\nDamped\nRightTi/ Good Siyahi Press Medium\nRa Bad1 Siyahi Bounce Light\nBad2 Siyahi(e) Press Medium\nTak Good Maidan Press Medium\nBad1 Maidan Bounce Light\nBad2 Kinar(e) Press Medium\nBad3 Siyahi(e) Press Medium\nTable 1 . Common articulations of bols in terms of position\nof articulation, manner of articulation, and hand pressure.\n2. ARTICULATION BASED CLASSIFICATION\nThe tabla is a set of two drums, the left bass drum (bayan)\nand the right, higher pitched drum (dayan). Each tabla\ndrum surface is composed of three major regions- siyahi,\nmaidan, and kinar as depicted in Figure 1. Each tabla\nstroke (bol) is characterized by a very speciﬁc combina-\ntion of the hand orientation with respect to the the posi-\ntion on drum surface, manner of striking, and pressure ap-\nplied to the drum head, and has a very distinctive sound.\nDue to the heavy dependence of perceived quality of tabla\nbols on articulation accuracy of the player, it is instruc-\ntive to understand the articulatory conﬁgurations of bols\nvia the taxonomy visualized in Figure 2. Mixed bols are\nbols where both tablas are struck simultaneously (e.g. Dha,\nDhin Dhit). Verbose bols (e.g. TiNaKeNa) consist of a\nsequence of strokes played in quick succession, whereas\natomic bols are single stroke bols. A resonant bol is one\nwhere the skin of the drum is allowed to freely vibrate af-\nter it is struck, and a damped bol is one where the skin is\nmuted in some way after it is struck.\nBols of each type (leaf nodes of Figure 2) can further\nbe classiﬁed based on the place of articulation, manner of\narticulation and amount of hand pressure applied on the\nskin of the tabla. For example, for the bol tun, the index\nﬁnger strikes the siyahi of the right tabla (dayan), with no\ndamping (hand does not touch the tabla, ﬁnger is lifted af-\nter striking) (Patel and Iversen [1]). These are the three\nmajor attributes that distinguish bols within a type, and258 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017are also what decide the perceptual goodness of a tabla\nstroke. For the same hand orientation, the drum can be\nstruck sharply followed by immediately lifting the ﬁnger\n(we call this the ‘bounce’ manner of articulation) or it can\nbe struck followed by leaving the ﬁnger or palm pressed\nagainst the drum head (we call this the ‘pressed’ manner of\narticulation). For the rest of the study we focus on atomic\nbols, which are sufﬁcient for coverage of all beginner tabla\nrhythms, as listed on raganet, an educational magazine on\nIndian music [7]. For simplicity, mixed bols are not cov-\nered, since they are combinations of simultaneous left and\nright tabla strokes.\nTwo tabla teachers were consulted on the common mis-\ntakes made by beginners while playing a particular bol.\nBased on these, multiple classes were deﬁned for each bol\nusing the aforementioned three attributes governing good-\nness of a bol. One of these classes represents the well-\nplayed version of that bol, whereas the others represent\nthe most common deviations that are perceptually distinct\nfrom the expected good sound. These are listed for all\nbols in Table 1, which explicitly shows the position, man-\nner and hand pressure for different articulations of each\nbol, where “(e)” refers to the edge of the speciﬁed region.\nFor example, a resonant right bol played on the maidan,\nwhile applying light hand pressure and lifting the ﬁnger af-\nter striking, constitutes a well-played Tin bol. However the\nsame played while applying medium to heavy hand pres-\nsure is a badly-played Tin bol.\n3. DATABASE AND ACOUSTIC\nCHARACTERISTICS\nA dataset composed of 626 isolated strokes of 7 different\nbols was recorded (sampling rate of 44:1kHz) by two ex-\nperienced tabla players on a ﬁxed tabla set that was tuned\nto D4 (294 Hz). The players were asked to play several\ninstances of each stroke while also simulating typical er-\nrors that a new learner is likely to make in realizing a\ngiven stroke. Thus our dataset consists of recordings of\neach of bols realized in different ways as listed in Table 1,\nwhich also provides an articulation based description of\nthe different realizations as executed by the tabla players.\nAll the recordings were perceptually validated by one of\nthe players who listened to each stroke and labeled it as\n”good” or ”bad”. In order to develop a system that provides\nspeciﬁc feedback on the quality of a stroke, we required\nbadly played instances of the bols as well. This made\nit impossible to use a publicly available dataset, as most\narchived recordings are from professional performances.\nAlso, since our dataset is generated with reference to con-\ntrolled variations in articulation as typical of a learner, it\nis likely to be more complete than the randomly sampled\nacoustic space of all possible productions.\nA number of recordings was made per bol as seen in the\nCount column of Table 3, but with a roughly equal distribu-\ntion of strokes across the classes corresponding to each bol\nin order to facilitate the construction of balanced training\nand test datasets for the classiﬁcation task. The only excep-\ntion to this is the bol Ge where a relatively large numberof instances of the good stroke were produced since it is\nthe only bol with pitch that can be modulated by changing\nthe amount of pressure applied on the drum surface while\nstriking. A number of such hand pressure based variations\nwere recorded for the correct articulatory settings of the Ge\nstroke in order to get a reasonably representative dataset\nfor the good quality bol Ge (124 out of the total of 187 Ge\nstrokes in Table 3). This was important to ensure that the\nclassiﬁer we build is robust to pitch variations and other ir-\nrelevant changes caused by an increase or decrease in hand\npressure.\nSince each stroke presented in Table 1 is characterized\nby speciﬁc articulation (in terms of place of articulation,\nmanner of articulation and amount of hand pressure), the\nacoustic variability is likely to cover more than one di-\nmension. By studying the short-time magitude spectra (i.e.\nspectrograms) of the recorded bols, we were able to isolate\nthe acoustic characteristics that distinguished the various\nclasses of each bol. Time-domain waveforms and short-\ntime magnitude spectra for two bols, Tin (a resonant right\nbol) and Ke (a damped left bol) are shown in Figure 3 and\nFigure 4 respectively. We observe that the rate of decay of\nthe time-domain waveforms clearly discriminate the good\nfrom bad strokes. Further, the saliency as well as rate of\ndecay of the individual harmonics (horizontal dark bands\nin the spectrograms) are seen to differ between the differ-\nently realised versions of each of the strokes. The resonant\nbol Tin is characterised by strong sustained harmonic com-\nponents for good quality. In contrast, the damped bol Ke\nhas a diffuse spectrum and rapidly decaying temporal en-\nvelope when realised correctly in Figure 4 top. A bounce\nin the hand gesture, on the other hand, degrades the stroke\nquality, contributing the prominent harmonics seen in the\nlow frequency region of the bottom most bad stroke in\nFigure 4.\n4. DEVISING FEATURES\nFrom acoustic observations similar to those outlined in the\nprevious section, across bols and goodness classes, we hy-\npothesize that the strength, concentration and sustain of\nparticular harmonics is critical to the quality of realization\nof a bol, especially for the resonant bols. Based on this, we\npropose and evaluate a harmonics based feature set which\nwe call Feature set A. The features are designed to capture\nper-harmonic strength, concentration and decay rates. Har-\nmonic based features are computed for each of the ﬁrst 15\nharmonics by extracting the corresponding spectral region\nby passing the signal through a narrow bandpass ﬁlter cen-\ntered around that harmonic. These are important for res-\nonant bols. The energy, spectral variance, and decay rate\nof each of the bandpass-ﬁltered signals are computed. The\ndecay rate is obtained as a parameter corresponding to an\nexponential envelope ﬁtted to the signal. The energy and\nvariance together constitute the strength of the harmonic,\nwhereas decay rate represents how quickly that particu-\nlar harmonic dies out. Spectral shaping features include\nvariance, skewness, kurtosis and high frequency content.\nThese features are extracted using Essentia [9], an open-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 259Figure 3 . Waveform (left) and spectrogram (right) for\ngood and selected bad recordings of Tin. Bad 1is played\nin the wrong position, on the siyahi. Bad 2is played with\nexcess hand pressure.\nFigure 4 . Waveform (left) and spectrogram (right) for\ngood and selected bad recordings of Ke. Bad 1is played\nin the wrong position, on the maidan. Bad 2is played\nloosely- by bouncing the palm instead of pressing it.\nFigure 5 . Exponential envelope ﬁtted to rectiﬁed wave-\nform for a Ge stroke. Dots mark the retained samples for\ncurve ﬁtting.\nsource library for audio analysis and audio-based music\ninformation retrieval. The temporal features include the\nenergy and decay rate of the signal, and are useful for de-\ntermining goodness of both damped and resonant bols. We\nalso evaluate a baseline feature set (termed Feature Set B)\nwhich is essentially the same as the features employed by\nChordia [6] in a tabla bol recognition task.\n4.1 Harmonic Based Features\nFor each resonant bol that is correctly rendered, clear har-\nmonics are visible in the spectrogram at multiples of a fun-\ndamental frequency. For resonant bols on the right tabla,\nthe fundamental frequency is equal to the tonic of the tabla,\nexcept for Tun, for which the fundamental frequency is two\nsemitones higher than the tonic [10]. However, these are\nnot always precise, and a pitch detection algorithm should\nbe used for determining the fundamental frequency of the\nrecorded bol, e.g. the YinFFT algorithm [11]. For our\ndataset, the fundamental frequencies were manually esti-\nmated by viewing the spectrogram. For the tabla set used\nin our experiments, the tonic was determined to be 294 Hz,\nand fundamental frequency for Tun to be 330 Hz. For the\nleft tabla stroke Ge the fundamental frequency was esti-\nmated to be 125 Hz.\nFor extracting harmonic based features, the signal is\nﬁrst passed through ﬁfteen second-order IIR band pass\nﬁlters with a bandwidths of 100 Hz and center frequen-\ncies at multiples of the fundamental frequency for that bol.\nThen an exponential envelope is ﬁtted to the resulting time\ndomain waveform. The waveform is full-wave rectiﬁed\n(A0(t) = jA(t)j), and only the maximum amplitude sam-\nple in every 50millisecond interval is retained (as marked\nin Figure 5). The onset sample of the signal (assumed to\nbe maximum amplitude sample over all time) is kept at\nt= 0. Next, SciPy’s curve ﬁt function [12] is used to ﬁt\nan exponential ( ae\u0000bt) to the obtained samples, and both\nparameters aandbare considered as features. arepre-\nsents the estimated maximum amplitude (referred in our260 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Bol Selected Features\nGeEnergy(overall, 250, 500, 750, 1000, 1125,\n1625), Decay(overall, 125, 250, 375, 625, 875),\nImpulse(125), Variance(125, 1500), MFCC(5,\n6, 8, 10), Attack Time, Temporal Centroid,\nZCR, Spectral Centroid\nKeEnergy(overall, 1764, 2352, 3528, 4116),\nDecay(overall, 294, 588, 2646, 3822),\nImpulse(294, 588, 882, 2352, 3234), MFCC(0,\n1, 7, 12), Attack Time\nTa/NaEnergy(overall, 294, 1176, 1470), Decay(882),\nImpulse(2058), Variance(882, 1470, 2352),\nMFCC(1), Temporal Centroid\nTakEnergy(588, 882, 1176, 1470, 2646, 4116),\nDecay(294), Impulse(588), Variance(294),\nMFCC(1, 3), Attack Time, Temporal Centroid\nTi/RaEnergy(588, 1764), Decay(588, 1176),\nImpulse(588), Variance(588), MFCC(11, 12),\nAttack Time, Temporal Centroid, ZCR\nTinEnergy(294, 2352, 3822), Decay(overall, 294,\n588, 1470), Impulse(1764), Variance(294,\n588), MFCC(2), Temporal Centroid\nTunEnergy(4950), Decay(330, 2310, 3960),\nImpulse(overall), Spectral Centroid, Temporal\nCentroid, ZCR\nTable 2 . Features selected from combination of set A and\nset B. The numbers in the bracket indicate the harmonic\nfrequencies selected for energy/decay/impulse/variance\nand the indexes of selected coefﬁcients (0-12) for MFCC.\nfeature set as ‘impulse’) of the signal and brepresents the\nestimated decay rate (inversely proportional to the decay\ntime). A similar curve ﬁtting is done to the unﬁltered time\ndomain waveform. From the spectrum of the unﬁltered sig-\nnal, we calculate the energy and variance of the spectrum\nin bands centered around the ﬁrst 15 harmonics with band-\nwidth equal to fundamental frequency. Finally the total en-\nergy of the signal is also taken as a feature. Finding energy\nand variance in a particular frequency range and band pass\nﬁltering were both done using routines from Essentia [9].\nA total of 63 features were extracted in this way.\n4.2 Baseline Feature Set\nThe baseline feature set consists of commonly used tem-\nporal and spectral features along with 13 MFCC’s. These\nwere used by Chordia [5] for tabla bol classiﬁcation, and\ntheir relevance and effectiveness is also described in detail\nby Brent [13]. The temporal features are zero crossing rate,\ntemporal centroid (the centroid of the time domain signal)\nand attack time. The attack time is calculated as time taken\nfor the signal envelope to go from 20% to 90% of its max-\nimum amplitude (default used in Essentia [9]). The spec-\ntral features are spectral centroid, skewness, and kurtosis.\nThese are all obtained from the magnitude spectrum com-\nputed over the full duration of the recorded stroke. All of\nthese features were computed using Essentia [9] routines.Bol Count Classes Set A Set BCombined\nSet\nGe 187 4 89.8 89.8 94.1\nKe 67 3 79.1 76.1 85.1\nTa/Na 86 4 89.5 86.1 91.9\nTak 101 4 80.2 82.2 86.1\nTi/Ra 79 3 77.2 96.2 91.1\nTin 48 3 89.6 93.8 97.9\nTun 29 3 81.0 91.4 93.1\nTable 3 . Percentage classiﬁcation accuracies (one good\nclass, multiple articulation based bad classes) Accuracies\nwith Harmonic Features (Set A), Baseline Features (Set B),\nand selected features from a combination of Set A and Set\nB (Combined Set).\n5. TRAINING AND EVALUATION OF BOL\nARTICULATION CLASSIFIERS\nGiven our set of features, engineered as presented in the\nprevious section, and the fact that our dataset is not very\nlarge, we employ a random forest classiﬁer for the stroke\nclassiﬁcation task. A random forest classiﬁer is an ensem-\nble approach based on decision trees. We test for k-way\nclassiﬁcation accuracy in 10 fold cross validation mode\nwith each of the different feature sets using the Weka [14]\ndata mining software. Here kis the number of classes for\na particular bol, consisting of one good class and multiple\narticulation based bad classes as shown in Table 3 where\nthe number of strokes in the dataset for each bol is pro-\nvided as well. For each instance the classiﬁer predicts\nwhether a bol is well-played (labeled good) or what mis-\narticulations were made while playing the bol (labeled as\nthe appropriate bad class). Apart from this, a subset of\nfeatures is selected from the union of the two feature sets,\nusing the CfsSubsetEval attribute selector with a GreedyS-\ntepwise search method from the Weka [14] data mining\nsoftware. The greedy search picks each succeeding fea-\nture based on the classiﬁcation improvement it brings to\nthe existing set, using a threshold on achieved accuray as\na stopping criterion. The set of selected features for each\nbol is shown in Table 2. Classiﬁcation accuracies obtained\nwith each of the 3 feature sets are presented in Table 3. We\nobserve that the combination of features performs better\nthan the baseline in nearly all cases. This indicates that\nthe new harmonics based features bring in some useful in-\nformation, complementary to the baseline features. In the\ncase of the bol Ti/Ra, there is a decrease in classiﬁcation\naccuracy with respect to the baseline. This is a damped bol\nand therefore harmonic features are not as important to it as\nspectral shaping features; however the issue of decreased\naccuracy after feature selection needs further investigation.\nFinally, Table 4 shows the results of two-way classiﬁcation\ninto good and bad strokes achieved by the combination fea-\ntures.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 261BolFeature\nDimensionAccuracy\nGe 25 96.3\nKe 20 95.5\nTa/Na 11 96.5\nTak 13 94.1\nTi/Ra 10 92.4\nTin 12 97.9\nTun 8 93.1\nTable 4 . Percentage classiﬁcation accuracies for two-way\nclassiﬁcation (good/bad stroke) based on features selected\nfrom the combined data set (as listed in Table 2).\n6. CONCLUSION\nUnlike many percussion instruments, the tabla is a musical\ninstrument with a diverse inventory of basic sounds that\ndemand extensive training and skill on the part of a player\nto elicit correctly. We proposed a taxonomy of strokes\nin terms of the main dimensions of articulation obtained\nthrough discussions with tabla teachers. This allowed us\nto construct a representative dataset of correct and com-\nmon incorrectly articulated strokes by systematically mod-\nifying the articulatory dimensions. The results of this study\nshow that nuanced changes in articulation are linked to\nperceptually signiﬁcant changes in the acoustics of a tabla\nstroke. We presented acoustic features extracted from the\nisolated stroke segments to detect the articulation accuracy\nand therefore the perceptual goodness of a stroke from its\naudio. The best choice of features was observed to depend\non the nature of the bol.\nThe present dataset was restricted to a single tabla set.\nFor future work we would like to continue this research\nusing a larger database from more sources, and to in-\nclude coverage of mixed bols. The latter would further re-\nquire measurements of relative timing between the atomic\nstrokes that make up the mixed bol. This study can also\neasily be extended to evaluate sequences of bols (talas) for\nbeginners- by a combination of rhythm scoring and evalu-\nation of segmented bols of the sequence individually. The\nconcept of expression and emotion in the playing of the\ntabla, which is vital to intermediate and expert players, is\nhowever a much more open ended question, and further\nresearch will hopefully lead to a characterization of that\nproblem as well.\n7. ACKNOWLEDGEMENT\nThe authors would like to thank Digant Patil and Abhisekh\nSankaran for their help in recording and evaluating the\nTabla strokes dataset. We are also indebted to Kaustuv\nKanti Ganguli for lending his expertise in Indian Classical\nmusic, to Hitesh Tulsiani for his help with feature selection\nalgorithms and classiﬁers, and to Shreya Arora for editing\nand rendering of graphs and images.8. REFERENCES\n[1] A. Patel and J. Iversen. “Acoustic and Perceptual Com-\nparison of Speech and Drum Sounds in the North In-\ndian Tabla Tradition: An Empirical Study of Sound\nSymbolism,” Journal of Research in Music Education,\nvol. 46 , pp. 522–534, 1998.\n[2] J. Geringe and C. Madsen. “Musicians ratings of good\nversus bad vocal and string performances,” Proc. of\nthe 15th international congress of phonetic sciences\n(ICPhS) , pp. 925–928, 2003.\n[3] O. Roman Picas, H. Parra Rodriguez, D. Dabiri, H.\nTokuda, W. Hariya, K. Oishi, and X. Serra. “A real-\ntime system for measuring sound goodness in instru-\nmental sounds,” Audio Engineering Society Conven-\ntion 138 , Audio Engineering Society, 2015.\n[4] G. Bandiera, O. Roman Picas, H. Tokuda, W. Hariya,\nK. Oishi, and X. Serra. “good-sounds. org: a Frame-\nwork To Explore Goodness in Instrumental Sounds,”\nProc. 17th International Society for Music Information\nRetrieval Conference , pp. 414–419, 2016.\n[5] O. Gillet, and G. Richard. “Automatic labelling of tabla\nsignals,” Johns Hopkins University , 2003.\n[6] P. Chordia. “Segmentation and Recognition of Tabla\nStrokes,” ISMIR , pp. 107–114, 2005.\n[7] A Batish. “Tabla Lesson 8 - Some Popular Tabla\nThekas,” Batish Institute of Indian Music and\nFine Arts ,http://raganet.com/Issues/8/\ntabla8.html , 2003.\n[8] P. Herrera, A. Yeterian, and F. Gouyon. “Automatic\nclassiﬁcation of drum sounds: a comparison of feature\nselection methods and classiﬁcation techniques,” Mu-\nsic and Artiﬁcial Intelligence , pp. 69–80, 2002.\n[9] D. Bogdanov, N. Wack, E. G ´omez, S. Gulati, P. Her-\nrera, O. Mayor, G. Roma, J. Salamon, J. Zapata, and X.\nSerra. “Essentia: An Audio Analysis Library for Music\nInformation Retrieval,” ISMIR , pp. 493–498, 2013.\n[10] C. V . Raman. “The Indian musical drums,” Proc. of the\nIndian Academy of Sciences - Section A , pp. 179–188,\n1934.\n[11] P. M. Brossier. “Automatic annotation of musical audio\nfor interactive applications,” Diss. Queen Mary, Uni-\nversity of London , 2006.\n[12] E. Jones, T. Oliphant, P. Peterson and others. “SciPy:\nOpen Source Scientiﬁc Tools for Python,” http://\nwww.scipy.org/ , 2001.\n[13] W. Brent. “Physical and perceptual aspects of percus-\nsive timbre,” UC San Diego Electronic Theses and Dis-\nsertations , 2010.\n[14] I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal. “Data\nMining: Practical machine learning tools and tech-\nniques,” Morgan Kaufmann , 2016.262 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[15] S. Gupta, A. Srinivasamurthy, M. Kumar, H. A.\nMurthy, X. Serra. ”Discovery of Syllabic Percussion\nPatterns in Tabla Solo Recordings,” Proc. of the 16th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2015.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 263"
    },
    {
        "title": "Scale- and Rhythm-Aware Musical Note Estimation for Vocal F0 Trajectories Based on a Semi-Tatum-Synchronous Hierarchical Hidden Semi-Markov Model.",
        "author": [
            "Ryo Nishikimi",
            "Eita Nakamura",
            "Masataka Goto",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416330",
        "url": "https://doi.org/10.5281/zenodo.1416330",
        "ee": "https://zenodo.org/records/1416330/files/NishikimiNGIY17.pdf",
        "abstract": "This paper presents a statistical method that estimates a se- quence of musical notes from a vocal F0 trajectory. Since the onset times and F0s of sung notes are considerably de- viated from the discrete tatums and pitches indicated in a musical score, a score model is crucial for improving time- frequency quantization of the F0s. We thus propose a hier- archical hidden semi-Markov model (HHSMM) that com- bines a score model representing the rhythms and pitches of musical notes with musical scales with an F0 model rep- resenting time-frequency deviations from a note sequence specified by a score. In the score model, musical scales are generated stochastically. Note pitches are then gener- ated according to the scales and note onsets are generated according to a Markov process defined on the tatum grid. In the F0 model, onset deviations, smooth note-to-note F0 transitions, and F0 deviations are generated stochastically and added to the note sequence. Given an F0 trajectory, our method estimates the most likely sequence of musical notes while giving more importance on the score model than the F0 model. Experimental results showed that the proposed method outperformed an HMM-based method having no models of scales and rhythms.",
        "zenodo_id": 1416330,
        "dblp_key": "conf/ismir/NishikimiNGIY17",
        "keywords": [
            "statistical",
            "method",
            "estimates",
            "musical",
            "notes",
            "vocal",
            "F0",
            "trajectory",
            "score",
            "model"
        ],
        "content": "SCALE- AND RHYTHM-AWARE MUSICAL NOTE ESTIMATION FOR\nVOCAL F0 TRAJECTORIES BASED ON A SEMI-TATUM-SYNCHRONOUS\nHIERARCHICAL HIDDEN SEMI-MARKOV MODEL\nRyo Nishikimi1Eita Nakamura1Masataka Goto2Katsutoshi Itoyama1Kazuyoshi Yoshii1;3\n1Graduate School of Informatics, Kyoto University, Japan3RIKEN AIP, Japan\n2National Institute of Advanced Industrial Science and Technology (AIST), Japan\nfnishikimi, enakamura, itoyama, yoshii g@sap.ist.i.kyoto-u.ac.jp, m.goto@aist.go.jp\nABSTRACT\nThis paper presents a statistical method that estimates a se-\nquence of musical notes from a vocal F0 trajectory. Since\nthe onset times and F0s of sung notes are considerably de-\nviated from the discrete tatums and pitches indicated in a\nmusical score, a score model is crucial for improving time-\nfrequency quantization of the F0s. We thus propose a hier-\narchical hidden semi-Markov model (HHSMM) that com-\nbines a score model representing the rhythms and pitches\nof musical notes with musical scales with an F0 model rep-\nresenting time-frequency deviations from a note sequence\nspeciﬁed by a score. In the score model, musical scales\nare generated stochastically. Note pitches are then gener-\nated according to the scales and note onsets are generated\naccording to a Markov process deﬁned on the tatum grid.\nIn the F0 model, onset deviations, smooth note-to-note F0\ntransitions, and F0 deviations are generated stochastically\nand added to the note sequence. Given an F0 trajectory,\nour method estimates the most likely sequence of musical\nnotes while giving more importance on the score model\nthan the F0 model. Experimental results showed that the\nproposed method outperformed an HMM-based method\nhaving no models of scales and rhythms.\n1. INTRODUCTION\nSinging voice analysis is important for music information\nretrieval because a singing voice usually forms a large part\nof the melody line of popular music, and provides much in-\nformation about music. Singing voice analysis techniques\nsuch as vocal F0 estimation [1,3,7,9,14] and singing voice\nseparation [8, 12] have actively been studied and applied\nto singer identiﬁcation [10, 22], karaoke generation [19],\nquery-by-humming [8], and active music listening [6]. To\nleverage musical information conveyed by singing voices,\nit is helpful to convert a vocal F0 trajectory to a musical\nscore containing only discrete symbols.\nc⃝Ryo Nishikimi, Eita Nakamura, Masataka Goto, Kat-\nsutoshi Itoyama, Kazuyoshi Yoshii. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nRyo Nishikimi, Eita Nakamura, Masataka Goto, Katsutoshi Itoyama,\nKazuyoshi Yoshii. “Scale- and Rhythm-Aware Musical Note Estimation\nfor V ocal F0 Trajectories Based on a Semi-Tatum-Synchronous Hierar-\nchical Hidden Semi-Markov Model”, 18th International Society for Mu-\nsic Information Retrieval Conference, Suzhou, China, 2017.\n/g37/g68/g85 /g37/g68/g85 /g37/g68/g85\n/g38/g3 /g80 /g68 /g77/g82/g85 /g54/g70/g68/g79/g72\n/g55 /g72 /g80/g83/g82 /g85/g68 /g79 /g3 /g71/g72 /g89 /g76 /g68 /g87 /g76 /g82 /g81\n/g57 /g82/g70/g68/g79/g3 /g41/g19/g41/g85/g72 /g84/g88/g72 /g81/g70 /g92/g3/g71/g72 /g89 /g76 /g68 /g87 /g76 /g82 /g81/g48/g88/g86/g76/g70/g68/g79\n/g86/g70/g82/g85/g72/g39/g72 /g83/g72 /g81/g71/g3/g82 /g81/g3/g87 /g75/g72 /g3 /g86/g70 /g68 /g79 /g72/g38/g3 /g80 /g68 /g77/g82/g85 /g36 /g80 /g76/g81 /g82 /g85/g54/g70/g82/g85/g72\n/g80 /g82/g71 /g72/g79\n/g41/g19\n/g80 /g82/g71 /g72/g79Figure 1 : The generative process of a vocal F0 trajectory\nbased on a hierarchical hidden semi-Markov model involv-\ning a score model and an F0 model.\nIn this study, we tackle musical note estimation for vo-\ncal F0 trajectories that tend to have large deviations from\noriginal musical scores. The pitches and onset times of\nmusical notes in a musical score can take only discrete val-\nues, whereas an F0 trajectory is a continuous signal that\ncan dynamically and smoothly vary over time. F0 trajecto-\nries are modulated by vibrato and changes smoothly from\none note to another by a portamento. Naive time-frequency\nquantization of an F0 trajectory therefore outputs a note se-\nquence that often includes statistically-rare chromatic note\nprogressions with unlikely rhythms.\nTo solve this problem, we propose a statistical method\nof scale- and rhythm-aware musical note estimation based\non integration of a score model describing the process of\ngenerating a note sequence and an F0 model describing the\nprocess of generating an F0 trajectory from the note se-\nquence (Fig. 1). In the score model, a sequence of musical\nscales (local keys) is determined by a Markov process and\nthe semitone-level pitch of each note is then determined ac-\ncording to both a scale of the note position and the pitch of\na previous note. The onset position of each note on a tatum\ngrid is determined according to that of a previous note\nto make rhythmic structures. In the F0 model, the time-\nfrequency deviations are added to a step-function-shaped\nF0 trajectory corresponding to a musical score given by\nthe score model. The integrated model is thus formulated\nas a hierarchical hidden semi-Markov model (HHSMM).\nGiven a vocal F0 trajectory with a tatum grid, the scales,\nmusical notes, and F0 deviations, which are all latent vari-\nables of the proposed model, are jointly estimated by us-376ing a Markov chain Monte Carlo algorithm. A key feature\nof our method is that musical scales and rhythms work as\nself-organizing constraints on time-frequency quantization\nof vocal F0 trajectories.\n2. RELATED WORK\nIn this section, we introduce related work on the analysis\nof singing voices.\n2.1 Vocal F0 Estimation for Music Audio Signals\nEstimation of vocal F0 trajectories for music audio signals\nhas actively been studied [1, 3, 7, 9, 14], and the outputs\nof these methods can be used as inputs of our method.\nOne of the most basic method is subharmonic summation\n(SHS) [7] that calculates the sum of the harmonic compo-\nnents of each candidate F0. Ikemiya et al. [9] improved\nF0 estimation based on SHS and singing voice separation\nbased on robust principle component analysis (RPCA) [8]\nby using the mutual dependency of those two tasks. Sala-\nmon et al. [21] estimated contours of the melody F0 can-\ndidates by calculating a salience function and then recur-\nsively removed contours which do not form a melody line\nby using the characteristics of each contour. Durrieu et\nal.[3] extracted a main melody by representing accompa-\nniments with a model inspired by non-negative matrix fac-\ntorization (NMF) and leading voices with a source-ﬁlter\nmodel. Mauch et al. [14] modiﬁed the YIN [1] in a prob-\nabilistic way so that the modiﬁed system could determine\nmultiple candidate fundamental frequencies and then se-\nlect one at each frame by using an HMM.\n2.2 Musical Note Estimation for Singing Voices\nEstimation of musical notes from sung melody has been\na hot research topic [6, 11, 13, 15, 17, 18, 20, 23]. A naive\nmethod is to take the majority of vocal F0s in each interval\nof a regular grid [6]. Paiva et al. [17] proposed a cascad-\ning method based on multipitch detection, multipitch tra-\njectory construction, segmentation of multipitch trajectory,\nelimination of irrelevant notes, and extraction of notes that\nform a main melody. Raphael [18] proposed an HMM-\nbased method that estimates pitches, rhythms, and tempos\nwhen the number of notes is given. The rhythm and on-\nset deviation models used in [18] are similar to those used\nin our method. Laaksonen et al. [11] divided audio data\ninto segments corresponding to scales and notes by focus-\ning on the boundaries of chords given as input, and inde-\npendently estimated the notes based on a score function.\nRyyn ¨anen et al. [20] proposed a method based on a hierar-\nchical HMM in order to capture the different kinds of vocal\nﬂuctuations (e.g., vibrato and portamento) within one note.\nIn this model, the transition between pitches is represented\nin the upper-level HMM and the transition between the\nvocal ﬂuctuations is represented in the lower-level HMM.\nMolina et al. [15] focused on the hysteresis characteris-\ntics of vocal F0s. Nishikimi et al. [16] proposed a method\nbased on an HHM that represents the generative process of\na vocal F0 trajectory considering the time and frequency\ndeviations. Yang et al. [23] proposed a method based on\na hierarchical HMM that represents the generative processof thef0-∆f0plane. Mauch et al. [13] developed a soft-\nware tool called Tony for extracting pitches. In this tool, a\nvocal F0 trajectory is estimated by PYIN [14], and musical\nnotes are estimated by a modiﬁed version of Ryyn ¨anen’s\nmethod [20].\n3. PROPOSED METHOD\nThis section explains the proposed method of estimating a\nsequence of musical notes from a vocal F0 trajectory. The\nmethod is based on an HHSMM (Fig. 1) that stochastically\ngenerates the F0 trajectory with time-frequency deviations\nfrom a sequence of musical notes depending on musical\nscales. The upper part of the proposed model is an HMM\nthat stochastically generates a sequence of musical notes\naccording to the scales that are assigned to bars. The lower\npart is an HSMM that represents the musical notes and\ntemporal deviations as latent variables and the frequency\ndeviations as F0 emission probabilities.\n3.1 Problem Speciﬁcation\nThe problem we tackle is deﬁned as follows:\nInput : A vocal F0 trajectory X=fxtgT\nt=1and16th-note-\nlevel tatumsY=f(un;vn)gN\nn=0,\nOutput : A sequence of notes Z=fzj=(pj;lj)gJ\nj=0,\nwhereTis the number of frames in a vocal F0 trajectory, xt\nis a log frequency at time t, andNis the number of 16th-\nnote-level tatums. un2 f1;:::;T +1gis the time of tatum\nnand the beginning and end of music are represented as\nu0= 1anduN=T+1, respectively. vn2 f0;:::; 15gis\nthe relative position of tatum nin a bar.Jis the number of\nmusical notes estimated by proposed methods, and the j-th\nnotezjis represented as a pair consisting of an pitch pj2\nf1;:::;K gand a note length lj2 f1;:::;L gin the unit of\ntatums, where Kis the number of kinds of semitone-level\npitches, and pjindicates any one in f\u00161;:::;\u0016Kg, which\nis a set of log frequencies corresponding to semitone-level\npitches. For convenience we introduce the initial note z0\nthat does not appear in the actual score.\n3.2 Probabilistic Modeling of Musical Scores\nThis section describes the score model constructed with an\nHMM that represents rhythms and pitches of musical notes\nunder musical scales.\n3.2.1 Modeling Scale Transitions\nScales are represented as S=fsmgM\nm=0, whereMde-\nnotes the number of bars in the musical piece and smde-\nnotes the scale at the m-th bar. For convenience, we intro-\nduce the initial bar s0to which the initial note z0belongs.\nInstead of ﬁxing one scale for the whole piece, the scale\nis allowed to change at bar lines. Each scale smtakes one\nof the 24values of fC;C#;\u0001 \u0001 \u0001;Bg \u0002 f major, minor g. The\nlatent variables Sare described by a Markov chain as\np(s0j\u0019) =\u0019s0; (1)\np(smjsm\u00001;\u0018sm\u00001) =\u0018sm\u00001sm; (2)\nwhere\u00192R24\n\u00150is a set of initial probabilities and \u0018s2R24\n\u00150\nis a set of transition probabilities.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 377/g40\n/g39/g6\n/g39\n/g38/g6\n/g38\n/g20 /g21 /g22 /g23 /g24 /g25 /g26 /g27 /g28 /g20/g19 /g20/g20 /g20/g21 /g20/g22 /g20/g23 /g19 /g20/g24 /g19/g55/g68/g87/g88/g80\n/g44/g81/g71/g72/g91/g51/g76/g87/g70/g75/g38 /g80/g68/g77/g82/g85\n/g55/g85/g68/g81/g86/g76/g87/g76/g82/g81/g3/g69/g72/g87/g90/g72/g72/g81/g3/g83/g76/g87/g70/g75/g72/g86\n/g55/g85/g68/g81/g86/g76/g87/g76/g82/g81/g3/g69/g72/g87/g90/g72/g72/g81/g3/g87/g68/g87/g88/g80/g86/g38/g82/g81/g87/g85/g82/g79/g3/g87/g75/g72/g3/g87/g72/g81/g71/g72/g81/g70/g92/g3/g82/g73/g3/g72/g68/g70/g75/g3/g83/g76/g87/g70/g75/g98/g86/g3/g68/g83/g83/g72/g68/g85/g68/g81/g70/g72Figure 2 : Overview of the score model.\n/g57/g82/g70/g68/g79/g3/g41/g19\n/g49/g82/g87/g72/g55/g68/g87/g88/g80/g3/g87/g76/g80/g72\n/g50/g81/g86/g72/g87/g3/g71/g72/g89/g76/g68/g87/g76/g82/g81\n/g41/g19/g3/g87/g85/g68/g81/g86/g76/g87/g76/g82/g81/g68/g79\n/g71/g88/g85/g68/g87/g76/g82/g81\n/g55/g76/g80/g72/g51/g76/g87/g70/g75\n(a) Temporal deviations\n/g57/g82/g70/g68/g79/g3/g41/g19\n/g49/g82/g87/g72/g41/g85/g72/g84/g88/g72/g81/g70/g92/g3/g71/g72/g89/g76/g68/g87/g76/g82/g81\n/g55/g76/g80/g72/g51/g76/g87/g70/g75 (b) Frequency deviations\nFigure 3 : Deviations in a vocal F0 trajectory.\n3.2.2 Modeling Pitch Transitions\nThe sequence of pitches Pis generated by a Markov chain\ndepending on scales Sas follows (Fig. 2):\np(p0js0;ϕs0) =ϕs0p0; (3)\np(pjjpj\u00001;sm; smpj\u00001) = smpj\u00001pj; (4)\nwhereϕs2RK\n\u00150is a set of initial probabilities,  sp2RK\n\u00150\nis a set of transition probabilities, and mis the index of\na bar to which the note zjbelongs. Moreover, ϕs0p0and\n smpj\u00001pjare deﬁned as\nϕs0p0=^ϕ^s0deg(p0;s0)∑K\np=1^ϕ^s0deg(p;s0); (5)\n smpj\u00001pj=^ ^smdeg(pj\u00001;sm)deg(pj;sm)∑K\np=1^ ^smdeg(pj\u00001;sm)deg(p;sm); (6)\nwhere ^s2fmajor;minor gis the mode of scale sanddeg(p;s)\n2f0;:::; 11gis the degree of pitch pin scales(deﬁned as\nthe relative pitch class of pfrom the tonic of scale s).^ϕ\u0003\nand ^ \u0003are the initial and transition probabilities of pitch\nclasses, given the scales.\n3.2.3 Modeling Onset Transitions\nConsidering the transition between onset positions of adja-\ncent notes, the model makes Zhave the plausible rhythm.\nLetrj\u000012 fvngN\nn=1be the onset position of the j-th note\nzj. The transition probability is given by\np(rjjrj\u00001;\u0010rj\u00001) =\u0010rj\u00001rj; (7)\nwhere the distance between rj\u00001andrjindicates the note\nvalueljof notezj. We assume that r0=v0andrJ=vN.\n3.3 Probabilistic Modeling of F0 Trajectories\nThe section describes the F0 model based on an HSMM\nthat represents the generative process of a vocal F0 trajec-\ntory. In our model, the pitches, onsets, and temporal devia-\ntions are represented as latent variables, and the frequency\ndeviations are represented as emission probabilities.\n/g2020/g3043/g3285/g3127/g3117/g2020/g3043/g3285/g1859/g3037/g2879/g2869\n/g1856/g3037 /g1856/g3037/g2878/g2869/g1859/g3037\n/g1864/g3037/g1873/g3041/g2879/g3039/g3285 /g1873/g3041\n/g55/g76/g80/g72/g3/g2028/g3037/g2879/g2869 /g2028/g3037 /g51/g76/g87/g70/g75\n/g62/g70/g72/g81/g87/g64\n/g2020 /g3548/g3047Figure 4 : The black bold line represents a sequence of the\nlocation parameters of the Cauchy distributions.\n3.3.1 Modeling Temporal Deviations\nWe assume that vocal F0 trajectories include the following\ntwo types of temporal deviations (Fig. 3a):\nOnset deviation : the gap between the vocal onset time\nand the note onset time.\nF0 transitional duration : the time it takes for singing\nvoices to ﬁnish transitioning from one pitch to the next.\nThe onset deviations G=fgjgJ\nj=0accompanying with\nZare represented as discrete latent variables. Each gjcan\ntake an integer value between \u0000GandG. As with the onset\nposition model, gj\u00001denotes the onset deviation of note\nzj. We assume that each gjis independently generated by\np(gjj\u001a) =\u001agj; (8)\nwhere\u001a2R2G+1\n\u00150is a set of onset deviation probabilities.\nWe assume that there are no deviations for the onset of the\nﬁrst note and the offset of the last note, i.e.,g0=gJ= 0.\nThe F0 transitional durations D=fdjgJ\nj=1accompa-\nnying withZare also represented as discrete latent vari-\nables. Each djcan take a value from 1toD. The con-\ntinuous transition of vocal F0s between notes zj\u00001andzj\nis represented by a slanted line spanning djframes. We\nassume that each djis independently generated as follows:\np(djj\u0011) =\u0011dj; (9)\nwhere\u00112RD\n\u00150is a set of duration probabilities.\n3.3.2 Modeling Frequency Deviations\nThe vocal F0 trajectory X=fxtgT\nt=1is generated by im-\nparting probabilistic frequency deviations to the sequence\nof notes to which probabilistic temporal deviations have\nalready been imparted (Fig. 3b). Assuming that xtis inde-\npendently generated at each frame, the emission probabil-\nity of thej-th notezjis given by\np(x\u001cj\u00001:\u001cj\u00001jpj\u00001;pj;lj;gj\u00001;gj;dj;^\u0016t;\u0015)\n=\u001cj\u00001∏\nt=\u001cj\u00001f\u000ext;voiced Cauchy(xtj^\u0016t;\u0015) +\u000ext;unvoiced g\n=epj\u00001pjljgj\u00001gjdj; (10)\nwherex\u001c′:\u001c\u00001indicatesx\u001c′;:::;x\u001c\u00001,\u0015is a scale param-\neter that represents the scale of the frequency deviations, \u000e\nis Kronecker’s delta, and ^\u0016t(Fig. 4) is a location parameter\ngiven by\n^\u0016t={\u0016pj\u0000\u0016pj\u00001\ndj(t\u0000\u001cj\u00001)+\u0016pj\u00001(\u001cj\u00001\u0014t<\u001cj+dj)\n\u0016pj (\u001cj\u00001+dj\u0014t<\u001cj):(11)\nWhen the onset of note zj+1is located at the n-th tatum,\n\u001cj=un+gjand\u001cj\u00001=un\u0000lj+gj\u00001.378 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017/g39/g72 /g74/g85/g72/g72 /g19 /g20 /g21 /g22 /g23 /g24 /g25 /g26 /g27 /g28 /g20/g19 /g20/g20/g2235/g2923 /g2911/g2920/g2925/g2928/g3109/g3553/g3404\n/g2235/g2923/g2919/g2924/g2925 /g2928/g3109/g3553/g3404Figure 5 : The conﬁguration of the hyperemarameter a^ϕ\n^s.\n3.4 Prior Distributions\nWe put conjugate Dirichlet priors on categorical model pa-\nrameters\u0019,\u0018,^ϕ,^ ,\u0010,\u001a, and\u0011as follows:\n\u0019\u0018Dirichlet(\na\u0019)\n;\u0018s\u0018Dirichlet(\na\u0018\ns)\n;\n^ϕ^s\u0018Dirichlet(\na^ϕ\n^s)\n;^ ^sdeg(p;s)\u0018Dirichlet(\na^ \n^sdeg(p;s))\n;\n\u0010r\u0018Dirichlet(\na\u0010\nr)\n;\n\u001a\u0018Dirichlet(\na\u001a)\n;\u0011\u0018Dirichlet(\na\u0011)\n; (12)\nwherea\u00192R26\n+,a\u0018\ns2R26\n+,a^ϕ\n^s2R12\n+,a^ \n^sdeg(p;s)2R12\n+,a\u0010\nr2R16\n+,\na\u001a2R2G+1\n+ , anda\u00112RD\n+are hyperparameters. The prob-\nability distribution over the 12 pitch classes under a scale\nis estimated using the priors on the initial and transitional\nprobabilities of those classes. As illustrated in Fig. 5, we\nset the hyperparameters a^ϕ\n^sanda^ \n^sdeg(p;s)so that the prob-\nability distributions represent the diatonic scales, respec-\ntively. Since the Cauchy distribution does not have a con-\njugate prior, we put a Gamma prior on \u0015as\n\u0015\u0018Gamma(\na\u0015\n0;a\u0015\n1)\n; (13)\nwherea\u0015\n0anda\u0015\n1are shape and rate hyperparameters.\n3.5 Bayesian Inference\nGiven an F0 trajectory X, we aim to calculate the posterior\ndistribution p(Q;S;\u0002jX), whereQ=fP;L;G;Dg\n(latent variables) and \u0002=f\u0019;\u0018;^ϕ;^ ;\u0010;\u001a;\u0011g(model\nparameters). Since this calculation is analytically intractab-\nle, we use Markov chain Monte Carlo (MCMC) methods.\nTo get samples of the latent variables SandQ, forward\nﬁltering-backward sampling algorithms are used. To get\nsamples of \u0002except for\u0015, a set of parameters with con-\njugate priors, a Gibbs sampling algorithm is used. Since\nthere is no conjugate prior for the parameter \u0015, we use\nthe Metropolis-Hastings (MH) algorithm. Since SandQ\nshare the sequence of notes Zand are mutually dependent,\neach variable is updated as follows:\n1.Initialize notes Zwith a majority-vote method.\n2.Update the sequence of scales Sbased on given Z.\n3.UpdateQbased on given S.\n4.Update the model parameters \u0002.\n5.Return to 2.\n3.5.1 Inferring Latent Variables S\nGiven the sequence of notes Z, eachsmis sampled in ac-\ncordance with the probability given by\n\fS\nsm=p(smjsm+1:M;Z); (14)\nwheresm+1:Mrepresentssm+1;:::;sM. The calculation\nof Eq. (14) and sampling of scales Sare performed by the\nforward ﬁltering-backward sampling method.\nIn forward ﬁltering, we recursively calculate the proba-\nbility\u000bS\nsmas follows:\n\u000bS\ns0=p(p0;s0) =p(p0js0)p(s0) =ϕs0p0\u0019s0;(15)\u000bS\nsm=p(p0:jm+1\u00001;sm)\n=jm+1\u00001∏\nj=jm smpj\u00001pj∑\nsm\u00001\u0018sm\u00001sm\u000bS\nsm\u00001;(16)\nwherejmis the index of the ﬁrst note whose onset belongs\nto them-th bar.jmcan be calculated from given note val-\nuesL.\nIn backward sampling, Eq. (14) is calculated by using\nthe values calculated in forward ﬁltering, and scales are\nsampled recursively as follows:\n\fS\nsM=p(sMjZ)/\u000bS\nsM; (17)\n\fS\nsm=p(smjsm+1:M;Z)/\u000bS\nsm\u0018smsm+1: (18)\n3.5.2 Inferring Latent Variables Q\nThe latent variables Qcan be estimated in a way similar\nto that in which the latent variables Sare inferred. In\nforward ﬁltering, we recursively calculate the probability\n\u000bQ\npnln;gndnas follows:\n\u000bQ\np0l0g0d0=p(p0jS) =ϕy0p0; (19)\n\u000bQ\npnlngndn=p(x1:\u001cn\u00001;pn;ln;gn;dnjS)\n=8\n>>>>>>>><\n>>>>>>>>:0 (ln>n)\n\u001agn\u0011dn\u0010r0rn\n\u0001∑\np0 s1p0pnep0pnln0gndn\u000bQ\np0l0g0d0(ln=n)\n∑\npn′;gn′min(n′;L)∑\nln′∑\ndn′\u001agn\u0011dn\u0010rn′rn sm(n′)pn′pn\n\u0001epn′pnlngn′gndn\u000bQ\npn′ln′gn′dn′(ln<n);(20)\nwhere\u001cn=un+gn,n′=n\u0000ln, andm(n′)is the index\nof the bar that the n′-th tatum belongs to. pn,ln,gn, and\ndnare the variables of forward messages that correspond\nto the note whose offset position is at the n-th tatumun.\nNote that these variables are different from j-indexed vari-\nablespj,lj,gj, anddj. Since the onset and offset times\nof the notezn= (pn;ln)are respectively the (n\u0000ln)-th\ntatum and the n-th tatum, the probability p(ln)which ap-\npears in the recursive calculation of Eq. (20) is replaced by\np(rnjrn\u0000ln).\nIn backward sampling, the posterior distribution of the\nlatent variables is calculated by using the values calculated\nin forward ﬁltering, and notes and temporal deviations are\nsampled recursively as follows:\n\fpNlNgNdN=p(pN;lN;gN;dNjX;S)/\u000bQ\npNlNgNdN;\n\fpn′ln′gn′dn′\n=p(pn′;ln′;gn′;dn′jpn:N;ln:N;gn:N;dn:N;X)\n/8\n><\n>:0 (ln>n)\nepn′pnlngn′gndn sm(n′)pn′pn\n\u0001\u0010rn′rn\u001agn\u0011dn\u000bQ\npn′ln′gn′dn′(ln\u0014n): (21)\n3.5.3 Learning Model Parameters \u0002\nThe posterior distributions of the model parameters with\nthe prior distributions are calculated using SandQob-\ntained in the backward sampling steps, and these parame-\nters are sampled according to the posterior distributions as\nfollows:\n\u0019\u0018Dirichlet(\na\u0019+b\u0019)\n;\u0018s\u0018Dirichlet(\na\u0018\ns+b\u0018\ns)\n;(22)Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 379^ϕ^s\u0018Dirichlet(\na^ϕ\n^s+b^ϕ\n^s)\n; (23)\n^ ^sdeg(p;s)\u0018Dirichlet(\na^ \n^sdeg(p;s)+b^ \n^sdeg(p;s))\n; (24)\n\u0010r\u0018Dirichlet(\na\u0010\nr+b\u0010\nr)\n; (25)\n\u001a\u0018Dirichlet(\na\u001a+b\u001a)\n;\u0011\u0018Dirichlet(\na\u0011+b\u0011)\n;(26)\nwhereb\u00192R26\n\u00150is a unit vector whose s0-th element is 1.\nb\u0018\ns2R26\n\u00150is a vector whose s′-th element indicates the num-\nber of transitions between adjacent scales sands′in the se-\nquence of latent variables Y.b\u001a2R2G+1\n\u00150is a vector whose\ng-th element indicates the number of vocal onset devia-\ntions ofgin sampledQ, andb\u00112RD\n\u00150is a vector whose\nd-th element represents the number of F0 transitional du-\nrations ofdin sampledQ.b\u0010\nr2R16\n\u00150is a vector whose r′-th\nelement represents the number of transitions between ad-\njacent note onset positions randr′inR=frjgJ\nj=0that\ncan be calculated from the note values Lsampled in back-\nward sampling. Regarding the vector b^ϕ\n^s2R12\n>0, when the\nscale of the initial bar and the pitch of the initial note are\ns0=sandp0=p, the value of the element b^ϕ\n^sdeg(p;s)\nis1, and the other elements are 0. Regarding the vector\nb^ \n^sdeg(p;s)2R12\n\u00150, the value of b^ \n^sdeg(p;s)deg(p′;s)is increased\nby one when there is a transition from a pitch pto a pitch\np′under a scale sin the sampled latent variables.\nTo apply the MH sampling to the parameter \u0015, we deﬁne\na random-walk proposal distribution as follows:\nq(\u0015\u0003j\u0015) = Gamma( \r\u0015;\r ); (27)\nwhere\u0015\u0003is a proposal, \u0015is the current sample, and \ris\na hyperparameter. The proposal \u0015\u0003is accepted as the next\nsample according to the probability given by\nA(\u0015\u0003;\u0015) = min{L(\u0015\u0003)q(\u0015j\u0015\u0003)\nL(\u0015)q(\u0015\u0003j\u0015)}\n; (28)\nwhereL(\u0015)is the complete joint likelihood of \u0015given by\nL(\u0015) = Gamma(\n\u0015ja\u0015\n0;a\u0015\n1)J∏\nj=1epj\u00001pjljgj\u00001gjdj;(29)\nfpj;lj;gj;djgJ\nj=0are the values sampled in the backward\nsampling. The value of \u0015is updated by \u0015\u0003only when the\nvalue ofA(\u0015\u0003;\u0015)is larger than a random number sampled\nfrom the uniform distribution U(0;1).\n3.6 Viterbi Decoding\nThe sequence of latent variables SandQare estimated\nwith the Viterbi algorithm with the model parameters that\nmaximize the joint distribution p(X;Q;S;\u0002j\b)in the\nlearning process. As in the inference of latent variables, we\ninitializeZby the majority-vote method, Sis estimated\nbased onZ, and thenQis estimated depending on the S\nestimated in the previous step.\nIn the Viterbi decoding on scales S, the value!S\nsis\nrecursively calculated as follows:\n!S\ns0= lnϕs0k0+ ln\u0019s0; (30)\n!S\nsm=jm+1\u00001∑\nj=jmln smpj\u00001pj+max\nsm\u00001{\nln\u0018sm\u00001sm+!S\nsm\u00001}\n:(31)\nIn the recursive calculation of !S\ns, the previous state sm\u00001that maximizes the value of !S\nsmis memorized as cS\nsm, and\nthe scalesSare recursively estimated as follows:\nsM= arg max\nsM\u000bS\nsM; sm\u00001=cS\nsm: (32)\nIn the Viterbi decoding on variables Q, the value!Q\nplgd\nis recursively calculated as follows:\n!Q\np0l0g0d0= wϕlnϕs0p0; (33)\n!Q\npnlngndn\n=8\n>>>>>>>>>>>>><\n>>>>>>>>>>>>>:\u0000inf (ln>n)\nw\u001aln\u001agn+w\u0011ln\u0011dn+w\u0010ln\u0010rnr0\n+ maxp0{\nw ln s1p0pn\n+ welnep0pnln0gndn+!Q\np0l0g0d0}\n(ln=n)\nw\u001aln\u001agn+w\u0011ln\u0011dn+w\u0010ln\u0010rnrn′\n+ max (pn′;ln′;gn′;dn′){\nw ln sm(n′)pn′pn\n+ welnepn′pnlngn′gndn+!Q\npn′ln′gn′dn′}\n(ln<n);\n(34)\nwhere wϕ,w ,w\u001a,w\u0011,w\u0010, and weare the weight pa-\nrameters that control the balance between probabilities. In\nthe recursive calculation of !Q\nplgd, the previous states pn′,\nln′,gn′, anddn′which maximize the value of !Q\npnlngndn\nare memorized as cQ\npnlngndn, and the variables Qare recur-\nsively estimated as follows:\n(pN;lN;gN;dN) = arg max\npN;lN;gN;dN\u000bQ\npNlNgNdN;(35)\n(pn′;ln′;gn′;dn′) =cQ\npnlngndn: (36)\n4. EVALUATION\nWe report comparative experiments conducted to evaluate\nthe performance of the proposed method in musical note\nestimation from vocal F0 trajectories.\n4.1 Experimental Conditions\nAmong the 100 pieces of popular music in the RWC mu-\nsic database [5], we used 63 pieces that do not include\n32nd notes, triplets, harmonizing parts, and overlaps of ad-\njacent notes, which the proposed method cannot deal with.\nThe input F0 trajectories were obtained from the annota-\ntion data [4] or automatically estimated by using the state-\nof-the-art melody extraction method proposed in [9]. The\nannotation data contain unvoiced regions and the estima-\ntion data do not. The tatum times and onset positions were\nobtained from the annotation data.\nThe Bayesian inference and Viterbi decoding were in-\ndependently conducted for each song. The onset transi-\ntion probabilities were learned in advance from a corpus\nof rock music [2] without Bayesian learning. The hyper-\nparameters were a\u0019=1,a\u0018\ns= /x31,a\u0010\nr=1,a\u001a=a\u0011=a\u0015\n0=\na\u0015\n1=\r=1, where /x31and1respectively represent the ma-\ntrix and vector whose elements are all ones. The elements\nofa^ϕ\n^sanda^ \n^sdeg(p;s)corresponding to musical notes on the\nscale of ^swere 10and the others were 1. The weight pa-\nrameters of the Viterbi algorithm were empirically set as\nwϕ= w = 29:4,w\u001a= 2:4,w\u0011= 2:9,w\u0010= 48:5, and\nwe= 3:8. To obtain musically-consistent sequences of380 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Model Input F0s Tatum level Note level\nProposed Ground-truth 72:4\u00061:7 28:1\u00062:1\nmethod Estimated 68:7\u00061:3 30:7\u00061:8\nWith Ground-truth 71:5\u00061:6 26:3\u00062:1\nonly rhythms Estimated 67:7\u00061:3 29:1\u00061:8\nWith Ground-truth 67:8\u00061:6 10:6\u00061:2\nonly scales Estimated 65:6\u00061:2 13:8\u00061:1\nWithout scales Ground-truth 67:2\u00061:5 9:8\u00061:2\n& rhythms Estimated 64:6\u00061:2 12:9\u00061:1\nMajority vote Ground-truth 54:1\u00061:5 20:1\u00061:4\nEstimated 61:0\u00061:4 22:0\u00061:5\nHMM [16] Estimated 68:0\u00061:2 14:8\u00061:3\nTable 1 : Average matching rates [%] and their standard\nerrors in tatum and note levels.\nmusical notes, we put more emphasis on the score model\nthan the F0 model.\nFor comparison, we tested the majority-vote method as\na baseline and the latest conventional method based on\na semi-beat-synchronous HMM [16]. Since the conven-\ntional method cannot deal with unvoiced regions in a vocal\nF0 trajectory given as input, we only tested the method\nfor the estimation data. To evaluate the effectiveness of\nthe score model, we tested four versions of the proposed\nmethod; a method that does not consider scales (scale tran-\nsition probabilities) and rhythms (onset transition proba-\nbilities), a method considering only scales, a method con-\nsidering only rhythms, the full method considering both\nscales and rhythms. To accelerate the inference, the search\nrange of pitches was limited around the pitches estimated\nby the majority-vote method.\nTo evaluate the performance of each method, we cal-\nculated tatum-level and note-level matching rates by com-\nparing the estimated sequences of musical notes with the\nground-truth data. The tatum-level matching rate is the rate\nof the number of tatum units whose pitches were estimated\ncorrectly to the total number of tatum units whose pitches\nexist in the ground-truth scores. The note-level match-\ning rate is the rate of the number of musical notes whose\npitches, onsets, and offsets were estimated correctly to the\ntotal number of musical notes in the ground-truth scores.\nIf adjacent notes in the ground-truth scores have the same\npitch or are connected by a tie, those notes were regarded\nas a single note. Since the compared method [16] outputs\na pitch in a 16th-note-wise manner, a sequence of the same\npitches was regarded as a single note.\n4.2 Experimental Results\nThe experimental results are shown in Table 11. The pro-\nposed method outperformed the majority-vote method and\nthe conventional method in terms of both measures. Com-\nparing the tatum-level matching rates obtained by the four\nversions of the proposed method, we conﬁrmed that the\nscore model improved the performance of musical note\nestimation. The use of the onset transition probabilities\n1The results of music note estimation by the proposed method are\navailable online: http://sap.ist.i.kyoto-u.ac.jp/members/nishikimi/demo/\nismir2017/\n/g38/g82/g85/g85/g72/g70/g87/g3/g86/g70/g82/g85/g72\n/g40/g86/g87/g76/g80/g68/g87/g72/g71/g3/g86/g70/g82/g85/g72\n/g40/g86/g87/g76/g80/g68/g87/g72/g71/g3/g86/g70/g82/g85/g72/g3/g11/g90/g76/g87/g75/g82/g88/g87/g3/g87/g75/g72/g3/g86/g70/g68/g79/g72/g3/g9/g3/g85/g75/g92/g87/g75/g80/g3/g80/g82/g71/g72/g79/g86/g12Figure 6 : Musical scores estimated from a ground-truth F0\ntrajectory by the proposed method and its variant without\nscale and rhythm constraints.\n(rhythm constraints) was found to be more effective than\nthat of the scale transition probabilities (scale constraints).\nAlthough the tatum-level matching rate obtained the pro-\nposed method (68.7%) was close to that obtained by the\nconventional method (68.0%), the note-level matching rate\nobtained the proposed method (30.7%) was better than that\nobtained by the conventional method (14.8%), This is a re-\nmarkable advantage of the proposed HHSMM that can di-\nrectly represent both the pitches and durations (onsets and\noffsets) of musical notes on symbolic musical scores, not\non continuous-time piano rolls.\nExamples of estimated musical scores are illustrated in\nFig. 6. The proposed method yielded the almost accurate\nmusical score except that some notes were merged. To cor-\nrectly recognize two adjacent notes with the same pitch, it\nis necessary to refer to original singing voices or music au-\ndio signals. The score estimated without considering the\nscore model, on the other hand, included a lot of wrong\nnotes that were inconsistent with music theory. This result\nalso shows the effectiveness of using the score model as\nmusical constraints on musical note estimation.\n5. CONCLUSION\nThis paper presented a statistical method for musical note\nestimation from a vocal F0 trajectory. Our method is based\non an HHSMM that combines a score model (HMM) rep-\nresenting the generative process of a musical score from\nmusical scales with an F0 model (HSMM) representing\nthe generative process of a vocal F0 trajectory with time-\nfrequency deviation from the musical score. We conﬁrmed\nthat the proposed method can yield more musically-consistent\nsequences of musical notes.\nOne of the most interesting directions of this research is\nto use the proposed model as a musically-meaningful prior\ndistribution on a vocal F0 trajectory in vocal F0 estimation\nfor music audio signals. We plan to integrate the proposed\n“language” model that generates an F0 trajectory from a\nmusical score with an acoustic model that generates a spec-\ntrogram from the F0 trajectory in a hierarchical Bayesian\nmanner. This enables us to jointly learn the vocal F0 tra-\njectory and musical score from music audio signals. Joint\nestimation of beat times and F0s is worth investigating to\novercome the problem of estimation-error accumulation in\nthe cascaded estimation approach.\nAcknowledgement: This study was partially supported by JSPS\nKAKENHI Grant Numbers 26700020, 16H01744, and 16J05486\nand JST ACCEL No. JPMJAC1602.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 3816. REFERENCES\n[1]A. de Cheveign ´e and H. Kawahara. YIN, a fundamen-\ntal frequency estimator for speech and music. The Jour-\nnal of the Acoustical Society of America , 111(4):1917–\n1930, 2002.\n[2]T. De Clercq and D. Temperley. A corpus analysis of\nrock harmony. Popular Music , 30(01):47–70, 2011.\n[3]J.-L. Durrieu, G. Richard, B. David, and C. F ´evotte.\nSource/ﬁlter model for unsupervised main melody ex-\ntraction from polyphonic audio signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n18(3):564–575, 2010.\n[4]M. Goto. Aist annotation for the RWC music database.\nInThe 7th International Conference on Music Infor-\nmation Retrieval (ISMIR 2006) , pages 359–360, 2006.\n[5]M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical and jazz mu-\nsic databases. In The 3rd International Conference on\nMusic Information Retrieval (ISMIR 2002) , pages 287–\n288, 2002.\n[6]M. Goto, K. Yoshii, H. Fujihara, M. Mauch, and\nT Nakano. Songle: A web service for active music\nlistening improved by user contributions. In Proc. of\nthe 12th International Society for Music Information\nRetrieval Conference (ISMIR 2011) , pages 311–316,\n2011.\n[7]Dik J. Hermes. Measurement of pitch by subharmonic\nsummation. The journal of the acoustical society of\nAmerica , 83(1):257–264, 1988.\n[8]P.-S. Huang, S. D. Chen, P. Smaragdis, and\nM. Hasegawa-Johnson. Singing-voice separation from\nmonaural recordings using robust principal compo-\nnent analysis. In 2012 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP\n2012) , pages 57–60, 2012.\n[9]Y . Ikemiya, K. Yoshii, and K. Itoyama. Singing voice\nanalysis and editing based on mutually dependent F0\nestimation and source separation. In 2015 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP 2015) , pages 574–578, 2015.\n[10] Y . E. Kim and B. Whitman. Singer identiﬁcation in\npopular music recordings using voice coding features.\nIn3rd International Conference on Music Information\nRetrieval (ISMIR 2002) , volume 13, page 17, 2002.\n[11] A. Laaksonen. Automatic melody transcription based\non chord transcription. In Proc. of the 15th Interna-\ntional Society for Music Information Retrieval (ISMIR\n2014) , pages 119–124, 2014.\n[12] Y . Li and D. Wang. Separation of singing voice from\nmusic accompaniment for monaural recordings. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 15(4):1475–1487, 2007.[13] M. Mauch, C. Cannam, R. Bittner, G. Fazekas, J Sala-\nmon, J. Dai, J. Bello, and S Dixon. Computer-aided\nmelody note transcription using the Tony software: Ac-\ncuracy and efﬁciency. In Proc. of the 1st International\nConference on Technologies for Music Notation and\nRepresentation (TENOR 2015) , pages 23–30, 2015.\n[14] M. Mauch and S. Dixon. pYIN: A fundamental fre-\nquency estimator using probabilistic threshold distri-\nbutions. In 2014 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP\n2014) , pages 659–663, 2014.\n[15] E. Molina, L. J. Tard ´on, A. M. Barbancho, and I. Bar-\nbancho. Sipth: Singing transcription based on hystere-\nsis deﬁned on the pitch-time curve. IEEE/ACM Trans-\nactions on Audio, Speech and Language Processing\n(TASLP) , 23(2):252–263, 2015.\n[16] R. Nishikimi, E. Nakamura, K. Itoyama, and K Yoshii.\nMusical note estimation for f0 trajectories of singing\nvoices based on a bayesian semi-beat-synchronous\nhmm. In Proc. of the 17th International Society for Mu-\nsic Information Retrieval Conference (ISMIR 2026) ,\npages 461–467, 2016.\n[17] R. P. Paiva, T. Mendes, and A. Cardoso. On the detec-\ntion of melody notes in polyphonic audio. In 6th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR 2005) , pages 175–182, 2005.\n[18] C. Raphael. A graphical model for recognizing sung\nmelodies. In 6th International Conference on Music\nInformation Retrieval (ISMIR 2005) , pages 658–663,\n2005.\n[19] M. Ryyn ¨anen, T. Virtanen, J. Paulus, and A. Kla-\npuri. Accompaniment separation and karaoke appli-\ncation based on automatic melody transcription. In\n2008 IEEE International Conference on Multimedia\nand Expo , pages 1417–1420, 2008.\n[20] M. P. Ryyn ¨anen and A. P. Klapuri. Automatic tran-\nscription of melody, bass line, and chords in poly-\nphonic music. Computer Music Journal , 32(3):72–86,\n2008.\n[21] J. Salamon and E. G ´omez. Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 20(6):1759–1770, 2012.\n[22] W.-H. Tsai and H.-M. Wang. Automatic singer recog-\nnition of popular music recordings via estimation and\nmodeling of solo vocal signals. IEEE Transactions on\nAudio, Speech, and Language Processing , 14(1):330–\n341, 2006.\n[23] L. Yang, A. Maezawa, J. B. L. Smith, and E. Chew.\nProbabilistic transcription of sung melody using a pitch\ndynamic model. In 2017 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP 2017) , pages 301–305, 2017.382 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "A Multiobjective Music Recommendation Approach for Aspect-Based Diversification.",
        "author": [
            "Ricardo S. Oliveira",
            "Caio Nóbrega",
            "Leandro Balby Marinho",
            "Nazareno Andrade"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417000",
        "url": "https://doi.org/10.5281/zenodo.1417000",
        "ee": "https://zenodo.org/records/1417000/files/OliveiraNMA17.pdf",
        "abstract": "Many successful recommendation approaches are based on the optimization of some explicit utility function defined in terms of the misfit between the predicted and the ac- tual items of the user. Although effective, this approach may lead to recommendations that are relevant but obvi- ous and uninteresting. Many approaches investigate this problem by trying to avoid recommendation lists in which items are very similar to each other (aka diversification) with respect to some aspect of the item. However, users may have very different preferences concerning what as- pects should be diversified and what should match their past/current preferences. In this paper we take this into consideration by proposing a solution based on multiobjec- tive optimization for generating recommendation lists fea- turing the optimal balance between the aspects that should be held fixed (maximize similarity with users actual items) and the ones that should be diversified (minimize similar- ity with other items in the recommendation list). We eval- uate our proposed approach on real data from Last.fm and demonstrate its effectiveness in contrast to state-of-the-art approaches.",
        "zenodo_id": 1417000,
        "dblp_key": "conf/ismir/OliveiraNMA17",
        "keywords": [
            "explicit utility function",
            "misfit between predicted and actual items",
            "avoid recommendation lists",
            "very similar items",
            "users preferences",
            "diversification",
            "item aspects",
            "multiobjective optimization",
            "optimal balance",
            "real data from Last.fm"
        ],
        "content": "A MULTIOBJECTIVE MUSIC RECOMMENDATION APPROACH FOR\nASPECT-BASED DIVERSIFICATION\nRicardo S. Oliveira Caio N ´obrega Leandro B. Marinho Nazareno Andrade\nUFCG - Federal University of Campina Grande, Brazil\nfricardooliveira, caionobrega g@copin.ufcg.edu.br, flbmarinho, nazareno g@computacao.ufcg.edu.br\nABSTRACT\nMany successful recommendation approaches are based on\nthe optimization of some explicit utility function deﬁned\nin terms of the misﬁt between the predicted and the ac-\ntual items of the user. Although effective, this approach\nmay lead to recommendations that are relevant but obvi-\nous and uninteresting. Many approaches investigate this\nproblem by trying to avoid recommendation lists in which\nitems are very similar to each other (aka diversiﬁcation)\nwith respect to some aspect of the item. However, users\nmay have very different preferences concerning what as-\npects should be diversiﬁed and what should match their\npast/current preferences. In this paper we take this into\nconsideration by proposing a solution based on multiobjec-\ntive optimization for generating recommendation lists fea-\nturing the optimal balance between the aspects that should\nbe held ﬁxed (maximize similarity with users actual items)\nand the ones that should be diversiﬁed (minimize similar-\nity with other items in the recommendation list). We eval-\nuate our proposed approach on real data from Last.fm and\ndemonstrate its effectiveness in contrast to state-of-the-art\napproaches.\n1. INTRODUCTION\nIn scenarios of vast and dynamic availability of con-\ntent, such as online music streaming services, users are\nquickly overloaded with a large and ever increasing space\nof choices. Recommender systems are successful tools\nfor addressing this issue by modeling the preferences of\nusers and anticipating their information needs. The most\nsuccessful recommendation approaches are usually those\nbased on the optimization of some explicit utility function\ndeﬁned in terms of the misﬁt between the predicted and the\nactual items consumed by the user. Although effective in\nmany scenarios, the recommendation algorithms that opti-\nmize this kind of function are prone to deliver recommen-\ndations that are relevant but possibly uninteresting. For ex-\nc\rRicardo S. Oliveira, Caio N ´obrega, Leandro B. Marinho,\nNazareno Andrade. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Ricardo S. Oliveira,\nCaio N ´obrega, Leandro B. Marinho, Nazareno Andrade. “A Multiob-\njective Music Recommendation Approach for Aspect-based Diversiﬁca-\ntion”, 18th International Society for Music Information Retrieval Confer-\nence, Suzhou, China, 2017.ample, for a user who only listens to American punk rock\nbands from the 70’s, a recommendation of more bands of\nthis kind would probably be accurate, but possibly tedious\ngiven that this user may very likely be able to ﬁnd these\nartists without aid.\nIn order to mitigate this problem, many approaches\nhave appeared with the aim of increasing diversity in rec-\nommendations [6, 14, 17, 19]. This is usually achieved\nby mechanisms that avoid recommendation lists in which\nitems are very similar to each other with respect to some\naspect of the items (e.g. music genre). Such approaches\ncan potentially increase users satisfaction by providing less\nobvious recommendations. However, users may have dif-\nferent preferences concerning what aspects should be di-\nversiﬁed and what should match their past/current prefer-\nences. For example, a user may be very conservative con-\ncerning the music genres she likes to listen (e.g. Bossa\nNova), but very open to discover how this genre is played\nacross different countries (e.g. Bossa Nova played in Japan\nand India). This is exactly the recommendation scenario\nwe investigate in this paper, i.e., we want to generate rec-\nommendation lists by explicitly holding one or more item\naspects (e.g. Bossa Nova) constant, but increasing diver-\nsity in others (e.g. locality and time period). The aspects\nthat are held ﬁxed are the ones that correspond to the users\npast/current preferences while the others correspond to the\nway the users are open for diversiﬁcation.\nThis problem has two different and possibly conﬂicting\nobjectives that need to be optimized for each user’s recom-\nmendation list: (i) ﬁnd the items that maximize the similar-\nity with the preferences of the user in terms of some set of\nselected aspects, and (ii) ﬁnd the items that minimize the\nintra-list similarity (i.e. pairwise similarity of items in the\nrecommendation list) regarding a different set of selected\naspects. To generate recommendation lists that balance\nthese two objectives we propose Multiobjective Aspect Di-\nversiﬁcation (MOAD), a recommendation approach that\ncast this problem as a multiobjective optimization prob-\nlem. MOAD uses the Nondominated Sorting Genetic Al-\ngorithm - NSGA-II, which is an efﬁcient solver for this\nkind of problem [5]. The main difference between our ap-\nproach and other related work from the literature is that\nwe allow the explicit choice of the aspects to diversify and\nhold constant.\nAlthough music recommendation may refer to many\ndistinct entities in the literature, such as song tracks, al-414bums and artists, in this paper we focus on artists due to\ntheir relative abundance of aspects publicly available. In\norder to assess the effectiveness of our proposed approach,\nwe collected artist listening historical data from Last.fm1,\na large online radio portal, and enriched this collection\nwith artists metadata collected from Music Brainz2and\nDBPedia3. We conduct several experiments by consider-\ning several switches between the ﬁxed and variable aspects\nand show that MOAD achieves the sought balance between\nboth objectives. We also compare our approach with sev-\neral diversiﬁcation algorithms from the literature and show\nthat our recommendations are more diverse and relevant in\nthe chosen aspects in comparison to those.\n2. PROBLEM FORMALIZATION\nThe problem that we address in this paper can be stated\nas follows: given a target user u2U(where Uis the set\nof users), her item consumption history Iu\u0012I(where I\nis the set of items)4, two disjoint sets XandYof item\naspects (possibly provided by the user), we want to ﬁnd\nthe top- nitems that are more similar to Iuregarding X\nand more dissimilar from each other regarding Y.\nItems may have different kinds of metadata associated\nto them, aka attributes, dimensions, contexts or side in-\nformation. For example, if the recommendable item is a\nmusic artist, we can think of each possible music genre as\na binary attribute. More formally, let Gbe the set of genres\nandd:G\u0002I!f0;1ga function that indicates if genre\ng2Gdescribes item i2I. The set Gis thus one aspect\nof the item and represents the set of attributes of the kind\ngenre.\nMore generally, let A=fA1; A2: : : ; A tgdenote\nthe set of tpossible aspects and fj:Aj\u0002I!R\nbe a feature extractor for any attribute a2Ajthat\ndescribes item i. An item may now be represented\nas a feature vector regarding aspect j. For example,\n~i= (f1(a1; i); f1(a2; i); : : : ; f 1(ap; i))represents a fea-\nture vector of item iwhere a1; a2; : : : ; a p2A1.\nThe input for the algorithm is a user u2U, her con-\nsumption history Iu, the size nof the recommendation\nlist and two disjoint sets of aspects: X; Y\u0012Awhere\nX\\Y=;. We also consider two similarity functions:\nga(R)that returns the intra-list similarity (cf. Section 4.4)\nof the items in the recommendation set Rw.r.t aspect a;\nandha(R; Iu)that returns the similarity between Rand\nthe user history Iu. Finally, let\ndiversity (R; X ) = 1\u00001\njXjX\na2Xga(R) (1)\ndenote the average intra-list distance for all aspects in X\nand\nafﬁnity (R; Y) =1\njYjX\na2Yha(R; Iu) (2)\n1http://www.last.fm/\n2https://musicbrainz.org/\n3http://wiki.dbpedia.org/\n4Iucan also be thought as a query in terms of MIRthe average similarity between RuandIufor all aspects\ninY. Now, given the aforementioned inputs, we want to\nﬁnd a set RnIu\u0012Iofnitems (i.e.jRj=n) that max-\nimize, at the same time, the objective functions deﬁned in\nequations 1 and 2, i.e.:\narg max\nR(diversity (R; X );afﬁnity (R; Y)) (3)\n3. RELATED WORK\nSeveral works have appeared in recent years proposing rec-\nommender systems concerned with other metrics beyond\naccuracy such as diversity, novelty or serendipity. Most\nof these works aim to maximize such an alternative metric\nwithout degrading accuracy. The seminal work of Ziegler\net al. [19] laid the foundations for achieving diversity based\non a re-ranking of collaborative ﬁltering algorithms results.\nSeveral other works appeared following similar principles\nbut based on different techniques such as graphs [8], ma-\nchine learning [7, 18] and information retrieval [15].\nAnother strand of work considers this problem as a mul-\ntiobjective optimization task. Realizing that accuracy, di-\nversity and novelty might be conﬂicting objectives, Ribeiro\net al. [11] proposed a hybrid recommendation system that\ncombines algorithms through an evolutionary approach to\nmaximize one objective, without sacriﬁcing the others.\nOuni et al. [9] proposed a genetic algorithm to recommend\nsoftware libraries, ﬁnding a trade-off between three objec-\ntives. Wang et al. [16] developed a multiobjective solu-\ntion to recommend accurate and unpopular items, called\nlong tail recommendations. Zuo et al. [20] proposed per-\nsonalized recommendations by balancing accuracy and di-\nversiﬁcation. And ﬁnally, Pampalk and Goto [10] pro-\nposed a graphic interface where users may adjust the rec-\nommendations received according to her desire by adjust-\ning music aspects. Our work also use multiobjective evo-\nlutionary algorithms for promoting diversiﬁcation, but dif-\nferently from the aforementioned related works, we enable\nthe explicit speciﬁcation of the aspects that should be di-\nversiﬁed.\n4. MULTIOBJETIVE ASPECT DIVERSIFICATION\nThe main motivation for this research is to give users more\ncontrol on their recommendations. We do this by letting\nthe aspects that should be held constant and the ones that\nshould be diversiﬁed user-deﬁnable. In this section we de-\nscribe in detail the components of our approach.\n4.1 Pareto Optimality\nIn multiobjective optimization the best solutions are the\nones that cannot be improved in any of the objectives with-\nout degrading at least one of the other objectives. This\nproperty is known as Pareto optimality . In our case, a fea-\nsible solution R\u0012I(i.e. a recommendation list of size n)\nis said to dominate another solution R0\u0012Iif:\n1.div(R; X )\u0015div(R0; X)^aff(R; Y)\u0015aff(R0; Y)Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4152.div(R; X )>div(R0; X)_aff(R; Y)>aff(R0; Y)\nwhere divandaffare abbreviations for diversity andafﬁn-\nityrespectively. A solution R\u0003\u0012Iis called Pareto op-\ntimal if there is no other solution that dominates it. The\nset of Pareto optimal solution is also known as the Pareto\nfront.\n4.2 Evolutionary Algorithm Approach\nDetermining the Pareto front is known to be a computa-\ntionally intensive task [2, 11]. In our case it basically re-\nquires an enumeration of all possible recommendation lists\nfor the objectives evaluation (i.e. O(2jIj)). Among the ap-\nproaches used for addressing this kind of problem, evolu-\ntionary algorithms appear as the most efﬁcient and used\nones [4,12,16,20]. Thus, we have decided to adopt a Mul-\ntiobjective Evolutionary Algorithm (MOEAs) for address-\ning our research problem.\nThe idea is to generate a population of recommendation\nlists as individuals such that the dominating individuals are\nconsidered the ﬁttest and are kept for the next generation.\nIf the dominating individuals are insufﬁcient to compose\nthe new generation, some dominated individuals are cho-\nsen to compose the next generation. The crossing over -\nswitching the items between neighbor individuals - and the\nmutation probability - used to replace some random items\nin individuals for items still not considered - allow new in-\ndividuals to approach the Pareto front throughout several\ngenerations.\n4.3 NSGA-II\nSimilarly to other related work, we have chosen the Non-\ndominated Sorting Genetic Algorithm II (NSGA-II) as the\nMOEA solution [9, 20]. Besides giving guarantees of con-\nvergence, it also offers a fast sorting function, called Fast\nNon-dominated Sorting, with O(MN2)where Mis the\nnumber of objectives and Nis the population size. This\nsorting function separates individuals into levels of dom-\ninance. For individuals in the same level, NSGA-II esti-\nmates the density of solutions, privileging a set of solutions\nthat are spread on the objective space, in a process called\nCrowding Distance Assignment.\nAlgorithm 1 presents NSGA-II pseudocode. The algo-\nrithm starts by creating an initial population of size N(line\n2). The following steps are repeated for each generation.\nA new offspring is created based on the current popula-\ntion (line 4) and the individuals are ordered and selected to\ncompose the population for the next generation (lines 5 to\n11). This ordering considers ﬁrst the selection of individu-\nals whose objectives are not dominated by other individu-\nals, made by fast-nondominated-sort (line 6), and second,\nthe density of individuals provided by crowding-distance-\nassignment (line 8). If the non-dominated individuals are\nnot enough to complete N, then individuals on the second\nlevel of dominance are chosen, and so on (lines 12 and 13).\nThe population Pt+1is the output for the algorithm, and\nwe select the individual with the greater sum of objective\nvalues as the ﬁnal recommendation list.Algorithm 1 NSGA-II\n1:procedure NSGA-II( N; nGen; mProb; cProb )\n2: P0=create-initial-population [N]\n3: fort in 0 to nGen -1 do\n4: Qt=create-new-offspring (Pt; mProb; cProb )\n5: Rt=Pt+Qt\n6: F=fast-nondominated-sort (Rt)\n7: whilejPt+1j+jFij6Ndo\n8: crowding-distance-assignment (Fi)\n9: Pt+1=Pt+1[Fi\n10: i=i+ 1\n11: end while\n12: Sort(Fi;\u001e)\n13: Pt+1=Pt+1[Fi[N\u0000jPt+1j]\n14: end for\n15:end procedure\n4.4 Item Representation and Similarity Metrics\nIn order to compute the objective functions deﬁned in\nequations 1 and 2 we need to compute similarities between\nitems concerning the sets of aspects used as input. Thus,\nwe deﬁne feature extraction functions for each aspect such\nthat similarity measures can be applied.\n4.4.1 Aspects Deﬁnition\nFirst we need to instantiate the set Aof aspects that we\nconsider in this paper:\n\u000fContemporaneity ( A1):refers to the year the artist\nwas born (if the artist is solo) or the year the band\nwas formed, in case the artist is a band.\n\u000fLocality (A2):refers often, but not always, to\nartist’s birth/formation country.\n\u000fGender ( A3):refers to the artist gender (when ap-\nplicable) together with its type (i.e. solo, band, or-\nchestra, etc.). This aspect is a combination of two\naspects where if the artist type is person (i.e. a solo\nartist) its gender is male orfemale . Otherwise, it has\nno gender but has a type that can be one of the fol-\nlowing: group ,orchestra ,choir ,character orother .\n\u000fMusic Genre (A4):refers to the artist music genres.\nWe have chosen this aspects for two main reasons: (i) they\nare used recurrently in related works (not necessarily to-\ngether) as side information for improving the preference\nmodeling of users; and (ii) they are publicly available in\nMusicBraiz and DBpedia.\n4.4.2 Similarity Metrics\nRegarding A1, each item is represented as one-dimensional\nvectors in which their single component is the year normal-\nized to the range [0;1]. More formally, for a given contem-\nporaneity (i.e. year) a2A1associated to artist i\nf1(a; i) =a\u0000min(A1)\nmax( A1)\u0000min(A1)416 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017where min(A1)andmax( A1)returns the minimum and\nmaximum contemporaneity values of A1respectively.\nNow, the similarity of two items iandjregarding their\nrespective contemporaneities ai; aj2A1is simply com-\nputed as the inverse of their distance, i.e.,\nsimA1(i; j) = 1\u0000(~i\u0000~j) (4)\nwhere ~i= (f1(ai; i))and~jis deﬁned analogously. The\nintuition here is that artists from the same time epoch tend\nto produce similar music.\nConcerning A2, the feature extraction f2is basically an\nidentity function, i.e., a function that returns the same value\nfound in the raw data. So, similarly to A1, items are rep-\nresented as one-dimensional vectors whose single compo-\nnent is a nominal value (e.g. country name). For com-\nputing similarities between items under this representation\nwe used the Occurrence Frequency (OF) metric [1] which\nbesides being suitable for categorical data, exploits the fre-\nquency of items with regard to the associated features. This\nmetric assigns 1 to items having the same feature value and\ndifferent scores to mismatches. A mismatch between less\nfrequent items regarding their features yields a lower value\nthan a mismatch between more frequent items. For ex-\nample, if we compare two artists from USA and England\nrespectively, two countries with a large number of artists\nin the dataset, their similarity will be greater than artists of\nUSA and Costa Rica, since Costa Rica has probably much\nless artists than USA. The idea is basically to avoid having\nzero similarities whenever a mismatch occurs.\nThe equation below deﬁnes OF (which is used as\nsimA2) of two items i; jregarding Locality :\nOFA2(i; j) =8\n<\n:1 if~i=~j\n1\n1+logjIj\nfreqA2(i)\u0002logjIj\nfreqA2(j))otherwise :\n(5)\nwhere freq A2(i)returns the number of artists having the\nsame feature value (country name in this case) as item i.\nRegarding A3,f3is analogous to f2, but the item rep-\nresentation is slightly different. Since Gender is actually a\ncombination of two aspects, items are represented by vec-\ntors containing two nominal values: ( type,gender ) where\ngender can be male or female if the artist type is person ,\nandneither if the artist is associated to any other type. For\ncalculating the similarity between items i; jwe apply equa-\ntion 5 separately for type andgender and take the average.\nMore formally,\nsimA3(i; j) =OF type(i; j) +OF gender(i; j)\n2(6)\nAs an example, the similarity between a male singer and\na female singer should return a greater similarity than be-\ntween a male singer and a band.\nAs an artist can be associated to multiple music gen-\nres, the feature extraction function for A4is the function\nf4:A4\u0002I!f0;1gthat indicates the genres that are as-\nsociated to a given artist. Each item is then represented by\na binary vector of genres. To measure similarity betweentwo items i; jregarding this aspect we use the well known\ncosine similarity function, i.e.,\nsimA4(i; j) =cos(~i;~j): (7)\nNow, we can ﬁnally instantiate functions ga(\u0001)and\nha(\u0001)introduced in section 2. For a given aspect a2A, a\nrecommendation list RandIu:\nga(R) =X\n(i;j)2R\u0002Rji6=jsima(i; j) (8)\nha(R; Iu) =X\n(i;j)2R\u0002Iuji6=jsima(i; j) (9)\n5. EV ALUATION\nIn this section we evaluate the effectiveness of MOAD\nfor music recommendation. All code for the evaluation is\navailable publicly online5.\n5.1 Data Collection and Preparation\nWe used three publicly available data sources: Last.fm,\nMusic Brainz and DBpedia. Last.fm is a social network\nwhere users share data about their listening habits. In par-\nticular, we have used a recent Last.fm dataset published\nand made available by Schedl [13].\nFor extracting the aspects about the artists available in\nthe Last.fm dataset, we have used Music Brainz, a music\nencyclopedia that provides rich metadata about artists and\nalbums. From Music Brainz we extracted Contemporane-\nity,Gender , and Locality . Finally, we used DBpedia to\nextract the Music Genre(s) of each artist.\nAfter enriching the artists with the aforementioned as-\npects, genres associated with less than 5 artists were re-\nmoved, as well as artists with no genre at all. Finally, a\nsample of 1,000 users from the Last.fm dataset was ran-\ndomly selected for the experiments. This number of users\nis in line with the size of other very well known and used\nLast.fm datasets in the music recommendation community,\nsee for example the Last-fm - 1K users dataset6[3]. In our\nsample, 3 users had no history and were thus discarded. We\nalso generated a train/test time split where the ﬁrst 80% of\nartists listened by each user was used for training and the\nremaining 20% for testing.\nOur sample includes the following statistics: 14,415\nartists, a median of 140 artists listened per user, 10 genders,\n437 localities, 847 genres, and contemporaneity ranging\nfrom 1212 to 2014. Figure 1 shows a ﬂow chart summa-\nrizing our approach.\n5.2 Evaluation Protocol and Metrics\nIdeally, the aspects to keep and the ones to be diversiﬁed\nshould be provided by the users themselves. Since this\nkind of online experiment can be very demanding, it will\nbe left for future work. In this paper, we will simulate some\n5https://github.com/ricooliveira/moad.git\n6http://www.dtic.upf.edu/ ˜ocelma/\nMusicRecommendationDataset/lastfm-1K.htmlProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 417possible scenarios and evaluate the extent to which MOAD\ncan cope with them. The evaluation scenario is the follow-\ning: one aspect is chosen to be diversiﬁed while the others\nare kept constant. Since we have four aspects, we end up\nwith four evaluation scenarios. For example, a given user\nwants artist recommendations that are diverse regarding lo-\ncality (artists countries) while maintaining genre, gender\nand contemporaneity constant (i.e. similar to previous lis-\ntened artists w.r.t. these aspects).\nRegarding evaluation metrics, we use diversity and\nafﬁnity deﬁned in equations 1 and 2 respectively. Diver-\nsityis actually related to a popular diversity metric known\nas ILD (Intra-List Diversity) [19] while afﬁnity tries to as-\nsess the relevance of the recommendations regarding the\naspects that were held constant. While diversity is only\nevaluated on the ﬁnal recommendation list, afﬁnity is eval-\nuated on the test set.\n# Diversify Maintain afﬁnity\n1 Cont. Gender, Locality, Genre\n2 Gender Cont., Locality, Genre\n3 Locality Cont., Gender, Genre\n4 Genre Cont., Gender, Locality\nTable 1 . Evaluated recommendation scenarios\nFigure 1 . Flow chart of MOAD.\n5.3 Baseline Algorithms\nWe have chosen baselines that are well known for promot-\ning diversiﬁcation without degrading accuracy. Since all of\nthem compute item similarities in order to select the items\nthat will compose the ﬁnal recommendation list, we used\nthe same similarity measures deﬁned in section 4.4. This\nmeans that each baseline focused on the diversiﬁcation of\nthe same aspect, depending on the recommendation sce-\nnario chosen, as MOAD.\nMore speciﬁcally, we compare our approach to the fol-\nlowing baselines:\n\u000fTopic Diversiﬁcation (TD): Receives an initial rec-\nommendation list of 50 items where the ﬁrst item ofthis list goes to the recommendation ﬁnal list in or-\nder to preserve accuracy. Next, items are selected in\nan iterative and greedy fashion based on their rank-\nings in the initial list and the similarity to the items\nalready in the ﬁnal list regarding the aspect of inter-\nest [19].\n\u000fRelevance-based eXplicit Query Aspect Diversi-\nﬁcation (RxQUAD): Performs a re-ranking over 50\nprecomputed items. During the greedy iterative step\neach item receives a score based on two factors:\ngiven an input aspect, the relevance of the aspect\nfor the user and the relevance of the aspect for the\nitem [15].\n\u000fUser-Based Collaborative Filtering (UBCF): We\nalso included a standard user-based collaborative ﬁl-\ntering based on k-nearest neighbors. Notice that this\nalgorithm is not aimed to promoting diversiﬁcation.\nWe have used the RankSys tool where these three algo-\nrithm are implemented [14, 15].\n5.4 Parameter Tuning\nFor determining suitable values to the NSGA-II parame-\nters such as the number of generations, size of the popula-\ntion and probability of mutation, we extracted a subsample\nof 30 users from the Last.fm experimental dataset and de-\ntermined a ﬁxed scenario to perform some executions of\nour approach. We use the third scenario of Table 1 and\nN= 10 ,nGen = 10 ,mProb = 0:1andcProb = 0:9\nas default values. For tuning a particular parameter, we\nﬁxed the other parameters to its default values and var-\nied the target parameter until no signiﬁcant changes were\nfound in the evaluation metrics. Due to the non-normality\nof the data, Wilcoxon non-parametric test was used. The\ntests determined that the ideal values to the parameters are\nN= 10 ,nGen = 50 and the default values to mProb and\ncProb .\n6. RESULTS AND DISCUSSION\nFor assessing MOAD variability across different execu-\ntions, we run MOAD 10 times on scenario 3 of Table 1 and\nperformed a Kruskal-Wallis test, which reported that there\nare no signiﬁcant changes within the executions. We thus\nassume that other scenarios will follow a similar trend and\nthus only make one run of the algorithm for each subse-\nquent scenario. The baseline algorithms are deterministic,\nso running them multiple times is not necessary.\nFor assessing the results we calculated diversity and\nafﬁnity for all users in the experimental dataset. As men-\ntioned earlier afﬁnity was computed in the test set of each\nuser. The boxplots of the results for all scenarios in Table 1\nare shown in Figure 2. Notice that MOAD achieved better\nresults than the baselines in all scenarios, considering both\nevaluation metrics, with a small variability across users.\nWilcoxon tests are performed comparing MOAD to each\nbaseline and all the differences are statistically signiﬁcant\nalbeit small in some cases.418 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n●●●●●●●●●●●●●●\n●●●●●●\n●●●\n●●●●●●●\n●●●●\n●●●●●●●\n●●\n●●●●\n●●\n●●●●●●●●●●●●●\n●●\n●●●\n●●●●●●\n●●●●●●●●●●●●●●●●\n●●●●\n●●●●\n●●●●\n●●●●●●●●●●\n●●●\n●\n●●●●\n●\n●●●●●●●●●●●●●\n●●●●●●\n●●●\n●\n●●\n●●\n●●\n●●●●\n●●\n●\n●\n●●\n●●●●●●●\n●\n●\n●●●●\n●●\n●●\n●●\n●●●●●●\n●●\n●\n●●\n●●●●●●●●●●\n●●●\n●●●\n●●●●●\n●●●\n●\n●●●●●●\n●●\n●●●●●●●●●●●●●●●●●●●●\n●●●●●●●●●●●●●●\n●●●●●●\n●●●●\n●●●●●●●●●●●●●\n●●●●●\n●\n●●●●\n●\n●●●\n●●\n●●●●●●●\n●\n●●●●●●\n●●●●●●\n●●●●\n●●●●●●●●●●●\n●●●\n●●●●\n●●●●●●●●●●●\n●●\n●●●●●●\n●●●●●●●●●●\n●●●\n●●●●●●●\n●●●●\n●●●●\n●●●●\n●●●●●●●●●\n●●●●●●●\n●●●●●●●\n●●●●●●●●●●●●\n●●●●●\n●●●●●●●●\n●●●●●●●●●●●●●●\n●●●\n●●●●●●\n●●●●●●●●●●\n●●●●●\n●●\n●\n●●●●\n●●●●●●●\n●●●●●●●●●\n●●●●●●●●●●\n●●●●●●●●\n●●●●●\n●●●\n●●\n●●●●●●●●●●●●●●●●\n●●●●●\n●●●\n●●●\n●●●●\n●\n●●●●●●●●\n●\n●●\n●●\n●●●●●●●●\n●●●\n●●●\n●\n●●●●●\n●\n●●\n●\n●●\n●●●\n●●●\n●●●●\n●●●●\n●●●\n●●●●\n●●\n●●●●\n●●●\n●●\n●●●\n●●●\n●●●\n●●●●●\n●●●\n●\n●●●●●●\n●●\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n●●●●\n●●●\n●●\n●●\n●●●●\n●●●●●●●●\n●\n●●\n●\n●●●●●\n●\n●●●●●\n●●●●●●●●●\n●●●●●\n●●\n●●●●●●●\n●●●Locality GenreContemporaneity Gender\nDiversity Affinity Diversity Affinity0.000.250.500.751.00\n0.000.250.500.751.00valueAlgorithm\nUBCF\nTD\nRxQuAD\nMOADFigure 2 . Comparison of MOAD and baselines\nWhen diversifying contemporaneity ,diversity shows\nvery low results for all algorithms. This may be explained\nby the range of the aspect, mentioned on subsection 5.1,\nwhich turns the time difference, even between artists from\ndifferent decades, very small when normalized. Gender\nis the scenario where the smallest differences between\nMOAD and the compared baselines are observed. A pos-\nsible explanation to this is the fact that Gender aspect has\nonly 10 possible values which does not leave much room\nfor diversiﬁcation. Locality and genre are the scenarios\nwhere we observed the highest gains, which is probably\nassociated to the large number of possible values for these\naspects.\nTable 2 shows an example of a real Last.fm experimen-\ntal user, receiving recommendations of three algorithms,\nbased on scenario 3: UBCF; TD, the best baseline in sce-\nnario 3 and MOAD. In the simulations, MOAD obtained\nan improvement of 23.7% in diversity compared to TD.\nThis means that MOAD may bring from 2 to 3 more artists\nfrom different countries than TD.\n7. CONCLUSIONS AND FUTURE WORK\nIn this paper we proposed MOAD, an approach for music\nrecommendations that are at the same time diverse, regard-\ning certain aspects, and similar to user preferences con-\ncerning other aspects. We cast this problem as a multi-\nobjective optimization task and use an efﬁcient algorithm\nbased on evolutionary algorithms for solving it. We have\ndeﬁned speciﬁc similarity functions for each considered\naspect and performed several simulations using real world\ndata to assess MOAD performance. We have compared\nMOAD to other well known baselines from the literature\nand show that it provides better results in all evaluated sce-Artist\nEngland\nUSA\nSweden\nIceland\nCanada\nItalia\nIndia\nMexico\nNorwayUBCFPink Floyd X\nIn Flames X\nDream Theater X\nIron Maiden X\nMegadeth X\nColdplay X\nBj¨ork X\nThe Beatles X\nThe Cure X\nMot¨orhead XTDPink Floyd X\nBj¨ork X\nJohnny Cash X\nIn Flames X\nClint Mansell X\nZo¨e Keating X\nDream Theater X\nIron Maiden X\nMegadeth X\nColdplay XMOADFilms of Colour X\nOndskapt X\nBeautiful Sin X\nI Ribelli X\nPlanes Mistaken for Stars X\nBanda Machos X\nJeff Healey X\nCole Swindell X\nMubarak Begum X\nFuck Buttons X\nTable 2 . Top-10 recommendations for a real Last.fm user\nnarios.\nAs future work, we intend to run MOAD in all possible\ncombinations of aspects to diversify and to hold constant\nand with more generations. We also intend to perform an\nonline experiment with real users. Finally, we intend to ap-\nproach the same problem under the perspective of MIR re-\nplacing the user’s history by a set of input artists, allowing\nthe user to discover new artists based on her instantaneous\ninformation needs.\nAcknowledgement: This work was partially funded by the\nEU-BR BigSea project (MCTI/RNP 3rd Coordinated Call).Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 4198. REFERENCES\n[1] Shyam Boriah, Varun Chandola, and Vipin Kumar.\nSimilarity measures for categorical data: A compara-\ntive evaluation. In Proceedings of the 2008 SIAM Inter-\nnational Conference on Data Mining , pages 243–254.\nSIAM, 2008.\n[2] Roc ´ıo L Cecchini, Carlos M Lorenzetti, Ana G Ma-\nguitman, and N ´elida B Brignole. Multiobjective evolu-\ntionary algorithms for context-based search. Journal of\nthe Association for Information Science and Technol-\nogy, 61(6):1258–1274, 2010.\n[3] O. Celma. Music Recommendation and Discovery in\nthe Long Tail . Springer, 2010.\n[4] Laizhong Cui, Peng Ou, Xianghua Fu, Zhenkun Wen,\nand Nan Lu. A novel multi-objective evolutionary al-\ngorithm for recommendation systems. Journal of Par-\nallel and Distributed Computing , 2016.\n[5] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and\nTAMT Meyarivan. A fast and elitist multiobjective ge-\nnetic algorithm: Nsga-ii. IEEE transactions on evolu-\ntionary computation , 6(2):182–197, 2002.\n[6] Tommaso Di Noia, Vito Claudio Ostuni, Jessica\nRosati, Paolo Tomeo, and Eugenio Di Sciascio. Adap-\ntive diversity in recommender systems. In IIR, 2015.\n[7] Komal Kapoor, Vikas Kumar, Loren Terveen, Joseph A\nKonstan, and Paul Schrater. I like to explore some-\ntimes: Adapting to dynamic user novelty preferences.\nInProceedings of the 9th ACM Conference on Recom-\nmender Systems , pages 19–26. ACM, 2015.\n[8] Onur K ¨uc ¸¨uktunc ¸, Erik Saule, Kamer Kaya, and ¨Umit V\nC ¸ ataly ¨urek. Diversiﬁed recommendation on graphs:\npitfalls, measures, and algorithms. In Proceedings of\nthe 22nd international conference on World Wide Web ,\npages 715–726. ACM, 2013.\n[9] Ali Ouni, Raula Gaikovina Kula, Marouane Kessen-\ntini, Takashi Ishio, Daniel M German, and Katsuro\nInoue. Search-based software library recommendation\nusing multi-objective optimization. Information and\nSoftware Technology , 83:55–75, 2017.\n[10] Elias Pampalk and Masataka Goto. Musicsun: A new\napproach to artist recommendation. In ISMIR , pages\n101–104, 2007.\n[11] Marco Tulio Ribeiro, Anisio Lacerda, Adriano Veloso,\nand Nivio Ziviani. Pareto-efﬁcient hybridization for\nmulti-objective recommender systems. In Proceedings\nof the sixth ACM conference on Recommender systems ,\npages 19–26. ACM, 2012.\n[12] Marco Tulio Ribeiro, Nivio Ziviani, Edleno Silva De\nMoura, Itamar Hata, Anisio Lacerda, and Adriano\nVeloso. Multiobjective pareto-efﬁcient approaches for\nrecommender systems. ACM Transactions on Intelli-\ngent Systems and Technology (TIST) , 5(4):53, 2015.[13] Markus Schedl. The lfm-1b dataset for music retrieval\nand recommendation. In Proceedings of the 2016 ACM\non International Conference on Multimedia Retrieval ,\nICMR ’16, pages 103–110, New York, NY , USA,\n2016. ACM.\n[14] Saul Vargas, Pablo Castells, and David Vallet. Intent-\noriented diversity in recommender systems. In Pro-\nceedings of the 34th International ACM SIGIR Confer-\nence on Research and Development in Information Re-\ntrieval , SIGIR ’11, pages 1211–1212, New York, NY ,\nUSA, 2011. ACM.\n[15] Sa ´ul Vargas, Pablo Castells, and David Vallet. Explicit\nrelevance models in intent-oriented information re-\ntrieval diversiﬁcation. In Proceedings of the 35th Inter-\nnational ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval , SIGIR ’12, pages\n75–84, New York, NY , USA, 2012. ACM.\n[16] Shanfeng Wang, Maoguo Gong, Haoliang Li, and\nJunwei Yang. Multi-objective optimization for long\ntail recommendation. Knowledge-Based Systems ,\n104:145–155, 2016.\n[17] Mi Zhang and Neil Hurley. Avoiding monotony: im-\nproving the diversity of recommendation lists. In Pro-\nceedings of the 2008 ACM conference on Recom-\nmender systems , pages 123–130. ACM, 2008.\n[18] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng,\nand Shuzi Niu. Learning for search result diversiﬁca-\ntion. In Proceedings of the 37th international ACM SI-\nGIR conference on Research & development in infor-\nmation retrieval , pages 293–302. ACM, 2014.\n[19] Cai-Nicolas Ziegler, Sean M McNee, Joseph A Kon-\nstan, and Georg Lausen. Improving recommendation\nlists through topic diversiﬁcation. In Proceedings of\nthe 14th international conference on World Wide Web ,\npages 22–32. ACM, 2005.\n[20] Yi Zuo, Maoguo Gong, Jiulin Zeng, Lijia Ma, and\nLicheng Jiao. Personalized recommendation based\non evolutionary multi-objective optimization [research\nfrontier]. IEEE Computational Intelligence Magazine ,\n10(1):52–62, 2015.420 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Multi-Label Music Genre Classification from Audio, Text and Images Using Deep Features.",
        "author": [
            "Sergio Oramas",
            "Oriol Nieto",
            "Francesco Barbieri",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417427",
        "url": "https://doi.org/10.5281/zenodo.1417427",
        "ee": "https://zenodo.org/records/1417427/files/OramasNBS17.pdf",
        "abstract": "Music genres allow to categorize musical items that share common characteristics. Although these categories are not mutually exclusive, most related research is traditionally focused on classifying tracks into a single class. Further- more, these categories (e.g., Pop, Rock) tend to be too broad for certain applications. In this work we aim to ex- pand this task by categorizing musical items into multiple and fine-grained labels, using three different data modal- ities: audio, text, and images. To this end we present MuMu, a new dataset of more than 31k albums classified into 250 genre classes. For every album we have collected the cover image, text reviews, and audio tracks. Addition- ally, we propose an approach for multi-label genre classi- fication based on the combination of feature embeddings learned with state-of-the-art deep learning methodologies. Experiments show major differences between modalities, which not only introduce new baselines for multi-label genre classification, but also suggest that combining them yields improved results.",
        "zenodo_id": 1417427,
        "dblp_key": "conf/ismir/OramasNBS17",
        "keywords": [
            "Music genres",
            "categorize musical items",
            "common characteristics",
            "mutually exclusive",
            "traditional research",
            "single class",
            "related research",
            "categories",
            "too broad",
            "expanding task"
        ],
        "content": "MULTI-LABEL MUSIC GENRE CLASSIFICATION FROM AUDIO, TEXT,\nAND IMAGES USING DEEP FEATURES\nSergio Oramas1, Oriol Nieto2, Francesco Barbieri3, Xavier Serra1\n1Music Technology Group, Universitat Pompeu Fabra\n2Pandora Media Inc.\n3TALN Group, Universitat Pompeu Fabra\nfsergio.oramas, francesco.barbieri, xavier.serra g@upf.edu, onieto@pandora.com\nABSTRACT\nMusic genres allow to categorize musical items that share\ncommon characteristics. Although these categories are not\nmutually exclusive, most related research is traditionally\nfocused on classifying tracks into a single class. Further-\nmore, these categories (e.g., Pop, Rock) tend to be too\nbroad for certain applications. In this work we aim to ex-\npand this task by categorizing musical items into multiple\nand ﬁne-grained labels, using three different data modal-\nities: audio, text, and images. To this end we present\nMuMu , a new dataset of more than 31k albums classiﬁed\ninto 250 genre classes. For every album we have collected\nthe cover image, text reviews, and audio tracks. Addition-\nally, we propose an approach for multi-label genre classi-\nﬁcation based on the combination of feature embeddings\nlearned with state-of-the-art deep learning methodologies.\nExperiments show major differences between modalities,\nwhich not only introduce new baselines for multi-label\ngenre classiﬁcation, but also suggest that combining them\nyields improved results.\n1. INTRODUCTION\nMusic genres are useful labels to classify musical items\ninto broader categories that share similar musical, regional,\nor temporal characteristics. Dealing with large collections\nof music poses numerous challenges when retrieving and\nclassifying information [3]. Music streaming services tend\nto offer catalogs of tens of millions of tracks, for which\ntasks such as music classiﬁcation are of utmost importance.\nMusic genre classiﬁcation is a widely studied problem in\nthe Music Information Research (MIR) community [40].\nHowever, almost all related work is concentrated in multi-\nclass classiﬁcation of music items into broad genres (e.g.,\nPop, Rock), assigning a single label per item. This is prob-\nlematic since there may be hundreds of more speciﬁc mu-\nsic genres [33], and these may not be necessarily mutually\nc\rSergio Oramas1, Oriol Nieto2, Francesco Barbieri3,\nXavier Serra1. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Sergio Oramas1, Oriol\nNieto2, Francesco Barbieri3, Xavier Serra1. “Multi-label Music Genre\nClassiﬁcation from audio, text, and images using Deep Features”, 18th In-\nternational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.exclusive (i.e., a song could be Pop, and at the same time\nhave elements from Deep House and a Reggae grove). In\nthis work we aim to advance the ﬁeld of music classiﬁ-\ncation by framing it as multi-label genre classiﬁcation of\nﬁne-grained genres.\nTo this end, we present MuMu , a new large-scale mul-\ntimodal dataset for multi-label music genre classiﬁcation.\nMuMu contains information of roughly 31k albums clas-\nsiﬁed into one or more 250 genre classes. For every al-\nbum we analyze the cover image, text reviews, and audio\ntracks, with a total number of approximately 147k audio\ntracks and 447k album reviews. Furthermore, we exploit\nthis dataset with a novel deep learning approach to learn\nmultiple genre labels for every album using different data\nmodalities (i.e., audio, text, and image). In addition, we\ncombine these modalities to study how the different com-\nbinations behave.\nResults show how feature learning using deep neu-\nral networks substantially surpasses traditional approaches\nbased on handcrafted features, reducing the gap between\ntext-based and audio-based classiﬁcation [29]. Moreover,\nan extensive comparative of different deep learning archi-\ntectures for audio classiﬁcation is provided, including the\nusage of a dimensionality reduction approach that yields\nimproved results. Finally, we show how the late fusion of\nfeature vectors learned from different modalities achieves\nbetter scores than each of them individually.\n2. RELATED WORK\nMost published music genre classiﬁcation approaches rely\non audio sources [2, 40]. Traditional techniques typically\nuse handcrafted audio features, such as Mel Frequency\nCepstral Coecients (MFCCs) [20], as input of a machine\nlearning classiﬁer (e.g., SVM) [39, 44]. More recent deep\nlearning approaches take advantage of visual representa-\ntions of the audio signal in form of spectrograms. These\nvisual representations are used as input to Convolutional\nNeural Networks (CNNs) [5, 6, 8, 9, 34], following ap-\nproaches similar to those used for image classiﬁcation.\nText-based approaches have also been explored for this\ntask. For instance, in [13, 29] album customer reviews\nare used as input for the classiﬁcation, whereas in [4, 22]\nsong lyrics are employed. By contrast, there are a limited\nnumber of papers dealing with image-based genre classi-23ﬁcation [18]. Most multimodal approaches for this task\nfound in the literature combine audio and song lyrics as\ntext [16, 27]. Moreover, the combination of audio and\nvideo has also been explored [37]. However, the authors\nare not aware of published multimodal approaches for mu-\nsic genre classiﬁcation that involve deep learning.\nMulti-label classiﬁcation is a widely studied problem\n[14, 43]. Despite the scarcity in terms of approaches for\nmulti-label classiﬁcation of music genres [36, 46], there is\na long tradition in MIR for tag classiﬁcation, which is a\nmulti-label problem [5, 46].\n3. MULTIMODAL DATASET\nTo the best of our knowledge, there are no publicly avail-\nable large-scale datasets that encompass audio, images,\ntext, and multi-label annotations. Therefore, we present\nMuMu , a new Multimodal Music dataset with multi-\nlabel genre annotations that combines information from\nthe Amazon Reviews dataset [23] and the Million Song\nDataset (MSD) [1]. The former contains millions of al-\nbum customer reviews and album metadata gathered from\nAmazon.com. The latter is a collection of metadata and\nprecomputed audio features for a million songs.\nTo map the information from both datasets we use Mu-\nsicBrainz1. For every album in the Amazon dataset, we\nquery MusicBrainz with the album title and artist name to\nﬁnd the best possible match. Matching is performed using\nthe same methodology described in [30], following a pair-\nwise entity resolution approach based on string similarity.\nFollowing this approach, we were able to map 60% of the\nAmazon dataset. For all the matched albums, we obtain the\nMusicBrainz recording ids of their songs. With these, we\nuse an available mapping from MSD to MusicBrainz2to\nobtain the subset of recordings present in the MSD. From\nthe mapped recordings, we only keep those associated with\na unique album. This process yields the ﬁnal set of 147,295\nsongs, which belong to 31,471 albums.\nThe song features provided by the MSD are not gener-\nally suitable for deep learning [45], so we instead use in our\nexperiments audio previews between 15 and 30 seconds re-\ntrieved from 7digital.com . For the mapped set of al-\nbums, there are 447,583 customer reviews in the Amazon\nDataset. In addition, the Amazon Dataset provides further\ninformation about each album, such as genre annotations,\naverage rating, selling rank, similar products, cover image\nurl, etc. We employ the provided image url to gather the\ncover art of all selected albums. The mapping between the\nthree datasets (Amazon, MusicBrainz, and MSD), genre\nannotations, data splits, text reviews, and links to images\nare released as the MuMu dataset3. Images and audio ﬁles\ncan not be released due to copyright issues.\n1https://musicbrainz.org/\n2http://labs.acousticbrainz.org/million-song-dataset-echonest-archive\n3https://www.upf.edu/web/mtg/mumu3.1 Genre Labels\nAmazon has its own hierarchical taxonomy of music gen-\nres, which is up to four levels in depth. In the ﬁrst level\nthere are 27 genres, and almost 500 genres overall. In our\ndataset, we keep the 250 genres that satisfy the condition\nof having been annotated in at least 12 albums. Every al-\nbum in Amazon is annotated with one or more genres from\ndifferent levels of the taxonomy. The Amazon Dataset con-\ntains complete information about the speciﬁc branch from\nthe taxonomy used to classify each album. For instance, an\nalbum annotated as Traditional Pop comes with the com-\nplete branch information Pop / Oldies / Traditional Pop . To\nexploit either the taxonomic and the co-occurrence infor-\nmation, we provide every item with the labels of all their\nbranches. For example, an album classiﬁed as Jazz / Vocal\nJazz andPop / Vocal Pop is annotated in MuMu with the\nfour labels: Jazz, V ocal Jazz, Pop, and V ocal Pop. There\nare in average 5.97 labels for each song (3.13 standard de-\nviation).\nTable 1 . Top-10 most and least represented genres\nGenre % of albums Genre % of albums\nPop 84.38 Tributes 0.10\nRock 55.29 Harmonica Blues 0.10\nAlternative Rock 27.69 Concertos 0.10\nWorld Music 19.31 Bass 0.06\nJazz 14.73 European Jazz 0.06\nDance & Electronic 12.23 Piano Blues 0.06\nMetal 11.50 Norway 0.06\nIndie & Lo-Fi 10.45 Slide Guitar 0.06\nR&B 10.10 East Coast Blues 0.06\nFolk 9.69 Girl Groups 0.06\nThe labels in the dataset are highly unbalanced, follow-\ning a distribution which might align well with those found\nin real world scenarios. In Table 1 we see the top 10 most\nand least represented genres and the percentage of albums\nannotated with each label. The unbalanced character of the\ngenre annotations poses an interesting challenge for music\nclassiﬁcation that we also aim to exploit. Among the mul-\ntiple possibilities that this dataset may offer to the MIR\ncommunity, we focus our work on the multi-label classiﬁ-\ncation problem, described next.\n4. MULTI-LABEL CLASSIFICATION\nIn multi-label classiﬁcation, multiple target labels may be\nassigned to each classiﬁable instance. More formally:\ngiven a set of nlabels L=fl1; l2; : : : ; l ng, and a set of\nmitems I=fi1; i2; : : : ; i mg, we aim to model a function\nfable to associate a set of clabels to every item in I, where\nc2[1; n]varies for every item.\nDeep learning approaches are well-suited for this prob-\nlem, as these architectures allow to have multiple outputs\nin their ﬁnal layer. The usual architecture for large multi-\nlabel classiﬁcation using deep learning ends with a logistic\nregression layer with sigmoid activations evaluated with\nthe cross-entropy loss, where target labels are encoded as\nhigh-dimensional sparse binary vectors [42]. This method,\nwhich we refer as LOGISTIC , implies the assumption that24 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017the classes are statistically independent (which is not the\ncase in music genres).\nA more recent approach [7], relies on matrix factor-\nization to reduce the dimensionality of the target labels.\nThis method makes use of the interrelation between labels,\nembedding the high-dimensional sparse labels onto lower-\ndimensional vectors. In this case, the target of the network\nis a dense lower-dimensional vector which can be learned\nusing the cosine proximity loss, as these vectors tend to be\nl2-normalized. We denote this technique as COSINE , and\nwe provide a more formal deﬁnition next.\n4.1 Labels Factorization\nLetMbe the binary matrix of items Iand labels Lwhere\nmij= 1 ifiiis annotated with label ljandmij= 0 oth-\nerwise. Using M, we calculate the matrix Xof Positive\nPointwise Mutual Information (PPMI) for the set of labels\nL. Given Lias the set of items annotated with label li, the\nPPMI between two labels is deﬁned as:\nX(li; lj) =max\u0012\n0;logP(Li; Lj)\nP(Li)P(Lj)\u0013\n(1)\nwhere P(Li; Lj) =jLi\\Ljj=jIjandP(Li) =jLij=jIj.\nThe PPMI matrix Xis then factorized using Singular\nValue Decomposition (SVD) such that X\u0019U\u0006V, where\nUandVare unitary matrices, and \u0006is a diagonal matrix\nof singular values. Let \u0006dbe the diagonal matrix formed\nfrom the top dsingular values, and let Udbe the matrix\nproduced by selecting the corresponding columns from U,\nthe matrix Cd=Ud\u0001p\u0006dcontains the label factors of d\ndimensions. Finally, we obtain the matrix of item factors\nFdasFd=Cd\u0001MT. Further information on this technique\nmay be found in [17].\nFactors present in matrices CdandFdare embedded in\nthe same space. Thus, a distance metric such as cosine\ndistance can be used to obtain distance measures between\nitems and labels. Similar labels are grouped in the space,\nand at the same time, items with similar sets of labels are\nnear each other. These properties can be exploited in the\nlabel prediction problem.\n4.2 Evaluation Metrics\nThe evaluation of multi-label classiﬁcation is not necessar-\nily straightforward. Evaluation measures vary according to\nthe output of the system. In this work we are interested\nin measures that deal with probabilistic outputs, instead\nof binary. The Receiver Operating Characteristic (ROC)\ncurve is a graphical plot that illustrates the performance of\na binary classiﬁer system as its discrimination threshold is\nvaried. Thus, the area under the ROC curve (AUC) is often\ntaken as an evaluation measure to compare such systems.\nWe selected this metric to compare the performance of the\ndifferent approaches as it has been widely used for genre\nand tag classiﬁcation problems [5, 9].\nThe output of a multi-label classiﬁer is a label-item ma-\ntrix. Thus, it can be evaluated either from the labels or\nthe items perspective. We can measure how accurate the\nclassiﬁcation is for every label, or how well the labels areranked for every item. In this work, the former point of\nview is evaluated with the AUC measure, which is com-\nputed for every label and then averaged. We are interested\nin classiﬁcation models that strengthen the diversity of la-\nbel assignments. As the taxonomy is composed of broad\ngenres which are over-represented in the dataset (see Ta-\nble 1), and more speciﬁc subgenres (e.g., V ocal Jazz, Brit-\npop), we want to measure whether the classiﬁer is focusing\nonly on over-represented genres, or on more ﬁne-grained\nones. To this end, catalog coverage (also known as aggre-\ngated diversity) is an evaluation measure used in the ex-\ntreme multi-label classiﬁcation [14] and the recommender\nsystems [32] communities. Coverage@k measures the per-\ncentage of normalized unique labels present in the top k\npredictions made by an algorithm across all test items. Val-\nues of k= 1;3;5are typically employed in multi-label\nclassiﬁcation.\n5. ALBUM GENRE CLASSIFICATION\nIn this section we exploit the multimodal nature of the\nMuMu dataset to address the multi-label classiﬁcation task.\nMore speciﬁcally, and since each modality on this set (i.e.,\ncover image, text reviews, and audio tracks) is associated\nwith a music album, our task focuses on album classiﬁca-\ntion.\n5.1 Audio-based Approach\nA music album is composed by a series of audio tracks,\neach of which may be associated with different genres. In\norder to learn the album genre from a set of audio tracks we\nsplit the problem into three steps: (1) track feature vectors\nare learned while trying to predict the genre labels of the\nalbum from every track in a deep neural network. (2) Track\nvectors of each album are averaged to obtain album feature\nvectors. (3) Album genres are predicted from the album\nfeature vectors in a shallow network where the input layer\nis directly connected to the output layer.\nIt is common in MIR to make use of CNNs to learn\nhigher-level features from spectrograms. These represen-\ntations are typically contained in RF\u0002Nmatrices withF\nfrequency bins and Ntime frames. In this work we com-\npute 96 frequency bin, log-compressed constant-Q trans-\nforms (CQT) [38] for all the tracks in our dataset using\nlibrosa [24] with the following parameters: audio sam-\npling rate at 22050 Hz, hop length of 1024 samples, Hann\nanalysis window, and 12 bins per octave. In addition, log-\namplitude scaling is applied to the CQT spectrograms. Fol-\nlowing a similar approach to [45], we address the vari-\nability of the length Nacross songs by sampling one 15-\nseconds long patch from each track, resulting in the ﬁxed-\nsize input to the CNN.\nTo learn the genre labels we design a CNN with four\nconvolutional layers and experiment with different number\nof ﬁlters, ﬁlter sizes, and output conﬁgurations (see Sec-\ntion 6.1).Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 255.2 Text-based Approach\nIn the presented dataset, each album has a variable num-\nber of customer reviews. We use an approach similar\nto [13, 29] for genre classiﬁcation from text, where all re-\nviews from the same album are aggregated into a single\ntext. The aggregated result is truncated at 1000 characters,\nthus balancing the amount of text per album, as more pop-\nular artists tend to have a higher number of reviews. Then\nwe apply a Vector Space Model approach (VSM) with tf-\nidf weighting [47] to create a feature vector for each album.\nAlthough word embeddings [25] with CNNs are state-of-\nthe-art in many text classiﬁcation tasks [15], a traditional\nVSM approach is used instead, as it seems to perform bet-\nter when dealing with large texts [31]. The vocabulary size\nis limited to 10k as it was a good balance of network com-\nplexity and accuracy.\nFurthermore, a second approach is proposed based on\nthe addition of semantic information, similarly to the\nmethod described in [29]. To semantically enrich the al-\nbum texts, we adopted Babelfy, a state-of-the-art tool for\nentity linking [26], a task to associate, for a given textual\nfragment candidate, the most suitable entry in a reference\nKB. Babelfy maps words from a given text to Wikipedia4.\nIn Wikipedia, categories are used to organize resources.\nWe take all the Wikipedia categories of entities identiﬁed\nby Babelfy in each document and add them at the end of\nthe text as new words. Then a VSM with tf-idf weight-\ning is applied to the semantically enriched texts, where the\nvocabulary is also limited to 10k terms. Note that either\nwords or categories may be part of this vocabulary.\nFrom this representation, a feed forward network with\ntwo dense layers of 2048 neurons and a Rectiﬁed Linear\nUnit (ReLU) after each layer is trained to predict the genre\nlabels in both LOGISTIC and COSINE conﬁgurations.\n5.3 Image-based Approach\nEvery album in the dataset has an associated cover art im-\nage. To perform music genre classiﬁcation from these\nimages, we use Deep Residual Networks (ResNets) [11].\nThey are the state-of-the-art in various image classiﬁcation\ntasks like Imagnet [35] and Microsoft COCO [19]. ResNet\nis a common feed-forward CNN with residual learning ,\nwhich consists on bypassing two or more convolution lay-\ners. We employ a slightly modiﬁed version of the original\nResNet5: the scaling and aspect ratio augmentation are\nobtained from [41], the photometric distortions from [12],\nand weight decay is applied to all weights and biases.\nThe network we use is composed of 101 layers (ResNet-\n101), initialized with pretrained parameters learned on Im-\nageNet. This is our starting point to ﬁnetune the network\non the genre classiﬁcation task. Our ResNet implementa-\ntion has a logistic regression ﬁnal layer with sigmoid acti-\nvations and uses the binary cross entropy loss.\n4http://wikipedia.org\n5https://github.com/facebook/fb.resnet.torch/5.4 Multimodal approach\nWe aim to combine all of these different types of data\ninto a single model. There are several works claiming\nthat learning data representations from different modali-\nties simultaneously outperforms systems that learn them\nseparately [10, 28]. However, recent work in multimodal\nlearning with audio and text in the context of music rec-\nommendation [31] reﬂects the contrary. We have observed\nthat deep networks are able to ﬁnd an optimal minimum\nvery fast from text data. However, the complexity of the\naudio signal can signiﬁcantly slow down the training pro-\ncess. Simultaneous learning may under-explore one of the\nmodalities, as the stronger modality may dominate quickly.\nThus, learning each modality separately warrants that the\nvariability of the input data is fully represented in each of\nthe feature vectors.\nTherefore, from each modality network described\nabove, we separately obtain an internal feature represen-\ntation for every album after training them on the genre\nclassiﬁcation task. Concretely, the input to the last fully\nconnected layer of each network becomes feature vector\nfor its respective modality. Given a set of feature vectors,\nl2-regularization is applied on each of them. They are then\nconcatenated into a single feature vector, which becomes\nthe input to a simple Multi Layer Perceptron (MLP), where\nthe input layer is directly connected to the output layer.\nThe output layer may have either a LOGISTIC or a COSINE\nconﬁguration.\n6. EXPERIMENTS\nWe apply the architectures deﬁned in the previous section\nto the MuMu dataset. The dataset is divided as follows:\n80% for training, 10% for validation, and 10% for test.\nWe ﬁrst evaluate every modality in isolation in the multi-\nlabel genre classiﬁcation task. Then, from each modality,\na deep feature vector is obtained for the best performing\napproach in terms of AUC. Finally, the three modality vec-\ntors are combined in a multimodal network. All results\nare reported in Table 2. Performance of the classiﬁcation\nis reported in terms of AUC score and Coverage@k with\nk= 1;3;5. The training speed per epoch and number\nof network hyperparameters are also reported. All source\ncode and data splits used in our experiments are available\non-line6.\nThe matrix of album genre annotations of the training\nand validation sets is factorized using the approach de-\nscribed in Section 4.1, with a value of d= 50 dimensions.\nFrom the set of album factors, those annotated with a sin-\ngle label from the top level of the taxonomy are plotted in\nFigure 1 using t-SNE dimensionality reduction [21]. It can\nbe seen how the different albums are properly clustered in\nthe factor space according to their genre.\n6.1 Audio Classiﬁcation\nWe explore three network design parameters: convolu-\ntion ﬁlter size, number of ﬁlters per convolutional layer,\n6https://github.com/sergiooramas/tartarus26 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Table 2 . Results for Multi-label Music Genre Classiﬁcation of Albums\nModality Target Settings Params Time AUC C@1 C@3 C@5\nAUDIO LOGISTIC TIMBRE -MLP 0.01M 1s 0.792 0.04 0.14 0.22\nAUDIO LOGISTIC LOW -3X3 0.5M 390s 0.859 0.14 0.34 0.54\nAUDIO LOGISTIC HIGH -3X3 16.5M 2280s 0.840 0.20 0.43 0.69\nAUDIO LOGISTIC LOW -4X96 0.2M 140s 0.851 0.14 0.32 0.48\nAUDIO LOGISTIC HIGH -4X96 5M 260s 0.862 0.12 0.33 0.48\nAUDIO LOGISTIC LOW -4X70 0.35M 200s 0.871 0.05 0.16 0.34\nAUDIO LOGISTIC HIGH -4X70 7.5M 600s 0.849 0.08 0.23 0.38\nAUDIO COSINE LOW -3X3 0.33M 400s 0.864 0.26 0.47 0.65\nAUDIO COSINE HIGH -3X3 15.5M 2200s 0.881 0.30 0.54 0.69\nAUDIO COSINE LOW -4X96 0.15M 135s 0.860 0.19 0.40 0.52\nAUDIO COSINE HIGH -4X96 4M 250s 0.884 0.35 0.59 0.75\nAUDIO COSINE LOW -4X70 0.3M 190s 0.868 0.26 0.51 0.68\nAUDIO (A) COSINE HIGH -4X70 6.5M 590s 0.888 0.35 0.60 0.74\nTEXT LOGISTIC VSM 25M 11s 0.905 0.08 0.20 0.37\nTEXT LOGISTIC VSM+S EM 25M 11s 0.916 0.10 0.25 0.44\nTEXT COSINE VSM 25M 11s 0.901 0.53 0.44 0.90\nTEXT (T) COSINE VSM+S EM 25M 11s 0.917 0.42 0.70 0.85\nIMAGE (I) LOGISTIC RESNET 1.7M 4009s 0.743 0.06 0.15 0.27\nA + T LOGISTIC MLP 1.5M 2s 0.923 0.10 0.40 0.64\nA + I LOGISTIC MLP 1.5M 2s 0.900 0.10 0.38 0.66\nT + I LOGISTIC MLP 1.5M 2s 0.921 0.10 0.37 0.63\nA + T + I LOGISTIC MLP 2M 2s 0.936 0.11 0.39 0.66\nA + T COSINE MLP 0.3M 2s 0.930 0.43 0.74 0.86\nA + I COSINE MLP 0.3M 2s 0.896 0.32 0.57 0.76\nT + I COSINE MLP 0.3M 2s 0.919 0.43 0.74 0.85\nA + T + I COSINE MLP 0.4M 2s 0.931 0.42 0.72 0.86\nNumber of network hyperparameters, epoch training time, AUC-ROC, and catalog\ncoverage at k= 1;3;5for different settings and modalities.\nFigure 1 . t-SNE of album factors.\nand target layer. For the ﬁlter size we compare three ap-\nproaches: square 3x3 ﬁlters as in [5], a ﬁlter of 4x96 that\nconvolves only in time [45], and a musically motivated ﬁl-\nter of 4x70, which is able to slightly convolve in the fre-\nquency domain [34]. To study the width of the convolu-\ntional layers we try with two different settings: HIGH with\n256, 512, 1024, and 1024 in each layer respectively, and\nLOW with 64, 128, 128, 64 ﬁlters. Max-pooling is applied\nafter each convolutional layer. Finally, we use the two dif-\nferent network targets deﬁned in Section 4, LOGISTIC and\nCOSINE . We empirically observed that dropout regulariza-\ntion only helps in the HIGH plus COSINE conﬁgurations.\nTherefore we applied dropout with a factor of 0.5 to these\nconﬁgurations, and no dropout to the others.\nApart from these conﬁgurations, a baseline approach is\nadded. This approach consists in a traditional audio-basedapproach for genre classiﬁcation based on the audio de-\nscriptors present in the MSD [1]. More speciﬁcally, for\neach song we aggregate four different statistics of the 12\ntimbre coefﬁcient matrices: mean, max, variance, and l2-\nnorm. The obtained 48 dimensional feature vectors are fed\ninto a feed forward network as the one described in Sec-\ntion 5.4 with LOGISTIC output. This approach is denoted\nasTIMBRE -MLP.\nThe results show that CNNs applied over audio spec-\ntrograms clearly outperform traditional approaches based\non handcrafted features. We observe that the TIMBRE -\nMLP approach achieves 0.792 of AUC, contrasting with the\n0.888 from the best CNN approach. We note that the LO-\nGISTIC conﬁguration obtains better results when using a\nlower number of ﬁlters per convolution ( LOW ). Conﬁgu-\nrations with fewer ﬁlters have less parameters to optimize,\nand their training processes are faster. On the other hand, in\nCOSINE conﬁgurations we observe that the use of a higher\nnumber of ﬁlters tends to achieve better performance. It\nseems that the ﬁne-grained regression of the factors bene-\nﬁts from wider convolutions. Moreover, we observe that\n3x3 square ﬁlter settings have lower performance, need\nmore time to train, and have a higher number of param-\neters to optimize. By contrast, networks using time con-\nvolutions only (4 X96) have a lower number of parameters,\nare faster to train, and achieve comparable performance.\nFurthermore, networks that slightly convolve across the\nfrequency bins (4 X70) achieve better results with only a\nslightly higher number of parameters and training time.\nFinally, we observe that the COSINE regression approach\nachieves better AUC scores in most conﬁgurations, and\nalso their results are more diverse in terms of catalog cov-\nerage.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 27Figure 2 . Particular of the t-SNE of randomly selected\nimage vectors from ﬁve of the most frequent genres.\n6.2 Text Classiﬁcation\nFor text classiﬁcation, we obtain two feature vectors as\ndescribed in Section 5.2: one built from the texts VSM,\nand another built from the semantically enriched texts\nVSM+S EM. Both feature vectors are trained in the multi-\nlabel genre classiﬁcation task using the two output conﬁg-\nurations LOGISTIC and COSINE .\nResults show that the semantic enrichment of texts\nclearly yields better results in terms of AUC and diver-\nsity. Furthermore, we observe that the COSINE conﬁgura-\ntion slightly outperforms LOGISTIC in terms of AUC, and\ngreatly in terms of catalog coverage. The text-based results\nare overall slightly superior to the audio-based ones.\nWe also studied the information gain of words in the\ndifferent genres. We observed that genre labels present in\nthe texts have important information gain values. How-\never, it is remarkable that band is a very informative word\nfor Rock, song for Pop, and dope ,rhymes , and beats are\ndiscriminative features for Rap albums. Place names have\nalso important weights, as Jamaica for Reggae, Nashvile\nfor Country, or Chicago for Blues.\n6.3 Image Classiﬁcation\nResults show that genre classiﬁcation from images has\nlower performance in terms of AUC and catalog coverage\ncompared to the other modalities. Due to the use of an al-\nready pre-trained network with a logistic output (ImageNet\n[35]) as initialization of the network, it is not straightfor-\nward to apply the COSINE conﬁguration. Therefore, we\nonly report results for the LOGISTIC conﬁguration.\nIn Figure 2 a set of cover images of ﬁve of the most fre-\nquent genres in the dataset is shown using t-SNE over the\nobtained image feature vectors. In the left top corner the\nResNet recognizes women faces on the foreground, which\nseems to be common in Country albums (red). The jazz\nalbums (green) on the right are all clustered together prob-\nably thanks to the uniform type of clothing worn by the\npeople of their covers. Therefore, the visual style of thecover seems to be informative when recognizing the album\ngenre. For instance, many classical music albums include\nan instrument in the cover, and Dance & Electronics covers\nare often abstract images with bright colors, rarely includ-\ning human faces.\n6.4 Multimodal Classiﬁcation\nFrom the best performing approaches in terms of AUC\nof each modality (i.e., A UDIO /COSINE /HIGH -4X70,\nTEXT /COSINE / VSM+S EMand I MAGE /LOGISTIC /\nRESNET), a feature vector is obtained as described in Sec-\ntion 5.4. Then, these three feature vectors are aggregated\nin all possible combinations, and genre labels are predicted\nusing the MLP network described in Section 5.4. Both out-\nput conﬁgurations LOGISTIC and COSINE are used in the\nlearning phase, and dropout of 0.7 is applied in the CO-\nSINE conﬁguration.\nResults suggest that the combination of modalities out-\nperforms single modality approaches. As image features\nare learned using a LOGISTIC conﬁguration, they seem to\nimprove multimodal approaches with LOGISTIC conﬁgu-\nration only. Multimodal approaches that include text fea-\ntures tend to improve the results. Nevertheless, the best\napproaches are those that exploit the three modalities of\nMuMu . C OSINE approaches have similar AUC than LO-\nGISTIC approaches but a much better catalog coverage,\nthanks to the spatial properties of the factor space.\n7. CONCLUSIONS\nAn approach for multi-label music genre classiﬁcation us-\ning deep learning architectures has been proposed. The\napproach was applied to audio, text, image data, and their\ncombination. For its assessment, MuMu , a new multi-\nmodal music dataset with over 31k albums and 135k songs\nhas been gathered. We showed how representation learn-\ning approaches for audio classiﬁcation outperform tradi-\ntional handcrafted feature based approaches. Moreover,\nwe compared the effect of different design parameters of\nCNNs in audio classiﬁcation. Text-based approaches seem\nto outperform other modalities, and beneﬁt from the se-\nmantic enrichment of texts via entity linking. While the\nimage-based classiﬁcation yielded the lowest performance,\nit helped to improve the results when combined with other\nmodalities. Multimodal approaches appear to outperform\nsingle modality approaches, and the aggregation of the\nthree modalities achieved the best results. Furtheremore,\nthe dimensionality reduction of target labels led to better\nresults, not only in terms of accuracy, but also in terms of\ncatalog coverage.\nThis paper is an initial attempt to study the multi-label\nclassiﬁcation problem of music genres from different per-\nspectives and using different data modalities. In addition,\nthe release of the MuMu dataset opens up a number of un-\nexplored research possibilities. In the near future we aim\nto modify the ResNet to be able to learn latent factors from\nimages as we did in other modalities and apply the same\nmultimodal approach to other MIR tasks.28 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20178. ACKNOWLEDGMENTS\nThis work was partially funded by the Spanish Ministry of\nEconomy and Competitiveness under the Maria de Maeztu\nUnits of Excellence Programme (MDM-2015-0502).\n9. REFERENCES\n[1] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInISMIR , 2011.\n[2] Dmitry Bogdanov, Alastair Porter, Perfecto Herrera,\nand Xavier Serra. Cross-collection evaluation for mu-\nsic classiﬁcation tasks. In ISMIR , 2016.\n[3] Michael A Casey, Remco Veltkamp, Masataka Goto,\nMarc Leman, Christophe Rhodes, and Malcolm\nSlaney. Content-based music information retrieval:\nCurrent directions and future challenges. Proceedings\nof the IEEE , 96(4):668–696, 2008.\n[4] Kahyun Choi, Jin Ha Lee, and J Stephen Downie.\nWhat is this song about anyway?: Automatic classi-\nﬁcation of subject using user interpretations and lyrics.\nInProceedings of the 14th ACM/IEEE-CS Joint Con-\nference on Digital Libraries , pages 453–454. IEEE\nPress, 2014.\n[5] Keunwoo Choi, George Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional neural\nnetworks. ISMIR , 2016.\n[6] Keunwoo Choi, George Fazekas, Mark Sandler,\nand Kyunghyun Cho. Convolutional recurrent neu-\nral networks for music classiﬁcation. arXiv preprint\narXiv:1609.04243 , 2016.\n[7] Franc ¸ois Chollet. Information-theoretical label embed-\ndings for large-scale image classiﬁcation. CoRR , pages\n1–10, 2016.\n[8] Sander Dieleman, Phil ´emon Brakel, and Benjamin\nSchrauwen. Audio-based music classiﬁcation with a\npretrained convolutional network. In ISMIR , 2011.\n[9] Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In Acoustics, Speech and\nSignal Processing (ICASSP), 2014 IEEE International\nConference on , pages 6964–6968. IEEE, 2014.\n[10] Matthias Dorfer, Andreas Arzt, and Gerhard Widmer.\nTowards score following in sheet music images. IS-\nMIR, 2016.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\nProceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition , pages 770–778, 2016.\n[12] Andrew G Howard. Some improvements on deep con-\nvolutional neural network based image classiﬁcation.\narXiv preprint arXiv:1312.5402 , 2013.[13] Xiao Hu, J Stephen Downie, Kris West, and Andreas F\nEhmann. Mining music reviews: Promising prelimi-\nnary results. In ISMIR , 2005.\n[14] Himanshu Jain, Yashoteja Prabhu, and Manik Varma.\nExtreme multi-label loss functions for recommenda-\ntion, tagging, ranking & other missing label applica-\ntions. In Proceedings of the 22nd ACM SIGKDD In-\nternational Conference on Knowledge Discovery and\nData Mining , pages 935–944. ACM, 2016.\n[15] Yoon Kim. Convolutional Neural Networks for Sen-\ntence Classiﬁcation. Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP 2014) , pages 1746–1751, 2014.\n[16] Cyril Laurier, Jens Grivolla, and Perfecto Herrera.\nMultimodal music mood classiﬁcation using audio\nand lyrics. In Machine Learning and Applications,\n2008. ICMLA’08. Seventh International Conference\non, pages 688–693. IEEE, 2008.\n[17] Omer Levy and Yoav Goldberg. Neural word embed-\nding as implicit matrix factorization. In Advances in\nneural information processing systems , pages 2177–\n2185, 2014.\n[18] Janis Libeks and Douglas Turnbull. You can judge an\nartist by an album cover: Using images for music an-\nnotation. IEEE MultiMedia , 18(4):30–37, 2011.\n[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects\nin context. In European Conference on Computer Vi-\nsion, pages 740–755. Springer, 2014.\n[20] Beth Logan et al. Mel frequency cepstral coefﬁcients\nfor music modeling. In ISMIR , 2000.\n[21] Laurens van der Maaten and Geoffrey Hinton. Visu-\nalizing data using t-sne. Journal of Machine Learning\nResearch , 9(Nov):2579–2605, 2008.\n[22] Rudolf Mayer, Robert Neumayer, and Andreas Rauber.\nRhyme and style features for musical genre classiﬁca-\ntion by song lyrics. In ISMIR , 2008.\n[23] Julian McAuley, Christopher Targett, Qinfeng Shi,\nand Anton Van Den Hengel. Image-based recommen-\ndations on styles and substitutes. In Proceedings of\nthe 38th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval ,\npages 43–52. ACM, 2015.\n[24] Brian Mcfee, Colin Raffel, Dawen Liang, Daniel P W\nEllis, Matt Mcvicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and Music Signal Analysis in Python.\nProc. of the 14th Python in Science Conf. , (Scipy):1–7,\n2015.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 29[25] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. Distributed representations of\nwords and phrases and their compositionality. In Ad-\nvances in Neural Information Processing Systems 26 ,\npages 3111–3119. 2013.\n[26] Andrea Moro, Alessandro Raganato, and Roberto Nav-\nigli. Entity Linking meets Word Sense Disambigua-\ntion: A Uniﬁed Approach. Transactions of the Associ-\nation for Computational Linguistics , 2:231–244, 2014.\n[27] Robert Neumayer and Andreas Rauber. Integration of\ntext and audio features for genre classiﬁcation in mu-\nsic information retrieval. In European Conference on\nInformation Retrieval , pages 724–727. Springer, 2007.\n[28] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan\nNam, Honglak Lee, and Andrew Y Ng. Multimodal\ndeep learning. In Proceedings of the 28th international\nconference on machine learning (ICML-11) , pages\n689–696, 2011.\n[29] Sergio Oramas, Luis Espinosa-Anke, Aonghus Lawlor,\net al. Exploring customer reviews for music genre clas-\nsiﬁcation and evolutionary studies. In ISMIR , 2016.\n[30] Sergio Oramas, Francisco G ´omez, Emilia G ´omez, and\nJoaqu ´ın Mora. Flabase: Towards the creation of a ﬂa-\nmenco music knowledge base. In ISMIR , 2015.\n[31] Sergio Oramas, Oriol Nieto, Mohamed Sordo, and\nXavier Serra. A Deep Multimodal Approach for Cold-\nstart Music Recommendation. ArXiv e-prints , June\n2017.\n[32] Sergio Oramas, Vito Claudio Ostuni, Tommaso Di\nNoia, Xavier Serra, and Eugenio Di Sciascio. Sound\nand music recommendation with knowledge graphs.\nACM Transactions on Intelligent Systems and Technol-\nogy (TIST) , 8(2):21, 2016.\n[33] Franc ¸ois Pachet and Daniel Cazaly. A taxonomy of\nmusical genres. In Content-Based Multimedia Infor-\nmation Access-Volume 2 , pages 1238–1245, 2000.\n[34] Jordi Pons, Thomas Lidy, and Xavier Serra. Experi-\nmenting with musically motivated convolutional neu-\nral networks. In Content-Based Multimedia Indexing\n(CBMI), 2016 14th International Workshop on , pages\n1–6. IEEE, 2016.\n[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan\nKrause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein,\nAlexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. International\nJournal of Computer Vision (IJCV) , 115(3):211–252,\n2015.\n[36] Chris Sanden and John Z. Zhang. Enhancing multi-\nlabel music genre classiﬁcation through ensemble tech-\nniques. In Proceedings of the 34th International ACMSIGIR Conference on Research and Development in In-\nformation Retrieval , SIGIR ’11, pages 705–714, New\nYork, NY , USA, 2011. ACM.\n[37] Alexander Schindler and Andreas Rauber. An audio-\nvisual approach to music genre classiﬁcation through\naffective color features. In European Conference on In-\nformation Retrieval , pages 61–67. Springer, 2015.\n[38] Christian Sch ¨orkhuber and Anssi Klapuri. Constant-\nQ transform toolbox for music processing. 7th Sound\nand Music Computing Conference , (JANUARY):3–64,\n2010.\n[39] Klaus Seyerlehner, Markus Schedl, Tim Pohle, and Pe-\nter Knees. Using block-level features for genre clas-\nsiﬁcation, tag classiﬁcation and music similarity esti-\nmation. Submission to Audio Music Similarity and Re-\ntrieval Task of MIREX , 2010, 2010.\n[40] Bob L Sturm. A survey of evaluation in music genre\nrecognition. In International Workshop on Adaptive\nMultimedia Retrieval , pages 29–66. Springer, 2012.\n[41] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-\nmanet, Scott Reed, Dragomir Anguelov, Dumitru Er-\nhan, Vincent Vanhoucke, and Andrew Rabinovich. Go-\ning deeper with convolutions. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition , pages 1–9, 2015.\n[42] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. Rethinking the in-\nception architecture for computer vision. In Proceed-\nings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 2818–2826, 2016.\n[43] Grigorios Tsoumakas and Ioannis Katakis. Multi-label\nclassiﬁcation: An overview. International Journal of\nData Warehousing and Mining , 3(3), 2006.\n[44] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nspeech and audio processing , 10(5):293–302, 2002.\n[45] Aaron van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommenda-\ntion. NIPS’13 Proceedings of the 26th International\nConference on Neural Information Processing Sys-\ntems, pages 2643–2651, 2013.\n[46] Fei Wang, Xin Wang, Bo Shao, Tao Li, and Mitsunori\nOgihara. Tag integrated multi-label music style classi-\nﬁcation with hypergraph. In ISMIR , 2009.\n[47] Justin Zobel and Alistair Moffat. Exploring the simi-\nlarity space. ACM SIGIR Forum , 32(1):18–34, 1998.30 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Sampling Variations of Sequences for Structured Music Generation.",
        "author": [
            "François Pachet",
            "Alexandre Papadopoulos",
            "Pierre Roy"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416588",
        "url": "https://doi.org/10.5281/zenodo.1416588",
        "ee": "https://zenodo.org/records/1416588/files/PachetPR17.pdf",
        "abstract": "Recently, machine-learning techniques have been success- fully used for the generation of complex artifacts such as music or text. However, these techniques are still unable to capture and generate artifacts that are convincingly struc- tured. In particular, musical sequences do not exhibit pat- tern structure, as typically found in human composed mu- sic. We present an approach to generate structured se- quences, based on a mechanism for sampling efficiently variations of musical sequences. Given an input sequence and a statistical model, this mechanism uses belief propa- gation to sample a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism uses local fields to bias the generation. We show experimentally that sampled sequences are in- deed closely correlated to the standard musical similarity function defined by Mongeau and Sankoff. We then show how this mechanism can be used to implement composi- tion strategies that enforce arbitrary structure on a musical lead sheet generation problem. We illustrate our approach with a convincingly structured generated lead sheet in the style of the Beatles.",
        "zenodo_id": 1416588,
        "dblp_key": "conf/ismir/PachetPR17",
        "keywords": [
            "machine-learning",
            "complex-artifacts",
            "musical-sequences",
            "statistical-model",
            "structured-sequences",
            "belief-propagation",
            "sampling",
            "local-fields",
            "musical-similarity",
            "composition-strategies"
        ],
        "content": "SAMPLING V ARIATIONS OF SEQUENCES FOR STRUCTURED MUSIC\nGENERATION\nFranc ¸ois Pachet\nSony CSL Paris\npachet@gmail.comAlexandre Papadopoulos\nUPMC Univ Paris 06, UMR 7606, LIP6\nalexandre.papadopoulos@lip6.frPierre Roy\nSony CSL Paris\nroypie@gmail.com\nABSTRACT\nRecently, machine-learning techniques have been success-\nfully used for the generation of complex artifacts such as\nmusic or text. However, these techniques are still unable to\ncapture and generate artifacts that are convincingly struc-\ntured. In particular, musical sequences do not exhibit pat-\ntern structure, as typically found in human composed mu-\nsic. We present an approach to generate structured se-\nquences, based on a mechanism for sampling efﬁciently\nvariations of musical sequences. Given an input sequence\nand a statistical model, this mechanism uses belief propa-\ngation to sample a set of sequences whose distance to the\ninput sequence is approximately within speciﬁed bounds.\nThis mechanism uses local ﬁelds to bias the generation.\nWe show experimentally that sampled sequences are in-\ndeed closely correlated to the standard musical similarity\nfunction deﬁned by Mongeau and Sankoff. We then show\nhow this mechanism can be used to implement composi-\ntion strategies that enforce arbitrary structure on a musical\nlead sheet generation problem. We illustrate our approach\nwith a convincingly structured generated lead sheet in the\nstyle of the Beatles.\n1. INTRODUCTION1\nRecent advances in machine learning, especially deep re-\ncurrent networks such as LSTMs, led to major improve-\nments in the quality of music generation [7, 10]. They\nachieve spectacular performance for short musical frag-\nments. However, musical structure typically exceeds the\nscope of statistical models. As Waite recently wrote, the\nmusic produced by recurrent models tend to lack a sense\nof direction and becomes boring after a short while [15].\nPionneering works on music composition with LSTMs al-\nready showed how some structure, such as chord struc-\nture [6] or metrical structure [5] can be spontaneously cap-\n1Authors are listed alphabetically: Pachet originated the general problem and contributed\nmusical examples; Papadopoulos developed and implemented the technical solution especially\nthe integration with the regular belief propagation model, devised and performed the evaluation\nprocedure; Roy brought the original idea and the technical solution, developed the ﬁrst prototype\nand the structured lead sheet generation procedures.\nc\rFranc ¸ois Pachet, Alexandre Papadopoulos, Pierre Roy.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Franc ¸ois Pachet, Alexandre Pa-\npadopoulos, Pierre Roy. “Sampling Variations of Sequences for Struc-\ntured Music Generation”, 18th International Society for Music Informa-\ntion Retrieval Conference, Suzhou, China, 2017.tured, but the general problem of generating music with\nrepetitive long-term structure remains open. In this paper,\nwe propose a method to explicitly enforce such structure in\na controlled way, in a “templagiarism” fashion [2, p. 49].\nMusical structure is the overall organisation of a compo-\nsition into sections, phrases, and patterns, very much like\nthe organisation of a text. The structure of musical pieces\nis scarcely, if ever, linear as it essentially relies on the\nrepetition of these elements, possibly altered. For exam-\nple, songs are decomposed into repeating sections, called\nverses and choruses, and each section is constructed with\nrepeating patterns. It has been shown that the listeners’\nemotional arousal responses to music is correlated with the\ndegree of similarity between musical fragments (high for\nrepetitions, moderate for variations, and least for contrast-\ning segments) [9]. In fact, the striking speech to song il-\nlusion discovered by [4] shows that repetition truly creates\nmusic, for instance by turning speech into music. This is\nfurther conﬁrmed by [11] who observed that inserting arbi-\ntrary repetition in non-repetitive music improves listeners\nrating and conﬁdence that the music was written by a hu-\nman composer.\nFigure 1 . The last eight bars of “Strangers in the Night” .\nVariations are a speciﬁc type of repetition, in which the\noriginal melody is altered in its rhythm, pitch sequence,\nand/or harmony. Variations are used to create diversity and\nsurprise by subtle, unexpected changes in a repetition. The\nsong “Strangers in the Night” is a typical 32-bar form with\nan AABA structure consisting of four 8-bar sections. The\nthree Asections are variations of each other. The last A\nsection, shown in Figure 1, consists of a two-bar cell which\nis repeated three times. Each occurrence is a subtle vari-\nation of the preceding one. The second occurrence (bars\n3-4) is a mere transposition of the original pattern by one\ndescending tone. The third instance (bars 5-6) is also trans-\nposed, but with a slight modiﬁcation in the melody, which\ncreates a surprise and concludes the song. Bars 5-6 are\nboth a variation of the original pattern in bars 1-2. Cur-\nrent models for music generation fail to reproduce such167long-range similarities between musical patterns. In this\nexample, it is statistically unlikely that bars 5-6 be almost\nidentical to bars 1-2.\nOur goal is to generate such structured musical pieces\nfrom statistical models. Our approach is to impose a prede-\nﬁned musical structure that speciﬁes explicitly repetitions\nand variations of patterns and sections, and use a statistical\nmodel to generate music that “instantiates” this structure.\nIn this approach, musical structure is viewed as a procedu-\nral process, external to the statistical model.\nOur approach subsumes previous attempts at generating\nmusic with an imposed long-term structure with Markov\nmodels such as [1]. Their approach lacks both a variation\nmechanism and a constrained Markov model. As a result,\nit is limited to strict repetitions of patterns. Furthermore,\nthe use of ad hoc joining techniques to glue copied frag-\nments, violates the Markov model, resulting in unnatural\ntransitions.\nAn essential ingredient to implementing our approach\nis a mechanism to generate variations of a given musical\npattern from a statistical model. Although it is impossi-\nble to characterise formally the notion of variation, it was\nshown that some measures of melodic similarity are efﬁ-\ncient at detecting variations of a theme [12]. We propose\nto use such a similarity measure in a generative context to\nsample from a Markov model, patterns that are similar to a\ngiven pattern. This method is related to work on stochas-\ntic edit distances [3, 14], but is integrated as a constraint\nin a more general model for the generation of musical se-\nquences [13]. Moreover, our approach relies on an exist-\ning similarity measure rather than on labeled data (pairs of\nthemes and related variations), which is not available. Sim-\nilar approaches exist in the context of text generation. For\nexample, [8] propose a model using a technique based on\nskip vectors. They train a model that learns the similarity\nbetween sentences. Using this model, they can predict the\nsemantic relatedness of two sentences, a standard similar-\nity measure for text, but they can also generate sentences\nsimilar to an existing sentence.\nWe remind the Mongeau & Sankoff similarity mea-\nsure [12] between melodies, and then, we describe our\nmodel for sampling melodic variations based on this simi-\nlarity, which we validate experimentally, Finally, we show\nexamples of variations of a melody, and a longer, struc-\ntured musical piece generated with an imposed structure.\n2. MELODIC SIMILARITY\nThe traditional string edit distance considers three editing\noperations: substitution ,deletion , and insertion of a char-\nacter. Mongeau and Sankoff [12] add two operations mo-\ntivated by the speciﬁcities of musical sequences, and in-\nspired by the time compression and expansion operations\nconsidered in time warping . The ﬁrst operation, called\nfragmentation , involves the replacement of one note by\nseveral, shorter notes. Similarly, the consolidation opera-\ntion, is the replacement of several notes by a single, longer\nnote. Mongeau and Sankoff proposed an algorithm to com-\npute the similarity between melodies in polynomial time.Considering melodies as sequences of notes, the algorithm,\nbased on dynamic programming, computes MGD (A;B),\nthe measure of similarity between the sequences of notes\nAandB. Note that this is not a distance, in particular\nMGD (A;B)is not necessarily equal to MGD (B;A).\nThe Mongeau & Sankoff measure is well-adapted to the\ndetection of variations, but has a minor weakness: there is\nno penalty associated with fragmenting a long note into\nseveral shorter notes of same pitch and same total dura-\ntion. The same applies to consolidation. This is not suited\nto a generative context, as fragmentation or consolidation\nchange the resulting melody.\nIn the dynamic programming recurrence equation given\nin their paper [12], Mongeau and Sankoff introduce var-\nious weight functions, denoting predeﬁned local weights\nassociated with the basic editing operations (substitution,\ndeletion, insertion, fragmentation and consolidation). We\nmodify the original measure by adding a penalty pto the\nweights of the consolidation and fragmentation operations.\nThe weight associated with a fragmentation of a note ai\ninto a sequence of notes bj\u0000k+1;:::;b jis:\nwfrag(ai;bj\u0000k+1;:::;b j) =wpitch(ai;bj\u0000k+1;:::;b j)\n+k1n(ai;bj\u0000k+1;:::;b j) +p\nFor consolidation, a similar extra-weight is added. The\nconsolidation weight is deﬁned by:\nwcons(ai;bj\u0000k+1;:::;b j) =wpitch(ai;bj\u0000k+1;:::;b j)\n+k1n(ai;bj\u0000k+1;:::;b j) +p:\n3. A MODEL FOR THE GENERATION OF\nMELODIC V ARIATIONS\nGiven an original theme , i.e. a melodic fragment, we gen-\nerate variations of this theme by sampling a speciﬁc graph-\nical model. This graphical model is a modiﬁed version of\nthe general model of lead sheets introduced by [13]. We\nnow brieﬂy describe this general model and explain how\nwe bias it to produce only melodies at a controlled Mon-\ngeau & Sankoff distance from the theme, the core technical\ncontribution of this paper. For full explanations and imple-\nmentation details, we refer the reader to [13].\n3.1 The Model of Lead Sheets\nThe overall model comprises two graphical models , one\nfor chord sequences, one for melodies. Both models are\nbased on a factor graph that combines a Markov model\nwith a ﬁnite state automaton. The Markov model, trained\non a corpus of lead sheets, provides the stylistic model.\nThe automaton represents hard temporal constraints that\nthe generated sequences should satisfy, such as metrical\nproperties (e.g., an imposed total duration) or user imposed\ntemporal constraints.\nEach factor graph is made of a sequence of variables,\nrepresented with circles, encoding the sequence of ele-\nments, related to unary and binary factors, represented by\nsquares. In this model, a variable is not associated with168 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017e; tUser constraints\nfrom chords to melodyHarmonic synchronisation\non melodyUser constraintsFactor graph for Chords\nFactor graph for Melodye; t f\nfon chords\nFigure 2 . The two-voice model for lead sheet generation\na speciﬁc temporal position in the sequence, but the val-\nuesit takes speciﬁes its temporal position. Each value is\na chord or a note e, with a ﬁxed duration d(e)along with\nits temporal position tin the sequence. This is a very pow-\nerful property of this model. It allows us to specify unary\ntemporal constraints, e.g., the second bar should start with\na rest. It also allows us to specify harmonic relations be-\ntween the chord sequence and the melody, e.g., the note\nat timetshould be compatible with the chord at time t.\nCrucially, we will exploit this property to implement our\nvariation mechanism.\nA binary factor is a conditional probability\nf((e;t)j(e0;t0))on transitions between elements. In [13],\nthe authors use binary factors to combine the Markov\ntransition probabilities with the ﬁnite-state automaton\ntransitions. Harmonic relationship between chords and\nnotes are also speciﬁed by binary factors.\nThe graphical model deﬁnes a distribution\np(e1;:::;e n)over the sequence of variables deﬁned\nby the product of all unary and binary factors. A belief\npropagation-based procedure samples successively the two\nmodels by taking into account partially ﬁlled fragments\nand propagating their effect to all empty sections.\n3.2 Generating Variations of a Theme\nWe introduce an extra binary factor \f(ejt;e0): the proba-\nbility of placing element eat timetand preceded by ele-\nmente0. We will use \fto implement the variation mech-\nanism. In practice, this additional binary factor is simply\nmultiplied with the existing binary factors, without affect-\ning the structure of the model on Figure 2. The probability\np0of a sequence in the resulting model becomes:\np0(e1;:::;e n) =p(e1;:::;e n)nY\ni=2\f(eijt;ei\u00001):\nWe set the value of \f(ejt;e0)according to a “localised”\nsimilarity measure between the sequence [e0;e]and the\nfragment of the theme between t\u0000d(e0)andt+d(e).\nBiases are set so that a bias of 1 does not modify the prob-\nability of putting element eat timetaftere0, and a bias less\nthan 1 decreases this probability.\nThe lead sheet in Figure 3 shows the ﬁrst four bars of\n“Solar” by Miles Davis. Suppose we train a lead sheet\nmodel on a corpus of all songs by Miles Davis. Sampling\nFigure 3 . The ﬁrst four bars of “Solar” , by Miles Davis.\nthis model produces new lead sheets in the style of Miles\nDavis, but not necessarily similar to Solar speciﬁcally. To\nfavour sequences with the same notes as the theme is to set\nthe\ffactors so that:\n\u000f\f(njt;n0) = 1 if the melodic fragment consisting of\nnoten0followed by note nat positiontappears in\nthe theme, e.g., we set \f(C5jt= 1:5;rest ) = 1 for\nnote C 5dotted quarter note ;\n\u000f\f(njt;n0)<1otherwise, and the value of \f(njt;n0)\nwill be set to very small values (close to zero), if the\nmelodic fragment made by n0andnat timetis very\ndifferent, musically, from the corresponding melodic\nfragment in the theme, e.g., \f(F44jt= 1:5;G25)\u001c\n1. On the contrary, if the two fragments are very\nsimilar, musically, the value of \f(njt;n0)will be set\nto a value closer to 1, e.g., \f(C5jt= 1:5;rest )\u001d0\nfor note C 5quarter note .\nMore precisely, we evaluate the similarity between each\npossible note nat a given position t, preceded by note n0in\nthe generated sequence, and the notes in the theme around\npositiont. We then set each bias \f(njt;n0)based on this\nsimilarity measure.\nTechnically, for every candidate note n, we consider\nall potential temporal positions tand all potential prede-\ncessorsn0. We compute MGD ([n0;n];t), the Mongeau\n& Sankoff similarity between the two-note melody [n0;n]\nand the melodic fragment of the theme between time\nt\u0000d(n0)andt+d(n), whered(n)is the duration of the\nnoten, i.e. the melodic fragment that would be replaced by\nplacing the melody [n0;n]at timet\u0000d(n0). The notes of\nthe theme that overlap the time interval [t\u0000d(n0);t+d(n)]\nare trimmed so that the extracted melody has the same du-\nration as the candidate notes. Similarly MGD ([n0];t)de-\nnotes the similarity of the one-note sequence [n0]starting\natt\u0000d(n0). We call those similarities localised Mongeau\n& Sankoff similarity measures. The idea is that the simi-\nlarity measure obtained by summing those localised mea-\nsures over a complete sequence approximates the actual\nMongeau & Sankoff similarity. This will be conﬁrmed ex-\nperimentally in the next section.\nTo convert the similarity measure into a weight between\n0 and 1, we rescale those values to the [0;1]interval, and\nthen invert their order, so that a value of 1 is the closest\nto the theme, and 0 the furthest away. Finally, we expo-\nnentiate the result, so that the logarithm of the product of\nthe biases achieved by the model is proportional to the ap-\nproximated Mongeau & Sankoff similarity. Formally, we\ndeﬁne\f(njt;n0)as follows, where MGD max is the maxi-\nmal value of localised Mongeau & Sankoff similarities:\n\f(njt;n0) = exp\u0012\n1\u0000MGD ([n0;n];t)\u0000MGD ([n0];t)\nMGD max\u0013Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 1693.3 Controlling the Similarity\nWe deﬁne an additional mechanism to control the inten-\nsityof the variation mechanism, i.e. the extent to which\nthe generated melodies should be similar to the imposed\ntheme. We introduce a parameter \u000b, which is used to ad-\njust the values of the biases \fto new values \f0, deﬁned as\n\f0(njt;n0) = max(0;(1\u0000\u000b):\f(njt;n0) +\u000b). In theory,\u000b\nranges from\u00001 to 1: a very small value will cause almost\nall adjusted biases \f0to be equal to 0, except when \fwas\nvery close to 1, leading to melodies highly similar to the\ntheme. Conversely, when \u000bis1, all adjusted biases \f0are\nequal to 1, and have no effect. The interesting, non-trivial,\nbehaviour is obtained with in-between values, which can\nbe chosen by the user of the variation mechanism. How-\never, the range of values where the non-trivial behaviour is\nobserved depends on a particular corpus and a given theme.\nThis means that a speciﬁc value of \u000bhas no general se-\nmantics, which hinders usability. As a result, we calibrate\nthe range of \u000b, by estimating the values for which the non-\ntrivial behaviour occurs, given a speciﬁc corpus and theme.\nWe estimate the values \u000b\u0000and\u000b+such that the average\nvalue of all adjusted biases \f0is a given value close to 0\nor close to 1, respectively. We estimate those values with a\nsimple binary search. Given those two values, the user of\nthe system then sets a parameter \u001b2[0;1], the strictness\nof the variation mechanism, and the actual value \u000bis de-\nduced by setting \u000b=\u001b(\u000b+\u0000\u000b\u0000) +\u000b\u0000. We evaluate the\neffect of\u001bin practice in the next section.\n4. EXPERIMENTAL RESULTS\nOur approach relies on the intuition that local similarities,\nfavoured by the biased model, will result in a global sim-\nilarity between the generated melodies and the theme. In\nthis section, we evaluate how the choice of the value for the\nparameter\u001binﬂuences the Mongeau & Sankoff similarity\nbetween the generated melodies and the original theme.\nIn particular, we show that the biased model favours se-\nquences closer to the theme and penalises sequences less\nsimilar to the theme. We then explain the result more ana-\nlytically, for \u001b= 0. We ﬁrst show that applying the bias to\nthe model approximates the localised Mongeau & Sankoff\nsimilarity, and then we show that this localised Mongeau\n& Sankoff similarity is a good approximation of the actual,\nglobal Mongeau & Sankoff similarity.\nIn the experiments below, the theme is the melody in\nthe ﬁrst four bars of “Solar” (Miles Davis, Figure 3). The\ntraining corpus contains 29 lead sheets by Miles Davis. In\neach experimental setup, we build a general model of 4-bar\nlead sheets in the style of Miles Davis, called the unbiased\nmodel , and then, we bias the model to favour the theme\nwith some value for \u001b. Actual examples of variations at\nvarious distances are shown in Section 5.1.\n4.1 Correlation between the Biases and the Mongeau\n& Sankoff Distance\nFor one value of \u001b, we generate 10 000 variations of the\noriginal theme (ﬁrst four bars of “Solar” ). For each se-quence, we compute its probability poin the unbiased\nmodel and its probability pbin the biased model, and then\nconsider the ratio pb=po. This probability ratio shows\nby how much the sequence has been favoured, for values\ngreater than 1, or conversely penalised, for values less than\n1, in the biased model. On Figure 4, points in blue are se-\nquences generated with the most biased model, i.e. \u001b= 0.\nFor each sequence, we plot its probability ratio, on a log\nscale, against its Mongeau & Sankoff similarity with the\ntheme. We observe that the logarithm of the probability\nratio tends to decrease linearly as the Mongeau & Sankoff\ndistance with the theme increases. Sequences at a distance\nless than 75 from the theme are boosted while sequences\nat a distance more than 75 from the theme are hindered.\nPoints in black are sequences generated with \u001b= 0:95, i.e.\nalmost no bias at all. We observe that most sequences have\na probability ratio of 1, i.e. that the biased model hardly af-\nfects the probability of sequences. Only sequences very far\nfrom the theme have their probability slightly decreased.\nPoints in the red are generated with \u001b= 0:5. They display\nan intermediate behaviour as expected.\n-1.0-0.50.00.5\n0 50 100 150\nmgdlog(ratio)\nFigure 4 . Sequence probability ratio (log) against Mon-\ngeau & Sankoff similarity to theme. Sequences in blue, red\nand black have been generated with \u001b= 0;\u001b= 0:5;\u001b=\n0:95, respectively.\n4.2 Explaining the Correlation\nWe explain the correlation observed by the application of\ntwo successive approximations. We concentrate on the\ncase where \u001b= 0, but similar results are obtained with\nother values. We can break our analysis in three steps.\nFirst, we note that for a given sequence, its probabil-\nity ratio is equal, by deﬁnition of the biased model, to the\nproduct of all the local biases applied to each element of\nthe sequence, up to a normalisation factor. We veriﬁed\nthis experimentally too: for each generated sequence, we\ncomputed the local bias of each of the elements of the se-\nquence, and computed the product of those local biases.\nWe observed that this product is perfectly correlated with170 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017the ratio of probabilities of the sequence. Second, we show\nhow the probability ratio compares with the approximated\nMongeau & Sankoff similarity measure obtained by sum-\nming the localised Mongeau & Sankoff similarity mea-\nsures. For each sequence, we sum, over all its elements,\nthe localised Mongeau & Sankoff that was used when com-\nputing the biases, as explained in Section 3.2. Then, we\ncompare this sum to the product of the local biases, equal\nto the probability ratio. We plot the result on Figure 5. We\nobserve that the approximated Mongeau & Sankoff simi-\nlarity measure is tightly correlated with the logarithm of\nthe product of the local biases, i.e., the logarithm of the\nproduct of the local biases approximates closely enough\nthe sum of the localised Mongeau & Sankoff distances.\n050100150\n1.0 1.5 2.0 2.5\n1 - log(prod_bias)sum_local_mgd\nFigure 5 . The sum of localised Mongeau & Sankoff sim-\nilarity measures against the product of local biases (log),\nfor\u001b= 0\nFinally, we show that this approximated Mongeau &\nSankoff similarity measure approximates the actual Mon-\ngeau & Sankoff similarity measure. On Figure 6, we plot\nfor each sequence, the approximate versus the actual simi-\nlarity measure. We observe that, although the actual mea-\nsure is a global, dynamic programming-based measure, it\nis adequately approximated by summing the localised ver-\nsions. This is probably because the localised measure cap-\ntures sufﬁciently the effect of a note on the global similar-\nity measure.\n5. GENERATING STRUCTURED LEAD SHEETS\nWe show examples of melodic variations produced with\nour techniques, to give a concrete illustration of the varia-\ntion mechanism. Then, we use the variation mechanism as\nthe key building block to generate structured lead sheets2.\n5.1 Melodic Variations\nFigure 7 shows six melodic variations of the ﬁrst four bars\nof“Solar” , by Miles Davis. These variations were created\n2All examples are available on http://www.flow-machines.\ncom/ismir-examples/\n050100150\n0 50 100 150\nmgdsum_local_mgdFigure 6 . The sum of localised Mongeau & Sankoff sim-\nilarity measures against the actual Mongeau & Sankoff\nmeasure, for \u001b= 0\nusing a model trained on 29 songs by Miles Davis (Sec-\ntion 4). The variations are presented in increasing order of\nMongeau & Sankoff distance with the original theme (Fig-\nure 3). Note that the variations are increasingly different\nfrom the theme, both rhythmically and melodically.\n(a) Mongeau & Sankoff distance 12: highly similar to the theme\n(b) Distance 86, minor enrichments in bars 1 and 3\n(c) Distance 87, minor enrichments in bars 1 and 3\n(d) Distance 224, with major differences in bars 2 and 3\n(e) Distance 285, interesting triplet rhythm in bar 1\n(f) Dist. 295, large initial interval (octave) and end of bar 3 differs\nfrom other variations\n(g) Dist. 906, ﬁrst bar uses a rhythm similar that of “Miles Ahead”\n(Miles Davis), and bar 3 is introduces a new rhythm, similar to that\nof the original theme, except with dotted quarter notes\nFigure 7 . Several variations of the ﬁrst four bars of “So-\nlar”, by increasing Mongeau & Sankoff distance.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 1715.2 Enforcing Structure\nWe describe our strategy for automatic composition of\nstructured lead sheets. We use the structure of “In a Senti-\nmental Mood” (Duke Ellington, Figure 8). This song has a\nclassical AABA 32-bar structure preceded by a pickup bar:\n\u000fSections: Pickup : bar 1; A1: bars 2 to 9, A2: bars 2\nto 8 and bar 10, B: bars 11 to 18; A3: bars 19 to 26.\n\u000fBar 12 is a transposed variation of bar 11;\n\u000fBars 15-16 are exact copies of bars 11-12;\n\u000fThe last bar 26 is a variation of bar 10, the ending of\nSection A2.\nFigure 8 .“In a Sentimental Mood” by Duke Ellington.\nRed boxes correspond to the basic blocks induced by the\nstructure of the piece.\nWe illustrate our approach with an automatically gener-\nated lead sheet that conforms to this structure. This struc-\nture induces a segmentation of the lead sheet into contigu-\nous blocks of music. We transform the description of the\nstructure into a procedure that executes it. The ﬁrst occur-\nrence of each block is generated using the general model\nof lead sheets. Subsequent occurrences, if any, are copied\nfrom the ﬁrst occurrence. If speciﬁed by the structure de-\nscription, we use the variation mechanism to obtain a vari-\nation instead of an exact copy, with a strictness that may be\nspeciﬁed by the structure description.\nEach block may appear in several places, but has been\ngenerated only once, without taking into account all possi-\nble contexts. This may have the adverse effect of creating\nawkward transitions that the model would not have created.\nIn these situations, we systematically apply the variation\nmechanism to ensure seamless transitions between blocks.\nSince these variations are not speciﬁed by the structure, we\nimpose a very strict variation to ensure minimal differences\nwith the structure description.\nThe chords are generated by the general model of lead\nsheets, either before the melody or after. In fact, there is\noften structure in the chord sequence too. For example,\nbars 4-5 of “In a Sentimental Mood” are a transposition of\nbars 2-3. We can apply the same approach, with a different\nnotion of distance on chords.\nFigure 9 shows a lead sheet with this structure and gen-\nerated from a stylistic model of the Beatles (trained from\nFigure 9 . A lead sheet with the structure of “In a Senti-\nmental Mood” but in the style of the Beatles. Note that\nbar 12 is a transposed variation of bar 11, as in the origi-\nnal song. The ending is also a variation of the ending of\nSection A1.\na corpus with 201 lead sheets by the Beatles). The music\ndoes not sound similar to “In a Sentimental Mood” at all,\nbut its structure, with multiple occurrences of similar pat-\nterns, make it feel like it was composed with some inten-\ntions. This is never the case of structureless 32-bar songs\ncomposed from the general model. Each part of the lead\nsheet has a strong internal coherence. The melody in the\nAparts use mostly small steps and fast sixteenth notes,\nmany occurrence of a rhythmic pattern combining a six-\nteenth note with a dotted eighth note. The Bpart uses\nmany leaps (thirds, fourth and ﬁfth) and a regular eighth\nnote rhythm. This internal coherence is a product of the\nimposed structure. For instance, in the Bpart, four out\nof eight bars come from a single original cell, consisting\nof bar 11. The fact that the AandBparts contrast with\none another is also a nice feature of this lead sheet. This\ncontrast simply results from the default behaviour of the\ngeneral model of lead sheets.\n6. CONCLUSION\nWe have presented a model for sampling variations of\nmelodies from a graphical model. This model is based on\nthe melodic similarity measure proposed by [12]. Techni-\ncally, we use an approximated version of the Mongeau &\nSankoff similarity measure to bias a more general model\nfor the generation of music. Experimental evaluation\nshows that this approximation allows us to bias the model\ntowards the generation of melodies that are similar to the\nimposed theme. Moreover, the intensity of the bias may be\nadjusted to control the similarity between the theme and\nthe variations. This makes this approach a powerful tool\nfor the creation of pieces complying with an imposed mu-\nsical structure. We have illustrated our method with the\ngeneration of a long structured lead sheet. A pop music\nalbum is currently being produced using this method.\nAcknowledgments: This research is conducted within172 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017the Flow Machines project funded by the European Re-\nsearch Council under the EU’s 7th Framework Programme\n(FP/2007-2013) / ERC Grant Agreement n. 291156.\n7. REFERENCES\n[1] Tom Collins and Robin Laney. Computer–generated\nstylistic compositions with long–term repetitive and\nphrasal structure. Journal of Creative Music Systems ,\n1(2), 2017.\n[2] David Cope. Virtual music: computer synthesis of mu-\nsical style . MIT press, 2004.\n[3] Ryan Cotterell, Nanyun Peng, and Jason Eisner.\nStochastic Contextual Edit Distance and Probabilistic\nFSTs. Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2:\nShort Papers) , pages 625–630, 2014.\n[4] Diana Deutsch, Trevor Henthorn, and Rachael La-\npidis. Illusory Transformation from Speech to Song.\nThe Journal of the Acoustical Society of America ,\n129(4):2245–2252, 2011.\n[5] Douglas Eck and Jasmin Lapalme. Learning musical\nstructure directly from sequences of music. University\nof Montreal, Department of Computer Science, CP ,\n6128, 2008.\n[6] Douglas Eck and Juergen Schmidhuber. Finding tem-\nporal structure in music: Blues improvisation with\nlstm recurrent networks. In Neural Networks for Signal\nProcessing, 2002. Proceedings of the 2002 12th IEEE\nWorkshop on , pages 747–756. IEEE, 2002.\n[7] G Hadjeres and F. Pachet. Deepbach: a steer-\nable model for bach chorales generation. Tech-\nnical report, arXiv:1612.01010, December 2016.\nhttps://arxiv.org/abs/1612.01010.\n[8] Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Antonio Torralba, Raquel Urta-\nsun, and Sanja Fidler. Skip-thought vectors. CoRR ,\nabs/1506.06726, 2015.\n[9] Steven R Livingstone, Caroline Palmer, and Emery\nSchubert. Emotional Response to Musical Repetition.\nEmotion , 12(3):552–567, 2012.\n[10] Qi Lyu, Zhiyong Wu, Jun Zhu, and Helen Meng. Mod-\nelling high-dimensional sequences with lstm-rtrbm:\nApplication to polyphonic music generation. In Pro-\nceedings of the 24th International Conference on Arti-\nﬁcial Intelligence , IJCAI’15, pages 4138–4139. AAAI\nPress, 2015.\n[11] Elizabeth Hellmuth Margulis. Aesthetic responses to\nrepetition in unfamiliar music. Empirical Studies of the\nArts, 31(1):45–57, 2013.\n[12] Marcel Mongeau and David Sankoff. Comparison of\nMusical Sequences. Computers and the Humanities ,\n24(3):161–175, 1990.[13] Alexandre Papadopoulos, Pierre Roy, and Franc ¸ois Pa-\nchet. Assisted Lead Sheet Composition using Flow-\nComposer. In Principles and Practice of Constraint\nProgramming – CP 2016 . Springer, 2016.\n[14] Eric Sven Ristad and Peter N Yianilos. Learning\nString-Edit Distance. Pattern Analysis and Machine\nIntelligence, IEEE Transactions on , 20(5):522–532,\n1998.\n[15] Elliot Waite. Generating Long-Term Structure\nin Songs and Stories. https://magenta.\ntensorflow.org/blog/2016/07/15/\nlookback-rnn-attention-rnn/ , 2016.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 173"
    },
    {
        "title": "The SEILS Dataset: Symbolically Encoded Scores in Modern-Early Notation for Computational Musicology.",
        "author": [
            "Emilia Parada-Cabaleiro",
            "Anton Batliner",
            "Alice Baird",
            "Björn W. Schuller"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415164",
        "url": "https://doi.org/10.5281/zenodo.1415164",
        "ee": "https://zenodo.org/records/1415164/files/Parada-Cabaleiro17.pdf",
        "abstract": "The automatic analysis of notated Renaissance music is re- stricted by a shortfall in codified repertoire. Thousands of scores have been digitised by music libraries across the world, but the absence of symbolically codified infor- mation makes these inaccessible for computational eval- uation. Optical Music Recognition (OMR) made great progress in addressing this issue, however, early notation is still an on-going challenge for OMR. To this end, we present the Symbolically Encoded Il Lauro Secco (SEILS) dataset, a new dataset of codified scores for use within computational musicology. We focus on a collection of Italian madrigals from the 16th century, a polyphonic secular a cappella composition characterised by strong musical-linguistic synergies. Thirty madrigals for five un- accompanied voices are presented in modern and early no- tation, considering a variety of digital formats: Lilypond, Music XML, MIDI, and Finale (a total of 150 symbolically codified scores). Given the musical and poetic value of the chosen repertoire, we aim to promote synergies between computational musicology and linguistics.",
        "zenodo_id": 1415164,
        "dblp_key": "conf/ismir/Parada-Cabaleiro17",
        "keywords": [
            "codified repertoire",
            "optical music recognition",
            "symbolically encoded il lauro secco",
            "dataset",
            "madrigals",
            "16th century",
            "polyphonic secular a cappella",
            "musical-linguistic synergies",
            "computational musicology",
            "linguistics"
        ],
        "content": "THE SEILS DATASET: SYMBOLICALLY ENCODED SCORES IN\nMODERN-EARLY NOTATION FOR COMPUTATIONAL MUSICOLOGY\nEmilia Parada-Cabaleiro1;2Anton Batliner1;2Alice Baird1;2Björn W. Schuller1;2;3\n1Chair of Complex and Intelligent Systems, University of Passau, Germany\n2Chair of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany\n3GLAM – Group on Language, Audio & Music, Imperial College London, U.K.\nemilia.paradacabaleiro@informatik.uni-augsburg.de\nABSTRACT\nThe automatic analysis of notated Renaissance music is re-\nstricted by a shortfall in codiﬁed repertoire. Thousands\nof scores have been digitised by music libraries across\nthe world, but the absence of symbolically codiﬁed infor-\nmation makes these inaccessible for computational eval-\nuation. Optical Music Recognition (OMR) made great\nprogress in addressing this issue, however, early notation\nis still an on-going challenge for OMR. To this end, we\npresent the Symbolically Encoded Il Lauro Secco (SEILS)\ndataset, a new dataset of codiﬁed scores for use within\ncomputational musicology. We focus on a collection of\nItalian madrigals from the 16thcentury, a polyphonic\nsecular a cappella composition characterised by strong\nmusical-linguistic synergies. Thirty madrigals for ﬁve un-\naccompanied voices are presented in modern and early no-\ntation, considering a variety of digital formats: Lilypond,\nMusic XML, MIDI, and Finale (a total of 150 symbolically\ncodiﬁed scores). Given the musical and poetic value of the\nchosen repertoire, we aim to promote synergies between\ncomputational musicology and linguistics.\n1. INTRODUCTION\nSince scores are the only remaining source of Renaissance\nmusic, they are essential for replication and analysis of\nthis repertoire. Through the analysis of an early score it\nis possible to identify musical similarities between com-\nposers [24], as well as correlations between poetry and\nmusic [32]. Due to this, libraries and museums spend great\neffort in the digitisation of early documents. This practice\nallows for easier dissemination of the repertoire and pre-\nserves it from the inevitable degradation.\nNevertheless, since this mass of scores have been\nscanned manually, no symbolically codiﬁable information\nis available, which makes them meaningless for computa-\ntional procedures (e. g., automatic analysis). Furthermore,\n© Emilia Parada-Cabaleiro, Anton Batliner, Alice Baird,\nBjörn W. Schuller . Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Emilia Parada-\nCabaleiro, Anton Batliner, Alice Baird, Björn W. Schuller . “The SEILS\ndataset: Symbolically Encoded Scores in Modern-Early Notation for\nComputational Musicology”, 18th International Society for Music Infor-\nmation Retrieval Conference, Suzhou, China, 2017.in digital libraries of symbolically encoded scores, tran-\nscriptions in modern notation of early musical repertoire\nare restricted, and early notation is almost completely ig-\nnored.\nTo resolve this issue, Optical Music Recognition\n(OMR) has been applied to early music [6, 10, 26, 28].\nHowever, the degraded conditions of early documents\n(some times unreadable), and the linguistic inconsistencies\nbetween the different voices (common in vocal polyphonic\nmusic), make expert intervention essential, in some cases.\nTherefore, despite obtaining promising results, early no-\ntated music is still an open challenge for OMR [3]. With\nthis in mind, we present the Symbolically Encoded Il\nLauro Secco (SEILS) dataset1. The SEILS dataset is a\ncorpus of scores encoded in a variety of digital formats\n(Lilypond [22]2, Music XML, MIDI and Finale3) and\nmusical notation styles ( white mensural notation [2] and\nmodern Western notation) deliberately selected to max-\nimise computational possibilities. Furthermore, consider-\ning the strong synergies between poetry and music typ-\nical of the chosen repertoire, the presented dataset aims\nto promote, from a musicological, linguistic and historic\nperspective, further understanding of the artistic manifes-\ntations of the ‘Humanism Renaissance’.\nIn particular, the SEILS dataset is suitable for musical-\nlinguistic pattern recognition, given the prominent rela-\ntionship between music and lyrics that characterise the Il\nLauro Secco anthology. Furthermore, since each madri-\ngal(piece) of the considered repertoire is composed by a\ndifferent composer, the SEILS dataset will also allow for\nautomatic identiﬁcation of composers’ similarities. In ad-\ndition, by presenting a codiﬁed version in white mensu-\nralnotation (ground truth), OMR technology will be able\nto evaluate its performance. In section 2 we will evaluate\nprevious studies related to the presented issue. In section\n3 the considered repertoire will be described. An overview\nof the criteria for symbolic codiﬁcation and an evaluation\nof the considered digital formats will be given in sections\n4 and 5. Finally, the conclusions in Section 6.\n1https://github.com/SEILSdataset/SEILSdataset\n2http://lilypond.org/\n3http://www.finalemusic.com5752. RELATED WORK\nEven though scores are a great source of knowledge, cod-\niﬁed symbolic information is missing for many. Some at-\ntempts have been made to improve this, mainly through\nOMR systems [4, 23, 31]. OMR has also been applied\nfor processing early music by several initiatives: SIMSSA\n[10]4, ARUSPIX [26]5, and GAMERA [6]. OMR, when\nused with early notation, examines tablature and mensural\nnotation [25], as well as primitive notation [15] and lyric\nrecognition [3]. Nevertheless, the degraded conditions of\nthe original source and the inconsistencies in the lyrics for\nvocal polyphonic music make human intervention crucial\nin many cases.\nThe score collections available online consider an in-\ncreasing variety of digital formats. The most commonly\nfound formats are Music XML and MIDI; however, other\ndigital formats are becoming more popular: e. g., the\n**kern format [17] (available in the ELVIS database [1]6,\nmusic21 [5]7, and the kernscores database [29]8); Lily-\npond ﬁles [22]9(available in the Petrucci Music Library –\nIMSLP10and the MUTOPIA project database11); or ﬁles\nencoded through the professional music notation software\nFinale12(available in IMSLP). Nevertheless, despite rare\nexceptions like Tasso in Music Project [27]13,The Maren-\nzio Online Digital Edition – MODE14,Josquin Research\nProject15, or the Liber Usualis [16] encoded in MEI16,\nearly music in such a variety of formats is still limited.\n3. THE SEILS DATASET REPERTOIRE\nThe musical repertoire considered for the presented dataset\nis the Italian madrigal of the 16thcentury, a secular poly-\nphonic vocal composition in the Italian language, com-\nmonly for ﬁve to six unaccompanied voices. This kind of\nmadrigal is characterised by meticulous musicalisation of\npoetic texts, a strategy known as madrigalism [14]. To-\nwards the end of the 16thcentury, this composition tech-\nnique was reﬁned and ﬂourished into a rich and virtuous\nmusic [7], characterised by its use of lyrics from great po-\nets of the time [21]. The synergy between poetry and mu-\nsic, prominent within these madrigals, makes them an icon\nof the ‘Humanism Renaissance’ [13]. Given the relevance\nof this intellectual movement to Western Europe, the con-\nsidered repertoire has great importance not only to Italian\nheritage [9], but also to musicological, linguistic, and his-\ntorical research.\n4https://simssa.ca/\n5http://www.aruspix.net/\n6https://database.elvisproject.ca/\n7http://web.mit.edu/music21/\n8http://kern.ccarh.org/\n9http://lilypond.org/\n10http://imslp.org/\n11http://www.mutopiaproject.org/\n12http://www.finalemusic.com\n13http://www.tassomusic.org/\n14https://d2q4nobwyhnvov.cloudfront.net/\n86940d50-206f-4db3-9b88-754dddb3486f/\n92KX7friyUw0WA/index.html\n15http://josquin.stanford.edu/\n16http://music-encoding.org/\nFigure 1 : Distribution of the 30 madrigals utilised, considering:\nnumber of madrigals (#M), measure length (#ms), time signature\n(4 / 4 and 2 / 2), and key signature (B ﬂat and no key signature).\n3.1Il Lauro Secco Anthology\nThe presented dataset is a codiﬁed version of the madrigal\nanthology Il lauro Secco (The dry laurel) [18], a collection\nof 31 Italian madrigals written by a variety of highly rep-\nutable composers from the end of the 16thcentury. For\nconsistency, only 30 of these madrigals (for ﬁve a cappella\nvoices, each written by a different composer), are avail-\nable in the presented dataset. The 31st (and last) madrigal\nin the anthology has been excluded from the dataset, as it is\nstarkly different from the others (for ten voices, and com-\nposed by one of the previously considered composers).\nThe presented anthology has been chosen for its high\nlevel of musical–linguistic consistency, i. e., composed\nwith both music and lyrics expressively written for the an-\nthology [20]. Such content is unique, as a standard for\nanthologies was to be created from pre-existing compo-\nsitions, without musical or linguistic relationships. This\nhomogeneity across the anthology allows for an inter-\nscore musical-linguistic analysis, which will enable for a\ndeeper understanding of composer similarities via auto-\nmatic recognition methods.\nBoth the music and lyrics of Il lauro Secco have been\nwritten by some of the most important Italian ﬁgures of this\nperiod. Several composers belong to the Compagnia Ro-\nmana , also known as Eccellenti Musici di Roma (Excellent\nMusicians of Rome) [24], a congregation of composers fa-\nmous for their proﬁciency. Furthermore, even though the\nauthorship of the lyrics is not declared in the anthology,\nmany have attributed this to the great Italian poet, Torquato\nTasso [8, 12, 30].\n3.2 The SEILS Dataset Statistic Evaluation\nConsidering the modern notated transcriptions, the pre-\nsented madrigals display a mean average length of 79.5\nmeasures (with a standard deviation of 15.7). Of the 30\nmadrigals, 21 are in 4 / 4 time signature and 9 in 2 / 2; 13\nhave a B ﬂat in the key signature and 17 do not have alter-\nations declared. In Figure 1, an overview of the distribution\nof madrigals is given, considering number of measures as\nwell as key and time signature.\nAlthough there is a high level of musical-linguistic con-\nsistency, the considered anthology is prominently charac-\nterised by its varying rhythms that differ between madri-576 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 201716th 8th 4th . 2nd breve acc\nBelli 0 42 17 251 4 63\nEremita 0 127 61 167 7 34\nFiorino 8 62 19 295 0 25\nLuzzaschi 0 65 15 348 5 11\nMacque 0 265 48 170 1 34\nMassaino 0 173 40 248 12 36\nPerue 2 35 11 168 0 21\nSpontone 2 73 28 269 2 7\nStrigio 0 252 85 271 2 29\nZoilo 0 27 2 187 2 25\n30M 60 3222 958 7399 117 817\nmean (30M) 2 107 :431:9246 :6 3:0 27:3\nsd (30M) 2:4 66:8 19:7 52:8 3:6 14:4\nTable 1 : Occurrence of sixteenth- (16th), eighth- (8th), quarter\ndotted- (4th .), half- (2nd), and double whole- (breve) notes, as\nwell as accidentals (acc) within the madrigals (identiﬁed by com-\nposer name). Max. and min. values, for occurrences across the\ndataset, are highlighted in bold. Mean and standard deviation\n(sd), are given considering all madrigals together (30M).\ngals. Some madrigals are virtuosic, i. e., showing more\n‘syncopation’ (rhythms off-beat generally represented in\nmusic by dotted-notes), and fast notes (sixteenth- and\neighth-notes). Others are more ‘sustained’, i. e., show-\ning more long notes (double whole-notes), or are ‘har-\nmonically’ more unstable, i. e., showing more ‘accidentals’\n(notes of a pitch that do not belong to the scale declared in\nthe key signature). To illustrate the distributions of notes\nand accidentals, in Table 1 statistics for speciﬁc madrigals\nare given which include extreme occurrences (maximum\nand minimum values), as well as across all madrigals in\nthe data set.\nThe considered anthology presents a balanced distribu-\ntion of voice types: 15 of the 30 madrigals are composed\nfor ‘medium’ vocal range (range from baritone tomezzo-\nsoprano ); the other 15 are composed for ‘extreme’ vocal\nrange, i. e., 7 for ‘high’ range ( tenor tosoprano ), and 8\nfor ‘low’ range ( bass tocontralto ). Two of the 15 madri-\ngals for ‘medium voices’ (those composed by Marenzio\nand Luzzaschi), display the maximum ‘extension’ (i. e.,\nvocal range considering all ﬁve voices) of the anthology\n(between G2 – 97.9 Hz, and G5 – 783.9 Hz). The high-\nest note performed is A5 – 880 Hz, being present only in\nMassaino’s madrigal; whereas the lowest is E2 – 82.4 Hz,\nperformed in the madrigal composed by Spontone.\n4. SYMBOLIC SCORE CODIFICATION\n4.1 Original Notation and Modern Transcription\nThe original notation in which the madrigals of Il Lauro\nSecco have been written in the 16thcentury is the white\nmensural notation (cf. Figure 2) [2]. Two editions of this\nmusical source in early notation are available [18], both\ndigitised and freely available online. The ﬁrst was printed\nin 1582 by Vittorio Baldini in Ferrara (Italy) and is avail-\nable from the Music Library of Bologna17as well as from\n17http://www.bibliotecamusica.it/cmbm/scripts/\ngaspari/scheda.asp?id=7156\nFigure 2 : First staff of Marenzio’s madrigal of the ﬁrst edition\n(1582) of Il Lauro Secco written in white mensural notation.\nIMSLP18. The second, printed in 1596 by Angelo Gar-\ndano in Venice, is available from the Gallica Digital Li-\nbrary19. Both editions have been used in the codiﬁcation\nof the symbolic scores, collecting missing information of\nthe ﬁrst from the second when necessary and vice versa.\nBased on the original source, two transcriptions have\nbeen made: one in white mensural notation (early nota-\ntion), and another in modern notation. Both types have\nbeen chosen for their inherent advantages, and are avail-\nable in Lilypond format. Since proﬁciency in early nota-\ntion requires a level of musicological expertise, rare even\nin subjects from the musical ﬁeld, symbolically codiﬁed\ntranscriptions in modern notation are essential, offering a\nmore understandable version of the repertoire.\nOn the other hand, the codiﬁed transcriptions in early\nnotation, having the same notation as in the original source,\nprovide the ground truth necessary to evaluate the perfor-\nmance of OMR systems (cf. Figure 3). Furthermore, since\nearly notation do not split the notated music in ‘measures’\n(segments within the ‘staff’ delimited by bar lines), ‘ties’\n(the symbol used to link notes with the same pitch across\ndifferent measures), are not required. This means that the\nsymbolic representation of rhythm is always exactly the\nsame, and never made of different note symbols, some-\nthing typical of modern notation (cf. Figure 4). Since in\nmodern notation, the codiﬁcation of a given rhythm within\na measure is different from the one across two measures,\nscores encoded in early notation would be more suitable\nfor musical pattern recognition.\n4.2 Musical Criteria\nEven though in the original scores the individual vocal\nlines are written on different sheets, when engraving visu-\nally the proposed codiﬁed versions in Lilypond format (for\nboth modern and early notation), the ﬁve voices are placed\nvertically superimposed (cf. Figure 3); the same holds for\nthe modern transcription encoded in Finale. Computation-\nally this does not make any difference, but we chose this ar-\nrangement because, from the musicological and linguistic\npoint of view, vertical alignment is essential for effective\nanalysis.\nAs early notation does not present ‘bar lines’, these are\nnot considered in the scores encoded in Lilypond format,\nneither for early nor for modern notation (to allow for a\nvisual comparison between both). Nevertheless, since bar\nlines are typical (if not mandatory) for modern notation,\ndashed bar lines have been considered incorporated in the\n18http://imslp.org/wiki/Il\\_Lauro\\_secco\\\n_(Various)\n19http://gallica.bnf.fr/ark:/12148/\nbtv1b8449068jProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 577Figure 3 : Visual representation of the transcription in white mensural notation (early notation) of the ﬁrst staff of Marenzio’s madrigal\nencoded by Lilypond. Unlike the original source, the voices are visually superimposed.\nFigure 4 : Two symbolic representations of the same rhythm. A)\nwithin a measure (encoded in Lilypond by: g4. a8 b8 c8); B)\nacross two measures (in Lilypond: g4 \u0018g8 a8 b8 c8).\nmodern notated scores encoded in Finale (as commonly\napplied for modern transcription of early repertoire).\nIn early notation, accidentals are not always clearly in-\ndicated. Due to this, in critical editions of early reper-\ntoire, a ‘cautionary accidental’ (accidental placed above\nthe note), is usually given by the musicologist as a sugges-\ntion. However, even these suggestions are given based on\nmusical rules, such as consonances and dissonances cre-\nated by the vertical collisions between notes, many times\nthere is no full agreement between musicologists. Indeed,\n‘cautionary accidentals’ can be displayed even by the mu-\nsicologists themselves in two different ways, i. e., enclosed\nin parentheses above the note (when suggested), or without\nparentheses (when strongly suggested).\nBased on these considerations, in the scores encoded\nin Lilypond, Music XML, and MIDI format, only the ac-\ncidentals shown in the original source will be taken into\naccount; whereas in the scores encoded in Finale, cau-\ntionary accidentals (both enclosed within parentheses or\nnot), have been included to assist musicological analysis\nand potential musical performance. Furthermore, the sym-\nbols given for the accidentals in the original source (sharps\nand ﬂats), had been respected in the early notated transcrip-\ntions, whereas these have been changed into naturals, when\nnecessary, in the modern notated transcriptions (according\nto modern notation rules).\nIn mensural notation, ‘ligatures’ are groups of notes en-\ncoded with a unique graphical symbols. The interpreta-\ntion of ligatures is made according to speciﬁc rules, and\nthe notes involved are at least semibreve, i. e., only ‘long’\nnotes are involved [2]. Ligatures are relatively rare in the\npresented repertoire, being only 18 in the 30 madrigals\n(consider that each madrigal has at least 550 note sym-\nbols). Moreover, ligatures are never involved in musical-\nlinguistic patterns (since these are made up of shorternotes). For these reasons, we encoded ligatures as single\nnotes instead of a unique graphical symbol. In the scores\nencoded in Finale, a square bracket has been used to indi-\ncate the notes originally involved in the ligature (as is usual\nin modern transcriptions of early repertoire).\nFinally, long rests (e. g., maxima rest), have been codi-\nﬁed differently lasting a maximum of the whole rest, i. e.,\nwhole measure. This is the normal practice in modern no-\ntation, but not in early notation, where values are not de-\ntermined by measure length. However, in order to save en-\ncoding time, and given that neither long rests nor graphical\nappearance have a role for musical analysis purposes, this\npractice has been adopted for the encoding in both early\nand modern notation.\n4.3 Linguistic Criteria\nIn the original source, lyrics are placed in two locations of\nthe score: under the notated music (for each one of the ﬁve\nvoices), and in a poem format at the left of each music-\nsheet. Differences between these lyrics are typical of this\nrepertoire, e. g., random use of abbreviations, missing ac-\ncents and punctuation, or different spelling of the same\nword (cf. Figure 5). These inconsistencies create a chal-\nlenge for OMR, and make automatic analysis an extensive\ntask (since no musical-linguistic patterns can be identiﬁed\nin a non-uniﬁed text). For this reason, to encode this reper-\ntoire according to a uniform version, considering musical-\nlinguistic criteria is essential.\nDifferences between the ﬁrst edition of the source\n(1582) and the second (1596) have been found as well, the\nreprinted version being characterised by the use of more\n‘textual contractions’ (e. g., verd’io instead of verde io , or\nsott’ai instead of sotto ai ). Evaluating this, in the presented\ndataset, the standardisation of the lyrics has been made ac-\ncording to the ﬁrst edition of the anthology (1582), and the\nlyrics have been presented only under the musical notation.\nThe following linguistic criteria have been considered [11]:\nI. Linguistic aspects faithful to the Italian language of\nthe 16thcentury:\nA) The etymological ‘h’ that does not produce pronun-\nciation changes (e. g., in ‘hor’), has been conserved;578 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 5 : First staff of the Marenzio’s madrigal for Alto (shown above) and Basso (shown below) voices. The inconsistencies of the\nlyrics between both voices are highlighted: uerde vsverde ,lauro vsLauro , and fùvsfu, between Alto andBasso , respectively.\nB) The graphical symbol ‘ti’, that must be pronounced\n‘zi’ according the modern Italian phonetic rule, has been\nconserved;\nC) The tironian symbol ‘&’ has been transcribed as ‘et’,\naccording the Italian orthographic rule of the 16thcentury;\nD) In the cases where contracted and not contracted\ntextual versions have been presented (e. g., altrov’adopra\nandaltrove adopra ), the not contracted version has been\nconsidered. Nevertheless, in the musical performance, the\nsynalepha, i. e., to merge two syllables into one, has to be\nmade.\nII. Linguistic aspects faithful to the modern Italian lan-\nguage:\nA) The diacritic mark has been normalised according to\nthe modern rule (e. g., ‘più’ instead of ‘piu’ , and ‘a’ instead\nof ‘à’, cf. Figure 5);\nB) The arbitrary use of ‘u’ and ‘v’ in the different voices\nhas been normalised according to the modern rule (e. g.,\nverde instead of uerde , cf. Figure 5);\nC) The abbreviation of ‘n’ and ‘m’ as superscripts\non vowels with\u0018has been normalised by the complete\nspelling (e. g., hanno instead of hãno );\nD) The abbreviation of ‘per’ through ‘~p’ has been nor-\nmalised by the complete spelling (e. g., perché instead of\n~pche);\nE) The abbreviation ‘ ij’, referring to the repetition of\nsentences or words, has been substituted by the complete\nform;\nF) Separated words have been normalised according to\nthe modern rule (e. g., invano instead of in vano , orpoiché\ninstead of poi che ).\nIII. Linguistic aspects considered in order to allow au-\ntomatic musical-linguistic pattern recognition:\nA) The punctuation has been standardised in all the\nvoices, considering the prosody of the text but at the\nsame time encouraging its simpliﬁcation in order to allow\nmusical-linguistic pattern recognition (where normalised\npunctuation between the different voices is essential);\nB) The use of capital- and minor-letters has been stan-\ndardised in all the voices, considering capital–case at thebeginning of each verse and personiﬁcation (cf. Figure 5).\nIn order to prioritise the coherence between the different\nvoices, vertical collisions between musical-linguistic pat-\nterns have been considered. According to this, the starting\nword of the repetitions of verses has been also capitalised.\nFinally, melismatic prosody between syllables of the\nsame word (i. e., a single syllable of text is sung through\nseveral different notes), has been graphically identiﬁed by\ndashes for both early and modern notation, as in the orig-\ninal source. However, when the melisma is placed at the\nend of the word, no graphical indication has been given in\nthe early notated scores, following the original source. On\nthe contrary, for the transcription in modern notation (both\nencoded in Lilypond and Finale), the length of the melisma\nhas been indicated by an underscore.\n5. DIGITAL FORMATS EVALUATION\nAs mentioned, the 30 madrigals have been encoded in four\ndigital formats: Lilypond, Music XML, MIDI, and Finale.\nEarly and modern notation are available in Lilypond for-\nmat (a total of 60 ﬁles), whereas the Finale format has been\nconsidered only to encode modern notated transcriptions\n(30 ﬁles), and from these, Music XML and MIDI ﬁles have\nbeen automatically created (30 for each).\nEach format has differing pros and cons for computa-\ntional musicology. For example, Music XML ﬁles show\nclear links between linguistic information and associated\nnotes, which helps for the automatic identiﬁcation of\nmusical-linguistic connections. In the following, we show\nthe Music XML code (Code 1), used to indicate the ﬁrst\nnote of the Alto voice in the transcription in modern no-\ntation of Marenzio’s madrigal (the original early notated\nversion of this is shown in the top staff of Figure 5).\nCode 1: Music XML syntax\n1 < n o t e d e f a u l t \u0000x =\"121\" >\n2 < p i t c h >\n3 < s t e p >B</ s t e p >\n4 < a l t e r > \u00001</ a l t e r >\n5 < octave >4 </ octave >\n6 </ p i t c h >\n7 < d u r a t i o n >8 </ d u r a t i o n >\n8 < voice >1 </ voice >Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 5799 < type >whole </ type >\n10 < l y r i c d e f a u l t \u0000y=\"\u000080\" number =\"1\" >\n11 < s y l l a b i c > begin </ s y l l a b i c >\n12 < t e x t >Men</ t e x t >\n13 </ l y r i c >\n14 </ note >\nAs we can see, each line of the code indicates a speciﬁc\nmusical parameter (e. g., line 3 the pitch, line 9 the note,\nline 12 the syllable). Nevertheless, this disposition breaks\nup the continuity of the musical patterns, complicating the\nperformance of automatic analysis.\nIn contrast, Lilypond ﬁles have a clearer distribution of\nthe musical patterns over the code lines, according to each\nmeasure (indicated in the following Lilypond code, i. e.,\nCode 2, by “ | % ”). This facilitates computational opera-\ntions such as automatic identiﬁcation of rhythmic-melodic\npatterns, especially in scores encoded in early notation\n(where a given rhythm never indicates different shapes). In\nthe following, we show the Lilypond code (Code 2) used to\nindicate the ﬁrst staff of the Alto voice in the transcription\nin modern notation of Marenzio’s madrigal (the original\nversion of this, is shown in the top staff of Figure 5).\nCode 2: Lilypond syntax\n1 \\ key f \\ major\n2 \\ t im e 4 / 4\n3 \\ autoBeamOff\n4 bes ’1 | % 1\n5 a4 bes4 . bes8 c4 | % 2\n6 d bes a8 g f e | % 3\n7 d4 bes ’ a2 | % 4\n8 a bes | % 5\n9 c4 . c8 c4 d | % 6\nAs we can see, in each line of Lilypond, a whole mea-\nsure is encoded, giving a more compact and meaningful\ndistribution of the music. Indeed, whereas in 14 lines of\nMusic XML, only one note is encoded, in the 9 lines of\nLilypond, 20 notes are encoded, i. e., the whole ﬁrst staff.\nIn these 9 lines, not only the note length is encoded but\nalso the pitch, accidentals, and octave (e. g., “4” means\nquarter-note, “bes” means B ﬂat, and “ ’ ” indicates the 4th\noctave), as well as additional graphical information (e. g.,\n“nautoBeamOff ” indicates not to link the eighth-notes by\na beam, typical of modern notation).\nHowever, in Lilypond format, the lyrics are described\nin a different section of the code respectively to the notes,\nand without measure wise alignment. The link between\nnotes and syllables is given by a single space to indicate\nthat the following syllable is aligned to the following note\nand does not belong to the same word. To link syllables\nof the same word that are aligned to different notes the\ncommand “\u0000\u0000” is used (rests are not considered). In a\nmelismatic passage, to indicate that an extra note must be\nskipped, the command “ nskip4 ” is used. Following this,\nthe ﬁrst verse sung by the Alto voice in the Marenzio’s\nmadrigal is encoded in Lilypond as follows (the original\nearly notated version of this, is shown in the top staff of\nFigure 5):Men\u0000\u0000 tre l’au\u0000\u0000 ra spi\u0000\u0000 rò nel ver\u0000\u0000n skip4\nnskip4nskip4nskip4 de lau\u0000\u0000ro\nMIDI is probably the most common digital format for\nmusic dissemination in the web, being also used in compu-\ntational approaches as pattern identiﬁcation on polyphonic\nmusic [19]. Nevertheless, early music is almost completely\noverlooked in the repertoire presented in this digital for-\nmat. As well as MIDI ﬁles, Finale ﬁles are also a standard\nformat always more common in digital music libraries.\nHowever, again this format is popular in sharing codiﬁed\nscores from other ‘classical’ musical periods but not for\nRenaissance music. With this in mind, we included in the\npresented dataset MIDI and Finale ﬁles.\nBeyond the symbolically codiﬁed ﬁles, a total of 180\npdf ﬁles have also been included. From these, 30 pdf are\nthe modern notated transcriptions of the Finale encoded\nmadrigals (to gain an easier evaluation of the repertoire).\nThe other 150 pdf are scanned copies of the ﬁrst edition\nof the original source (5 pdf ﬁles for each madrigal, one\nfor each voice). In total, the SEILS dataset encompasses\n330 ﬁles: 180 of them are pdf ﬁles; whereas the remaining\n150 are symbolic ﬁles digitally encoded in different for-\nmats. Of these 150 symbolic ﬁles: 60 are encoded by Lily-\npond (.ly), 30 in each of the considered notations (early\nand modern); 30 are encoded by Music XML (.xml); 30 by\nMIDI (.mid); and 30 by Finale (.musx).\n6. CONCLUSIONS\nThe presented dataset of codiﬁed scores aims to encourage\nautomatic musical analysis in Renaissance music. Con-\nsidering the strong connections between music and poetry\nof the chosen repertoire, the presented dataset is specif-\nically suitable for creating synergies between musicology\nand linguistics. We present symbolically encoded scores of\ntheIl Lauro Secco anthology considering the original white\nmensural early notation, which will allow for the evalua-\ntion of OMR performance.\nSince each digital format has some advantages and dis-\nadvantages, it is our belief that through this combination,\neach limitation found in the formats can be overcome (e. g.,\nby combining Lilypond and Music XML ﬁles, it is possi-\nble to clearly identify lyrics with musical patterns). With\nthis in mind, the SEILS dataset makes available a variety of\ndigital formats: Lilypond, Music XML, MIDI, and Finale.\nOur next priority is to complete the analytic annotation\nof the presented dataset in **kern format, through the iden-\ntiﬁcation of different types of madrigalisms (e. g., based on\ncontrapuntal and homorhythmic textures, or in consonant\nand dissonant vertical sonorities, among others), within\neach madrigal.\n7. ACKNOWLEDGEMENT\nThis work was supported by the European\nUnionn’s Seventh Framework and Horizon\n2020 Programmes under grant agreements\nNo. 338164 (ERC StG iHEARu) and No. 688835 (RIA\nDE-ENIGMA).580 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20178. REFERENCES\n[1] C. Antila and J. Cumming. The vis framework: An-\nalyzing counterpoint in large datasets. In Proc. of IS-\nMIR, pages 71–76, Taipei, Taiwan, 2014.\n[2] W. Apel. The notation of polyphonic music, 900-\n1600 . Medieval Academy of America, Cambridge,\nUK, 1961.\n[3] J. A. Burgoyne, Y . Ouyang, T. Himmelman, J. De-\nvaney, L. Pugin, and I. Fujinaga. Lyric extraction and\nrecognition on digital images of early music sources.\nInProc. of ISMIR , pages 723–727, Kobe, Japan, 2009.\n[4] L. Chen, E. Stolterman, and C. Raphael. Human-\ninteractive optical music recognition. In Proc. of IS-\nMIR, pages 647–653, New York, NY , USA, 2016.\n[5] M. S. Cuthbert and C. Ariza. music21: A toolkit for\ncomputer-aided musicology and symbolic music data.\nInProc. of ISMIR , pages 637–642, Utrecht, Nether-\nlands, 2010.\n[6] M. Droettboom, I. Fujinaga, K. MacMillan, G. S.\nChouhury, T. DiLauro, M. Patton, and T. Anderson.\nUsing the gamera framework for the recognition of cul-\ntural heritage materials. In Proc. of the 2nd ACM/IEEE-\nCS, pages 11–17, Portland, OR, USA, 2002.\n[7] E. Durante and A. Martellotti. Madrigali segreti per\nle dame di Ferrara: Il manoscritto musicale F . 1358\ndella Biblioteca Estense di Modena . Studio per edi-\nzioni scelte, Firenze, Italia, 2000.\n[8] E. Durante and A. Martellotti. Giovinetta peregrina:\nLa vera storia di Laura Peperara e Torquato Tasso . LS\nOlschki, Firenze, Italia, 2010.\n[9] A. Einstein. The Italian Madrigal . Princeton Univer-\nsity Press, Princeton, NJ, USA, 1971.\n[10] I. Fujinaga and A. Hankinson. Simssa: Single isnter-\nface for music score searching and analysis. Journal of\nthe Japanese Society for Sonic Arts , 6(3):25–30, 2005.\n[11] G. Gialdroni. Di Giovanni Battista Moscaglia. Il sec-\nondo Libro de’ Madrigali a Quattro Voci . Fondazione\nPierluigi da Palestrina, Palestrina, Italy, 2007.\n[12] M. Giuliani. I lieti amanti: Madrigali di venti musicisti\nferraresi e non . Leo S. Olschki, Firenze, Italia, 1990.\n[13] A. Goodman and A. MacKay. The impact of humanism\non Western Europe . Taylor and Francis, London, UK,\n2013.\n[14] D. J. Grout and C. V . Palisca. A history of western mu-\nsic, volume 1. Norton, New York, NY , USA, 2001.\n[15] A. Hankinson, J. A. Burgoyne, G. Vigliensoni, and\nI. Fujinaga. Creating a large-scale searchable digital\ncollection from printed music materials. In Proc. of the\n21st Int. Conf. on World Wide Web , pages 903–908,\nLyon, France, 2012.\n[16] A. Hankinson, J. A. Burgoyne, G. Vigliensoni,\nA. Porter, J. Thompson, W. Liu, R. Chiu, and I. Fu-jinaga. Digital document image retrieval using optical\nmusic recognition. In Proc. of ISMIR , pages 577–582,\nPorto, Portugal, 2012.\n[17] D. Huron. Music information processing using the\nhumdrum toolkit: Concepts, examples, and lessons.\nComputer Music Journal , 26(2):11–26, 2002.\n[18] F. Lesure. Recueils imprimes XVIe-XVIIe siecles .\nHenle, Munich, Germany, 1960.\n[19] B. Meudic and E. St-James. Automatic extraction of\napproximate repetitions in polyphonic midi ﬁles based\non perceptive criteria. In Int. Symp. on Computer Music\nModeling and Retrieval , pages 124–142, Montpellier,\nFrance, 2003.\n[20] A. Newcomb. The three anthologies for laura peverara,\n1580–1583. Rivista Italiana di Musicologia , 10:329–\n345, 1975.\n[21] A. Newcomb. The Madrigal at Ferrara: 1579-1597 .\nPrinceton University Press, Princeton, NJ, USA, 1980.\n[22] H.-W. Nienhuys and J. Nieuwenhuizen. Lilypond, a\nsystem for automated music engraving. In Proc. of the\n14th Colloquium on Musical Informatics , volume 1,\npages 167–172, Firenze, Italia, 2003.\n[23] V . Padilla, A. McLean, A. Marsden, and K. Ng. Im-\nproving optical music recognition by comining outputs\nfrom multiple sources. In Proc. of ISMIR , pages 517–\n523, Málaga, Spain, 2015.\n[24] N. Pirrotta. Dolci affetti: I Musici di Roma e il madri-\ngale. L. S. Olschki, Firenze, Italia, 1985.\n[25] L. Pugin and T. Crawford. Evaluating OMR on the\nearly music online collection. In Proc. of ISMIR , pages\n439–444, Curitiba, Brazil, 2013.\n[26] L. Pugin, J. Hockman, J. A. Burgoyne, and I. Fujinaga.\nGamera versus Aruspix – two optical music recogni-\ntion approaches. In Proc. of ISMIR , pages 419–424,\nPhiladelphia, PA, USA, 2008.\n[27] E. Ricciardi. The tasso in music project. Early Music ,\n43(4):667–671, 2015.\n[28] P. Roland, A. Hankinson, and L. Pugin. Early music\nand the music encoding initiative. Early Music , pages\n605–611, 2014.\n[29] C. S. Sapp. Online database of scores in the humdrum\nﬁle format. In Proc. of ISMIR , pages 664–665, London,\nUK, 2005.\n[30] A. Vassalli. Il tasso in musica e la trasmissione dei testi:\nalcuni esempi. Tasso, la Musica, i Musicisti , pages 45–\n90, 1988.\n[31] V . Viro. Peachnote: Music score search and analysis\nplatform. In Proc. of ISMIR , pages 359–362, Miami,\nFL, USA, 2011.\n[32] J. A. Winn. Unsuspected eloquence: A history of the\nrelations between poetry and music . Yale University\nPress, New Haven, CT, USA, 1981.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 581"
    },
    {
        "title": "Confidence Measures and Their Applications in Music Labelling Systems Based on Hidden Markov Models.",
        "author": [
            "Johan Pauwels",
            "Ken O&apos;Hanlon",
            "György Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1418155",
        "url": "https://doi.org/10.5281/zenodo.1418155",
        "ee": "https://zenodo.org/records/1418155/files/PauwelsOFS17.pdf",
        "abstract": "Inspired by previous work on confidence measures for tempo estimation in loops, we explore ways to add confi- dence measures to other music labelling tasks. We start by reflecting on the reasons why the work on loops was suc- cessful and argue that it is an example of the ideal scenario in which it is possible to define a confidence measure in- dependently of the estimation algorithm. This requires ad- ditional domain knowledge not used by the estimation al- gorithm, which is rarely available. Therefore we move our focus to defining confidence measures for hidden Markov models, a technique used in multiple music information re- trieval systems and beyond. We propose two measures that are oblivious to the specific labelling task, trading off per- formance for computational requirements. They are exper- imentally validated by means of a chord estimation task. Finally, we have a look at alternative uses of confidence measures, besides those applications that require a high precision rather than a high recall, such as most query re- trievals.",
        "zenodo_id": 1418155,
        "dblp_key": "conf/ismir/PauwelsOFS17",
        "keywords": [
            "confidence measures",
            "tempo estimation",
            "music labelling tasks",
            "hidden Markov models",
            "domain knowledge",
            "performance",
            "computational requirements",
            "chord estimation task",
            "alternative uses",
            "query retrievals"
        ],
        "content": "CONFIDENCE MEASURES AND THEIR APPLICATIONS IN MUSIC\nLABELLING SYSTEMS BASED ON HIDDEN MARKOV MODELS\nJohan Pauwels Ken O’Hanlon György Fazekas Mark B. Sandler\nCentre for Digital Music, Queen Mary University of London, UK\n{j.pauwels, k.o.ohanlon, g.fazekas, mark.sandler}@qmul.ac.uk\nABSTRACT\nInspired by previous work on conﬁdence measures for\ntempo estimation in loops, we explore ways to add conﬁ-\ndence measures to other music labelling tasks. We start by\nreﬂecting on the reasons why the work on loops was suc-\ncessful and argue that it is an example of the ideal scenario\nin which it is possible to deﬁne a conﬁdence measure in-\ndependently of the estimation algorithm. This requires ad-\nditional domain knowledge not used by the estimation al-\ngorithm, which is rarely available. Therefore we move our\nfocus to deﬁning conﬁdence measures for hidden Markov\nmodels, a technique used in multiple music information re-\ntrieval systems and beyond. We propose two measures that\nare oblivious to the speciﬁc labelling task, trading off per-\nformance for computational requirements. They are exper-\nimentally validated by means of a chord estimation task.\nFinally, we have a look at alternative uses of conﬁdence\nmeasures, besides those applications that require a high\nprecision rather than a high recall, such as most query re-\ntrievals.\n1. INTRODUCTION\nMost of the efforts in music information retrieval research\nare directed towards improving the performance of vari-\nous automatic labelling tasks. This consists of develop-\ning algorithms that are increasingly good at approximating\nsome reference labels, often produced by human annota-\ntors, based on an input audio ﬁle. These labels represent\ndifferent musical concepts, such as genre, tempo, instru-\nmentation or musical key.\nWhen such algorithms are deployed in real world sce-\nnarios, however, no explicit comparison is made between\nthe generated labels and a reference. An example is the\nretrieval of audio based on musically meaningful search\nterms. The only relevant measure of performance here is\nthe degree of satisfaction of the user with the returned au-\ndio ﬁles. The user will subconsciously verify if the re-\nturned audio corresponds somewhat to the query term, and\nc\rJohan Pauwels, Ken O’Hanlon, György Fazekas, Mark\nB. Sandler. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Johan Pauwels, Ken\nO’Hanlon, György Fazekas, Mark B. Sandler. “Conﬁdence measures and\ntheir applications in music labelling systems based on hidden Markov\nmodels”, 18th International Society for Music Information Retrieval Con-\nference, Suzhou, China, 2017.be dissatisﬁed if it doesn’t, but this implicit and informal\nevaluation is nowhere as rigorous as the numerical evalua-\ntion performed to demonstrate algorithmic improvements.\nThis gap between algorithmic evaluation and user evalu-\nation makes that increases in algorithmic performance do\nnot necessary lead to increases in user satisfaction.\nCrucially, in many retrieval tasks the precision is more\nimportant than the recall. The users only judge the qual-\nity of the returned audio, the amount of potentially use-\nful audio ﬁles that are not returned to them are unknown\nand irrelevant (once the amount of returned ﬁles reaches a\nminimally acceptable number of course). A relatively easy\nway to improve the perceived quality of the returned au-\ndio (and thereby user satisfaction) would be to only return\nthose ﬁles for which the generated labels are known to be\nof a high quality. This necessitates a reliable measure of\nconﬁdence for the generated output labels, which must be\ncalculated without relying on a known reference output.\nDespite its obvious use-case, not much work has been\nperformed on conﬁdence measures for music labelling. For\ntempo estimation in music loops speciﬁcally, a thorough\nstudy has been recently performed by Font and Serra [5].\nThey propose a new conﬁdence measure and compare it to\nearlier efforts of Zapata et al. [18]. In this paper, we devise\nnew methods for conﬁdence estimation that are not spe-\nciﬁc to a single task, but work with all systems based on\nhidden Markov models (HMMs). To this end, we start by\nanalysing the reasons why the work on loops was success-\nful and what we can and cannot reuse from it in Section 2.\nThen we look at the general framework of HMMs, propose\nsome candidate conﬁdence measures and evaluate them for\nchord estimation in Section 3. Next, a novel application\nfor conﬁdence measures is discussed in Section 4. We end\nby formulating some conclusions and directions for future\nwork in Section 5.\n2. DOMAIN-BASED VERSUS\nALGORITHM-BASED CONFIDENCES\nMEASURES\nTo aid coming up with conﬁdence measures for a wider\nrange of tasks, it is useful to ﬁrst reﬂect on the underly-\ning conditions that made Font and Serra’s work [5] suc-\ncessful. They managed to deﬁne a conﬁdence measure\nthat can be calculated from just the generated output. It\nis therefore oblivious to the algorithm that was used to cal-\nculate the output. The advantage is that knowledge of and279access to the inner workings of the algorithm are not re-\nquired in order to use the conﬁdence measure. However,\nthis type of conﬁdence measure relies on extra prior knowl-\nedge about the application domain, which is used to ver-\nify the output against in the absence of internal states of\nthe algorithm and (obviously) the target labels. Therefore\nwe call these conﬁdence measures domain-speciﬁc , as op-\nposed to algorithm-speciﬁc measures. For example, the\ndomain knowledge used for loops is that they are cut in\nsuch a way that each loop contains an exact number of\nbeats.\nIt is of the utmost importance that this domain informa-\ntion hasn’t been exploited yet by the estimation algorithm.\nAll tested software in [5] fulﬁls this criterion, as they were\ndeveloped for music pieces in general, not just loops. If\nalgorithms would already rely on this prior knowledge, the\noutput would be internally adjusted by keeping only those\ntempo candidates that lead to an exact multiple of beats for\nthe duration of the loop. The conﬁdence measure would\nthen always be maximal and therefore useless.\nFinding such unexploited knowledge for a speciﬁc ap-\nplication is not always possible, and if there is one, it also\nneeds to be distinctive enough. Take for instance the case\nof key estimation in loops. A reasonable prior would be to\nassume that there are no key changes for the duration of a\nloop. Even if a key estimation algorithm is capable of pro-\nducing key changes, it is unlikely that multiple keys will\nbe generated over the short duration of a typical loop. No\nreliable conﬁdence measure can then be derived from this\nadditional information.\nWe argue that having sufﬁcient unexploited and distinc-\ntive domain knowledge for a particular task is a rare event.\nIn practice, it is therefore more likely that we need recourse\nto algorithm-speciﬁc conﬁdence measures. These are de-\nﬁned using the intermediate states of the algorithm, which\nunfortunately means that separate measures need to be for-\nmulated for each algorithm and that the resulting conﬁ-\ndence cannot easily be compared between algorithms. The\nupside is that they are not tied to a particular domain.\nIn the remainder of this paper, we propose some can-\ndidate algorithm-based conﬁdence measures. To mitigate\ntheir algorithm-speciﬁcity, we will look at the framework\nof hidden Markov Models [15], which is commonly used\nin a variety of estimation tasks. Our hope is that the\nproposed solutions will therefore be task-independent and\nwidely applicable.\n3. HIDDEN MARKOV MODEL-BASED\nCONFIDENCE MEASURES\n3.1 Hidden Markov Model Basics\nAccording to Ghahramani [7], “a hidden Markov model\n(HMM) is a tool for representing probability distributions\nover sequences of observations.” It is widely used to model\nsequences in applications as diverse as speech recogni-\ntion [15] and bioinformatics [4]. In music information re-\ntrieval, it is commonly used to take temporal dependencies\ninto account when observations are localised.Formally, an HMM is a doubly stochastic process that\nconsists of a ﬁrst-order Markov chain of hidden states that\ncan only be observed through another, visible stochastic\nprocess. Both processes are sampled at discrete inter-\nvals, so they can be represented by an index variable t.\nThis sequence index often represents time. The observed\nvariables can be discrete or continuous, ﬁnite or inﬁnite,\nunivariate or multivariate, or any combination thereof, as\nlong as they have a probability distribution associated with\nthem. The state variables, on the other hand, are always\ndiscrete and there’s a ﬁnite number Nof them. The val-\nues the state variable can possibly take are therefore enu-\nmerated asSn;81\u0014n\u0014N. The value of the speciﬁc\nhidden state at index tis represented as Yt, soYt2S=\nfS1;:::;S Ng. The observed variable at index tis repre-\nsented asXt. It can potentially take an uncountable num-\nber of values, so we can’t enumerate them, only represent\ntheir space by O. Furthermore, an observed variable Xtis\nassumed to depend only on the hidden state Ytat the cor-\nresponding position tin the sequence, not on the hidden or\nobserved variables at any other positions.\nHidden Markov models are entirely described by spec-\nifying three probability distributions: (1) the initial state\ndistribution; (2) the state transition distribution, which is\ntime-invariant in a standard HMM; (3) the observation dis-\ntribution, which is also time-invariant in a standard HMM\nP(Y1=Si)\u0011\u0019i;81\u0014i\u0014N (1)\nP(Yt+1=SjjYt=Si)\u0011aij;81\u0014i;j\u0014N (2)\np(Xt=OjYt=Sj)\u0011bj(O);81\u0014j\u0014N;8O2O(3)\nThe set of parameters of an HMM can therefore be\nsummarised as \u0015=f\u0005;A;Bg, where \u0005 =f\u0019igi,A=\nfaijgi;j,B=fbj(O)gj;O\nThe context in which conﬁdence measures are use-\nful assumes that the HMM parameters \u0015are already de-\ntermined. Given a particular sequence of observations\nx1:T=x1;:::;x T, we want to determine the underlying\nstate sequence (called path) y1:T=y1;:::;y Tthat pro-\nduced these observations and a value cthat indicates how\nmuch conﬁdence we have in the generated hidden state se-\nquence. The process that determines the optimal hidden\nstate sequence is called “decoding” the HMM and is well\nestablished in the literature [15]. Our goal is to ﬁnd out\nwhich of the internal states of the decoding process could\nbe repurposed as a conﬁdence measure.\nThe most common way to decode an HMM is to ﬁnd the\nsingle path ^y1:Tthat is the maximum a posteriori (MAP)\nestimate:\n^y1:T= argmax\ny1:T2STp\u0000\ny1:Tjx1:T;\u0015\u0001\n(4)\n= argmax\ny1:T2STp\u0000\ny1:T;x1:Tj\u0015\u0001\n(5)\nThis path can be found by following the Viterbi-\nalgorithm [17]. For more details about its implementation,\nwe refer to Rabiner’s well-known tutorial [15].280 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20173.2 Experimental Setup\nIn order to experimentally validate the theoretical conﬁ-\ndence measures we’re about to propose, we need a con-\ncrete labelling system based on an HMM. In this section,\nwe describe the chord estimation system that will be used\nto this end.\nOur system ﬁrst converts an audio ﬁle into mono and\nextracts a time-chroma [6] representation from it that will\nbe used as observations in the HMM. We use two different\nchroma extractors, such that we can verify that the conﬁ-\ndence measure works regardless of features. The ﬁrst vari-\nant we use is recently developed by Korzeniowski et al. [9].\nThey trained a three layer dense neural network to map\nquarter-tone log-frequency magnitude spectra to chromas.\nThe second variant is the Compressed Log Pitch (CLP)\nchroma [12], which applies logarithmic compression be-\nfore summing the semi-tone log-frequency power spectra\ninto one octave. Both implementations are taken from the\nmadmom library [2], version 0.15.1 . The parameters were\nset to the values proposed in their original papers.\nBecause the features that are fed into the HMM are\nchromas, this means the observation space consists of\ntwelve-dimensional positive real numbers O=R12\n\u00150. The\nobservation probability distribution bj(O)overOis cal-\nculated using template matching. This requires that each\nchord stateSjhas a template Mjassociated with it and\na similarity measure that maps observation-template pairs\n(O;M j)to probabilities. We use the normalised cosine\nsimilarity, deﬁned as\nbj(O) =hO;M ji\nkOkL2kMjkL2(6)\nwherehO;M jirepresents the inner product between Oand\nMj.\nIn our experiments, we set the number of chord states to\n48. We discern four chord types (maj-min-dim-aug triads)\nfor each of the twelve possible roots. The associated chord\ntemplates are binary, with the chromas that are theoreti-\ncally present in the chord set to one and the other chromas\nset to zero.\nThe last parts of the HMM that need to be conﬁgured\nare the initialisation and the transition probabilities. Both\nsystems are initialised uniformly, i.e. the probability to\nstart in a speciﬁc state is 1=48for all chords. The transi-\ntion probabilities are kept deliberately simple. The state\nself-transition probabilities are all assigned the same value\naii=\u001c;81\u0014i\u0014N (7)\nwhereas the state-changing probabilities are distributed\nuniformly\naij=1\u0000\u001c\nN\u00001;81\u0014i;j\u0014N;i6=j (8)\nThis reduces the number of parameters considerably,\nthereby reducing the potential that our experiments don’t\ntranslate to other datasets, but it has also been demon-\nstrated that such a simple transition matrix is enough to getmost of the beneﬁts of applying an HMM to the observa-\ntions. In [13], it is shown that the state-changing probabil-\nities in an HMM where states represent relative chords in\na key can at most improve the estimation performance by\n2–3 %-points, whereas [3] show that this is even less when\nstates represent absolute chords, without the context of a\nkey (as is the case here too). The HMM then effectively\nacts as a probabilistic temporal smoother, and does not take\ninto account the speciﬁc values of surrounding states, only\ntheir duration. The only remaining parameter \u001cis deter-\nmined through an exhaustive search on the test data.\nThe score that would ideally be predicted by the conﬁ-\ndence measure is calculated by the open-source MusOOE-\nvaluator1tool [14]. This is the same software that is used\nfor MIREX. We use the “MirexMajMin” preset for chord\nevaluation.\nFinally, we use two datasets for testing the chord esti-\nmation systems and their conﬁdence measures, in order to\ninvestigate data-speciﬁc behaviour. The ﬁrst is the “Iso-\nphonics”2dataset [11]. Speciﬁcally, we use the subset\nthat is used for the MIREX chord estimation task. It con-\ntains 217 songs and is comprised of 12 Beatles albums (180\nsongs), a Queen compilation (19 songs) and one Zweieck\nalbum (18 songs). The second is the “RWC Popular”3\ndataset [8]. The latter contains 100 Japanese pop songs\npurposefully recorded for music information retrieval re-\nsearch.\n3.3 Sequence Probability as Conﬁdence Measure\nAs part of the MAP decoding, the probability of the opti-\nmal label sequence gets returned:\np\u0000\n^y1:T\u0001\n= max\ny1:T2STp\u0000\ny1:Tjx1:T;\u0015\u0001\n(9)\n= max\ny1:T2STp\u0000\ny1:T;x1:Tj\u0015\u0001\n(10)\nAlthough this seems like an obvious candidate for a\nconﬁdence measure, as far as we know, nobody has ever\nexamined the correlation between optimal path probability\nand labelling score. We know from the deﬁnition that the\noptimal path probability has the highest value relative to\nthe probabilities of any other paths, but in order for it to be\nuseful as a conﬁdence measure, its absolute value matters.\nSince the joint probability p\u0000\ny1:T;x1:T\u0001\ncan be decom-\nposed as\np\u0000\ny1:T;x1:T\u0001\n=P(y1)p(x1jy1)TY\nt=2P(ytjyt\u00001)p(xtjyt)(11)\na ﬁrst step that needs to be taken is to normalise the\npath probability with respect to the song duration T, as\np\u0000\ny1:T;x1:T\u0001\ngets progressively smaller with T4. This\n1https://github.com/jpauwels/MusOOEvaluator\n2annotations available at http://isophonics.net/\ncontent/reference-annotations\n3annotations available at https://github.com/tmc323/\nChord-Annotations\n4In practice, we’re working in the logarithmic domain precisely to\nmitigate this vanishing probability problemProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 281−1,500−1,000−500 0050100\nGeometric mean of the frame contributionsChord score(a)\n−2.8 −2.6050100\nMedian of the frame contributionsChord score (b)\n0.2 0.4 0.6 0.8050100\nPointwise path differenceChord score (c)\nFigure 1 : Chord estimation scores for CLP features on the Isophonics dataset: as a function of (a) log\u0010\nTp\np(^y1:T)\u0011\n,\n(b)median t(log [P(ytjyt\u00001)p(xtjyt)]), (c) the pointwise path difference\n0 500 1,000 1,500−4−3\nFrame index tlog[P(ˆyt|ˆyt−1)p(xt|ˆyt)]\nThe Beatles - Please Please Me - 03 Anna (Go To Him)\nFigure 2 : Separate contributions to the optimal path prob-\nability log [P(ytjyt\u00001)p(xtjyt)]for every frame t\nway, we can compare songs of different length. Be-\ncause the probabilities per frame get accumulated through\na product, it makes sense to take the T-th root of the path\nprobability. Then we are effectively calculating the geo-\nmetric mean of the contributions per frame to the overall\npath probability.\nWe’ve plotted in Figure 1a an example of the scores as\na function of the resulting geometric mean for the system\nwith CLP features run on the Isophonics dataset. It is im-\nmediately clear from the ﬁgure that there is little correla-\ntion between the two axes and therefore the optimal path\nprobability is not suitable as a conﬁdence measure.\nTo ﬁnd out why this is the case, we take a look at the in-\ndividual contributions per frame P(^ytj^yt\u00001)p(xtj^yt);8t\nseparately. In Figure 2, we show these probabilities for an\nexample song. We can see that there are some strong out-\nliers in these probabilities, and this is the case for every\nsong, not just this example. Because the contributions per\nframe are multiplied5to form the overall probability, we\npostulate that the limited number of outliers dominate the\noverall probability regardless of the performance on other\nframes, such that there is no longer a relation between the\noverall probability and how well ^y1:Texplainsx1:T. Note\nthat the presence of such an outlier at frame tdoes not nec-\nessarily mean that the global optimal path strays from the\nlocally optimal path at that frame. It could also mean that\nnone of the states can explain that particular observation\nwellbj(xt)\u00190;8j. Cases like this are not problematic\nfor the determination of the globally optimal path, since\n5The values depicted in Figure 2 are actually summed, because we\nwork in the logarithmic domain.only the difference between observation distributions per\nstate is relevant for the Viterbi algorithm, but the overall\nprobability will be affected.\nSince the outliers of (P(ytjyt\u00001)p(xtjyt))prove to be\nso problematic, it makes sense to aggregate the frames\ndifferently than through a geometric mean. Instead, we\ntake the median, such that the exact magnitude of the low-\nest probabilities doesn’t matter. We plotted the results in\nFigure 1b for the same chord estimator conﬁguration as\nFigure 1a. The ﬁgure shows a marked improvement with\nrespect to the geometric mean based conﬁdence measure.\nWe do believe that it should be possible to achieve a\nclearer linear relationship between score and conﬁdence\nmeasure though. However, as far as repurposing the inter-\nnal variables of the standard Viterbi algorithm go, we feel\nwe have reached a limit. The advantage of the median-\nbased conﬁdence measure is that it requires practically\nno more computation time than standard MAP decoding.\nIt only requires the so-called lattice of intermediate path\nprobabilities p\u0000\nx1:t;y1:t;yt=Sij\u0015\u0001\n;8i;tto be kept in its\nentirety, as opposed to only needing to keep the previous\nand current frame ( tandt\u00001), which obviously leads to\nan increase in memory. In the next sections, we will com-\npare this measure with a more computationally expensive\none and report on more thorough experiments.\n3.4 Combining Decoders as Conﬁdence Measure\nWhile the Viterbi algorithm returns the globally optimal la-\nbel sequence in the MAP sense of the word, the deﬁnition\nof “optimal” is inherently ambiguous. Another criterion\nof optimality leads to another decoding method. A com-\nmon alternative to MAP decoding is pointwise maximum\na posteriori (PMAP) decoding6. If we represent the path\nestimate returned through PMAP decoding from now on as\n~y1:T, then we ﬁnd\n~yt= argmax\nyt2Sp\u0000\nytjx1:T;\u0015\u0001\n(12)\nAs the name implies, the optimal path is determined\npoint-by-point in such a way that the expected number\n6Confusingly, this decoding algorithm is known under many names,\nposterior decoding or max-gamma decoding just a few of them. More\nalternative names can be found in [10, p. 4].282 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017of correct individual states is maximised. The probabil-\nity to be in a given state sat frametis found by means\nof helper forward \u000bt(s)and backward \ft(s)variables ob-\ntained through a process known as the forward-backward\nalgorithm [1]:\np\u0000\nsjx1:T\u0001\n=\u000bt(s)\ft(s)P\nr2S\u000bt(r)\ft(r)(13)\nFor further implementation details, we again refer to [15].\nIt depends strongly on the application whether MAP\nor PMAP decoding will lead to the best results7and to\nwhich extent they produce different paths. However, it is\nknown that MAP decoding is most effective when a single\npath through the HMM strongly dominates all other ones,\nwhereas PMAP decoding gives better results when multi-\nple competing paths have similar overall probabilities [4].\nWe postulate that when the two methods of decoding\nyield the same path, this gives a good indication that the\npath will have a good score. Thus we derive a new con-\nﬁdence measure PPD\u0000\ny1:T\u0001\nbased on the pointwise path\ndifferences (PPD) as\nPPD\u0000\ny1:T\u0001\n=1\nTTX\nt=1\u000e(^yt;~yt) (14)\nwhere\u000eis the Kronecker-delta, and ^yt/^ytthe MAP/PMAP\npath estimates at time index t.\nTo illustrate the PPD, we take once more the same\nHMM conﬁguration as in Figure 1a and plot the score as\na function of the PPD in Figure 1c. Here the relation be-\ntween the two is more clearly linear. The drawback of this\nconﬁdence measure is obviously that it requires an addi-\ntional PMAP decoding step, which requires extra compu-\ntational power. Some steps, such as the calculation of the\nobservation probabilities, are the same for both decoding\nalgorithms though, so there’s potential for reuse. Note that\nwe still return the same MAP path as before, because it\ngives better results for our HMM conﬁgurations than the\nPMAP path, but the latter is available too.\n3.5 Evaluation\nAs we now have two candidate conﬁdence measures, we\ncan systematically test them on the four combinations of\nfeatures and datasets. Therefore we perform a similar\nexperiment as in [5]. We start by taking the duration-\nweighted average score over the complete dataset and then\nprogressively ﬁlter the ﬁles by ﬁrst excluding those for\nwhich the conﬁdence measure is the lowest. A good con-\nﬁdence measure will then lead to a monotonic increase in\nscore as the ﬁltering threshold increases. The results for\nthe Isophonics dataset and the RWC Popular dataset can\nbe found in Figure 3a, respectively Figure 3b.\nIn all cases, we observe that the PPD is working well\nas a conﬁdence measure. The score of the ﬁltered dataset\nincreases monotonically, save for a few exceptions when\nthe number of remaining songs in the dataset becomes so\n7We tried both and veriﬁed that MAP decoding generally gives the\nbest result for our proposed chord estimation systemlow that the average scores become noisy. The curves of\nthe ﬁltered dataset size as a function of conﬁdence cutoff\nare also close to straight, which means the PPD is nearly\nlinearly distributed between its extrema. As expected, the\nmedian of the per frame contributions to the optimal path\nis less suitable as a conﬁdence measure. The ﬁltered score\ninitially increases in all situations though, so it can still be\nused to remove the ﬁles with the lowest conﬁdence from\nthe dataset. Doing so will increase the precision when\nlooking for a particular chord sequence in a dataset, for\nexample, at the expense of decreasing the recall. Partic-\nularly for the CLP features, the median-based conﬁdence\nmeasure seems to work less well. A possible explanation\nis that the neural network based chromas take context into\naccount. The observations derived from the CLP features\nare therefore noisier, which will affect the median more.\n4. OPTIMAL CHANNEL SELECTION BASED ON\nCONFIDENCE MEASURE\nIn this section, we explore an alternative usage for conﬁ-\ndence measures. Traditionally, labels in music information\nretrieval are estimated from mixed down mono audio. Us-\ning the mono mix ensures that all information present in\nthe audio is used for the label estimation. For certain types\nof labels however, it might be beneﬁcial to selectively ig-\nnore some of the information. For example, ignoring per-\ncussion while estimating chords can be helpful, which has\nled to percussion separation techniques [16].\nIf we have multi-channel audio at our disposal, it is\ntherefore possible that analysing a speciﬁc channel or com-\nbination of channels leads to higher quality labels than\nwhen the mono mixdown is used. The problem is then\nhow to determine this (combination of) channel(s). Next,\nwe’ll verify if a conﬁdence measure can be used for this.\nIdeally, we’d use multi-channel or multi-track audio for\nthis experiment, but since there is no such dataset with an-\nnotated chords, we propose an alternative. Starting from\nstereo Isophonics audio ﬁles, we demix the left (L) and\nright (R) channel according to their panning position into\ncentre (C), hard left (HL) and hard right (HR). We em-\nploy the technique used by the “center cut”8audio ﬁlter\nof the open-source video editor VirtualDub. It operates in\nthe complex spectral domain and relies on the fact that HL\nand HR are perpendicular to each other, such that L = C\n+ HL and R = C + HR. In addition to these channels, we\ncalculate the mono (L + R) and sides (HL + HR), such that\nwe end up with seven virtual channels per song.\nFor each channel, we estimate the chord sequences from\nCLP features and their conﬁdences. We ﬁrst aim to de-\ntermine the theoretical limits of optimal channel selection\nby performing an oracle-style experiment where we select\nthe channels that lead to the biggest increase and biggest\ndecrease in chord score when compared to the reference\nmono channel. Then we check how well we can retrieve\nthe optimal channel by selecting the one that returns the\nchord sequence with the highest conﬁdence.\n8http://www.virtualdub.org/blog/pivot/entry.\nphp?id=102Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 2830 0.2 0.4 0.6 0.8 10100200\nRelative conﬁdence cutoffFiltered dataset sizeConﬁdence measure ﬁltering on the Isophonics dataset\n0 0.2 0.4 0.6 0.8 160708090\nChord score\nCLP Median CLP PPD Deep Median Deep PPD(a)\n0 0.2 0.4 0.6 0.8 1050100\nRelative conﬁdence cutoffFiltered dataset sizeConﬁdence measure ﬁltering on the RWC Popular dataset\n0 0.2 0.4 0.6 0.8 1406080\nChord score\nCLP Median CLP PPD Deep Median Deep PPD (b)\nFigure 3 : Conﬁdence ﬁltered chord scores for two datasets. The marks indicate the average score over the ﬁltered dataset.\nThe lines represent the number of remaining ﬁles in the database as a function of the conﬁdence cutoff.\nAlbum title Mono scoreOracle\nbest\u0001Oracle\nworst\u0001Median\nconf.\u0001 PPD conf. \u0001\nThe Beatles - Please Please Me 52.86 6.01 -13.93 -7.23 1.17\nThe Beatles - With the Beatles 56.22 0.60 -26.81 -18.72 -2.11\nThe Beatles - A Hard Day’s Night 56.21 3.05 -31.91 -21.88 2.24\nThe Beatles - Beatles for Sale 63.98 9.33 -3.26 5.91 7.33\nThe Beatles - Help! 52.54 11.82 -13.23 7.90 10.03\nThe Beatles - Rubber Soul 59.25 5.44 -18.89 -3.00 2.77\nThe Beatles - Revolver 66.06 5.64 -17.63 -0.70 3.71\nThe Beatles - Sgt. Pepper’s Lonely Hearts Club Band 51.33 4.93 -19.81 -4.83 -1.74\nThe Beatles - Magical Mystery Tour 66.77 3.52 -18.33 -3.10 0.54\nThe Beatles - The Beatles (CD1) 62.45 6.32 -17.82 1.76 1.68\nThe Beatles - The Beatles (CD2) 52.05 6.16 -18.91 1.38 1.67\nThe Beatles - Abbey Road 63.86 8.39 -17.47 4.33 4.11\nThe Beatles - Let It Be 61.16 11.96 -8.09 9.24 8.21\nQueen - Greatest Hits I 47.50 7.86 -3.54 6.53 6.66\nQueen - Greatest Hits II 66.16 4.02 -5.49 -1.45 -1.50\nZweieck - Zwielicht 54.73 7.73 -8.10 5.35 6.26\nOverall 57.81 6.60 -14.97 -0.39 3.42\nTable 1 : Channel selection results grouped per album, using CLP features. The reference mono channel score is reported\nalong with the absolute score differences for the best and worst oracle-style and the conﬁdence-based channel selection.\nThe channel selection results overall and per album are\nreported in Table 1. From the oracle experiments we learn\nthat a sizeable improvement in chord score can potentially\nbe achieved by selecting the optimal channel, but also that\nthe consequences of choosing the wrong channel can be\nsevere. The PPD measure can be used successfully to de-\ntermine a better channel than the mono reference, and man-\nages to get a bit more than half the theoretically maximal\nimprovement. The median-based conﬁdence measure, on\nthe other hand, is not suitable to select the optimal channel.\nBased on the individual results per album, no relation\nwith mixing style can be established. The mixing practices\nrange from the mono-like early Beatles albums to the hard-\npanned late Beatles albums, with more modern Queen and\nZweieck in between, but no trends in the (potential) score\nincrease can be identiﬁed. Note that when we repeated the\nexperiments with the DeepChroma chord estimation sys-\ntem, the oracle-based maximal increase was barely over\n2%-points, and the PPD increase proportionate. A reason\nmight be that the neural network is trained on mono mixes.\n5. CONCLUSION AND FUTURE WORK\nIn this paper, we investigated conﬁdence measures for\nHMM-based music labelling systems. We formulated twomeasures, a simple one that doesn’t require extra compu-\ntational power and a better one that is more demanding.\nThey were tested for their ability to ﬁlter low-quality out-\nput of a chord estimation system. Finally, the capacity of\nthe conﬁdence measures to select the most optimal channel\nto use for chord estimation has been evaluated.\nWe hope that the applicability of the proposed conﬁ-\ndence measures to other labelling tasks will be veriﬁed\nin the future, by ourselves or by others. To improve the\nchance of the latter, we’ve created a new general and mod-\nular HMM library9, usable with C++ and Python, that in-\ncludes the proposed measures. The code speciﬁc to the\nchord estimation experiments and the presented ﬁgures can\nalso be found on-line10.\nFurther work will include investigating whether a con-\nﬁdence measure can be used to select the optimal HMM\nparametrisation. For instance, the self-transition probabil-\nity\u001cis currently set to a dataset-wide optimal value, even\nthough it is clearly related to harmonic rhythm and there-\nfore song-dependent. It might be worth investigating if the\nbest value out of a number of candidates can be selected\nbased on conﬁdence.\n9https://github.com/jpauwels/Hiddini\n10https://github.com/jpauwels/\nchord-estimation-confidence284 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. ACKNOWLEDGEMENTS\nThis work has been partly funded by the UK Engineering\nand Physical Sciences Research Council (EPSRC) grant\nEP/L019981/1 and by the European Union’s Horizon 2020\nresearch and innovation programme under grant agreement\nN\u000e688382.\n7. REFERENCES\n[1] Leonard E. Baum and J. A. Eagon. An inequality with\napplications to statistical estimation for probabilistic\nfunctions of markov processes and to a model for ecol-\nogy. Bulletin of the American Mathematical Society ,\n73(3):360–363, May 1967.\n[2] Sebastian Böck, Filip Korzeniowski, Jan Schlüter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: a new\npython audio and music signal processing library. In\nProceedings of the 24th ACM International Conference\non Multimedia , pages 1174–1178. ACM, 2016.\n[3] Taemin Cho and Juan P. Bello. On the relative im-\nportance of individual components of chord recogni-\ntion systems. IEEE Transactions on Audio, Speech and\nLanguage Processing , 22(2):477–492, February 2014.\n[4] Piero Fariselli, Pier Luigi Martelli, and Rita Casadio. A\nnew decoding algorithm for hidden markov models im-\nproves the prediction of the topology of all-beta mem-\nbrane proteins. BMC bioinformatics , 6(Suppl 4):S12,\n2005.\n[5] Frederic Font and Xavier Serra. Tempo estimation\nfor music loops and a simple conﬁdence measure.\nInProceedings of the 17th Conference of the Inter-\nnational Society for Music Information Retrieval (IS-\nMIR) , pages 269–275, 2016.\n[6] Takuya Fujishima. Realtime chord recognition of mu-\nsical sound: a system using Common Lisp Music. In\nProceedings of the International Computer Music Con-\nference (ICMC) , pages 464–467, 1999.\n[7] Zoubin Ghahramani. An introduction to hidden\nMarkov models and Bayesian networks. International\njournal of pattern recognition and artiﬁcial intelli-\ngence , 15(01):9–42, 2001.\n[8] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nPopular, classical and jazz music databases. In Pro-\nceedings of the 3rd International Symposium on\nMusic Information Retrieval (ISMIR) , volume 2, pages\n287–288, 2002.\n[9] Filip Korzeniowski and Gerhard Widmer. Feature\nlearning for chord recognition: the deep chroma ex-\ntractor. In Proceedings of the 17th Conference of the\nInternational Society for Music Information Retrieval\n(ISMIR) , 2016.[10] Jüri Lember and Alexey A. Koloydenko. Bridging\nViterbi and posterior decoding: a generalized risk\napproach to hidden path inference based on hidden\nMarkov models. Journal of Machine Learning Re-\nsearch , 15(1):1–58, 2014.\n[11] Matthias Mauch, Chris Cannam, Matthew Davies, Si-\nmon Dixon, Chris Harte, Sefki Kolozali, Dan Tidhar,\nand Mark Sandler. OMRAS2 metadata project 2009.\nInProceedings of the 10th International Conference on\nMusic Information Retrieval (ISMIR) , 2009.\n[12] Meinard Müller and Sebastian Ewert. Chroma tool-\nbox: Matlab implementations for extracting variants\nof chroma-based audio features. In Proceedings of the\n12th International Conference on Music Information\nRetrieval (ISMIR) , pages 215–220, 2011.\n[13] Johan Pauwels and Jean-Pierre Martens. Combining\nmusicological knowledge about chords and keys in a\nsimultaneous chord and local key estimation system.\nJournal of New Music Research , 43(3):318–330, 2014.\n[14] Johan Pauwels and Geoffroy Peeters. Evaluating auto-\nmatically estimated chord sequences. In Proceedings\nof the IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2013.\n[15] Lawrence R. Rabiner. A tutorial on hidden Markov\nmodels and selected applications in speech recogni-\ntion. Proceedings of the IEEE , 77(2):257–286, Febru-\nary 1989.\n[16] Yushi Ueda, Yuki Uchiyama, Takuya Nishimoto,\nNobutaka Ono, and Shigeki Sagayama. HMM-based\napproach for automatic chord detection using reﬁned\nacoustic features. In Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 5518–5521, March 2010.\n[17] Andrew J. Viterbi. Error bounds for convolutional\ncodes and an asymptotically optimum decoding al-\ngorithm. IEEE Transactions on Information Theory ,\n13(2):260–269, April 1967.\n[18] Jose Ricardo Zapata, Matthew E. Davies, Andre\nHolzapfel, Joao L. Oliveira, and Fabien Gouyon. As-\nsigning a conﬁdence threshold on automatic beat an-\nnotation in large datasets. In Proceedings of the 13th\nInternational Conference on Music Information Re-\ntrieval (ISMIR) , pages 157–162, 2012.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 285"
    },
    {
        "title": "A Formalization of Relative Local Tempo Variations in Collections of Performances.",
        "author": [
            "Jeroen Peperkamp",
            "Klaus Hildebrandt",
            "Cynthia C. S. Liem"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415052",
        "url": "https://doi.org/10.5281/zenodo.1415052",
        "ee": "https://zenodo.org/records/1415052/files/PeperkampHL17.pdf",
        "abstract": "Multiple performances of the same piece share similari- ties, but also show relevant dissimilarities. With regard to the latter, analyzing and quantifying variations in collec- tions of performances is useful to understand how a mu- sical piece is typically performed, how naturally sounding new interpretations could be rendered, or what is peculiar about a particular performance. However, as there is no formal ground truth as to what these variations should look like, it is a challenge to provide and validate analysis meth- ods for this. In this paper, we focus on relative local tempo variations in collections of performances. We propose a way to formally represent relative local tempo variations, as encoded in warping paths of aligned performances, in a vector space. This enables using statistics for analyzing tempo variations in collections of performances. We elab- orate the computation and interpretation of the mean vari- ation and the principal modes of variation. To validate our analysis method despite the absence of a ground truth, we present results on artificially generated data, representing several categories of local tempo variations. Finally, we show how our method can be used for analyzing to real- world data and discuss potential applications.",
        "zenodo_id": 1415052,
        "dblp_key": "conf/ismir/PeperkampHL17",
        "keywords": [
            "performances",
            "similari- ties",
            "dissimilarities",
            "collections",
            "performances",
            "tempo variations",
            "vector space",
            "mean variation",
            "principal modes",
            "real-world data"
        ],
        "content": "A FORMALIZATION OF RELATIVE LOCAL TEMPO V ARIATIONS IN\nCOLLECTIONS OF PERFORMANCES\nJeroen Peperkamp Klaus Hildebrandt Cynthia C. S. Liem\nDelft University of Technology, Delft, The Netherlands\njbpeperkamp@gmail.com fk.a.hildebrandt, c.c.s.liem g@tudelft.nl\nABSTRACT\nMultiple performances of the same piece share similari-\nties, but also show relevant dissimilarities. With regard to\nthe latter, analyzing and quantifying variations in collec-\ntions of performances is useful to understand how a mu-\nsical piece is typically performed, how naturally sounding\nnew interpretations could be rendered, or what is peculiar\nabout a particular performance. However, as there is no\nformal ground truth as to what these variations should look\nlike, it is a challenge to provide and validate analysis meth-\nods for this. In this paper, we focus on relative local tempo\nvariations in collections of performances. We propose a\nway to formally represent relative local tempo variations,\nas encoded in warping paths of aligned performances, in\na vector space. This enables using statistics for analyzing\ntempo variations in collections of performances. We elab-\norate the computation and interpretation of the mean vari-\nation and the principal modes of variation. To validate our\nanalysis method despite the absence of a ground truth, we\npresent results on artiﬁcially generated data, representing\nseveral categories of local tempo variations. Finally, we\nshow how our method can be used for analyzing to real-\nworld data and discuss potential applications.\n1. INTRODUCTION\nWhen performing music that is written down in a score,\nmusicians produce sound that subtly differs from what is\nwritten. For example, to create emphasis, they can vary\nthe time between notes, the dynamics, or other instrument-\nspeciﬁc parameters, such as which strings to use on a violin\nor how to apply the pedals on a piano. In this paper, we fo-\ncus on variations in timing, contributing a method to detect\nlocal tempo variations in a collection of performances.\nSolving this problem is made difﬁcult by the fact that it\nis not clear what we are trying to ﬁnd: there is generally\nno ground truth that tells us what salient variations there\nare for a given piece. Furthermore, it is difﬁcult to discern\nwhether a given performance is ‘common’ or ‘uncommon’.\nc\rJeroen Peperkamp, Klaus Hildebrandt, Cynthia C. S.\nLiem. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Jeroen Peperkamp, Klaus Hilde-\nbrandt, Cynthia C. S. Liem. “A formalization of relative local tempo\nvariations in collections of performances”, 18th International Society for\nMusic Information Retrieval Conference, Suzhou, China, 2017.To overcome this, we propose an approach for statistical\nanalysis of relative local tempo variations among perfor-\nmances in a collection. To this end, we elaborate the com-\nputation of the mean variation and the principal modes of\nvariation. The basis of the approach is the insight that after\nnormalization, the set of possible tempo variations, repre-\nsented by temporal warping paths, forms a convex subset\nof a vector space. We test our approach on artiﬁcially gen-\nerated data (with controllable variations in a collection),\nand on recorded real performances. We discuss two appli-\ncations: analysis of tempo variations and example-guided\nsynthesis of performances.\n2. RELATED WORK\n2.1 Performance Analysis\nMost closely related to the present work are the works\nin [9, 11] and [21, 22], focusing on statistical comparison\nof performances, targeting local tempo variations without\nground truth. [9, 11] focus especially on temporal warping\npaths with respect to a reference performance. Further-\nmore, [10] analyzes main modes of variation in compara-\ntive analysis of orchestral recordings. We differ from these\nworks in offering a more formalized perspective on varia-\ntion, a more thorough and controlled validation procedure\non artiﬁcially generated data, and ways to perform analyses\nwith respect to a full collection of performances, beyond a\nsingle reference performance.\nFurther work in comparative performance analysis con-\nsidered features such as dynamics [6]: here, it was shown\nthat dynamic indications in a score do not lead to absolute\nrealizations of loudness levels. [8] and [1] provide compar-\native analyses on many expressive features, although the\nlatter work also ﬁnds that musicians ﬁnd it difﬁcult to think\nabout the aspects of their performance in the quantitative\nfashion that is common in the MIR literature.\nThe absence of a clear-cut ground truth also poses\nchallenges when automatically creating a natural-sounding\nrendition of a piece of music, as noted in [3] as well as [26].\nIndeed, the system in the latter work explicitly relies “on\na ‘correct’ or ‘appropriate’ phrase structure analysis”, sug-\ngesting it is not trivial to get such an analysis.\nQuite some work has also gone into the task of structure\nanalysis, e.g. [12, 14–16, 18, 19, 23]. It turns out, however,\nthat for some genres, the structure may be perceived am-\nbiguously, as observed with professional annotators [23],\nperformers [17] and listeners [24].1582.2 Dynamic Time Warping\nFor obtaining temporal warping paths between perfor-\nmances, we use Dynamic Time Warping (DTW). In a nut-\nshell, DTW matches points from one time series to points\nfrom another time series such that the cumulative distance\nbetween the matched points is as small as possible, for\nsome suitable distance function; the matching can then be\ninterpreted as a warping path. A thorough overview of\nDTW is given in [13].\n3. FORMAL ANALYSIS FRAMEWORK\nWe start with a formalization of tempo variations and then\ndescribe the proposed statistical analysis. The tempo vari-\nations we consider can be described by warping paths,\nwhich can be obtained from recordings of performances\nby using DTW.\n3.1 Formal Properties\nWe wish to compare tempo variations between different\nperformances of a piece. In this section, we consider an\nidealized setting in which only the local tempo is varied.\nIn the next section, we will discuss how this can be used\nfor analyzing variations in actual performances.\nFor our formal framework, we ﬁrst need a representa-\ntion of a performance. We will call the reference perfor-\nmanceg: [0;lg]!Rd, withlgthe length of the perfor-\nmance anddthe dimensionality of some suitable feature\nspace in which the performance can be represented. Other\nperformances in a collection, displaying tempo variations\nwith respect to the reference performance, can be deﬁned\nas follows:\nDeﬁnition 1. A performance of gwith varied tempo is a\nfunctionf=g\u000e : [0;lf]!Rd, withlfandddeﬁned as\nabove, and : [0;lf]![0;lg]a function with nonnegative\nderivative, i.e., _ \u00150. We call a tempo variation.\nFor the analysis of tempo variations between fandg,\nwe distinguish between average and relative tempo varia-\ntion. The average tempo variation can be observed by look-\ning at the length of the interval over which the functions are\nparametrized; it is simply the difference in average overall\ntempo of each performance. Clearly, the longer the in-\nterval, the slower the performance is on average. There\nis more structure in the details, of course, which is what\nthe relative variations attempt to capture. Speciﬁcally, this\nrefers to an analysis of tempo variations given that the per-\nformances are parametrized over an interval of the same\nlength, for instance, the unit interval.\nNow, to implement the concept of relative tempo varia-\ntions, we ﬁrst reparametrize the performances over the unit\ninterval. Given f: [0;lf]!Rd, we consider the nor-\nmalized performance f\u0003=f\u000e\u001a: [0;1]!Rd, where\n\u001a: [0;1]![0;lf]is given by\u001a(t) =lft. Now we can go\ninto more detail about these relative tempo variations.3.1.1 Structure of the Set of Relative Tempo Variations\nRelative tempo variations can be described by reparame-\ntrizations that relate the performances in question. Due to\nthe normalization of the performances, the reparametriz-\nations map the unit interval to itself. The relative tempo\nvariations'and their derivatives _'are characterized by\nthe following two properties:\nProperty 1. '(0) = 0 ,'(1) = 1 .\nProperty 2. _'(n)\u00150for anyn2[0;1].\nExamples of such relative tempo variations are shown\nin Figure 1 (left), along with insets to see what happens\nwhen one zooms in. When working with the normalized\nperformances, every performance with varied tempo f\u0003of\na reference performance g\u0003has the form f\u0003=g\u0003\u000e'.\nThe beneﬁt of splitting average and relative variation is\nthat the set of relative variations has a geometric structure:\nthe following lemma shows that it is a convex set in an\nvector space. This enables us to use classical methods from\nstatistical analysis to analyze the relative tempo variations,\nas explained in Section 3.2.\nLemma 1. Convex combinations of relative tempo varia-\ntions are relative tempo variations.\nProof. Let\u000b= (\u000b1;:::;\u000b m)be a vector of nonnegative\nnumbers,\u000bi\u00150, with unit`1norm,Pm\ni=1\u000bi= 1, and\nlet'i: [0;1]7![0;1]be relative tempo variations ( 1\u0014\ni\u0014m). We show that '=Pm\ni=1\u000bi'iis a relative tempo\nvariation. As a sum of functions on the unit interval, 'is\nalso a function on the unit interval. Since the \u000bisum to 1,Pm\ni=1\u000bi'i(0) = 0 andPm\ni=1\u000bi'i(1) = 1 , which means\nthat Property 1 holds. Finally, since all \u000biare nonnegative,\n_'\u00150is also maintained.\n3.2 Analysis of Prominent Variations\nIn the following, we consider a set of performances (with\nvaried tempo) and show how our approach allows us to\ncompute statistics on the set. Explicitly, we take the mean\nand perform principal component analysis (PCA). As a\nﬁrst step, we reparametrize the performances over the unit\ninterval [0;1], as described above. We distinguish two\nsettings for our analysis. First, we describe a setting in\nwhich we consider one reference performance. An exam-\nple of such a reference performance in practice is a ren-\ndered MIDI, which has a linear timing to which we relate\nthe actual performances in the set. In the second setting,\nwe avoid the use of a reference performance by incorpo-\nrating all pairwise comparisons between performances.\n3.2.1 Comparing to the Reference Performance\nComparing a set of performances ff1;f2;:::;f ngto a ref-\nerenceg\u0003means obtaining for each normalized perfor-\nmancef\u0003\nithe corresponding relative tempo variation 'i,\nsuch thatf\u0003\ni=g\u0003\u000e'i. Lemma 1 shows that we can build\na continuous set of relative tempo variations by building\nconvex combinations. Geometrically speaking, we con-\nsider the simplex spanned by the 'i. Though not neededProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 159Figure 1 . Several reparametrizations 'relating professional human performances of Chopin’s Mazurka op. 30 no. 2 to a\ndeadpan MIDI version. Original 'with zoomed insets (left) and their derivatives _'(right).\nfor our analysis, extrapolation out of the simplex is possi-\nble, as long as Property 2 is satisﬁed.\nA particularly interesting convex combination for our\npurposes is the mean of the set of performances. The mean\nrelative tempo variation \u0016'can be computed by setting all\nthe\u000bito the same value in Lemma 1 above. The mean\nof the normalized performances ff\u0003\nigis given asg\u0003\u000e\u0016'.\nTo obtain the mean of the performances, g\u0003\u000e\u0016'is lin-\nearly rescaled to the average length of the performances\nfi. The mean \u0016'gives information about which local tempo\nvariations away from g\u0003are the most prevalent among the\nperformances under analysis. Of course, the mean does\nnot capture the variance in the set, for example, deviations\nin opposite directions, as when some performers speed up\nand others slow down, which would be evened out.\nThe variance in a set can be analyzed using PCA. To\nperform a PCA on the set 'i, we need a scalar product\non the space of relative tempo variations. Since these are\nfunctions on the unit interval, any scalar product on this\nfunction space can be used. For our experiments, we used\ntheL2-scalar product of the derivatives of the functions\n(in other words the Sobolev H1\n0-scalar product). The rea-\nson for using a scalar product of the derivatives, rather\nthan the function values, is that the derivatives describe\nthe variations in tempo, and the function values encode\nthe alignment of the performance. See Figure 1 (right) for\nan example of how this brings out the variation. Once a\nscalar product is chosen, we construct the covariance ma-\ntrix, whose entries are the mutual scalar products of the\nfunctions'i\u0000\u0016'(the distance of the tempo variations to\nthe mean). The eigenvectors of the covariance matrix yield\nthe principal modes of variation in the set 'i. These ex-\npress the main variations away from the mean in the set\nand the eigenvalues indicate how much variance there is\nin the set of performances by how much of the variance\nis explained by the corresponding modes. The modes ex-\npress the tendency of performers to speed up or slow down\nobserved in the set of performances.3.2.2 Incorporating All Pairwise Comparisons\nWhen using a reference performance, one has to choose\nwhich performance to use as g\u0003, or to produce an artiﬁ-\ncial performance for g\u0003(as we do in Section 4). This way,\nthe comparison becomes dependent on the choice of g\u0003,\nwhich may not be desirable, as there may be ‘outlier’ per-\nformances that would not necessarily be the best choice\nfor a reference performance (though other things can be\nlearned from them [17]).\nTo avoid the need to choose g\u0003, we propose an alterna-\ntive analysis using all pairwise comparisons. This means\nobtaining reparametrizations 'for every pair of perfor-\nmancesf\u0003andg\u0003such thatf\u0003=g\u0003\u000e'. This makes\nsense, as it is not guaranteed that for three normalized per-\nformancesf\u0003,g\u0003andh\u0003and reparametrizations 'iand'j\nsuch thatg\u0003=f\u0003\u000e'iandh\u0003=g\u0003\u000e'j, we would get\nh\u0003=f\u0003\u000e'i\u000e'j. In other words, reparametrizations may\nviolate the triangle inequality, so we obtain more informa-\ntion by taking into account all possible reparametrizations.\nThe same techniques can be applied once we have the\n(extended) set of reparametrizations '. That is, we can\ntake the mean of all the 'or perform a PCA on them. Em-\npirically, it turns out there tends to be repeated information\nin the reparametrizations, which results in a certain amount\nof natural smoothing when taking the mean; this effect can\nbe seen in Figure 3.\n4. EXPERIMENTAL V ALIDATION\nIn Section 3, we considered a collection of performances\nwith tempo variations as compared to a reference perfor-\nmance. To perform the analyses described, we take the\nfollowing steps. First, we map the audio into some suitable\nfeature space; we take the chroma features implemented in\nthe MIRtoolbox [7] to obtain sequences of chroma vectors.\nWe then normalize these sequences to functions over the\nunit interval. Finally, we use DTW to compute the relative\ntempo variations 'that best align the performances.\nExplicitly, let f\u0003;g\u0003: [0;1]!Rdbe sequences of160 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017chroma vectors (in our case, d= 12 , as analysis at the\nsemitone resolution sufﬁces). Then DTW ﬁnds the func-\ntion'that satisﬁes Properties 1 and 2 and minimizes\nkf\u0003\u0000(g\u0003\u000e')k2, i.e., theL2norm of the difference be-\ntweenf\u0003and the reparametrized g\u0003. We generate 'in this\nway for all performances in the collection.\nOur goal is to analyze variations between performances.\nLocal tempo variation should be reﬂected in ', provided\nthere is not too much noise and the same event sequence is\nfollowed (e.g. no inconsistent repeats). The way we bring\nout the local tempo variation is by taking the derivative _'\n(cf. Section 3.2). A derivative larger/smaller than 1 indi-\ncates that the tempo decreases/increases relative to the ref-\nerence performance. Since the tempo variations are given\nas a discrete functions, we need to approximate the deriva-\ntives. We do this by ﬁtting a spline to the discrete data and\nanalytically computing the spline’s derivative.\nTo avoid the ground truth issue mentioned in Section 2,\nwe devise several classes of artiﬁcial data, representing dif-\nferent types of performance variations for which we want\nto verify the behavior of our analysis. We verify whether\nthe analysis is robust to noise and uniform variation in the\noverall tempo (the scalar value mentioned in Section 3).\nFurthermore, we consider different types of local tempo\nvariations, which, without loss of generalization, are in-\nspired by variations typically expected in classical music\nperformances.\nIn the previous section, we mentioned two possible\nanalysis strategies: considering alignments to a reference\nperformance or between all possible pairs of performances.\nSince the artiﬁcial data are generated not to have outliers,\nit is difﬁcult to apply the analysis that uses all possible\npairs to the artiﬁcial data. We therefore focus on the case\nof using a single reference performance, although we will\nbrieﬂy return to the possibility of using all pairs in Section\n5.\n4.1 Generating Data\nThe data were generated as follows. We start with a se-\nquenceg2R12\u0002mofm12-dimensional Gaussian noise\nvectors. Speciﬁcally, for each vector gi, each element gi;j\nis drawn from the standard normal distribution N(0;1).\nWe then generate a collection Cof ‘performances’ based\nong, for seven different variation classes. We normalize\nthe vectors inCsuch that each element is between 0 and 1,\nas it would be in natural chroma vectors. The classes are\ndeﬁned as follows:\nClass 1: Simulate minor noise corruption. A new sequence\ncis generated by adding a sequence h2R12\u0002mof 12-\ndimensional vectors, where each element hi;j\u0018N(0;1\n4),\nsoc=g+h. We expect this does not lead to any signiﬁ-\ncant alignment difﬁculty, so the derivative of the resulting\n\u0016'(which we will call _\u0016') will be mostly ﬂat.\nClass 2: Simulate linear scaling of the overall tempo by\nstretching the time. Use spline interpolation to increase the\nnumber of samples in g, to simulate playing identically, but\nwith varying overall tempo. If there are nsequences gen-erated, vary the number of samples from m\u0000n\n2tom+n\n2.\nSince this only changes ‘performances’ on a global scale,\nthis should give no local irregularities in the resulting _\u0016'.\nClass 3: Simulate playing slower for a speciﬁc section\nof the performance, with sudden tempo decreases towards\na ﬁxed lower tempo at the boundaries, mimicking com-\nmon tempo changes in an A-B-A song structure. Interpo-\nlate the sequence to have 1.2 times as many samples be-\ntween indices l=1\n3m\u00001\n2Xandh=2\n3m+1\n2X, where\nX\u0018U(0;m\n10)(the same randomly drawn Xis used in\nboth indices). We expect _\u0016'to be larger in the B part than\nin A parts. Since in different samples, the tempo change\nwill occur at different times, transitions are expected to be\nobserved at the tempo change intervals.\nClass 4: A variation on class 3. Simulate a disagreement\nabout whether to play part of the middle section slower.\nLetk=h\u0000l. With a probability of 0:5, do not interpolate\nthe section from l+k\n3toh\u0000k\n3. We expect similar results\nas for class 3 with the difference that in the middle of the\nB part, we expect an additional jump in _\u0016'. In the B part, _\u0016'\nwill jump to a lower value, which should still be larger than\nthe value in the A part since only half of the performances\ndecrease the tempo.\nClass 5: Simulate a similar A-B-A tempo structure as\nin class 3, but change the tempo gradually instead of in-\nstantly over intervals of size roughly1\n6m. From index\nl1=1\n4m\u00001\n2Xtol2=5\n12m+1\n2X, gradually slow\ndown to 120% of the original tempo by interpolating over\na quadratic query interval1, then gradually speed up again\nthe same way between indices h1=7\n12m\u00001\n2Xand\nh2=3\n4m+1\n2X. Here,X\u0018U(0;1\n18m)and is drawn\nonly once. Here again, we expect to see smaller values\nof_\u0016'in the A parts and a higher value in the B part. Due\nto the gradual change in tempo, we expect a more gradual\ntransition between A-B and B-A.\nClass 6: A variation on class 5. Instead of varying the\ninterval using X, vary the tempo. First speed up the\ntempo by a factor 1:3 +Ytimes the starting value (with\nY\u0018U(\u00001\n10;1\n10)), then gradually slow down to a lower\ntempo and again speed up before the regular tempo of A\nis reached again. Here we expect to see a peak in _\u0016'at the\ntransition from A to B, before the lower value in the B part\nis reached and again a peak in the transition from B to A.\nClass 7: Another variation on class 5: disagreement about\nspeeding up or slowing down. Toss a fair coin ( p= 0:5);\non heads, gradually increase the tempo between l1andl2\nto1:2+Ytimes the starting value and decrease it again be-\ntweenh1andh2as in class 5. On tails, decrease the tempo\nto0:8+Ytimes the starting value between l1andl2and in-\ncrease it again between h1andh2, withY\u0018U(\u00001\n10;1\n10).\nWe expect this to give much more noisy alignment, though\nthere may be a more stable area in _\u0016'where the tempos do\nnot change, even though they are different.\n1Normal linear interpolation corresponds to a constant tempo curve,\nbut if the tempo curve changes linearly, the query interval for interpola-\ntion becomes quadratic.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 1610 0.2 0.4 0.6 0.8 1\ntime (normalized)0.9511.051.1˙¯ϕClass 1\nClass 2\nClass 3\nClass 4\n0 0.2 0.4 0.6 0.8 1\ntime (normalized)0.9511.051.11.151.2˙¯ϕClass 5\nClass 6\nClass 7\n0 0.2 0.4 0.6 0.8 1\ntime (normalized)-0.0500.050.10.153 modes of class 4\n1st mode\n2nd mode\n3rd modeFigure 2 . On the left: _\u0016'for class 1–4. In the middle, _\u0016'for class 5–7. On the right: the ﬁrst three PCA modes for class 4.\nWhen running our analysis on the classes of artiﬁcial\ndata thus generated, we always took m= 500 and gen-\nerated 100 sequences for each class. We used Matlab to\ngenerate the data, using 2017 as the seed for the (default)\nrandom number generator. A GitHub repository has been\nmade containing the code for the analysis and for gener-\nating the test data2. The experiment was run 100 times,\nresulting in 100 \u0016's and 100 sets of PCA modes; we took\nthe mean for both and show the results in ﬁgures: Figure 2\n(left and middle) show the derivatives when taking the\nmean (each time) as described in Section 3, while Figure 2\n(right) shows what happens when taking the PCA, as also\ndescribed in Section 3. We show the ﬁrst three modes\nbecause these empirically turn out to cover most (around\n90%) of the variance.\n4.2 Discussion\nWe now brieﬂy discuss what the analyses on artiﬁcial data\ntell us. First of all, the observed outcomes match our ex-\npectations outlined above. This demonstrates that our anal-\nysis can indeed detect the relative tempo variations that we\nknow are present in performances of music.\nWe want to note that Figure 2 shows the derivatives of\nthe relative tempo variation. For example, for class 3, all\nperformances are shorter than the reference; therefore, they\nare stretched during the normalization. Consequently, the\n_\u0016'in part A in the normalized performance is smaller than\n1. This effect could be compensated by taking the length\nof the performances into account.\nThe PCA modes provide information about the varia-\ntion in the set of performances. Figure 2 shows the ﬁrst\nthree modes found in Class 4. These three modes are the\nmost dominant and explain more than 90% of the varia-\ntion. The ﬁrst mode has a large value in the middle part of\nthe B section. This follows our expectation as only 50% of\nthe performances slow down in this part, hence we expect\nmuch variation in this part. In addition, there are small\nvalues in the other parts of the B section. This is due to\nthe fact that the performances do not speed up at the same\ntime, so we expect some variation in these parts. Note that\nthe principal modes are linear subspaces, hence sign and\nscale of the plotted function are arbitrary. An effect of this\n2https://github.com/asharkinasuit/\nismir2017paper .is that the modes cannot distinguish between speeding up\nthe tempo or slowing it down. Since the ﬁrst mode cap-\ntures the main variation in the middle part of the B section,\nin the second mode the transitions between A and B are\nmore emphasized. The third mode emphasizes the transi-\ntions too.\nFinally, we note that it becomes possible to zoom in on\na particular time window of a performance, in case one\nwants to do a detailed analysis. A hint of this is shown in\nFigure 1, left, where zoomed versions of 'are shown in\ninsets. We have defaulted in our experiments to analyz-\ning performances at the global level, and consider it future\nwork to explore what information will be revealed when\nlooking at the warping paths up close.\n5. APPLICATIONS\nNow that we have validated our approach, we describe sev-\neral applications in which our method can be employed.\n5.1 Analyzing Actual Performances\nAs mentioned in Section 3, we can analyze relative dif-\nferences between a chosen reference performance and the\nother performances, or between all possible pairs of per-\nformances. We have access to the Mazurka dataset con-\nsisting of recordings of 49 of Chopin’s mazurkas, partially\nannotated by Sapp [21]. Note that our analysis can handle\nany collection of performances and does not require anno-\ntations. Since we have no ground truth, it is difﬁcult to\nmake quantitative statements, but in this and the following\nsubsection, we will discuss several illustrative qualitative\nexamples.\nIn Figure 3, we show _\u0016'for Mazurka op. 30 no. 2 for\nboth approaches. Taking all pairs into consideration results\nin lower absolute values, as well as an apparent lag. For\nboth approaches, it turns out the most important structural\nboundaries generally show up as the highest peaks. An-\nother feature that stands out in both plots is the presence of\npeaks at the beginning and end. These can be interpreted\nas boundary effects, but we believe the ﬁnal peak also is\ninﬂuenced by intentional slowing down by the musicians\nin a ﬁnal retard [25].\nAnother example of applying the analysis on all pairs of\nperformances is given in Figure 4. Here, we see two more162 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170 0.2 0.4 0.6 0.8 1\ntime (normalized)0.811.21.41.61.8˙¯ϕAll to MIDI\nAll to allFigure 3 . Sample showing _\u0016'for Mazurka op. 30 no. 2,\ncomparing warping to a deadpan MIDI and warping ev-\nerything to everything. Note the smoothing effect in the\nlatter case. Salient structural parts are indicated with verti-\ncal lines: repeats (dotted) and structural boundaries (solid).\ninteresting features of the analysis. Firstly, it tends to hint\nat the musicians’ interpretation of the structure of the piece\n(as also in Figure 3); the start of the melody is indicated\nwith the vertical dashed line. Most performers emphasize\nthis structural transition by slowing down slightly before it.\nHowever, the time at which they slow down varies slightly\n(compare this to e.g. class 3 and 5 of our artiﬁcial data).\nThis will show in ', and consequently in _\u0016'. Secondly, we\nnote that ornaments tend not to vary tempo as much: the\nthin section in the ﬁgure is closer to 1 than the peak near\nthe start of the melody. This helps corroborate Honing’s\nresults, e.g. [2, 5].\n5.2 Guiding Synthesis\nFor the performances in question, we know the piece that is\nperformed and we have a score available. A direct acous-\ntic rending of the score (via MIDI) would sound unnatu-\nral. Now, reparametrizations and their means are just func-\ntions, which we can apply to any other suitably deﬁned\nfunction. Following the suggestion in [20] that a generated\n‘average’ performance may be more aesthetically pleasing,\nwe can now use these functions for this: by applying the \u0016'\nderived from a set of performances to a MIDI rendition, a\nmore natural-sounding result will indeed be obtained. As\nan example, we ran our analysis on Chopin’s mazurka op.\n24 no. 2 with the MIDI rendition as reference performance\nand applied the resulting reparametrization to the MIDI3.\nNote that, as in Figure 3, the tempo naturally decreases to-\nwards the end.\nApplying \u0016'directly to audio is not the only thing that\nwe can do. One possibility is exaggeration of tempo varia-\ntion. To amplify sections that show major tempo variation,\nwe can modify the 'by squaring it. Alternatively, to better\ndisplay the tempo variations in an individual performance,\nwe can rescale the function '\u0000\u0016', capturing the difference\nof the actual performance to the mean in a performance\n3See https://github.com/asharkinasuit/\nismir2017paper , which includes the original for comparison.\n0 0.05 0.1 0.15 0.2 0.25\ntime (normalized)0.9511.051.11.15˙¯ϕFigure 4 ._\u0016'of the start of mazurka op. 17 no. 4. The start\nof the melody is marked with a vertical dashed bar, while\nthedelicatissimo section is drawn in a thinner line.\ncollection. Such modiﬁcations offer useful analysis tools\nfor bringing out more clearly the sometimes subtle effects\nemployed by professional musicians.\nAnother possibility is to take 'from various sources,\ne.g., by generating 'for several different reference perfor-\nmances, and applying them to a MIDI rendition with vari-\nous coefﬁcients to achieve a kind of mixing effect. Finally,\nthe principal modes of variation in the set can be used to\nmodify the tempo in which the MIDI is rendered. Exam-\nple audio ﬁles are available on request for any of these dif-\nferent ways of rendering musical scores using information\nfrom actual performances.\n6. CONCLUSIONS AND FUTURE WORK\nWe have presented a formal framework for analyzing rel-\native local tempo variations in collections of musical per-\nformances, which enables taking the mean and computing\na PCA of these variations. This can be used to analyze a\nperformed piece, or synthesize new versions of it.\nSome challenges may be addressed in the future. One\nwould be to give a more rigorous interpretation to the case\nof taking all pairwise comparisons into account. Further-\nmore, quantiﬁcation of variation still presently is used in\na relative fashion; our analysis indicates some amount of\nvariation, but further interpretation of this amount would\nbe useful. One might also substitute other DTW variants\nthat can e.g. deal more intuitively with repeat sections [4].\nFurthermore, while the studied variation classes were\ninspired by local tempo variations in classical music per-\nformances, it should be noted that our framework allows\nfor generalization, being applicable to any collection of\nalignable time series data. Therefore, in future work, it\nwill be interesting to investigate applications of our pro-\nposed method on other types of data, such as motion track-\ning data.\n7. REFERENCES\n[1] A. Benetti Jr. Expressivity and musical performance:\npractice strategies for pianists. In 2nd Performance\nStudies Network Int. Conf. , 2013.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 163[2] P. Desain and H. Honing. Does expressive timing in\nmusic performance scale proportionally with tempo?\nPsychological Research , 56(4):285–292, 1994.\n[3] S. Flossmann, M. Grachten, and G. Widmer. Expres-\nsive Performance Rendering with Probabilistic Mod-\nels. In Guide to Computing for Expressive Music Per-\nformance , pages 75–98. Springer, 2013.\n[4] M. Grachten, M. Gasser, A. Arzt, and G. Widmer. Au-\ntomatic alignment of music performances with struc-\ntural differences. In ISMIR , 2013.\n[5] H. Honing. Timing is Tempo-Speciﬁc. In ICMC , 2005.\n[6] K. Kosta, O. F. Bandtlow, and E. Chew. Practical Im-\nplications of Dynamic Markings in the Score: Is Piano\nAlways Piano? In 53rd AES Conf. on Semantic Audio ,\n2014.\n[7] O. Lartillot and P. Toiviainen. A matlab toolbox for\nmusical feature extraction from audio. In Int. Conf.\nDigital Audio Effects , pages 237–244, 2007.\n[8] E. Liebman, E. Ornoy, and B. Chor. A phylogenetic ap-\nproach to music performance analysis. Journal of New\nMusic Research , 41(2):195–222, 2012.\n[9] C. C. S. Liem and A. Hanjalic. Expressive Timing from\nCross-Performance and Audio-based Alignment Pat-\nterns: An Extended Case Study. In ISMIR , pages 519–\n524, 2011.\n[10] C. C. S. Liem and A. Hanjalic. Comparative analysis\nof orchestral performance recordings: an image-based\napproach. In ISMIR , 2015.\n[11] C. C. S. Liem, A. Hanjalic, and C. S. Sapp. Expres-\nsivity in musical timing in relation to musical structure\nand interpretation: a cross-performance, audio-based\napproach. In 42nd AES Conf. Semantic Audio , 2011.\n[12] L. Lu, M. Wang, and H. Zhang. Repeating pattern dis-\ncovery and structure analysis from acoustic music data.\nIn6th ACM SIGMM Int. Workshop on Multimedia In-\nformation Retrieval , pages 275–282. ACM, 2004.\n[13] M. M ¨uller. Fundamentals of Music Processing: Audio,\nAnalysis, Algorithms, Applications . Springer, 2015.\n[14] M. M ¨uller and S. Ewert. Joint Structure Analysis with\nApplications to Music Annotation and Synchroniza-\ntion. In ISMIR , pages 389–394, 2008.\n[15] M. M ¨uller and F. Kurth. Enhancing similarity matrices\nfor music audio analysis. In IEEE Int. Conf. Acoustics,\nSpeech and Signal Processing , volume 5. IEEE, 2006.\n[16] O. Nieto and T. Jehan. Convex non-negative matrix fac-\ntorization for automatic music structure identiﬁcation.\nInIEEE Int. Conf. Acoustics, Speech and Signal Pro-\ncessing , pages 236–240. IEEE, 2013.[17] M. Ohriner. What can we learn from idiosyncratic per-\nformances? Exploring outliers in corpuses of Chopin\nrenditions. In Proc. of the Int. Symp. on Performance\nScience , pages 635–640, 2011.\n[18] Y . Panagakis, C. Kotropoulos, and G. R. Arce. `1-graph\nbased music structure analysis. In ISMIR , 2011.\n[19] J. Paulus and A. Klapuri. Music structure analysis by\nﬁnding repeated parts. In Proc. of the 1st ACM Audio\nand Music Computing Multimedia Workshop , pages\n59–68. ACM, 2006.\n[20] B. H. Repp. The aesthetic quality of a quantitatively\naverage music performance: Two preliminary exper-\niments. Music Perception: An Interdisciplinary Jour-\nnal, 14(4):419–444, 1997.\n[21] C. S. Sapp. Comparative Analysis of Multiple Musical\nPerformances. In ISMIR , pages 497–500, 2007.\n[22] C. S. Sapp. Hybrid numeric/rank similarity metrics for\nmusical performance analysis. In ISMIR , pages 501–\n506, 2008.\n[23] J. Serr `a, M. M ¨uller, P. Grosche, and J. L. Arcos. Un-\nsupervised music structure annotation by time series\nstructure features and segment similarity. IEEE Trans.\nMultimedia , 16(5):1229–1240, 2014.\n[24] J. B. L. Smith, I. Schankler, and E. Chew. Listening as\na Creative Act: Meaningful Differences in Structural\nAnnotations of Improvised Performances. Music The-\nory Online , 20(3), 2014.\n[25] J. Sundberg and V . Verrillo. On the anatomy of the re-\ntard: A study of timing in music. Journal of the Acous-\ntical Society of America , 68:772–779, 1980.\n[26] G. Widmer and A. Tobudic. Playing Mozart by Anal-\nogy: Learning Multi-level Timing and Dynamics\nStrategies. Journal of New Music Research , 32(3):259–\n268, 2003.164 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Score-Informed Syllable Segmentation for A Cappella Singing Voice with Convolutional Neural Networks.",
        "author": [
            "Jordi Pons",
            "Rong Gong",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.345490",
        "url": "https://doi.org/10.5281/zenodo.345490",
        "ee": "https://ismir2017.smcnus.org/wp-content/uploads/2017/10/46_Paper.pdf",
        "abstract": "This dataset is a collection of syllable boundary annotations and syllable duration annotations of a cappella singing performed by jingju (京剧, Beijing opera) professional and amateur singers. This dataset was used as the experimental dataset in the following work:\n\n\nRong Gong, Nicolas Obin, Georgi Dzhambazov and Xavier Serra, Score-Informed syllable segmentation for jingju a cappella singing voice with Mel-frequency intensity profiles, inFolk Music Analysis workshop (FMA) 2017, Mlaga, Spain\n\n\nAudio Content\n\nThe audio files are the a cappella singing arias recordings, which are stereo or mono, sampled at 44.1 kHz, and stored as wav files. They can be found at this link http://doi.org/10.5281/zenodo.344932\n\nThe wav files are recorded by two institutes: those file names ending with qm are recorded by C4DM Queen Mary University of London; others file names ending with upf or lon are recorded by MTG-UPF. If you use the dataset in your work, please cite the following publication.\n\n\nD. A. A. Black, M. Li, and M. Tian, Automatic Identification ofEmotional Cues in Chinese Opera Singing, in13th Int. Conf. on MusicPerception and Cognition(ICMPC-2014), 2014, pp. 250255.\n\n\nAnnotations\n\nThe syllable boundary annotation is in Textgrid format (Praat). The annotation is done in both phrase-level and syllable-level. The syllable duration annotation is in cvs format. Please consult Readme text in both folders for further details. The parsing code of the annotation files is provided in pycode folder.\n\nAvailability of the Dataset\n\nThe annotations and codes in this dataset are licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.\n\nContact\n\nIf you have any questions or comments about the dataset, please feel free to write to us.\n\nRong Gong: rongdotgongatupfdotedu\n\nRafael Caro Repetto: rafaeldotcaroatupfdotedu",
        "zenodo_id": 345490,
        "dblp_key": "conf/ismir/PonsGS17",
        "keywords": [
            "This dataset is a collection of syllable boundary annotations and syllable duration annotations",
            "jingju (京剧",
            "Beijing opera) professional and amateur singers",
            "experimental dataset in the following work",
            "audio files are a cappella singing arias recordings",
            "stored as wav files",
            "recorded by C4DM Queen Mary University of London and MTG-UPF",
            "Automatic Identification of Emotional Cues in Chinese Opera Singing",
            "Creative Commons Attribution-NonCommercial 4.0 International License",
            "contact information for Rong Gong and Rafael Caro Repetto"
        ]
    },
    {
        "title": "Quantized Melodic Contours in Indian Art Music Perception: Application to Transcription.",
        "author": [
            "H. G. Ranjani",
            "Deepak Paramashivan",
            "Thippur V. Sreenivas"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417567",
        "url": "https://doi.org/10.5281/zenodo.1417567",
        "ee": "https://zenodo.org/records/1417567/files/RanjaniPS17.pdf",
        "abstract": "R¯agas in Indian Art Music have a florid dynamism asso- ciated with them. Owing to their inherent structural intri- cacies, the endeavor of mapping melodic contours to mu- sical notation becomes cumbersome. We explore the po- tential of mapping, through quantization of melodic con- tours and listening test of synthesized music, to capture the nuances of r¯agas. We address both Hindustani and Car- natic music forms of Indian Art Music. Two quantization schemes are examined using stochastic models of melodic pitch. We attempt to quantify the salience of r¯aga per- ception from reconstructed melodic contours. Perception experiments verify that much of the r¯aga nuances inclu- sive of the gamaka (subtle ornamentation) structures can be retained by sampling and quantizing critical points of melodic contours. Further, we show application of this re- sult to automatically transcribe melody of Indian Art Mu- sic.",
        "zenodo_id": 1417567,
        "dblp_key": "conf/ismir/RanjaniPS17",
        "keywords": [
            "Indian Art Music",
            "R¯agas",
            "Florid dynamism",
            "Structural intricacies",
            "Mapping melodic contours",
            "Musical notation",
            "Quantization schemes",
            "Stochastic models",
            "Salience of r¯aga perception",
            "Automatic transcription"
        ],
        "content": "QUANTIZED MELODIC CONTOURS IN INDIAN ART MUSIC\nPERCEPTION: APPLICATION TO TRANSCRIPTION\nRanjani, H. G.\nDept of ECE,\nIndian Institute of Science,\nBangalore\nranjani@iisc.ac.inDeepak Paramashivan\nDept of Music,\nUniversity of Alberta,\nCanada\nparamash@ualberta.caThippur V . Sreenivas\nDept of ECE,\nIndian Institute of Science,\nBangalore\ntvsree@iisc.ac.in\nABSTRACT\nR¯agas in Indian Art Music have a ﬂorid dynamism asso-\nciated with them. Owing to their inherent structural intri-\ncacies, the endeavor of mapping melodic contours to mu-\nsical notation becomes cumbersome. We explore the po-\ntential of mapping, through quantization of melodic con-\ntours and listening test of synthesized music, to capture the\nnuances of r¯agas . We address both Hindustani and Car-\nnatic music forms of Indian Art Music. Two quantization\nschemes are examined using stochastic models of melodic\npitch. We attempt to quantify the salience of r¯aga per-\nception from reconstructed melodic contours. Perception\nexperiments verify that much of the r¯aganuances inclu-\nsive of the gamaka (subtle ornamentation) structures can\nbe retained by sampling and quantizing critical points of\nmelodic contours. Further, we show application of this re-\nsult to automatically transcribe melody of Indian Art Mu-\nsic.\n1. INTRODUCTION\nMelody contours are often perceived as continuous func-\ntions though generated from notes which assume discrete\npitch values. The rendition of a r¯aga, the melodic frame-\nwork of Indian Art Music (IAM), is a ﬂorid movement\nacross notes, embellished with suitable ornamentations\n(gamakas ). Several engineering approaches to analyse\nand/or model pitch contours rely on ‘stable’ notes [5, 12];\nyet, it contradicts the perceptions and claims of musicians\nin both Carnatic and Hindustani forms of music and also\nthat of detailed experiments which assert that it is actu-\nally the manner of approaching notes that characterizes a\nr¯aga[1, 6]. Algorithms to automatically align note tran-\nscription to melodic contours show promise more at a\nrhythm cycle level rather than at a note level [21], lead-\ning to a hypothesis that it is necessary to study the role of\npitch in rendering notes rather than ﬁnding / transcribing\nc\rRanjani, H. G., Deepak Paramashivan, Thippur V .\nSreenivas. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Ranjani, H. G., Deepak\nParamashivan, Thippur V . Sreenivas. “Quantized melodic contours in\nIndian Art Music Perception: Application to transcription”, 18th Inter-\nnational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.\n00.20.40.60.8 11.21.41.6\nTime (s) !100150200250300Pitch (Hz) !M G PFigure 1 . [Color Online] Contour of a vocal melodic clip\n(rendition by Vid. Deepak Paramashivan) in Thodi r ¯agaof\nCarnatic music form (tonic frequency at 146.83 Hz). The\ntranscribed notes correspond to ‘ MaPaGa ’. The presence\nof ornamentations pose difﬁculty for transcription.\nnotes from pitch contours [11]. The difﬁculty involved in\nidentifying notes from a rendered melodic contour can be\nseen in Figure 1.\nIn this work, we analyze pitch contours in an attempt to\nunderstand (i) how musicians possibly assess a correctly\nrendered note (ii) how they approach subsequent note(s) in\nar¯aga. We explore the possibility to incorporate this un-\nderstanding to engineer an automated framework to repre-\nsent a r¯again terms of note sequences. A perceptual study\nof the effects of two quantization schemes on r¯agachar-\nacteristics ( r¯aga-bhava ) is explored. For a more detailed\nexposition of r¯agas in Indian Art Music, interested readers\ncan refer to [19, 20].\n1.1 Complexity of Pitch Contours in Indian Art Music\nAr¯agacontains 3 structures of information : (i) Pitch po-\nsitions of notes ( swarasth ¯ana) (ii) Ornamentation of notes\n(Gamaka ) (iii) Note movement ( swarasanch ¯ara). All the\nthree structures are coupled in a r¯agarendition. The note\nposition is embellished with gamakas , and is also depen-\ndent on the note transitions themselves.\nIn [13], different notes and their transitions are stud-\nied and classiﬁed as ‘inﬂected intervals’, ‘transient notes’\nand ‘transient inﬂexions’, while acknowledging that musi-174cal insight and knowledge is necessary to distinguish be-\ntween different transitions.\nFrom an engineering perspective, in a r¯aga, we observe\nthat inspite of all such note transitions detected at both ﬁner\nand coarser levels, the peaks, valleys and stable regions\ncorrespond to discrete pitch values with an error factor. It\nis also logical that a musician perceives these points to as-\nsess if the intended note has been reached1. The salience\nof peaks, valleys and stable regions is utilized for motif\nspotting in Carnatic music in [10]. However, not all peaks,\nvalley regions can correspond to notes as per conventional\ntranscriptions [15]. As an engineering approach, we pro-\npose to discretize or quantize the pitch contours at these\ncritical points using a semi-continuous Gaussian mixture\nmodel (SC-GMM) proposed in [18] (Section 2) and thus\nmap continuous pitch contours to discrete note sequences.\nWe believe such a mapping brings us closer to understand-\ning the structures of r¯agas in accordance with the theory\nthat discrete elements carry the structural information of\nmusical forms while expressions are realized in continu-\nous variations [14].\nWhile analysing contours w.r.t. discrete pitch values,\nwe often encounter scenarios in which pitch values can ei-\nther overshoot or undershoot the intended values as can be\nseen in Figure 1; this is also reported in literature [13, 15].\nThe following reasons can be attributed to such detours\nw.r.t. discrete pitch values - (i) Performers’ intent to gener-\nate certain perceived effect in the listener (ii) Possible de-\nviations during learning/ fast renditions (iii) Creative free-\ndom and margin of error allowed in rendering a r¯agaas an\nart form. Any deviation which does not bring about the re-\nquired perceptual effect can cause a connoisseur/musician\nto not appreciate the rendition in its totality.\nIn this work, we assume the deviations to be due to any\nof above reasons and hence is part of errors in quantizing\npitch contours. If the quantization process has disregarded\nthe musically intended overshooting and undershooting of\npitch values, it only implies that the effect of the r¯aga is\nnot captured completely in the quantized sample. In or-\nder to analyze the importance of limits of quantization,\nwe reconstruct the melody from quantized sequences and\nconduct perception experiments on these melodies (Sec-\ntion 3.3). Further, we propose a framework by using these\nquantized notes to transcribe a contour (Section 4.1).\n2. QUANTIZATION MODEL\nGiven pitch contours, y(t)estimated from audio record-\nings, it is possible to identify the tonic frequency fTas\nshown in [7, 18]. The pitch contours are tonic normal-\nized and mapped to a common tonic, fU; letyn(t) =\ny(t)\u0003fU=fTdenote pitch contours mapped to common\ntonic frequency2. This helps to analyze different rendi-\n1This also explains the fact that music listeners do not perceive in-\ntermediate notes during note transitions which are greater than a semi-\ntone; for example, when a musician glides from Sa (tonic) to Pa (ﬁfth)\nin ar¯aga, we do not perceive all the intermediatary semi-tones which the\nglide passes through.\n2In this work, fUis chosen as 146.83 Hz corresponding to D3note\nof Western scale.tions of same r¯aga. Let\u001c=ftjryn(t) = 0gbe the set\nof critical points and x=fyn(\u001c)gbe the corresponding\ncritical pitch values. The tuple X= (x;\u001c)are the critical\npoints ofyn(t). Mathematically, critical points can be ob-\ntained only if a function is differentiable. We estimate X\nfrom the zero crossings of numerical gradient of yn(t).\n2.1 Semi-Continuous Gaussian Mixture Model\nConsider the Semi-Continuous Gaussian Mixture Model\n(SC-GMM) [18] with Knumber of components whose\nmeans,\u0016k8k2f1;2;:::;Kgwithin an octave are ﬁxed in\naccordance to the note ratios used in IAM, as shown in Ta-\nble 1. The distribution of pitch values in ynand the critical\npitch valuesxcan be modeled using SC-GMM as:\np(y) =KX\nk=1\u000bk;yN(yn;\u0016k;\u001bk;y) (1)\np(x) =KX\nk=1\u000bk;xN(x;\u0016k;\u001bk;x) (2)\nFor a ﬁxedKcomponents, the set of parameters estimated\nfrom distribution of pitch are f\u000by;\u001bygandf\u000bx;\u001bxg.\u0016\nparameters are not estimated since they are ﬁxed and are\nsame in both cases.\n2.2 Quantization using SC-GMM\nWe use the above model to quantize pitch contours. Each\npitch sample of yn(t)can be quantized to a nearest com-\nponent of SC-GMM which maximizes its probability:\nk\u0003\ny(t) = arg max\nk2f1;2;:::;Kg\u000bk;yN(yn(t);\u0016k;\u001bk;y) (3)\nSimilarly, every critical pitch of xcan be quantized as:\nk\u0003\nx(\u001c) = arg max\nk2f1;2;:::;Kg\u000bk;xN(x(\u001c);\u0016k;\u001bk;x) (4)\nThus, bothynandxare now quantized and correspond\nto a sequence of notes; their temporal information (corre-\nsponding toftgandf\u001cg) are retained.\n3. SYNTHESIS FROM QUANTIZED SEQUENCE\nOF NOTES\nTo check if this mapping process captures the essence of\nr¯agas and to assess the effect of quantization on r¯agaper-\nception, we conduct perception experiments. Audio clips\nare synthesized for perception. In order to synthesize audio\nfrom quantized note sequences, we ﬁrst synthesize melody\ncontours, and use the same to synthesize audio clips.\n3.1 Quantized Pitch Contour\nThe un-quantized yn(t), the quantized k\u0003\ny(t)andk\u0003\nx(\u001c)are\ninterpolated to obtain a contour sampled at Fs, the sam-\npling frequency of the discrete-time audio signal3. Piece-\nwise cubic hermite interpolating polynomial is used with\n3yn(t)also requires interpolation as it is estimated at frame rate\ncoarser than Fs.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 175(t;yn(t)),(t;\u0016k\u0003y(t))and(\u001c;\u0016k\u0003x(\u001c))being knots for each\nof the interpolation. These result in 3 different pitch con-\ntours for signal synthesis. The pitch contour obtained from\ninterpolating (t;yn(t))can be considered (for all practi-\ncal purposes) as a reference pitch contour. A compari-\nson of pitch contours obtained by interpolating (\u001c;k\u0003\nx)and\n(t;k\u0003\ny(t))are shown in Figure 2.\nFigure 2 . [Color Online] Pitch contours obtained by inter-\npolation of (a) yn(t)(b)k\u0003\ny(t)and (c)k\u0003\nx(\u001c)(S¯averi r ¯aga).\nfU= 146:83Hzfor all 3 contours. Red dots are knots of\ninterpolation; solid blue lines are the interpolated contours.\nArrows highlight local distortions in interpolated contours.\nWe see from Figure 2(b) that quantizing every pitch\nsample (k\u0003\ny(t)) can potentially lead to introducing addi-\ntional stable notes (which are not perceived in the origi-\nnal contour). This may result in an unfavorable shape of\nthe contour and hence dynamics of notes may be altered\nduring interpolation. The advantage of quantized critical\npitch samples ( k\u0003\nx(\u001c)) is that, on interpolation, the dynam-\nics around a note are more likely to be retained as seen in\nFigure 2(c). Some subtle gamakas , in-spite of being cap-\ntured, could be quantized onto single note due to statistical\nweight of an adjacent note, resulting in synthesizing a ﬂat\nregion. In this work, pitch values at critical points are per-\nturbed while the critical points themselves are unaffected;\nhence slopes might be perturbed in a different manner in\nvarious sections of the pitch contours and hence percep-\ntual effect of the same is not simple to predict.\n3.2 Synthesizing Audio\nThe audio signal for perception can be synthesized from\nthe interpolated pitch contours (sampled at Fs) using the\ntime-varying sinusoidal synthesis model. The model can\nbe expressed as:\n^sf(t) =a(t)\u0003 HX\ni=1sin\u00122\u0019h\nFsZt\n0f(t)dt\u0013!\n(5)\nwherea(t)represents the vocal-tract shaping ﬁlter, \u0003\nis the convolution operation, Hdenotes the number ofharmonics, Fsis the sampling frequency, f(t)repre-\nsents the pitch contour which is to be synthesized (ex-\nplained in Section 3.1) andRt\n0is approximated as cu-\nmulative sum for discrete implementation. The vocal-\ntract shaping ﬁlter is chosen to be time-invariant and is\nthat of vowel / ¯a/. An all-pole model is used to synthe-\nsize the transfer function using formant frequencies and\nbandwidth of / ¯a/ as (730;1090;2440;3781;4200)Hzand\n(60;50;102;309:34;368)Hzrespectively [17]. A drone\nsignal is added to the synthesized audio so that reference\ntonic is present in it.\n3.3 Perception Test Experiments\nLet^sref(t)be the audio signal synthesized from interpo-\nlated (t;yn(t)),^sy(t)be the audio synthesized from inter-\npolated (t;k\u0003\ny(t))and^sx(t)synthesized from interpolated\n(\u001c;k\u0003\nx(t)). We quantize using both 22-note and 12-note in-\ntervals to study the effect of number of quantization levels\nonr¯agaperception i.e., ^sy22(t)and^sx22(t)are the audio\nsignals synthesized using (3) and (4) with K= 22\u00033\n(covering 3 octaves), while ^sy12(t)and^sx12(t)correspond\ntoK= 12\u00033levels. The means within an octave of the\nSC-GMM are as chosen according to Table 1.\nWe choose certain r¯agas along with the correspond-\ning pitch features from the publicly available Carnatic and\nHindustani music database used in [8, 9]; in this database,\npitch has been estimated every 4:44msusing Essentia [3].\n3.3.1 Comparison of ^sy(t)and^sx(t)\nAs argued earlier, we hypothesize ^sx(t)to be a closer rep-\nresentative of ^sref(t)than ^sy(t). To verify which among\n^sx(t)is indeed perceptually closer to ^sref(t), a MUSHRA\n(MUltiple Stimuli with Hidden Reference and Anchor)\nkind of experiment is performed. We select K= 12\u00033for\nquantization levels. 6 musically trained listeners are tasked\nwith three experiments - different reference clips (average\n7 s duration) from Nattai r ¯agaof Carnatic music are pre-\nsented in each experiment. Within each experiment - (i)\n^sy12(t)(ii)^sx12(t)(iii) hidden ^sref(t)- form 3 audio stim-\nuli presented to listeners in randomized order along with\nan explicit reference clip ^sref(t). Listeners are asked to\nrate the closeness of each to the reference signal on a scale\nof 1-100 (100 implies the stimuli is indistinguishable from\nthe reference). We refer to this as perception test 1 (PT-1).\nFrom the results, ^sy12(t)is consistently rated least by all\nthe listeners. These audio clips are perceived to be ‘elec-\ntronic’, with temporal distortion clearly heard. Listeners\nhave rated ^sx12(t)at an average of 88.88% close to ex-\nplicit reference, while the hidden reference ^sref(t)is rated\nat an average of 96.5% closeness to explicit ^sref(t); this\nis because ^sx12(t)is confused with the hidden ^sref(t)in\n38.8% cases by the subjects. ^sy12(t)is rated at an average\n56.27% close to explicit reference. The bane of synthesiz-\ning melody with k\u0003\ny(t)sequence is easily perceivable by all\ntrained listeners. This validates our hypothesis that ^sx(t)\nis closer to ^sref(t)than^sy(t).176 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20173.4R¯agaPerception Experiment\nWith ^sx(t)being a close model of ^sref(t), we hypothesize\nmuch of gamaka structures in a r¯agarendition is retained;\nalso, micro-tonal dynamics of r¯agamight be better cap-\ntured withK= 22\u00033levels thanK= 12\u00033.\nNote name 22 Notes (Position) Pitch Ratio 12 Notes(Position)\nSa S(1) 1 S(1)\nRiR11(2) 256/243\nR12(3) 16/15 R1(2)\nR21(4) 10/9\nR22(5) 9/8 R2=G1(3)\nGaG11(6) 32/27\nG12(7) 6/5 R3=G2(4)\nG21(8) 5/4 G3(5)\nG22(9) 81/64\nMaM11(10) 4/3 M1(6)\nM12(11) 27/20\nM21(12) 45/32 M2(7)\nM22(13) 729/512\nPa P(14) 3/2 P(8)\nDaD11(15) 128/81\nD12(16) 8/5 D1(9)\nD21(17) 5/3\nD22(18) 27/16 D2=N1(10)\nNiN11(19) 16/9\nN12(20) 9/5 D3=N2(11)\nN21(21) 15/8 N3(12)\nN22(22) 243/128\nSa0S(next octave)(23) 2/1 S(upper octave)(13)\nTable 1 . Pitch ratios in an octave for 22-note system of\nIndian Art Music. The ratios used in 12-note system are in\nbold face.\n3.4.1 Experimental Setup\nWe choose some r¯agas (shown in Table 2) which are con-\nsidered by experts to be musically challenging to render\nas they contain lot of gamakas and micro-tonal structures.\nFor each listener, two different renditions (by different\nsingers) are presented for every r¯aga. The singer identity\nis masked as a result of time-invariant / ¯a/, the shaping ﬁlter\nfor the pitch contour; hence any bias factor due to singer in\nthe listening experiments is reduced.\nCarnatic music Hindustani music\n1. Begada Bhairav\n2. Bhairavi Darbari\n3. Saveri Marwa\n4. Sahana Puriya Dhanashree\n5. Sindhu Bhairavi Yaman\n6. Thodi\nTable 2 .R¯agas chosen for perception experiment.\nTo verify if ^sx(t)captures the r¯aganuances along with\nthegamakas in its entirety as represented in ^sref(t), in\neach experiment, we present a 1 min duration clip of^sref(t)and its corresponding (i) ^sx22(t)and (ii) ^sx12(t)\n(synthesized) audio clips4.\nWe ﬁrst present to music experts, ^sref(t)as the refer-\nence and ask them to rate on a scale of 1-10 for r¯aga char-\nacteristics present in ^sref(t). The same listener is now\npresented with ^sx22(t)and^sx12(t)(not necessarily in that\norder) and asked to rate closeness of each with respect to\nr¯aganuances of ^sref(t)on the scale of 1-10. Lower rating\nimplies r¯aga nuances are compromised due to quantiza-\ntion. Thus, each listener for Hindustani music form par-\nticipates in 10 (5 r¯agas with 2 different renditions) such\nexperiments; and, 12 experiments are presented for each\nCarnatic expert listener. This is perception test 2 (PT-2).\n3.4.2 PT-2 Results and Analysis\n5 performing Carnatic musicians were selected for the\nperception test in Carnatic music; similarly, 5 musicians\ntrained in Hindustani music were considered for the Hin-\ndustani music perception tests.\nThe average ratings of ^sx12(t)and^sx22(t)w.r.t. refer-\nence ^sy(t)for each r¯agaconsidered in Carnatic and Hin-\ndustani music is as shown in Figure 3 (a) and (b) respec-\ntively.\nBegada Bhairavi Sahana Saveri Sindhu Bhairavi Thodi012345678910Reference\nK=12\nK=22\nBhairav Darbari Marwa Puriya Dhanashree Yaman012345678910Reference\nK=12\nK=22\nFigure 3 . [Color Online] Perception rating for r¯agachar-\nacteristics for ^sy(t),^sx12(t)and^sx22(t)for (a) Carnatic\nmusic averaged over 5 listeners (b) Hindustani music aver-\naged over 5 listeners.\nThe^sref(t)ratings absorbs anomalies such as sudden\nbreaks and octave errors which commonly occur in pitch\n4The clip is a part of the starting portion of the original rendition but\nbetween the region 30 s to 90 s. While the r¯agacharacterizing phrases\nwill be brought about initially, we hypothesize that r¯aganuances must be\nshowcased at any chunk of time.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 177estimation algorithms, as well as errors committed by the\nartist in the rendition. In both Carnatic and Hindustani mu-\nsic clips, ^sx12(t)and^sx22(t)are reported to be quite close\nto^sref(t)and requires more careful inspection by repeated\ncontrasts against ^sref(t)as described ahead.\nIn order to pin-point distortions perceived, expert lis-\nteners had to intently re-listen the ^sref(t)multiple times to\nconﬁrm if perceived foreign/distorted notes are present in\n^sx12(t)or^sx22(t)clips and not in ^sref(t)clip itself. Some\nof the commonly observed distortions are: gamakas be-\ning ﬂattened, a foreign note perceived in ^sref(t)clip being\ncorrected and note shift at micro-tonal level. A Hindus-\ntani music performer after multiple listenings, could report\n\u001813perceivable distortions (in each ^sx12(t)and^sx22(t))\nwith respect to total 10 reference clips.\nIn the perception experiment in Hindustani music, at\nK= 12\u00033andK= 22\u00033quantization levels, given\n^sref(t), expert listeners have given equal rating to both\n^sx12(t)and^sx22(t)for42% of the test cases; and, for\n\u001832% of the cases, the listeners have rated ^sx22(t)to be\ncloser to reference than ^sx12(t). Also, the overall average\nrating for ^sx12(t)is very close to ^sx22(t)for most of the lis-\nteners. This is perhaps due to the inherent predisposition\nof performers of Hindustani music to elaborate individual\nnotes, thereby minimizing the transitions between the 22\nnote positions.\nSimilar analysis on perception of Carnatic music shows\nthat in 36% cases, ratings for ^sx12(t)and^sx22(t)were the\nsame. In\u001840% cases, ^sx22(t)was found to be closer to\nreference than ^sx12(t).\nIn order to obtain measure of overall inter-listener\nagreement, we ﬁrst categorize the ratings in each exper-\niment into 3 categories - (i) ^sx22(t)closer to ^sref(t)(ii)\n^sx12(t)closer to ^sref(t)(iii) equal ratings to both ^sx22(t)\nand^sx12(t). For anithexperiment, the agreement among\ntheLnumber of listeners can be deﬁned as [4]:\nPi=1\nL(L\u00001)3X\nj=1n2\nij\u0000L (6)\nwhere,nijis the number of raters who have assigned jth\ncategory inithexperiment.\nIn PT-2 perception test of Hindustani music, the average\ninter-listener agreement per experiment is found to be 0.37\nwhile for Carnatic music, the average is 0.34.\nFrom PT-1, we could infer that quantization at every\npitch sample results in perceivable loss of r¯agastructure.\nThe results of PT-2 shows that it is possible to quantize\nat critical points while retaining the r¯agastructure. There\ncould be a few note omissions and distortions at micro-\ntonal levels which are not perceivable in one listening, im-\nplying r¯agastructure is well retained. This also implies\nthatquantizing critical pitch values keeps much of the\ngamaka structure (which has been indeﬁnable till now)\nintact . Expert musicians show sensitivity to 22-note posi-\ntions; in some clips, musicians appreciate the approach to\na note as interpolated by ^sx22(t)more than ^sref(t)5. We\n5Sometimes, ^sx12(t)is also reported to interpolate transitions better\nthan^sref(t)infer that both K= 12\u00033andK= 22\u00033, depict close\nscores and retain r¯agastructure well.\n3.5 Relation to Waveform Quantizers\nThe model corresponding to Equation (3) is a waveform\nquantizer. While an uniform quantizer assumes yn(t)\nto have uniform distribution, the model corresponding to\nEquation (1) and (3) is a non-uniform, parameterized,\nstochastic waveform quantizer. The stochastic SC-GMM\nincorporates shape of the pdf through its parameters to\nderive rendition-speciﬁc and/or r¯aga-speciﬁc quantization\nthresholds. While a well-designed optimum waveform\nquantizer with sufﬁcient bit-depth can result in hi-ﬁdelity\naudio, we have shown, from results of perception experi-\nment PT-1, that non-uniform, parameterized pitch ‘wave-\nform’ quantization unsettles the r¯aga-bhava even within a\nsmall 7 s melodic phrase. Increasing bit-depth without cor-\nrelating to essential pitch-ratios (within an octave) will be\nof limited utility.\nFrom model deﬁned by Equation (2) and (4), we have\nseen from results of PT-2 that sub-sampling (at criti-\ncal points) and then using a non-uniform, parameterized,\nstochastic quantizer results in melodic contours which can\nreconstruct r¯aga-bhava with less distortions. Increasing\nbit-depth (from K= 12\u00033toK= 22\u00033) need not\nalways result in lesser ‘perceptual’ distortions in melody\nsignals which are inherently structured.\n4. APPLICATIONS OF QUANTIZED PITCH\nCONTOUR\n4.1 Note Transcription\nA direct application of discretizing melody contours is in\nnote transcription. While attempts have been made to cap-\nture regions corresponding to discrete notes, we now theo-\nrize that discrete notes can occur as points and/or regions in\nthe melodic-temporal domain; elongated notes result in re-\ngions, while other-wise they can be essentially considered\nas points.\n4.1.1 Experimental Setup\nWe have recorded a total of close to 50 phrases each\nin Hindustani and Carnatic music forms; the phrases are\nspread across 5 r¯agas (as listed in Table 3) and is a mod-\nest database to quantify accuracy of note transcription.\nThese phrases contain r¯aga speciﬁc gamakas such that\ntheir conventional transcription differs from their rendi-\ntions. The Hindustani database is rendered with S¯arangi\ninstrument, while the Carnatic database contains vocally\nrendered phrases. Each phrase is associated with 2 note\nsequences - (i) note sequences as transcribed by musicians\n(referred as TA transcription) (ii) note sequences as musi-\ncians render it with the associated gamakas , but now ex-\nplicitly notated (referred to as TB transcription). Figure 4\nis a sample depicting the differences between TA and TB.\nThe transcription notation used here consider only the note\nsequences and do not include duration information.178 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Pitch is extracted using Praat [2] every 8 ms. A SC-\nGMM model is built for each r¯aga by combining all\nphrases; note sequences are obtained as per Equation 4.\n4.1.2 Results and Analysis of transcribed sequences\nThe performance of automatic note transcription task is\nmeasured using TA and TB transcriptions as ground truths.\nThe automatically obtained note sequence is aligned with\nTA (or TB) using Needleman Wunsch global alignment al-\ngorithm [16] with gap penalty set to zero. Performance is\nreported in terms of recall accuracy and insertion rate. Ta-\nble 3 summarizes the performance of automatic transcrip-\ntion usingk\u0003\ny12andk\u0003\nx12note sequences in both Carnatic\nand Hindustani music.\nTranscription using k\u0003\ny12sequences always shows high\nrecall results (as expected) and also results in high inser-\ntion rate; as every pitch sample is quantized, there is less\nlikelihood of missing any note but more chances of false\nalarms.\nWith TA transcription as ground truth, recall rates using\nk\u0003\nx12sequences is comparable to that using k\u0003\ny12in Hin-\ndustani music; for Carnatic music, recall rate performance\nofk\u0003\nx12sequence is seen to have decreased. Due to dy-\nnamic nature of Carnatic music, some notes in TA are not\nrepresentative of the rendition. For example in r¯aga Thodi ,\nthough TA contains note Ga, it is rendered as Ma\u0000Ri(cf.\nFigure 1). Also, frequent notes or stable notes have domi-\nnant presence as Gaussian component (reﬂected as \u000b); any\ncritical pitch value in the vicinity of a dominant note can\nbe assigned a higher probability in-spite of its distance to\nanother adjacent but not-frequent note. This can cause in-\ncorrect pitch-to-note mapping.\nDrastic reduction in transcription insertion rate can be\nattributed to k\u0003\nx12sequences being estimated from sub-\nsampled version of yn(t). Thus, not all points are tran-\nscribed.\nWith TB transcription as ground truth, insertion rate\n0 0.5 1 1.5 2 2.5\ntime (s) !100110120130140150160Pitch (Hz) !\nPaDa1Ri1\nSa\nNi3\nFigure 4 . [Color Online] The blue contour corre-\nsponds to a phrase of r¯aga Keervani (Carnatic). Red\nlines indicate the pitch values of notes used in the\nr¯aga, while black lines denote pitch of notes that are\nnot used. This phrase is transcribed as ‘ SaNiPaDa ’\n(TA). Considering the gamakas involved, it is rendered as\n‘SaNiRiSaPaDaPaDaPaDaPaDa ’ (TB).is reduced for both k\u0003\ny12andk\u0003\nx12sequences. This is at-\ntributed to TB version of ground truth being a more elab-\norate explanation of a rendition. A sample depiction of\nthe same can be seen in Figure 5. The number of false\nnote assignment is reduced with transcription using k\u0003\nx12\nas againstk\u0003\ny12.\n------------SN-----RS----------PDPDPD----PD\n                 ||       ||              || || ||      ||\nSRSNSGRSNSRSRSMPDPDPDPDPDPMPDS\n---SNR---SPDPDPDPD--\n     |||      | || || |||  \nSNSNRPDPDPDPDPDMa)\nb)\nd)c)\nFigure 5 . Melodic notes for pitch contour of Figure 4,\ncorresponding to (a) Ground truth, TB (b) Transcription\nobtained using k\u0003\ny12(c) Ground truth, TB (d) Transcription\nobtained using k\u0003\nx12\n(a) Hindustani\nTA TB\nk\u0003\ny12(t)k\u0003\nx12(t)k\u0003\ny12(t)k\u0003\nx12(t)\nR¯aga Rec Ins Rec Ins Rec Ins Rec Ins\nBihag 1 3.65 1 1.55 0.93 1.39 0.88 0.39\nGoud Sarang 1 4 1 2 0.88 1.97 0.83 0.88\nKeervani 0.97 4.59 0.95 2.06 0.88 1.69 0.8 0.6\nMadhuvanti 1 3.93 0.96 1.77 0.98 1.97 0.96 0.67\nMarwa 1 7.18 0.97 3.36 0.96 2.4 0.88 0.90\n(b) Carnatic\nTA TB\nk\u0003\ny12(t)k\u0003\nx12(\u001c)k\u0003\ny12(t)k\u0003\nx12(\u001c)\nR¯aga Rec Ins Rec Ins Rec Ins Rec Ins\nBegada 1 8.58 0.85 1.91 0.98 2.93 0.80 0.32\nBhairavi 1 7.75 0.92 2.17 0.81 2.32 0.54 0.57\nHamsadvani 1 7.15 0.87 2 0.97 2.61 0.82 0.45\nHindola 1 10.8 1 3 0.9 3.03 0.86 0.46\nKeervani 0.98 7.27 0.90 2.45 0.81 2.25 0.70 0.54\nThodi 1 10 0.88 2.55 0.92 3.04 0.88 0.36\nTable 3 . Performance of k\u0003\ny12andk\u0003\nx12sequences for au-\ntomatic transcription in terms of average recall rate (Rec)\nand insertion rate (Ins) w.r.t. TA and TB ground truth tran-\nscription of phrases in (a) Hindustani (b) Carnatic music.\n5. CONCLUSIONS\nWe have explored two different quantization techniques us-\ning stochastic models for mapping continuous melody con-\ntours to discrete pitch values; perception experiments show\nthatr¯aga-bhava can be preserved by quantizing the pitch\ncontour at critical points instead a waveform-quantization\ntype of approach. The stochastic, parameterized SC-GMM\nassimilates information in pitch pdf to derive quantiza-\ntion thresholds. Applying results of perception experi-\nments to automatic transcription task results in a detailed\ndescription of a melodic piece; such a detailed transcrip-\ntion can inherently aid in mapping r¯aga dynamics and\ngamakas into musical notation.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 1796. REFERENCES\n[1] A. Bellur, V . Ishwar, and H. A Murthy. Motivic anal-\nysis and its relevance to raga identiﬁcation in carnatic\nmusic. In Proc. 2nd CompMusic Workshop , pages 153–\n157, Istanbul, Turkey, 2012.\n[2] P. Boersma and D. Weenink. Praat: doing phonetics by\ncomputer (version 6.0.14), 2016.\n[3] D. Bogdanov, N. Wack, E. G ´omez, S. Gulati, P. Her-\nrera, O. Mayor, G. Roma, J. Salamon, J. R. Zapata, and\nX. Serra. Essentia: An audio analysis library for music\ninformation retrieval. In Proc. Int. Soc. on Music Info.\nRetr. Conf (ISMIR) , pages 493–498, 2013.\n[4] J.L. Fleiss et al. Measuring nominal scale agree-\nment among many raters. Psychological Bulletin ,\n76(5):378–382, 1971.\n[5] K. K. Ganguli, S. Gulati, P. Rao, and X. Serra. Data-\ndriven Exploration of Melodic Structures in Hindustani\nMusic. Proc. Int. Soc. on Music Info. Retr. Conf (IS-\nMIR) , pages 605–611, 2016.\n[6] K. K. Ganguli and P. Rao. Perceptual Anchor or attrac-\ntor: How do Musicians perceive Raga Phrases. Proc.\nFrontiers in Research in Speech and Music (FRSM) ,\npages 174–178, 2016.\n[7] S. Gulati, A. Bellur, J. Salamon, H. G. Ranjani, V . Ish-\nwar, H. Murthy, and X. Serra. Automatic tonic identiﬁ-\ncation in Indian art music: approaches and evaluation.\nJournal of New Music Research , 43(1):53–71, 2014.\n[8] S. Gulati, J. Serr `a, K. K. Ganguli, S. S ¸ent ¨urk, and\nX. Serra. Time-Delayed Melody Surfaces for r ¯aga\nRecognition. In Proc. Int. Soc. on Music Info. Retr.\nConf (ISMIR) , pages 751–757, New York (USA), 2016.\n[9] S. Gulati, J. Serra, V . Ishwar, S. Sent ¨urk, and X. Serra.\nPhrase-based r ¯aga recognition using vector space mod-\neling. In Intl. Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 66–70. IEEE, 2016.\n[10] V . Ishwar, S. Dutta, A. Bellur, and H. A. Murthy. Mo-\ntif spotting in an alapana in carnatic music. In Proc.\nInt. Soc. on Music Info. Retr. Conf (ISMIR) , pages 499–\n504, 2013.\n[11] G. K. Koduri. Towards a multimodal knowledge base\nfor Indian art music: A case study with melodic intona-\ntion. PhD thesis, Universitat Pompeu Fabra, Barcelona,\n2016.\n[12] G. K. Koduri, V . Ishwar, J. Serr `a, and X. Serra. Intona-\ntion analysis of r ¯agas in carnatic music. Journal of New\nMusic Research , 43:72–93, 2014.\n[13] A. Krishnaswamy. Pitch measurements versus percep-\ntion of south indian classical music. In Proc. of the\nStockholm Music Acoustics Conference (SMAC) , pages\n627–630, 2003.[14] S. McAdams. Psychological constraints on form-\nbearing dimensions in music. Contemporary Music Re-\nview, 4(1):181–198, 1989.\n[15] Wim van der Meer and S. Rao. What you hear isn’t\nwhat you see: The representation and cognition of fast\nmovements in Hindustani music. In Frontiers in Re-\nsearch in Speech and Music (FRSM) , pages 1–8, Luc-\nknow, India, 2006.\n[16] S. B. Needleman and C. D Wunsch. A general method\napplicable to the search for similarities in the amino\nacid sequence of two proteins. Journal of molecular\nbiology , 48(3):443–453, 1970.\n[17] L. R. Rabiner and B. H. Juang. Fundamentals of\nSpeech Recognition . PTR Prentice Hall, 1993.\n[18] H. G. Ranjani, S. Arthi, and T. V . Sreenivas. Car-\nnatic music analysis: Shadja, Swara identiﬁcation and\nRaga veriﬁcation in Alapana using stochastic models.\nInProc. Workshop on Applicat. of Signal Process. to\nAudio and Acoust. , pages 29–32, Oct 2011.\n[19] G. Ruckert, A.A. Khan, and U.A. Khan. The Classical\nMusic of North India: The ﬁrst years study . Munshiram\nManoharlal Publishers, 1998.\n[20] P. Sambamoorthy. South Indian Music . Number v. 1-\n2 in South Indian Music. Indian Music Publishing\nHouse, 1963.\n[21] S. S ¸ent ¨urk, G. K. Koduri, and X. Serra. A Score-\nInformed Computational Description of Svaras Using\na Statistical Model. In Proc. Conf. Sound and Mu-\nsic Computing (SMC) , pages 427–433, Hamburg, Ger-\nmany, 2016.180 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "In Search of the Consensus Among Musical Pattern Discovery Algorithms.",
        "author": [
            "Iris Yuping Ren",
            "Hendrik Vincent Koops",
            "Anja Volk",
            "Wouter Swierstra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417105",
        "url": "https://doi.org/10.5281/zenodo.1417105",
        "ee": "https://zenodo.org/records/1417105/files/RenKVS17.pdf",
        "abstract": "Patterns are an essential part of music and there are many different algorithms that aim to discover them. Based on the improvements brought by using data fusion methods to find the consensus of algorithms on other MIR tasks, we hypothesize that fusing the output from musical pat- tern discovery algorithms will improve the pattern discov- ery results. In this paper, we explore two methods to com- bine the pattern output from ten state-of-the-art algorithms using two datasets. Both provide human-annotated pat- terns as ground truth. We show that finding the consen- sus among the output of different musical pattern discov- ery algorithms is challenging for two reasons: First, the number of patterns found by the algorithms exceeds pat- terns in human annotations by several orders of magnitude, with little agreement on what constitutes a pattern. Sec- ond, the algorithms perform inconsistently across different pieces. We show that algorithms lack a consensus with each other. Therefore, it is difficult to harness the collec- tive wisdom of the algorithms to find ground truth patterns. The main contribution of this paper is a meta-analysis of the (dis)similarities among pattern discovery algorithms’ output and using the output in two fusion methods. Fur- thermore, we discuss the implication of our results for the MIREX task.",
        "zenodo_id": 1417105,
        "dblp_key": "conf/ismir/RenKVS17",
        "keywords": [
            "algorithms",
            "music",
            "patterns",
            "data fusion",
            "MIR tasks",
            "consensus",
            "discovery",
            "fusion methods",
            "datasets",
            "ground truth"
        ],
        "content": "IN SEARCH OF THE CONSENSUS AMONG\nMUSICAL PATTERN DISCOVERY ALGORITHMS\nIris Yuping Ren\nUtrecht University\ny.ren@uu.nlHendrik Vincent Koops\nUtrecht University\nh.v.koops@uu.nlAnja Volk\nUtrecht University\na.volk@uu.nlWouter Swierstra\nUtrecht University\nw.s.swierstra@uu.nl\nABSTRACT\nPatterns are an essential part of music and there are many\ndifferent algorithms that aim to discover them. Based on\nthe improvements brought by using data fusion methods\nto ﬁnd the consensus of algorithms on other MIR tasks,\nwe hypothesize that fusing the output from musical pat-\ntern discovery algorithms will improve the pattern discov-\nery results. In this paper, we explore two methods to com-\nbine the pattern output from ten state-of-the-art algorithms\nusing two datasets. Both provide human-annotated pat-\nterns as ground truth. We show that ﬁnding the consen-\nsus among the output of different musical pattern discov-\nery algorithms is challenging for two reasons: First, the\nnumber of patterns found by the algorithms exceeds pat-\nterns in human annotations by several orders of magnitude,\nwith little agreement on what constitutes a pattern. Sec-\nond, the algorithms perform inconsistently across different\npieces. We show that algorithms lack a consensus with\neach other. Therefore, it is difﬁcult to harness the collec-\ntive wisdom of the algorithms to ﬁnd ground truth patterns.\nThe main contribution of this paper is a meta-analysis of\nthe (dis)similarities among pattern discovery algorithms’\noutput and using the output in two fusion methods. Fur-\nthermore, we discuss the implication of our results for the\nMIREX task.\n1. INTRODUCTION\nAn important property of music is its recurring structures\n[18]. Musically meaningful repetitions in the form of mu-\nsical patterns or musical motifs [29] provide one of the\nmost intensely researched aspects both for analyzing in-\ndividual musical pieces [24] and groups or collections of\nmusical pieces for identifying musical style based on mu-\nsical patterns [8,23,34]. Automatic pattern discovery is an\nactive research area in Music Information Retrieval ( MIR)\nthat aims to discover these patterns automatically. Differ-\nent pattern discovery methods have been introduced, such\nas string-based approaches [4, 7, 14, 16, 17, 25], geometric\napproaches [3,6,21,31], data mining approaches [28], and\nc\rIris Yuping Ren, Hendrik Vincent Koops, Anja V olk,\nWouter Swierstra. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Iris Yuping Ren, Hen-\ndrik Vincent Koops, Anja V olk, Wouter Swierstra. “Finding the consen-\nsus among musical pattern discovery algorithms”, 18th International So-\nciety for Music Information Retrieval Conference, Suzhou, China, 2017.machine learning approaches [26]. Musical pattern discov-\nery algorithms have been used for different applications:\nfor determining similarity between musical pieces [1], for\nautomatic compositions [11], and for describing musical\nstyle characteristics [8].\nAlthough many approaches have been developed over\nthe recent decades (for a detailed overview see [12]), mu-\nsical pattern discovery algorithms face a number of chal-\nlenges. Music is inherently ambiguous: musicologists of-\nten do not agree on what the important musical patterns are\nin a given piece [5]. This makes it difﬁcult to evaluate the\nquality of automatically extracted musical patterns. Fur-\nthermore, each algorithm is historically tested on unasso-\nciated datasets with disparate metrics [12]. One attempt to\nsystematically evaluate the algorithms is the MIREX Dis-\ncovery of Repeated Themes & Sections task initiated in\n2014. In the task, a pattern is deﬁned as a set of time-pitch\npairs that occurs at least twice in a piece of music [10].\nAlthough the state-of-the-art algorithms cannot reproduce\nthe human-annotated patterns yet, they perform acceptably\nwell according to the evaluation metrics in this task. How-\never, the algorithms perform inconsistently across different\npieces which makes it hard to determine whether there ex-\nists a single ‘best’ performing algorithm.\nAnother problem is that algorithms tend to ﬁnd far\nmore patterns than human annotators do [10]. Hence the\nchallenge is to ﬁnd which potential patterns are musically\nmeaningful. The poor performance of automatically ex-\ntracted patterns in the compression and classiﬁcation task\non the Dutch Song Database in [1] also shows that pattern\ndiscovery is far from being a solved problem in MIR and\nComputational Music Analysis.\nIntegrating different algorithms using data fusion has\nbeen shown to be a successful approach to improving over-\nall performance in other areas dealing with ambiguous mu-\nsical data, such as in Automatic Chord Estimation [15]. To\naddress the challenges in musical pattern discovery, we hy-\npothesize that integrating the output of state-of-the-art al-\ngorithms to ﬁnd a consensus among these algorithms will\nhelp us to achieve an overall better pattern discovery re-\nsult. To this end, we explore two fusion methods: a new\nalgorithm, the Pattern Polling Algorithm ( PPA), and the\nTime Indexed Novelty Algorithm ( TINA ), which is based\non commonly used time indexed novelty scores. Using\nthese two methods, we aim to integrate the patterns found\nby multiple pattern discovery algorithms to a consensus\nand therefore employ their collective wisdom.671Fusing the patterns produced by individual algorithms\nis challenging since there are different assumptions,\ndatasets and methods behind the development of each algo-\nrithm. By exploring PPAand TINA using the MIREX dataset\nand the Annotated Corpus from the Dutch Song Database\n[32], we identify two problems with using these fusion\nmethods. First, because the number of patterns taken as\ninput of the fusion process is several orders of magnitude\nlarger than the human-annotated ground truth patterns and\nthey are disparate in terms of the pattern location, the pat-\ntern length, pattern overlap, and pattern coverage of the\nmusic pieces, it makes it difﬁcult to ﬁnd agreements among\nthese patterns. The disagreement reﬂects the ambiguity of\nthe pattern discovery task and a need for better deﬁnitions\nof musical patterns. Second, the individual algorithms per-\nform inconsistently on different pieces of music. The lack\nof large musical pattern discovery data sets aggregates the\nissue of the inconsistency and prevents further improve-\nments on using machine learning algorithms.\nIn this paper, we make two main contributions : First,\nwe undertake a meta (dis)similarity comparison among the\noutput of musical pattern discovery algorithms using two\nfusion methods, TINA and PPA(Section 2 and Section 3).\nSecond, based on this research, we discuss issues of the\nMIREX Discovery of Repeated Themes & Sections task\nand suggest future directions for improving musical pat-\ntern discovery research (Section 4).\n2. METHODS\nIn this section, we introduce the two fusion methods of PPA\nand TINA along with our evaluation methods. We use the\nMIREX monophonic version of Chopin’s Mazurka Op. 24\nNo. 4 as an example to illustrate the algorithms. The code\nof the algorithms and supportive explanations can be found\nin https://github.com/irisyupingren/2017Pattern.\n2.1 Algorithms Overview\nThe two new methods we use to explore musical pattern\nfusion have different goals. PPAfocuses on using the gath-\nered information to extract local pattern features (pattern\nboundaries), while TINA focuses on globally integrating\nthe output patterns of individual algorithms to a probability\ndistribution (pattern distribution).\nWe devise PPAbased on the fact that all pattern discov-\nery algorithms aim at ﬁnding the salient parts in musical\ncompositions. We assume that each algorithm’s output can\nbe taken as a vote on whether or not a given time point par-\nticipates in a salient part of the composition, e.g. is part\nof a musical pattern. Moreover, we deﬁne a salience de-\ngree of a time point which corresponds to the number of\npatterns that the time point participates in. In essence, the\nPPAis a voting system in which each algorithm votes on\nthe salience degree of a time point based on the discovered\npatterns. The resulting polling curve is then taken as a base\nto detect pattern beginnings and endings.\nTINA is devised based on taking the polling curve and\nthe ground truth patterns and normalize them to a proba-\nFigure 1 . The pipeline of the fusion and evaluation. Same\ndatasets and evaluation methods are used to compare two\nfusion methods ( PPAand TINA ) with individual algorithms.\nbility distribution. Along with the polling curve, we use\nthe time indexed novelty score [9], which is produced by\ncorrelating a checkerboard kernel along the main diagonal\nof the similarity matrix of pattern votes. The time indexed\nnovelty scores are then taken as a base to compare with the\npattern distributions of individual algorithms, the polling\ncurve, and the human-annotated patterns.\nThe pipeline of the entire fusion and evaluation process\ncan be found in Figure 1. For a set of music data and mu-\nsical pattern discovery algorithms, we ﬁrst determine the\nmusical patterns discovered by each algorithm on each mu-\nsical piece. Then we use PPAto extract pattern boundaries\nand use TINA to calculate the pattern distributions. Finally,\nwe analyze the fusion results and the individual algorithms.\n2.2 Pattern Polling Algorithm ( PPA)\nPPAstarts with calculating a polling curve by taking into\naccount all musical patterns output of all algorithms. After\nsmoothing the polling curve, the algorithm takes the criti-\ncal points (i.e. where the derivatives equal to zero) of the\ncurve and the ﬁrst derivative as the boundaries of the pat-\nterns (the beginnings and endings of the patterns). This is\nbecause changes in salience values could potentially reveal\nstructural changes in music.\nPolling Curve. The polling curve ( PC) is created using\nthe output from all individual algorithms. We let each al-\ngorithm vote at a given time point to decide whether it is a\nsalient part of the music. To create the voting time points in\nthe music, we use the resolution of one quarter note length.\nThe time points where the algorithms vote are therefore in\nthe vectorT:= [0;1;:::;n ]with the unit of a quarter note.\nThe voting is realized by looking up discretized time\npoints in the occurrences of output patterns: if there is an\noccurrence interval which covers the time point, we count\nthat there is a valid vote. Finally, we add up the voting672 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170 100 200 300 400 500\nTime050100150200Salience / Pattern CountFigure 2 . The polling curve of Chopin’s Mazurka\nOp. 24 No. 4 using algorithms from the MIREX task (see\nSection 3). The horizontal bars show where the ground\ntruth patterns are present. The x-axis represents time in the\nunit of quarter note and the y-axis represents the salience\nvalue, which is the number of pattern counts if each vote\ncarries the same weight. We see promising correspon-\ndences between the polling curve and human annotations.\nfrom all the algorithms and produce the polling curve P(t),\nwhich is a time series consisting of the salience values at\ntime points in T.\nSince PPAuses a combination of algorithms, we should\nconsider which algorithms we want to include or exclude.\nPPA could be extended if we have extra information on\nwhich algorithms should be trusted more and make a port-\nfolio of algorithms as the input. The portfolio is essen-\ntially a way of assigning binary weights to the algorithms’\nvotes: when the algorithm is included, its patterns have\nweight one, and when not included, weight zero. We can\nalso generalize the weight to a continuous value.\nTo formalize the process of voting:\nP(t) =X\nAX\nPX\nOIA;P\nO(t) (1)\nwhereAstands for Algorithm, Pstands for Pattern, O\nstands for occurrence, and IA;P\nO(t)is the weighted indi-\ncator function of an occurrence in the pattern Pin the al-\ngorithmA:\nIA;P\nO(t) =(\n!At2O\u0012P\u0012A\n0t62O\u0012P\u0012A(2)\nwhere!Ais the weight assigned to algorithm A.\nAn exemplary polling curve of Chopin’s Mazurka\nOp. 24 No. 4 using several algorithms from the MIREX task\nis shown in Figure 2. The polling curve provides us with\na clue of where there is a salience change in the music.\nCritical values (i.e. prominent changes) in salience values\nwill be regarded as boundaries in the polling curve times\nseries. In the following subsection, we will explain how to\ndecide what are the prominent changes and how to reduce\nthe possibly irrelevant micro-changes in the polling curve\nand then ﬁnd the pattern boundaries.\nSmoothing. One common way to reduce the effects of\npossibly irrelevant micro-changes in time series is smooth-\ning. In our algorithm, we use the Savitzky-Golay ﬁlter\n[30], which is a linear least-square polynomial ﬁtting ﬁl-\nter. Each time we apply the smoothing, we reduce some\n0 100 200 300 400 500\nTimeSalience / Pattern Count / DerivativesFirst Derivatives\nSecond Derivatives\nGround TruthFigure 3 . Extracted pattern boundaries using PPA. The\ndashed vertical lines are the boundaries. Many dashed\nlines are aligned with the boundaries of human annota-\ntions. We also plotted the polling curve, the ground truth,\nﬁrst and second derivatives for reference.\neffects of micro-changes, but at the same time, we might\nalso lose potentially valuable details. With different de-\ngrees of smoothing, we capture different levels of details\nin salience’s changes. Therefore, we make the degree of\nsmoothness, s, to be one of the two parameters in PPA.\nDerivative. After smoothing, to ﬁnd the prominent\nchanges of the salience in music, we calculate the ﬁrst\nand second discrete derivatives of the polling curve and\ntake their critical zero-crossing points as the pattern bound-\naries. More formally: let P0(t) =P(t+ 1)\u0000P(t)and let\nP00(t) =P0(t+ 1)\u0000P0(t),t >0;t2T. We are in-\nterested in the zero crossing \u0016tofP0(t)andP00(t)because\nthe zero crossing points \u0016trepresent a change of direction\nin the polling curve. For example, when P0(t)<0and\nP0(t+ 1)>0, we have a dipping point P0(\u0016t) = 0 in the\ncurve. There are more patterns discovered starting from\nthis point: it is likely to be a beginning of a pattern.\nOne question remains as for how strong the dipping,\ntipping, concave and convex in the curve should be so that\nwe pick it as a boundary. Here we introduce the second\nparameter: a threshold on the steepness of the zero cross-\ning points\u0015. With different values of \u0015, we create a set\nof boundary sets which consist of the time at which zero\ncrossing happens. In Figure 3, an example of the extracted\nboundaries can be found. We notice that some boundaries\nline up well with ground truth boundaries. We will evalu-\nate the extracted pattern boundaries in Section 2.4.\n2.3 Time Indexed Novelty Algorithm ( TINA )\nSince PPA extracts local boundaries, we use TINA to as-\nsess globally how the extracted patterns are similar to\nhuman-annotated patterns. Using the notions provided\nin Section 2.2, TINA can be described concisely as fol-\nlows: We use the pattern vote representation in Equa-\ntion (2) as the input. Formally, the input matrix is M=\n(IA;P 1\nO(t);IA;P 2\nO(t);:::;IA;Pn\nO(t)), wherenis the count of\noutput patterns we would like to combine. The main com-\nponent of TINA is the calculation of the time indexed nov-\nelty scores described in [9]. This includes calculating the\nsimilarity matrix SofMusing the Euclidean distance and\nthen multiplying the diagonal with a checkerboard kernel\nK= (1;\u00001;\u00001;1), which gives us the novelty curveProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 6730 100 200 300 400 500\nTimeProbabilityN - TINA output\nGT - Human annotation\nPC - TINA outputFigure 4 . The TINA output novelty curve (N) distri-\nbution calculated using patterns from all algorithms, the\nTINA output polling curve ( PC) distribution calculated us-\ning patterns from MIREX algorithms (see Section 3) and\nthe ground truth (GT) pattern distribution. The x-axis rep-\nresents time in the unit of quarter note. Correspondences\nof the time series can be seen from the three curves.\nN(t). The novelty curve represents the changing rate of\nthe pattern vote time series IA;Pi\nO(t), serving the same role\nas the derivatives in PPA. In the end, we obtain a novelty\ncurve for each algorithm and the ensemble of algorithms,\ndepending on which patterns are included in M.\nNext, the comparison in TINA requires the input from\nthe human-annotated ground truth patterns and the polling\ncurve. To convert the ground truth into the same time series\nformat as the novelty curve and the polling curve, we con-\nstruct the polling curve from the ground truth patterns as\nGT(t). Furthermore, taking the frequentists point of view,\nwe normalize the time series by the sum of the entire time\nseries so that we get the distributions of the novelty curve\nN(t), the polling curve P(t)and the ground truth patterns\nGT(t). Similarly, we can also construct the pattern distri-\nbutions of individual algorithms PA(t).\nIn Figure 4, we give an example of the novelty curve\ndistribution, the polling curve distribution and the ground\ntruth pattern distribution. In an initial visual inspect, we\nsee some correspondences among the three curves: some\nﬂuctuations and the tipping/dipping points of the curves\ntend to coincide. We will evaluate the distribution similar-\nities globally in the next subsection.\n2.4 Evaluation\nWe use two evaluation methods to assess how similar the\nhuman-annotated patterns are to the output boundaries of\nPPAand the output distributions of TINA .\nPattern Boundaries. To evaluate the extracted pat-\ntern boundaries, we use the boundaries of the ground truth\npatterns. Following the standard MIREX evaluation met-\nrics, we calculate the precision, recall and F1 score of the\nboundaries with a degree of fuzziness: we look for a match\nof boundaries with a tolerance of one quarter note length\nbecause of the one-quarter-length discretization we used\nfor creating the polling curve.\nPattern Distribution. To evaluate globally how similar\nthe normalized novelty curve and the polling curve are to\nthe ground truth pattern distribution, we calculate the Bhat-\ntacharyya coefﬁcients [13] and the Pearson correlation co-efﬁcients. Bhattacharyya coefﬁcients measure the amount\nof overlaps between two distributions and the Pearson cor-\nrelation coefﬁcients measure the linear correlation of dis-\ntributions. For the extracted patterns to be similar with the\nground truth patterns, we expect high correlation values\nand high overlap values.\n3. RESULTS\nIn this section, we ﬁrst introduce the input we use for PPA\nand TINA and provide a meta-analysis on the individual\nalgorithms. Then we explore the effects of the two param-\neterssand\u0015inPPAand the necessity of cross-validation.\nUsing our evaluation metrics, we show the performance\nof the two fusion algorithms is on average similar to indi-\nvidual algorithms, and we provide analysis as to why the\nfusion methods do not excel.\n3.1 Input: Algorithms and Music Data\nWe use two sets of algorithms and music data. The\nﬁrst set is from the Annotated Corpus of the Dutch Song\nDatabase ( MTC -ANN) and the algorithms used in [1],\nnamely PatMinr [17], Motives Extractor ( ME) [25], SIATEC\n[22], COSIATEC [19], and MGDP [7]. MTC -ANN [32] con-\nsists of 360 dutch folk songs in 26 tune families. Because\nwe are interested in ﬁnding shared patterns between songs\nin the same tune family, the pattern discovery algorithms\nare computed on the concatenation of the songs in the same\ntune family, and then the patterns discovered on the bound-\naries of concatenation are ﬁltered out (same as the intra-\nopus task described in [1]). The 360 individual songs are\ntaken as the input of PPAand TINA .\nThe second set is from the MIREX Discovery of Re-\npeated Themes & Sections task. For music data, we\nuse a subset of the task’s training dataset. The original\ndataset contains ﬁve pieces in both polyphonic and mono-\nphonic format. We take three pieces in the monophonic\nformat: Chopin’s Mazurka Op. 24 No. 4, Mozart’s Pi-\nano Sonata K. 282, 2nd movement, and Beethoven Piano\nSonata Op. 2 No. 1, 3rd movement. For the sake of the\nconsistency of the task and the compatibility with MTC -\nANN, we leave out the two music pieces which are con-\nstructed by a concatenation of voices in the piece. The\nalgorithm input consists of all algorithms submitted to\nthe MIREX task during 2014-2016: Motives Extractor ( ME)\n[25], SIATECC ompress- TLP (SIAP ),SIATECC ompress-\nTLF1(SIAF1),SIATECC ompress- TLR (SIAR ) [20], OL1&\nOL2[17], VM1&VM2[33], SYMCHM (SC) [27], along\nwith SIARCT -CFP (SIACFP ) [6], the algorithm developed\nby the task captain. The output patterns of these state-\nof-the-art algorithms for our example piece are shown in\nFigure 5. We make several observations:\n1. Different algorithms ﬁnd very different patterns: some\ntend to ﬁnd shorter patterns, some longer; some ﬁnd\nmany patterns while others are more “picky”.\n2. We have three algorithm families ( SIA,VM, and OL)\nwhich consist of more than one algorithm. The algo-\nrithms from the same algorithm family tend to ﬁnd sim-674 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170 100 200 300 400 500 600\nTimePatterns and their occurrences\nSIAF1\nSIAP\nSIAR\nVM1\nVM2\nOL1\nOL2\nSC\nME\nSIACFP\nGround TruthFigure 5 . Patterns extracted by all algorithms submitted to\ntheMIREX task 2014-2016 plus SIARCT -CFPon the mono-\nphonic Chopin’s Mazurka Op. 24 No. 4. The horizontal\nbars show where the patterns are present. The x-axis rep-\nresents time in the unit of quarter note. We can see the\nalgorithms ﬁnd different amount of patterns and patterns\nof different length, etc.\nilar patterns. Similarities here include the number of\npatterns discovered, the coverage of the song and the\noverlaps of the occurrences.\n3. The ground truth is sparse in comparison to the patterns\ndiscovered by the algorithms.\n4. From eyeballing the entire visualization, we see some\ncorrespondence and similarities between the algo-\nrithms and the ground truth patterns.\n3.2 PPA: Parameter Space and Cross Validation\nPPAextracts the local boundaries using the output patterns\nfrom individual algorithms. We start investigating the ef-\nfectssand\u0015inPPAusing the MIREX set as input, because\na small number of music pieces gives us a clear idea of\nthe relation between the parameters and the performance\nofPPA. Ideally, if there is a consistent best-performing s\nand\u0015across the three pieces for precision, recall and F1\nscore, it would be possible that the parameters can be gen-\neralized. However, we ﬁnd that no single choice of sand\n\u0015performs well across all pieces. Nevertheless, to avoid\nover-ﬁtting using the ground truth patterns, we perform aAlgorithm Precision Recall F1\nME (0.125, 0.086) (0.184, 0.077) (0.149, 0.083)\nSC (0.396, 0.022) (0.419, 0.068) (0.402, 0.046)\nOL1 (0.420, 0.038) (0.565, 0.044) (0.462, 0.023)\nOL2 (0.422, 0.061) (0.565, 0.044) (0.483, 0.054)\nSIAF1 (0.139, 0.049) (0.670, 0.005) (0.228, 0.041)\nSIAR (0.213, 0.039) (0.427, 0.000) (0.279, 0.021)\nSIAP (0.117, 0.043) (0.596, 0.008) (0.195, 0.037)\nVM1 (0.137, 0.035) (1.0,0.0) (0.240, 0.029)\nVM2 (0.206, 0.073) (0.543, 0.024) (0.296, 0.060)\nSIACFP (0.819 , 0.030) (0.82, 0.064) (0.815 , 0.046)\nPPA-P 0.478 0.206 0.249\nPPA-R 0.228 0.867 0.35\nPPA-F1 0.248 0.738 0.360\nTable 1 .MIREX : (Mean, Variance) of the precision, recall\nand F1 score of different algorithms at the pattern bound-\nary extraction task. The PPA-P,PPA-Rand PPA-F1are ob-\ntained using a 3-fold cross-validation training process opti-\nmizing the precision, the recall and the F1 scores. Because\nwe only have one piece in the test set, there is no variance\nvalue. Bold numbers are the best results from individual\nalgorithms and PPA.\nAlgorithm Precision Recall F1\nPatMinr (0.465, 0.054) (0.957, 0.020) (0.598, 0.050)\nME (0.366, 0.103) (0.353, 0.098) (0.314, .0879)\nCOSIATEC (0.482, 0.049) (0.774, 0.042) (0.569, 0.040)\nSIATEC (0.468, 0.046) (0.975 , 0.017) (0.610 , 0.041)\nMGDP (0.515 , 0.072) (0.754, 0.093) (0.557, 0.065)\nPPA-P (0.489 , 0.135) (0.201, 0.023) (0.264, 0.035)\nPPA-R (0.486, 0.057) (0.657 , 0.046) (0.534 , 0.044)\nPPA-F1 (0.477, 0.054) (0.652, 0.047) (0.526, 0.042)\nTable 2 .MTC -ANN results in the format of Table 1,\nthe only difference being that we use a 10-fold cross-\nvalidation. Best results are bold.\nthree-fold cross-validation using a split of two-pieces train-\ning and one piece testing in the MIREX dataset. The results\nof the MIREX set are shown in Table 1 and the results of\nMTC -ANN are shown in Table 2.\nIn the MIREX set, the best F1 score of PPA ranks the\nﬁfth out of ten when using the optimal parameters found\nby cross-validation. The best F1 score of PPA0:360is bet-\nter than the average of the F1 scores of individual algo-\nrithms 0:3549 . The SIACFP algorithm performs overall the\nbest on the MIREX set. With small differences, PPAranks\nthe fourth out of ﬁve algorithms in MTC -ANN. However,\nthe best F1 score 0:534ofPPAis better than the average\nF1 score of four individual algorithms 0:510. Although\nPatMinr has the best F1 score in this set of music data and\nalgorithms, other algorithms follow very closely and there-\nfore it is hard to determine whether there is a best algorithm\nin this set of data and algorithms. On both datasets, we ob-\nserve that PPAperforms slightly better than the average of\nthe individual algorithms.\n3.3 TINA : Pattern Distributions\nFrom a global point of view, to measure the similarities of\nnovelty distributions, we calculate the polling curve distri-\nbutions and the pattern distributions of ground truth and\nindividual algorithms using TINA . To evaluate how similar\nthe distributions are, we calculate the Bhattacharyya coef-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 675GT\nNovelty\nPatMinr\nME\nCOSIATEC\nMGDP\nSIATEC\nPCGT\nNovelty\nPatMinr\nME\nCOSIATEC\nMGDP\nSIATEC\nPC\nGT\nNovelty\nPatMinr\nME\nCOSIATEC\nMGDP\nSIATEC\nPC\n0.00.20.40.60.81.0Figure 6 . Left: Pairwise Pearson correlation coefﬁcients\nof the ground truth distribution, individual algorithm dis-\ntributions, the novelty curve distribution (Novelty) and the\npolling curve ( PC) distribution using the 360 songs in MTC -\nANN. All p-values\u001c0:05. Right: Pairwise Bhattacharyya\ncoefﬁcients of the same distributions.\nﬁcients and the Pearson coefﬁcients. The pairwise values\nof the two measurements of the MTC -ANN set of music\ndata and algorithms are shown in Figure 6.\nAn obvious observation in both ﬁgures is the large dis-\ntance and small correlation between the ground truth pat-\ntern distribution and the output of algorithms. Using the\nBhattacharyya coefﬁcients, we see that, in comparison to\nthe distribution overlap differences between the ground\ntruth and the individual algorithms, the differences among\nthe individual algorithms and the fusion algorithms are\nsmaller. Using the Pearson correlation coefﬁcients, we see\nless linear correlation between the polling curve distribu-\ntion and the ground truth distribution. Other algorithms\nhave similar correlation values except SIATEC and the nov-\nelty curve, which have specially high correlation. This\nmeans the novelty curve is largely based on the SIATEC\nalgorithm’s output, and this is caused by the large num-\nber of output patterns generated by the algorithm. Look-\ning at both ﬁgures, from a global point of view, output\nof the algorithms have similarities among themselves, but\nthey show less correlation and similarity compared to the\nground truth patterns. Similar observations are made in the\nMIREX dataset and hence the matrices are not shown here.\n3.4 Analysis on the Results\nCombining all evaluation results, we identify why the fu-\nsion methods do not excel over individual algorithms as\nthe fusion approach applied in [15]. First, the available\ndatasets are small and the ground truth patterns are sparse,\nwhich is problematic for training the parameters and eval-\nuating a stable performance. Second, the algorithms dis-\nagree with each other on pattern length and pattern overlap\netc., which reﬂects the inherent ambiguity of music and a\nlack of uniﬁed goal/application of the musical pattern dis-\ncovery task. Third, because there are well-performing al-\ngorithms and relatively less well-performing algorithms in\nthe fusing portfolio, fusion results are understandably of\naverage quality since it combines results from all these dif-\nferent sources. In the end, although we observed promis-\ning correspondence and consensus among algorithms in\nFigure 4 and Figure 5, a systematic evaluation reveals thatthe degree of consensus is not yet enough for helping to\nﬁnd patterns that agree with the annotated patterns.\n4. DISCUSSION AND CONCLUSION\nIn this paper, we attempt to combine the output of musi-\ncal pattern discovery algorithms to improve musical pat-\nterns discovery. We devise a new algorithm, PPA, and ap-\nply an established method from the audio music similar-\nity ﬁeld, TINA , to musical pattern discovery. We test the\nfusion algorithms on pieces in the MIREX and MTC -ANN\ndatasets. The results show that PPA and TINA on aver-\nage do not improve the performance signiﬁcantly. More\nspeciﬁcally, the results from PPAshow that we can extract\nlocal boundaries using a combination of musical pattern\ndiscovery algorithms, but we need to select the parame-\nters properly. The results from TINA show that the ground\ntruth probability distributions of musical patterns are dif-\nferent from the ones produced by algorithms. The results\nof using two datasets show that algorithms perform dif-\nferently given different pieces and it is sometimes hard to\nselect a single ‘winner’. The reason of the dissatisfying\nperformance of the fusion algorithms lies in a large num-\nber of disagreeing patterns and the sparsity of the human-\nannotated patterns: the salient parts of music identiﬁed by\nthe extracted musical patterns do not align with the human\nannotations. To break the current limitations of applying\ndata fusion in this domain, our work implies a need for an\nimproved dataset and musical pattern discovery task for-\nmulation. It is also possible to improve the fusion methods\nby incorporating and learning more parameters from the\ndata source.\nMIREX From using the MIREX dataset in the fusion\ntask, we identify three potential improvements for the task.\nFirst, the ground truth data from the MIREX dataset is\nsparse and consists of only a few pieces. It would be de-\nsirable to obtain more annotations from experts. In addi-\ntion, the current ground truth consists of annotations from\ndifferent sources, which could be improved by adopting a\ncollaborative ground truth creation process [2]. Second, an\nopen question is whether the patterns of algorithms should\nbe compared to humanly annotated patterns as a way of\nevaluation, given that musicologists often disagree on the\npatterns: more aspects of subjectivity should be taken into\naccount. In addition, since we see that pattern discovery\nalgorithms produce very different patterns, one might ask\nwhether different algorithms’ output might be useful for\ndifferent application scenarios. In the future of the MIREX\ntask, instead of measuring the agreement with annotated\npatterns only, the testing of pattern quality by providing\na range of subtasks which employ extracted patterns into\nvarious applications, constitutes a promising direction for\nimproving the evaluation of pattern discovery algorithms.\nAcknowledgements. We thank all authors of the algo-\nrithms for providing their algorithms and output, and the\nanonymous reviewers for valuable suggestions.676 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20175. REFERENCES\n[1] Peter Boot, Anja V olk, and W. Bas de Haas. Evaluat-\ning the role of repeated patterns in folk song classiﬁca-\ntion and compression. Journal of New Music Research ,\n45(3):223–238, 2016.\n[2] John Ashley Burgoyne, Jonathan Wild, and Ichiro Fuji-\nnaga. An expert ground truth set for audio chord recog-\nnition and music analysis. In Proceedings of the 12th\nInternational Society for Music Information Retrieval\nConference , pages 633–638, 2011.\n[3] Chantal Buteau and Guerino Mazzola. Motivic anal-\nysis according to Rudolph Reti: Formalization by a\ntopological model. Journal of Mathematics and Music ,\n2(3):117–134, 2008.\n[4] Emilios Cambouropoulos. Musical parallelism and\nmelodic segmentation. Music Perception , 23(3):249–\n268, 2006.\n[5] Tom Collins. Improved methods for pattern discovery\nin music, with applications in automated stylistic com-\nposition. PhD thesis. Milton Keynes, UK: Faculty of\nMathematics, Computing and Technology, The Open\nUniversity , 2011.\n[6] Tom Collins, Andreas Arzt, Sebastian Flossmann, and\nGerhard Widmer. SIARCT-CFP: Improving precision\nand the discovery of inexact musical patterns in point-\nset representations. In Proceedings of the 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 549–554, 2013.\n[7] Darrell Conklin. Discovery of distinctive patterns in\nmusic. Intelligent Data Analysis , 14(5):547–554, 2010.\n[8] Darrell Conklin and Christina Anagnostopoulou. Com-\nparative pattern analysis of cretan folk songs. Journal\nof New Music Research , 40(2):119–125, 2011.\n[9] Matthew Cooper and Jonathan Foote. Summarizing\npopular music via structural similarity analysis. In Pro-\nceedings of the 7th IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , pages 127–\n130. IEEE, 2003.\n[10] Music Information Retrieval Evaluation eXchange\n(MIREX) 2013. Discovery of Repeated Themes &\nSections. http://www.musicir.org/mirex/\nwiki/2013 . Accessed: 2017-05-04.\n[11] Dorien Herremans and Elaine Chew. Morpheus: Au-\ntomatic music generation with recurrent pattern con-\nstraints and tension proﬁles. Technical Report, Queen\nMary University of London (2016) .\n[12] Berit Janssen, W. Bas de Haas, Anja V olk, and Peter\nvan Kranenburg. Finding repeated patterns in music:\nState of knowledge, challenges, perspectives. In Pro-\nceedings of the 10th International Symposium on Com-\nputer Music Modeling and Retrieval , pages 277–297.\nSpringer, 2013.[13] Thomas Kailath. The divergence and Bhattacharyya\ndistance measures in signal selection. IEEE Trans-\nactions on Communication Technology , 15(1):52–60,\n1967.\n[14] Ian Knopke and Frauke J ¨urgensen. A system for identi-\nfying common melodic phrases in the masses of Palest-\nrina. Journal of New Music Research , 38(2):171–181,\n2009.\n[15] Hendrik Vincent Koops, W. Bas de Haas, Dimitrios\nBountouridis, and Anja V olk. Integration and quality\nassessment of heterogeneous chord sequences using\ndata fusion. In Proceedings of the 17th International\nSociety for Music Information Retrieval Conference ,\npages 178–184, 2016.\n[16] Olivier Lartillot. Multi-dimensional motivic pattern\nextraction founded on adaptive redundancy ﬁltering.\nJournal of New Music Research , 34(4):375–393, 2005.\n[17] Olivier Lartillot. PatMinr: In-depth motivic analysis of\nsymbolic monophonic sequences. Music Information\nRetrieval Evaluation eXchange (MIREX 2014) , 2014.\n[18] Elizabeth Hellmuth Margulis. On repeat: How music\nplays the mind . Oxford University Press, 2014.\n[19] David Meredith. COSIATEC and SIATECCompress:\nPattern discovery by geometric compression. In Music\nInformation Retrieval Evaluation Exchange (MIREX\n2013) , 2013.\n[20] David Meredith. Using SIATECCompress to discover\nrepeated themes and sections in polyphonic music.\nInMusic Information Retrieval Evaluation Exchange\n(MIREX 2016) , 2016.\n[21] David Meredith, Kjell Lemstr ¨om, and Geraint A. Wig-\ngins. Algorithms for discovering repeated patterns in\nmultidimensional representations of polyphonic music.\nJournal of New Music Research , 31(4):321–345, 2002.\n[22] David Meredith, Geraint A. Wiggins, and Kjell Lem-\nstr¨om. Pattern induction and matching in polyphonic\nmusic and other multidimensional datasets. In Pro-\nceedings of the 5th World Multiconference on Sys-\ntemics, Cybernetics and Informatics , pages 22–25,\n2001.\n[23] Leonard B. Meyer. Style and music: Theory, History,\nand Ideology . Chicago: University of Chicago Press,\n1989.\n[24] Jean-Jacques Nattiez and Jonathan M. Dunsby. Fonde-\nments d’une s ´emiologie de la musique. Perspectives of\nNew Music , 15(2):226–233, 1977.\n[25] Oriol Nieto and Morwaread M. Farbood. Identifying\npolyphonic patterns from audio recordings using music\nsegmentation techniques. In Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference , pages 411–416, 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 677[26] Matevz Pesek, Ales Leonardis, and Matija Marolt. A\ncompositional hierarchical model for music informa-\ntion retrieval. In Proceedings of the 15th International\nSociety for Music Information Retrieval Conference ,\npages 131–136, 2014.\n[27] Matevz Pesek, Ur ˇsa Medve ˇsek, Ale ˇs Leonardis, and\nMatija Marolt. SymCHM: a compositional hierarchi-\ncal model for pattern discovery in symbolic music rep-\nresentations. Music Information Retrieval Evaluation\nExchange (MIREX 2015) , 2015.\n[28] Iris Yuping Ren. Closed patterns in folk music and\nother genres. In Proceedings of the 6th International\nWorkshop on Folk Music Analysis , 2016.\n[29] Rudolph Reti. The Thematic Process in Music . New\nYork: Macmillan, 1951.\n[30] Ronald W. Schafer. What is a Savitzky-Golay ﬁl-\nter? IEEE Signal Processing Magazine , 28(4):111–\n117, 2011.\n[31] Wai Man Szeto and Man Hon Wong. A graph-\ntheoretical approach for pattern matching in post-\ntonal music analysis. Journal of New Music Research ,\n35(4):307–321, 2006.\n[32] Peter van Kranenburg, Berit Janssen, and Anja V olk.\nThe Meertens Tune Collections: The Annotated Cor-\npus (MTC-ANN) versions 1.1 and 2.0.1. Meertens On-\nline Reports , 2016(1), 2016.\n[33] Gissel Velarde and David Meredith. A wavelet-based\napproach to the discovery of themes and sections in\nmonophonic melodies. Music Information Retrieval\nEvaluation Exchange (MIREX 2014) , 2014.\n[34] Anja V olk, W. Bas de Haas, and Peter van Kranenburg.\nTowards modelling variation in music as foundation\nfor similarity. In Proceedings of the 12th International\nConference on Music Perception and Cognition , pages\n1085–1094, 2012.678 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "A Collection of Music Scores for Corpus Based Jingju Singing Research.",
        "author": [
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416346",
        "url": "https://doi.org/10.5281/zenodo.1416346",
        "ee": "https://zenodo.org/records/1416346/files/RepettoS17.pdf",
        "abstract": "The MIR research on jingju (also known as Beijing or Peking opera) music has taken audio as the main source of information. Music scores are an important resource for the musicological research of this tradition, but no machine readable ones have been available for computational analy- sis. In order to explore the potential of symbolic score data for jingju music research, we have expanded the Comp- Music Jingju Music Corpus, which contains mostly audio, with a collection of 92 machine readable scores, for a to- tal of 897 melodic lines. Since our purpose is the study of jingju singing in terms of its musical system elements, we have selected the arias used as examples in reference jingju music textbooks. The collection is accompanied by scores metadata, curated annotations per score and melodic line, and a set of software tools for extracting statistical infor- mation from it. All the gathered data and developed soft- ware are available for research purposes. In this paper we first discuss the culture specific concepts that are needed for understanding the contents of the collection, followed by a detailed description of it. We then present a series of computational analyses performed on the scores and dis- cuss some musicological findings.",
        "zenodo_id": 1416346,
        "dblp_key": "conf/ismir/RepettoS17",
        "keywords": [
            "MIR",
            "jingju music",
            "audio",
            "machine readable scores",
            "Comp-Music Jingju Music Corpus",
            "melodic lines",
            "jingju singing",
            "musical system elements",
            "arias",
            "scores metadata"
        ],
        "content": "A COLLECTION OF MUSIC SCORES FOR CORPUS BASED JINGJU\nSINGING RESEARCH\nRafael Caro Repetto, Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona\nrafael.caro@upf.edu ,xavier.serra@upf.edu\nABSTRACT\nThe MIR research on jingju (also known as Beijing or\nPeking opera) music has taken audio as the main source\nof information. Music scores are an important resource for\nthe musicological research of this tradition, but no machine\nreadable ones have been available for computational analy-\nsis. In order to explore the potential of symbolic score data\nfor jingju music research, we have expanded the Comp-\nMusic Jingju Music Corpus, which contains mostly audio,\nwith a collection of 92 machine readable scores, for a to-\ntal of 897 melodic lines. Since our purpose is the study of\njingju singing in terms of its musical system elements, we\nhave selected the arias used as examples in reference jingju\nmusic textbooks. The collection is accompanied by scores\nmetadata, curated annotations per score and melodic line,\nand a set of software tools for extracting statistical infor-\nmation from it. All the gathered data and developed soft-\nware are available for research purposes. In this paper we\nﬁrst discuss the culture speciﬁc concepts that are needed\nfor understanding the contents of the collection, followed\nby a detailed description of it. We then present a series of\ncomputational analyses performed on the scores and dis-\ncuss some musicological ﬁndings.\n1. INTRODUCTION\nIn recent years, jingju (also known as Beijing or Peking\nopera) music received an increasing attention from MIR re-\nsearchers for tasks such as mood recognition [7], pitch con-\ntour analysis [8, 12, 13, 21–23], lyrics to audio alignment\n[10], timbre analysis [11, 17], percussion analysis [16, 19],\nand structural segmentation [18], all of them taking audio\nas the main source of information. In order to carry out\nthese tasks, several corpora for jingju music research were\ngathered. The Jingju Music Corpus gathered in the Comp-\nMusic project [14] contains a collection of commercial au-\ndio recordings and their metadata, as well as some datasets\nfor speciﬁc tasks.1The corpus built in [18] also consists of\n1http://compmusic.upf.edu/corpora\nc\rRafael Caro Repetto, Xavier Serra. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Rafael Caro Repetto, Xavier Serra. “A collection of music\nscores for corpus based jingju singing research”, 18th International Soci-\nety for Music Information Retrieval Conference, Suzhou, China, 2017.commercial recordings, annotated for structural segmenta-\ntion analysis. Only the corpus created in [7]2contains a\ncappella recordings made by its authors, as well as annota-\ntions for the task of mood recognition.\nThe core component of jingju music is singing. In order\nto study the musical system developed in this tradition (see\nSection 1.1) it is therefore essential to analyse the sung\nmelodic line. However, extracting its pitch contour from\ncommercial recordings did not produce fully satisfactory\nresults [13]. On the other hand, the a cappella recordings\ncontained in the corpus gathered in [7] do not cover the\nwhole jingju musical system and are not annotated for this\ngoal, since it was created for a different purpose. Music\nscores are a meaningful alternative to audio recordings.\nThe lack of MIR research based on jingju scores could\nbe possibly motivated for two reasons: the secondary role\nthat music scores play in this tradition (see Sections 1.1\nand 1.2), and the lack of music scores in machine readable\nformat available for computational analysis.\nThe goal of the Jingju Music Scores Collection (JMSC)\npresented in this paper is to offer a comprehensive and\ncomplete resource for the study of jingju singing in terms\nof its musical system. That is to say, jingju singing, as op-\nposed to instrumental accompaniment, is taken as our re-\nsearch object with the aim of understanding not its acoustic\nor intonation characteristics, but the elements that build the\njingju musical system.\n1.1 Jingju Musical System\nWhen talking about music in jingju, there are two im-\nportant general considerations to take into account. First,\njingju is a theatrical genre, in which the main structural\nelement is the dramatic plot. Music, as the rest of the dis-\nciplines that integrate this comprehensive art form, is at the\nservice of this dramatic goal, which implies that lyrics de-\ntermine the music structure. Secondly, jingju music origi-\nnated as folk music,3a fact that carries two implications.\nThis music was not traditionally created by a composer,\nbut arranged by the actors and musicians from a reposi-\ntory of folk tunes to ﬁt the lyrics of new plays. Secondly,\nit has been orally transmitted, and even nowadays written\nmaterial takes a secondary role.\nIn order to convey the expressive needs of the dra-\nmatic plot, music in jingju has been deeply convention-\n2http://isophonics.net/SingingV oiceDataset\n3During the 20thcentury it evolved into composed music.46Figure 1 . Example of a couplet opening line and its corresponding melodic line, both in its original jianpu notation (b) and\nthe transnotated version (a) in JMSC. The three sections of the line are marked.\nalised according to three elements that form its musical\nsystem, namely shengqiang ,banshi , and role type. Each of\nthe original folk tunes upon which jingju music was built\nevolved into a melodic framework called shengqiang , and\nit was associated to a speciﬁc emotional atmosphere. Al-\nthough jingju uses many of them, xipianderhuang are the\nones that give this genre its musical identity.\nMore precise expressive functions are achieved by\nrhythmic transformations of the shengqiang ’s melodic\nframework. Each of these transformations is codiﬁed in a\npattern called banshi . There are about 12 commonly used\nones, although no ﬁxed number is agreed in the literature.\nThebanshi in which the melody is conceived as the closest\nform to the shengqiang ’s original tune, in medium tempo\nand 2/4 metre, is called yuanban .Manban is conceived\nas the transformation of yuanban ’s melody to the slow-\nest tempo range and 4/4 metre, kuaiban is conceived as\nits transformation to the fastest tempo range and 1/4 metre,\nand each intermediate tempo range forms a different ban-\nshi. Besides, there is a set of banshi in which the melody\nis rendered in different forms of free metre.\nThe third element is the role type (that can be under-\nstood as an acting proﬁle), which the singing character be-\nlongs to. There are four broad categories of role types, with\ndifferent degrees of subdivision. In terms of the musical\nsystem however, all of them can be classiﬁed in either the\nmale or female styles of singing, represented respectively\nby the role types laosheng anddan.\nA last important factor to consider is the lyrics structure,\nwhich determines the musical structure of the arias.4The\nlyrics of jingju arias are arranged in couplets, and each of\nthe two lines of the couplet is usually subdivided in three\nsections. The melodic unit in jingju corresponds to the line\nof lyrics, so that each shengqiang deﬁnes a melodic line\nfor the opening line of the couplet, and another one for the\nclosing line, both of them subdivided into three melodic\nsections corresponding to the poetic ones (see Figure 1).\nOne single aria is usually set to only one shengqiang , but\naccording to its expressive content, it can contain different\nbanshi . When dialogues are set to music, characters of the\n4Although translating the original term changduan as aria might\ncause some misconceptions, for the sake of clarity we will use this term\nthroughout the paper to refer to sung sections in a jingju play.same or different role types can sing in the same aria.5\n1.2 The Concepts of Work and Score in Jingju\nBefore describing the collection, two more considerations\nare needed to be taken in order to completely understand\nits usability and representativeness of the repertory. As\nstated in Section 1.1, jingju music was originally created\nand transmitted orally. Professional performers rarely rely\non music scores, but try to convey what they learnt from\ntheir teachers, although inevitable changes occur in this\ntransmission line. Therefore, music scores are notations\nof preexisting music, either for documentation purposes,\nor as learning material for amateurs. Most scores do not\nreference a source, what might indicate that the author is\nnotating what he or she recalls as a standard version. Con-\nsequently, different score editions of the same work present\nnoticeable discrepancies, and very rarely any of them per-\nfectly matches a commercial audio recording of that work,\nalthough the melodic core generally is common to all.\nThe ﬁnal remark has to do with the very concept of\nwork.6The plots of jingju plays are rarely original, they\nare taken from well-known historical events, literary works\nor legends. Hence, a jingju play is just a passage from\na greater encompassing story already known by the audi-\nence. Furthermore, since jingju plays did not have an au-\nthor, but were adapted from these preexisting stories, per-\nformers have the possibility of adapting those passages to\nthe needs of a speciﬁc performance. As a consequence, the\ntitle of a play always refers to the same plot, characters and\nusually the same set of arias, but speciﬁc performances can\nomit, extend or modify certain elements.\nIn a similar way, arias are sung passages of greater en-\ncompassing dramatic plots. Although most of them have\nmusical signals indicating their start and end, they are usu-\nally intermingled with acting and reciting sections, what\nblurs their limits. When recorded or notated for commer-\ncial releases, the arias are named after its ﬁrst line, but dif-\nferent performers or transcribers might have different cri-\nteria for deciding how to delimit the aria or how to deal\n5For a more detailed description in English of the jingju musical sys-\ntem, please refer to [20].\n6We take the term work from MusicBrainz’s terminology as “a distinct\nintellectual or artistic creation” (https://musicbrainz.org/doc/Work).Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 47with non musical interpolations. For example, an aria set\nto more than one banshi can be recorded or transcribed in\none release as a unit, whilst in another release each ban-\nshican be listed separately. All these considerations pose\nimportant challenges for organizing our collection and in-\ntegrating it in our Jingju Music Corpus (see Section 2.3).\nThe remaining of the paper is structured as follows. In\nthe next section the jingju music scores collection is de-\nscribed in detail. In Section 3 we present computational\nanalyses performed on it, and some musicological ﬁndings\nare discussed in Section 4. In the last section we present\nsome concluding remarks and point out future work.\n2. DESCRIPTION OF THE COLLECTION\n2.1 Sources and Generation Process\nIn order to ensure the representativity of the selected\nscores, we have taken as sources three jingju music text-\nbooks [3–5]. These textbooks have been suggested as ref-\nerence works for the study of the jingju musical system\nby professors from the National Academy of Chinese The-\natre Arts (NACTA), where the ﬁrst author did ﬁeld work.\nIn these textbooks, the explanations of the music elements\npresented in Section 1.1 are illustrated with speciﬁc arias.\nIn [5] and [3], the scores were given directly in the text-\nbook. In the case of [4], some scores were given, while\nothers were referenced by title.\nlaosheng dan laosheng +dan total\nerhuang 20 17 1 38\nxipi 24 27 3 54\ntotal 44 44 4 92\nTable 1 . Content of JMSC, according to role type and\nshengqiang\nTo build JMSC, we have taken the arias used to illus-\ntrate the main metred banshi , namely yuanban ,manban ,\nandkuaiban (see Section 1.1), as they are used in the two\nmain shengqiang , that is, xipi anderhuang , for the two\nmain role types, laosheng anddan. When other related\nbanshi are explained together with these, their sample arias\nhave been also included in our collection. Those only ref-\nerenced by title in [4] have been looked for in two printed\nscores collections that accompany our Jingju Music Cor-\npus [1, 2]. Only eight have not been found. If different\nscores for the same aria occur, all versions have been in-\ncluded. As a result, JMSC contains 92 scores covering\n80 arias. Table 1 shows the distribution of scores per role\ntype and shengqiang .Banshi is not included here because\nsome arias contain more than one. However, since the main\nmelodic unit is the line, the information in Table 2 is a bet-\nter representation of JMSC’s potential for the study of the\njingju musical system elements. All the arias have been\nincluded in its full original form, resulting in some sam-\nples of other instances of the musical system elements also\nbeing present in the collection, as shown in Table 2.\nThe original sources use a style of cypher notation\ncalled jianpu (see Figure 1.b). We transtonated them man-daeh daxp lseh lsxp total ldeh\nmanban 72 50 66 17 205\nsanyan 12 17 29 2\nzhongsanyan 6 6\nkuaisanyan 14 26 6 46\nyuanban 54 55 112* 47 268\nerliu 12 12\nliushui 121 78 199\nkuaiban 47 85 132\ntotal 146 285 216 250 897\ndaoban 1\nsanban 2 2 3\nyaoban 1 1 8\n*kutou 4\nTable 2 . Content of JMSC per melodic line, according\nto role type, shengqiang (columns), and banshi (rows). On\nthe upper heading, dastands for dan,lsforlaosheng ,ldfor\nlaodan ,ehforerhuang andxpforxipi. Gray background\nindicates samples of instances not considered as focus of\nour research.\nually using MuseScore 2.17to staff notation, and exported\nthem to MusicXML format. Since both notation systems\nare based in the same principles, the transnotation was\nstraightforward. The main decision taken was the key, not\ngiven in the sources.8We uniﬁed all the scores in E major,\na common key for the two role types considered, as stated\nin the reference textbooks.\nJingju singing is accompanied by an instrumental en-\nsemble in heterophony. Therefore, in order to represent the\ninstrumental accompaniment it is customary to only notate\nthe lead instrument in the ensemble, namely the jinghu, in\nan independent staff. In these cases, both the voice and\nthe instrumental lines are notated in different staves in our\ntransnotation. In other cases, since the voice and the in-\nstrumental ensemble conceptually play the same melody,\nthe original jianpu notation represent both in one single\nstaff with the instrumental sections in brackets, as shown\nin Figure 1.b. In those cases, our transnotation divides the\noriginal single staff into two, one for the voice and one\nfor the accompaniment, as shown in Figure 1.a. Of the 92\nscores, 53 (57.61 %) of them contain full accompaniment.\n2.2 Coverage, Completeness and Reusability\nSerra proposes in [15] ﬁve criteria for building research\ncorpora which we followed for creating JMSC. Among\nthem, purpose has been described in detail in Section 1,\nand we argue that the quality of the scores is assured inso-\nfar as our transnotation, as described in Section 2.1, main-\ntains all the information contained in the original sources.\nTherefore, we discuss here the remaining criteria.\nIn terms of coverage , the collection includes the two\nmain shengqiang ,xipianderhuang , and the two main role\ntypes, laosheng anddan. In order to evaluate their rele-\nvance for jingju music research, we have measured their\noccurrence in the recordings collection of the CompMu-\nsic Jingju Music Corpus, as published in [14]. Xipi and\n7https://musescore.org/\n8The speciﬁc tuning is chosen, within a certain range, according to the\nperformer’s needs.48 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017erhuang stand for 80.25 % of the shengqiang present in\nthe recordings collection, and laosheng anddanstand for\n73.66 % of the role types. The whole range of metred ban-\nshiis represented in the collection, with special focus on\nyuanban ,manban ,liushui andkuaiban (see Table 2), as the\nmost relevant ones (see Section 1.1). There are two main\nreasons for excluding the non-metred ones: metred ban-\nshipresent more direct relations between them, what fa-\ncilitates the study of the transformation processes. On the\nother hand, we are still looking for a satisfactory method\nto process non metred melodies. There also exist a few\nauxiliary banshi , whose occurrence is very occasional, and\ntherefore no representative of the norm.\nConsidering completeness , the collection contains the\nmetadata of the scores and annotations both at the score\nand the line level, organised in separate spreadsheets. For\nthe scores, the metadata contain the title of the work in\nChinese, role type, shengqiang ,banshi , whether it contains\nfull accompaniment (see Section 2.1), the reference of the\noriginal score, and if existing, the list of the MusicBrainz\nIDs of the recordings in our Jingju Music Corpus corre-\nsponding to the same work as the score. As for the lines,\neach of them is annotated with the role type, shengqiang ,\nbanshi , line type, that is, opening—which in the case of er-\nhuang is divided in two types—or closing, the lyrics for the\nwhole line and for each of its sections, the linguistic tones\nof the lyrics, and the starting and ending offsets of the line\nand each of its sections. Annotations have been done by the\nﬁrst author, linguistic tones have been extracted automat-\nically9and corrected manually by the ﬁrst author and by\na Chinese native speaker knowledgeable about jingju, and\nspeciﬁc doubts about line segmentation have been solved\nby two professors in NACTA.\nRegarding reusability , all the scores, including meta-\ndata and annotations, are available for research purposes.\nDue to copyright issues, they are only available on demand,\nby contacting the authors.\n2.3 Integration in the Jingju Music Corpus\nJMSC was gathered as part of the Jingju Music Corpus\nfrom the CompMusic project, also created with the pur-\npose of studying jingju singing in terms of its musical sys-\ntem elements. To exclude the inﬂuence of academic com-\npositional techniques applied to jingju during the 20thcen-\ntury, only plays from the traditional repertoire were con-\nsidered for the corpus, including JMSC. Consequently, two\nsamples of revolutionary plays from [5] were discarded.\nThe scores are integrated in the corpus via the work they\nrepresent. If a particular recording in the corpus contains\na performance of a work to which a particular score is re-\nlated, due to the circumstances described in Section 1.2, it\ncan not be assumed that the recording contains a perfor-\nmance of the score. Consequently, scores and recordings\nare indirectly related through works. Taking this into ac-\ncount, 63 of the 92 scores (68.48 %) are related to works\nassociated to one or more audio recordings.\n9The software used for extracting the linguistic tones can be obtained\nhere: https://github.com/zangsir/Native-to-Pinyin2.4 Potential of the Collection\nJMSC has great potential for the computational research of\njingju singing in terms of its musical system elements. In\nthe following section we present statistical analyses with\nthat aim. However, it is also suitable for other research\ntasks. Since the variety of melodic material is conceived\nas transformations of original tunes, these scores offer an\nexcellent opportunity for pattern discovery and similarity\nanalysis. These tasks can beneﬁt from the accompanying\nannotations to develop culture speciﬁc heuristics. The an-\nnotated linguistic tones for the lyrics allow the research of\ntheir relationship with melody using symbolic data, whilst\nsuch studies have been carried out so far using only au-\ndio [22, 23]. Since the scores contain full or partial nota-\ntion of the accompaniment, they are a good resource for\nthe analysis of the relationship between the singing and in-\nstrumental lines. Finally, although scores and recordings\nare not directly related, the similarity between those which\nshare a common work still allows combined analyses.\n3. ANALYSES PERFORMED ON THE JINGJU\nMUSIC SCORES COLLECTION\nIn order to take advantage of JMSC, we have extracted sta-\ntistical information with the aim of testing musicological\nclaims made in the reviewed literature [3–6, 20]. To pro-\ncess the scores we have used the music21 toolkit [9], and\nthe developed code is openly available.10We introduce\nnow the four types of features considered for analysis, to-\ngether with their musicological motivations:\nPitch histograms . Jingju music is based on an an-\nhemitonic pentatonic scale, each of whose degrees can be\nthe ﬁnalis of a mode ( diao), a deﬁning characteristic of\neach shengqiang . Hence, xipi is associated to the gong\nmode, which has the ﬁrst degree as ﬁnalis, and erhuang to\ntheshang mode, whose ﬁnalis is the second degree. The\n4thand 7thdegrees, usually omitted, can be used as expres-\nsive notes. Male and female styles of singing, represented\nby the laosheng anddanrole types are deﬁned by different\npitch registers. Pitch histograms can help to gain a deeper\nunderstanding of pitch distributions for each shengqiang\nand pitch register for each role type, as well as to evaluate\nthe role of the expressive notes.\nInterval histograms . One of the melodic features given\nin the literature for distinguishing xipifrom erhuang is that\nit uses larger intervals. Analysing intervals through his-\ntograms will shed light upon this claim.\nCadential notes . One of the more common character-\nistics given in the literature when comparing xipiander-\nhuang , and closely related to their modal associations, is\na schema of cadential notes ( laoyin ) for each line of the\ncouplet and each of their three sections.\nMelodic density . Understood as the proportion of notes\nper syllable, this feature is used for characterizing banshi ,\narguing that the slower the banshi is, the more notes are\nused for singing each syllable.\n10https://github.com/MTG/Jingju-Scores-Analysis/releases/tag/v1.0Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 49Our code computes the aforementioned features for any\ncombination of role type, shengqiang ,banshi and line\ntype. Histograms show blue bars for laosheng and orange\nones for dan, whilst shengqiang is indicated by the hatch,\nnforxipiand / for erhuang (see Tables 3, 4, and 5). A solid\nred line marks the ﬁrst degree (E4) in pitch histograms, and\na dashed one its higher octave (see Tables 3, and 4). Inter-\nval analysis can consider interval direction or not. Caden-\ntial notes are computed for each section of the line, distin-\nguishing opening and closing lines (see Table 6). Melodic\ndensity box plots show results for individual scores and for\nthe average of all of them (see Table 7).\nFor this paper we computed the four aforementioned\nfeatures for the whole collection considering all the com-\nbinations of role type, shengqiang , and banshi . For\na ﬁrst approach to the results, we grouped the banshi\nin three groups: yuanban and erliu , in 2/4 metre and\nmedium tempo ranges; manban ,sanyan ,zhongsanyan and\nkuaisanyan , in 4/4 metre and slower tempo ranges than the\nprevious group; and kuaiban andliushui , in 1/4 metre and\nfaster tempo ranges than the ﬁrst group. To ease readabil-\nity, each group is referred to in the next section by the ﬁrst\nbanshi of the group. All the resulting plots are available.10\n4. DISCUSSION OF THE RESULTS\nFrom the results of the aforementioned analyses we have\nobtained the following musicological ﬁndings. Pitch his-\ntograms in Table 3 show that the role types laosheng and\ndanare well deﬁned in terms of pitch range. The predom-\ninance of the 5thand 6thdegrees for dan(B4 and C#4) is\nexplained in the literature as a transposition of the modal\ncenter a ﬁfth higher. In terms of pitch distribution, it can be\nobserved that the ﬁnalis of each of the modes associated to\neach shengqiang , namely 1st(E4) for xipiand 2nd(F#4) for\nerhuang , or their transpositions a ﬁfth higher for dan, are\nin fact the most predominant pitches, but not very far from\nother degrees. The exception is laosheng erhuang , whose\nmost predominant pitch is the lower 6thdegree (C#3), and\nthat can be characterised by a relatively major importance\nxipi erhuanglaosheng\n dan\nTable 3 . Pitch histograms for the role types laosheng and\ndan, and the shengqiang xipi anderhuang . All banshi in-\ncludedmanban yuanbanlaosheng xipi\n laosheng erhuang\nTable 4 . Pitch histograms for the role type laosheng , com-\nparing the banshi groups manban andyuanban for the\nshengqiang xipi anderhuang\nof the lower region of its register. The use of the 7thde-\ngree (D#4) is also remarkable, specially prominent for dan,\nwhat can be explained by the transportation of the modal\ncenter, resulting in that this pitch acts as a 3rddegree. The\n4thdegree (A4) appears, but very rarely, and also its sharp-\nened version (A#4). The appearance of these two versions\ncould be due to the fact that its absolute tuning differs from\nthe equal temperament [8], and transcribers use different\napproaches to notate this pitch.\nWe have also found that pitch histograms are useful for\ncharacterizing different banshi . In Table 4 it can be ob-\nserved how slow banshi (manban group) explore lower\npitch regions than the ones closer to the original tune\n(yuanban group). That is especially relevant in xipi. It can\nalso be observed how the former make a more frequent use\nof the expressive tone D#4, the 7thdegree.\nxipi manban erhuang manbanlaosheng\n dan\nTable 5 . Interval histogram for the role types laosheng and\ndanand the shengqiang xipi anderhuang , considering only\nthebanshi group manban\nOur ﬁndings on interval distributions support the mu-\nsicological claims stated previously, but also help to ob-\nserve some nuances. Table 5 shows that, in the case of\nlaosheng , diatonic steps are more frequent than minor third50 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017erhuang manban xipi manbanlaosheng\n dan\nTable 6 . Cadential notes in the banshi group manban for\nthe role types laosheng anddan and the shengqiang er-\nhuang andxipi, for opening (Op.) and closing (Cl.) lines\nin each of their three sections (S1, S2, and S3)\nsteps in erhuang than in xipi. However, in the case of\ndan, the results do not support the claim. In all cases, it\ncan be observed that intervals larger than the minor 3rdare\nrare in both shengqiang , with the exception of the perfect\n4th, whose distribution is similar in the four cases. Conse-\nquently, the major use of large intervals in xipias compared\nwith erhuang can be nuanced in light of these plots.\nIt is an agreement in jingju music literature to deﬁne\nlaosheng xipi as cadencing in the 2nddegree for the open-\ning line of the couplet, and in the 1st degree for the closing\nline, and erhuang as the opposite, 1stdegree for the open-\ning line and 2nddegree for the closing line. In the case\nofdan, cadences will be 6thand 5thdegrees for opening\nand closing lines in xipi, and higher octave 1stand 5thde-\ngrees for erhuang . Different sources refer with different\ndegrees of precision to cadential notes for each section of\nthe line and to exceptions to these general rules. Focusing\nnow only in line cadential notes, that is, those for section\n3 (S3), Table 6 shows that only closing lines in laosheng\nxipi manban completely match the rules given previously,\npresenting the 1st degree (E4) as cadential note in all cases.\nAs for laosheng erhuang manban , the cadential notes es-\ntablished in theory occupy a very small percentage, espe-\ncially noticeable in opening line 1. That is also the case for\ndan erhuang , which presents a more varied range of possi-\nbilities for cadential notes than dan xipi , and where those\nestablished in the literature stand only for a minimum per-\ncentage in opening lines.\nFinally, Table 7 shows how different banshi groups can\nbe characterized in terms of melodic density. The duration\nunit for measuring a syllable length is the crotchet. Two\naspects are interesting in these plots. The median for each\ngroup shows, as expected, that sung syllables are longer\nas the tempo decreases, and compared with laosheng , the\ndanrole type shows slightly higher median values in yuan-\nban. But the outliers also provide meaningful information.\nThese correspond to a singing feature known as tuoqiang ,laosheng xipi dan xipimanban\n yuanban\n kuaiban\nTable 7 . Melodic density for the banshi groups manban ,\nyuanban , and kuaiban and the role types laosheng anddan\nin the shengqiang xipi\nliterally “dragged tune”, by which the melody of a syllable,\nusually at the end of a line or a line section, is extended by\na long melisma. Table 7 shows how much more frequently\nthis phenomenon occurs in manban than in yuanban , and\nhow it is almost non-existing in kuaiban . However, when\nit appears in this banshi , it can be longer than in yuanban .\n5. CONCLUSIONS AND FUTURE WORK\nIn this paper we presented the ﬁrst collection of machine\nreadable scores for the study of jingju singing in terms of\nits musical system elements. It includes 92 scores covering\n897 melodic lines, and is accompanied by their metadata\nand curated annotations per score and melodic line. The\ncollection is part of the corpus gathered in the CompMusic\nproject for jingju music research. Its potential for jingju\nsinging analysis has been tested by a series of statistical\nanalyses, and some musicological ﬁndings have been dis-\ncussed. The whole collection, together with the metadata,\nthe annotations, and the developed code are available for\nresearch purposes.\nIn future work, it is expected to expand the collection by\nincluding other instances of the musical system elements\nthat are not currently present, especially non metred ban-\nshi. At the same time, the collection’s potential, as pointed\nout in Section 2.4, will be exploited for different research\ntasks. Among them, pattern discovery is a promising topic,\nsince the accompanying structural annotations can be used\nto design heuristics to incorporate to state of the art ap-\nproaches, as well as the analysis of the relationship be-\ntween linguistic tones and melody. Most importantly, we\nhope that JMSC contributes to open up a new range of pos-\nsibilities for MIR research on jingju music.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 516. ACKNOWLEDGEMENTS\nThe authors would like to thank Mr. Shuo Zhang for au-\ntomatically extracting the linguistic tones of the scores’\nlyrics, Prof. Shen Pengfei and Prof. Xiong Luxia from\nNACTA for helping to solve some questions about line\nsegmentation, and Mr. Rong Gong for reviewing and cor-\nrecting the annotations of linguistic tones. This research is\nfunded by the European Research Council under the Eu-\nropean Union’s Seventh Framework Program (FP7/2007-\n2013), as part of the CompMusic project (ERC grant agree-\nment 267583).\n7. REFERENCES\n[1]Jingju qupu jicheng 京剧曲谱集成(Collection of\njingju scores). Shanghai wenyi chubanshe, Shanghai,\n1998. 10 V ols.\n[2]Zhongguo jingju liupai jumu jicheng 中国京剧流派剧\n目集成(Collection of plays of Chinese jingju schools).\nXueyuan chubanshe, Beijing, 2006–2010. 21 V ols.\n[3] Liu J.刘吉典.Jingju yinyue gailun 京剧音乐分析(In-\ntroduction to jingju music). Renmin yinyue chubanshe,\nBeijing, 1992.\n[4] Zhang Z.张正治.Jingju chuantongxi pihuang\nchangqiang jiegou fenxi 京剧传统戏皮黄唱腔结构\n分析(Structural analysis of pihuang singing in jingju\ntraditional plays). Renmin yinyue chubanshe, Beijing,\n1992.\n[5] Cao B. 曹宝荣, editor. Jingju changqiang banshi jiedu\n京剧唱腔板式解读(Deciphering banshi in jingju\nsinging). Renmin yinyue chubanshe, Beijing, 2010.\n[6] Jiang J. 蒋菁.Zhongguo xiqu yinyue 中国戏曲音乐\n(Music of Chinese traditional opera). Renmin yinyue\nchubanshe, Beijing, 2000.\n[7] D. A. A. Black, M. Li, and M. Tian. Automatic identi-\nﬁcation of emotional cues in Chinese opera singing. In\n13thICMPC , pages 250–255, Seoul, Korea, 2014.\n[8] K. Chen. Characterization of pitch intonation of Bei-\njing opera. Master’s thesis, Universitat Pompeu Fabra,\nBarcelona, Spain, 2013.\n[9] Michael Scott Cuthbert and Christopher Ariza.\nmusic21 : A toolkit for computer-aided musicology\nand symbolic music data. In 11thISMIR , pages 637–\n642, Utrecht, Netherlands, 2010.\n[10] G. Dzhambazov, Y . Yang, R. Caro Repetto, and\nX. Serra. Automatic alignment of long syllables in\na cappella Beijing opera. In 6thFMA , pages 88–91,\nDublin, Ireland, 2016.\n[11] R. Gong, N. Obin, G. Dzhambazov, and X. Serra.\nScore-informed syllable segmentation for jingju a cap-\npella singing voice with mel-frequency intensity pro-\nﬁles. In 7thFMA , pages 107–113, M ´alaga, Spain, 2017.[12] R. Gong, Y . Yang, and X. Serra. Pitch contour segmen-\ntation for computer-aided jingju singing training. In\n13thSMC , pages 172–178, Hamburg, Germany, 2016.\n[13] R. Caro Repetto, R. Gong, N. Kroher, and X. Serra.\nComparison of the singing style of two jingju schools.\nIn16thISMIR , pages 507–513, M ´alaga, Spain, 2015.\n[14] R. Caro Repetto and X. Serra. Creating a corpus\nof jingju (Beijing opera) music and possibilities for\nmelodic analysis. In 15thISMIR , pages 313–318,\nTaipei, Taiwan, 2014.\n[15] X. Serra. Creating research corpora for the compu-\ntational study of music: the case of the compmusic\nproject. In AES 53rdInternational Conference on Se-\nmantic Audio , pages 1–9, London, UK, 2014.\n[16] A. Srinivasamurthy, R. Caro Repetto, H. Sundar, and\nX. Serra. Transcription and recognition of syllable\nbased percussion patterns: The case of Beijing opera.\nIn15thISMIR , pages 431–436, Taipei, Taiwan, 2014.\n[17] J. Sundberg, L. Gu, Q. Huang, and P. Huang. Acous-\ntical study of classical Peking opera. Journal of Voice ,\n26(2):137–143, 2012.\n[18] M. Tian and M. B. Sandler. Towards music structural\nsegmentation across genres: Features, structural hy-\npotheses, and annotation principles. ACM Transactions\non Intelligent Systems and Technology (TIST) , 8(2):23–\n41, 2017.\n[19] M. Tian, A. Srinivasamurthy, M. Sandler, and X. Serra.\nA study of instrument-wise onset detection in Beijing\nopera percussion ensembles. In ICASSP 2014 , pages\n2174–2178, Florence, Italy, 2014.\n[20] E. Wichmann. Listening to theatre: The aural dimen-\nsion of Beijing opera . University of Hawaii Press, Hon-\nolulu, 1991.\n[21] L. Yang, M. Tian, and E. Chew. Vibrato characteristics\nand frequency histogram envelopes in Beijing opera\nsinging. In 5thFMA , pages 139–140, Paris, France,\n2015.\n[22] S. Zhang, R. Caro Repetto, and X. Serra. Study of the\nsimilarity between linguistic tones and melodic pitch\ncontours in Beijing opera singing. In 15thISMIR , pages\n343–348, Taipei, Taiwan, 2014.\n[23] S. Zhang, R. Caro Repetto, and X. Serra. Predicting\npairwise pitch contour relations based on linguistic\ntone information in Beijing opera singing. In 16thIS-\nMIR, pages 107–113, M ´alaga, Spain, 2015.52 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Identifying Raga Similarity Through Embeddings Learned from Compositions&apos; Notation.",
        "author": [
            "Joe Cheri Ross",
            "Abhijit Mishra",
            "Kaustuv Kanti Ganguli",
            "Pushpak Bhattacharyya",
            "Preeti Rao"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417032",
        "url": "https://doi.org/10.5281/zenodo.1417032",
        "ee": "https://zenodo.org/records/1417032/files/RossMGBR17.pdf",
        "abstract": "Identifying similarities between ragas in Hindustani mu- sic impacts tasks like music recommendation, music in- formation retrieval and automatic analysis of large-scale musical content. Quantifying raga similarity becomes ex- tremely challenging as it demands assimilation of both in- trinsic (viz., notes, tempo) and extrinsic (viz. raga singing- time, emotions conveyed) properties of ragas. This pa- per introduces novel frameworks for quantifying similar- ities between ragas based on their melodic attributes alone, available in the form of bandish (composition) notation. Based on the hypothesis that notes in a particular raga are characterized by the company they keep, we design and train several deep recursive neural network variants with Long Short-term Memory (LSTM) units to learn dis- tributed representations of notes in ragas from bandish notations. We refer to these distributed representations as note-embeddings. Note-embeddings, as we observe, capture a raga’s identity, and thus the similarity between note-embeddings signifies the similarity between the ragas. Evaluations with perplexity measure and clustering based method show the performance improvement in identifying similarities using note-embeddings over n-gram and uni- directional LSTM baselines. While our metric may not capture similarity between ragas in their entirety, it could be quite useful in various computational music settings that heavily rely on melodic information.",
        "zenodo_id": 1417032,
        "dblp_key": "conf/ismir/RossMGBR17",
        "keywords": [
            "ragas",
            "hindustani music",
            "music recommendation",
            "music information retrieval",
            "automatic analysis",
            "quantifying similarities",
            "melodic attributes",
            "bandish notation",
            "note-embeddings",
            "perplexity measure"
        ],
        "content": "IDENTIFYING RAGA SIMILARITY THROUGH EMBEDDINGS LEARNED\nFROM COMPOSITIONS’ NOTATION\nJoe Cheri Ross1Abhijit Mishra3Kaustuv Kanti Ganguli2\nPushpak Bhattacharyya1Preeti Rao2\n1Dept. of Computer Science & Engineering,2Dept. of Electrical Engineering\nIndian Institute of Technology Bombay, India\n3IBM Research India\njoe@cse.iitb.ac.in\nABSTRACT\nIdentifying similarities between ragas in Hindustani mu-\nsic impacts tasks like music recommendation, music in-\nformation retrieval and automatic analysis of large-scale\nmusical content. Quantifying raga similarity becomes ex-\ntremely challenging as it demands assimilation of both in-\ntrinsic (viz., notes, tempo) and extrinsic (viz.raga singing-\ntime, emotions conveyed) properties of ragas. This pa-\nper introduces novel frameworks for quantifying similar-\nities between ragas based on their melodic attributes alone,\navailable in the form of bandish (composition) notation.\nBased on the hypothesis that notes in a particular raga\nare characterized by the company they keep , we design\nand train several deep recursive neural network variants\nwith Long Short-term Memory (LSTM) units to learn dis-\ntributed representations of notes in ragas from bandish\nnotations. We refer to these distributed representations\nasnote-embeddings . Note-embeddings, as we observe,\ncapture a raga’s identity, and thus the similarity between\nnote-embeddings signiﬁes the similarity between the ragas.\nEvaluations with perplexity measure and clustering based\nmethod show the performance improvement in identifying\nsimilarities using note-embeddings over n-gram and uni-\ndirectional LSTM baselines. While our metric may not\ncapture similarity between ragas in their entirety, it could\nbe quite useful in various computational music settings that\nheavily rely on melodic information.\n1. INTRODUCTION\nHindustani music is one of the Indian classical music tradi-\ntions developed in northern part of India getting inﬂuences\nfrom the music of Persia and Arabia [17]. The south Indian\nmusic tradition is referred to as Carnatic music [30]. The\ncompositions and their performances in both these classi-\ncal traditions are strictly based on the grammar prescribed\nc\rJoe Cheri Ross, Abhijit Mishra, Kaustuv Kanti Ganguli,\nPushpak Bhattacharyya, Preeti Rao. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nJoe Cheri Ross, Abhijit Mishra, Kaustuv Kanti Ganguli, Pushpak Bhat-\ntacharyya, Preeti Rao. “Identifying Raga Similarity Through embeddings\nlearned from Compositions’ notation”, 18th International Society for Mu-\nsic Information Retrieval Conference, Suzhou, China, 2017.by the raga framework. A raga is a melodic mode or tonal\nmatrix providing the grammar for the notes and melodic\nphrases, but not limiting the improvisatory possibilities in\na performance [25].\nRaga being one of the most prominent categorization\naspect of Hindustani music, identifying similarities be-\ntween them is of prime importance to many Hindustani\nmusic speciﬁc tasks like music information retrieval, mu-\nsic recommendation, automatic analysis of large-scale mu-\nsical content etc. Generally similarity between ragas is\ninferred through attributes associated with the ragas. For\ninstance, in Hindustani music, classiﬁcation of ragas based\non the tonal material involved is termed as thaat . There are\n10thaats in Hindustani music [8]. prahar ,jati,vadi,sam-\nvadi etc. are the other important attributes. Most of the\naccepted similarities between ragas encompass the simi-\nlarities in many of these attributes. But these similarities\ncannot always be derived exclusively from these attributes.\nMelodic similarity is a strong substitute and close to per-\nceived similarity. The melodic similarity between Hindus-\ntani ragas is not largely available in documented form. This\nnecessitates systems for raga similarity measurement to be\ndevised, even though the number of ragas in the Hindustani\nclassical framework is ﬁxed.\nA composed musical piece termed as bandish is writ-\nten to perform in a particular raga, giving ample freedom\nto the performer to improvise upon. As the literal mean-\ning suggests, bandish is tied to its raga, tala(rhythm) and\nlyrics. Bandish is taken as the basic framework for a per-\nformance which gets enriched with improvisation while\nthe performer renders it. Realization of a bandish in a per-\nformance brings out all the colors and characteristics of\na raga. Given this fact, audio performances of the ban-\ndishes can be deemed to be excellent sources for analyzing\nraga similarities from a computational perspective. How-\never, methods for automatic transcription of notations from\naudio performances have been elusive; this restricts the\npossibilities of exploiting audio-resources. Our work on\nraga similarity identiﬁcation, thus, relies on notations hav-\ning abstract representation of a performance covering most\ndimensions of the composition’s raga. We use bandish no-\ntations dataset available from swarganga.org [16].\nOur proposed approach, based on deep recursive\nneural network with bi-directional LSTM as recurrent515units, learns note-embeddings for each raga from the\nbandish notations available for that raga. We partition\nour data by raga and train the model independently\nfor each raga. It produces as many note-embeddings, as\nmany different ragas we have represented in the dataset.\nThecosine similarity between the note-embeddings serves\nfor analyzing the similarity between the ragas. Our evalu-\nations with perplexity measure and clustering based meth-\nods show the performance improvement in identifying sim-\nilarities using note-embeddings using our approach over\n(a) a baseline that uses n-gram overlaps of notes in ban-\ndishfor raga similarity computation (b) a baseline that uses\npitch class distribution (PCD) and (c) our approach with\nuni-directional LSTM. We believe, our approach can be\nseamlessly adopted to the Carnatic music style as it fol-\nlows most of the principles as Hindustani music.\n2. RELATED WORK\nTo the best of our knowledge no such attempts to identify\nraga similarity have been made so far. The work closest\nto ours is by Bhattacharjee and Srinivasan [5] who dis-\ncuss raga identiﬁcation of Hindustani classical audio per-\nformances through a transition probability based approach.\nHere they also discuss about validating the raga identiﬁca-\ntion method through identifying known raga relationship\nbetween 10 ragas considered for this work. A good num-\nber of research works have been carried out pertaining to\nraga identiﬁcation in Hindustani music using note intona-\ntion [3], chromagram patterns [11], note histogram [12].\nPandey et al. [22] proposed an HMM based approach on\nautomatically transcribed notation data from audio. There\nhas been quite a few raga recognition attempts in Carnatic\nmusic also [28, 4, 27, 24].\n3. RAGA SIMILARITY BASED ON NOTATION:\nMOTIVATION AND CENTRAL IDEA\nWhile the general notion of raga similarity is based on var-\nious dimensions of ragas like thaat ,prahar ,jati,vadi,sam-\nvadi etc. , the similarities perceived by humans (musicians\nand expert listeners) is predominantly substantiated upon\nthe melodic structure. A raga-similarity method solely\nbased on notational (melodic) information can be quite rel-\nevant to computational music tasks involving Indian clas-\nsical music.\nTheoretically, the identity of a raga lies in how certain\nnotes and note sequences (called phrases) are used in its\ncompositions. We hypothesize that capturing the semantic\nassociation between different notes appearing in the com-\nposition can possibly reveal the identity of a raga. More-\nover, it can also provide insights into how similar or dis-\nsimilar two ragas can be, based on how similar / dissimilar\nthe semantic associations of notes in the compositions are.\nWe believe , notes for a speciﬁc raga can be represented in\ndistributed forms (such as vectors), reﬂecting their seman-\ntic association with other notes in the same raga (analogous\nto words having distributed representations in the domain\nof computational linguistics [18]). These representations\n++++\nSoftMax  \nLSTM\tLSTM\tLSTM\tLSTM\tLSTM\tLSTM\t\nLSTM\tLSTM\tLSTM\tLSTM\tLSTM LSTM\tLSTM\tLSTM\t.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t[Merge] [Merge] [Note distribution] \n[ |𝑽|×𝒅  representation] LSTM\tLSTM\t++++SoftMax  SoftMax  SoftMax  \ne2\te1\te3\ten\tx2\tx1\tx3\txn\tC1\tC2\tC3\tCn\tFigure 1 . Bi-directional LSTM architecture for learning\nnote-embeddings\ncould account for how notes are preceded and succeeded\nby other notes in compositions.\nFormally, in a composition, a note x2V(where Vrep-\nresents a vocabulary all notes in three octaves) can be rep-\nresented as a ddimensional vector that captures semantic-\ninformation speciﬁc to the raga that the compositions be-\nlong to. Such distributed note-representations, referred to\nasnote-embeddings (jVj\u0002dmatrix) can be expected to\ncapture more information than other forms of sparse rep-\nresentations (like presenting notes with unique integers).\nWe propose a bi-directional LSTM [14] based architecture\nthat is motivated by the the work of Huang and Wu [15] to\nlearn note-embeddings characterizing a particular style of\nmusic. We learn note-embeddings for each raga separately\nfrom the compositions available for the raga.\nHow can note-embeddings help capture similarities be-\ntween ragas? We hypothesize that embeddings learned for\na given note for similar ragas will have more similarity. For\nexample, the representation for note Ma-elevated (equiva-\nlent note F#in C-scale) in raga Yaman can be expected to\nbe very similar to that of Yaman Kalyan as both of these\nragas share very similar melodic characteristics.\n4. NEURAL NETWORK ARCHITECTURE FOR\nLEARNING NOTE-EMBEDDINGS\nWe design a deep recurrent neural network (RNN), with\nbi-directional LSTMs as recurrent units, that learns to pre-\ndict the forth-coming notes that are highly likely to ap-\npear in a bandish composition, given input sequences of\nnotes. This is analogous to neural language models built\nfor speech and text synthesis [19]. While our network tries\nto achieve this objective, it learns distributed note repre-\nsentations by regularly updating the note-embedding ma-\ntrix. The choice of this architecture is due to the facts that516 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017(a) for sequence learning problems like ours, RNNs with\nLSTM blocks have proven useful [29, 13], and (b) in Hin-\ndustani music a note rendered at a moment has dependence\non patterns preceding and succeeding it, motivating us to\nuse bi-directional LSTM.\nThe model architecture is shown in Figure 1. Suppos-\ning that a sequence in a composition has nnotes ( nto be\nkept constant by padding wherever necessary), denoted as\nx1; x2; x3; :::; x n, where8i2n; xi2V. The note xi\ncan be represented in one-hot format, with the jthcom-\nponent of ajVjdimensional zero-vector set to 1, if xiis\nthejthelement of vocabulary V. Each note is input to a\nnote-embedding layerWof dimensionjVj\u0002dwhere dis\nthe note-embedding dimension. The output of this layer\nis a sequence of embeddings eiof dimension d, obtained\nby performing a matrix multiplication between xiwithW.\nThe embedding sequences e1; e2; e3; :::; e nare input to two\nlayers of bi-directional LSTMs.\nFor each time-step ( i2n), the context-representations\nlearned by the outer-bidirectional LSTM layer ( Ci) is\npassed through a softmax layer that computes the con-\nditional probability distribution of all possible notes given\nthe context representations given by LSTM layers.\nFor each time-step, the prediction of the forthcoming\nnote in the sequence is done by choosing the note that max-\nimizes the likelihood given the context i.e.\n^x=argmax\nj2jVjP(xi+1=vjjCi) (1)\nwhere Ciis the merged context representations learned by\nthe forward and backward sequences in the bi-directional\nLSTM layers. Probability of a note at a time-step is com-\nputed by the softmax function as,\nP(xi+1=vjjCi) =exp(UjTCi+bj)\nPjVj\nk=1exp(UkTCi+bk)(2)\nwhere Uis the weight matrix in the softmax layer and\nbjis bias term corresponding to note vj.\nThe embedding layer is initialized randomly and during\ntraining, errors (in terms of cross-entropy) are back prop-\nagated upto the embedding layer, resulting in the updation\nof the embedding-matrix. Cross-entropy is computed as,\n1\nM\u0002TMX\ni=1TX\nt=1cross entropy (yi\nt;^yi\nt) (3)\ncross entropy (y;^y) =\u0000jVjX\np=1yplog^yp (4)\nWhere Mis the number of note sequences in a raga and T\nis the sequence length. yi\ntdenotes the expected distribution\nofithnote sequence at time-step t(bit corresponding to\nthe expected note set to 1 and rest to 0s) and ^yi\ntdenotes the\npredicted distribution. Since our main objective is to learn\nsemantic representation of notes through note-embeddings\n(and not predict note sequences), we do not heavily reg-\nularize our system. Moreover, our network design is in-\nspired by Mikolov et al. [18], who also do not heavily\nregularize their system while learning word-embeddings.4.1 Raga Similarities from Note-embeddings\nFor each raga our network learns a jVj\u0002dmatrix repre-\nsentingjVjnote-embeddings. We compute (dis)similarity\nbetween two ragas by computing pairwise cosine distance\nbetween embedding vectors of every note in Vand then\naveraging over all notes. This is based on the assump-\ntion that distributed representations of notes (as captured\nby the embeddings) will be similar across ragas that are\nsimilar. The choice of cosine similarity (or cosine distance)\nfor computing the similarity between the note-embeddings\nis driven by its robustness as a measure of vector similarity\nfor vectors and its predominant usage for measuring word\nembedding similarity [20]. Appropriate distance measures\nhave been adopted for non-LSTM based baselines.\n5. BASELINES FOR COMPARISON\nTo conﬁrm the validity, we compare our approach with a\nfew baseline approaches.\n5.1 N-gram Based Approach\nThe N-gram based baseline creates an n-gram proﬁle based\non the count of each n-gram from the available compo-\nsitions in a raga. We compute the n-gram for n ranging\nfrom 1 to 4. The distance between two ragas is computed\nusing the out-of-place measure described in Cavnar et al.\n[7]. Out-of-place measure depends on the rank order statis-\ntics of the two proﬁles. It computes how far 2 proﬁles are\nout-of-place w.r.t the n-gram rank order statistics. The dis-\ntance is taken as the l2norm of all the n-gram rank differ-\nences, normalized by the number of n-grams. Intuitively,\nthe more similar two ragas are, more would the N-gram\nproﬁles overlap, reducing the l2norm.\n5.2 Pitch Class Distribution (PCD)\nThis method computes the distribution of notes from the\ncount of notes in a raga’s bandish dataset. 36 notes(across\n3 octaves) are considered separately for computing PCD.\nAs the method describes, sequence information is not cap-\ntured here. The similarity distance between two ragas is\ncomputed by taking the euclidean distance between the\ncorresponding pitch class distributions; the assumption is\nthat each pitch class two similar ragas will share similar\nprobability value, thereby reducing the euclidean distance.\nFor the raga recognition task by Chordia et al. [9], eu-\nclidean distance is used for computing the distance be-\ntween pitch class distributions in one of their approaches.\nThis baseline is to verify the relevance of sequence infor-\nmation in capturing raga similarity.\n5.3 Uni-directional LSTM\nThe effectiveness of a bi-directional LSTM for modeling\nHindustani music is veriﬁed with this baseline. The ar-\nchitecture is same as described in Figure 1, except for the\nreplacement of bi-directional LSTMs with uni-directional\nLSTMs. Since there is only forward pass in uni-directionalProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 517LSTM, the merge operation in bi-directional LSTM design\nis not required here.\n6. DATASET\nOur experiments are carried out with the Hindustani ban-\ndish dataset available from swarganga.org , created by\nSwarganga music foundation. This website is intended to\nsupport beginners in Hindustani music. This has a large\ncollection of Hindustani bandishes , with lyrics, notation,\naudio and information on raga, tala andlaya. Figure 2\nFigure 2 . Abandish instance from swarganga website.\nshows a bandish instance from swarganga. The name of\nthisbandish is‘jaane naa jaane haree’ in raga Adana and\ninteen taal (16 beats cycle). The ﬁrst row contains the bol\ninformation which details the tabla strokes corresponding\nto the talaof the bandish . Other rows have lyrics (bottom)\nalong with the notes (top) corresponding to the lyrical sec-\ntions. Each row corresponds to a talacycle. In Hindustani\nnotation system S r R g G m M P d D n N corresponds to C\nC#D D#E F F#G G#A A#B notes in western music no-\ntation system, when the tonic is at C. A note followed by\na single quotation at the right shows it is in the higher oc-\ntave and a single quotation at the left implies lower octave.\nNotes mentioned within parenthesis are kannotes (grace\nnotes). Each column represents a beat duration.\nFrom this dataset we have considered 144 ragas for our\nstudy which are represented well with sufﬁcient number of\nbandishes . Table 1 presents dataset statistics.\n#bandishes #ragas #notes#kan swaras\n(grace notes)\n2955 144 2,95,411 50,749\nTable 1 . Dataset\n6.1 Data Pre-processing\nWe take all bandishes in a raga for training the note-\nembeddings for the raga. Kan notes are also treated in\nthe same way as other notes in the composition, since the\nkannotes also follow the raga rules. The notes are en-\ncoded into 36 unique numbers. The notes corresponding\nto atala(rhythm) cycle is taken as a sequence. The inputsequence length is determined by taking the average length\nof the sequences in a raga dataset; zero-padding (to the left)\nand left-trimming of sequences are applied to sequences\nshorter and longer than the average length respectively. If\nthe length of a sequence is more than double the deﬁned\nsequence length, it is split into 2 separate sequences.\n7. EXPERIMENTS\n7.1 Evaluation Methods\nWe rely on 2 different evaluation methods to validate our\napproach. The ﬁrst one is based on perplexity that eval-\nuates how well a note-sequence generator model (neural-\nnetwork based, n-gram based etc.) can predict a new se-\nquence in a raga. Since note-embeddings are an integral\npart of our architecture, a low-perplexed note-sequence\ngenerator model should learn more accurate note embed-\ndings. The second method relies on clustering of ragas\nbased on different raga-similarity measures computed us-\ning our approach and baselines.\n7.1.1 Perplexity\nPerplexity for a language model [2], is computed based on\nthe probability values a learned model assigns to a vali-\ndation set [10]. For a given model, perplexity (PP) of a\nvalidation set with notes N1; N2; :::; N nis deﬁned as\nPP(N1; N2; :::; N n) =ns\n1\nP(N1; N2; :::; N n)(5)\nwhere P(N1; N2; :::; N n)is the joint probability of notes\nin the validation set. A better performing model will\nhave a lower perplexity over the validation set. For each\nraga dataset, perplexity is measured with a validation set\ntaken from the dataset. For the LSTM based methods,\nthe learned neural model provides the likelihood of a note,\nwhereas the n-gram baseline uses the learned probabilities\nfor different n-grams.\n7.1.2 Clustering\nFor this evaluation, we take 14 ragas for which similari-\nties between all the ragas and subsets of these ragas are\nknown. These similarities are determined with the help of\na professional Hindustani musician. The selected ragas are\nShuddha Kalyan, Yaman Kalyan, Yaman, Marwa, Puriya,\nSohni, Alhaiya Bilawal, Bihag, Shankara, Kaﬁ, Bageshree,\nBhimpalasi, Bhairav and Jaunpuri . The ﬁrst clustering\n(Clustering 1 ) checks if all the 14 ragas are getting\nclustered according to their thaat .Thaat wise grouping of\nthese 14 ragas are shown in Table 2. Since there are 6 dif-\nferent thaats ,kis taken as 6 for this clustering. For the\nother clusterings, different subsets of ragas are selected ac-\ncording to the similarities to be veriﬁed. Other similarities\nand the ragas chosen (from the 14 ragas) to verify that are\nas listed below\n\u000fClustering 2 :Sohni is more similar to Ya-\nman andYaman Kalyan compared to ragas in other\nthaats because they share the same characteristic518 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Thaat Ragas\nKalyan Shuddha Kalyan, Yaman Kalyan, Yaman\nMarwa Marwa, Puriya, Sohni\nBilawal Alhaiya Bilawal, Bihag, Shankara\nKaﬁ Kaﬁ, Bageshree, Bhimpalasi\nBhairav Bhairav\nAsavari Jaunpuri\nTable 2 . Thaat based grouping of the selected ragas\nphrase ( MDNS ). To verify this, Sohni, Yaman, Ya-\nman Kalyan, Kaﬁ, Bhairav are considered taking\nk=3 and we expect the ﬁrst 3 ragas to get clustered\ntogether and, Kaﬁ and Bhairav in 2 different clus-\nters.\n\u000fClustering 3 : Within Kaﬁ thaat ,Bhimpalasi\nandBageshree are more similar compared to their\nsimilarity with Kaﬁ because of the similarity in these\nragas’ characteristic phrases ( mDnS, mPnS ). To ver-\nify this, these 3 ragas are considered for clustering\ntaking k=2 and we expect Bhimpalasi andBageshree\nto get clustered together and Kaﬁ in another cluster.\n\u000fClustering 4 : Raga Jaunpuri is more similar to\nKaﬁ thaat ragas because they differ only by a note.\nTo verify this, Jaunpuri, Kaﬁ, Bageshree, Bhim-\npalasi, Bhairav, Shuddha Kalyan, Puriya, Bihag are\nconsidered taking k=5. We expect Jaunpuri to be\nclustered together with Kaﬁ, Bageshree and Bhim-\npalasi and the other ragas in 4 different clusters.\nWe apply these four clustering methods on our test dataset\nand evaluation scores pertaining to each clustering method\nis averaged to get a single evaluation score.\n7.2 Setup\nFor the experiments, we consider notes from 3 octaves,\namounting to a vocabulary size of 37 (including the null\nnote). The common hyper-parameters for the LSTM based\nmethods (our approach and one of the baselines) are kept\nthe same. The number of LSTM blocks used in the LSTM\nlayer is set to the sequence length. Each LSTM block has\n24hidden units, mapping the output to 24dimensions. For\nall our experiments, embedding dimension is empirically\nset to 36. We use tensorﬂow (version: 0.10.0) [1] for the\nLSTM implementations. Note sequences are picked from\neach raga dataset ensuring the presence of \u0018100 notes in\ntotal for the validation set. This size is made variable in\norder to accommodate variable length sequences. While\ntraining the network, the perplexity of the validation set\nis computed during each epoch and used for setting the\nearly-stopping criterion. Training stops on achieving min-\nimum perplexity and the note-embeddings at that instance\nare taken for our experiments.\nFor the clustering baseline, we employ one of the hierar-\nchical clustering methods, agglomerative clustering ( link-\nage:complete ). In our setting, a hierarchical method is\npreferred over K-means because, K-means work well only\nwith isotropic clusters [21] and it is empirically observedthat our clusters are not always isotropic. Also when exper-\nimented, the clustering scores with K-means are less com-\npared to agglomerative clustering for all the approaches.\nFor implementing the clustering methods (both agglomer-\native and k-means) we use scikit-learn toolkit [23].\n8. RESULTS\nBefore reporting our qualitative and quantitative results, to\nget a feel of how well note-embeddings capture raga simi-\nlarities, we ﬁrst visualize the 37\u000236note-embedding ma-\ntrices by plotting their heatmaps, higher intensity indicat-\ning higher magnitude of the vector component. Figure 3\nshows heatmaps of embedding matrices for three ragas viz.\nYaman Kalyan ,Yaman andPilu.Yaman Kalyan andYa-\nman are more similar to each other than Pilu. This is quite\nevident from the embedding heatmaps.\nFigure 3 . Note-embeddings visualization of (a) Yaman\nKalyan (b) Yaman (c) Pilu\nThe results of quantitative evaluation is now reported\nwith the evaluation methods described in Section 7.1. Fur-\nther, a manual evaluation is done with the help of trained\nHindustani musician considering all the 144 ragas men-\ntioned in the dataset, to better understand the distinctions\nbetween bi-LSTM and uni-LSTM. Table 3 shows perplex-\nExperiment Perplexity\nN-gram 6.39\nuni-LSTM 6.40\nbi-LSTM 2.31\nTable 3 . Results: Comparison with perplexity on valida-\ntion set (Best performance in bold)\nity values (averaged across all the ragas in the dataset)\nwith the validation set for our approach (bi-LSTM) and\nthe baseline approaches with n-gram and uni-directional\nLSTM (uni-LSTM). We can not report perplexity for the\nPCD approach as the likelihood of the notes (and hence,\nthe perplexity of the model) can not be determined with\nPCD. We observe that the perplexity values of n-gram and\nuni-LSTM are quite similar. The lower perplexity value\nwith bi-LSTM shows its capability in generating a new\nnotes sequence adhering to the raga rules. This shows\nthe performance advantage of bi-LSTM over the base-\nlines on note-sequence generation task, thereby provid-\ning indications on the goodness of the note-embeddings\nlearned. Moreover, the bi-LSTM model, having the lowest\nperplexity, is able to capture the semantic association be-\ntween notes more accurately, yielding more accurate note-\nembeddings.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 519Experiment Homogeneity Completeness V-measure\nN-gram 0.3973 0.4036 0.4004\nPCD 0.6430 0.6488 0.6451\nuni-LSTM 0.7828 0.7858 0.7843\nbi-LSTM 0.9008 0.9069 0.9038\nTable 4 . Results: Comparison of clustering results with\ndifferent clustering metrics (Best performance in bold)\nTable 4 shows the results of clustering using a stan-\ndard set of metrics for clustering, viz.homogeneity, com-\npleteness and V-measure [26]. The clustering scores with\nn-gram and PCD baselines show their inability towards\nidentifying the known similarities between the ragas. The\nbi-LSTM approach performs better compared to the base-\nlines; the performance of uni-LSTM baseline is compara-\nble with bi-LSTM approach. On analyzing each individual\nclustering, we observed,\n\u000fN-gram approach does not do well for all the indi-\nvidual clusterings, resulting in poor clustering scores\ncompared to other approaches. A relatively better\nperformance is observed only with Clustering\n4.\n\u000fPCD has better scores compared to n-gram\nas it out-performs n-gram with a huge mar-\ngin in Clustering 1 . PCD’s performance in\nClustering 1 is superior to the LSTM ap-\nproaches as well. However, its performance is quite\ninferior to that of other approaches in the other three\nclustering settings. PCD’s ability in modeling notes\ndistribution efﬁciently helps in thaat based cluster-\ning (Clustering 1 ), because thaat based clas-\nsiﬁcation quite depends on the distribution of tonal\nmaterial.\n\u000funi-LSTM performance is better than bi-LSTM in\nClustering 1 where the ragas are supposed to\nbe clustered according to the thaat . But it fails\nto cluster Sohni ,Yaman and Yaman Kalyan in\nthe same cluster, leading to poor performance in\nClustering 2\n\u000fEven though bi-LSTM gives slightly lower scores\nwithClustering 1 , it does perfect clustering for\nthe other three clustering schemes. This gives an in-\ndication on the capability of bi-LSTM approach for\nidentifying melodic similarities beyond thaat .\nOverall, these observations show the practicality of both\nthe LSTM based methods to learn note-embeddings with\nthe aim of identifying raga similarity.\nFigures 4 show Multi-Dimensional Scaling (MDS)\n[6] visualizations showing the similarity between note-\nembeddings of the selected 14 ragas (same color speci-\nﬁes same thaat ) with bi-LSTM approach. These visual-\nizations give an overall idea on how well the similarities\nare captured. The ﬁner similarities observed in the clus-\ntering evaluations are not clearly perceivable from these\nvisualizations.\nFigure 4 . MDS visualization of bi-LSTM note-\nembeddings similarities\nWe have also carried out separate experiments by in-\ncluding note duration information along with the notes by\npre-processing the data, but the performance is worse com-\npared to the reported results. Chordia [9] has also reported\nthat weighting by duration had no impact on their raga\nrecognition task.\nTo conﬁrm the validity of our approach, one expert\nmusician checked the MDS visualizations of similarities\nbetween all 144 ragas with bi-LSTM and uni-LSTM ap-\nproaches1. The musician identiﬁed clusters of similar ra-\ngas in both the visualizations matching with his musical\nnotion. A few observations made are: Asavari thaat ragas\nappear to be closer to each other with bi-LSTM compared\nto uni-LSTM. Also Miyan ki todi, Multani, Gujari Todi\nwhich are very similar ragas are found closer in bi-LSTM.\nBut the same thaat ragas Marwa, Puriya and Sohni are\nfound to be more similar to each other with uni-LSTM.\n9. CONCLUSION AND FUTURE WORK\nThis paper investigated on the effectiveness of note-\nembeddings for unveiling the raga similarities and on\nmethods to learn note-embeddings. The perplexity\nbased evaluation shows the superior performance of bi-\ndirectional LSTM method over unidirectional-LSTM and\nother baselines. The clustering based evaluation also con-\nﬁrms this, but it also shows that the performance of unidi-\nrectional approach is comparable to the bi-directional ap-\nproach for certain cases.\nThe utility of our approach is not conﬁned only to raga\nsimilarity; it can also be extended to verify if a given ban-\ndish complies with the raga rules. This immensely ben-\neﬁts to Hindustani music pedagogy; for instance, it helps\nto select the right bandish for a learner. In future, for bet-\nter learning of note-embeddings, we plan to design a net-\nwork to handle duration information effectively. The cur-\nrent experiments take one line in the bandish as a sequence.\nWe plan to experiment with more meaningful segmentation\nschemes like lyrical phrase delimited by a long pause.\n1The note-embeddings of all 144 ragas are available for\ndownload from https://github.com/joecheriross/\nraga-note-embeddings520 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 201710. ACKNOWLEDGMENTS\nWe would like to thank Swarganga.org and its founder\nAdwait Joshi for letting us to use the rich bandish\ndataset for research. We also thank Anoop Kunchukut-\ntan, Arun Iyer and Aditya Joshi for their valuable sugges-\ntions. This work received partial funding from the Euro-\npean Research Council under the European Unions Sev-\nenth Framework Programme (FP7/2007-2013)/ERC grant\nagreement 267583 (CompMusic).\n11. REFERENCES\n[1] Mart ´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\net al. Tensorﬂow: A system for large-scale machine\nlearning. In Proceedings of the 12th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion (OSDI). Savannah, Georgia, USA , 2016.\n[2] Lalit R Bahl, Frederick Jelinek, and Robert L Mercer.\nA maximum likelihood approach to continuous speech\nrecognition. IEEE transactions on pattern analysis and\nmachine intelligence , pages 179–190, 1983.\n[3] Shreyas Belle, Rushikesh Joshi, and Preeti Rao. Raga\nidentiﬁcation by using swara intonation. Journal of ITC\nSangeet Research Academy , 23, 2009.\n[4] Ashwin Bellur, Vignesh Ishwar, and Hema A Murthy.\nMotivic analysis and its relevance to raga identiﬁcation\nin carnatic music. In Proceedings of the 2nd Comp-\nMusic Workshop; 2012 Jul 12-13; Istanbul, Turkey.\nBarcelona: Universitat Pompeu Fabra; 2012. p. 153-\n157. Universitat Pompeu Fabra, 2012.\n[5] Arindam Bhattacharjee and Narayanan Srinivasan.\nHindustani raga representation and identiﬁcation: a\ntransition probability based approach. International\nJournal of Mind, Brain and Cognition , 2(1-2):66–91,\n2011.\n[6] I Borg and P Groenen. Modern multidimensional scal-\ning: theory and applications. Journal of Educational\nMeasurement , 40(3):277–280, 2003.\n[7] William B Cavnar and John M Trenkle. N-gram-based\ntext categorization. Ann Arbor MI , 48113(2):161–175,\n1994.\n[8] Soubhik Chakraborty, Guerino Mazzola, Swarima\nTewari, and Moujhuri Patra. Computational Musicol-\nogy in Hindustani Music . Springer, 2014.\n[9] Parag Chordia. Automatic raag classiﬁcation of pitch-\ntracked performances using pitch-class and pitch-class\ndyad distributions. In Proceedings of the International\nComputer Music Conference , 2006.\n[10] Philip Clarkson and Tony Robinson. Improved lan-\nguage modelling through better language model eval-\nuation measures. Computer Speech & Language ,\n15(1):39–53, 2001.[11] Pranay Dighe, Parul Agrawal, Harish Karnick, Sid-\ndartha Thota, and Bhiksha Raj. Scale independent\nraga identiﬁcation using chromagram patterns and\nswara based features. In IEEE International Confer-\nence on Multimedia and Expo Workshops (ICMEW)\n2013 , pages 1–4. IEEE, 2013.\n[12] Pranay Dighe, Harish Karnick, and Bhiksha Raj. Swara\nhistogram based structural analysis and identiﬁcation\nof indian classical ragas. In The 14th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 35–40, 2013.\n[13] Douglas Eck and Juergen Schmidhuber. A ﬁrst look\nat music composition using lstm recurrent neural net-\nworks. Istituto Dalle Molle Di Studi Sull Intelligenza\nArtiﬁciale , 103, 2002.\n[14] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\n[15] Allen Huang and Raymond Wu. Deep learning for mu-\nsic.arXiv preprint arXiv:1606.04930 , 2016.\n[16] Adwait Joshi. swarganga.org, 2004.\n[17] Manfred Junius, Alain Dani ´elou, Ernst Waldschmidt,\nRose Waldschmidt, and Walter Kaufmann. The ragas\nof northern indian music, 1969.\n[18] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781 , 2013.\n[19] Tomas Mikolov, Stefan Kombrink, Luk ´aˇs Burget, Jan\nˇCernock `y, and Sanjeev Khudanpur. Extensions of re-\ncurrent neural network language model. In Proceed-\nings of 2011 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n5528–5531. IEEE, 2011.\n[20] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\nLinguistic regularities in continuous space word rep-\nresentations. In Proceedings of the 12th annual confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics , volume 13, pages 746–\n751, 2013.\n[21] George Nagy. State of the art in pattern recognition.\nProceedings of the IEEE , 56(5):836–863, 1968.\n[22] Gaurav Pandey, Chaitanya Mishra, and Paul Ipe.\nTansen: A system for automatic raga identiﬁcation. In\nProceedings of the 1st Indian International Conference\non Artiﬁcial Intelligence , pages 1350–1363, 2003.\n[23] Fabian Pedregosa, Ga ¨el Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, et al. Scikit-learn: Machine learning\nin python. Journal of Machine Learning Research ,\n12(Oct):2825–2830, 2011.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 521[24] HG Ranjani, S Arthi, and TV Sreenivas. Carnatic mu-\nsic analysis: Shadja, swara identiﬁcation and raga ver-\niﬁcation in alapana using stochastic models. In 2011\nIEEE Workshop on Applications of Signal Processing\nto Audio and Acoustics (WASPAA) , pages 29–32. IEEE,\n2011.\n[25] Suvarnalata Rao and Preeti Rao. An overview of hin-\ndustani music in the context of computational musi-\ncology. Journal of New Music Research , 43(1):24–33,\n2014.\n[26] Andrew Rosenberg and Julia Hirschberg. V-measure:\nA conditional entropy-based external cluster evaluation\nmeasure. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing-CoNLL ,\nvolume 7, pages 410–420, 2007.\n[27] Surendra Shetty, KK Achary, and Sarika Hegde. Clus-\ntering of ragas based on jump sequence for automatic\nraga identiﬁcation. In Wireless Networks and Compu-\ntational Intelligence , pages 318–328. Springer, 2012.\n[28] Rajeswari Sridhar, Manasa Subramanian, BM La-\nvanya, B Malinidevi, and TV Geetha. Latent dirichlet\nallocation model for raga identiﬁcation of carnatic mu-\nsic.Journal of Computer Science , 7(11):1711, 2011.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Se-\nquence to sequence learning with neural networks. In\nAdvances in neural information processing systems ,\npages 3104–3112, 2014.\n[30] T Viswanathan and Matthew Harp Allen. Music in\nsouth india, 2004.522 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "An Analysis/Synthesis Framework for Automatic F0 Annotation of Multitrack Datasets.",
        "author": [
            "Justin Salamon",
            "Rachel M. Bittner",
            "Jordi Bonada",
            "Juan J. Bosch",
            "Emilia Gómez",
            "Juan Pablo Bello"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1481170",
        "url": "https://doi.org/10.5281/zenodo.1481170",
        "ee": "https://ismir2017.smcnus.org/wp-content/uploads/2017/10/164_Paper.pdf",
        "abstract": "MDB-mf0-synth\n=============\n\nMDB-mf0-synth (c) by Justin Salamon, Rachel Bittner, Jordi Bonada, Juan Jose Bosch, Emilia Gmez and Juan Pablo Bello.\nMDB-mf0-synth is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0).\nYou should have received a copy of the license along with this work. If not, see http://creativecommons.org/licenses/by-nc/4.0/\n\n\nCreated By\n----------\n\nJustin Salamon*, Rachel Bittner*, Jordi Bonada^, Juan Jose Bosch^, Emilia Gmez^ and Juan Pablo Bello*.\n* Music and Audio Research Lab (MARL), New York University, USA\n^ Music Technology Group, Universitat Pompeu Fabra, Spain\nhttp://synthdatasets.weebly.com/\nhttp://steinhardt.nyu.edu/marl/\nhttps://www.upf.edu/web/mtg\n\nVersion 1.0.0\n\n\nDescription\n-----------\n\nMDB-mf0-synth contains 85 songs from the MedleyDB dataset (http://medleydb.weebly.com/) in which polyphonic pitched\ninstruments (such as piano and guitar) have been removed and all monophonic pitched instruments (such as bass and voice)\nhave been resynthesized to obtain perfect f0 annotations using the analysis/synthesis method described in the following\npublication:\n\nJ. Salamon, R. M. Bittner, J. Bonada, J. J. Bosch, E. Gmez, and J. P. Bello. An analysis/synthesis framework for\nautomatic f0 annotation of multitrack datasets. In 18th Int. Soc. for Music Info. Retrieval Conf., Suzhou, China,\nOct. 2017.\n\nThis dataset includes:\n* 85 stereo wav files of song mixes where:\n * polyphonic pitched instruments (such as piano and guitar) have been removed\n * all monophonic pitched instruments (such as bass and voice) have been resynthesized using the analysis/synthesis\n  method described in the paper\n* 85 csv files containing a perfect multiple-f0 annotation of all the (monophonic) pitched instruments in the mix,\n obtained via the analysis/synthesis method described in the paper\n\nThe data come in two folders, the contents of which is described below.\n\n\naudio_mix\n---------\nContains 85 stereo wav files of song mixes in which polyphonic pitched instruments (such as piano and guitar) have been\nremoved and all monophonic pitched instruments (such as bass and voice) have been resynthesized using the\nanalysis/synthesis method described in the paper. Non-pitched tracks (percussion) are kept unchanged (i.e. the\noriginal stems are used). All the stems (tracks) are automatically mixed together as described in the paper.\n\nNaming convention:\nartist_songtitle_MIX_mf0synth.wav\n\nExample:\nAClassicEducation_NightOwl_MIX_mf0synth.wav\n\n\nannotation_mf0\n--------------\nContains 85 csv files containing a perfect multiple-f0 annotation of all pitched stems (tracks) in the mix, obtained\nvia the analysis/synthesis method described in the paper.\n\nFormat:\nThe annotations follow the MIREX multiple-f0 estimation (frame-basis) format:\nhttps://www.music-ir.org/mirex/wiki/2018:Multiple_Fundamental_Frequency_Estimation_%26_Tracking#I.2FO_format\nThis format is also support by mir_eval: https://github.com/craffel/mir_eval\n\nEach row in the annotation starts with a timestamp, followed by 0 or more tab separated frequency values in Hz\nrepresenting the f0 of each active pitched instrument present in the time frame represented by the row. The first\nframe in the annotation is zero-centered. The hop size of the annotation is exactly 10 ms.\n\nIMPORTANT: no assumptions can be made as to the ordering of the f0 values in each row. The frequency values are NOT\nordered neither by instrument nor by frequency, and should thus be treated as a bag of frequencies (a set) without\nany assumptions as to which frequency belongs to which instrument.\n\nNaming convention:\nartist_songtitle_MIX_mf0synth.csv\n\nExample:\nAClassicEducation_NightOwl_MIX_mf0synth.csv\n\n\nPlease Acknowledge MDB-mf0-synth in Academic Research\n-----------------------------------------------------\n\nPlease cite the following publication when using MDB-mf0-synth:\n\nJ. Salamon, R. M. Bittner, J. Bonada, J. J. Bosch, E. Gmez, and J. P. Bello. An analysis/synthesis framework for\nautomatic f0 annotation of multitrack datasets. In 18th Int. Soc. for Music Info. Retrieval Conf., Suzhou, China,\nOct. 2017.\n\nFor information about the original MedleyDB dataset please see (and cite):\n\nR. M. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam, and J. P. Bello. MedleyDB: A multitrack dataset for\nannotation-intensive MIR research. In 15th Int. Soc. for Music Info. Retrieval Conf., pages 155160, Taipei, Taiwan,\nOct. 2014.\n\n\nConditions of Use\n-----------------\n\nDataset created by Justin Salamon, Rachel Bittner, Jordi Bonada, Juan Jose Bosch, Emilia Gmez and Juan Pablo Bello.\n\nThe MDB-mf0-synth dataset is offered free of charge under the terms of the Creative Commons\nAttribution-NonCommercial 4.0 International License (CC BY-NC 4.0): http://creativecommons.org/licenses/by-nc/4.0/\n\nThe dataset and its contents are made available on an as is basis and without warranties of any kind, including\nwithout limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or\ncompleteness, or absence of errors. Subject to any liability that may not be excluded or limited by law, NYU is not\nliable for, and expressly excludes, all liability for loss or damage however and whenever caused to anyone by any use of\nthe MDB-mf0-synth dataset or any part of it.\n\n\nFeedback\n--------\n\nPlease help us improve MDB-mf0-synth by sending your feedback to: justin.salamon@gmail.com\nIn case of a problem report please include as many details as possible.\n",
        "zenodo_id": 1481170,
        "dblp_key": "conf/ismir/SalamonBBBGB17",
        "keywords": [
            "polyphonic pitched instruments",
            "monophonic pitched instruments",
            "perfect f0 annotations",
            "analysis/synthesis method",
            "MedleyDB dataset",
            "85 songs",
            "85 csv files",
            "audio mixes",
            "85 stereo wav files",
            "multiple-f0 annotation"
        ]
    },
    {
        "title": "PiPo, a Plugin Interface for Afferent Data Stream Processing Operators.",
        "author": [
            "Norbert Schnell",
            "Diemo Schwarz",
            "Joseph Larralde",
            "Riccardo Borghesi"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416912",
        "url": "https://doi.org/10.5281/zenodo.1416912",
        "ee": "https://zenodo.org/records/1416912/files/SchnellSLB17.pdf",
        "abstract": "We present PiPo, a plugin API for data stream processing with applications in interactive audio processing and music information retrieval as well as potentially other domains of signal processing. The development of the API has been motivated by our recurrent need to use a set of signal pro- cessing modules that extract low-level descriptors from au- dio and motion data streams in the context of different au- thoring environments and end-user applications. The API is designed to facilitate both, the develop- ment of modules and the integration of modules or module graphs into applications. It formalizes the processing of streams of multidimensional data frames which may rep- resent regularly sampled signals as well as time-tagged events or numeric annotations. As we found it sufficient for the processing of incoming (i.e. afferent) data streams, PiPo modules have a single input and output and can be connected to sequential and parallel processing paths. Af- ter laying out the context and motivations, we present the concept and implementation of the PiPo API with a set of modules that allow for extracting low-level descriptors from audio streams. In addition, we describe the integra- tion of the API into host environments such as Max, Juce, and OpenFrameworks.",
        "zenodo_id": 1416912,
        "dblp_key": "conf/ismir/SchnellSLB17",
        "keywords": [
            "data stream processing",
            "interactive audio processing",
            "music information retrieval",
            "signal processing",
            "plugin API",
            "multidimensional data frames",
            "audio and motion data streams",
            "low-level descriptors",
            "sequential and parallel processing paths",
            "host environments"
        ],
        "content": "PIPO, A PLUGIN INTERFACE FOR AFFERENT DATA STREAM\nPROCESSING MODULES\nNorbert Schnell\nUMR STMS\nIRCAM-CNRS-UPMC\nschnell@ircam.frDiemo Schwarz\nUMR STMS\nIRCAM-CNRS-UPMC\nschwarz@ircam.frJoseph Larralde\nUMR STMS\nIRCAM-CNRS-UPMC\nlarralde@ircam.frRiccardo Borghesi\nUMR STMS\nIRCAM-CNRS-UPMC\nborghesi@ircam.fr\nABSTRACT\nWe present PiPo , a plugin API for data stream processing\nwith applications in interactive audio processing and music\ninformation retrieval as well as potentially other domains\nof signal processing. The development of the API has been\nmotivated by our recurrent need to use a set of signal pro-\ncessing modules that extract low-level descriptors from au-\ndio and motion data streams in the context of different au-\nthoring environments and end-user applications.\nThe API is designed to facilitate both, the develop-\nment of modules and the integration of modules or module\ngraphs into applications. It formalizes the processing of\nstreams of multidimensional data frames which may rep-\nresent regularly sampled signals as well as time-tagged\nevents or numeric annotations. As we found it sufﬁcient\nfor the processing of incoming (i.e. afferent ) data streams,\nPiPo modules have a single input and output and can be\nconnected to sequential and parallel processing paths. Af-\nter laying out the context and motivations, we present the\nconcept and implementation of the PiPo API with a set\nof modules that allow for extracting low-level descriptors\nfrom audio streams. In addition, we describe the integra-\ntion of the API into host environments such as Max, Juce,\nand OpenFrameworks.\n1. INTRODUCTION\n1.1 Context and Motivation\nMany of the interactive audio applications that we have de-\nveloped over the past years in collaboration with artists and\nother researchers rely on signal processing techniques to\nautomatically analyse and annotate audio and motion sen-\nsor streams. We often refer to the techniques we deploy in\nthis context as content-based audio processing [1]. These\ntechniques generally allows for interactively transforming\nrecorded audio materials as a function of low-level audio\ndescriptions such as pitch, intensity, and timbre descrip-\ntions as well as segmentations into temporal units such as\nc\rNorbert Schnell, Diemo Schwarz, Joseph Larralde, Ric-\ncardo Borghesi. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Norbert Schnell, Diemo\nSchwarz, Joseph Larralde, Riccardo Borghesi. “ PiPo, A Plugin Interface\nfor Afferent Data Stream Processing Modules ”, 18th International Soci-\nety for Music Information Retrieval Conference, Suzhou, China, 2017.\nmodelling,generators,synthesis,etc.\nextraction,ﬁltering,segmentation,etc.audio or sensor data streams\nmixing,spatialization,effects,etc.audio or controldata streamsafferent stream processingefferent stream processing\ninstrument pluginsaudio analysis pluginsinteraction andsound generationsensors,recording,ﬁle storage,ﬁle storage,rendering,actuators,effect plugins(e.g. VST, AudioUnits, LADSPA)(e.g. FEAPI, VAMP)existing audio solutions……Figure 1 . An interactive audio system with afferent and\nefferent data streams. The labels at the bottom cite existing\nplugin interfaces for audio applications.\nnotes, syllables, and musical phrases. Similar processing\napplies in this context to motion capture streams to extract\nmovement qualities and meaningful events and temporal\nunits such as onsets and gestures. In this processing, we\nfrequently reuse a set of algorithms such as ﬁlters, pro-\njections, extractors, and detectors that may apply in real-\ntime to incoming data streams or, ofﬂine, to data streams\nrecorded into ﬁles. Since these data streams occur in the\noverall interactive systems we develop as inputs, we refer\nto them as afferent data streams.\nFigure 1 shows the overall structure of such an inter-\nactive system. The schema does not distinguish whether\nthe audio and motion data streams actually enter the sys-\ntem in real-time or whether they are read from ﬁles. Af-\nferent streams processed in real-time typically originate\nfrom a microphone or motion sensors. In some of the sys-\ntems we have developed, the streams are used to control\nan interactive system through sound (e.g. voice) or move-\nment. In many of these systems, the same — or very sim-\nilar — processing that is applied in real-time, applies to\ndata streams read from ﬁles. For example for concatenative\nsynthesis [21], audio descriptors are extracted from pre-\nrecorded materials. While some interactive systems may\ngenerate sound in real-time, the generated description and\nannotations may be used to create visualizations (e.g. in\nthe context of musicology or education) as well as to sup-\nport the editing and transformation of recordings in post-\nproduction systems.\nIn general, the processing of afferent streams can be\ndescribed as reducing the data streams in terms of their\ncomplexity, dimensionality, and data rate. Typical terms\nused to characterize this processing include ﬁltering ,anal-361ysis,extraction ,description ,detection ,recognition ,scal-\ning, and mapping .\nThe PiPo API ( Plug-in Interface for Processing Ob-\njects) formalizes modules that in this sense transform an\nincoming data stream into an output data stream allowing\nfor a possibly wide range of streams as well as for mod-\nules of arbitrary complexity going from simple scalings to\nsophisticated machine learning algorithms.\nThe major motivations for developing the PiPo API can\nbe summarized as follows:\n\u000ffacilitating the integration of algorithms of different\norigins (i.e. developers) into a given application\n\u000ffacilitating the use/comparison of different algo-\nrithms of similar functionalities in a given context\n(e.g. applying different ﬁlters, extractors or classi-\nﬁers to the same input stream)\n\u000ffacilitating the integration of a given algorithm into\ndifferent contexts and applications\n\u000ffacilitating the development of applications where\nthe same algorithm applies to data streams in real-\ntime and ofﬂine\nUltimately, the motivation for developing the API is\nthe idea of enabling the development of an ecosystem of\nstream modules and host environments in particular do-\nmains as well as across different domains of signal pro-\ncessing.\n1.2 Requirements\nIn this section, we give an overview over the most impor-\ntant general requirements for an afferent data stream pro-\ncessing framework for real-time and off-line use. These re-\nquirements concern speciﬁc functionalities as well as their\nefﬁcient implementation in a real-time system (see [22]).\n1.2.1 Functional Requirements\nScheduling Processing should run either in batch on\nsound ﬁles and buffers, or on a live audio stream\nSegmentation Allow several streams of segmentations in\nparallel and overlapping segments, or an implicit seg-\nmentation, where segments are analysis frames, ele-\nmentary waveforms, or whole sound ﬁles.\nTemporal Modeling Any number of temporal modeling\nalgorithms can be integrated, either universal (modeling\nall descriptors, e.g. mean) or speciﬁc (modeling speciﬁc\ndescriptors only, e.g. geometric mean for pitch).\nData Type Data can be numeric scalars, vectors, matrices,\nor strings\nMulti-Modality The input data type and rate should allow\nmotion and other data and not be limited to audio only.\nUser Composability Modules should be composable by\nthe user in the host environment (without having to\nwrite and compile code), e.g. chaining feature ex-\ntractors, smoothing ﬁlters, segmentation, and temporalmodeling, in order to allow experimentation and rapid\nprototyping.\n1.2.2 Implementation Requirements\nEasy Integration and Efﬁciency It should be easy to in-\ntegrate the framework in any platform and environment,\nincluding real-time and resource-constrained systems\n(e.g. single-board computers). This basically stipulates\nthat the API be written in C or C++.\nDynamic Plugin Loading It should be possible to add\nprocessing modules as plugins to an existing host in-\nstallation, e.g. by leveraging dynamic linking of shared\nlibraries.\nEfﬁcient Modularisation The framework should allow\nan efﬁcient implementation, notably by sharing com-\nmonly used calculation results , most of all the FFT rep-\nresentation, between modules, by avoiding copying and\nre-sending data, instead writing them directly to its des-\ntination.\nExternal Data External data streams and sources of seg-\nmentation, such as a human tapping on attacks oder ex-\nisting analysis ﬁles, must be integratable into the data\nﬂow.\nReanalysis A subset of descriptors or only the segmenta-\ntion and subsequent temporal modeling can be re-run\nwith changed parameters.\nAlmost all of these requirements are fulﬁlled by PiPo ,\nwith the exception of the possibility to pass strings as data\nelements. This has been avoided to simplify the API and\navoid problems of memory-handling. A ﬁxed set of strings\n(such as class labels for machine-learning) can always be\ntransmitted by their index.\nThe top-level requirements, that best distinguish PiPo\nfrom other frameworks are dynamic linking of plugins,\nmulti-modality, and user-composability of modules.\n1.3 Related Work\nIn the rich existing work, we must distinguish audio anal-\nysis libraries and toolboxes (see the recent overview [15])\nfrom plugin APIs which impliy a formalization of the in-\nput/output formats and the dynamic loading of modules.\nSeveral plugin APIs are commonly used in the\nworld of audio signal processing and virtual instruments,\nnamely LADSPA (Linux Audio Developer’s Simple Plu-\ngin API),1VST (Virtual Studio Technology by Stein-\nberg),2and AU (Audio Units by Apple).3These APIs\nare mainly designed for transforming an input audio stream\n(effect processing) or for generating an audio stream in\nreaction to incoming MIDI events (virtual instruments).\nThus they are not applicable to the demands of general data\nprocessing or audio feature extraction.\nMany monolithic or collections of analysis mod-\nules for popular real-time environments exist, such as\n1http://www.ladpsa.org\n2http://ygrabit.steinberg.de\n3http://developer.apple.com/audio/audiounits.html362 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017input data streamoutput data streamPiPo hostframe sinkframe sourceerror reporting\nPiPooperatorA\nPiPooperatorB\nPiPooperatorCFigure 2 . A chain of modules in a host environment.\nanalyser˜ [11], the patch-based ZSA [13] for M AX,\nimtr-analysis [22], TimbreID [3] for PureData. None of\nthese can be integrated into other environments.\nThe ﬁrst descriptor analysis frameworks that would al-\nlow the dynamic inclusion of external modules are either\nplugin frameworks such as the sadly defunct FEAPI [12],\nand the more lively V AMP [6].4However, the latter does\nnot propose user-composability nor multi-modality (the in-\nput is always audio).\nThere are many existing libraries for audio descrip-\ntor analysis ( Yaafe5[14], Essentia6[2],OpenSmile7[7,\n8],libXtract ,8IrcamDescriptor [18]) see this compari-\nson [15]. None of them allow for dynamic linking, easy\nintegration of new algorithms, or user composability with-\nout having to code a new module.\nThe M ARSYAS framework, dedicated to music informa-\ntion retrieval, is concerned with scheduling [5] as well as\nCLAM,9but neither is a common environment for real-\ntime sound and music applications.\nIn summary, no existing API combines all three top-\nlevel requirements of dynamic linking of plugins, multi-\nmodality, and user-composability of modules.\n2. CONCEPTS AND FORMALIZATION\nThe PiPo API formalizes modules as objects that receive\na data stream as a succession of frames at their input and\nsend a stream as output. As shown in ﬁgure 2, modules\ncan be connected to a chain by connecting the input of one\nmodule to the output of another. In the simplest case, the\nprocessing requires a single module. A PiPo host , con-\nstructs the modules and connects to the input of the ﬁrst\nmodule of the chain as the source of the stream of frames\nto be processed. In addition, the host connects to the out-\nput of the last module of the chain as the terminating sink\nthat receives the resulting stream.\nThe data streams received and produced by PiPo mod-\nules are described by a set of stream attributes that are de-\nﬁned before the modules actually receive and produce any\nframes. This way, the initialization of a module may de-\npend on the attributes of the incoming stream and the mod-\nule may determine the attributes of the stream it produces\nas a function of the attributes of the incoming stream.\n4http://vamp-plugins.org\n5http://yaafe.sourceforge.net\n6http://essentia.upf.edu/\n7http://opensmile.sourceforge.net/\n8http://jamiebullock.github.io/LibXtract/documentation/\n9http://clam-project.orgThe propagation of the stream parameters and the actual\nprocessing of frames are separated into two phases that are\nboth initiated by the host through its connection to the ﬁrst\nmodule. In both phases, each module receives information\nfrom its predecessor in the chain and sends information to\nits successor. In the initialization phase, the host sends out\nthe stream parameters of the input stream to the ﬁrst mod-\nule which sends its output stream parameters to the input of\nthe next module, and so forth, until the last module sends\nthe resulting stream parameters back to the host connected\nto its outlet. Similarly, once the modules are initialized,\nthe host can start sending frames into the input of the ﬁrst\nmodule and receives the resulting frames from the output\nof the last module. Only in the case of error, as for example\nwhen a module cannot accept a stream with a given set of\nattribute values at its input, a module would report directly\nto the host, which in turn can output the error message to\nthe host environment.\n2.1 Streams of Frames\nEach frame of a data stream is composed of a time-tag and\na two-dimensional matrix of numeric values. A data stream\nis described by the following set of attributes:\n\u000fframe rate of the stream\n\u000fwhether the frames of the stream are time-tagged\n\u000fdimensions of the frames’ two-dimensional matrix\n\u000flabels describing the columns of the data matrix\n\u000fwhether the frames’ data matrices have a variable\nnumber of rows\nIn case of streams of time-tagged frames of an irregular\nrate, the frame rate attribute should announce the worst\ncase (the fastest) rate, so that succeeding modules — or\nthe host — can take this parameter into account (e.g. for\nallocating memory).\nThis formalization of data streams allows for represent-\ning a large spectrum of different signals, event streams, and\nnumeric annotations. For example:\n\u000fmono or multi-channel audio signals are represented\nas scalars or multi-dimensional vectors of a constant\nframe rate\n\u000freal or complex frames of spectral data are repre-\nsented as single column vectors or matrices of two\ncolumns (i.e. labeled ’real’ and’imag’ ), usually of a\nconstant frame rate\n\u000fmulti-dimensional motion capture data streams are\nrepresented as multi-dimensional row-vectors (e.g.\nlabeled ’x’,’y’,’z’) that may be time-tagged or of a\nconstant frame rate\n\u000fonset markers are represented as time-tagged frames\nwithout numeric data (i.e. an empty matrix)\n\u000fsegments are represented as time-tagged frames with\na row-vector of data including a ’duration’ column\nand, optionally, multiple columns of values describ-\ning the segment (e.g. ’pitch’ ,’intensity’ ,’category’ )Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 363\u000fharmonics are represented as two-dimensional ma-\ntrices with variable number of rows, one row for\neach harmonic, with multiple columns ( ’frequency’ ,\n’amplitude’ ,’phase’ ) of a constant frame rate\nFrom the host’s point of view, once constructed, an ar-\nbitrary chain of modules is deﬁned by the stream it pro-\nduces as a function of the input stream provided by the\nhost. Before starting the actual stream processing by send-\ning frames into the chain, the host retrieves the attributes\nof the output stream that can be used, for example, to al-\nlocate memory or bandwidth and automatically determine\ndisplay options as well as to conﬁgure and generate other\ninteractions with connected sub-systems or users.\n2.2 Chains of Modules\nAs mentioned above, PiPo modules have a single input and\noutput and can be connected to chains. Hereby, a chain of\nmodules — conceptually as well as by implementation —\nmay appear as a single module communicating with its en-\nvironment (i.e. a host or connected modules) through a\nsingle in- and output and an error channel.\nApart from the stream attributes of its incoming stream,\neach module is conﬁgured and controlled by a set of typed\nmodule parameters that are explicitly declared through the\nPiPo API. Possible parameter types are single values of 64-\nbit ﬂoat, 32-bit integer, string, and declared enumerated\ntypes as well as ﬁxed or variable sized arrays of values\nof these types and heterogeneous variable sized arrays. In\naddition to its type, a module parameter is declared with a\nname, a short description, and a ﬂag whether changing a\ngiven module parameter requires the reinitialization of the\nmodule — and consequently of the following modules in a\nchain.\nAn important feature of the design of the API is that\nit allows for implementing modules of virtually any com-\nplexity and for composing chains of modules of any\ngranularity. An extractor of MEL cepstrum coefﬁcients\n(MFCCs), for example, may be implemented as a single\nmonolithic module or composed of a chain of modules that\ninclude the successive calculation of STFT frames, MEL\ncoefﬁcients, and DFT coefﬁcients.\n2.3 Graphs Beyond Chains\nThe construction of certain algorithms from basic modules\nrequires more complex graphs of modules. For example,\nthe extraction of a set of basic audio descriptors shown\nin ﬁgure 3 requires to split and merge the processing of\nthe implied data streams. While the ﬁrst split allows for\nprocessing the same audio frames in time and frequency\ndomain, the second applies the calculation of a loudness\ndescriptor and a spectral centroid to the same frequency\ndomain frames produced by the STFT. The ﬁnal set of esti-\nmated descriptor values (i.e. pitch, periodicity, AC1, loud-\nness, and spectral moments) is merged to a single vector at\nthe output of the sub-graph.\nAs described in section 2.2, any chain (or sequence) of\nmodules can be considered as a single module. In the for-\nPiPo descr\nslice\nscale\nsum\nmoments\nyinf0, periodicity, ac0, ac1\nfftpower spectrumspectral momentsloudnesswindowed framesf0, periodicity, ac0, ac1, loudness, spectral momentsaudio samplesFigure 3 . A complex graph of PiPo modules for calculat-\ning 9 basic audio descriptors.\nsequence modulemodule 1module 2module n. . .\nparallel modulemodule 1module 2\nmodule n...\nFigure 4 . Any number of modules connected in sequence\nor in parallel can be reduced to a single module.\nmalization of graphs in the PiPo API, parallel modules can\nalso be reduced to a single module. These two rules, il-\nlustrated by ﬁgure 4, provide a consistent basis to build a\nlarge variety of complex PiPo graphs.\nFigure 5 shows the structure of the pipo.descr mod-\nule expressed in terms of sequence and parallel compo-\nnents.\n2.4 Hosts\nIn summary, a PiPo host has to provide the following func-\ntionalities:\n\u000fconstructing a single or a graph of modules\n\u000fparametrizing the modules\nscalesumfftyin\nmo-mentsslicesequenceparallel\nFigure 5 . Decomposition of the pipo.descr module\ninto sequence and parallel elements.364 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017\u000fconnecting a terminating sink to the output of the\nchain\n\u000facquiring the input stream\n\u000finitializing the modules by sending the input stream\nattributes into the chain\n\u000fhandling initialization errors emitted by the modules\n\u000fsending the frames of the input stream into the chain\nand handling the frames of the output stream\n\u000fallowing for real-time parametrization of the mod-\nules (if applicable)\nThePiPo API includes abstractions that support the im-\nplementation of hosts.\n3. IMPLEMENTATION\nPiPo is an API that essentially consists of a single C++\nheader ﬁle. This ﬁle deﬁnes the base PiPo class, and its\ndeclared parameters.10.\n3.1 The PiPo API\nThe minimal module must inherit from the class PiPo\nand implement at least the streamAttributes and\nframes methods:\nInstreamAttributes , all initialisation can be\ndone, as all input stream attributes are known. The\noutput stream attributes are passed on to the receiv-\ning module via propagateStreamAttributes . In\nframes , only data processing and, when needed, buffer-\ning should be done. Output frames are passed on with\npropagateFrames .\nIf the module can produce additional output data af-\nter the end of the input data (e.g. ﬁlters), it must im-\nplement finalize , from within which more calls to\npropagateFrames can be made, followed by a manda-\ntory call to propagateFinalize .\nIf the module keeps internal state or buffering, it should\nimplement the reset method to put itself into a clean\nstate.\nA segmentation module calls the method\npropagateSegment to signal the onset, offset and\nexact boundaries of a new segment to following temporal\nmodeling modules (which implement segment ).\nThe utility function signalError can be used to pass\nan error message to the host.\n3.2 Module Parameters\nThe template class PiPo::Attr permits to deﬁne scalar,\nenum, or variable or ﬁxed size vector parameters of a pipo\nmodule that are exposed to the host environment.\nThey are initialised in the module constructor with a\nshort name, a description, a ﬂag if a change of value means\nthe fundamental stream parameters must be reset (if true,\n10https://github.com/Ircam-RnD/pipo-sdk/tree/master/includestreamAttributes will be called again for the whole\nchain), and a default value.\nTheir value can be queried in streamAttributes\norframes (in real-time hosts, a parameter’s value can\nchange over time) with PiPo::Attr::get() .\n3.3 Example of a Minimal PiPo Module\nclass PiPoGain : public PiPo\nf\nprivate:\nstd::vector<PiPoValue> buffer;\npublic:\nPiPoScalarAttr<double> factor;\nPiPoGain (Parent *parent, PiPo *receiver = NULL)\n: PiPo(parent, receiver),\nfactor(this, \"factor\", \"Gain Factor\", false, 1.0) f g\n\u0018PiPoGain (void) f g\nint streamAttributes (bool hasTimeTags, double rate,\ndouble offset, unsigned int width, unsigned int height,\nconst char **labels, bool hasVarSize,\ndouble domain, unsigned int maxFrames)\nf// can not work in place, create output buffer\nbuffer.resize(width *height *maxFrames);\nreturn propagateStreamAttributes(hasTimeTags, rate,\noffset, width, height, labels,\nhasVarSize, domain, maxFrames);\ng\nint frames (double time, PiPoValue *values,\nunsigned int size, unsigned int num)\nf// get gain factor here, it could change while running\ndouble f = factor.get();\nPiPoValue *ptr = &buffer[0];\nfor (unsigned int i = 0; i < num; i++)\nf\nfor (unsigned int j = 0; j < size; j++)\nptr[j] = values[j] *f;\nptr += size;\nvalues += size;\ng\nreturn propagateFrames(time, &buffer[0], size, num);\ng\ng;\n3.4 Existing Modules\nThe list of existing PiPo modules can be organized into the\nfollowing categories:\nStream Processing slice (windowing), scale ,sum,\nselect (get columns),\nFiltering biquad (biquad ﬁlter), mvavrg (moving aver-\nage ﬁlter), median (median ﬁlter), delta (deriva-\ntive), finitedif [9],bayesfilter [10]\nSegmentation onseg (segments starting at signal onset),\nchop (segments of regular intervals), gate (seg-\nments excluding weak sections), sylseg [16]\nTemporal Modeling mean ,std,meanstd ,min,max\nAnalysis descr (basic audio descriptors), yin (pitch\nextractor), moments (centroid, spread, skew-\nness, kurtosis), lpc (linear predictive coding),\nlpcformants (formant extraction), psy (pitch\nsynchronous markers), ircamdescriptor [18]Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 365Frequency Domain Processing fft (FFT from pre-\nwindowed frames), dct (discrete cosine transform),\nbands (MEL bands and similar from power or\namplitude spectrum), mel (MEL bands from au-\ndio stream), mfcc (MFCC from audio stream),\nwavelet (wavelet transform from audio stream)\nThey can be instantiated from C++ code us-\ning the precompiled libpipo library, thanks to the\nPiPoCollection class deﬁned in the PiPo SDK, or be\nused in one of the environments described in 3.6. The\nPiPoCollection class acts as a module factory. It is\nable to instantiate PiPo graphs from a simple syntax, which\ncan then be used as simple PiPo s in a host environment. It\nalso allows users of the API to add their own PiPo mod-\nules to the original collection. Once added, more complex\ngraphs combining modules from libpipo and these new\nmodules can be instantiated and run inside a PiPo host.\n3.5 PiPo Graph Construction\nGraphs of PiPo modules can be either constructed in C++\ncode or — within a given host environment — through ex-\npressions of a very simple syntax. For the ﬁrst case, the\nAPI deﬁnes a set of primitives that can be used to con-\nstruct graphs of any complexity by arranging modules in\nsequence and in parallel. In the latter case, these primi-\ntives are instantiated by a parser function provided by the\nAPI.\n3.5.1 Speciﬁc Graph Construction Modules\nAdditional to the data processing modules listed above,\nthere are two internal modules that handle the connection\nof processing modules in sequence or in parallel.\nThesequence module simply connects the upstream\nmodule to the downstream one (i.e. setting the latter as\nreceiver, so that the API calls get propagated through the\nchain). The parallel module essentially consists of an\nordered set of modules that receive the same input stream\nand output towards an internal merge module. Each in-\ncoming frame is processed by each of the parallel modules\nin the given order, whereby the merge module concate-\nnates the output data column-wise into a single matrix that\nis output towards the receiver of the parallel module.\n3.5.2 Graph Construction Syntax\nThe construction of sequences and parallel modules is also\navailable at the user level via a simple syntax inspired by\nFAUST [17], with the following operators:\n:(sequence)\n<(branch)\n,(parallel)\n>(merge)\nFor example, the pipo.descr module de-\nscribed in section 2.3 would be written like this:\nslice<yin,fft<sum:scale,moments>>\nA ﬁfth operator, _(identity), allows the propagation of\nintermediate analysis results to the end of the graph. Fol-lowing the sequence and parallel reduction rules from sec-\ntion 2.3, any PiPo graph is equivalent to a PiPo , and as a\nconsequence must have a single input and a single output,\nwhich implies that the graph syntax must contain the exact\nsame number of branch and merge operators.\n3.6 Bindings\n3.6.1 Max\nThe PiPo modules are available within the M AXvisual\nprogramming environment via the MuBu package, [19]\nwhere they can run in real-time using the pipo\u0018and\npipo MAXobjects, or ofﬂine using the mubu.process\nobject.\n3.6.2 Juce, OpenFrameworks, OpenMusic, Unity3D\nPiPo has been integrated into the J UCE11framework,\nthe creative coding framework O PENFRAMEWORKS ,12\nthe computer-aided composition environment O PEN-\nMUSIC [4] and the game development environment\nUNITY 3D.13Most if these developments are based on the\nIAE (Interactive Audio Engine) library. [20] The library\nallows for loading a sound ﬁle as input of a user-speciﬁed\nPiPo chain and to retrieve the result at the output.\n4. CONCLUSIONS AND FUTURE WORK\nThePiPo API and modules are in production use in our de-\npartment, and with research project partners, and artists in\ninteractive gesture and music installations and digital in-\nstruments. We feel it could help a wider community for\neasy prototyping and transfer to developed products.\nThe PiPo API is currently in the process of being in-\ntegrated in the RAPIDMIX API, a wider C++ software\necosystem including machine learning, signal feature ex-\ntraction and audio processing libraries, as a standardized\nway of building modular signal descriptors and machine\nlearning algorithms, integrating them into a global work-\nﬂow and allowing users of this ecosystem to build sustain-\nable code on top of a base collection of algorithms, by pro-\nviding a ﬂexible mean of interaction between its software\ncomponents.\nWe made ﬁrst steps to add an API similar to PiPo to also\nintegrate the iterative training of machine learning and data\nprocessing easily into the same host environments.\nThePiPo SDK that supports the development of mod-\nules as well as hosts, has been published under the\nBSD 3-Clause license at https://github.com/Ircam-RnD/\npipo-sdk .\n5. ACKNOWLEDGMENTS\nThe development of the PiPo API has received support\nfrom the RAPID-MIX project (H2020-ICT-2014-1 Project\nID 644862), funded by the European Union’s Horizon\n2020 research and innovation programme.\n11https://www.juce.com\n12http://openframeworks.cc\n13http://unity3d.com366 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. REFERENCES\n[1] X. Amatriain, J. Bonada, A. Loscos, J. Arcos, and\nV . Verfaille. Content-based transformations. Journal of\nNew Music Research , 32(1):95–114, 2003.\n[2] Dmitry Bogdanov, Nicolas Wack, Emilia G ´omez,\nSankalp Gulati, Perfecto Herrera, Oscar Mayor, Gerard\nRoma, Justin Salamon, Jos ´e Zapata, and Xavier Serra.\nEssentia: An open-source library for sound and music\nanalysis. In Proceedings of the 21st ACM International\nConference on Multimedia , MM ’13, pages 855–858,\nNew York, NY , USA, 2013. ACM.\n[3] W Brent. A Timbre Analysis and Classiﬁcation Toolkit\nfor Pure Data. In International Computer Music Con-\nference , New York City, NY , 2010.\n[4] Jean Bresson, Carlos Agon, and G ´erard Assayag.\nOpenMusic – Visual Programming Environment for\nMusic Composition, Analysis and Research. In ACM\nMultiMedia (MM’11) , Scottsdale, United States, 2011.\n[5] Neil Burroughs, Adam Parkin, and George Tzanetakis.\nFlexible scheduling for dataﬂow audio processing. In\nProceedings of the International Computer Music Con-\nference (ICMC) , New Orleans, Louisiana, USA, Au-\ngust 2006.\n[6] Chris Cannam. The V AMP Audio Analysis Plugin\nAPI: A Programmers Guide. http://vamp-plugins.\norg/guide.pdf , 2008.\n[7] Florian Eyben, Felix Weninger, Florian Gross, and\nBj¨orn Schuller. Recent developments in opensmile, the\nmunich open-source multimedia feature extractor. In\nProceedings of the 21st ACM International Conference\non Multimedia , MM ’13, pages 835–838, New York,\nNY , USA, 2013. ACM.\n[8] Florian Eyben, Martin W ¨ollmer, and Bj ¨orn Schuller.\nOpensmile: The munich versatile and fast open-source\naudio feature extractor. In Proceedings of the 18th\nACM International Conference on Multimedia , MM\n’10, pages 1459–1462, New York, NY , USA, 2010.\nACM.\n[9] B. Fornberg. Finite difference method. Scholarpedia ,\n6(10):9685, 2011. revision #91262.\n[10] Jules Franc ¸oise. Motion-Sound Mapping by Demon-\nstration . PhD thesis, Universit ´e Pierre et Marie Curie,\n2015.\n[11] Tristan Jehan. Musical signal parameter estimation.\nMaster’s thesis, IFSIC, Universit ´e de Rennes, France,\nand Center for New Music and Audio Technologies\n(CNMAT), University of California, Berkeley, USA,\n1997.\n[12] Alexander Lerch, Gunnar Eisenberg, and Koen\nTanghe. FEAPI: A Low Level Feature Extraction Plu-\ngin API. In 8th International Conference on Digital\nAudio Effects (DAFx05 , 2005.[13] M. Malt and E. Jourdan. Zsa. Descriptors: a library for\nreal-time descriptors analysis. In 5th Sound and Music\nComputing Conference , pages 134–137, Berlin, Ger-\nmany, August 2008.\n[14] Benoit Mathieu, Slim Essid, Thomas Fillon, Jacques\nPrado, and Ga ¨el Richard. YAAFE, an easy to use and\nefﬁcient audio feature extraction software. In Proceed-\nings of the International Symposium on Music Infor-\nmation Retrieval (ISMIR) , 2010.\n[15] David Moffat, David Ronan, Joshua D Reiss, et al.\nAn evaluation of audio feature extraction toolboxes.\nInProceedings of the COST-G6 Conference on Digi-\ntal Audio Effects (DAFx) , Trondheim, Norway, 2015.\n[16] Nicolas Obin, Franc ¸ois Lamare, and Axel Roebel. Syll-\no-matic: an adaptive time-frequency representation for\nthe automatic segmentation of speech into syllables. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2013.\n[17] Yann Orlarey, Dominique Fober, and St ´ephane Letz.\nFaust: an efﬁcient functional approach to dsp program-\nming. New Computational Paradigms for Computer\nMusic , 290, 2009.\n[18] Geoffroy Peeters. A large set of audio features for\nsound description (similarity and classiﬁcation) in the\nCuidado project. Technical Report version 1.0, Ircam –\nCentre Pompidou, Paris, France, April 2004.\n[19] Norbert Schnell, Axel R ¨obel, Diemo Schwarz, Geof-\nfroy Peeters, and Ricardo Borghesi. MuBu & friends –\nassembling tools for content based real-time interactive\naudio processing in Max/MSP. In Proceedings of the\nInternational Computer Music Conference (ICMC) ,\nMontreal, Canada, August 2009.\n[20] Norbert Schnell, Diemo Schwarz, Roland Cahen,\nand Victor Zappi. IAE & IAEOU. In Roland Ca-\nhen, editor, Topophonie research project : Audio-\ngraphic cluster navigation (2009-2012) , Les Carnets\nd’Experimentation de l’Ecole Nationale Superieure de\nCreation Industrielle, pages 50–51. ENSCI - Les Ate-\nliers / Paris Design Lab, December 2012.\n[21] Diemo Schwarz. Corpus-based concatenative synthe-\nsis.IEEE Signal Processing Magazine , 24(2):92–104,\nMarch 2007. Special Section: Signal Processing for\nSound Synthesis.\n[22] Diemo Schwarz and Norbert Schnell. A modular sound\ndescriptor analysis framework for relaxed-real-time\napplications. In Proceedings of the International Com-\nputer Music Conference (ICMC) , New York, NY , 2010.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 367"
    },
    {
        "title": "Multi-Pitch Detection and Voice Assignment for A Cappella Recordings of Multiple Singers.",
        "author": [
            "Rodrigo Schramm",
            "Andrew McLeod",
            "Mark Steedman",
            "Emmanouil Benetos"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417671",
        "url": "https://doi.org/10.5281/zenodo.1417671",
        "ee": "https://zenodo.org/records/1417671/files/SchrammMSB17.pdf",
        "abstract": "This paper presents a multi-pitch detection and voice as- signment method applied to audio recordings containing a cappella performances with multiple singers. A novel ap- proach combining an acoustic model for multi-pitch detec- tion and a music language model for voice separation and assignment is proposed. The acoustic model is a spectro- gram factorization process based on Probabilistic Latent Component Analysis (PLCA), driven by a 6-dimensional dictionary with pre-learned spectral templates. The voice separation component is based on hidden Markov mod- els that use musicological assumptions. By integrating the models, the system can detect multiple concurrent pitches in vocal music and assign each detected pitch to a specific voice corresponding to a voice type such as soprano, alto, tenor or bass (SATB). This work focuses on four-part com- positions, and evaluations on recordings of Bach Chorales and Barbershop quartets show that our integrated approach achieves an F-measure of over 70% for frame-based multi- pitch detection and over 45% for four-voice assignment.",
        "zenodo_id": 1417671,
        "dblp_key": "conf/ismir/SchrammMSB17",
        "keywords": [
            "acoustic model",
            "multi-pitch detection",
            "voice separation",
            "music language model",
            "spectrogram factorization",
            "Probabilistic Latent Component Analysis (PLCA)",
            "hidden Markov models",
            "voice type",
            "SATB",
            "F-measure"
        ],
        "content": "MULTI-PITCH DETECTION AND VOICE ASSIGNMENT FOR A\nCAPPELLA RECORDINGS OF MULTIPLE SINGERS\nRodrigo Schramm1;3\u0003, Andrew McLeod2\u0003, Mark Steedman2, Emmanouil Benetos3\n1Department of Music, Universidade Federal do Rio Grande do Sul, Brazil\n2School of Informatics, University of Edinburgh, UK\n3Centre for Digital Music, Queen Mary University of London, UK\nrschramm@ufrgs.br, A.McLeod-5@sms.ed.ac.uk, steedman@inf.ed.ac.uk,\nemmanouil.benetos@qmul.ac.uk\nABSTRACT\nThis paper presents a multi-pitch detection and voice as-\nsignment method applied to audio recordings containing a\ncappella performances with multiple singers. A novel ap-\nproach combining an acoustic model for multi-pitch detec-\ntion and a music language model for voice separation and\nassignment is proposed. The acoustic model is a spectro-\ngram factorization process based on Probabilistic Latent\nComponent Analysis (PLCA), driven by a 6-dimensional\ndictionary with pre-learned spectral templates. The voice\nseparation component is based on hidden Markov mod-\nels that use musicological assumptions. By integrating the\nmodels, the system can detect multiple concurrent pitches\nin vocal music and assign each detected pitch to a speciﬁc\nvoice corresponding to a voice type such as soprano, alto,\ntenor or bass (SATB). This work focuses on four-part com-\npositions, and evaluations on recordings of Bach Chorales\nand Barbershop quartets show that our integrated approach\nachieves an F-measure of over 70% for frame-based multi-\npitch detection and over 45% for four-voice assignment.\n1. INTRODUCTION\nAutomatic music transcription is deﬁned as the process of\nconverting an acoustic music signal into some form of mu-\nsic notation [3]. In the past years, several signal processing\nand machine learning approaches have been proposed for\nautomatic music transcription, with applications in music\ninformation retrieval, music education, computational mu-\nsicology, and interactive music systems. A core problem\nof automatic transcription is multi-pitch detection, i.e. the\ndetection of multiple concurrent pitches.\nFor multi-pitch detection, spectrogram factorization\nmethods have been used extensively in the last decade [3].\n* Authors 1 and 2 contributed equally to this work.\n© Rodrigo Schramm, Andrew McLeod, Mark Steedman,\nEmmanouil Benetos. Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Rodrigo Schramm,\nAndrew McLeod, Mark Steedman, Emmanouil Benetos. “Multi-pitch\ndetection and voice assignment for a cappella recordings of multiple\nsingers”, 18th International Society for Music Information Retrieval Con-\nference, Suzhou, China, 2017.However, despite promising results of template-based\ntechniques [4, 11, 17], the considerable variation in the\nspectral shape of pitches produced by different sources can\nstill affect generalization performance. Recent research on\nmulti-pitch detection has also focused on deep learning ap-\nproaches: in [13, 22], feedforward, recurrent and convolu-\ntional neural networks were evaluated towards the problem\nof automatic piano transcription.\nOn approaches for automatic transcription of vocal mu-\nsic, Bohak and Marolt [5] proposed a method for tran-\nscribing folk music containing both instruments and vo-\ncals, which explores the repetitions of melodic segments\nusing a musicological model for note-based transcription.\nA less explored type of music is a cappella ; in particular,\nvocal quartets constitute a traditional form of Western mu-\nsic, typically dividing a piece into multiple vocal parts such\nas soprano, alto, tenor, and bass (SATB). In [21], an acous-\ntic model based on spectrogram factorisation was proposed\nfor multi-pitch detection of such vocal quartets.\nA small group of methods have attempted to go be-\nyond multi-pitch detection, towards instrument assignment\n(also called timbre tracking) [1, 8, 11], where a system de-\ntects multiple pitches and assigns each pitch to a speciﬁc\nsource that produced it. Bay et al. [1] tracked individual in-\nstruments in polyphonic instrumental music using a spec-\ntrogram factorisation approach with continuity constraints\ncontrolled by a hidden Markov model (HMM).\nAn emerging area of automatic music transcription at-\ntempts to combine acoustic models (i.e. based on audio in-\nformation only) with music language models , which model\nsequences of notes and other music cues based on knowl-\nedge from music theory or from constraints automatically\nderived from symbolic music data. This is in direct anal-\nogy to automatic speech recognition systems, which typi-\ncally combine an acoustic model with a spoken language\nmodel. An example of such an integrated system is the\nwork by Sigtia et al. [22] which combined neural network-\nbased acoustic and music language models for multi-pitch\ndetection in piano music.\nCombining instrument assignment with this idea of\nusing a music language model, it is natural to look at\nthe ﬁeld of voice separation, which is the separation of\npitches into monophonic streams of notes, called voices,\nmainly addressed in the context of symbolic music pro-552cessing [6, 14, 16]. Several voice separation approaches\nare based on voice leading rules, which were investigated\nin [12,23,24] from a cognitive perspective. Among the nu-\nmerous rules pointed out by these authors, common char-\nacteristics are that large melodic intervals between consec-\nutive notes within a single voice should be avoided and that\ntwo voices should not cross in pitch. A third principle sug-\ngested by [12] is the idea that the stream of notes should\nbe relatively continuous within a single voice, and not have\ntoo many gaps of silence, ensuring temporal continuity.\nThe overarching aim of this work is to create a system\nable to detect multiple pitches in polyphonic vocal music\nand assign each detected pitch to a single voice of a spe-\nciﬁc voice type (e.g. soprano, alto, tenor, bass). Thus, the\nproposed method is able to perform both multi-pitch detec-\ntion and voice assignment. Our approach uses an acoustic\nmodel for multi-pitch detection based on probabilistic la-\ntent component analysis (PLCA), which is modiﬁed from\nthe model proposed in [21], and a music language model\nfor voice assignment based on the HMM proposed in [16].\nAlthough previous work has integrated musicological in-\nformation for note event modelling [5, 19, 22], to the au-\nthors’ knowledge, this is the ﬁrst attempt to incorporate an\nacoustic model with a music language model for the task of\nvoice or instrument assignment from audio, as well as the\nﬁrst attempt to propose a system for voice assignment in\npolyphonic a cappella music. The approach described in\nthis paper focuses on recordings of singing performances\nby vocal quartets without instrumental accompaniment; to\nthat end we use two datasets containing a capella record-\nings of Bach Chorales and Barbershop quartets. The pro-\nposed system is evaluated both in terms of multi-pitch\ndetection and voice assignment, where it reaches an F-\nmeasure of 70% and 45% for the two respective tasks.\n2. PROPOSED METHOD\nIn this section, we present a system for multi-pitch detec-\ntion and voice assignment from audio recordings of poly-\nphonic vocal music where the number of voices is known\na priori, that integrates an acoustic model with a music\nlanguage model. First, we describe the acoustic model,\na spectrogram factorization process based on probabilistic\nlatent component analysis (PLCA). Then, we present the\nmusic language model, an HMM-based voice assignment\nmodel. Finally, a joint model is proposed for the integra-\ntion of these two components. Figure 1 illustrates the pro-\nposed system pipeline.\n2.1 Acoustic Model\nThe acoustic model is a variant of the spectrogram\nfactorisation-based model proposed in [21]. The model\nuses a ﬁxed dictionary of log-spectral templates and aims\nto decompose an input time-frequency representation into\nseveral components denoting the activations of pitches,\nvoice types, tuning deviations, singer subjects, and vow-\nels. As time-frequency representation we use a normalised\nvariable-Q transform (VQT) spectrogram [20] with a hop\nAUDIO TIME/FREQUENCY\nACOUSTIC MODELMulti-Pitch Detection\nDICTIONARYVoice AssignmentMUSIC LANGUAGE\nMODEL\nREPRESENTATIONFigure 1 : Proposed system diagram.\nsize of 20 msec and 20cent resolution.\nThe input VQT spectrogram is denoted as X!;t2\nR\n\u0002T, where!denotes log-frequency and ttime. In the\nmodel,X!;tis approximated by a bivariate probability dis-\ntributionP(!;t), which is in turn decomposed as:\nP(!;t) = (1)\nP(t)X\ns;p;f;o;v\bPt(sjp)Pt(fjp)Pt(ojp)P(v)Pt(pjv)\nwhereP(t)is the spectrogram energy (known quantity).\n\b =P(!js;p;f;o;v )is the ﬁxed pre-extracted spectral\ntemplate dictionary (for details about the dictionary con-\nstruction, refer to [21]). Variable p2f21;:::;108gdenotes\npitch in MIDI scale, sdenotes the singer index (out of the\ncollection of singer subjects used to construct the input dic-\ntionary),odenotes the vowel type, vdenotes the voice type\n(e.g. soprano, alto, tenor, bass), and fdenotes tuning de-\nviation from 12-tone equal temperament in 20 cent resolu-\ntion (f2f1;:::; 5g, withf= 3 denoting ideal tuning).\nUnlike in [21], this model decomposes the probabilities\nof pitch and voice type as P(v)Pt(pjv). That is,Pt(pjv)\ndenotes the pitch activation for a speciﬁc voice type (eg.\nSATB) over time and P(v)can be viewed as a mixture\nweight that denotes the overall contribution of each voice\ntype to the whole input recording. The contribution of spe-\nciﬁc singer subjects from the training dictionary is mod-\nelled byPt(sjp), i.e. the singer contribution per pitch over\ntime.Pt(fjp)is the tuning deviation per pitch over time\nand ﬁnallyPt(ojp)is the time-varying vowel contribution\nper pitch1.\nThe factorization can be achieved by the expectation-\nmaximization (EM) algorithm [7], where the unknown\nmodel parameters Pt(sjp),Pt(fjp),Pt(ojp),Pt(pjv), and\nP(v)are iteratively estimated. In the Expectation step we\ncompute the posterior as:\nPt(s;p;f;o;vj!) = (2)\n\bPt(sjp)Pt(fjp)Pt(ojp)P(v)Pt(pjv)P\ns;p;f;o;v \bPt(sjp)Pt(fjp)Pt(ojp)P(v)Pt(pjv)\nIn the Maximization step, each unknown model param-\neter is then updated using the posterior from Eqn (2):\n1Although Pt(ojp)is not explicitly used in this proposed approach, it\nis kept to ensure consistency with the RWC audio dataset structure.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 553Pt(sjp)/X\nf;o;v;!Pt(s;p;f;o;vj!)X!;t (3)\nPt(fjp)/X\ns;o;v;!Pt(s;p;f;o;vj!)X!;t (4)\nPt(ojp)/X\ns;f;v;!Pt(s;p;f;o;vj!)X!;t (5)\nPt(pjv)/X\ns;f;o;!Pt(s;p;f;o;vj!)X!;t (6)\nP(v)/X\ns;f;o;p;!;tPt(s;p;f;o;pj!)X!;t (7)\nThe model parameters are randomly initialised, and the\nEM algorithm iterates over Eqns (2)-(7). In our experi-\nments we use 30iterations.\nThe output of the acoustic model is a semitone-scale\npitch activity tensor for each voice type and a pitch shift-\ning tensor, given by P(p;v;t ) =P(t)P(v)Pt(pjv)and\nP(f;p;v;t ) =P(t)P(v)Pt(pjv)Pt(fjp)respectively. By\nstacking together slices of P(f;p;v;t )for all values of p,\nwe can create a 20 cent-resolution time-pitch representa-\ntion for each voice type v:\nP(f0;t;v) = [P(f;21;v;t):::P(f;108;v;t)] (8)\nwheref0= 1;:::;880 denotes pitch in 20 cent resolu-\ntion. The overall multi-pitch detection without voice as-\nsignment, is given by P(p;t) =P\nvP(p;v;t ). Finally,\nthe voice-speciﬁc pitch activation output P(p;v;t )is bi-\nnarized and post-processed through a reﬁnement step de-\nscribed in [21], where each pitch is aligned with the nearest\npeak to it in the input log-frequency spectrum.\n2.2 Music Language Model\nThe music language model attempts to assign each de-\ntected pitch to a single voice based on musicological con-\nstraints. It is a variant of the HMM-based approach pro-\nposed in [16], where the main change is to the emission\nfunction (here it is probabilistic, while in the previous work\nit was deterministic). The model separates sequential sets\nof multi-pitch activations into monophonic voices (of type\nSATB) based on three principles: (1) consecutive notes\nwithin a voice tend to occur on similar pitches; (2) there\nare minimal temporal gaps between them; and (3) voices\nare unlikely to cross.\nThe observed data for the HMM are notes generated\nfrom the acoustic model’s binarised multi-pitch activations\nP(p;t), where each generates a note nwith pitch\u001a(n) =\np, onset time \u000e(n) =t, and an offset time \u001c(n) =t+ 1.\nOtrepresents this observed data at frame t.\n2.2.1 HMM: State Space\nIn the HMM, a state Stat frametcontains a list of M\nmonophonic voices Vi,1\u0014i\u0014M. The initial state S0\ncontainsMempty voices, and at each frame, each voice\nis assigned either no note, or a note with pitch \u001a(n)2\nf21;:::;108g. Each voice contains the entire history of thenotes which have been assigned to it from frame 1tot. The\nstate space of our model blows up exponentially (though it\nis reduced signiﬁcantly when the model is run discrimi-\nnatively as we do),, so instead of precomputed transition\nand emission probabilities, we use transition and emission\nprobability functions, presented in the following sections.\nConceptually, it is helpful to think of each state as sim-\nply a list of Mvoices, rather than to consider each voice\nto also be a list of notes. Thus, each state transition is cal-\nculated based on each voice in the previous state (though\nsome of the probability calculations require knowledge of\nindividual notes).\n2.2.2 HMM: Transition Function\nA stateSt\u00001has a transition to state Stif and only if each\nvoiceVi2St\u00001can be transformed into the corresponding\nVi2Stby assigning to it up to 1note with onset time t.\nThis transition from St\u00001toStcan be represented by\nthe variable TSt\u00001;Nt;Wt, whereSt\u00001is the original state,\nNtis a list of those notes ncontained by any voice in St\nwhere\u000e(n) =t, andWtis a list of integers, each represent-\ning the voice assignment index for a single note n2Nt.\nFor each index i,1\u0014i\u0014jNtj=jWtj, noteniis as-\nsigned to voice Vwi2St. Here,Ntonly contains those\nobserved notes which are assigned to a voice in St, not all\nobserved notes. Since all of our voices are monophonic,\nno two elements in Wtmay be equal.\nWe now deﬁne the HMM transition probability\nP(StjSt\u00001)as P(TSt\u00001;Nt;Wt):\nP(TSt\u00001;Nt;Wt) = \t(Wt)Y\n1\u0014i\u0014jNtj\u0002(St\u00001;ni;wi)\u0003(Vwi;ni):\n(9)\nThe ﬁrst term in this product is deﬁned as\n\t(W) =Y\n1\u0014j\u0014M(\n\u0007j2W\n1\u0000\u0007j =2W(10)\nwhere the parameter \u0007represents the probability that a\ngiven voice contains any note in a frame.\n\u0002(St\u00001;n;w )is a penalty function used to minimize\nthe voice crossings. It returns by default 1, but its output is\nmultiplied by a parameter \u0012—representing the probability\nof a voice being out of pitch order with an adjacent voice—\nfor each of the following cases that applies:\n1.w>1and\u001f(Vw\u00001)>\u001a(n)\n2.w<jMjand\u001f(Vw+1)<\u001a(n)\n\u001f(V)represents the pitch of a voice, calculated as a\nweighted sum of the pitches of its most recent notes. Cases\n1 and 2 apply when a note is out of pitch order with the pre-\nceding or succeeding voice in the state respectively.\n\u0003(V;n)is used to calculate the probability of a note n\nbeing assigned to a voice V, and is the product of a pitch\nscore \u0001pand a gap score \u0001g:\n\u0003(V;n) = \u0001 p(V;n)\u0001g(V;n) (11)\nThe pitch score, used to minimise melodic jumps within a\nvoice, is computed as shown in Eqn (12), where N(\u0016;\u001b)554 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017represents a normal distribution with mean \u0016and standard\ndeviation\u001b, and\u001bpis a parameter. The gap score is used to\nprefer temporal continuity within a voice, and is computed\nusing Eqn (13), where \u001c(V)is the offset time of the most\nrecent note in Vand\u001bgandgminare parameters. Both \u0001p\nand\u0001greturn 1ifVis empty.\n\u0001p(V;n) =N(\u001a(n)\u0000\u001f(V);\u001bp) (12)\n\u0001g(V;n) =max\u0012\nln\u0000\n\u0000\u000e(n)\u0000\u001c(V)\n\u001bg+ 1\u0001\n+1;gmin\u0013\n(13)\n2.2.3 HMM: Emission Function\nA stateStemits a set of notes containing only those which\nhave an onset at frame t, and a state containing a voice with\na note at frame tmust emit that note. The probability of\na stateStemitting the note set Otis shown in Eqn (14),\nusing the voice posterior Pt(vjp)from the acoustic model.\nP(OtjSt) =Y\nn2Ot(\nPt(v=ijp=\u001a(n))n2Vi2St\n1 otherwise\n(14)\nA state is not penalised for emitting notes not assigned to\nany of its voices. This allows the model to better handle\nfalse positives from the multi-pitch detection. For exam-\nple, if the acoustic model detects more than Mpitches,\nwe allow a state to emit the corresponding notes without\npenalty. We do, however, penalise a state for not assign-\ning a voice any note during a frame, but this is handled by\n\t(W)from Eqn (10).\n2.2.4 HMM: Inference\nTo ﬁnd the most likely ﬁnal state given our observed note\nsets, we use the Viterbi algorithm [26] with beam search\nwith beam size b. That is, after each iteration, we save only\ntheb= 50 most likely states given the observed data to that\npoint, in order to handle the complexity of the HMM.\n2.3 Model Integration\nIn this section, we describe the integration of the acous-\ntic model and the music language model into a single sys-\ntem which jointly performs multi-pitch detection and voice\nassignment from audio. This integration is done in two\nstages. First, using only the acoustic model from subsec-\ntion 2.1, the EM algorithm is run for 15 iterations, when\nthe multi-pitch detections converge. Next, the system runs\nfor 15 more EM iterations, this time also using the music\nlanguage model from subsection 2.2. In each iteration, the\nacoustic model is run ﬁrst, and then the language model is\nrun on the resulting multi-pitch detections. To intergrate\nthe two models, we apply a fusion mechanism inspired by\nthe one used in [9] to improve the acoustic model’s pitch\nactivations based on the resulting voice assignments.\nThe output of the language model is introduced into the\nacoustic model as a prior to Pt(pjv). During the acoustic\nmodel’s EM updates, Eqn (6) is modiﬁed as:\nPnew\nt(pjv) =\u000bPt(pjv) + (1\u0000\u000b)\u001et(pjv); (15)where\u000bis a weight parameter controlling the effect of the\nacoustic and language model and \u001eis a hyperparameter\ndeﬁned as:\n\u001et(pjv)/Pa\nt(pjv)Pt(pjv): (16)\nPa\nt(pjv)is calculated from the most probable ﬁnal\nHMM stateStmaxusing the pitch score \u0001p(V;n)from the\nHMM transition function of Eqn (12). For V, we use the\nvoiceVv2Stmaxas it was at frame t\u00001, and forn, we\nuse a note at pitch p. The probability values are then nor-\nmalised over all pitches per voice. The pitch score returns\na value of 1when theVis an empty voice (thus becoming\na uniform distribution over all pitches). The hyperparam-\neter of Eqn (16) acts as a soft mask, reweighing the pitch\ncontribution of each voice regarding only the pitch neigh-\nbourhood previously detected by the model.\nThe ﬁnal output of the integrated system is a list of the\ndetected pitches at each time frame which are assigned to\na voice in the most probable ﬁnal HMM state Stmax, along\nwith the voice assignment for each. Figure 2 shows an\nexample output of the integrated system.\n3. EVALUATION\n3.1 Datasets\nWe evaluate the proposed model on two datasets of a\ncapella recordings2: one of 26Bach Chorales and another\nof22Barbershop quartets, in total 104 minutes. These\nare the same datasets used in [21], allowing for a direct\ncomparison between it and the acoustic model proposed\nin Section 2.1. Each ﬁle is in wave format with a sample\nrate of 22.05 kHz and 16 bits per sample. Each record-\ning has four distinct vocal parts (SATB), with one part per\nchannel. The recordings from the Barbershop dataset each\ncontain four male voices, while the Bach Chorale record-\nings each contain a mixture of two male and two female\nvoices. A frame-based pitch ground truth for each vocal\npart was extracted using a monophonic pitch tracking al-\ngorithm [15] on each individual monophonic track. Exper-\niments are conducted using the mix down of each audio\nﬁle (i.e. polyphonic content), not the individual tracks.\n3.2 Evaluation Metrics\nWe evaluate the proposed system on both multi-pitch de-\ntection and voice assignment using the frame-based pre-\ncision, recall and F-measure as deﬁned in the MIREX\nmultiple-F0 estimation evaluations [2], with a frame hop\nsize of 20ms. The F-measure obtained by the multi-pitch\ndetection is denoted as Fmp, and for this, we combine the\nindividual voice ground truths into a single ground truth for\neach recording. For voice assignment, we simply use the\nindividual voice ground truths and deﬁne voice-speciﬁc F-\nmeasures of Fs,Fa,Ft, andFbfor each respective SATB\nvocal part. We also deﬁne an overall voice assignment F-\nmeasureFvafor a given recording as the arithmetic mean\nof its four voice-speciﬁc F-measures.\n2Original recordings available at http://www.pgmusic.com/\n{bachchorales.htm|barbershopquartet.htm} .Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 555t(sec)MIDI pitch MIDI pitch MIDI pitch\n(c)(b)(a)\n1 2 3 4 5 6 7 8 9 101 2 3 4 5 6 7 8 9 101 2 3 4 5 6 7 8 9 10\n203040506070203040506070203040506070Figure 2 : Multi-pitch detection and voice assignment for\na 10-sec excerpt of “O Sacred Head Sore Wounded” from\nthe Bach Chorales dataset. Each vocal part is shown in a\ndistinct shade of grey. (a) Ground truth. (b) Pitch activa-\ntionP(p;t). (c) Output of the integrated system.\n3.3 Training\nTo train the acoustic model, we use recordings from the\nRWC dataset [10] to generate the 6-dimensional dictionary\nof log-spectral templates speciﬁed in Section 2.1, follow-\ning the procedure described in [21]. The recordings used\nto generate the dictionary contain sequences of notes fol-\nlowing a chromatic scale, in ﬁve distinct English vowels\n(/a/, /æ/, /i/, / 6/, /u/). The dictionary contains templates\ngenerated from 15 distinct singers (9 male and 6 female,\nconsisting of 3 human subjects for each voice type: bass,\nbaritone, tenor, alto, soprano).\nFor all parameters in the music language model, we use\nthe values reported in [16] that were used for voice sepa-\nration in fugues. We also introduce two new parameters\nto the system: the voice order probability \u0012and the voice\nprobability \u0007. We use MIDI ﬁles of 50Bach Chorales3\n(none of which appear in the test set), splitting the notes\ninto20ms frames, and measure the proportion of frames\nin which a voice was out of pitch order with another voice,\nand the proportion of frames in which each voice contains\na note. This results in values of \u0012= 0:006and\u0007 = 0:99,\nwhich we use for testing.\nTo train the model integration weight \u000b, we use a grid\nsearch on the range [0:1;0:9]with a step size of 0:1, max-\nimisingFvafor each dataset. This results in a value of\n0:6when trained on the Chorale recordings and 0:3when\ntrained on the Barbershop recordings. To avoid overﬁtting,\n3MIDI ﬁles available at http://kern.ccarh.org/ .we employ cross-validation, using the \u000bvalue that max-\nimises the Chorales’ Fvawhen evaluating the Barbershop\nquartets, and vice versa.\n3.4 Results\nWe compare our model’s multi-pitch detection results with\nthose of three baseline methods: VINC+ [25], which uses\nan adaptive spectral decomposition based on unsupervised\nNMF; PERT+ [18], which selects candidates among spec-\ntral peaks, validating candidates through additional au-\ndio descriptors; and MSINGERS†+ [21], a PLCA model\nfor multi-pitch detection from multi-singers, similar to the\nacoustic model of our proposed system, although it also\nincludes a binary classiﬁer to estimate the ﬁnal pitch de-\ntections from the pitch activations. To the authors’ knowl-\nedge, there is no existing system for multi-pitch detection\nand voice assignment that can be used as a baseline for\nour model’s voice assignment. However, for the sake of\ncomparison, we include results from voice assignments\nderived from the model proposed in [21], which we call\nMSINGERS-V A, despite the fact that the original model\nwas not designed for the task.\nWe evaluate the above systems against two versions of\nour proposed model: VOCAL4-MP, using only the acous-\ntic model described in Section 2.1; and VOCAL4-V A, us-\ning the fully integrated model. From the multi-pitch detec-\ntion results in Table 1, it can be seen that MSINGERS†+\nachieves the highest Fmpon the Bach chorales, narrowly\nedging out VOCAL4-V A, but VOCAL4-V A achieves state-\nof-the-art results on the Barbershop quartets. In both\ndatasets, VOCAL4-V A outperforms VOCAL4-MP sub-\nstantially, indicating that the music language model is able\nto drive the acoustic model to a more meaningful factori-\nsation. The voice assignment results are shown in Table 2,\nwhere it is clear that VOCAL4-V A outperforms the other\nmodels, suggesting that perhaps a language model is al-\nmost necessary for the task. Also interesting to note is that\nit performs signiﬁcantly better on the bass voice than on\nthe other voices. Overtones are a major source of errors\nin our model, and the bass voice avoids these since it is\nalmost always the lowest voice.\nA further investigation into our model’s performance\ncan be found in Figure 3, which shows all of the VOCAL4-\nV A model’s F-measures, averaged across all songs in the\ncorresponding dataset after each EM iteration. The ﬁrst\nthing to notice is the large jump in performance at itera-\ntion15, when the language model is ﬁrst integrated into\nthe process. This jump is most signiﬁcant for voice assign-\nment, but is also clear for multi-pitch detection. The main\nsource of the improvement in multi-pitch detection is that\nthe music language model helps to eliminate many false\npositive pitch detections using the integrated pitch prior.\nIn fact, the multi-pitch detection performance continues to\nimprove until it ﬁnally converges after iteration 30.\nThe voice assignment results, however, are less straight-\nforward. After the signiﬁcant improvement on the 15th it-\neration, the results either remain relatively stable (in the\nBarbershop quartets) or even drop slightly (in the Bach556 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Model Bach Chorales Barbershop Quartets\nVINC+ 53.58 51.04\nPERT+ 67.19 63.85\nMSINGERS†+ 70.84 71.03\nVOCAL4-MP 63.05 59.09\nVOCAL4-V A 69.66 73.46\nTable 1 : Multi-pitch detection results.\nModelBach Chorales\nFvaFsFaFtFb\nMSINGERS-V A 18.02 15.37 17.59 26.32 12.81\nVOCAL4-MP 21.84 12.99 10.27 22.72 41.37\nVOCAL4-V A 45.31 26.07 37.63 49.61 67.94\nModelBarbershop Quartets\nFvaFsFaFtFb\nMSINGERS-V A 12.29 9.70 14.03 27.93 9.48\nVOCAL4-MP 18.35 2.40 10.56 16.61 43.85\nVOCAL4-V A 46.92 40.01 35.57 29.76 82.34\nTable 2 : V oice assignment results.\nchorales) before convergence. This slight drop is due to\nthe fact that the language model initially receives noisy\nmulti-pitch detections that include false positives (mainly\novertones). Incorporating these overtones into the voice as-\nsignment can cause the removal of correct pitch detections,\nwhich in turn reduces the voice assignment F-measures.\nAs mentioned earlier, the bass voice assignment outper-\nforms all other voice assignments in almost all cases, since\nfalse positive pitch detections from the acoustic model of-\nten correspond with overtones from lower notes that occur\nin the same pitch range as the correct notes from higher\nvoices. Another common source of errors (for both multi-\npitch detection and voice assignment) is vibrato. The\nacoustic model can have trouble detecting vibrato, and the\nmusic language model prefers voices with constant pitch\nover voices alternating between two pitches, leading to\nmany off-by-one errors in pitch detection. An example of\nboth of these types of errors can be found in Figure 4.\n4. CONCLUSION\nIn this paper, we have presented a system for multi-pitch\ndetection and voice assignment for a cappella recordings\nof multiple singers. It consists of two integrated compo-\nnents: a PLCA-based acoustic model and an HMM-based\nmusic language model. To our knowledge, ours is the ﬁrst\nsystem to be designed for the task4.\nWe have evaluated our system on both multi-pitch de-\ntection and voice assignment on two datasets: one of Bach\nchorales, and another of Barbershop quartets. Our model\noutperforms baseline multi-pitch detection systems on the\nBarbershop quartets, and achieves near state-of-the-art per-\nformance on the chorales. We have shown that integrating\nthe music language model improves multi-pitch detection\nperformance compared with a simpler version of our sys-\ntem with only the acoustic model. This suggests, as has\nbeen shown in previous work, that incorporating such mu-\nsic language models into other acoustic MIR tasks might\n4Supporting material for this work is available at\nhttp://inf.ufrgs.br/~rschramm/projects/msingers\nFbFtFaFsFvaFmp\nIterationF-measure\n5 10 15 20 25 3000.10.20.30.40.50.60.70.80.91(a)\nFbFtFaFsFvaFmp\nIterationF-measure\n5 10 15 20 25 3000.10.20.30.40.50.60.70.80.91\n(b)\nFigure 3 : The VOCAL4-V A model’s F-measures after\neach EM iteration, averaged across all songs in each\ndataset: (a) Bach Chorales. (b) Barbershop Quartets.\nFigure 4 : Pitch detections (red) and ground truth (black)\nfor the soprano voice at the beginning of the excerpt from\nFigure 2, showing errors from both overtones and vibrato.\nalso be of some beneﬁt, since they can guide acoustic mod-\nels using musicological principles.\nWe also presented results for voice assignment, and\nshow that while our model performs well given the difﬁ-\nculty of the task, there is certainly room for improvement.\nAvenues for future work include a better handling of over-\ntones in the acoustic model, and better recognition of vi-\nbrato in both the acoustic and the music language model.\nWe will also investigate the use of timbral information for\nfurther improving voice assignment performance. Addi-\ntionally, our model could be applied to different styles of\nmusic (e.g., instrumental, or those containing both instru-\nments and vocals) by learning a new dictionary for the\nacoustic model and retraining the parameters of the music\nlanguage model, and we intend to investigate the generality\nof our model in that context.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 5575. ACKNOWLEDGEMENT\nRS is supported by a UK Newton Research Collaboration\nProgramme Award (grant no. NRCP1617/5/46). AM and\nMS are supported by a gift from the 2017 Bloomberg Data\nScience Research Grant program and EU ERC H2020 Ad-\nvanced Fellowship GA 742137 SEMANTAX. EB is sup-\nported by a UK Royal Academy of Engineering Research\nFellowship (grant no. RF/128).\n6. REFERENCES\n[1] M. Bay, A. F. Ehmann, J. W. Beauchamp,\nP. Smaragdis, and J. Stephen Downie. Second\nﬁddle is important too: Pitch tracking individual\nvoices in polyphonic music. In ISMIR , pages 319–324,\n2012.\n[2] M. Bay, A. F. Ehmann, and J. S. Downie. Evaluation\nof multiple-F0 estimation and tracking systems. In IS-\nMIR, pages 315–320, October 26-30 2009.\n[3] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and\nA. Klapuri. Automatic music transcription: challenges\nand future directions. J. Intell. Inf. Syst. , 41(3):407–\n434, 2013.\n[4] E. Benetos and T. Weyde. An efﬁcient temporally-\nconstrained probabilistic model for multiple-\ninstrument music transcription. In ISMIR , pages\n701–707, 2015.\n[5] C. Bohak and M. Marolt. Transcription of polyphonic\nvocal music with a repetitive melodic structure. J. Au-\ndio Eng. Soc , 64(9):664–672, 2016.\n[6] E. Cambouropoulos. V oice and stream: Perceptual and\ncomputational modeling of voice separation. Music\nPerception: An Interdisciplinary Journal , 26(1):75–94,\n2008.\n[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-\nmum likelihood from incomplete data via the em algo-\nrithm. J. Royal Statistical Society , 39(1):1–38, 1977.\n[8] Z. Duan, J. Han, and B. Pardo. Multi-pitch streaming of\nharmonic sound mixtures. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , 22(1):138–\n150, Jan 2014.\n[9] D. Giannoulis, E. Benetos, A. Klapuri, and M. D.\nPlumbley. Improving instrument recognition in poly-\nphonic music through system integration. In ICASSP ,\npages 5222–5226, 2014.\n[10] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Music genre database and mu-\nsical instrument sound database. In ISMIR , pages 229–\n230, 2004.\n[11] G. Grindlay and D. P. W. Ellis. Transcribing\nmulti-instrument polyphonic music with hierarchical\neigeninstruments. IEEE J. Selected Topics in Signal\nProcessing , 5(6):1159–1169, October 2011.[12] D. Huron. Tone and voice: A derivation of the rules of\nvoice-leading from perceptual principles. Music Per-\nception , 19(1):1–64, 2001.\n[13] R. Kelz, M. Dorfer, F. Korzeniowski, S. B ¨ock, A. Arzt,\nand G. Widmer. On the potential of simple framewise\napproaches to piano transcription. In ISMIR , pages\n475–481, 2016.\n[14] P. B. Kirlin and P. E. Utgoff. VOISE: learning to seg-\nregate voices in explicit and implicit polyphony. In IS-\nMIR, pages 552–557, 2005.\n[15] M. Mauch and S. Dixon. pYIN: A fundamental fre-\nquency estimator using probabilistic threshold distri-\nbutions. In ICASSP , pages 659–663, 2014.\n[16] A. McLeod and M. Steedman. HMM-based voice sep-\naration of MIDI performance. Journal of New Music\nResearch , 45(1):17–26, 2016.\n[17] G. J. Mysore and P. Smaragdis. Relative pitch estima-\ntion of multiple instruments. In ICASSP , pages 313–\n316, 2009.\n[18] A. Pertusa and J. M. I ˜nesta. Efﬁcient methods for joint\nestimation of multiple fundamental frequencies in mu-\nsic signals. EURASIP Journal on Advances in Signal\nProcessing , 2012.\n[19] M. P. Ryynanen and A. Klapuri. Polyphonic mu-\nsic transcription using note event modeling. In IEEE\nWorkshop on Applications of Signal Processing to Au-\ndio and Acoustics, 2005. , pages 319–322, Oct 2005.\n[20] C. Sch ¨orkhuber, A. Klapuri, N. Holighaus, and\nM. D ¨orﬂer. A Matlab toolbox for efﬁcient perfect\nreconstruction time-frequency transforms with log-\nfrequency resolution. In AES 53rd Conference on Se-\nmantic Audio , January 2014.\n[21] R. Schramm and E. Benetos. Automatic transcrip-\ntion of a cappella recordings from multiple singers.\nInAES International Conference on Semantic Audio ,\nJune 2017.\n[22] S. Sigtia, E. Benetos, and S. Dixon. An end-to-end neu-\nral network for polyphonic piano music transcription.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 24(5):927–939, May 2016.\n[23] D. Temperley. A probabilistic model of melody percep-\ntion. Cognitive Science , 32(2):418–444, 2008.\n[24] D. Tymoczko. Scale theory, serial theory and voice\nleading. Music Analysis , 27(1):1–49, 2008.\n[25] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch esti-\nmation. IEEE Trans. Audio, Speech, and Lang. Pro-\ncessing , 18(3):528–537, March 2010.558 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[26] A. Viterbi. Error bounds for convolutional codes and\nan asymptotically optimum decoding algorithm. IEEE\ntransactions on Information Theory , 13(2):260–269,\n1967.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 559"
    },
    {
        "title": "A Post-Processing Procedure for Improving Music Tempo Estimates Using Supervised Learning.",
        "author": [
            "Hendrik Schreiber 0001",
            "Meinard Müller"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415046",
        "url": "https://doi.org/10.5281/zenodo.1415046",
        "ee": "https://zenodo.org/records/1415046/files/SchreiberM17.pdf",
        "abstract": "Tempo estimation is a fundamental problem in music in- formation retrieval and has been researched extensively. One problem still unsolved is the tendency of tempo es- timation algorithms to produce results that are wrong by a small number of known factors (so-called octave errors). We propose a method that uses supervised learning to pre- dict such tempo estimation errors. In a post-processing step, these predictions can then be used to correct an algo- rithm’s tempo estimates. While being simple and relying only on a small number of features, our proposed method significantly increases accuracy for state-of-the-art tempo estimation methods.",
        "zenodo_id": 1415046,
        "dblp_key": "conf/ismir/SchreiberM17",
        "keywords": [
            "tempo estimation",
            "fundamental problem",
            "music information retrieval",
            "octave errors",
            "supervised learning",
            "post-processing step",
            "accuracy",
            "state-of-the-art",
            "features",
            "algorithm"
        ],
        "content": "A POST-PROCESSING PROCEDURE FOR IMPROVING MUSIC TEMPO\nESTIMATES USING SUPERVISED LEARNING\nHendrik Schreiber\ntagtraum industries incorporated\nhs@tagtraum.comMeinard M ¨uller\nInternational Audio Laboratories Erlangen\nmeinard.mueller@audiolabs-erlangen.de\nABSTRACT\nTempo estimation is a fundamental problem in music in-\nformation retrieval and has been researched extensively.\nOne problem still unsolved is the tendency of tempo es-\ntimation algorithms to produce results that are wrong by a\nsmall number of known factors (so-called octave errors).\nWe propose a method that uses supervised learning to pre-\ndict such tempo estimation errors. In a post-processing\nstep, these predictions can then be used to correct an algo-\nrithm’s tempo estimates. While being simple and relying\nonly on a small number of features, our proposed method\nsigniﬁcantly increases accuracy for state-of-the-art tempo\nestimation methods.\n1. INTRODUCTION\nTempo-related tasks are well established in music informa-\ntion retrieval (MIR) [1]. One common task is to estimate\nthetempo humans “tap” along to a beat when listening to\nmusic. Another task, beat tracking , attempts to determine\nthe exact times at which beats occur. In this paper, we\ndeal with tempo estimation exclusively. While in some\ngenres—like Romantic music—local tempo changes are\ncommon [11], Pop, Rock, and Dance music often have one\nsteady, global tempo, i.e. it can be represented by a single\nnumber usually speciﬁed in beats per minute (BPM). The\nmethod proposed in this paper is only suitable for music\nwith such a global tempo.\nOver the years, many different approaches to tempo\nestimation have been taken. Gouyon et al. [9] provided\na comparative evaluation of the systems that participated\nin the ISMIR 2004 contest. Five years later, Zapata and\nG´omez gave an updated overview [30]. To our knowledge,\nthe most recent comprehensive evaluations are presented\nin [2, 23, 24]. For a textbook-style introductory overview\ndescribing different approaches, see [20] by M ¨uller.\nMany methods divide the estimation problem into two\nphases. First, via an onset strength signal (OSS ) or nov-\nelty curve, beat candidates are found. Second, one or more\nc\rHendrik Schreiber, Meinard M ¨uller. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Hendrik Schreiber, Meinard M ¨uller. “A post-processing\nprocedure for improving music tempo estimates using supervised learn-\ning”, 18th International Society for Music Information Retrieval Confer-\nence, Suzhou, China, 2017.periodicities are extracted from the OSS . Methods for ﬁnd-\ning periodicities are often based on the Fourier transform,\nbut also include autocorrelation [23], tempograms [29], the\ninteronset interval (IOI) histograms [26], and resonating\ncomb ﬁlters [2]. The decision for a ﬁnal result is based on\nsimple heuristics, genre classiﬁcation [15, 25], secondary\ntempo estimates [24], the discrete cosine transform of IOI\nhistograms [6], or a feature-based learning approach like\nGaussian mixture models (GMM) [22], support vector ma-\nchines (SVM) [8,23], k-nearest neighbor classiﬁcation (k-\nNNC) [29], and neural networks [5].\nFor evaluation, results are typically compared with a\nground truth allowing a 4%tolerance. This measure is\ncalled Accuracy1 . Because many algorithms have a ten-\ndency to under- or over-estimate the true value by a factor\nof2or3, a second measure called Accuracy2 has been in-\ntroduced. Accuracy2 allows for errors that correspond to a\nfactor of 2,3,1/2, or 1/3, also known as octave errors . De-\nspite evidence that algorithms as well as humans can dis-\ntinguish between slow and fast music [13, 18], Accuracy1\nvalues for state-of-the-art algorithms are still below Accu-\nracy2 . One way to change this may lie in genre- or style-\nrelated knowledge [4]. Many genres are partially deﬁned\nby a certain tempo or tempo range, which can be exploited\nto pick the right octave. Schuller et al. [25] demonstrated\nthis for the Ballroom dataset and H ¨orschl ¨ager et al. [15]\ndid the same for the GiantSteps tempo dataset. The fact\nthat the excellent method by B ¨ock et al. [2] scores remark-\nable 95% when trained on the Ballroom dataset with 8-\nfold cross validation, but reaches only 66:8%on the mixed\ngenre GTZAN dataset further supports this notion.\nIn this paper we describe a supervised learning ap-\nproach to correct common tempo estimation errors. This\nis achieved by re-framing error correction as a classiﬁca-\ntion problem. We are able to demonstrate that the pro-\nposed method performs better or as well as state-of-the-art\nalgorithms when combined with a simple tempo estima-\ntion method. Furthermore, because our error correction\napproach can be trained for any tempo detection method,\nwe are able to show improvements in Accuracy1 for previ-\nously published algorithms via post-processing.\nThe remainder of this paper is structured as follows:\nin Section 2 we describe a simple tempo estimation algo-\nrithm, test datasets, measures, and investigate common es-\ntimation error classes. Then, in Section 3, we explain our\npost-processing procedure, which corrects tempo estimates\nusing supervised learning based on a small number of au-235dio features. In Section 4 we evaluate the proposed fea-\ntures, and then compare our results with those from other\nmethods. Finally, in Section 5, we present our conclusions.\n2. TEMPO ESTIMATION\nTo lay the groundwork for our error correction method, we\nﬁrst describe a simple tempo estimation algorithm, then in-\ntroduce several test datasets and discuss common pitfalls.\nIn Section 2.5, we introduce performance metrics and de-\nscribe observed errors.\n2.1 Algorithm\nTo estimate the dominant pulse we follow the approach\ntaken in [24], which is similar to [23, 28]: We ﬁrst con-\nvert the signal to mono and downsample to 11025 Hz .\nThen we compute the power spectrum Yof93 ms win-\ndows with half overlap, by applying a Hamming win-\ndow and performing an STFT. The power for each bin\nk2[0 :K] :=f0;1;2;:::;Kgat timem2[0 :M] :=\nf0;1;2;:::;Mgis given byY(m;k), its positive logarith-\nmic powerYln(m;k) := ln (1000\u0001Y(m;k) + 1) , and its\nfrequency by F(k)given in Hz. We deﬁne the onset signal\nstrength OSS(m)as the sum of the bandwise differences\nbetween the logarithmic powers Yln(m;k)andYln(m\u0000\n1;k)for thosekwhere the frequency F(k)2[30;720]\nandY(m;k)is greater than \u000bY(m\u00001;k)(see [16]):\nI(m;k) =8\n<\n:1ifY(m;k)>\u000bY (m\u00001;k)\nandF(k)2[30;720];\n0otherwise(1)\nOSS(m) =X\nk(Yln(m;k)\u0000Yln(m\u00001;k))\u0001I(m;k)\nBoth the factor \u000b= 1:76and the frequency range were\nfound experimentally [24].\nTheOSS(m)is transformed using a DFT with length\n8192 . At the given sample rate, this ensures a resolution\nof0:156 BPM . The peaks of the resulting beat spectrum\nBrepresent the strength of BPM values in the signal [7],\nbut do not take harmonics into account [10, 21]. There-\nfore we derive an enhanced beat spectrum BEthat boosts\nfrequencies supported by harmonics:\nBE(k) =2X\ni=0jB(bk=2i+ 0:5c)j (2)\nSimilar to an enhanced beat histogram [28], BEincor-\nporates harmonics by simply adding to each bin the mag-\nnitudes of the bins denoted by half and by a quarter of its\nown frequency—or, if not available—the closest available\nbin. We choose to use fractions instead of multiples for\nmodeling harmonics and thus essentially model the fourth\nharmonic, not the ﬁrst. This allows us to take advantage of\nthe full DFT resolution without oversampling, as each bin\nfor the ﬁrst harmonic is mapped to four different bins for\nthe fourth harmonic. To estimate the tempo Tof the dom-\ninant pulse (or periodicity in the OSS ), we determine the0102030% of tracksSMC \u0016= 78:01;\u001b= 31:81;N= 217\n0102030% of tracksISMIR2004 Songs \u0016= 89:80;\u001b= 27:83\nN= 464\n0102030% of tracksGTZAN \u0016= 94:55;\u001b= 24:39\nN= 999\n0102030% of tracksACM MIRUM \u0016= 102:72;\u001b= 32:58\nN= 1410\n0102030% of tracksHainsworth \u0016= 113:30;\u001b= 28:78\nN= 222\n0102030% of tracksBallroom \u0016= 129:77;\u001b= 39:61\nN= 69820–30\n30–40\n40–50\n50–60\n60–70\n70–80\n80–90\n90–100\n100 –110\n110 –120\n120 –130\n130 –140\n140 –150\n150 –160\n160 –170\n170 –180\n180 –190\n190 –200\n200 –210\n210 –220\n220 –230\n230 –240\n240 –250\n250 –2600102030\nTempo intervals in BPM% of tracksGiantSteps \u0016= 136:66;\u001b= 28:33\nN= 664\nFigure 1 . Tempo distributions for the test datasets.\nhighest value of BE, divide its frequency by 4to ﬁnd the\nﬁrst harmonic, and ﬁnally convert its associated frequency\nto BPM:\nT=F(argmax\nkBE(k))\u000160\n4(3)\nTo ensure meaningful results for most kinds of Western\nmusic, we constrain Tto[40;250] by halving or doubling\nits value, if necessary.\n2.2 Test Datasets\nIt has become customary to benchmark tempo estimation\nmethods with results reported for a small set of well known\ndatasets. These are ACM MIRUM [18, 22], Ballroom [9],\nGTZAN [27], Hainsworth [12], ISMIR04 Songs [9], and\nSMC [14]. The latter was speciﬁcally designed to be difﬁ-236 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Dataset E0 E1 E2E1=2E3E1=3E4E1=4E3=2E2=3E5=4E4=5E4=3E3=4\nACM˙MIRUM 0:7 73 :5 16 :0 7 :0 0:8 0 :0 0:1 0 :0 1 :8 0 :1 0 :0 0 :0 0 :0 0 :1\nBallroom 0:4 64 :3 1 :0 29 :7 0:0 1 :1 0:0 0 :7 0 :1 2 :3 0 :0 0 :0 0 :0 0 :3\nHainsworth 8:6 64 :4 1 :4 19 :4 0:0 0 :5 0:0 0 :0 1 :8 1 :8 0 :9 0 :5 0 :5 0 :5\nGTZAN 4:0 72 :2 15 :7 5 :1 0:4 0 :1 0:0 0 :0 1 :0 0 :4 0 :1 0 :4 0 :5 0 :1\nISMIR04 Songs 4:3 64 :7 19 :4 6 :3 1:1 0 :2 0:0 0 :0 2 :4 0 :4 0 :4 0 :2 0 :2 0 :4\nSMC 29:0 37 :8 6 :0 10 :6 0:0 2 :3 0:0 0 :0 5 :1 2 :3 0 :5 2 :3 0 :9 3 :2\nCombined 3:9 68 :1 12 :3 11 :2 0:5 0 :4 0:0 0 :1 1 :5 0 :8 0 :1 0 :3 0 :2 0 :4\nGiantSteps 7:1 63 :1 4 :1 21 :5 0:0 0 :0 0:0 0 :0 0 :6 0 :3 0 :8 0 :9 0 :5 1 :2\nTable 1 . Error class distribution for tempo estimates T(given in BPM) for different datasets in percent.\nDataset Sweet Oct. Cov. 90% 95%\nACM MIRUM 69\u0000138 72 :850\u0000152 50\u0000170\nBallroom 71\u0000142 71 :184\u0000204 82\u0000204\nHainsworth 79\u0000158 82 :458\u0000150 57\u0000167\nGTZAN 66\u0000132 80 :955\u0000130 52\u0000138\nISMIR04 Songs 59\u0000118 74 :148\u0000131 36\u0000136\nSMC 51\u0000102 68 :732\u0000115 32\u0000143\nCombined 69\u0000138 72 :940\u0000150 50\u0000180\nGiantSteps 91\u0000182 88 :185\u0000175 80\u0000180\nTable 2 . Sweet octaves and their respective coverage in\npercent for the test datasets (left). Shortest BPM inter-\nvals required to achieve a test set coverage of 90% or95%\n(right).\ncult for beat trackers. Where applicable, we used the cor-\nrected annotations from [23]. We refer to the union of these\nsix datasets as the Combined dataset. Additionally, we test\nagainst the recently published GiantSteps dataset for elec-\ntronic dance music (EDM) [17]. It is not included in Com-\nbined to allow direct comparisons with older literature.\nNot surprisingly, all mentioned datasets differ in their\ncomposition (Figure 1). The mean tempo ranges from\n78 BPM (SMC) to 137 BPM (GiantSteps) and the stan-\ndard deviation spans from 24(GTZAN) to 40(Ballroom).\nFurthermore, the tempo distributions of Ballroom and Gi-\nantSteps contain some distinct spikes, while the other\ndatasets more closely resemble normal distributions. None\nof the datasets have uniformly distributed tempi.\n2.3 Octave Bias\nIf a dataset’s tempo distribution is not uniform and most\nvalues fall into a relatively small interval, constraining re-\nsults to this interval may lead to fewer octave errors. We\ncall deliberately choosing such an interval octave bias .\nTo illustrate this, assume an algorithm for the Gi-\nantSteps dataset with 50% Accuracy1 , but 100% Accu-\nracy2 . Further assume that all errors are by a factor of\n2or1=2.88:1%of all tempi in GiantSteps happen to be in\n[91;182) . If we constrained results to this interval by halv-\ning and doubling, Accuracy1 would increase from 50% to\n88:1%.\nEach described dataset has such a sweet octave , i.e. a\ntempo interval [j;2j)that contains more of the dataset’s\nsongs than any other octave (Table 2). In the absence of a\nuniform test set, it is therefore important to test the same al-\ngorithm against datasets with different sweet octaves, thus\nrevealing the effects of octave bias. On the positive side,a specialized or genre-aware algorithm may beneﬁt from\nexploiting knowledge about the test dataset (e.g. EDM-\nspeciﬁc tempi [15]). Additionally to sweet octaves, Table 2\nlists the shortest BPM intervals required to achieve a cer-\ntain test set coverage. For example, to cover 90% of the\ntempi in the ACM MIRUM test set, one only needs to look\nat the interval [50;152] and not at the considerably larger\ninterval [37;257] required for full coverage.\n2.4 Genre Bias\nWhile octave bias describes how algorithms can exploit\nconstraining results to certain tempo intervals, genre bias\ndescribes a technique for algorithms to constrain their out-\nput to a relatively small set of distinct tempi that are char-\nacteristic for the genres in the dataset.\nA good example for this is the Ballroom dataset. Even\nthough the dataset contains 698songs, only 63different\ntempi occur. Assuming an unbiased algorithm with integer\nprecision is constrained to the [40;250] BPM interval, it\nsolves a task equivalent to choosing one out of 210classes.\nAn algorithm trained on the Ballroom dataset using k-fold\ncross validation “knows” that there are only 63classes and\ntherefore has a considerably easier task to solve.\n2.5 Measures\nAs mentioned above, tempo estimation algorithms are usu-\nally evaluated with two metrics: Accuracy1 , deﬁned as the\npercentage of correct estimates with 4%tolerance, and Ac-\ncuracy2 , the percentage of correct estimates ignoring er-\nrors caused by the factors 2,3,1=2, and 1=3.\nBecause we aim to correct estimation errors, we need to\ntest our tempo estimation method against the test datasets\nand record not just accuracies, but also the kinds of er-\nrors. To do so, we deﬁne the error classes E2,E3,E4,\nE3=2,E5=4,E4=3and their reciprocals with the index in-\ndicating the error factor. Just like Accuracy1 andAccu-\nracy2 we allow a 4%tolerance. Since not all estimates\nare wrong and some errors are not covered by the men-\ntioned classes, we deﬁne E1for correct estimates (equiv-\nalent to Accuracy1 ) andE0for errors not described oth-\nerwise. This leads to a total of 14classes forming the\nlabel set E:=fE0;E1;:::g. Table 1 shows the distri-\nbution of estimated tempi over Efor the test datasets us-\ning the tempo estimation method from Section 2.1. For\nCombined ,12:3%of all tempi are in E2, while 11:2%\nare inE1=2. Only 3:9%of all estimated values cannot beProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 237explained by one of the deﬁned factors and thus are col-\nlected under the label E0. This implies an upper bound of\n96:1%Accuracy1 for any error correction scheme based\nonEw.r.t. the Combined datasets.\n3. TEMPO ERROR CORRECTION\nAs we have seen, most wrong tempo estimates are off by\na limited number of factors. Therefore the correction of\nTcan be re-framed as a classiﬁcation problem, which is\nsolvable using supervised machine learning. Knowing the\nerror class for an estimate then allows us to calculate the\ntrue tempo. In the following subsections we describe the\nfeatures used for classiﬁcation, the training dataset, and the\ntempo correction procedure.\n3.1 Features\nIn order to keep the algorithm simple, we use as features\nonlyTand a very small set of audio features. While not at-\ntempting to speciﬁcally model genres, the features we use\naim at characterizing rhythm, tonality and beat intensity.\nCombined, we expect them to capture essential informa-\ntion about a musical piece.\n3.1.1 Log Beat Spectrum\nThe tempi corresponding to the most common estimation\nclassesE1=2,E1, andE2fall onto a logarithmic scale. To\nmirror this, we use a logarithmic beat spectrum (LBS ) to\ndescribe the different periodicities in the signal. LBS is\ncomputed by resampling/interpolating Binto10logarith-\nmically spaced bands representing tempi ranging from 40\nto500 BPM . Subsequently it is normalized so that its\nhighest value is 1.\nWhile LBS provides a more complete picture of the pe-\nriodicities than just the dominant tempo T, it does not add\nany information about the frequency bands these periodic-\nities stem from. As a second modiﬁcation to B, we create\ndifferent versions of LBS based on the ﬁve slightly over-\nlapping bands [30;110],[100;220],[200;440],[400;880]\nand[800;1600] Hz . Combined, these spectra form the\nmultiband logarithmic beat spectrum (LBS M). For a given\nsong, LBS Mconsists of 5\u000210 = 50 features.\n3.1.2 Spectral Flatness\nTo represent tonality we use spectral ﬂatness (SF), also\nknown as Wiener entropy. It is deﬁned as the ratio be-\ntween the geometric and the arithmetic mean of the power\nspectrum:\nSF(m) =(QK\u00001\nk=0Y(m;k))1\nK\n1\nKPK\u00001\nk=0Y(m;k)(4)\nTo determine SF(m), we re-use the power spectra\nY(m;k)already computed in Section 2.1. For increased\nrobustness against low sample rates, we limit ktoF(k)2\n[30;3000] . As the two features for a given song we use\nboth the mean and the variance of all its SF(m).Unknown\n29%\nBallroom29%\nPop9% Hip Hop8%R&B 6%Rock\n6%Dance\n4%Other\n9%20–30\n30–40\n40–50\n50–60\n60–70\n70–80\n80–90\n90–100\n100 –110\n110 –120\n120 –130\n130 –140\n140 –150\n150 –160\n160 –170\n170 –180\n180 –190\n190 –200\n200 –210\n210 –220\n220 –230\n230 –240\n240 –250\n250 –2600102030\nTempo intervals in BPM% of tracksTrain \u0016= 115:64;\u001b= 28:69\nN= 13127\nFigure 2 . Distribution of genres and tempi for Train .\n3.1.3 Temporal Flatness\nTo represent onset intensity we use a feature called tem-\nporal ﬂatness (TF). Instead of calculating the Wiener en-\ntropy along the frequency axis of Y(m;k), as we did for\nSF, we calculate it over a window of length `along the\ntime axis:\nTF(m;`;k ) =(Q`\u00001\ni=0Y(m+i;k))1\n`\n1\n`P`\u00001\ni=0Y(m+i;k)(5)\nTo compute TFvalues, we again re-use Yand limit\nktoF(k)2[30;3000] .Yis split into non-overlapping\nwindows with length `= 100 . For each bin kin a given\nwindow we compute TF. We then calculate the average\nTFW(m;`)over allk. As the two features for a song we\nuse the mean and the variance of all its TFW(m;`)values.\n3.2 Training Dataset\nTo avoid learning the test datasets, we use a dataset for\ntraining the classiﬁer that has been created separately.\nTrain is the union of an annotated, private music collec-\ntion and the Extended Ballroom dataset [19] minus the 354\nsongs also occurring in the regular Ballroom set. Genre la-\nbels are available for 71% of the recordings. The genre as\nwell as the tempo distribution are shown in Figure 2.\n3.3 Correcting the Tempo Estimate\nDifferences between the training dataset’s true tempo val-\nues and the estimated tempo values let us derive error class\nlabels. With those and the proposed features, we can train\na classiﬁer. Using the classiﬁer, we are then able to predict\nan estimated error class E2Efor any song for which we\nalso have features and a tempo estimate. Note that this es-\ntimate does not have to stem from our own algorithm intro-\nduced in Section 2.1. One main idea of this paper, indeed,238 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Features Accuracy1 Accuracy2\nbase 69.00- 94.31\nLBS 75.84- 94.16\nLBS + SF 76.68 94.09\nLBS + TF 77.41 94.11\nLBS + SF + TF 77.31 94.04\nLBS M 76.66 93.87\nLBS M+ SF 75.91- 94.21\nLBS M+ TF 77.31 93.97\nLBS M+ SF + TF 76.21 94.14\nTable 3 .Accuracy1 andAccuracy2 for different feature\ncombinations trained on Train and tested against Com-\nbined . The ‘\u0000’ signs indicate a statistically signiﬁcant dif-\nference between the marked results and LBS + TF .\nis that the classiﬁcation model is algorithm-speciﬁc. In\nother words, the classiﬁer must be trained for each tempo\nestimation algorithm. Once trained, it can be used to cor-\nrect octave errors inherent to the given tempo estimation\nalgorithm.\nFor the prediction process itself, we use a Random For-\nest [3] with 300trees and a maximum depth of 25.\nGiven the estimated tempo Tand the predicted error E\nthe calculation of the corrected tempo Tcorrected is straight\nforward:\nJ(T;E) =\u001a1ifE=E0\niforEi(6)\nTcorrected =T\u0001J(T;E)\n4. EVALUATION\nIn a ﬁrst evaluation step, we compute Accuracy1 for differ-\nent feature combinations. We then compare the best com-\nbination with publicly available algorithms as well as other\nsimple correction schemes.\n4.1 Feature Evaluation\nWe trained the classiﬁer using the dataset Train with differ-\nent combinations of the proposed audio features and mea-\nsured the performance against the dataset Combined . All\ntested feature combinations clearly outperformed the base-\nline algorithm base (Twithout correction) by at least 6pp\n(percentage points) for Accuracy1 (Table 3). As was to be\nexpected, Accuracy2 didn’t change signiﬁcantly. The best\nperforming feature combination was LBS + TF with an\nAccuracy1 of77:41%. When testing for signiﬁcance with\nMcNemar’s test and a signiﬁcance level of p<0:01[30],\nwe found that LBS + TF performed signiﬁcantly better\nw.r.t. Accuracy1 than LBS ,LBS M+ SF , and base . In\nthe following we refer to the error classiﬁer trained with\nLBS + TF asnew. If no tempo estimation algorithm is\nexplicitly mentioned, the method from Section 2.1 is oth-\nerwise implied.\n4.2 Comparative Evaluation\nWe compared our method base+new to its baseline base\nand the three publicly available algorithms b¨ock,stem ,base stem b¨ock schr5060708090100\n69\n69:1\n72:4\n72:877:4\n74:4\n74:5\n76:6\nTempo estimation algorithmAccuracy1 in %algorithm algorithm+new\nFigure 3 .Accuracy1 forCombined for different algo-\nrithms with and without new error correction. All algo-\nrithms reach signiﬁcantly higher scores when combined\nwithnew.\nnone Sweet Octave 90% 95% new5060708090100\n69\n69:2\n70:6\n70\n77:469:1\n68:4\n69:9\n69:1\n74:472:4\n70:6\n74:3\n74:1\n74:572:8\n69:4\n72:7\n72:8\n76:6\nTempo estimate error correction methodAccuracy1 in %base stem b¨ock schr\nFigure 4 .Accuracy1 forCombined using no error correc-\ntion, constraint-based correction, and new correction for\nvarious tempo estimation algorithms.\nandschr using the test datasets described in Section 2.2.\nb¨ock1is the algorithm published by B ¨ock et al. in [2],\nbut trained with different datasets—among them our test\ndata, i.e. the algorithm is “familiar” with the test sets. Ac-\ncording to the authors, this conﬁguration participated in\nMIREX 2016. stem is an algorithm aiming for low com-\nputational complexity published by Percival et al. [23].\nWe used the implementation contained in Marsyas 0.5.0.2\nLastly, schr3was published by Schreiber et al. in [24].\nSince our error estimation and correction method can be\nused as a post-processor for any tempo estimator, we also\ntrained the classiﬁer for each of these three tempo estima-\ntion algorithms to investigate potential improvements.\nFigure 3 shows the Accuracy1 results for the four al-\ngorithms when tested against Combined , both with and\nwithout new post-processing. All of them score signif-\nicantly higher values when combined with new than in\ntheir plain form (McNemar, p < 0:01). The algorithms\nbase (77:4%, increase of +8:4pp) and stem (74:7%,\n+5:3pp) clearly beneﬁt the most, but also b¨ock (74:5%,\n+2:1pp) andschr (76:6%,+3:9pp) gain several percent-\n1https://github.com/CPJKU/madmom/\n2http://marsyas.info/\n3http://www.tagtraum.com/download/schreiber_\nicassp2014.zipProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 239Dataset base base+new stem stem+new b¨ock b ¨ock+new schr schr+new\nACM MIRUM 73.6 81.8+ 74.3 79.9+ 74.5 76.1+ 76.3 81.3+\nISMIR04 Songs 65.5 69.2 60.1 62.3 55.4 58.4+ 74.1 70.7-\nBallroom 64.3 85.1+ 64.0 81.8+ 84.8 90.4+ 67.0 85.0+\nHainsworth 64.9 73.4+ 69.8 74.8+ 84.2 85.6 73.0 75.7\nGTZAN 73.5 78.8+ 77.9 76.9 70.7 71.1 78.0 76.2\nSMC 45.2 39.6 29.5 29.5 51.1 51.6 41.5 35.5-\nDataset Average 64.5 71.3 62.6 67.5 70.1 72.2 68.3 70.7\nCombined 69.0 77.4+ 69.1 74.4+ 72.4 74.5+ 72.8 76.6+\nGiantSteps 64.5 64.0 47.0 65.0+ 61.5 70.9+ 58.0 60.1\nGiantSteps+Combined 68.4 75.5+ 66.0 73.1+ 70.8 74.0+ 70.7 74.3+\nTable 4 . Tempo results for Accuracy1 in percent. The ‘ +’ and ‘\u0000’ signs indicate a statistically signiﬁcant difference\nbetween an algorithm and the same algorithm enhanced with new. Bold numbers mark the best-performing algorithm(s)\nfor a dataset. Dataset Average is the mean of the algorithms’ results for each dataset except GiantSteps.\nage points.\nAs discussed in Section 2.3, a simple error correction\nscheme can be based on octave bias exploiting statisti-\ncal properties of the test dataset. We therefore compared\nour method with such a constraint-based scheme, where\ntempi below a lower interval bound are doubled and above\nan upper bound are halved. For intervals we used the\nsweet octave and those listed in Table 2 with 90% and\n95% coverage. Results are shown in Figure 4. Except for\nb¨ock, none of the algorithms beneﬁtted much from the\nsimple correction—perhaps a certain bias is already built-\nin. When comparing b¨ock+new andb¨ock+90% we were\nnot able to observe a signiﬁcant difference. It appears, as if\nb¨ock’s octave errors are harder to predict and correct than\nthose of the other algorithms, perhaps because they are less\nsystematic in nature.\nTable 4 provides a detailed overview of Accuracy1 re-\nsults for each of the test algorithms for all test datasets.\nAs mentioned, base+new reaches the highest score for\ntheCombined dataset ( 77:4%). To the best of our knowl-\nedge, this is the highest Accuracy1 score reported for Com-\nbined to date. For four of the six Combined datasets,\nbase+new reaches signiﬁcantly higher values than base\n(indicated by ‘ +’ signs in Table 4). The largest improve-\nment was achieved for the Ballroom test set. The score\nforbase+new is more than 20pp higher than base ’s.\nThe fact that 29% ofTrain consists of ballroom tracks cer-\ntainly plays a role here. While the base+new score for\nISMIR04 Songs is 3:7pphigher than base ’s, the improve-\nment is not signiﬁcant. Similarly, the change for the SMC\ndataset (\u00005:6pp) is not signiﬁcant, but noteworthy. We be-\nlieve that both octave and genre bias may play a role here.\nTracks in SMC are very different in style from those in\nTrain . And compared to SMC, Train contains relatively\nfew examples for slow tracks with 60 BPM or less. Infor-\nmal tests conﬁrm that choosing a different training dataset\nleads to better results.\nDataset-speciﬁc scores for b¨ock+new are all higher\nthan those for b¨ock—more than half of them signiﬁcantly.\nThe largest increase can be observed for the GiantSteps\ndataset. Plain b¨ock scores 61:5%—combined with new\nit reaches 70:9%(+9:4pp). To the best of our knowl-\nedge, this is the highest reported value for an unbiased,non-commercial algorithm to date.4\nDataset Average is the mean of the results for each of\nthe six datasets in Combined . Because it is an unweighted\naverage, it is not dominated by the larger datasets. But just\nlike for Combined , we can observe higher scores for all al-\ngorithms when combined with new. With 72:2%(+2:1pp)\nb¨ock+new reaches the highest score, closely followed\nbybase+new with 71:3%(+6:8pp).stem (67:5%,\n+4:9pp) and schr (70:7%,+2:4pp) beneﬁtted as well.\nThough not the topic of this paper, we also measured\nAccuracy2 . As expected, the results did not surprise and\nstayed stable.\n5. CONCLUSIONS\nWe have shown that the proposed error correction method\nbased on supervised learning of tempo estimation errors is\ncapable of signiﬁcantly improving Accuracy1 results for\nexisting tempo estimation algorithms. It does so in an\nalgorithm-speciﬁc post-processing step. Combined with a\nsimple tempo estimation algorithm, it outperforms other\nstate-of-the-art algorithms for most of the tested datasets.\nWe believe the error correction method can be enhanced\neven further by carefully selecting and incorporating other\ngenre-related features.\nWe also discussed different kinds of biases that can have\na large inﬂuence on the accuracy of tempo estimation al-\ngorithms. Ideally, evaluations of general purpose tempo\nestimators should be based on datasets with a mostly uni-\nform tempo and genre distribution. Because better train-\ning data potentially leads to better results, training datasets\nshould be an integral part of the comparison to make fair\nbenchmarking possible. Deﬁned train/test splits for exist-\ning datasets could be a ﬁrst step in this direction.\nAdditional Material:\nBinaries and other material are available at http://\nwww.tagtraum.com/tempo_estimation.html .\nAcknowledgments:\nThe International Audio Laboratories Erlangen are a\njoint institution of the Friedrich-Alexander-Universit ¨at\nErlangen-N ¨urnberg (FAU) and Fraunhofer Institut f ¨ur In-\ntegrierte Schaltungen IIS.\n4http://www.cp.jku.at/datasets/giantsteps/240 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. REFERENCES\n[1] Juan Pablo Bello, Laurent Daudet, Samer Abdal-\nlah, Chris Duxbury, Mike Davies, and Mark B. San-\ndler. A tutorial on onset detection in music signals.\nIEEE Transactions on Speech and Audio Processing ,\n13(5):1035–1047, 2005.\n[2] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nAccurate tempo estimation based on recurrent neu-\nral networks and resonating comb ﬁlters. In Proceed-\nings of the 16th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 625–631,\nM´alaga, Spain, 2015.\n[3] Leo Breiman. Random forests. Machine learning ,\n45(1):5–32, 2001.\n[4] Nick Collins. Towards a style-speciﬁc basis for com-\nputational beat tracking. In Proceedings of the 9th In-\nternational Conference on Music Perception and Cog-\nnition (ICMPC9) and 6th Triennial Conference of the\nEuropean Society for the Cognitive Sciences of Music\n(ESCOM) , Bologna, Italy, 2006.\n[5] Anders Elowsson. Beat tracking with a cepstroid in-\nvariant neural network. In Proceedings of the 17th In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , pages 351–357, New York, NY ,\nUSA, 2016.\n[6] Anders Elowsson and Anders Friberg. Modeling the\nperception of tempo. The Journal of the Acoustical So-\nciety of America , 137(6):3163–3177, 2015.\n[7] Jonathan Foote and Shingo Uchihashi. The beat spec-\ntrum: A new approach to rhythm analysis. In Proceed-\nings of the International Conference on Multimedia\nand Expo (ICME) , Los Alamitos, CA, USA, 2001.\n[8] Aggelos Gkiokas, Vassilios Katsouros, and George\nCarayannis. Reducing tempo octave errors by periodic-\nity vector coding and svm learning. In Proceedings of\nthe 13th International Society for Music Information\nRetrieval Conference (ISMIR) , pages 301–306. FEUP\nEdic ¸ ˜oes, 2012.\n[9] Fabien Gouyon, Anssi P. Klapuri, Simon Dixon,\nMiguel Alonso, George Tzanetakis, Christian Uhle,\nand Pedro Cano. An experimental comparison of au-\ndio tempo induction algorithms. IEEE Transactions on\nAudio, Speech, and Language Processing , 14(5):1832–\n1844, 2006.\n[10] Peter Grosche, Meinard M ¨uller, and Frank Kurth.\nCyclic tempogram – a mid-level tempo representa-\ntion for music signals. In Proceedings of IEEE Inter-\nnational Conference on Acoustics, Speech, and Sig-\nnal Processing (ICASSP) , pages 5522 – 5525, Dallas,\nTexas, USA, 2010.\n[11] Peter Grosche, Meinard M ¨uller, and Craig Stuart Sapp.\nWhat makes beat tracking difﬁcult? A case study onChopin Mazurkas. In Proceedings of the 11th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 649–654, Utrecht, The Netherlands, 2010.\n[12] Stephen Webley Hainsworth. Techniques for the Auto-\nmated Analysis of Musical Audio . PhD thesis, Univer-\nsity of Cambridge, UK, September 2004.\n[13] Jason Hockman and Ichiro Fujinaga. Fast vs slow:\nLearning tempo octaves from user data. In Proceed-\nings of the 11th International Conference on Music In-\nformation Retrieval (ISMIR) , pages 231–236, Utrecht,\nThe Netherlands, 2010.\n[14] Andre Holzapfel, Matthew EP Davies, Jos ´e R Zap-\nata, Jo ˜ao Lobato Oliveira, and Fabien Gouyon. Selec-\ntive sampling for beat tracking evaluation. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n20(9):2539–2548, 2012.\n[15] Florian H ¨orschl ¨ager, Richard V ogl, Sebastian B ¨ock,\nand Peter Knees. Addressing tempo estimation octave\nerrors in electronic music by incorporating style in-\nformation extracted from wikipedia. In Proceedings of\nthe Sound and Music Computing Conference (SMC) ,\nMaynooth, Ireland, 2015.\n[16] Anssi P. Klapuri. Sound onset detection by applying\npsychoacoustic knowledge. In Proceedings of the IEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing (ICASSP) , pages 3089–3092, Wash-\nington, DC, USA, 1999.\n[17] Peter Knees, ´Angel Faraldo, Perfecto Herrera, Richard\nV ogl, Sebastian B ¨ock, Florian H ¨orschl ¨ager, and Mick-\nael Le Goff. Two data sets for tempo estimation and\nkey detection in electronic dance music annotated from\nuser corrections. In Proceedings of the 16th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 364–370, M ´alaga, Spain, October\n2015.\n[18] Mark Levy. Improving perceptual tempo estimation\nwith crowd-sourced annotations. In Proceedings of the\n12th International Conference on Music Information\nRetrieval (ISMIR) , pages 317–322, 2011.\n[19] Ugo Marchand and Geoffroy Peeters. The extended\nballroom dataset. In Late Breaking Demo of the Inter-\nnational Conference on Music Information Retrieval\n(ISMIR) , New York, NY , USA, 2016.\n[20] Meinard M ¨uller. Fundamentals of Music Processing\n– Audio, Analysis, Algorithms, Applications . Springer\nVerlag, 2015.\n[21] Geoffroy Peeters. Template-based estimation of time-\nvarying tempo. EURASIP Journal on Advances in Sig-\nnal Processing , 2007(1):158–158, 2007.\n[22] Geoffroy Peeters and Joachim Flocon-Cholet. Percep-\ntual tempo estimation using GMM-regression. In Pro-\nceedings of the second international ACM workshopProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 241on Music information retrieval with user-centered and\nmultimodal strategies , MIRUM ’12, pages 45–50, New\nYork, NY , USA, 2012. ACM.\n[23] Graham Percival and George Tzanetakis. Streamlined\ntempo estimation based on autocorrelation and cross-\ncorrelation with pulses. IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) ,\n22(12):1765–1776, 2014.\n[24] Hendrik Schreiber and Meinard M ¨uller. Exploiting\nglobal features for tempo octave correction. In Pro-\nceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\npages 639–643, Florence, Italy, 2014.\n[25] Bj ¨orn Schuller, Florian Eyben, and Gerhard Rigoll.\nTango or waltz?: Putting ballroom dance style into\ntempo detection. EURASIP Journal on Audio, Speech,\nand Music Processing , 2008:12, 2008.\n[26] Jarno Sepp ¨anen. Tatum grid analysis of musical sig-\nnals. In Proceedings of the IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics\n(WASPAA) , pages 131–134, 2001.\n[27] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nSpeech and Audio Processing , 10(5):293–302, 2002.\n[28] George Tzanetakis and Graham Percival. An ef-\nfective, simple tempo estimation method based on\nself-similarity and regularity. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP) , Vancouver, Canada,\n2013.\n[29] Fu-Hai Frank Wu and Jyh-Shing Roger Jang. A super-\nvised learning method for tempo estimation of musical\naudio. In Control and Automation (MED), 2014 22nd\nMediterranean Conference of , pages 599–604. IEEE,\n2014.\n[30] Jose R. Zapata and Emilia G ´omez. Comparative eval-\nuation and combination of audio tempo estimation ap-\nproaches. In 42nd AES Conference on Semantic Audio ,\nIlmenau, Germany, 2011.242 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Modeling Harmony with Skip-Grams.",
        "author": [
            "David R. W. Sears",
            "Andreas Arzt",
            "Harald Frostel",
            "Reinhard Sonnleitner",
            "Gerhard Widmer"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416196",
        "url": "https://doi.org/10.5281/zenodo.1416196",
        "ee": "https://zenodo.org/records/1416196/files/SearsAFSW17.pdf",
        "abstract": "String-based (or viewpoint) models of tonal harmony often struggle with data sparsity in pattern discovery and predic- tion tasks, particularly when modeling composite events like triads and seventh chords, since the number of distinct n-note combinations in polyphonic textures is potentially enormous. To address this problem, this study examines the efficacy of skip-grams in music research, an alternative viewpoint method developed in corpus linguistics and nat- ural language processing that includes sub-sequences of n events (or n-grams) in a frequency distribution if their con- stituent members occur within a certain number of skips. Using a corpus consisting of four datasets of Western classical music in symbolic form, we found that including skip-grams reduces data sparsity in n-gram distributions by (1) minimizing the proportion of n-grams with negligi- ble counts, and (2) increasing the coverage of contiguous n-grams in a test corpus. What is more, skip-grams sig- nificantly outperformed contiguous n-grams in discovering conventional closing progressions (called cadences).",
        "zenodo_id": 1416196,
        "dblp_key": "conf/ismir/SearsAFSW17",
        "keywords": [
            "data sparsity",
            "pattern discovery",
            "prediction tasks",
            "composite events",
            "triads",
            "seventh chords",
            "polyphonic textures",
            "n-note combinations",
            "corpus linguistics",
            "natural language processing"
        ],
        "content": "MODELING HARMONY WITH SKIP-GRAMS\nDavid R. W. Sears Andreas Arzt Harald Frostel\nReinhard Sonnleitner Gerhard Widmer\nDepartment of Computational Perception, Johannes Kepler University, Linz, Austria\ndavid.sears@jku.at\nABSTRACT\nString-based (or viewpoint ) models of tonal harmony often\nstruggle with data sparsity in pattern discovery and predic-\ntion tasks, particularly when modeling composite events\nlike triads and seventh chords, since the number of distinct\nn-note combinations in polyphonic textures is potentially\nenormous. To address this problem, this study examines\nthe efﬁcacy of skip-grams in music research, an alternative\nviewpoint method developed in corpus linguistics and nat-\nural language processing that includes sub-sequences of n\nevents (or n-grams) in a frequency distribution if their con-\nstituent members occur within a certain number of skips.\nUsing a corpus consisting of four datasets of Western\nclassical music in symbolic form, we found that including\nskip-grams reduces data sparsity in n-gram distributions\nby (1) minimizing the proportion of n-grams with negligi-\nble counts, and (2) increasing the coverage of contiguous\nn-grams in a test corpus. What is more, skip-grams sig-\nniﬁcantly outperformed contiguous n-grams in discovering\nconventional closing progressions (called cadences ).\n1. INTRODUCTION\nCorpus studies employing string-based (or viewpoint )\nmethods in music research often suffer from the contigu-\nity fallacy —the assumption that note or chord events on\nthe musical surface depend only on their immediate neigh-\nbors. For example, in symbolic music corpora, researchers\noften divide the corpus into contiguous sequences of n\nevents (called n-grams) for the purposes of pattern discov-\nery [4], classiﬁcation [5], similarity estimation [16], and\nprediction [17]. And yet since much of the world’s mu-\nsic is hierarchically organized such that certain events are\nmore stable (or prominent) than others [1], non-contiguous\nevents often serve as focal points in the sequence [11]. As\na consequence, the contiguous n-gram method yields in-\ncreasingly sparse distributions as nincreases, resulting in\nthe well-known zero-frequency problem [27], in which n-\ngrams encountered in the test set do not appear in the train-\ning set. Perhaps worse, the most highly recurrent temporal\nc\rSears, Arzt, Frostel, Sonnleitner, Widmer. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Sears, Arzt, Frostel, Sonnleitner, Widmer. “Mod-\neling Harmony with Skip-grams”, 18th International Society for Music\nInformation Retrieval Conference, Suzhou, China, 2017.\n[VI i6ii6\n5 V7i\nPAC\nVln I\nVln II\nVla\nVc\nFigure 1 : Haydn, String Quartet in C minor, Op. 17/4, i,\nmm. 6–8. Non-chord tones are shown with orange\nnoteheads, and Roman numeral annotations appear below,\nwith the chords of the perfect authentic cadence (PAC)\nprogression embraced by a horizontal square bracket.\npatterns in tonal music—melodic formulæ, conventional\nchord progressions, etc.—are rarely included.\nBy way of example, consider the closing measures of\nthe main theme from the ﬁrst movement of Haydn’s string\nquartet Op. 17, No. 4, shown in Figure 1. The passage\nculminates in a perfect authentic cadence , a syntactic clos-\ning formula that features a conventional chord progression\n(V–I) and a falling upper-voice melody ( ^2–^1). In the music\ntheory classroom, students are taught to reduce this musi-\ncal surface to a succession of chord symbols, such as the\nRoman numeral annotations shown below. Yet despite the\nubiquity of this pattern throughout the history of Western\ntonal music, string-based methods generally fail to retrieve\nthis sequence of chords due to the presence of intervening\nnon-chord tones (shown in orange), a limitation one study\nhas called the interpolation problem [3].\nTo discover the organizational principles underlying\ntonal harmony using data-driven methods, this study ex-\namines the efﬁcacy of skip-grams in music research, an\nalternative viewpoint method developed in corpus linguis-\ntics and natural language processing that includes sub-\nsequences in an n-gram distribution if their constituent\nmembers occur within a certain number of skips. In lan-\nguage corpora, skip-grams have been shown to reduce data\nsparsity in n-gram distributions [13], discover multi-word\nexpressions (or collocations ) in pattern discovery tasks\n[22], and minimize model uncertainty in word prediction\ntasks [12].\nModels for the discovery of harmonic progressions\nin polyphonic corpora typically exclude higher-order se-\nquences (when n > 2) due to the sparsity of their dis-332tributions [18], so this paper examines the utility of skip-\ngrams for 2-grams, 3-grams, and 4-grams. We begin in\nSection 2 by describing the voice-leading type (VLT), an\noptimally reduced chord typology that models every pos-\nsible combination of note events in the dataset, but that re-\nduces the number of distinct chord types based on music-\ntheoretic principles. Following a formal deﬁnition of skip-\ngrams in Section 3, Section 4 describes the datasets used\nin the present research and then presents the experimen-\ntal evaluations, which consider whether skip-grams reduce\ndata sparsity in n-gram distributions by (1) minimizing\nthe proportion of rare n-grams (i.e., that feature negligible\ncounts), and (2) covering more of the contiguous n-grams\nin a test corpus. We conclude by considering avenues for\nfuture research.\n2. DATA-DRIVEN CHORD TYPOLOGIES\nCorpus studies in music research often treat the note event\nas the unit of analysis, examining features like chromatic\npitch [18], melodic interval [23], and chromatic scale de-\ngree [15]. Using computational methods to identify com-\nposite events like triads and seventh chords in complex\npolyphonic textures is considerably more complex, since\nthe number of distinct n-note combinations associated with\nany of the above-mentioned features is enormous.\nTo derive chord progressions from symbolic corpora us-\ning data-driven methods, many music analysis software\nframeworks perform a full expansion of the symbolic en-\ncoding, which duplicates overlapping note events at every\nunique onset time.1Shown in Figure 2, expansion re-\nsults in the identiﬁcation of 23 unique onset times. Since\nexpansion is less likely to under-partition more complex\npolyphony compared to other partitioning methods [4], we\nadopt this technique for the analyses that follow.\nTo reduce the vocabulary of potential chord types, pre-\nvious studies have represented each chord according to\nthe simultaneous relations between its note-event members\n(e.g., vertical intervals) [21], the sequential relations be-\ntween its chord-event neighbors (e.g., melodic intervals)\n[4], or some combination of the two [19]. The skip-gram\nmethod can model any of these representation schemes, but\nfor the purposes of this study, we have adopted the voice-\nleading type (VLT) representation developed in [19, 20],\nwhich produces an optimally reduced chord typology that\nstill models every possible combination of note events in\nthe dataset. The VLT scheme consists of an ordered tuple\n(S; I) for each chord in the sequence, where Sis a set of up\nto three intervals above the bass in semitones modulo the\noctave, resulting in 133(or 2197) possible combinations;2\nandIis the melodic interval (again modulo the octave)\nfrom the preceding bass note to the present one.\nBecause the VLT representation makes no distinction\nbetween chord tones and non-chord tones, the syntactic\n1InHumdrum , this technique is called ditto [14], while Music21 calls\nitchordifying [6].\n2The value of each vertical interval is either undeﬁned (denoted by\n?), or represents one of twelve possible interval classes, where 0 denotes\na perfect unison or octave, 7 denotes a perfect ﬁfth, and so on.Vln I\nVln II\nVla\nVc\n<4,9,?> < 3,7,9> < 4,7,10>\n<3,?,?>\n2 5\n 2\nFigure 2 : Full expansion of Op. 17/4, i, mm. 6–8.\nNon-chord tones are shown with orange noteheads, and\nthe most representative chord onsets of the PAC\nprogression are annotated with the VLT scheme.\ndomain of voice-leading types is still very large. To re-\nduce the domain to a more reasonable number, we have ex-\ncluded pitch class repetitions in S(i.e., voice doublings),\nand we have allowed permutations. Following [19], the\nassumption here is that the precise location and repeated\nappearance of a given interval are inconsequential to the\nidentity of the chord. By allowing permutations, the major\ntriadsh4;7;0iandh7;4;0itherefore reduce to h4;7;?i.\nSimilarly, by eliminating repetitions, the chords h4;4;10i\nandh4;10;10ireduce toh4;10;?i. This procedure re-\nstricts the domain to 233unique VLTs when n= 1 (i.e.,\nwhen Iis undeﬁned). Figure 2 presents the VLT encoding\nfor the PAC progression annotated in Figure 1, with the\nvertical interval classes Sprovided below each chord on-\nset, and the melodic interval classes Iinserted under hori-\nzontal angle brackets.\n3. DEFINING SKIP-GRAMS\nIn corpus linguistics, researchers often discover recurrent\npatterns by dividing the corpus into n-grams, and then de-\ntermining the number of instances (or tokens ) associated\nwith each unique n-gram type in the corpus. N-grams con-\nsisting of one, two, or three events are often called uni-\ngrams ,bigrams , and trigrams , respectively, while longer\nn-grams are typically represented by the value of n.\n3.1 Contiguous N-grams\nEach piece mconsists of a contiguous sequence of VLTs,\nso let krepresent the length of the sequence in each piece,\nand let Cdenote the total number of pieces in the corpus.\nThe number of contiguous n-gram tokens in the corpus is\nCX\nm=1km\u0000n+ 1 (1)\nThis formula ensures that the total number of tokens is nec-\nessarily smaller than the total number of events in the se-\nquence when n >1.\n3.2 Non-Contiguous N-grams\nThe most serious limitation of contiguous n-grams is that\nthey offer no alternatives; every event depends only on itsProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 333a\n b\n c\n d\n e\nContiguous\nSkip\nt 2-grams\n0 ab bc cd de\n1 ac bd ce\n2 ad be\n3 ae\nFigure 3 : Top: A 5-event sequence, with arcs denoting all\ncontiguous (solid) and non-contiguous (dashed) 2-gram\ntokens. Bottom: All 2-gram tokens, with tindicating the\nnumber of skips.\nimmediate neighbors. Without this limitation, the number\nof associations between events in the sequence necessarily\nexplodes in combinatorial complexity as nandkincrease.\nThe top plot in Figure 3 depicts the contiguous and\nnon-contiguous 2-gram tokens for a 5-event sequence with\nsolid and dashed arcs, respectively. According to (1), the\nnumber of contiguous 2-grams in a 5-event sequence is\nk\u0000n+ 1, or4tokens. If all possible non-contiguous\nrelations are also included, the number of tokens is given\nby the combination equation:\n\u0012k\nn\u0013\n=k!\nn!(k\u0000n)!=k(k\u00001)(k\u00002): : :(k\u0000n+ 1)\nn!\n(2)\nThe notation\u0000k\nn\u0001\ndenotes the number of possible com-\nbinations of nevents from a sequence of kevents. By\nincluding the non-contiguous associations, the number of\n2-grams for a 5-event sequence increases to 10. As nand\nkincrease, the number of patterns can very quickly be-\ncome unwieldy: a 20-event sequence, for example, con-\ntains 190 possible 2-grams, 1140 3-grams, 4845 4-grams,\nand 15,504 5-grams.\n3.2.1 Fixed-Skip N-grams\nTo overcome the combinatoric complexity of counting to-\nkens in this way, researchers in natural language process-\ning have limited the investigation to what we will call ﬁxed-\nskip n -grams [13], which only include n-gram tokens if\ntheir constituent members occur within a ﬁxed number of\nskips t. Shown in the bottom plot in Figure 3, acandbd\nconstitute 1-skip tokens (i.e., t= 1), while adandbecon-\nstitute 2-skip tokens. Thus, up to 7 tokens occur when\nt= 1, up to 9 occur when t= 2, and up to 10 occur\nwhen t= 3.3.2.2 Variable-Skip N-grams\nFor natural language texts, the temporal structure of a se-\nquence of linguistic utterances is not clearly deﬁned. Yet\nfor music corpora, temporal characteristics like onset time\nand duration play an essential role in the realization and re-\nception of musical works. For example, the upper bound-\nary under which listeners can group successive events into\ntemporal sequences is around 2s [10]. Thus, as an alterna-\ntive to the ﬁxed-skip method, we also include variable-skip\nn-grams, which include n-gram tokens if the inter-onset\ninterval(s) (IOI) between their constituent members occur\nwithin a speciﬁed upper boundary (e.g., 2s).\n4. EXPERIMENTAL EV ALUATIONS\nThis section describes the datasets in the present research\nand then examines whether the inclusion of skip-grams (1)\nminimizes the proportion of n-gram types with negligible\ncounts, and (2) covers more of the contiguous n-gram to-\nkens in a test corpus.\n4.1 Datasets & Pre-Processing\nShown in Table 1, this study includes four datasets of\nWestern classical music that feature symbolic representa-\ntions of both the notated score (e.g., metric position, rhyth-\nmic duration, pitch, etc.) and a recorded expressive perfor-\nmance (e.g., onset time and duration in seconds, velocity,\netc.). Altogether, the corpus totals over 20 hours of music.\nTheKod´aly/Haydn dataset consists of 50 Haydn string\nquartet movements encoded in MIDI format [21]. The data\nwere manually aligned at the downbeat level to recorded\nperformances by the Kod ´aly Quartet, and then the onset\ntime for each chord event in the symbolic representation\nwas estimated using linear interpolation.\nThe Batik/Mozart dataset consists of 13 complete\nMozart piano sonatas encoded in MATCH format [24].\nThe data were aligned to performances by Roland Batik\nthat were recorded on a B ¨osendorfer SE 290 computer-\ncontrolled piano, which is equipped with sensors on the\nkeys and hammers to measure the timing and dynamics of\neach note [25].\nThe remaining two datasets were encoded in Mu-\nsicXML format, and were also aligned to performances\nthat were recorded on a B ¨osendorfer computer-controlled\npiano. The Zeilinger/Beethoven dataset consists of 9\nComposer (Performer) Npieces Nchords Ntokens>3\nHaydn (Kod ´aly) 50 73,704 0\nMozart (Batik) 39 63,418 969\nBeethoven (Zeilinger) 30 42,157 910\nChopin (Magaloff) 156 147,871 3666\nTotal 275 327,150 5545\nNote .Ntokens>3denotes n-gram tokens that initially consisted of more\nthan three interval classes.\nTable 1 : Datasets and descriptive statistics for the corpus.334 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017complete Beethoven piano sonatas performed by Clemens\nZeilinger [8], while the Magaloff/Chopin dataset consists\nof 156 Chopin piano works that were performed by Nikita\nMagaloff [8, 9].\nPerforming a full expansion on all four datasets pro-\nduced 327,150 unique onsets from which to derive chords.\nUnfortunately, some onsets presented more than three ver-\ntical interval classes, but since the VLT scheme only per-\nmits up to three interval classes Sabove the bass, it was\nnecessary to replace these chords. Each onset containing\nmore than three distinct vertical interval classes was re-\nplaced either with (1) the closest maximal subset estimated\nfrom the immediate surrounding context (i.e., \u00065chords);\n(2) the most common maximal subset estimated from the\nentire piece; or ﬁnally (3) the most common maximal sub-\nset estimated from all pieces in the corpus.\n4.2 Reducing Sparsity\nIn natural language corpora, n-gram distributions of indi-\nvidual words ( n= 1) and multi-word expressions ( n <5)\ndemonstrate a power-law relationship between frequency\nand rank, with the most frequent (i.e., top-ranked) types\naccounting for the majority of the tokens in the distribu-\ntion [26]. In music corpora, however, this relationship be-\ncomes increasingly linear as nincreases due to the greater\nproportion of types featuring negligible counts. Such rare\nn-grams are thus more difﬁcult to retrieve and model in\ndiscovery and prediction tasks, so this section examines\nwhether the inclusion of skip-grams minimizes the propor-\ntion of rare n-grams in chord distributions.\n4.2.1 Methods\nContiguous n-gram distributions were calculated from n=\n1ton= 7, along with 4-grams that include the following\nskip levels: Fixed – up to 1, 2, 3, or 4 skips; Variable – all\npossible skips occurring within a maximum IOI of .5, 1,\n1.5, or 2s.\nSkip Ntypes Ntokens\nNo Skip\n135,331 326,034\nFixed – Skip boundary (#)\n1 850,222 2,604,972\n2 2,364,840 8,780,643\n3 4,765,289 20,786,976\n4 8,207,123 40,548,000\nVariable – IOIaboundary (s)\n0.5 2,213,148 10,150,852\n1 12,498,736 90,278,381\n1.5 31,591,468 306,289,766\n2 59,147,107 718,717,231\naIOI denotes the maximum permitted inter-onset interval in seconds be-\ntween adjacent members of each n-gram.\nTable 2 : Counts associated with 4-gram types and tokens\nusing both ﬁxed and variable skips.4.2.2 Results\nTable 2 presents the counts for 4-gram types and tokens\nwith both ﬁxed and variable skips. As expected, including\nskips of either type signiﬁcantly increased the number of\ntypes and tokens. When skips were not included, the cor-\npus produced over 300 thousand tokens, but this number\nincreased to over 40 million tokens for skip-grams includ-\ning up to 4 skips, or over 700 million tokens for skip-grams\nincluding all skips occurring within an IOI of 2s.\nTo visualize the increasing impact of data sparsity on\nthen-gram distribution as nincreases, the top plot in\nFigure 4 presents the cumulative probability distributions\nfor contiguous n-gram types from n= 1 ton= 7. Types\nappearing to the right of each marker feature only one\ntoken in the corpus. When nis small, the distributions\nloosely conform to the family of power laws used in lin-\nguistics to describe the frequency-of-occurrence of words\nin language corpora, where a small proportion of types\naccount for most of the encountered tokens. When nin-\ncreases, however, the proportion of types featuring negligi-\nble counts also increases, resulting in increasingly uniform\ndistributions.\nShown in the bottom plot in Figure 4, the power-law re-\nlationship returns in the 4-gram distributions when skips\nare included. What is more, the proportion of types featur-\ning negligible counts also decreases, thereby minimizing\nRank (proportion)\nCumulative Probability\n1-grams\n2-grams\n3-grams\n4-grams\n5-grams\n6-grams\n7-grams\nRank (proportion)\nCumulative Probability\nNo Skip\nFixed – 4 Skips\nVariable – 2s IOI\n4-grams0.20.40.60.81\n0\n0.20.40.60.81\n00.2 0.4 0.6 0.8 1\n0.2 0.4 0.6 0.8 1\nFigure 4 : Cumulative probability distributions for (top)\ncontiguous n-gram types, with types appearing to the right\nof each marker featuring only one token in the corpus; and\n(bottom) 4-gram types featuring no skips, up to four skips,\nor all skips occurring within an IOI of 2s.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 335the potential for data sparsity in the VLT distribution.\n4.3 Increasing Coverage\nThis section examines whether the inclusion of skip-gram\ntypes during training covers more of the contiguous n-gram\ntokens in a test corpus.\n4.3.1 Methods\n2-gram, 3-gram, and 4-gram distributions were calculated\nfor the following skip levels: Fixed – no skip, or up to 1,\n2, 3, or 4 skips; Variable – no skip, or all possible skips\noccurring within an IOI of .5, 1, 1.5, or 2s. To evaluate\nskip-gram coverage, we employed 10-fold cross-validation\nstratiﬁed by composer [7], using the proportion of contigu-\nousn-gram types in the test set that appeared in the train-\ning set as a measure of performance. To create folds con-\ntaining the same number of compositions andchords, we\ncomputed the mean number of chords that should appear in\neach fold m, and then selected the fold indices for which\neach fold (1) contained an approximately equal number of\ncompositions, and (2) contained a total number of chords\nthat was\u00061% ofm.\n4.3.2 Analysis\nTo examine the potential increase in coverage at each suc-\ncessive (ﬁxed or variable) skip, we calculated a planned\ncomparison statistic that does not assume equal variances,\ncalled the Welch ttest.3The mean of each skip was com-\npared to the mean of the previous skip using backward-\ndifference coding (e.g., Fixed : 2 skips vs. 1 skip, 3 skips\nvs. 2 skips, etc.). To minimize the risk of committing a\nType I error, each comparison was corrected with Bonfer-\nroni adjustment, which divides the signiﬁcance criterion by\nthe number of planned comparisons.\n4.3.3 Results\nFigure 5 displays line plots of the mean proportion of con-\ntiguous n-gram tokens from the test that appeared dur-\ning training using either ﬁxed or variable skips. Table 3\nprovides the mean coverage estimates and planned com-\nparisons. For 2-grams, on average the contiguous types\ncovered nearly 96% of the tokens in the test set. When\nskips were included, this estimate improved signiﬁcantly\nto 98.3% of the tokens for up to two ﬁxed skips, or up to\n99.2% percent of the tokens for all skips occurring within\nan IOI of 1.5 s.\nAsnincreased, the proportion of tokens that appeared\nduring training using contiguous n-grams decreased sub-\nstantially. For 3-grams, the contiguous types only covered\n70.7% of the tokens on average. This estimate improved\ndramatically when either ﬁxed or variable skips were in-\ncluded, however. For the ﬁxed-skip factor, including up to\n3In hypothesis testing, planned comparisons typically follow an om-\nnibus statistic like the Fratio, which indicates whether the differences\nbetween the means of a given factor are signiﬁcant. In this case, the\nWelch Ftest was signiﬁcant for every model, so we forgo reporting those\nstatistics here, and instead simply report the planned comparisons, which\nindicate whether coverage increased signiﬁcantly as the number of skips\n(or the size of the temporal boundary) increased.\n0.30.40.50.60.70.80.91\nNo Skip 1 2 3 4\nNumber of SkipsMean Proportion (CI)\n0.30.40.50.60.70.80.91\n2-grams\n3-grams\n4-grams\nNo Skip 0.5 1.0 1.5 2.0\nUpper Boundary (s)Mean Proportion (CI)\nFigure 5 : Line plots of the mean proportion of n-gram\ntokens from the test that were covered during training\nusing either ﬁxed (top) or variable (bottom) skips.\nWhiskers represent the 95% conﬁdence interval (CI)\naround the mean.\nfour skips during training covered an additional 20% of the\ntokens during test, resulting in a mean coverage estimate of\nover 90%. In the variable-skip condition, this estimate fur-\nther improved to 94.3% when all skips occurring within an\nIOI of 2s were included. Finally, for 4-grams, the contigu-\nous types covered just 36.5% of the tokens, but this esti-\nmate improved to 71.1% in the ﬁxed-skip condition, and to\n82.4% in the variable-skip condition.\n5. SUMMARY AND CONCLUSION\nTo reduce data sparsity in n-gram distributions of tonal har-\nmony, this study examined the efﬁcacy of skip-grams, an\nalternative viewpoint method that includes sub-sequences\nin an n-gram distribution if their constituent members oc-\ncur within a certain number of skips ( ﬁxed ), or a speciﬁed\ntemporal boundary ( variable ). To that end, we compiled\nfour datasets of Western classical music that feature sym-\nbolic representations of the notated score. Our ﬁndings\ndemonstrate that the inclusion of skip-grams reduces spar-\nsity in higher-order n-gram distributions by (1) minimiz-\ning the proportion of n-grams with negligible counts, thus\nrecovering the power-law relationship between frequency\nand rank when n <5that was previously lost in the cor-\nresponding contiguous distributions, and (2) increasing the\ncoverage of the contiguous n-grams in a test set, thereby\nmitigating the severity of the zero-frequency problem.\nIn our view, this approach would directly beneﬁt tasks336 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20172-grams 3-grams 4-grams\nSkip Mcoverage t p Mcoverage t p Mcoverage t p\nNo Skip\n.959 .707 .365\nFixed – Skip boundary (#)\n1 .976 7.144 <.001 .813 9.726 <.001 .529 10.963 <.001\n2 .983 4.000 .003 .859 5.518 <.001 .618 6.023 <.001\n3 .986 2.529 .085 .884 3.620 .008 .672 3.948 .003\n4 .988 1.848 .327 .901 2.814 .046 .711 3.063 .027\nVariable – IOI boundary (s)\n0.5 .979 8.439 <.001 .837 12.744 <.001 .595 15.795 <.001\n1 .988 6.598 <.001 .904 10.132 <.001 .727 9.786 <.001\n1.5 .992 3.647 .010 .929 5.313 <.001 .788 5.266 <.001\n2 .993 2.311 .132 .943 3.564 .009 .824 3.808 .005\nTable 3 : Mean coverage estimates and planned comparisons for 2-gram, 3-gram, and 4-gram tokens using either ﬁxed or\nvariable skips.\nrelated to pattern discovery and prediction, since recur-\nrent temporal patterns rarely appear on the musical surface,\nthereby forcing n-gram models to either exclude higher-\norder n-grams (e.g., where n > 2) due to the sparsity of\nthe distributions, or calculate escape probabilities to ac-\ncommodate patterns that do not appear (contiguously) in\nthe training set [2]. Consider, for example, the two four-\nchord cadential progressions in Table 4: the semplice ca-\ndence, which features a dominant-to-tonic progression in\nroot position (e.g., I6-ii6-V7-I); and the composta cadence,\nwhich also features a six-four suspension above the ca-\ndential dominant (e.g., ii6-“I6\n4”-V7-I). These cadences are\nubiquitous in music of the classical style, and yet the VLT\nconﬁgurations representing these progressions rarely ap-\npear on the surface; the semplice cadence never appears\ncontiguously, while the composta cadence is featured in\nSkip I6-ii6-V7-I ii6-“I6\n4”-V7-I\nNo Skip\n0 7\nFixed – Skip boundary (#)\n1 3 16\n2 10 36\n3 13 50\n4 15 63\nVariable – IOIaboundary (s)\n0.5 5 8\n1 10 33\n1.5 21 51\n2 32 77\nNote . VLT encodings for these progressions appear in the major and\nminor mode, and feature the pre-dominant and dominant harmonies both\nwith and without the seventh (e.g., ii6and ii6\n5).\nTable 4 : Number of pieces containing semplice or\ncomposta four-chord progressions using both ﬁxed and\nvariable skips.just seven pieces. When skips are included, however, the\ntwo progressions appear in 32 and 77 of the 245 pieces in\nthe corpus, respectively.\nDue to the combinatoric complexity of the task, one\nlimitation of the skip-gram method is that execution times\nbecome unfeasible beyond certain values of nandt. Nev-\nertheless, if the organizational principles underlying hier-\narchical stimulus domains like natural language or poly-\nphonic music reﬂect limitations of human auditory pro-\ncessing, it seems reasonable to impose similar restrictions\non the sorts of contiguous and non-contiguous relations\nthe skip-gram method should model. Given the restric-\ntions imposed in this study, retrieving all 4-gram tokens\nfrom a sequence of 1,000 chords using commodity hard-\nware produced runtimes of less than 100ms in the largest\nﬁxed-skip condition ( t= 4 skips), and less than 3s in the\nlargest variable-skip condition ( t= 2s), proving skip-gram\nmodeling is entirely attainable in a research setting.\nOf course, counting all possible skip-grams in this way\nassumes no a priori knowledge about the sorts of non-\ncontiguous relations analysts might hope to discover. For\nexample, collocation extraction algorithms in the NLP\ncommunity typically exclude infrequent n-grams, or use\nparts-of-speech tags to privilege syntactically meaningful\nutterances [22]. Music researchers could adopt similar\nmethods by excluding (or weighting) each n-gram by the\ntemporal proximity or periodicity of its members [21], or\nprivileging patterns that appear in strong metric positions\nor feature changes of harmony. Together with the skip-\ngram method, these techniques could usher in a new suite\nof inductive, data-driven tools for the discovery of musical\norganization.\n6. ACKNOWLEDGMENTS\nThis research is supported by the European Research\nCouncil (ERC) under the EUs Horizon 2020 Frame-\nwork Programme (ERC Grant Agreement number 670035,\nproject “Con Espressione”).Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 3377. REFERENCES\n[1] J. J. Bharucha and C. L. Krumhansl. The representation\nof harmonic structure in music: Hierarchies of stability\nas a function of context. Cognition , 13:63–102, 1983.\n[2] J. G. Cleary and I. H. Witten. Data compression\nusing adaptive coding and partial string matching.\nIEEE Transactions on Communications , 32(4):396–\n402, 1984.\n[3] T. Collins, A. Arzt, H. Frostel, and G. Widmer.\nUsing geometric symbolic ﬁngerprinting to discover\ndistinctive patterns in polyphonic music corpora. In\nD. Meredith, editor, Computational Music Analy-\nsis, pages 445–474. Springer International Publishing,\nCham, 2016.\n[4] D. Conklin. Representation and discovery of vertical\npatterns in music. In C. Anagnostopoulou, M. Fer-\nrand, and A. Smaill, editors, Music and Artiﬁcal Intel-\nligence: Lecture Notes in Artiﬁcial Intelligence 2445 ,\nvolume 2445, pages 32–42. Springer-Verlag, 2002.\n[5] D. Conklin. Multiple viewpoint systems for music clas-\nsiﬁcation. Journal of New Music Research , 42(1):19–\n26, 2013.\n[6] M. S. Cuthbert and C. Ariza. music21: A toolkit for\ncomputer-aided musicology and symbolic music data.\nIn J. S. Downie and R. C. Veltkamp, editors, Proc. 11th\nInternational Society for Music Information Retrieval\n(ISMIR) , pages 637–642, 2010.\n[7] T. G. Dietterich. Approximate statistical tests for com-\nparing supervised classiﬁcation learning algorithms.\nNeural Computation , 10(7):1895–1923, 1998.\n[8] S. Flossmann. Expressive Performance Rendering with\nProbabilistic Models — Creating, Analyzing, and Us-\ning the Magaloff Corpus . Phd thesis, Johannes Kepler\nUniversity, Linz, Austria, 2010.\n[9] S. Flossmann, W. Goebl, M. Grachten, B. Nie-\ndermayer, and G. Widmer. The Magaloff project:\nAn interim report. Journal of New Music Research ,\n39(4):363–377, 2010.\n[10] P. Fraisse. Rhythm and tempo. In D. Deutsch, editor,\nThe Psychology of Music , pages 149–180. Academy\nPress, New York, 1982.\n[11] R. O. Gjerdingen. “Historically informed” corpus stud-\nies.Music Perception , 31(3):192–204, 2014.\n[12] J. T. Goodman. A bit of progress in language modeling.\nComputer Speech & Language , 15:404–434, 2001.\n[13] D. Guthrie, B. Allison, W. Liu, L. Guthrie, and\nY . Wilks. A closer look at skip-gram modelling. In\nProc. 5th International Conference on Language Re-\nsources and Evaluation (LREC-2006) , pages 1222–\n1225. European Language Resources Association,\n2006.[14] D. Huron. The Humdrum Toolkit: Software for Music\nResearch . Center for Computer Assisted Research in\nthe Humanities, Stanford, CA, 1993.\n[15] E. H. Margulis and A. P. Beatty. Musical style, psy-\nchoaesthetics, and prospects for entropy as an analytic\ntool. Computer Music Journal , 32(4):64–78, 2008.\n[16] D. M ¨ullensiefen and M. Pendzich. Court decisions on\nmusic plagiarism and the predictive value of similar-\nity algorithms. Musicæ Scientiæ , Discussion Forum\n4B:257–295, 2009.\n[17] M. T. Pearce. The Construction and Evaluation of Sta-\ntistical Models of Melodic Structure in Music Per-\nception and Composition . Phd thesis, City University,\nLondon, 2005.\n[18] M. T. Pearce and G. A. Wiggins. Improved methods for\nstatistical modelling of monophonic music. Journal of\nNew Music Research , 33(4):367–385, 2004.\n[19] I. Quinn. Are pitch-class proﬁles really “key for key”?\nZeitschrift der Gesellschaft der Musiktheorie , 7:151–\n163, 2010.\n[20] I. Quinn and P. Mavromatis. V oice-leading prototypes\nand harmonic function in two chorale corpora. In\nC. Agon, E. Amiot, M. Andreatta, G. Assayag, J. Bres-\nson, and J. Manderau, editors, Mathematics and Com-\nputation in Music , pages 230–240. Springer, Heidel-\nberg, 2011.\n[21] D. R. W. Sears. The Classical Cadence as a Closing\nSchema: Learning, Memory, and Perception . Phd the-\nsis, McGill University, Montreal, Canada, 2016.\n[22] F. Smadja. Retrieving collocations from text: Extract.\nComputational Linguistics , 19(1):143–177, 1993.\n[23] P. G. V os and J. M. Troost. Ascending and descending\nmelodic intervals: Statistical ﬁndings and their percep-\ntual relevance. Music Perception , 6(4):383–396, 1989.\n[24] G. Widmer. Using AI and machine learning to study\nexpressive music performance: Project survey and ﬁrst\nreport. AI Communications , 14(3):149–162, 2001.\n[25] G. Widmer. Discovering simple rules in complex data:\nA meta-learning algorithm and some surprising mu-\nsical discoveries. Artiﬁcial Intelligence , 146:129–148,\n2003.\n[26] J. Williams, P. R. Lessard, S. Desu, E. M. Clark,\nJ. P. Bagrow, C. M. Danforth, and P. S. Dodds. Zipf’s\nlaw holds for phrases, not words. Scientiﬁc Reports ,\n5(12209), 2015.\n[27] I. H. Witten and T. C. Bell. The zero-frequency prob-\nlem: Estimating the probabilities of novel events in\nadaptive text compression. IEEE Transactions on In-\nformation Theory , 37(4):1085–1094, 1991.338 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Onset Detection in Composition Items of Carnatic Music.",
        "author": [
            "Jilt Sebastian",
            "Hema A. Murthy"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1414830",
        "url": "https://doi.org/10.5281/zenodo.1414830",
        "ee": "https://zenodo.org/records/1414830/files/SebastianM17.pdf",
        "abstract": "Complex rhythmic patterns associated with Carnatic music are revealed from the stroke locations of percussion instru- ments. However, a comprehensive approach for the detec- tion of these locations from composition items is lacking. This is a challenging problem since the melodic sounds (typically vocal and violin) generate soft-onset locations which result in a number of false alarms. In this work, a separation-driven onset detection ap- proach is proposed. Percussive separation is performed us- ing a Deep Recurrent Neural Network (DRNN) in the first stage. A single model is used to separate the percussive vs the non-percussive sounds using discriminative train- ing and time-frequency masking. This is then followed by an onset detection stage based on group delay (GD) pro- cessing on the separated percussive track. The proposed approach is evaluated on a large dataset of live Carnatic music concert recordings and compared against percussive separation and onset detection baselines. The separation performance is significantly better than that of Harmonic- Percussive Separation (HPS) algorithm and onset detec- tion performance is better than the state-of-the-art Con- volutional Neural Network (CNN) based algorithm. The proposed approach has an absolute improvement of 18.4% compared with the detection algorithm applied directly on the composition items.",
        "zenodo_id": 1414830,
        "dblp_key": "conf/ismir/SebastianM17",
        "keywords": [
            "Carnatic music",
            "percussion instruments",
            "rhythmic patterns",
            "detec-tion",
            "composition items",
            "Deep Recurrent Neural Network",
            "onset detection",
            "group delay",
            "live recordings",
            "state-of-the-art CNN"
        ],
        "content": "ONSET DETECTION IN COMPOSITION ITEMS OF CARNATIC MUSIC\nJilt Sebastian\nIndian Institute of Technology, Madras\njiltsebastian@gmail.comHema A. Murthy\nIndian Institute of Technology, Madras\nhema@cse.itm.ac.in\nABSTRACT\nComplex rhythmic patterns associated with Carnatic music\nare revealed from the stroke locations of percussion instru-\nments. However, a comprehensive approach for the detec-\ntion of these locations from composition items is lacking.\nThis is a challenging problem since the melodic sounds\n(typically vocal and violin) generate soft-onset locations\nwhich result in a number of false alarms.\nIn this work, a separation-driven onset detection ap-\nproach is proposed. Percussive separation is performed us-\ning a Deep Recurrent Neural Network (DRNN) in the ﬁrst\nstage. A single model is used to separate the percussive\nvs the non-percussive sounds using discriminative train-\ning and time-frequency masking. This is then followed by\nan onset detection stage based on group delay (GD) pro-\ncessing on the separated percussive track. The proposed\napproach is evaluated on a large dataset of live Carnatic\nmusic concert recordings and compared against percussive\nseparation and onset detection baselines. The separation\nperformance is signiﬁcantly better than that of Harmonic-\nPercussive Separation (HPS) algorithm and onset detec-\ntion performance is better than the state-of-the-art Con-\nvolutional Neural Network (CNN) based algorithm. The\nproposed approach has an absolute improvement of 18.4%\ncompared with the detection algorithm applied directly on\nthe composition items.\n1. INTRODUCTION\nDetecting and characterizing musical events is an impor-\ntant task in Music Information Retrieval (MIR), especially\nin Carnatic music, which has a rich rhythm repertoire.\nThere are seven different types of repeating rhythmic pat-\nterns known as t¯alas, which when combined with 5 j¯atis\ngive rise to 35 combinations of rhythmic cycles of ﬁxed\nintervals. By incorporating 5 further variations called\ngati/nadai , 175 rhythmic cycles are obtained [13]. A t¯ala\ncycle is made up of m¯atr¯as, which in turn are made up of\nakshar ¯as or strokes at the fundamental level. Another com-\nplexity in Carnatic music is that the start of the t¯alacycle\nand of the composition need not be synchronous. Never-\nc\rJilt Sebastian, Hema A. Murthy. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Jilt Sebastian, Hema A. Murthy. “ONSET DETECTION IN\nCOMPOSITION ITEMS OF CARNATIC MUSIC”, 18th International\nSociety for Music Information Retrieval Conference, Suzhou, China,\n2017.theless, percussion keeps track of rhythm . The detection\nof percussive syllable locations aids higher level retrieval\ntasks such as akshar ¯atranscription, sama (start of t¯ala) and\ned.uppu (start of composition) detection and t¯alatracking.\nVarious methods have been proposed for detecting on-\nsets from music signals using a short-term signal, the lin-\near prediction error signal, spectral magnitude or phase,\nenergy and their combination [1, 3, 11, 14, 15]. In [2], var-\nious acoustic features are analyzed for this task and in [7],\nspectral methods are modiﬁed to enable onset detection.\nThese and other algorithms are analyzed in detail in [5].\nRecent efforts include the use of Recurrent (RNN) [17] and\nConvolutional Neural Networks (CNN) [19] for onset de-\ntection. All of the above techniques are primarily for the\ndetection of monophonic musical onsets.\nEvery item in Carnatic music has, at its core, a compo-\nsition. Every item in a concert is characterized by three\nsections. A lyrical composition section that is performed\ntogether by the lead performer, accompanying violinist and\nthe percussion artist. This section is optionally preceded\nby a pure melody section ( ¯al¯apana ) in which only the lead\nperformer and the accompanying violinist perform. The\ncomposition section is optionally followed by a pure per-\ncussion section ( tani ¯avarthanam ). Onset detection and\nakshar ¯atranscription in tani ¯avarthanam s are performed\nin [15], and [16] respectively. Percussive onset detection\nfor an entire concert that is made up of 10-12 items, each\nassociated with its own t¯alacycle, is still challenging as\nthe composition items are made up of ensembles of a lead\nvocal, violin/ensembles of the lead instrument(s) and per-\ncussion.\nOnset detection in polyphonic music/ensemble of per-\ncussion either use audio features directly [4], or performs\ndetection on the separated sources. Dictionary learning-\nbased methods using templates are employed in the sep-\naration stage in certain music traditions [10, 22]. Har-\nmonic/percussive separation (HPS) from the audio mixture\nis successfully attempted on Western music in [8] and [9].\nOnset detection of notes is performed on polyphonic music\nin [4] for transcription. Efﬁcient percussive onset detection\non monaural music mixtures is still a challenging prob-\nlem. The current approaches lead to a signiﬁcant number\nof false positives, owing to the difﬁculty in detecting only\nthe percussive syllables with varying amplitudes and the\npresence of melodic voices.\nIn a Carnatic music concert, the lead artist and all the\naccompanying instruments are tuned to the same base fre-\nquency called tonic frequency and it may vary for each560concert. This leads to the overlapping of pitch trajec-\ntories. The bases do not vary over time in the case of\ndictionary-based separation methods, leading to a limited\nperformance in Carnatic music renderings. HPS model [8]\ndoes not account for the melodic component and variation\noftonic across the concerts. The state-of-the-art solo on-\nset detection techniques, when applied to the polyphonic\nmusic, perform poorer ( \u001920% absolute) than on the solo\nsamples [22].\nIn this paper, a separation-driven approach for percus-\nsive onset detection is presented. A deep recurrent model\n(DRNN) is used to separate the percussion from the com-\nposition in the ﬁrst stage. It is followed by the onset detec-\ntion based on signal processing in the ﬁnal stage. The pro-\nposed approach achieves signiﬁcant improvement (18.4%)\nover the onset detection algorithm applied to the mixture\nand gracefully degrades (about 4.6% poorer) with respect\nto onset detection on solo percussion. The proposed ap-\nproach has better separation and detection performance,\nwhen compared to that of the baseline algorithms.\n2. DATASETS\nMulti-track recordings of six live vocal concerts ( '14\nhours) are considered for extracting the composition items.\nThese items contain composition segments with vocal\nand/or violin segments in ﬁrst track and percussive seg-\nments in the second track. To create the ground truth,\nonsets are marked (manually by the authors) in the per-\ncussive track. These onsets are veriﬁed by a professional\nartist1. Details of the datasets prepared from various con-\ncerts are given in Table 1. The composition items consist\nof recordings from both male and female artists sampled at\n44.1 kHz. Some of the strokes in the mridangam are de-\npendent on the tonic, while others are not. The concerts\nSS and KD also include ghatam andkhanjira , which are\nsecondary percussion instruments. Recordings are also af-\nfected by nearby sources, background applauses and the\nperpetual drone .\nConcertTotal Length Comp. SegmentsNo. of Strokeshh:mm:ss mm:ss (Number)\nKK 2:15:50 1:52 (3) 541\nSS 2:41:14 0:38(4) 123\nMH 2:31:47 1:16 (3) 329\nND 1:15:20 1:51 (3) 330\nMO 2:00:15 7:14 (3) 1698\nKD 2:20:23 5:32 (3) 1088\nTotal 13:41:59 18:23 (19) 4109\nTable 1 :Details of the dataset\nTraining examples for the percussion separation stage\nare obtained from the ¯al¯apana (vocal solo, violin solo) and\nmridangam tani¯avarthanam segments. These are mixed to\ncreate the polyphonic mixture. A total of 12 musical clips\nare extracted from four out of six recordings, to obtain the\ntraining set (17min and 5s), and the validation set (4min\nand 10s). Hence, around 43% of the data is found to be suf-\n1Thanks to musician Dr. Padmasundari for the veriﬁcationﬁcient for training. 10% of the dataset is used for the val-\nidation of neural network parameters and the rest for test-\ning the separation performance. The concert segments KK\nand ND are only used for testing the proposed approach\nto check the generalizability of the approach across vari-\nous concerts. The composition segments shown in Table 1\ncolumn 3 (with ground truth) are used as the test data. On-\nset detection is then performed on the separated percussive\ntrack.\nFigure 1 :Block diagram of the proposed approach.\n3. PROPOSED APPROACH\nThe proposed method consists of two stages: percussive\nseparation stage and solo onset detection stage. Initially,\nthe time-frequency masks speciﬁc to percussive voices\n(mainly mridangam) are learned using a DRNN frame-\nwork. The separated percussion source is then used as in-\nput to the onset detection algorithm. Figure 1 shows the\nblock diagram of the overall process which is explained\nsubsequently in detail.\n3.1 Percussive Separation Stage\nA deep recurrent neural network framework originally pro-\nposed for singing voice separation [12] is adopted for sep-\narating the percussion from the other voices. ¯Al¯apana seg-\nments are mixed with tani¯avarthanam segments for learn-\ning the timbral patterns corresponding to each source. Fig-\nure 2 shows the time-frequency patterns of the composi-\ntion mixture segment, melodic mixture and the percussive\nsource in Carnatic music. The patterns associated with dif-\nferent voices are mixed in composition segments leading\nto a fairly complex magnitude spectrogram (Figure 2 left)\nwhich makes separation of percussion a nontrivial task.\nThe DRNN architecture for percussive separation stage is\nshown in Figure 3. The network takes the feature vec-\ntor corresponding to the composition items ( xt) and esti-\nmates the mask corresponding to the percussive ( y01\nt) and\nnon-percussive ( y02\nt) sources. The normalized mask corre-\nsponding to the percussive source ( M1(f)) is used to ﬁlter\nthe mixture spectrum and then combined with the mixture\nphase to obtain the complex-valued percussive spectrum:\nbSp(f) =M1(f)Xt(f) (1)\nSp(t) =ISTFT (bSp6Xt) (2)Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 561Figure 2 :Spectrograms of a segment of composition (left) ob-\ntained from the mixture (KK dataset) containing melodic sources,\nvocal and violin (middle) and the percussive source (right) .\nwhere, ISTFT refers to inverse short-time Fourier trans-\nform,bSpis the estimated percussive spectrum, 6(Xt)is the\nmixture phase at time tand, Sp(t)is the percussive signal\nestimated for tthtime frame.\nWe use the short-time Fourier transform (STFT) feature\nas it performs better than conventional features in musical\nsource separation tasks [21]. The regression problem of\nﬁnding the source speciﬁc-magnitude spectrogram is for-\nmulated as a binary mask estimation problem where each\ntime-frequency bin is classiﬁed as either percussive or non-\npercussive voice. The network is jointly optimized with the\nnormalized masking function ( M1(f)) by adding an extra\ndeterministic layer to the output layer. We use a single\nmodel to learn both these masks despite the fact that only\npercussive sound is required in the second stage. Thus, dis-\ncriminative information is also used for the learning prob-\nlem. The objective function (Mean Squared Error) that is\nminimized is given by:\njjby1t\u0000y1tjj2+jjby2t\u0000y2tjj2\u0000g(jjby1t\u0000y2tjj2+jjby2t\u0000y1tjj2)\n(3)\nwherebytandytare the estimated and original magnitude\nspectra respectively. The gparameter is optimized such\nthat more importance is given to minimizing the error for\nthe percussive voices than maximizing the difference with\nrespect to the other sources. This is primarily to ensure\nthat the characteristics of percussive voice are not affected\nsigniﬁcantly by separation, as the percussive voice will be\nused later for onset detection. The recurrent connections\nare employed to capture the temporal dynamics of the per-\ncussive source which are not captured using the contextual\nwindows. The network has a recurrent connection at the\nsecond hidden layer and is parametrically chosen based on\nthe performance on development data. The second hidden\nlayer output is calculated from the current input and output\nof the same hidden layer in the previous time-step as:\nh2(xt) =f(W2h2(xt)+b2+V2h2(xt\u00001)) (4)\nwhere, WandVare the weight matrices, Vbeing the tem-\nporal weight matrix and the function f(\u0001)is the ReLU ac-\ntivation [12].\nA recurrent network trained with ¯Al¯apana and tani\n¯avarthanam separates the percussion from the voice by\ngenerating a time-frequency percussive mask. This mask\n2Example redrawn from [12]\nFigure 3 :Percussive separation architecture2\nis used to separate the percussive voice in the composition\nsegment of a Carnatic music item. The separated signal is\nused for onset detection in the next stage (Figure 1).\n3.2 Onset Detection Stage\nThe separated percussive voice is used as the source sig-\nnal for the onset detection task. Note that this signal has\nother source interferences, artifacts and other distortions.\nThe second block in Figure 1 corresponds to the onset de-\ntection stage. Onset detection consists of two steps. In the\nﬁrst step a detection function is derived from the percus-\nsive strokes which is then used in onset detection in the\nsecond step.\nIt is observed that the percussive strokes in Carnatic mu-\nsic can be modeled by an AM-FM signal based on am-\nplitude and frequency variations in the vicinity of an on-\nset [15]. An amplitude and frequency modulated signal\n(x(t)) is given by,\nx(t) =m1(t)cos(wct+kfZ\nm2(t)dt) (5)\nwhere, kfis the frequency modulation factor, wcis the car-\nrier frequency and, m1(t)andm2(t)are the message sig-\nnals. The changes in the frequency are emphasized in the\namplitude of the waveform by ﬁnding the differences of\nthe time-limited discrete version of the signal, x[n]. The\nenvelope function e[n]is the amplitude part of x0[n]. The\nreal-valued envelope signal can be represented by the cor-\nresponding analytic signal deﬁned as:\nea[n] =e[n]+ieH[n] (6)\neH[n]is the Hilbert transform of the envelope function.\nThe magnitude of ea[n]is the detection function for the\nonsets. The high-energy positions of the envelope signal\n(e[n]) corresponds to the onset locations. However, these\npositions have a large dynamic range and the signal has\na limited temporal resolution. It has been shown in [20]\nthat minimum-phase group delay (GD) based smoothing562 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017-0.2 0 0.2\n 0  20000  40000  60000  80000  100000  120000  140000(a)\n-0.05 0 0.05\n 0  20000  40000  60000  80000  100000  120000  140000(a) (b)\n 0 0.002 0.004 0.006\n 0  2000  4000  6000  8000  10000  12000  14000(a) (b) (c)\n-1-0.6-0.2 0.2 0.6 1\n 0  2000  4000  6000  8000  10000  12000  14000\nTime in Samples(a) (b) (c) (d)Figure 4 :Solo onset detection algorithm. (a) Percussion signal (b) Derivative of (a). c) Envelope estimated on (b) using Hilbert\ntransform. (d) Minimum phase group delay computed on (c).\nleads to a better resolution for any positive signal that is\ncharacterized by peaks and valleys. The envelope is a non-\nminimum phase signal and it needs to be converted to a\nminimum phase equivalent to apply this processing.\nIt is possible to derive such an equivalent representation\nwith a root cepstral representation. The causal portion of\nthe inverse Fourier transform of the magnitude spectrum\nraised to a power of ais always minimum phase [18].\ne0[k] =fs[k]jk>0;s[k] =IFT((e(n)+e[\u0000n])a)g(7)\nNote that e0[k]is in root cepstral domain and kis the que-\nfrency index. This minimum-phase equivalent envelope is\nthen subjected to group delay processing.\nThe group delay is deﬁned as negative frequency deriva-\ntive of the unwrapped phase function. It can be computed\ndirectly from the cepstral domain input signal e0[k]as:\nt(w) =XR(ejw)YR(ejw)+XI(ejw)YI(ejw)\njX(ejw)j2(8)\nwhere, X(ejw)andY(ejw)are the discrete Fourier trans-\nforms of e0[k]andne0[k]respectively. Also, RandIdenote\nthe real and imaginary parts respectively. The high reso-\nlution property of the group delay domain emphasizes the\nonset locations. Onsets are reported as instants of signiﬁ-\ncant rise, above a threshold.\nFigure 4 illustrates the different steps in the algorithm\nusing a mridangam excerpt taken from a tani ¯avarthanam\nsegment. It is interesting to note that in the ﬁnal step, the\ngroup delay function emphasizes all the strokes approx-\nimately to an equal amplitude, and even those onsets inwhich there is no noticeable change in amplitude are also\nobtained as peaks (highlighted area in Figure 4).\n4. PERFORMANCE EVALUATION\nThe proposed percussive onset detection approach is de-\nveloped speciﬁcally for rhythm analysis in Carnatic music\ncomposition items. However, it is instructive to compare\nthe performance with other separation and onset detection\nalgorithms. Also, it is important to note that the proposed\napproach could be applied to any music tradition with\nenough training musical excerpts to extract the onset lo-\ncations from the polyphonic mixture. The dataset for these\ntasks is described in Section 2. The vocal-violin channel\n(¯al¯apana ) and the percussion channel ( tani ¯avarthanam )\nare mixed at 0 dB SNR. The STFT with a window length\nof 1024 samples and hop size of 512 samples is used as the\nfeature for training a DRNN with 3 hidden layers (1000\nunits/layer) and temporal connection at the 2ndlayer. This\narchitecture shows a very good separation for the singing\nvoice separation task [12]. The dataset consists of seg-\nments with varying tempo, loudness and number of sources\nat a given time. The challenge lies in detecting the onsets\nin the presence of the interference caused by other sources\nand the background voices.\n4.1 Evaluation Metrics\nSince the estimation of percussive onsets also depends on\nthe quality of separation, it is necessary to evaluate the sep-\narated track. We measure this using three quantitative mea-\nsures based on BSS-EV AL 3.0 metrics [23]: Source to Ar-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 563tifacts Ratio (SAR), Source to Interference Ratio (SIR) and\nSource to Distortion Ratio (SDR). The artifacts introduced\nin the separated track is measured by SAR. The suppres-\nsion achieved for the interfering sources (vocal and violin)\nis represented in terms of SIR which is an indicator of the\ntimbre differences between the vocal-violin mixture and\npercussive source. SDR gives the overall separation qual-\nity. The length-weighted means of these measures are used\nfor representing the overall performance in terms of global\nmeasures (GSAR, GSIR and GSDR).\nThe conventional evaluation metric for the onset detec-\ntion is F-measure, which is the harmonic mean of precision\nand recall. An onset is treated as correct ( True Positive )\nif it is reported within a \u000650ms threshold of the ground\ntruth [6] as strokes inside this interval are usually unre-\nsolvable. Additionally, this margin accounts for the pos-\nsible errors in the manual annotation. The F-measure is\ncomputed from sensitivity and precision. Since it is im-\npossible to differentiate between simple and composite3\nstrokes for mridangam, the closely spaced onsets (within\n30ms) are not merged together unlike in [5].\n4.2 Comparison Methods\nThe performance of the separation stage is compared with\na widely used Harmonic/Percussive Separation (HPS) al-\ngorithm [8] for musical mixtures. It is a signal processing-\nbased algorithm in which median ﬁltering is employed on\nthe spectral features for separation. Other supervised per-\ncussive separation models were speciﬁc to the music tra-\nditions. We have not considered the Non negative Matrix\nFactorization (NMF)-based approaches since the separa-\ntion performance was worse on Carnatic music, hinting the\ninability of a constant dictionary to capture the variability\nacross the percussive sessions and instruments.\nThe onset detection performance is compared with the\nstate-of-the-art CNN-based onset detection approach [19].\nIn this approach, a convolutional network is trained as a\nbinary classiﬁer to predict whether the given set of frames\nhas an onset or not. It is trained using percussive and\nnon percussive solo performances. We evaluate the per-\nformance of this algorithm on the separated percussive\ntrack and, on the mixture . The onset threshold amplitude\nis optimized with respect to the mixture and percussive\nsolo channel for evaluating the performance on the sepa-\nrated and mixture tracks respectively for both of these al-\ngorithms.\n5. RESULTS AND DISCUSSION\n5.1 Percussive Separation\nThe results of percussive separation are compared with that\nof the HPS algorithm in Table 2. The large variability\nof the spectral structure with respect to the tonic , strokes\nand the percussive instruments (different types of mridan-\ngam as well) cause the HPS model to perform poorly with\nrespect to the proposed approach. The DRNN separa-\ntion beneﬁts from the training whereas the presence of the\n3both left and right strokes co-occurring in the mridangamDRNN HPS\nConcert GSDR GSIR GSAR GSDR GSIR GSAR\nSS 7.00 13.70 8.61 3.39 6.73 7.93\nND 7.54 17.30 8.98 0.46 3.05 7.67\nKK 7.37 13.93 8.93 0.66 2.04 10.09\nMH 6.40 15.64 7.63 0.82 3.31 7.79\nKR 7.37 13.93 8.93 1.32 2.43 9.09\nMD 6.40 15.64 7.63 2.40 8.06 4.78\nAverage 7.01 15.02 8.45 1.50 4.27 7.89\nTable 2 :Percussive separation performance in terms of BSS\nevaluation metrics for the proposed approach and HPS algorithm\nmelodic component with rich harmonic content adds to the\ninterference in HPS method. This results in a poor sep-\naration of melodic mixture and percussive voice in HPS\napproach as indicated by an overall difference of 5.51 dB\nSDR with respect to DRNN approach. Although DRNN is\nnot trained on the concerts KK and MD, separation mea-\nsures are quite similar to other concerts. This is an indi-\ncator of the generalization capability of the network since\neach concert is of a unique tonic (base) frequency, and is\nrecorded under a different environment. Separated sound\nexamples are available online4.\n5.2 Onset Detection\nConcert Proposed Direct Solo CNN CNN Sep.\nSS 0.747 0.448 0.864 0.685 0.656\nND 0.791 0.650 0.924 0.711 0.740\nKK 0.891 0.748 0.972 0.587 0.636\nMH 0.874 0.687 0.808 0.813 0.567\nKR 0.891 0.748 0.972 0.859 0.848\nMD 0.874 0.687 0.808 0.930 0.919\nAverage 0.845 0.661 0.891 0.764 0.727\nTable 3 :Comparison of F-measures for the proposed approach,\ndirect onset detection on the mixture, solo percussion channel,\nCNN on the mixture and on the separated percussive channel.\nThe accuracy of onset detection is evaluated using F-\nmeasure in Table 3. The performance varies with the\ndataset and the results with the maximum average F-\nmeasure is reported. The degradation in performance with\nrespect to the solo source is only about 4.6%, while the\nimprovement in performance compared to the direct onset\ndetection on the composite source is 18.4%. The separa-\ntion step plays a crucial role in onset detection of the com-\nposition items as the performance has improved for allthe\ndatasets upon separation. It should be noted that the al-\ngorithm performs really well for solo percussive source.\nThis is reason for making comparisons with solo perfor-\nmances. For SS data (Table 1) with fast tempo (owing to\nmultiple percussive voices) and signiﬁcant loudness vari-\nation (Example online4), the direct onset method causes\na large number of false positives resulting in lower preci-\nsion whereas the proposed approach results in a reduced\nnumber of false positives. Figure 5 shows an example of a\n4https://sites.google.com/site/\npercussiononsetdetection564 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017−0.0200.02Amplitude\n(a) A segment of composition item with the ground truth onsets  \n−0.400.4GD Amplitude\n(b) Group delay representation for the mixture signal with the detected onsets\n0 0.045 0.09 0.135 0.18 0.225 0.27−0.500.5GD Amplitude\n(c) Group delay representation for the separated signal with the detected onsetsTime in Seconds  Figure 5 : An excerpt from SS dataset illustrating the performance of the proposed approach with respect to the direct onset\ndetection method. Red dotted lines represent the ground truth onsets, violet (b) and green (c) lines represent the onsets\ndetected on the mixture signal and the separated percussive signal respectively.\ncomposition item taken from the SS dataset. It compares\nthe performance of the proposed approach with that of the\nonset detection algorithm applied directly on the mixture.\nBy adjusting the threshold of onset, the number of false\npositives can be reduced. However, it leads to false neg-\natives as shown in Figure 5(b). The proposed approach is\nable to detect almost all of the actual onset locations (5(c)).\nThe proposed approach is then compared with the CNN\nalgorithm. The optimum threshold of the solo algorithm\nfor the Carnatic dataset [15] is used to evaluate the per-\nformance. The proposed method performs better than the\nCNN algorithm applied on the mixture (Table 3). This\nis because the CNN method is primarily for solo onset\ndetection. The performance of the baseline on the sepa-\nrated channel is also compared with the group delay-based\nmethod. The threshold is optimized with respect to the per-\nformance of the baseline algorithm on the mixture track.\nThe average F-measure of the proposed approach is 11.8%\nbetter than that of the CNN-based algorithm. This is be-\ncause CNN-based onset detection requires different thresh-\nolds for different concert segments. This suggests that\nthe GD based approach generalizes better in the separated\nvoice track and is able to tolerate the inter-segment vari-\nability. A consistently better F-measure is obtained by the\nGD based method across all recordings. This separation-\ndriven algorithm can be extended to any music tradition\nwith sharp percussive onsets and having enough numberof polyphonic musical ensembles for the training. These\nonset locations can be used to extract the strokes of per-\ncussion instruments and perform t¯alaanalysis.\n6. CONCLUSION AND FUTURE WORK\nA separation-driven approach for percussive onset detec-\ntion in monaural music mixture is presented in this paper\nwith a focus on Carnatic music. Owing to its tonic depen-\ndency and improvisational nature, conventional dictionary-\nbased learning methods perform poorly on percussion sep-\naration in Carnatic music ensembles. V ocal and violin seg-\nments from the ¯al¯apana and mridangam phrases from the\ntani ¯avarthanam of concert recordings are used to train a\nDRNN for the percussive separation stage. The separated\npercussive source is then subjected to onset detection. The\nperformance of the proposed approach is comparable to\nthat of the onset detection applied on the solo percussion\nchannel and achieves 18.4% absolute improvement over its\ndirect application to the mixture. It compares favourably\nwith the separation and onset detection baselines on the\nsolo and separated channels. The onset locations can be\nused for analyzing the percussive strokes. Using repeat-\ning percussion patterns, the t¯alacycle can be ascertained.\nThis opens up a plethora of future tasks in Carnatic MIR.\nMoreover, the proposed approach is generalizable to other\nmusic traditions which include percussive instruments.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 5657. ACKNOWLEDGEMENTS\nThis research is partly funded by the European Research\nCouncil under the European Unions Seventh Framework\nProgram, as part of the CompMusic project (ERC grant\nagreement 267583). Authors would like to thank the mem-\nbers of Speech and Music Technology Lab for their valu-\nable suggestions.\n8. REFERENCES\n[1] Juan P Bello, Chris Duxbury, Mike Davies, and Mark\nSandler. On the use of phase and energy for musical\nonset detection in the complex domain. IEEE Signal\nProcessing Letters , 11(6):553–556, 2004.\n[2] Juan Pablo Bello, Laurent Daudet, Samer Abdal-\nlah, Chris Duxbury, Mike Davies, and Mark B San-\ndler. A tutorial on onset detection in music signals.\nIEEE Transactions on Speech and Audio Processing ,\n13(5):1035–1047, 2005.\n[3] Juan Pablo Bello and Mark Sandler. Phase-based note\nonset detection for music signals. In International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , volume 5, pages V–441. IEEE, 2003.\n[4] Emmanouil Benetos and Simon Dixon. Polyphonic\nmusic transcription using note onset and offset de-\ntection. In Proc. of the International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 37–40. IEEE, 2011.\n[5] Sebastian Böck, Florian Krebs, and Markus Schedl.\nEvaluating the online capabilities of onset detection\nmethods. In Proc. of the 13th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 49–54, 2012.\n[6] Sebastian Böck and Gerhard Widmer. Local group de-\nlay based vibrato and tremolo suppression for onset\ndetection. In Proc. of the 14th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 361–366, Curitiba, Brazil, November 2013.\n[7] Simon Dixon. Onset detection revisited. In Proc. of the\n9th Int. Conference on Digital Audio Effects (DAFx) ,\npages 133–137, 2006.\n[8] Derry Fitzgerald. Harmonic/percussive separation us-\ning median ﬁltering. In Proceedings of the 13th Inter-\nnational Conference on Digital Audio Effects (DAFx) ,\npages 15–19, 2010.\n[9] Derry Fitzgerald, Antoine Liukus, Zafar Raﬁi, Bryan\nPardo, and Laurent Daudet. Harmonic/percussive sep-\naration using kernel additive modelling. In Proc. of\nthe 25th IET Irish Signals & Systems Conference 2014\nand 2014 China-Ireland International Conference on\nInformation and Communications Technologies (ISSC\n2014/CIICT 2014) , pages 35–40, 2014.[10] Masataka Goto and Yoichi Muraoka. A sound source\nseparation system for percussion instruments. Trans-\nactions of the Institute of Electronics, Information\nand Communication Engineers (IEICE) , 77:901–911,\n1994.\n[11] Masataka Goto and Yoichi Muraoka. Beat tracking\nbased on multiple-agent architecture a real-time beat\ntracking system for audio signals. In Proc. of 2nd In-\nternational Conference on Multiagent Systems , pages\n103–110, 1996.\n[12] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis. Singing-voice separation from\nmonaural recordings using deep recurrent neural net-\nworks. In Proc. of International Society for Music In-\nformation Retrieval (ISMIR) , pages 477–482, 2014.\n[13] M Humble. The development of rhythmic organization\nin indian classical music. MA dissertation, School of\nOriental and African Studies, University of London. ,\npages 27–35, 2002.\n[14] Anssi Klapuri. Sound onset detection by applying psy-\nchoacoustic knowledge. In Proc. of International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , volume 6, pages 3089–3092. IEEE, 1999.\n[15] Manoj Kumar, Jilt Sebastian, and Hema A Murthy.\nMusical onset detection on carnatic percussion instru-\nments. In Proc. of 21st National Conference on Com-\nmunications (NCC) , pages 1–6. IEEE, 2015.\n[16] Jom Kuriakose, J Chaitanya Kumar, Padi Sarala,\nHema A Murthy, and Umayalpuram K Sivaraman. Ak-\nshara transcription of mrudangam strokes in carnatic\nmusic. In Proc. of the 21st National Conference on\nCommunications (NCC) , pages 1–6. IEEE, 2015.\n[17] Erik Marchi, Giacomo Ferroni, Florian Eyben, Stefano\nSquartini, and Bjorn Schuller. Audio onset detection:\nA wavelet packet based approach with recurrent neural\nnetworks. In Proc. of the International Joint Confer-\nence on Neural Networks (IJCNN) , pages 3585–3591,\nJuly 2014.\n[18] T Nagarajan, V K Prasad, and Hema A Murthy. The\nminimum phase signal derived from the magnitude\nspectrum and its applications to speech segmentation.\nInSpeech Communications , pages 95–101, July 2001.\n[19] Jan Schlüter and Sebastian Böck. Improved Musical\nOnset Detection with Convolutional Neural Networks.\nInProc. of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\npages 6979–6983, Florence, Italy, May 2014.\n[20] Jilt Sebastian, Manoj Kumar, and Hema A Murthy. An\nanalysis of the high resolution property of group delay\nfunction with applications to audio signal processing.\nSpeech Communications , pages 42–53, 2016.566 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[21] Jilt Sebastian and Hema A Murthy. Group delay based\nmusic source separation using deep recurrent neural\nnetworks. In Proc. of International Conference on Sig-\nnal Processing and Communications (SPCOM) , pages\n1–5. IEEE, 2016.\n[22] Mi Tian, Ajay Srinivasamurthy, Mark Sandler, and\nXavier Serra. A study of instrument-wise onset detec-\ntion in beijing opera percussion ensembles. In Proc. of\nthe International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 2159–2163, 2014.\n[23] Emmanuel Vincent, Rémi Gribonval, and Cédric\nFévotte. Performance measurement in blind audio\nsource separation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 14(4):1462–1469,\n2006.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 567"
    },
    {
        "title": "Modeling and Digitizing Reproducing Piano Rolls.",
        "author": [
            "Zhengshan Shi",
            "Kumaran Arul",
            "Julius O. Smith III"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416634",
        "url": "https://doi.org/10.5281/zenodo.1416634",
        "ee": "https://zenodo.org/records/1416634/files/ShiAS17.pdf",
        "abstract": "Reproducing piano rolls are among the early music storage mediums, preserving fine details of a piano or organ per- formance on a continuous roll of paper with holes punched onto them. While early acoustic recordings suffer from poor quality sound, reproducing piano rolls benefit from the fidelity of a live piano for playback, and capture all features of a performance in what amounts to an early dig- ital data format. However, due to limited availability of well maintained playback instruments and the condition of fragile paper, rolls have remained elusive and generally inaccessible for study. This paper proposes methods for modeling and digitizing reproducing piano rolls. Starting with an optical scan, we convert the raw image data into the MIDI file format by applying histogram-based image pro- cessing and building computational models of the musical expressions encoded on the rolls. Our evaluations show that MIDI emulations from our computational models are accurate on note level and proximate the musical expres- sions when compared with original playback recordings.",
        "zenodo_id": 1416634,
        "dblp_key": "conf/ismir/ShiAS17",
        "keywords": [
            "Reproducing piano rolls",
            "early music storage mediums",
            "fine details",
            "continuous roll of paper",
            "holes punched onto them",
            "early digital data format",
            "live piano performance",
            "limited availability",
            "fragile paper",
            "unavailable for study"
        ],
        "content": "MODELING AND DIGITIZING REPRODUCING PIANO ROLLS\nZhengshan Shi\nCCRMA\nStanford University\nkittyshi@ccrma.stanford.eduKumaran Arul\nDepartment of Music\nStanford University\nkarul2@stanford.eduJulius O. Smith\nCCRMA\nStanford University\njos@ccrma.stanford.edu\nABSTRACT\nReproducing piano rolls are among the early music storage\nmediums, preserving ﬁne details of a piano or organ per-\nformance on a continuous roll of paper with holes punched\nonto them. While early acoustic recordings suffer from\npoor quality sound, reproducing piano rolls beneﬁt from\nthe ﬁdelity of a live piano for playback, and capture all\nfeatures of a performance in what amounts to an early dig-\nital data format. However, due to limited availability of\nwell maintained playback instruments and the condition\nof fragile paper, rolls have remained elusive and generally\ninaccessible for study. This paper proposes methods for\nmodeling and digitizing reproducing piano rolls. Starting\nwith an optical scan, we convert the raw image data into the\nMIDI ﬁle format by applying histogram-based image pro-\ncessing and building computational models of the musical\nexpressions encoded on the rolls. Our evaluations show\nthat MIDI emulations from our computational models are\naccurate on note level and proximate the musical expres-\nsions when compared with original playback recordings.\n1. INTRODUCTION AND MOTIV ATION\nThe invention of acoustic recordings in the late nineteenth\ncentury is widely accepted as a watershed moment in the\nhistory of music. However, piano rolls, which were in\nwidespread use from approximately 1905 to 1940, are mis-\ntakenly treated as a footnote. Researchers studying early\nacoustic recordings have signiﬁcant challenges with tran-\nscribing the nuances of a performance due to the poor\nsound ﬁdelity, high noise artifacts, and limited recording\nlength. Piano rolls did not share these shortcomings and\nwere praised for their faithfulness and accuracy, providing\na virtual transcription of a performance by punching holes\non a paper scroll. Many important musicians who recorded\non piano rolls never made acoustic recordings and were\namong the oldest generation to be recorded. These include\ncomposers like Claude Debussy, Scott Joplin, and Carl\nReinecke, among others [13]. Modern digital audio work-\nstations were inspired by this old music storage format and\nc\rZhengshan Shi, Kumaran Arul, Julius O. Smith. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Zhengshan Shi, Kumaran Arul, Julius O.\nSmith. “Modeling and Digitizing Reproducing Piano Rolls”, 18th In-\nternational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.provide graphical display of MIDI (Music Instrument Dig-\nital Interface) ﬁles in what usually refers to a piano-roll\neditor. Thus, it can be claimed that piano rolls are among\nthe most signiﬁcant early commercial recordings and data\nstorage media.\nPlaying and recording original rolls has been difﬁcult\ndue to the fragile paper and cumbersome demands of main-\ntaining original instruments. Some recent efforts have pur-\nsued an alternative approach to accessing rolls by design-\ning dedicated roll scanners to create image ﬁles of the rolls.\nThis serves the purpose of archival preservation but does\nnot allow piano rolls to be heard as the musical documents\nthey were originally intended to be.\nIn this paper, we present a method for faithful automatic\nplayback of reproducing piano rolls from image scans. Our\nresearch aims to digitize the data captured through analysis\nof notes, timings, and dynamics. The latter involves decod-\ning proprietary expression mechanisms controlled through\nholes on the sides of the rolls. We evaluate our accuracy by\ncomparing the digital ﬁle with acoustic recordings of rolls\nplayed on original player instruments.\n2. RELATED WORK\nThere has been limited work on digitizing piano rolls by in-\nstitutional researchers and independent hobbyists. Wayne\nStahnke [14] is one of the ﬁrst to scan piano rolls and\nconvert them into a digital format. Stahnke transferred\nimage information derived from a self-built roll scanner\ninto a proprietary data format preserving the details of the\npunched holes. Colmenares et al [2] converted Stahnke’s\npre-processed data for piano rolls into MIDI information\nby using sparse matrices. A few researchers such as\nTrimpin [7], Trachtman [15], and Malosio et al [9] have\nworked on the emulation of piano rolls. Other individuals\nsuch as Anthony Robinson [12] developed scanning ma-\nchines and software that allows manual adjustment of the\npunch holes for reﬁnement and error correction in the pro-\ncess of MIDI generation. These pioneering efforts remain\nhowever largely inaccessible, as key algorithms are not re-\nvealed or evaluated and are unavailable for consideration\nand review. They also typically require intensive manual\nlabor in the transfer process and are usually limited to one\nformat of roll.\nOur work aims to ﬁll this gap by proposing tech-\nniques to digitize and model reproducing piano rolls in-\ncluding novel methods of transcribing the musical expres-197Figure 1 . Excerpt from a Welte-Mignon piano roll image scan, consisting of the left marker, channels 1 to 98, and the right\nmarker.\nsion markings on those piano rolls. We evaluate our system\nby comparing the resultant MIDI emulation to the actual\nacoustic playback of the reproducing piano roll on a player\npiano.\n3. REPRODUCING PIANO ROLLS FORMATS\nA piano roll is a continuous roll of paper with perforations\nthat store musical note data. It captured in real time the\nnotes and rhythms of a pianist playing a special recording\ninstrument. Music recorded on rolls are often performed\nby a player piano, a pneumatic machine that can decode\nthe music data on the perforations and operate the piano\naction. Reproducing piano rolls are standard piano rolls\nwith expression (dynamic and pedal) capabilities that can\nalso be automatically executed by player piano. Expres-\nsions were captured in variable ways but were usually tran-\nscribed in shorthand and then coded onto the roll for bass\nand treble respectively. The complex process allowed for\ndetailed editing of notes, rhythms and expressions.\nThe competitive environment of the early roll business\ncreated multiple different reproducing-piano systems, with\nvariations in the size of rolls, conﬁguration of holes, and\npneumatic player designs. Although industry efforts at\nstandardization eventually created some overlap, a thor-\nough inventory of historical roll systems would number\nover two dozen. Most reproducing rolls of value to schol-\nars are found in the catalogs of a handful of primary play-\ners, including Welte-Mignon (the ﬁrst reproducing roll\nmaker), Ampico, Duo-Art, and Hupfeld. Playing any re-\nproducing roll requires a suitable player piano manufac-\ntured speciﬁcally for that format of roll. In some cases, as\nformats evolved over time, multiple players are needed to\nplay back a manufacturer’s rolls (early Red, T-100 Welte\nrolls do not play on later Green, T-98 Welte players, forexample).\nIn this paper, we focus on the Welte-Mignon Licensee\nformat (the third Welte format, a derivation of the T-98\nrolls). A typical Welte-Mignon Licensee roll is 11 1=4\ninches in width and holes spaced 9 per inch across [5], con-\nsisting of 98 channels1of punched holes [11]. As shown\nin Figure 1, channels 9 through 88 represent notes span-\nning from C1 to G7, with note F#4 as the splitting note for\nbass and treble. Channels 1 through 6 control bass (left-\nhand) expression, and channels 93 through 98 indicate tre-\nble (right-hand) expression. The pedal information is in-\ncluded in channel 7 and 8 (soft pedal) as well as channel\n91 and 92 (sustain pedal). The expression channels com-\nbine to determine the dynamics of the piano performance.\n4. ALGORITHM FOR DIGITIZING\nREPRODUCING PIANO ROLLS\nIn this section, we describe our method for digitizing and\nmodeling the reproducing rolls. We ﬁrst obtain piano roll\nimages through a scanner, then construct a template that\nmatches the layout geometry of the piano roll. Based on\nthe template and locations of the perforations, we recover\nthe note matrix that contain onset times for each note and\nmusical expression. Finally we model musical expressions\ninto dynamics and pedal information, that can be preserved\nin MIDI ﬁles.\n4.1 Scanning\nThe ﬁrst step in digitizing the reproducing-piano rolls is to\ngenerate the image ﬁle by scanning. The scanner used for\nthis project was purposely built for rolls with a transport\n1For the purpose of this paper, we refer to a “channel” as a column of\nthe punched holes.198 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017mechanism that allows continuous image capture by a con-\ntact image sensor (CIS) module which produces a graphic\nimage ﬁle in either CIS or bitmap graphic format. The\nscan produces a directory of bitmap image ﬁles, initially\npreserving the 8-bit grayscale information. Each scanned\nimage is 7296 pixels wide, juxtaposing front and back of\nthe roll onto one side, with a pixel resolution of 324 dots-\nper-inch (dpi). We convert the grayscale raw image data\nto binary and invert the digit, so that 0 indicates no hole\n(white), and 1 indicates a hole coverage (black) at each\npixel. On average, the size of a punched hole is 17 by 28\npixels.\n4.2 Channel Grid Construction\nGiven a ﬁxed piano-roll format, the edges of the zones\nstay sufﬁciently constant due to the general uniformity of\nthe piano rolls. Thus, a quantitative template specifying\npiano-roll layout geometry such as the exact location for\neach channel and the spacing between different zones is\nnecessary. We refer to it as a Channel Grid .\nHowever, to the best of our knowledge, there is no such\nprecise grid of hole placement available. Only general his-\ntorical and empirical evidence from playback are available.\nThus, using documentation of roll formats [11], and the no-\ntation of the musical works recorded, we derived reference\npoints which were used to create a quantitative template\nlocating the notes and expression holes on the roll.\nHistogram-based image processing [6] has been used\npreviously in optical music recognition to determine grid\nlayouts of musical scores. Fujinaga [4] applied the his-\ntogram method to successfully detect staff lines on sheet\nmusic. We applied a similar method for the scanned piano\nroll images by projecting the piano-roll image onto its x\naxis to form a histogram of the holes in each channel. From\nthex-projection, we process each note-channel histogram\nto ﬁnd the center point of each histogram peak relative to\nthe left edge of the piano roll, as illustrated in Figure 2. We\ntake that as deﬁning center lines for each channel. Because\nthe hole-channels are adequately straight and nearly paral-\nlel to the edge of the paper, the xprojection histogram pro-\nduces well separated “channel piles”. The set of all such\nchannel lines forms a grid on the piano roll in which each\nchannel line is a ﬁxed measured distance from the left edge\nof the paper. To map these note-center x-coordinates to\nMIDI note numbers, we use the ﬁrst note of Waltz in E ﬂat\nMajor, Op.42 No.2 by Fr ´ed´eric Chopin as an anchor note,\nnamely E[4 (MIDI note number 63). Under current im-\nage resolution, the median gap between each note channel\nwas found to be approximately 33 pixels, which matches\nthe roll speciﬁcation of 9 holes per inch as mentioned in\nsection 2.\nBased on the distribution of the punched holes as well\nas our manual inspection of the roll image, we further par-\ntition the piano roll into three zones: zone 1 on the left in-\ncludes the bass dynamics, zone 2 in the middle contains all\nthe note information for the 80 keys, and zone 3 spans the\ntreble dynamics, as shown in Figure 1. Our entire collec-\ntion of Welte-Mignon Licensee rolls appears to be compat-\nFigure 2 . (Top left) Scanned image; (Top right) Zoomed-\nin view; (Bottom left) Histogram for top left; (Bottom\nright) Histogram for top right. Note: aspect ratio has been\nmodiﬁed for top left and top right images in order to ﬁt into\nplots.\nible with this template. Thus the template formation based\non one example is found to be sufﬁcient. We are able to\nprescribe the edges of the zones according to the plot of\nchannel histograms generated by the system, as described\nabove and further below.\n4.3 Note Identiﬁcation\nWe create a note matrix based on the channel grid we gen-\nerated. We ﬁrst match each hole with a note on the chan-\nnel grid. We consider a match to occur when a hole inter-\nsects with any note on the channel grid. Note that the size\nof a hole is wider than one pixel. Then we perform a y-\nprojection of the hole to determine the note onset time. The\nnote matrixMis of size 80-by- N, whereNis the length of\nthe scanned roll in pixels, and Mi;j= 1if a punched hole\ncovers the particular pixel. We then convert the sparse ma-\ntrix into readable format, row by row, consisting of MIDI\nnote number and note onset times. We scale the note on-\nset time from pixels to milliseconds. We determine the\nscale factor according to the time information indicated at\nthe beginning of each roll. For example, for a piece with\ntempo mark 70, the roll should be played at a speed of 7\nfeet per minute [5]. We thus deﬁne the scale factor Fto be\nthe length pixels divided by the time it takes to ﬁnish the\npiano roll (in seconds). In the case of the Welte-Mignon\nFormat,F= 455 pixels per second.\nFor long notes, the punching system needs to punch\nmultiple holes closely spaced along the channel grid. Thus\nto obtain the actual note durations, we deﬁne a minimum\nthreshold between two holes as 11 pixels. This threshold\nwas determined empirically based on observations of the\nreference piano roll. If the gap between holes is smaller\nthan this minimum threshold, the holes are considered to\nbelong to the same note.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 199Figure 3 . (A) Excerpt from the original (distorted) image\n(B) Edge proﬁle of the left and right markers for the origi-\nnal image. (C) Excerpt from the corrected image (D) Edge\nproﬁle of the left and right markers for the corrected image.\n4.4 Distortion Correction\nDue to instability in the scanner system, some scanned im-\nage were warped, as shown in Figure 3A. We ﬁx this dis-\ntortion by locating reference lines. On each roll, there are\ntwo straight bold lines marking the region of the punch\nholes. We refer to them as markers , as shown in Figure 1B.\nWe ﬁrst locate the two markers in the roll, and then use\nthem as reference point to evaluate if the paper is warped or\ndistorted. Our system iterates through each row and scale\nthe pixels between the two markers such that the markers\nare enforced to be vertically straight and have a constant\ndistance in between. Thus the imperfection in scanning\ncan be detected and corrected, as in Figure 3C. Figure 3D\nshows straight left and right marker after the distortion cor-\nrection.\n4.5 Modeling Musical Expressions\nNext we decode and model the expression markings on\nthe left and right sides of the piano roll next to the mark-\ners. We obtain the location of each expression channel by\nconstructing an expression channel grid similar to the note\nchannel grid, as illustrated in Figure 4.\nThere are three types of musical expressions in Welte-\nMignon piano rolls: constant velocity, changing velocity,\nand pedal information. The left zone of expression chan-\nnels correspond to bass notes (notes below F ]4), and the\nzone on the right speciﬁes the expression for treble notes\n(notes from G4) with the pedal information controlling the\nwhole register.\nFor the constant-velocity controls, we simply map each\nindicated piano-key velocity to a chosen constant MIDI ve-\nlocity, a seven-bit value between 0 and 127. Based on lis-\nFigure 4 . Musical expression holes in black with the ex-\npression channel grid in red vertical lines\ntening tests and calibration experiments described in the\nnext section, we chose to map the normal (default) veloc-\nity to MIDI velocity 72, mezzoforte to 80, and forzando to\n88. The piano samples used in this study is the Steinway\nGrand Piano in Logic Pro X2.\nTo model the changing-velocity controls crescendo and\ndecrescendo , we need some understanding of the expres-\nsion pneumatics system itself.\nIn most Welte-Mignon systems, the expression is im-\nplemented using pallet valves [11]. Speciﬁcally, when\nthe system reads a hole on the crescendo channel, the\ncrescendo valve will be opened, introducing a suction to\nthe expression pneumatic that pulls the pneumatic closed,\nproducing a crescendo in the music. It takes signiﬁcant\ntime for the air to come into the pneumatic system to take\neffect. We model this process as an exponential approach\nto a target value, using one-pole unity-gain lowpass ﬁl-\nter having impulse-response time-constant \u001cthat is set to\nmatch the observed dynamics.\nThere are two types of crescendo:\n1. a “very slow” crescendo produced by turning on the\ncrescendo channel. This control is latching, so that\none hole can turn it on.\n2. a “fast” crescendo that is notlatching.\nA string of fast crescendo holes can used to speed up\nthe slow crescendo, thereby providing many ultimate\ncrescendo rates, as well as nonuniform crescendos. They\nare like little bursts of additional suction along the way as\nthe slow crescendo develops.\nLet the observed time-constant of the slow crescendo\nbe denoted by \u001cs(sfor “slow”). Then the slow-crescendo\none-pole ﬁlter has digital transfer function\nHs(z) =1\u0000ps\n1\u0000psz\u00001(1)\n2https://www.apple.com/logic-pro/200 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017where its pole psis deﬁned as\nps=e\u0000T=\u001c s\nwithTdenoting the digital sampling interval in seconds\n(typicallyT= 1=fs, wherefsdenotes the sampling\nrate, andfs= 44100 Hz or greater). The time-domain\ndifference-equation used to implement the slow crescendo\nis given by\nyn= (1\u0000ps)xn+psyn\u00001; n = 0;1;2;::: ;\nwherexnis set to the target velocity at sample n, which is\nforzando for a crescendo, and either normal ormezzoforte\nfor a decrescendo, depending on the last constant-velocity\nsetting.\nThe fast crescendo is modeled exactly like the slow\ncrescendo, but using a smaller time-constant \u001cf\u001c\u001cs.\n5. EV ALUATION\nWe used MIDI ﬁle tools for Matlab3to convert our piano-\nroll matrix into MIDI format, and we synthesized the MIDI\nﬁle in Logic Pro X using the Steinway Piano software-\ninstrument that comes with Logic. We evaluated the suc-\ncess of our algorithm by comparing the synthesized audio\nto a recording of the player-piano, both driven from the\nsame piano roll. Due to limited access to the machine and\nrolls, we only include one roll in our discussion here. How-\never, given that each roll format possesses a ﬁx template,\nwe can assume that it will work for many additional rolls.\n5.1 Recording Setup\nWe set up a recording environment for the player piano in\na concert hall. The player we used is called a “push-up”\nbecause one pushes it up to a real piano where it plays\nthe piano using padded wooden mechanical “ﬁngers” (see\nFigure 5), as described further below. We recorded the\npush-up’s performance on a 9’ Steinway grand piano. The\npiano-roll chosen was the Chopin Op.42 Waltz in A [, per-\nformed by Katherine Bacon, and published by Welte in\n1924.\nFigure 5 . The Push-up Player\nFor the acoustic recording, we set up two cardioid mi-\ncrophones above the Steinway grand piano, one on the left,\n3https://github.com/kts/matlab-midi\nFigure 6 . Acoustic Recording Setup of the player piano\ncapturing most of the energy from the bass, and one on the\nright, for the treble, as shown in Figure 6. The push-up\nplayer is aligned at the piano and the roll is set in the player,\nattaching the lead to the take up spool. The playback speed\nis set manually on the player. There can be some variation\nin playback on different players due to subtle differences in\ncondition, however, most features of rolls are reproduced\nconsistently on well functioning instruments. The instru-\nment used in this project has been evaluated by player pi-\nano technicians to be in good working condition.\n5.2 Note Similarity Measurement\nAn example MIDI ﬁle4is shown in Figure 7, with the raw\nimage ﬁle on the top, and the MIDI ﬁle displayed in “piano\nroll” editor window in the program Logic Pro X for Mac\nOS X. We can see that the overall shape and trend of the\nnotes are visually identical.\nFigure 7 . Visual inspection of the scanned image (top) and\nthe ”piano roll” image of the synthesized MIDI in Logic\nPro X (bottom).\nWe measure the similarity of the audio content by cal-\nculating a similarity matrix [3] of the spectrogram between\nthe MIDI-synthesized audio ﬁle and the audio recording.\nWe then apply dynamic time warping [10] to align the\nMIDI ﬁle with audio and retrieve a path between the two.\nFigure 8 shows the similarity matrix comparing the spec-\ntrogram of the audio recording and MIDI-synthesized au-\n4MIDI synthesized audio ﬁle on Chopin Waltz Op.42 No.2 can be\nheard at https://tinyurl.com/kvyxc3sProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 201Figure 8 . The spectrogram similarity matrix comparing\nthe live audio recording and the synthesized MIDI, with an\noverlay of the time alignment line (red). Note: a straight\ndiagonal line means perfect alignment.\ndio, with the red diagonal line representing the time align-\nment between the recording and the MIDI. We see that the\ndiagonal line is near straight and slope 1, meaning that we\nget almost perfect alignment between the audio and the\nMIDI. We found that there is some variation and curva-\nture towards the end of the diagonal line and that the over-\nall length of the MIDI emulation is 8-second longer than\nrecording of the player piano.\n5.3 Dynamics Measurement\nTo measure the success of modeling the dynamic expres-\nsions, we calculated the root mean square energy of both\naudio ﬁle. As in Figure 9, we see that their dynamic lev-\nels are similar, but we see that the original playback has a\nwider dynamic range than our synthesized MIDI ﬁle. We\nplan to optimize our setting of the dynamic variability to\nmatch the acoustic recording, but that will be the center\n(default) setting of a knob that can be varied from “ﬂat”\n(no dynamics at all) to “exaggerated” (expanded dynam-\nics). With this control users can adjust to taste. For exam-\nple, it is nice to be able to make ﬂatter renderings for noisy\nlistening environments such as cars.\nNote that every player piano will give a slightly differ-\nent result due to variabilities in manufacturing and settings.\nThis is another reason to make the end-result easily ad-\njustable.\nWe do not yet include pedal information because the\npedal on the player piano was not working perfectly at the\ntime of our recording. We presently do not have control\nover how much pedal and pedal delay.\n5.4 Discussion\nWe found that the MIDI ﬁle matched the original record-\ning quite well, both visually in the graphs and audibly in\nrecordings. As pointed out in the similarity measurement\nsection, we observe that the MIDI roll is not quite at the\nFigure 9 . Comparison of the root mean square energy for\nlive audio recording (blue) and synthesized MIDI (red).\nsame speed as the instrument playback of the roll. The\nMIDI roll appears to be “slowing down” towards the end\ncompared to the original playback. Our research suggests\nthat this is likely a deliberate design from the factory to\ncompensate for increasing tempo change due to the chang-\ning diameter of the spool as it unwinds the paper upon\nplayback. Wider spacing of the holes towards the end of\nthe roll would keep the speed of the playback constant [1].\nThis would be consistent with our observation which ﬁnds\nthe original roll playback faster than the MIDI of the paper\nroll itself. This observation will be explored with further\nevidence as more rolls are scanned and digitized.\n6. CONCLUSION AND FUTURE WORK\nWe proposed methods for decoding reproducing piano roll\nimages into MIDI ﬁles. We also proposed an apparently\nnovel method for interpreting the expression markings on\na piano roll. Though the system is designed to recognize\nthe Welte-Mignon piano-roll format, our note matrix tem-\nplate is adaptable to all other systems with small modiﬁ-\ncations. However, the expression template is not adaptable\nfrom system to system. We also developed models for the\nDuo-art format. We have not yet evaluated this model with\na playback comparison due to limited availability of an ap-\npropriate instrument. However, future work is planned for\ninterpreting all piano-roll types, and including comparing\ndifferent player instruments of the same type in order to\nmeasure variabilities. We further plan to create a master\npunch-matrix for the system types for correcting errors and\nrepunching the piano rolls, thereby “restoring” them. We\nplan to develop a batch processing system for all the roll\nimages created by the new scanning device that is presently\nbeing built for the Stanford Music Library [8]. Finally, we\nplan to release these digitized piano rolls on the Web as a\nfree online resource.202 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20177. ACKNOWLEDGEMENT\nWe’d like to thank Anthony Robinson and Jerry McBride\nfor providing valuable information about reproducing pi-\nano rolls. We thank the Stanford Archive of Recorded\nSound for providing the player piano as well as piano rolls.\n8. REFERENCES\n[1] Mechanical music digest archives. http://www.\nmmdigest.com/Archives . Accessed: 2017-04-\n18.\n[2] Gustavo Colmenares, Ren ´e Escalante, Juan F Sans, and\nRina Sur ´os. Computational modeling of reproducing-\npiano rolls. Computer Music Journal , 35(1):58–75,\n2011.\n[3] Jonathan Foote. Visualizing music and audio using\nself-similarity. In Proceedings of the seventh ACM in-\nternational conference on Multimedia (Part 1) , pages\n77–80. ACM, 1999.\n[4] Ichiro Fujinaga. Optical music recognition using pro-\njections . McGill University, 1990.\n[5] The Pianola Institute. The reproducing piano -\nwelte-mignon. http://www.pianola.org/\nreproducing/reproducing_welte.cfm .\nAccessed: 2017-04-15.\n[6] Jagat Narain Kapur, Prasanna K Sahoo, and An-\ndrew KC Wong. A new method for gray-level pic-\nture thresholding using the entropy of the histogram.\nComputer vision, graphics, and image processing ,\n29(3):273–285, 1985.\n[7] Sasha Leitman. Trimpin: An interview. Computer Mu-\nsic Journal , 35(4):12–27, 2011.\n[8] Stanford University Libraries. About the player\npiano project. https://library.stanford.\nedu/projects/player-piano-project/\nabout-project . Accessed: 2017-05-16.\n[9] Matteo Malosio, Flavio Pedrazzini, and Perego Nic-\ncol. The sisar project. The Music Box , 26(6):221–223,\n2014.\n[10] Meinard M ¨uller. Fundamentals of Music Processing:\nAudio, Analysis, Algorithms, Applications . Springer,\n2015.\n[11] Arthur A Reblitz. Player Piano: Servicing and Re-\nbuilding . Vestal Press, 1997.\n[12] Anthony Robinson. Anthony’s roll scanning. http:\n//semitone440.co.uk/scanner/ . Accessed:\n2017-02-17.\n[13] Larry Sitsky. The Classical Reproducing Piano Roll: A\nCatelogue . New York: Greenwood Press, 1990.[14] Wayne Stahnke. Stahnke’s roll archival methodol-\nogy. http://mmd.foxtail.com/Archives/\nAuthors/Aut318.html . Accessed: 2017-04-15.\n[15] Warren Trachtman. Recovering inherent roll punch\nmatrix spacing information and using it to dynamically\nself-correct roll scans. http://www.trachtman.\norg/rollscans . Accessed: 2017-03-15.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 203"
    },
    {
        "title": "Automatic Interpretation of Music Structure Analyses: A Validated Technique for Post-Hoc Estimation of the Rationale for an Annotation.",
        "author": [
            "Jordan B. L. Smith",
            "Elaine Chew"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415196",
        "url": "https://doi.org/10.5281/zenodo.1415196",
        "ee": "https://zenodo.org/records/1415196/files/SmithC17.pdf",
        "abstract": "Annotations of musical structure usually provide a low le- vel of detail: they include boundary locations and section labels, but do not indicate what makes the sections similar or distinct, or what changes in the music at each boundary. For those studying annotated corpora, it would be useful to know the rationale for each annotation, but collecting this information from listeners is burdensome and difficult. We propose a new algorithm for estimating which musical fea- tures formed the basis for each part of an annotation. To evaluate our approach, we use a synthetic dataset of music clips, all designed to have ambiguous structure, that was previously used and validated in a psychology experiment. We find that, compared to a previous optimization-based algorithm, our correlation-based approach is better able to predict the rationale for an analysis. Using the best version of our algorithm, we process examples from the SALAMI dataset and demonstrate how we can augment the struc- ture annotation data with estimated rationales, inviting new ways to research and use the data.",
        "zenodo_id": 1415196,
        "dblp_key": "conf/ismir/SmithC17",
        "keywords": [
            "annotations",
            "musical structure",
            "boundary locations",
            "section labels",
            "rational for each annotation",
            "synthetic dataset",
            "music clips",
            "ambiguous structure",
            "psychology experiment",
            "correlation-based approach"
        ],
        "content": "AUTOMATIC INTERPRETATION OF MUSIC STRUCTURE ANALYSES:\nA VALIDATED TECHNIQUE FOR POST-HOC ESTIMATION OF THE\nRATIONALE FOR AN ANNOTATION\nJordan B. L. Smith\nNational Institute of Advanced Industrial\nScience and Technology (AIST), Japan\njordan.smith@aist.go.jpElaine Chew\nQueen Mary University of London\nelaine.chew@qmul.ac.uk\nABSTRACT\nAnnotations of musical structure usually provide a low le-\nvel of detail: they include boundary locations and section\nlabels, but do not indicate what makes the sections similar\nor distinct, or what changes in the music at each boundary.\nFor those studying annotated corpora, it would be useful to\nknow the rationale for each annotation, but collecting this\ninformation from listeners is burdensome and difﬁcult. We\npropose a new algorithm for estimating which musical fea-\ntures formed the basis for each part of an annotation. To\nevaluate our approach, we use a synthetic dataset of music\nclips, all designed to have ambiguous structure, that was\npreviously used and validated in a psychology experiment.\nWe ﬁnd that, compared to a previous optimization-based\nalgorithm, our correlation-based approach is better able to\npredict the rationale for an analysis. Using the best version\nof our algorithm, we process examples from the SALAMI\ndataset and demonstrate how we can augment the struc-\nture annotation data with estimated rationales, inviting new\nways to research and use the data.\n1. INTRODUCTION\nListeners perceive structure in music, and trying to pre-\ndict the structures they perceive is a popular task in the\nMIR community [14]. Since the perception of structure\nis a complex phenomenon, the community focuses on a\nsimpler, operational version: we imagine that structure,\nas perceived, can be characterized as a set of time points\nregarded as boundaries, and a set of labels that indicate\nwhich of the intervening segments repeat similar material.\nThis simpliﬁcation is not made na ¨ıvely: those who cre-\nate annotations of musical structure are aware of its limita-\ntions, and the methodologies for annotating [1, 16, 21] and\nevaluating [7, 9, 11] structural analyses have become their\nown important subtopics in MIR.\nStill, the simpliﬁcation is unfortunate because musical\nsimilarity is multi-dimensional. If a listener declares that\nc\rJordan B. L. Smith, Elaine Chew. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: Jordan B. L. Smith, Elaine Chew. “Automatic interpretation of\nmusic structure analyses: A validated technique for post-hoc estimation\nof the rationale for an annotation”, 17th International Society for Music\nInformation Retrieval Conference, 2016.two excerpts are “similar”, they could mean with respect\nto melody, contour, rhythm, timbre, or any combination of\nthese or other musical attributes. This is in addition to the\nissue that structure itself is multi-dimensional; as pointed\nout in [16], boundaries may be perceived for reasons of\nmusical similarity, musical function, or instrumentation.\nThus, in the transition from structure perception to struc-\nture annotation, we usually fail to capture why a listener\nhas included a boundary or chosen a label. This infor-\nmation, if preserved (or reconstructed), would help us to\nunderstand the content of the annotations, and could lead\nto fairer evaluations of structure segmentation algorithms.\nIt would also provide more meaningful data to analyze in\nmusicology or music perception research.\nHow feasible is it to collect this information? As we\nfound in [23], to transcribe the rationale for every aspect\nof an annotation is difﬁcult and requires prolonged self-\ninterrogation. Even before that, it is difﬁcult to decide what\ninformation to collect, and how to collect it: should the\ndata be collected after a listener has provided the segmen-\ntation, in the manner of music perception experiments [2]?\nOr should each piece be annotated several times, each time\nwith a focus on a single feature [19]? No matter how it is\ndone, collecting this information is burdensome.\nA more practical possibility is to estimate this informa-\ntion automatically from existing annotations, which was\nour motivation in [22]. Our algorithm compared self-dis-\ntance matrices (SDMs) for different features to the ground\ntruth annotation, and found which parts of the feature-based\nSDMs best re-created the annotation-based SDM. While\n[22] presented some examples to demonstrate the plausi-\nbility of the approach, we offered no experimental valida-\ntion.\nValidation requires paired responses: a set of listeners’\nanalyses, and the listeners’ justiﬁcations for each analysis.\nProducing this data is time-consuming and burdensome for\nthe reasons described above. However, we recently pro-\nduced data suited to this purpose for a music perception\nstudy [20]. The goal of that study was to determine what\nrole attention plays in the perception of structure.\nIn this article, we make three main contributions: ﬁrst,\nwe test whether the approach described in [22] can effec-\ntively predict the attention of the listeners, based on the\ndataset created for [20]. Second, we explain some short-435comings of the previous approach, and suggest and test\ntwo improvements. Third, we demonstrate how the val-\nidated algorithm can be used to analyze and to augment\nreal-world data with new information layers.\nThe next two sections recap the studies on which this\narticle builds. In Section 2, we brieﬂy recall [22]’s algo-\nrithm, point out some shortcomings, and introduce a re-\nﬁned approach. In Section 3, we summarize the results of\nthe experiment in [20], and describe in more detail the data\ndeveloped for that study and used in this one. In Sections\n4 and 5, we outline the validation experiment and discuss\nthe results, and in Section 6 we use the algorithm to create\nnew information layers for examples from SALAMI [21].\nWe close with a few observations on the limitations of the\npresent work and recommendations for future research.\n2. AN ALGORITHM FOR ESTIMATING\nFEATURE RELEVANCE\nIn [22] we estimated the relevance of musical features to a\nlistener’s analysis section-by-section by ﬁnding the weigh-\nted sum of feature-derived SDMs that best matched the\nanalysis. The analysis is represented as a binary SDM,\nexpanded to the same timescale as the feature SDMs. A\nnumber nof feature matrices are computed; from each, we\nderive msingle-section SDMs by taking only the rows and\ncolumns associated with that single segment, as deﬁned by\nthe annotation. (This row and column selection is done by\nmultiplying the SDM with a segment mask.) This gives\nn\u0001mcomponent matrices. A quadratic program (QP) is\nused to ﬁnd the weights for these components whose sum\noptimally reproduces the annotation-derived SDM; these\nweights, the reconstruction coefﬁcients, are taken to indi-\ncate feature relevance.\nThe method is illustrated in Fig. 1. The sound exam-\nple has the form ABAB with respect to harmony, AABB\nwith respect to rhythm, and ABBA with respect to tim-\nbre. If a listener gives the analysis ABAB, segmenting the\naudio at the 1/4, 2/4 and 3/4 marks, we obtain the four seg-\nment masks given in the top row. We compute four audio\nfeatures, each related to a different musical attribute (see\nSection 4.1 for details), which are pointwise multiplied by\nthe masks to give 8 potential components. The QP ﬁnds\nthe optimal combination of components to reproduce the\nannotation in the top-left corner, and gives the coefﬁcients\nshown above each component. In this case, the algorithm\nhas successfully identiﬁed that bass chroma is the feature\nthat best justiﬁes the analysis.\n2.1 Algorithm Improvements\nOne limitation of this approach is that none of the feature\nmatrices may properly reﬂect the homogeneity of a given\nsection. We could include additional SDMs that have been\nsmoothed at different timescales (as demonstrated in [22]),\nbut the smoothing can blur the boundaries between sec-\ntions even as they make the sections more homogeneous.\nWe could use stripe-based instead of block-based masks\nin order to capture repetitions of feature sequences, but in\nFigure 1 . Illustration of component-building for QP al-\ngorithm. Four beat-indexed feature matrices (at left) are\nmultiplied by the masks (top) given by the segmentation,\nwhich here is ABAB. The number above each component\nis the QP’s estimate of the component’s importance.\nnon-square blocks (which occur whenever two segments\nhave unequal lengths), it is not easy to guess the best ori-\nentation or placement of the stripes.\nA second problem is that it is unclear how to inter-\npret some aspects of the QP. Should the individual recon-\nstruction coefﬁcients, or their sum, be bounded? Leaving\nthem unbounded can lead to unconstrained solutions, but\nif bounds are imposed, how should they be interpreted?\nA third problem is that by ﬁnding the single optimal\nsum of matrix components, some good explanations may\nbe ignored. For example, if there are two matrix compo-\nnents which both justify a particular part of the analysis,\nthe QP may ﬁnd that only one is necessary. Thus, we can-\nnot conclude that features omitted from the solution are\nnecessarily irrelevant, which is a big limitation.\nFor the ﬁrst problem, we propose that instead of using\nthe original SDMs, with all their heterogeneities, we re-\nduce them to segment-indexed SDMs, a common practice\nsince [4]. Similar to [13], we may take the distance be-\ntween each pair of segments to be the average distance of\nall the pixels in the submatrix over which the segments in-\ntersect. The segment-indexed SDM can then be analyzed\nwith the QP as before, although with a substantial reduc-\ntion in complexity.\nA second way to address the problem is to use a diago-\nnal stripe-based mask instead of a block-based mask. Since\nthe diagonals are the most salient portions of the SDM,\nit makes sense to focus on reconstructing this portion of\nthe SDM. Emphasizing stripes is a common SDM analy-\nsis technique, and a comparison of block and stripe fea-\ntures found that when boundaries were given, stripe fea-436 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 2 . Illustration of proposed segment-indexed ap-\nproach, with both QP- and correlation-based estimates of\nfeature relevance.\ntures were more effective [12]. We remember the caveat\nabove, that repeated segments with different durations pose\na problem for creating a stripe-based mask, but we can still\ntest it in cases where this is not an issue.\nA third proposal, which addresses the second and third\nproblems above, is to dispense with the QP altogether and\nsimply take the element-wise Pearson correlation between\nthe feature-derived and annotation-derived matrices. (The\nsame section-by-section method still applies.) Correlations\nare perhaps more intuitive than QPs and reconstruction co-\nefﬁcients, and using them would permit second-place fea-\ntures to be more readily identiﬁed in the solution.\nFig. 2 shows the output for an example of a three-part\nstimulus, using the suggested improvement of segment-\nindexing: the features have been averaged over the blocks\ngiven by the segmentation. The sum of the reconstruction\ncoefﬁcients obtained using the QP method are given in the\n“QP sum” column, and the mean point-wise correlation be-\ntween the masked regions is shown in the “Mean corr.”\ncolumn. Fig. 3 shows the output for the same example but\nusing the stripe-based mask. The mask is constructed by\ndrawing a diagonal line across each block of the original\nbeat-indexed SDM, and then applying a 2D convolution\nwith a Gaussian kernel of width 5 beats.\nTo sum up, we suggest three improvements to the algo-\nrithm: (1) using the correlation between submatrices, in-\nstead of QP, to estimate their relevance; (2) using a segment-\nindexed version of the SDM; and (3) applying a stripe mask\nto the SDM, instead of using the blocks.\n3. A DATASET OF VALIDATED ANALYSES\nResearchers in music psychology, like those in MIR, are\ninvested in modeling how listeners perceive structure. (For\none discussion, see [15].) The goal of [20] was to de-\ntermine whether listeners could be inﬂuenced to perceive\nFigure 3 . Illustration of proposed stripe approach with cor-\nrelation. Like in Fig. 2, QP coefﬁcients are in the middle\ncolumn, correlations on the right.\ndifferent structures by manipulating the musical feature to\nwhich they paid attention. In order to test this, we com-\nposed a set of artiﬁcial musical stimuli in which four dif-\nferent features (harmony, melody, rhythm and timbre) were\nsystematically changed at different times, creating musical\npassages with ambiguous forms. These four features were\nchosen because they ﬁgured most prominently in studies\nwhere listeners were asked to justify why they perceived a\ngiven boundary, such as [2].\nThe three-part stimuli had two potential structures, AAB\nor ABB, with different features changing at different times.\nFor example, the passage in Fig. 4a has form AAB with re-\nspect to harmony, and form ABB with respect to melody.\nThe four-part stimuli had three potential structures, AABB,\nABAB or ABBA, so that at every boundary there were\ntwo features that changed. For example, in the passage\nin Fig. 4d, the rhythm and harmony both change after the\nsecond measure.\nAs stated above, validating the algorithm requires musi-\ncal examples where listeners’ analyses are paired with their\njustiﬁcations—i.e., with the musical attributes to which they\nwere paying attention. Many datasets of structural analy-\nses exist, but none indicate which musical attributes justify\nthe analyses. Also, in typical pieces of music, attributes\nchange frequently, to different extents, and often simulta-\nneously. To validate this algorithm we should use music\nwith known, controlled changes. Hence, artiﬁcial stimuli\nsuch as these are valuable resources to validate the algo-\nrithm: each passage contains precise change points related\nto known musical attributes; and the link between the at-\ntributes and the different forms has been afﬁrmed by lis-\nteners in an experimental setting.\nMore artiﬁcial stimuli could be generated and tested\nin future work; this may be a convenient way to provide\ndeep-learning algorithms with the quantity of labelled dataProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 437(a)\n2 /dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.23333/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/clefs.F/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/rests.2/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/dots.dot/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/timesig.C44/clefs.FOrgan/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/timesig.C44/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/timesig.C44/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/timesig.C44/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/timesig.C44/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Organ/clefs.F/timesig.C44/rests.1/dots.dot/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Harpsichord/clefs.F/timesig.C44/dots.dot/dots.dot/rests.1/dots.dot/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Piano/clefs.F/timesig.C44/dots.dot/dots.dot/dots.dot/rests.1/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Piano/clefs.F/timesig.C44/accidentals.flat/accidentals.flat/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nMusic engraving by LilyPond 2.18.2—www.lilypond.org\n(b)\n Harmony/Timbre and Melody/Rhythm/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Melody A / Rhythm A/noteheads.s2/clefs.G/noteheads.s2333/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Melody A / Rhythm B/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Melody B / Rhythm B/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2333/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/rests.2/noteheads.s2Melody B / Rhythm A/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/clefs.F/timesig.C44/noteheads.s2Organ/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2Harmony A / Timbre A/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/rests.1/dots.dotHarmony B / Timbre BHarpsichord/clefs.F/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2Harmony A / Timbre BHarpsichord/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2Harmony B / Timbre AOrgan/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u6/noteheads.s2/noteheads.s2/noteheads.s2/flags.u6/noteheads.s2/flags.u6/noteheads.s2/flags.u6/noteheads.s2/flags.u6/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2333/rests.1/rests.1/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.F/dots.dot/timesig.C44/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2Organ/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2333/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2333/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.G/noteheads.s2/rests.1/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/rests.1/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2Harpsichord/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/timesig.C44/clefs.FOrgan/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/rests.1/dots.dotOrgan/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.133/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/timesig.C44/clefs.F/timesig.C44/dots.dot/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n(c)\n Harmony/Timbre and Melody/Rhythm/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Melody A / Rhythm A/noteheads.s2/clefs.G/noteheads.s2333/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Melody A / Rhythm B/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Melody B / Rhythm B/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2333/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/rests.2/noteheads.s2Melody B / Rhythm A/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/clefs.F/timesig.C44/noteheads.s2Organ/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2Harmony A / Timbre A/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/rests.1/dots.dotHarmony B / Timbre BHarpsichord/clefs.F/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2Harmony A / Timbre BHarpsichord/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2Harmony B / Timbre AOrgan/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u6/noteheads.s2/noteheads.s2/noteheads.s2/flags.u6/noteheads.s2/flags.u6/noteheads.s2/flags.u6/noteheads.s2/flags.u6/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2333/rests.1/rests.1/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.F/dots.dot/timesig.C44/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2Organ/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2333/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2333/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.G/noteheads.s2/rests.1/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot333/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2Harpsichord/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.FOrgan/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/clefs.G/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/rests.1/dots.dotOrgan/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.133/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/timesig.C44/clefs.F/timesig.C44/dots.dot/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n(d)\nFigure 4 . (a) Example stimulus with harmonic form AAB\nand melodic form ABB. (b) Harmonic form AAB, timbral\nform ABB. (c) Rhythmic form AAB, timbral form ABB.\n(d) Example four-part stimulis with melodic form AABB,\nrhythmic form ABAB, and harmonic form ABBA.\nthey require. However, it is not as simple as sonifying a\nsymbolic score, since scores must be annotated in order to\nknow the perceived structure and the musical features that\nmotivate that analysis. The stimuli in our study are rare in\nthat they were (1) composed so that musical features var-\nied systematically, and (2) used in a listening experiment\nto validate that the intended structures were perceived, for\nthe intended reason.\n3.1 Stimulus Details\nFor [20], we composed three sets of stimuli. Each stim-\nulus contains two voices, and in each set of stimuli, each\nvoice potentially expresses changes in two different fea-\ntures. The examples in Fig. 4 are from the “HT-MR” set,\nwhere one voice expresses changes in harmony and timbre,\nand the other, changes in melody and rhythm. “HM-RT”\nand “HR-MT” sets were also composed.\nSince in each set of stimuli, certain features are “con-\nvolved,” some incorrect answers are less wrong than oth-\ners. For instance, the feature that changes at the second\nboundary of the example in Fig. 4c is rhythm, but if an al-\ngorithm said that the boundary was justiﬁed by melody, it\nwould be partially right.\nThe stems for the stimuli were composed using a Dig-\nital Audio Workstation with standard instrument patches.\nThe 8 stems for each set were systematically recombined\nto generate 192 three-part stimuli and 384 four-part stim-\nuli, for a total of 1728 stimuli among all sets. Efforts were\nmade to keep constant all musical features other than har-mony, melody, rhythm and timbre: the tempo of all stimuli\nis 140 bpm, and the loudness of each voice and each pas-\nsage is approximately equal. The stimuli are now freely\navailable on Github.1\n4. EXPERIMENT\n4.1 Features\nThe stimuli manipulated four different musical attributes\n(in three environments): harmony, melody, rhythm and\ntimbre. We want to extract audio features that match each\nof these attributes independently. Each audio feature should\nchange when the related musical feature changes, and be\nrobust to changes in other musical features. We selected\ntwo audio features for each musical feature, all available\nas Vamp plugins2and listed in Tab. 1. We used ground\ntruth beat locations, and median feature values were taken\nfor each beat. Each dimension was normalized (indepen-\ndently for each stimulus) to zero mean, unit variance. All\nfeatures were extracted using Sonic Annotator [3] using\nthe default settings. For some features, we performed ad-\nditional processing:\nChords : Chord labels were estimated from Chordino\nand reconverted back to a chroma-like representation. This\nfeature is thus based on the same information as bass chro-\nma, but reﬁned with the chord-estimation algorithm.\nMelody : The chroma of the estimated melody, and the\ninterval between the current steady-state note and the pre-\nvious one, each a 12-dimensional feature per frame. We\nalso used the register of the melody: low, middle or high.\nAutocorrelation : this was computed on an onset detec-\ntion function with a sliding window.\nLow level features : a concatenation of loudness, RMS\namplitude, rolloff, sharpness, smoothness, tristimulus, zero-\ncrossing rate, and the centroid, kurtosis, skewness, and\nslope of the spectrum.\nFeature Vamp plugins used to obtain feature\nHarmonyBass chroma , from Chordino and NNLS\nChroma plugin [8]\nChord notes [8]\nMelodyTreble chroma [8]\nMelody , based on MELODIA [18]\nRhythmCyclic tempogram [6]\nAutocorrelation , based on UAPlugins’s\nNote Onset Detector [17]\nTimbreMFCCs (2nd to 13th), from Chris Cannam\nand Jamie Bullock’s LibXtract library\nLow level features , a set of ﬁfteen one-di-\nmensional descriptors from LibXtract\nTable 1 . List of features chosen, and Vamp plugins used to\nobtain them\n1https://github.com/jblsmith/\nmusic-structure-stimuli .\n2vamp-plugins.org438 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20174.2 Results\nWe applied the algorithms, discussed in Section 2, on the\nstimuli discussed in Section 3. For each three-part stimu-\nlus, we ran the algorithms twice: once with analysis AAB,\nonce with ABB. Likewise, we ran the algorithm thrice on\neach four-part stimulus to ﬁnd the best justiﬁcations for\nforms AABB, ABAB and ABBA. Each algorithm takes\none of these analyses as input. The output of each algo-\nrithm is a matrix of feature relevance values xi;j: one per\nsection i, per feature j. The importance of feature jis the\nsum across all the sections: sj=P\nixi;j. The importance\nof each musical attribute ais the sum of the two values sj\nrelated to that feature: ya=sa1+sa2. We end up with\nfour values ya.\nWe test whether the maximum value correctly predicts\nthe feature related to the analysis with arg max aya. The\nfraction of trials with correct guesses is the accuracy. Each\ntrial has one focal pattern and three potential wrong an-\nswers, so the random baseline performance is 25%.\nThe ﬁve algorithm options were: whether to use cosine\nor Euclidean distance (in either case, the values were re-\nscaled between 0 and 1); whether to compute beat-indexed\nor segment-indexed SDMs; whether to apply stripe-based\nmasks to the SDMs; whether to use the QP or correlation-\nbased approach; and ﬁnally, if using QP, what constraint\nto use. We tested three constraints: (a)P\ni;jxi;j=C\n(the sum of the coefﬁcients over the entire piece has a\nﬁxed value); (b)P\njxi;j=C(the sum of the coefﬁcients\nfor each section in the piece has a ﬁxed value); and (c)\n0\u0014xi;j\u00141. These options were tested in a full-factorial\ndesign, replicated across three variables that were not part\nof the algorithm: the relevant feature; the music environ-\nment; and the stimulus length (3 or 4 sections).\nWe ﬁt a linear model to the results and used ANOV A\nto interpret the eight factors. With 268,512 trials, three\nfactors were insigniﬁcant ( p > 0:05): stimulus length,\ndistance metric, and QP constraint. The other ﬁve fac-\ntors all had p < 0:0001 , and main effect plots for each\nare shown in Fig. 5. They show that performance varied\ngreatly among the music examples and features. However,\nthe three proposed changes to the original algorithm—using\ncorrelation instead of QP, using stripe masks, and using\nsegment-indexed SDMs—all saw improvements, albeit a\nminor one in the case of segment indexing.\nTab. 2 gives the accuracy for different parameter set-\ntings. It shows that although the main effects appear mod-\nest in Fig. 5, their impact is additive: the original approach\nachieved 47% accuracy, and the three changes (using cor-\nrelation, segment-indexing, and applying a stripe mask) to-\ngether raised the accuracy to nearly 70%.\nThese are the accuracies for choosing the most correct\nanswer, but not all errors are equally bad: guessing a fea-\nture that was convolved in the stimulus with the correct one\nis sometimes a fair mistake. However, Tab. 2 shows that the\n“convolved-with-correct” answer was not given any spe-\ncial weight by the algorithms. There are 3 features besides\nthe correct one, so the chance of randomly guessing the\nconvolved feature is 33%. In all cases, fewer than a third\nFigure 5 . Main effect of signiﬁcant factors on accuracy\n(i.e., rate of correct guesses).\nMethod: Quad. Prog. Correlation\nSettings :Correct Conv. Correct Conv.\nRegular 47.1 13.7 52.3 12.8\nSeg.-indexing 46.9 16.3 60.6 8.9\nStripe mask 52.4 13.5 62.4 11.9\nSeg. and stripes 59.6 12.8 69.6 7.2\nTable 2 . Comparison of QP-based and correlation-based\nalgorithms. Columns indicate how often the guessed fea-\nture was correct (“Correct”) or convolved with the correct\nfeature (“Conv.”). For example, in the HT-MR environ-\nment, if the correct feature for a trial is timbre, guessing\nharmony could be half-right.\nof the incorrect answers related to the convolved feature.\nPrediction accuracy varied greatly among the features,\nas can be seen in the confusion matrices for the algorithms.\nThree are shown in Fig. 6, one for each music environment.\nThese are the results for the best-performing algorithm.\nFor harmony, we can observe that chord notes were more\neffective than bass chroma, the feature from which they\nderive. Bass chroma were especially misled in the HM-\nRT setting, possibly due to the difference in bass drum be-\ntween the two timbre settings. With melody, it was also the\ncase that the 2nd-order feature (the estimated predominant\npitch and interval) was better than the lower-level feature\n(treble chroma).\n5. DISCUSSION\nThe results validate the algorithm proposed in [22]. How-\never, they also show that a simpler correlation-based ap-\nproach is better at predicting how best to justify an anal-\nysis: it outperformed the QP approach by roughly 10%.\nTwo other reﬁnements, the stripe-based mask and the seg-\nment indexing, increased accuracy by roughly another 10%.\nHowever, the confusion matrices revealed great dispar-\nities between the features we chose to use: some, such as\nChordino, were effective; others, such as the tempogram\nand MFCCs, were often wrong. Arguably, it is na ¨ıve for us\nto presume that off-the-shelf features can detect the types\nof musical changes we created in the stimuli. Perhaps it\nis no accident that the four features we tweaked or assem-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 439Figure 6 . Confusion matrix for algorithm using correla-\ntion, stripe masks, and segment-indexed SDMs. The rows\ngives the correct musical attribute; the column indicates\nthe audio feature with maximum relevance.\nbled for this purpose (chord notes, MELODIA-based fea-\nture, onset autocorrelation and low-level features) tended\nto outperform their off-the-shelf rivals.\nStill, the underperformance is surprising, since the stim-\nuli are highly constrained: in the study for which the stim-\nuli were created, listeners identiﬁed the attribute that chan-\nged at a boundary with 85% accuracy [20]. It seems rea-\nsonable to expect that, say, MFCCs will change more when\na trumpet is swapped for a ﬂute than when a trumpet plays\na different melody; or that when the harmony changes,\nbass chroma will be more affected than the tempogram.\nYet these are among the errors made by the features in this\nstudy. The results thus remind us of the utility of carefully-\ndesigned features, such as timbre-invariant chroma [10].\nAn alternative to testing hand-crafted features is to learn\nfeatures with deep learning, but as mentioned earlier, this\nwould require building a much larger, more representative\nstimulus set—more stimuli than can easily be validated in\na listener study. The small set used here is suitable for\ntesting existing features, but not learning new ones.\n6. APPLICATION\nThe correlation algorithm can be used to interpret annota-\ntions in the SALAMI corpus [21]. We used the segment-\nindexing setting but not the stripe-masking, which (as noted\nin Section 2.1) is not applicable when unequal segment\nlengths give rectangular blocks. The audio processing was\nthe same except that BeatRoot [5] was used to locate beats.\nFig. 7 visualizes a listener’s analysis of “We Are The\nChampions” by Queen at the long and short timescales.\nEach vertical slice corresponds to a single section, and the\nbrightness of each cell indicates the correlation of that fea-\nFigure 7 . Example augmented annotation for the song\n“We Are The Champions” by Queen. The letters and col-\nors both encode the section labels. Brightness indicates a\nfeature’s relevance to a section.\nture to that section. We can see that on a long timescale, the\nverse sections ( A) were characterized by their harmonic\nand melodic content, while the chorus sections ( B) were\ncharacterized more by their timbre. However, on a short\ntimescale, subsection awas also characterized by timbre,\nand many of the subsections of Bwere more strongly char-\nacterized by harmony and melody compared to Bitself.\nThis, it turns out, is an accurate description of the song:\nina, Freddie Mercury sings above a piano and bass only;\nthe electric guitar enters quietly in b, but the drums come\nin with cin a raucous crescendo to the chorus. The tim-\nbral inconsistency of Ameans that timbre would be a poor\nfeature to use to justify grouping the ﬁrst four subsections\ninto a larger unit.\nOn the other hand, the timbre of the choruses is rela-\ntively homogeneous; this makes it a good feature to justify\ngrouping the Bsections together, but also makes it a poor\nfeature to justify giving the subsections of Bdifferent la-\nbels. The fact that subsections d,e,fandghave different\nlabels must therefore reﬂect their pitch content.\n7. CONCLUSION\nWe have validated the algorithm proposed by [22], and pro-\nposed three modiﬁcations to improve its effectiveness. Al-\nthough we restricted this study to stimuli that were vali-\ndated in a psychology experiment, it would be possible to\ngenerate large amounts of artiﬁcial music, with more com-\nplicated patterns of repetition and variation, and changes\nin more musical parameters, like loudness, tempo, synco-\npation, dissonance, and so on.\nThe accuracy of the algorithms fell short of human per-\nformance. Given the disparities among the features, this\nmust be due in part to the mismatch between the audio fea-\ntures we chose and the musical attributes manipulated in\nthe stimuli. Despite this, the algorithm is useful for visual-\nizing the structure of pieces in a new way: by highlighting\nthe musical features that explain the annotation.440 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20178. REFERENCES\n[1] Fr ´ed´eric Bimbot, Emmanuel Deruty, Gabriel Sargent,\nand Emmanuel Vincent. Semiotic structure labeling of\nmusic pieces: Concepts, methods and annotation con-\nventions. In Proceedings of ISMIR , pages 235–240,\nPorto, Portugal, 2012.\n[2] Michael Bruderer, Martin McKinney, and Armin\nKohlrausch. The perception of structural boundaries\nin melody lines of Western popular music. Musicæ-\nScientæ , 13(2):273–313, 2009.\n[3] Chris Cannam, Michael O. Jewell, Christophe Rhodes,\nMark Sandler, and Mark d’Inverno. Linked data and\nyou: Bringing music research software into the seman-\ntic web. Journal of New Music Research , 39(4):313–\n325, 2010.\n[4] Matthew Cooper and Jonathan Foote. Summarizing\npopular music via structural similarity analysis. In Pro-\nceedings of the IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics (WASPAA) ,\npages 127–30, New Paltz, NY , United States, 2003.\n[5] Simon Dixon. Automatic extraction of tempo and beat\nfrom expressive performances. Journal of New Music\nResearch , 30(1):39–58, 2001.\n[6] Peter Grosche, Meinard M ¨uller, and Frank Kurth.\nCyclic tempogram - a mid-level tempo representation\nfor music signals. In Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing , Dallas, TX, USA, 2010.\n[7] Hanna Lukashevich. Towards quantitative measures of\nevaluating song segmentation. In Proceedings of IS-\nMIR, pages 375–380, Philadelphia, PA, USA, 2008.\n[8] Matthias Mauch and Simon Dixon. Approximate note\ntranscription for the improved identiﬁcation of difﬁ-\ncult chords. In Proceedings of ISMIR , pages 135–140,\nUtrecht, Netherlands, 2010.\n[9] Brian McFee, Oriol Nieto, and Juan Pablo Bello. Hier-\narchical evaluation of segment boundary detection. In\nProceedings of ISMIR , M´alaga, Spain, 2015.\n[10] Meinard M ¨uller and Sebastian Ewert. Towards timbre-\ninvariant audio features for harmony-based music.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 18(3):649–662, 2010.\n[11] Oriol Nieto, Morwaread Farbood, Tristan Jehan, and\nJuan Pablo Bello. Perceptual analysis of the f-measure\nto evaluate section boundaries in music. In Proceedings\nof ISMIR , Taipei, Taiwan, 2014.\n[12] Jouni Paulus and Anssi Klapuri. Acoustic features\nfor music piece structure analysis. In Proceedings of\nthe International Conference on Digital Audio Effects\n(DAFx) , pages 309–312, Espoo, Finland, 2008.[13] Jouni Paulus and Anssi Klapuri. Music structure anal-\nysis using a probabilistic ﬁtness measure and a greedy\nsearch algorithm. IEEE Transactions on Audio, Speech\n& Language Processing , 17(6):1159–1170, 2009.\n[14] Jouni Paulus, Meinard M ¨uller, and Anssi Klapuri.\nAudio-based music structure analysis. In Proceedings\nof ISMIR , pages 625–636, Utrecht, The Netherlands,\n2010.\n[15] Marcus T. Pearce, Daniel M ¨ullensiefen, and Geraint A.\nWiggins. The role of expectation and probabilistic\nlearning in auditory boundary perception: A model\ncomparison. Perception , 39:1367–1391, 2010.\n[16] Geoffroy Peeters and Emmanuel Deruty. Is music\nstructure annotation multi-dimensional? A proposal\nfor robust local music annotation. In Proceedings of\nthe International Workshop on Learning the Semantics\nof Audio Signals , pages 75–90, Graz, Austria, 2009.\n[17] Antonio Pertusa and Jos ´e Manuel I ˜nesta. Note on-\nset detection using one semitone ﬁlter-bank for mirex\n2009. In MIREX Audio Onset Detection , Kobe, Japan,\n2009.\n[18] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. IEEE Transactions on Audio, Speech\nand Language Processing , 20:1759–1770, 2012.\n[19] Chris Sanden, Chad R. Befus, and John Z. Zhang.\nA perceptual study on music segmentation and\ngenre classiﬁcation. Journal of New Music Research ,\n41(3):277–293, 2012.\n[20] Jordan B. L. Smith. Explaining listener differences in\nthe perception of musical structure. September 2014.\n[21] Jordan B. L. Smith, J. Ashley Burgoyne, Ichiro Fuji-\nnaga, David De Roure, and J. Stephen Downie. De-\nsign and creation of a large-scale database of structural\nannotations. In Proceedings of ISMIR , pages 555–560,\nMiami, FL, United States, 2011.\n[22] Jordan B. L. Smith and Elaine Chew. Using Quadratic\nProgramming to estimate feature relevance in struc-\ntural analyses of music. In Proceedings of the ACM\nInternational Conference on Multimedia , pages 113–\n122, Barcelona, Spain, 2013.\n[23] Jordan B. L. Smith, Isaac Schankler, and Elaine Chew.\nListening as a creative act: Meaningful differences\nin structural annotations of improvised performances.\nMusic Theory Online , 20(3), 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 441"
    },
    {
        "title": "Multi-Part Pattern Analysis: Combining Structure Analysis and Source Separation to Discover Intra-Part Repeated Sequences.",
        "author": [
            "Jordan B. L. Smith",
            "Masataka Goto"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417685",
        "url": "https://doi.org/10.5281/zenodo.1417685",
        "ee": "https://zenodo.org/records/1417685/files/SmithG17.pdf",
        "abstract": "Structure is usually estimated as a single-level phe- nomenon with full-texture repeats and homogeneous sec- tions. However, structure is actually multi-dimensional: in a typical piece of music, individual instrument parts can re- peat themselves in independent ways, and sections can be homogeneous with respect to several parts or only one part. We propose a novel MIR task, multi-part pattern analysis, that requires the discovery of repeated patterns within in- strument parts. To discover repeated patterns in individual voices, we propose an algorithm that applies source separa- tion and then tailors the structure analysis to each estimated source, using a novel technique to resolve transitivity er- rors. Creating ground truth for this task by hand would be infeasible for a large corpus, so we generate a synthetic corpus from MIDI files. We synthesize audio and produce measure-by-measure descriptions of which instruments are active and which repeat themselves exactly. Lastly, we present a set of appropriate evaluation metrics, and use them to compare our approach to a set of baselines.",
        "zenodo_id": 1417685,
        "dblp_key": "conf/ismir/SmithG17",
        "keywords": [
            "structure",
            "single-level",
            "phenomenon",
            "full-texture",
            "homogeneous",
            "sections",
            "multi-dimensional",
            "instrument parts",
            "pattern analysis",
            "source separation"
        ],
        "content": "MULTI-PART PATTERN ANALYSIS:\nCOMBINING STRUCTURE ANALYSIS AND SOURCE SEPARATION\nTO DISCOVER INTRA-PART REPEATED SEQUENCES\nJordan B. L. Smith Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\njordan.smith@aist.go.jp, m.goto@aist.go.jp\nABSTRACT\nStructure is usually estimated as a single-level phe-\nnomenon with full-texture repeats and homogeneous sec-\ntions. However, structure is actually multi-dimensional: in\na typical piece of music, individual instrument parts can re-\npeat themselves in independent ways, and sections can be\nhomogeneous with respect to several parts or only one part.\nWe propose a novel MIR task, multi-part pattern analysis,\nthat requires the discovery of repeated patterns within in-\nstrument parts. To discover repeated patterns in individual\nvoices, we propose an algorithm that applies source separa-\ntion and then tailors the structure analysis to each estimated\nsource, using a novel technique to resolve transitivity er-\nrors. Creating ground truth for this task by hand would\nbe infeasible for a large corpus, so we generate a synthetic\ncorpus from MIDI ﬁles. We synthesize audio and produce\nmeasure-by-measure descriptions of which instruments are\nactive and which repeat themselves exactly. Lastly, we\npresent a set of appropriate evaluation metrics, and use\nthem to compare our approach to a set of baselines.\n1. INTRODUCTION\nMusic structure is important to listeners and researchers,\nbut annotating music is hard because typical songs include\nmultiple independent instrument parts. For example, if two\nsections share the same basic melody, but one features an\nextra horn part, should one section be labeled as a repeti-\ntion of the other? To decide, the annotator must consider\nall the ways in which the two sections are similar or dif-\nferent, but the outcome of their decision is encoded in a\nsingle bit: whether the label is the same or not. The anno-\ntation discards many of the decisions made by the listener,\nespecially when these are made at the timescale of entire\nsections. For example, the second verse of Oasis’ “Won-\nderwall” has the same chords and melody as the ﬁrst, but\ndifferent lyrics, and it includes two new instruments, cello\nand drums—the latter of which enters a measure late. A\nc\rJordan B. L. Smith, Masataka Goto. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Jordan B. L. Smith, Masataka Goto. “Multi-part pattern\nanalysis: Combining structure analysis and source separation to discover\nintra-part repeated sequences”, 18th International Society for Music In-\nformation Retrieval Conference, Suzhou, China, 2017.\nFigure 1 . Example multi-part pattern description for the\nﬁrst 40 measures of“Come Together”. Measures that re-\npeat are given the same letter label. In this and later ﬁgures,\nthe colors highlight repeated sequences instead of individ-\nual labels: if label iis always followed by j, andjalways\nfollows i, their color assignments are merged.\nsingle large-scale section label cannot encode this interest-\ning situation.\nThe multi-dimensional nature of structure has been\ncommented on [22], and recent corpora of annotations\nhave addressed it in different ways: the SALAMI dataset\nprovides descriptions at two timescales, and of functions\nand leading instrument [29], and the INRIA dataset de-\nscribes how segments and their component patterns are hi-\nerarchically related [2]. For music cognition research, [26]\nsuggested that music be annotated multiple times on a per-\nfeature basis: e.g., once while focusing only on harmony,\nagain while focusing on timbre, and so forth. However,\nthe challenge of hierarchy is different from the challenge\nof multiple independent parts. We argue that estimating\nthe structure of these independent parts—i.e., creating a\nmulti-part pattern analysis—should be a new MIR goal.\nAn example of a multi-part pattern description is shown\nin Fig. 1. It is derived from a MIDI transcription of “Come\nTogether” by The Beatles from the Lakh dataset [23].\nIt indicates whenever an instrument in the mixture re-\npeats itself, at the timescale of measures. This represen-\ntation makes clear that the organ part (here substituting\nthe lead vocals) is varied in the second verse, while the\nother instruments repeat themselves exactly. Compared to\na one-dimensional structural analysis, the richer detail of a\nmulti-part description would be more suitable for applica-716tions like automatically editing videos or choreographies\nto match an audio ﬁle.\nWe make four main contributions in this work. First,\nwe deﬁne a new goal for MIR research; second, we pro-\npose an algorithm for accomplishing it, which uses exist-\ning technology and some new techniques; third, we pro-\npose an evaluation framework for the task, including met-\nrics, baselines, and how to obtain ground truth; and ﬁnally,\nwe conduct an evaluation.\nIn the next section, we discuss how our proposed task\nrelates to existing MIR tasks. We present our algorithm in\nSection 3, present the evaluation framework in Section 4,\nand discuss the results in Section 5.\n2. RELATED WORK\nIdentifying repeating motives has long been of interest\nto musicologists in MIR. Although most research in this\narea has focused on symbolic data analysis (see, e.g., [5]),\nwhen “Discovery of Repeated Themes” was added to\nMIREX in 2013, it included both symbolic and audio\ntracks (e.g., [21])—but the focus of that task is different:\nin it, the challenge is precisely to ignore the differences\nbetween instruments (if the piece being analyzed contains\nmultiple parts) as well as, potentially, to ignore differences\nin key or modality. Our task, multi-part pattern analysis,\ninvolves a separate challenge: discovering repetitions ex-\npressed by a single voice within the mixture.\nSince it involves describing the independent patterns in\na mixture of tracks, the task is clearly related to source sep-\naration. Recently, approaches to source separation have\nbecome more structural, taking better advantage of the\nredundancies offered by repetition in music. One com-\nmon technique, non-negative matrix factorization (NMF),\nseparates sources by modeling steady states in the spec-\ntrum; an extended version, NMF decomposition (NMFD),\nmodels short sequences that are time-varying but exactly-\nrepeating [28], and NMF was recently used to detect\nlong loops [15]. Median ﬁltering, which was used to ef-\nﬁciently perform harmonic-percussive source separation\n(HPSS) [6], was used in the REPET algorithm to separate\na repeating background from a mixture [14]; REPET was\nlater adapted to looping backgrounds that change over time\nand heterogenous backgrounds [25]. Although estimating\na multi-part pattern analysis will require source separa-\ntion, the desired output is an abstract description, not a set\nof separated tracks. Thus, whereas a source separation is\nevaluated with signal reconstruction error, a pattern analy-\nsis will be evaluated more like a structural analysis.\nAs for structural analysis, it has evolved toward mod-\neling hierarchy. Early segmentation-only approaches [7]\nwere followed by approaches that also estimate labels [8],\nand by approaches that model similarity differently at dif-\nferent timescales [11]. Since the creation of the multi-scale\nSALAMI and INRIA annotations, approaches to hierarchi-\ncal description have been reﬁned [17], as has the methodol-\nogy for evaluating them [18]. Hierarchy is partly a conse-\nquence of multiple sources behaving independently: three\nrepetitions of the chorus could be considered the same at a\nChromaPiano rollsMIDI fileAudio filePiano rollsPiano rollsPiano rolls(N channels)Ground truth downbeatsCenterLeftRight H.Center H.Left H.Right P.Center P.Left P.Analyze sequence structureRight1.BHarmonicpercussive source separation1.AStereo-based source separationCompare subsequences\nAlgorithm input\nGround truthsourceMulti-track estimated descriptionMulti-channel ground truth description\nEvaluation\nRMSActivation functionRecurrence plot3.Threshold to minimize transitivity errors2.k-means binary thresholdSegment labelsFigure 2 . Algorithm and ground truth generation pipeline.\ncoarse timescale (the context of the song), but differences\nin range or instrumentation could differentiate them at a\nﬁner timescale (the context of the three choruses). Mod-\nels of hierarchy will always be ambiguous, since its per-\nception is ambiguous [12]. In contrast, the multi-layered\ncomposition of a song can be described more concretely.\nThus, multi-part pattern analysis is worth treating sepa-\nrately from hierarchical structure, and a good multi-part\nanalysis may be very useful for describing hierarchy.\nFinally, two works have directly bridged source separa-\ntion and structural analysis: First, [10] found that structure\nanalysis could be performed more accurately with multi-\ntrack audio as input. Second, [27] discovered that spikes\nin the reconstruction error of a source separation algorithm\ncan indicate structural boundaries. In deﬁning the task of\nmulti-part pattern description, we hope to bring these ﬁelds\ncloser together.\n3. PROPOSED APPROACH\nOur proposed algorithm is outlined in Fig. 2, and data at\ncertain intermediate steps are illustrated in Fig. 3. The\nthree key stages of the algorithm are:\n1. Source separation. We apply source separation to\nthe audio to convert the stereo recording to an estimated\nmulti-track recording. We do this with two median spectral\nﬁlters [13]: ﬁrst, we take the median of the left and right\nchannels to estimate the center channel, and subtract this\nfrom the original signals, resulting in three tracks. Second,\nwe apply HPSS to each track [6]. Even if a track con-\ntains multiple pitched instruments, HPSS can separate in-\nstruments with different attacks, such as piano vs. strings,\nor rhythm guitar vs. organ. We end up with 6 audio tracks\n(see Fig. 3b).\n2. Activation function estimation. The separated\ntracks may be sparse: e.g., if the left channel contains\nonly strings, the left-percussive component may be nearly\nempty. We compute RMS to estimate when the channels\nare active. At this stage, we also use the ground truth\ndownbeat labels to deﬁne our segment windows. In futureProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 7170:00 0:50 1:40 2:30 3:20(a) Original audio\n(b) Source-separated audio\n0 10 20 30 40 50 60 70(c) Estimated activation functions\n0 25 50(d) Center-Harmonic SSM\n0 25 500\n20\n40\n60(e) Thresholded SSM\n0 10 20 30 40 50 60 700\n5(f) Estimated descriptionFigure 3 . Data in intermediate stages of algorithm\npipeline. The SSMs in plots (d) and (e) correspond to the\ncenter-harmonic track, which is the ﬁrst track in plots (b)\nand (c). Sound example is “Across The Universe.”\nwork, beats and downbeats could be estimated instead.\nWe take the mean over each window (i.e., each mea-\nsure), and apply a k-means clustering to the RMS values,\nwithk= 2, to classify windows as either silent or active.\nEven if the classes are very uneven, the difference between\nthe two with respect to RMS tends to be extreme enough\nthat this method is effective. At the end of this stage we\nhave a set of estimated activation functions (see Fig. 3c).\n3. Sequence analysis. We use self-similarity matri-\nces (SSMs) to discover repetitions in each track. We com-\npute chroma with the madmom package [4] and compute a\nmeasure-indexed SSM: element i; jgives the cosine simi-\nlarity between the sequences of beat-synchronized chroma\nfeatures of the ithandjthmeasures. We also use the\npreviously-estimated activation functions to zero out the\nSSM when the track was judged inactive, as shown at the\nbeginning and end of the track in Fig. 3d.\nTo estimate segment labels from the real-valued SSM,\nwe choose a threshold tto binarize the matrix; then, to em-\nphasize diagonal lines, we apply a single erosion-dilation\noperation (in time-lag space) with a kernal size k. We\nchoose tandkin a novel way: my ﬁnding the values that\nminimize the number of transitivity errors. These errors\nare resolved with a novel lexical-sort approach. Transitiv-\nity errors are cases where a segment iis judged to be sim-\nilar to both jandk, but segments jandkare not similar\nto each other; resolving these inconsistencies is a difﬁcult\npart of interpreting structure from SSMs (e.g., see [20]).\n123456781234567814258367123456781425836714258367\n1425836712345678142583671234567812345678sort columnssort rows\nre-order rowsre-order columnsdelete errors (X) 14258367\nXXYYZZFigure 4 . Eliminating transitivity errors with lexical sort-\ning. Errors appear as inconsistent blocks in the sorted\nSSM. We can ﬁx the error by eliminating pixels X, or pix-\nels Y , or adding pixels at Z.\nGiven a binary SSM, we can collect repeating pixels\ninto groups by sorting the rows lexically (i.e., alphabeti-\ncally). The process is illustrated in Fig. 4: after sorting the\nSSM’s rows and columns, groups of repeating elements\nbecome blocks on the main diagonal, and all other pixels\nrepresent transitivity errors. The third SSM in Fig. 4 can\nbe ﬁxed in three ways: zeroing the pixels at X, or at Y ,\nor adding pixels at Z. We greedily eliminate the errors by\nwalking along the main diagonal from the upper left and\ndiscarding off-diagonal elements that do not ﬁt the current\nblock, which corresponds to zeroing X. When the cleaned\nSSM is re-ordered, the result is guaranteed to be transitive.\nWe call the number of pixels deleted from an SSM the\n“strain”, and the number of off-diagonal pixels that remain\nthe “coverage”. (For the example in Fig. 4, strain is 2 and\ncoverage is 4.) Our goal is to choose tandkto maximize\ncoverage and minimize strain, while avoiding redundant\ncases such as an empty SSM or an SSM that is all ones.\nWe sweep values of tbetween 0:99and0:8, and kbe-\ntween 4 and 8 measures. A set of real-world examples\nare shown in Fig. 5. The left column contains 5 binary\nSSMs derived from chroma computed on an audio track.\nThe second and third columns give the lexically-ordered\nSSMs (and their strains) and their cleaned versions (and\ncoverages). The fourth column gives the cleaned SSMs\nand the difference between coverage and strain, which is\nmaximized by choosing k= 7. The result is a binary SSM\nthat is sparse but not empty, and free of transitivity errors,\nas in Fig. 3e. It is then trivial to label the segments. The\nsix estimated part descriptions are collected in Fig. 3f.\nIn structure analysis, we typically search for long re-\npeating subsequences and long homogeneous stretches,\nand apply strong smoothing to the SSM to gloss over varia-\ntions. In contrast, the above pipeline was designed to focus\non tracking shorter patterns and to ﬁnd when they repeat\nexactly, with the expectation of obtaining a much sparser\nSSM with few transitivity errors.\n4. EVALUATION FRAMEWORK\n4.1 Data and Ground Truth\nTo test the quality of a multi-part pattern analysis algo-\nrithm, we need audio ﬁles with multiple layers, with each718 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Difference: –16Difference: +10Difference: –6Difference: –68Difference: –6Figure 5 . Illustration of strain-coverage optimization ap-\nproach on a track estimated from a recording of “All My\nLoving.” The four columns, from left, give: (1) binary\nSSMs ﬁltered with different kernel sizes; (2) lexically\nsorted SSMs; (3) SSMs with errors removed; (4) cleaned\nSSMS restored to original column and row order. Kernel\nsize 7 maximizes coverage while minimizing strain.\nlayer annotated to indicate repeating patterns. Creating\nground truth for this task by hand would be infeasible for\na large corpus. There are many public datasets of multi-\ntrack audio, but only rarely are the tracks annotated in\ndetail. The existing dataset that most closely meets our\nneeds is MedleyDB [3], which contains multitrack audio\nand melody f0 annotations for a subset of stems, but not\nannotations of repetitions in each track.\nHowever, we can generate an appropriate dataset from\nMIDI. We used a portion of Lakh MIDI dataset [23] called\nthe “Clean MIDI subset”, which contains most of the Beat-\nles catalogue, and used FluidSynth1to convert these to au-\ndio ﬁles. When there were duplicate MIDI ﬁles to choose\nfrom, we selected the version where the average panning\nsetting of the tracks had the highest standard deviation.\n(Many MIDI transcriptions have no panning information\nat all, which would work against our algorithm.)\nWe processed the MIDI ﬁles (using Pretty MIDI [24])\nto create, for each MIDI channel, a ground truth descrip-\ntion of the measure-level patterns. The procedure for this\nis similar to our analysis algorithm (see Fig. 3). From\na downbeat-segmented piano roll (Fig. 6a), we obtain an\nactivation pattern, i.e., a timeline of 1s and 0s indicating\nwhether an instrument has any MIDI note events during\neach measure-long window (Fig. 6b). Next, we estimate\nthe similarity of every pair of measures within a track with\nan SSM (Fig. 6c). To compare two piano roll windows,\nwe take the percentage of active note spans that overlap.\nTo focus on exact repetitions, we should use a threshold of\n1http://www.fluidsynth.org/\nFigure 6 . Data in intermediate stages of ground truth gen-\neration for the vocal channel of “All My Loving.” The\nsong’s multi-part description is shown in (e).\n1.0, but in practice, due to small timing differences and ex-\npressive gestures in the MIDI transcription, a threshold of\n1.0 leads to extremely sparse recurrence plots—but on the\nother hand, lowering the threshold can lead to transitivity\nerrors, as before. However, we found that a threshold of\n0.9 was generally suitable to obtain non-empty recurrence\nplots without producing a large number of transitivity er-\nrors (Fig. 6d). Doing this for every track gives a multi-part\ndescription (Fig. 6e).\n4.2 Evaluation Metrics\nAfter processing the MIDI data, we obtain a “ground truth”\nmatrix of instrument patterns Awhere the element Ai;j\nindicates the pattern label for the ithinstrument during the\njthmeasure. (Such information is displayed in Fig. 1 and\nFig. 6e.) We set Ai;j= 0 when the ithinstrument is not\nactive. Similarly, we can obtain an estimated description\nEwith elements Ei;j, such as in Fig. 3f.\nTo compare two single-track descriptions (two rows\nofAandE), we can use any metrics from the ﬁeld of\nstructure analysis, such as the pairwise f-measure met-\nric [19]. (For a comparison of structure evaluation metrics,\nsee [16].) However, the rows of AandEare not necessar-\nily aligned in the correct order. Moreover, the number of\nestimated tracks in Emay be smaller or greater than the\nnumber of MIDI channels in A. We present two sets of\nevaluation metrics: one that requires matching the layers,\nand one that does not. We also devise a set of baselines.\nEvaluation of descriptions. Suppose we have an N-\nlayer estimated description and an M-layer ground truthProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 719Figure 7 . Optimal pairing of estimated tracks (columns)\nwith ground truth channels (rows), according to pairwise\nf-measure between descriptions (which are illustrated as\nrecurrence plots) The mean pwfis 0.44.\ndescription, and let L= min( M; N ). We can compute\nthe pairwise f-measure between all pairs of layers, giv-\ning a table of values like the one in Fig. 7. The Hungar-\nian Algorithm2gives us the optimal one-to-one match-\ning between Llayers to maximize the average f-measure.\nUsing the optimal pairings, we can compute the average\nprecision and recall. Together, these serve as our set of\n3 “generous” metrics, since it does not punish cases when\nN6=M. If there are unmatched layers, whether in AorE,\nthese should count against the estimate. We can compute a\nstricter mean f-measure by taking pwf\u0003L=max(M; N ).\nEvaluation of activations. The activation matrix that\nwe estimate (e.g., Fig. 3c) is an important intermediate\nstep. It is worth evaluating on its own, and we can do\nso without matching tracks to channels. We treat each\ncolumn of the activation matrix, an N-dimensional binary\nvector, as a ‘timbre label,’ such that each unique column\ngets a unique label. (This calls to mind the timbre-mixture\nestimation of [1].) We perform the same process on the\nground truth activation matrix. Then we can use pairwise\nf-measure to compare the two sequences of timbre labels.\nThis metric ignores the difference between an instru-\nment being added to or subtracted from the mixture. To\nevaluate the retrieval of entrances and exits, we use a ver-\nsion of the boundary retrieval f-measure [19, p. 220],\ncounting each entrance (or exit) in the ground truth as be-\ning correctly estimated only if some instrument in the esti-\nmated description also enters (or exits, respectively) in the\nsame measure.\n4.3 Baseline Methods\nWe compare our algorithm against a set of naive baselines\nto gauge the success of our algorithm, but also to learn\n2https://en.wikipedia.org/wiki/Hungarian_\nalgorithmhow the proposed evaluation metrics behave. The labeling\nbaselines are:\n\u000fBconstant : all measures repeat the same pattern;\n\u000fBnull: all measures are unique;\n\u000fBperiodic : there are three concurrent tracks playing\nsequence loops of length 2, 4 and 8 measures: i.e.,\nthree sequences [ab]\u0003(i.e., abrepeated), [abcd]\u0003,\nand[abcdefgh ]\u0003;\n\u000fBblock: there are three concurrent tracks that alter-\nnate static textures with periods 2, 4 and 8 measures:\ni.e.,[ab]\u0003,[aabb]\u0003, and [aaaabbbb ]\u0003.\nThe activation matrix baselines are:\n\u000fBuniform : the song has a single texture;\n\u000fBbuildup : new instruments enter in measures 3, 5\nand 7.\nIn addition to these naive baselines, we tested two sim-\npliﬁed versions of our proposed approach. The ﬁrst skips\nthe source separation step: instead of estimating patterns\nfrom 6 separated tracks, we can estimate patterns from the\nfull-audio chroma features, and then duplicate the result 6\ntimes to match the number of estimated sources as the pro-\nposed approach (“Chr. w/o SS”). Second, since the activa-\ntion matrix is evaluated as if it were a timbre label, we also\nestimate timbre labels by computing full-audio MFCCs,\nand using NMF to label the measures (“MFCCs”). All sec-\ntion transitions are treated as predictions of entrances and\nexits.\n5. RESULTS AND DISCUSSION\nWe applied all the approaches described above to the\ndataset of 200 Beatles songs. The results for the multi-\npart pattern description task are shown in Table 1 (“Stan-\ndard approach”). We ﬁnd that the proposed approach out-\nperforms the naive baselines, but that the simpler approach\nthat skips the source separation step performs even better,\neven though it has lower recall. The pwfvalues are al-\nmost all dominated by the lower precision values; like in\nstructure analysis, it seems harder to achieve high preci-\nsion than high recall. By tweaking the evaluation metric,\nwe can understand why. In the bottom half of the table, we\ncompute pwfcounting the elements on the main diagonal.\nTheBnull baseline, which guesses that every measure is\ndifferent, now becomes very competitive.\nThe explanation is that unlike in the usual structure\nanalysis task, the ground truth for this task is very sparse.\nRecall that pairwise f-measure tells us how well the sim-\nilarity relationships of one description are captured by the\nsimilarity relationships in another. In other words, given\ntwo binary SSMs that encode similarity descriptions, pwf\nassesses how well the positive parts of these SSMs coin-\ncide. Since it is trivial to guess that each segment is similar\nto itself, we should ignore the contributions of the main di-\nagonal. This does not usually affect the outcome of struc-\nture evaluation, since the repeating blocks ensure that the720 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Standard approach\nGenerous Strict\npwfpwppwrpwf\nProposed .245 .211 .71 .184\nChr. w/o SS .297 .312 .529 .222\nBconstant .144 .092 .95 .106\nBnull 0 0 0 0\nBperiodic .149 .136 .318 .112\nBblock .06 .103 .074 .044\nCounting self-labeled measures\npwfpwppwrpwf\nProposed .365 .309 .819 .274\nChr. w/o SS .466 .442 .695 .346\nBconstant .183 .115 .962 .135\nBnull .515 .749 .477 .384\nBperiodic .255 .218 .461 .192\nBblock .411 .44 .465 .309\nTable 1 . Above: results for estimated multi-track descrip-\ntion quality using the proposed metric. Below: the results\nif self-labeled measures are counted as correct. The high\nretrieval for Bnullillustrates the sparseness of the ground\ntruth.\nground truth SSM has very many off-diagonal pixels to es-\ntimate. However, in our application, the ground truth ma-\ntrices are extremely sparse: in cases where a part never\nrepeats itself exactly, there are no off-diagonal elements.\nOn the other hand, this task is unlike structure analysis\nbecause in our case, elements on the main diagonal can\nequal 0, if the corresponding source is not active. This\nmeans that the Bnull baseline does not actually achieve\nperfect precision: from the bottom part of Table 1 we can\nsee that on average, sources are active for 75% of the song.\nResults for the activation detection task are shown in\nTable 2. According to the pwfmeasure, the best approach\nto characterize the changing timbre of the piece was our\nproposed one. However, the uniform baseline performs\nalmost as well according to this metric. Although some\nsongs have over a dozen tracks, with many entrances and\nexits, it seems that the majority of songs have an instru-\nmentation that changes little. As a result, the uniform and\nbuildup baselines achieve near-perfect recall while preci-\nsion does not fall below 30%. That said, these naive base-\nlines fail to detect nearly all the entrances and exits of\ninstruments from the mixture, so the proposed approach\nbeats them handily on entrance/exit f-measure.\nIn contrast, the MFCC approach tends to ﬁnd a majority\nof the entrances and exits, and narrowly beats the proposed\napproach in terms of entrance/exit f-measure. The cost of\nthis apparent over-segmentation is lower pairwise retrieval,\nand the lowest overall pwf, for labeling the timbres.\nIn designing the evaluation, we made an effort to re-\nuse metrics that are used for structure analysis. We did\nnot expect the sparseness of the ground truth to have such\nan impact on the metrics, but the impact is plain to see\nin the success of the baselines. Perhaps we should haveTimbre labeling Entrance/exit\npwfpwppwrf p r\nProposed .450 .456 .546 .248 .271 .296\nMFCCs .3 .549 .319 .273 .195 .566\nBuniform .433 .306 .962 0 0 0\nBbuildup .446 .328 .909 .071 .351 .045\nTable 2 . Results for estimated activation matrix quality.\nanticipated this: data sparseness is often a problem when\ntranslating a one-dimensional function (here, the overall\nstructure) into a higher-dimensional space (a per-channel\nrepresentation). One potential way to resolve this issue\nis to automatically process both the ground truth and the\nestimated descriptions using a ﬁxed sequences-to-blocks\nconversion step, such as that proposed by [9]. This would\nallow us to compare nearly-equivalent representations that\nare much less sparse.\nNeedless to say, the multi-track analysis approach we\nhave proposed could be improved in many ways. We have\nused two source separation kernels, in a ﬁxed way, but it is\npossible to apply more kernels, and to do so in an optimiza-\ntion framework to increase the independence of the esti-\nmated tracks [13]. Future work should also test a greater\nvariety of source separation methods, especially NMF-\nbased approaches. However, this ﬁrst effort has helped us\nto understand the special challenge of this task, which is\nthe sparseness of the ground truth.\n6. CONCLUSION AND FUTURE WORK\nWe have described a new MIR task, multi-part pattern\nanalysis, in which the goal is to describe each indepen-\ndent layer of a piece of music. The task complements re-\ncent work on estimating hierarchical structure. We have\nalso proposed a method for estimating multi-part pattern\nanalyses using a combination of existing source-separation\ntools, SSM-based structure estimation methods, and a\nnovel approach to thresholding SSMs in order to minimize\ntransitivity errors.\nTo support future work on this problem, we have pro-\nposed a method of creating ground truth annotations from\nMIDI ﬁles, and a set of evaluation metrics that can esti-\nmate the similarity between two multi-part descriptions or\ntwo multi-part activation functions.\nIn our evaluation, we found the sparseness of the data to\nbe an issue, but it is a direct consequence of how we chose\nto create the ground truth. As we reﬁne the methodology\nfor this task in future work, we will study the impact of dif-\nferent ways of converting multi-channel ﬁles into ground\ntruth recurrence plots.\n7. ACKNOWLEDGEMENTS\nThis work was supported in part by JST ACCEL Grant\nNumber JPMJAC1602, Japan.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 7218. REFERENCES\n[1] J.-J. Aucouturier and Mark Sandler. Segmentation of\nmusical signals using hidden Markov models. In Proc.\nof the Audio Engineering Society Convention , Amster-\ndam, The Netherlands, 2001.\n[2] Fr ´ed´eric Bimbot, Gabriel Sargent, Emmanuel Deruty,\nCorentin Guichaoua, and Emmanuel Vincent. Semiotic\ndescription of music structure: An introduction to the\nQuaero/Metiss structural annotations. In Proc. of the\nAES Conference on Semantic Audio , 2014.\n[3] Rachel M. Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo\nBello. MedleyDB: A multitrack dataset for annotation-\nintensive mir research. In Proc. of ISMIR , pages 155–\n160, Taipei, Taiwan, 2014.\n[4] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: a new\nPython Audio and Music Signal Processing Library.\nInProc. of the ACM International Conference on Mul-\ntimedia , pages 1174–1178, Amsterdam, The Nether-\nlands, 2016.\n[5] Tom Collins, Andreas Arzt, Sebastian Flossman, and\nGerhard Widmer. SIARCT-CFP: Improving precision\nand the discovery of inexact musical patterns in point-\nset representations. In Proc. of ISMIR , pages 549–554,\nCuritiba, Brazil, 2013.\n[6] Derry FitzGerald. Harmonic/percussive separation us-\ning median ﬁltering and amplitude discrimination. In\nProc. of the International Conference on Digital Audio\nEffects , Graz, Austria, September 2010.\n[7] Jonathan Foote. Automatic audio segmentation using\na measure of audio novelty. In Proc. of the IEEE In-\nternational Conference on Multimedia & Expo , pages\n452–455, 2000.\n[8] Jonathan Foote and Matthew Cooper. Media segmen-\ntation using self-similarity decomposition. In Minerva\nYeung, Rainer Lienhart, and Chung-Sheng Li, editors,\nProc. of the SPIE: Storage and Retrieval for Media\nDatabases , volume 5021, pages 167–175, Santa Clara,\nCA, USA, 2003.\n[9] Harald Groghanz, Michael Clausen, Nanzhu Jiang,\nand Meinard M ¨uller. Converting path structures into\nblock structures using eigenvalue decompositions of\nself-similarity matrices. In Proc. of ISMIR , 2013.\n[10] Steven Hargreaves, Anssi Klapuri, and Mark San-\ndler. Structural segmentation of multitrack audio. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 20(10):2637–2647, 2012.\n[11] Tristan Jehan. Hierarchical multi-class self similarities.\nInProc. of the IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics , pages 311–314,\nNew Paltz, NY , United States, 2005.[12] Fred Lerdahl and Ray S. Jackendoff. A Generative The-\nory of Tonal Music . MIT Press, 1983.\n[13] Antoine Liutkus, Derry Fitzgerald, Zafar Raﬁi, Bryan\nPardo, and Laurent Daudet. Kernel additive models for\nsource separation. IEEE Transactions on Signal Pro-\ncessing , 62(16):4298–4310, 2014.\n[14] Antoine Liutkus, Zafar Raﬁi, Roland Badeau, Bryan\nPardo, and Ga ¨el Richard. Adaptive ﬁltering for mu-\nsic/voice separation exploiting the repeating musical\nstructure. In Proc. of the IEEE International Con-\nference on Acoustics, Speech and Signal Processing ,\npages 53–56, Kyoto, Japan, 2012. IEEE.\n[15] Patricio L ´opez-Serrano, Christian Dittmar, Jonathan\nDriedger, and Meinard M ¨uller. Towards modeling and\ndecomposing loop-based electronic music. In Proc. of\nISMIR , pages 502–508, New York, NY , USA, 2016.\n[16] Hanna Lukashevich. Towards quantitative measures of\nevaluating song segmentation. In Proc. of ISMIR , pages\n375–380, Philadelphia, PA, USA, 2008.\n[17] Brian McFee and Daniel Ellis. Analyzing song struc-\nture with spectral clustering. In Proc. of ISMIR , pages\n405–410, Taipei, Taiwan, 2014.\n[18] Brian McFee, Oriol Nieto, and Juan Pablo Bello. Hier-\narchical evaluation of segment boundary detection. In\nProc. of ISMIR , M´alaga, Spain, 2015.\n[19] Meinard M ¨uller. Music structure analysis. In Funda-\nmentals of Music Processing: Audio, Analysis, Algo-\nrithms, Applications , pages 167–236. Springer Inter-\nnational Publishing, 2015.\n[20] Meinard M ¨uller, Nanzhu Jiang, and Peter Grosche. A\nrobust ﬁtness measure for capturing repetitions in mu-\nsic recordings with applications to audio thumbnailing.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 21(3):531–543, 2013.\n[21] Oriol Nieto and Morwaread M. Farbood. Identifying\npolyphonic patterns from audio recordings using mu-\nsic segmentation techniques. In Proc. of ISMIR , pages\n411–416, Taipei, Taiwan, 2014.\n[22] Geoffroy Peeters and Emmanuel Deruty. Is music\nstructure annotation multi-dimensional? A proposal\nfor robust local music annotation. In Proc. of the Inter-\nnational Workshop on Learning the Semantics of Audio\nSignals , pages 75–90, Graz, Austria, 2009.\n[23] Colin Raffel. Learning-Based Methods for Comparing\nSequences, with Applications to Audio-to-MIDI Align-\nment and Matching . PhD thesis, Columbia University,\n2016.\n[24] Colin Raffel and Daniel P. W. Ellis. Intuitive anal-\nysis, creation and manipulation of midi data with\npretty midi. In ISMIR Late Breaking and Demo Pa-\npers, pages 84–93, 2014.722 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[25] Zafar Raﬁi, Antoine Liutkus, and Bryan Pardo. REPET\nfor background/foreground separation in audio. In\nG. R. Naik and W. Wang, editors, Blind Source Sepa-\nration , Signals and Communication Technology, pages\n395–411. Springer-Verlag, 2014.\n[26] Chris Sanden, Chad R. Befus, and John Z. Zhang.\nA perceptual study on music segmentation and\ngenre classiﬁcation. Journal of New Music Research ,\n41(3):277–293, 2012.\n[27] Prem Seetharaman and Bryan Pardo. Simultaneous\nseparation and segmentation in layered music. In Proc.\nof ISMIR , pages 495–501, New York, NY , USA, 2016.\n[28] Paris Smaragdis. Non-negative matrix factor decon-\nvolution: Extraction of multiple sound sources from\nmonophonic inputs. In Independent Component Anal-\nysis and Blind Signal Separation , volume 3195 of\nLecture Notes in Computer Science , pages 494–499.\nSpringer-Verlag, Berlin, Heidelberg, 2004.\n[29] Jordan B. L. Smith, J. Ashley Burgoyne, Ichiro Fuji-\nnaga, David De Roure, and J. Stephen Downie. Design\nand creation of a large-scale database of structural an-\nnotations. In Proc. of ISMIR , pages 555–560, Miami,\nFL, United States, 2011.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 723"
    },
    {
        "title": "Automatic Drum Transcription for Polyphonic Recordings Using Soft Attention Mechanisms and Convolutional Neural Networks.",
        "author": [
            "Carl Southall",
            "Ryan Stables",
            "Jason Hockman"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415616",
        "url": "https://doi.org/10.5281/zenodo.1415616",
        "ee": "https://zenodo.org/records/1415616/files/SouthallSH17.pdf",
        "abstract": "Automatic drum transcription is the process of generating symbolic notation for percussion instruments within audio recordings. To date, recurrent neural network (RNN) sys- tems have achieved the highest evaluation accuracies for both drum solo and polyphonic recordings, however the ac- curacies within a polyphonic context still remain relatively low. To improve accuracy for polyphonic recordings, we present two approaches to the ADT problem: First, to cap- ture the dynamism of features in multiple time-step hidden layers, we propose the use of soft attention mechanisms (SA) and an alternative RNN configuration containing ad- ditional peripheral connections (PC). Second, to capture these same trends at the input level, we propose the use of a convolutional neural network (CNN), which uses a larger set of time-step features. In addition, we propose the use of a bidirectional recurrent neural network (BRNN) in the peak-picking stage. The proposed systems are evalu- ated along with two state-of-the-art ADT systems in five evaluation scenarios, including a newly-proposed evalua- tion methodology designed to assess the generalisability of ADT systems. The results indicate that all of the newly proposed systems achieve higher accuracies than the state- of-the-art RNN systems for polyphonic recordings and that the additional BRNN peak-picking stage offers slight im- provement in certain contexts.",
        "zenodo_id": 1415616,
        "dblp_key": "conf/ismir/SouthallSH17",
        "keywords": [
            "drum transcription",
            "recurrent neural network",
            "polyphonic recordings",
            "soft attention mechanisms",
            "additional peripheral connections",
            "convolutional neural network",
            "bidirectional recurrent neural network",
            "peak-picking stage",
            "evaluation scenarios",
            "newly-proposed evaluation methodology"
        ],
        "content": "AUTOMATIC DRUM TRANSCRIPTION FOR POLYPHONIC\nRECORDINGS USING SOFT ATTENTION MECHANISMS AND\nCONVOLUTIONAL NEURAL NETWORKS\nCarl Southall, Ryan Stables and Jason Hockman\nDMT Lab, Birmingham City University\nBirmingham, United Kingdom\nfcarl.southall, ryan.stables, jason.hockman g@bcu.ac.uk\nABSTRACT\nAutomatic drum transcription is the process of generating\nsymbolic notation for percussion instruments within audio\nrecordings. To date, recurrent neural network (RNN) sys-\ntems have achieved the highest evaluation accuracies for\nboth drum solo and polyphonic recordings, however the ac-\ncuracies within a polyphonic context still remain relatively\nlow. To improve accuracy for polyphonic recordings, we\npresent two approaches to the ADT problem: First, to cap-\nture the dynamism of features in multiple time-step hidden\nlayers, we propose the use of soft attention mechanisms\n(SA) and an alternative RNN conﬁguration containing ad-\nditional peripheral connections (PC). Second, to capture\nthese same trends at the input level, we propose the use\nof a convolutional neural network (CNN), which uses a\nlarger set of time-step features. In addition, we propose the\nuse of a bidirectional recurrent neural network (BRNN) in\nthe peak-picking stage. The proposed systems are evalu-\nated along with two state-of-the-art ADT systems in ﬁve\nevaluation scenarios, including a newly-proposed evalua-\ntion methodology designed to assess the generalisability\nof ADT systems. The results indicate that all of the newly\nproposed systems achieve higher accuracies than the state-\nof-the-art RNN systems for polyphonic recordings and that\nthe additional BRNN peak-picking stage offers slight im-\nprovement in certain contexts.\n1. INTRODUCTION\nMusic notation, which portrays the instrumentation and\nplaying techniques used within a musical recording, is pro-\nduced through the process of automatic music transcription\n(AMT). Fast and accurate production of music notation\nwould beneﬁt multiple areas including the creative, analyt-\nical and educational industries. The majority of previous\nAMT systems has been developed to address pitched in-\nstrumentation, while relatively few systems have focussed\nc\rCarl Southall, Ryan Stables and Jason Hockman. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Carl Southall, Ryan Stables and Jason\nHockman. “Automatic Drum Transcription for Polyphonic Recordings\nUsing Soft Attention Mechanisms and Convolutional Neural Networks”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.on the transcription of percussive instruments. Automatic\ndrum transcription (ADT) systems soley focus on produc-\ning notation for drum instruments, which strongly portray\nthe rhythm, groove and feel of the piece. High ADT ac-\ncuracies have been achieved on audio recordings contain-\ning only basic drum classes such as kick drum, snare drum\nand hi-hats [15, 19]. However, accuracies are signiﬁcantly\nlower in a polyphonic context —in which the recordings\ncontain either additional percussion (e.g., toms, cymbals)\nor pitched instrumentation (e.g., guitar, piano) [20].\n1.1 Background\nSeveral early ADT systems have been proposed that per-\nform well on solo drum recordings [3, 5, 10, 13, 18, 23],\nhowever a relatively small number of systems have demon-\nstrated the capacity for high performance in a polyphonic\ncontext. Wu and Lerch [21] proposed a non-negative ma-\ntrix factorisation technique with a specialised basis func-\ntion to capture harmonic activity outside of those for the\ndrum classes under observation. Paulus et al. [12] used a\nhidden Markov model to detect the presence of individual\ndrum onsets within frames of a spectrogram. Southall et\nal. [15] and V ogl et al. [19] also formalise ADT as a frame-\nwise drum onset detection problem, using recurrent neural\nnetworks (RNN) for classiﬁcation. Southall et al. [15] pre-\nsented a bidirectional RNN (BRNN) system and V ogl et\nal. [19] presented a RNN system with time-shifted clas-\nsiﬁcation labels. RNN systems have achieved the best\ndrum solo performance to date, however their accuracies\nin the polyphonic context has been marginalised. V ogl et\nal. [20] later proposed the incorporation of gated recurrent\nunit (GRU) cells, which incorporate more time-step infor-\nmation into the RNN model, resulting in the highest ADT\naccuracies to date in a polyphonic context.\n1.2 Motivation\nThe increase in accuracy achieved by the GRU RNN in\n[20] over the standard RNN in [19] demonstrates the ef-\nfect of storing additional information on classiﬁcation per-\nformance. In a solo drum context, instrumentation over-\nlap is limited to the drums under observation, whereas in\na polyphonic context, drums are present along with other\ninstruments. This may obscure the presence of features be-\nlonging to the drums under observation, and is mitigated by\nthe incorporation of additional time-step information in the606xt-1 xtxt+1  Output LayerOutputHidden Layer 2Hidden Layer 1Inputyt   ~  xt-1 xtxt+1  yt   ~  \ntanhtanhsoftmax+tanhOutput LayerOutput\nHidden Layer 2Hidden Layer 1InputSoft Attention Mechanism\nSoft Attention BRNN (SA)\nBRNN with Peripheral Connections (PC)Figure 1 : Overview of the proposed SA and PC systems.\nSolid lines depict connections of a standard BRNN con-\nﬁguration and dashed lines depict additional SA and PC\nconnections when attention number a= 1.xtand~yare in-\nput features and output activation function at time step t.\f\nin the SA system represents element-wise multiplication.\nGRU RNN. Inclusion of additional information in previous\nRNN ADT systems however, is still restricted by a bottle-\nneck at the output layer, which is determined by the hidden\nstate sizes. Additionally, larger input feature sizes can not\nbe used at each time step, due to the computational cost of\nfully-connected layers. We present two approaches in an\nattempt to overcome the above-stated limitations to ADT\nin a polyphonic context: First, to capture the dynamism of\nfeatures in multiple time-step hidden layers, we propose\nthe use of soft attention mechanisms (SA) and an alter-\nnative RNN conﬁguration containing additional peripheral\nconnections (PC). Second, to capture these same trends at\nthe input level, we propose the use of a convolutional neu-\nral network (CNN), which uses a larger set of time-step\nfeatures. To further improve the accuracy of the systems,\nwe also propose the use of an additional BRNN for select-\ning drum onsets from the output activation functions, as\npeak-picking within a polyphonic context has proven to bemore difﬁcult than that of drum solos [15, 19, 20].\nThe remainder of this paper is structured as follows:\nSection 2 presents our three newly proposed systems and\nour new peak-picking technique. The evaluation is out-\nlined in Section 3 and the results are presented in Section\n4. Conclusions and future work are provided in Section 5.\n2. METHOD\nFor the three new proposed systems, we use the same\nframe-wise classiﬁcation ADT technique outlined in [15].\nInput features are fed into a separate pre-trained neural net-\nwork for each instrument under observation. Peak-picking\nis then performed on the resulting activation functions to\ndetermine onset locations.\n2.1 Soft Attention BRNN (SA)\nAttention mechanisms allow the network to focus on dif-\nferent parts of the data stored within a RNN for different\ntasks. This is achieved by enabling the information fed\nto the output layer to be created from multiple time-step\nﬁnal hidden layers. This was initially achieved through bi-\nnary connections in hard attention mechanisms and then by\nweighted connections in soft attention mechanisms (SA).\nThey have improved RNN results in multiple ﬁelds includ-\ning: machine translation [1] and image caption genera-\ntion [11, 22]. An overview of the implemented SA ADT\nsystem based on [6] is given at the top of Figure 1. We use\na BRNN with each hidden layer containing 100 long short-\nterm memory cells with peephole connections (LSTMP) as\nthe basis of the system. This is due to its ability to pass in-\nformation through its memory cell c, which is updated us-\ning the input i, forgetfand outputogates. The equations\nfor a LSTMP cell layer are:\nit\nl=\u001b(Wil\u0002\nxt;ht\u00001\nl;ct\u00001\nl\u0003\n+bil) (1)\nft\nl=\u001b(Wfl\u0002\nxt;ht\u00001\nl;ct\u00001\nl\u0003\n+bfl) (2)\n~ct\nl=tanh(Wcl\u0002\nxt;ht\u00001\nl;ct\u00001\nl\u0003\n) (3)\nct\nl=ft\fct\u00001\nl+it\f~ct\nl+bcl) (4)\not\nl=\u001b(Wol\u0002\nxt;ht\u00001\nl;ct\nl\u0003\n+bol) (5)\nht\nl=ot\nl\ftanh(ct\nl); (6)\nwhereht\nlis the hidden layer of layer lat time step t, the\nweightsW, and the biases b.xis the input feature where\nxt=ht\nl\u00001ifl > 1. After each hidden layer dropouts\n[16] are implemented with a probability of p. Based on\npreliminary tests, we use 2 hidden layers as using more\ndid not improve performance.\nThe SA feeds the LSTM BRNN output into the output\nlayer as a weighted combination of 2a+ 1time-step ﬁnal\nhidden layers, centred on the current time-step t, wherea\nis the attention number. First, an intermediate variable mis\ndetermined for each attention step i(i=t\u0000a:t+a) using\nthe concatenated outputs of the forwards and backwards\ndirectional LSTMs Q(Q=\u0002\ny!\nL,y \nL\u0003\n) and a context U:Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 60710 5x5 ﬁlters5 x v stridelength convolutional layer2 x e max pooling layerInput features2 x e max pooling layer100 neuron fully connectedlayer Output layerZeropaddingDropout layer+Batch normalisationZeropadding20 5x5 ﬁlters5 x 1 stride length convolutional layerDropout layer+Batch normalisation\nCNN21024 x 5 x 1205 x 5 x 10103 x 5 x 1021 x 5 x  2011 x 5 x  201002 CNN51024 x 11 x 1205 x 11 x 10103 x 11 x 1021 x 11 x 2011 x 11 x 201002 CNN101024 x 21 x 1205 x 21 x 10103 x 21 x 1021 x 21 x 2011 x 11 x  201002 CNN201024 x 41 x 1205  x 41 x 10103 x 21 x 1021 x 21 x  2011 x 11 x  2010021025 x 9 x 11025 x 15 x 11025 x 25 x 11025 x 45 x 1105 x 9 x 10105 x 15 x 10 105 x 25 x 10105 x 25 x 10(v = 1)(v = 1)(v = 1)(v = 1)(e = 1)(e = 1)(e = 1)(e = 2)(e = 1)(e = 1)(e = 2)(e = 2)SystemnameFigure 2 : Overview of the proposed CNN. Information ﬂows through the network from left to right; solid lines represent\nconnections, with dashed lines representing convolution and dash-dotted lines representing max pooling.\nmi=tanh(WqQi+WUU): (7)\nThe aim of Uis to feed the SA mechanism information\nregarding the wider scope of the current data. We ﬁrst at-\ntempted to use the the cell state of the ﬁnal hidden layer\ncLas in [8], however using the outputs of the ﬁrst hidden\nlayer (U=\u0002\nht!\n1;ht \n1\u0003\n) resulted in better performance\nduring preliminary testing. The attention weights sare de-\ntermined using a softmax function across i:\nsi/expWT\nmmi; (8)\nso thatP\nisi= 1. The output layer input zis then calcu-\nlated usingQandsand fed into an output layer similar to\nthe BRNN architecture in [15]:\nz=X\nisi\fQi(9)\n~yt=softmax (Wzz+bz): (10)\nscan be thought of as percentage determining how much\ninformation from each of the time-step ﬁnal hidden layers\nQis used in the input to the output layer z.\n2.2 BRNN with Peripheral Connections (PC)\nAlthough the SA system allows the information fed into\nthe output layer to be determined directly from multiple\ntime-step hidden layers, the amount of information is still\nlimited by the hidden layer size. We propose an increase in\nthe amount of information passed to the output layer by in-\ncluding direct connections from multiple time-step hidden\nlayers to the output layer, which we term peripheral con-\nnections (PC). An overview of the PC system is presented\nat the bottom of Figure 1. The PC system is the same as\nthe SA system in eqns. 1–6. However, these connections\nare implemented in the output layer using:\n~yt=softmax (WvQt\u0000a:t+a+bv); (11)\nwherevhighlights the weights and biases belonging to the\nPC output layer and Qt\u0000a:t\u0000ais the concatenation of mul-\ntiple LSTM time-step outputs:\nQ=\u0002\nh!t\u0000a\nL;::::;h!t+a\nL;h t\u0000a\nL;::::;h t+a\nL\u0003\n:(12)\nIfa= 0, then both the SA and PC systems are the same as\na standard BRNN network with LSTMP cells.2.3 Convolutional Neural Network (CNN)\nAs RNNs contain fully-connected layers, large input fea-\nture sizes can not be used as they become extremely com-\nputationally expensive. Convolutional neural networks\n(CNN) overcome this problem by combining feature learn-\ning, dimensionality reduction and classiﬁcation stages in a\nsingle trainable network. This ability has enabled CNNs\nto achieve higher accuracies than RNNs in the closely re-\nlated ﬁelds of onset detection [14] and downbeat detec-\ntion [4]. We propose to use a convolutional neural network\nto enable multiple time-step features to be used as input for\neach frame classiﬁcation. An overview of the implemented\nCNN ADT system is outlined in Figure 2 where jframes\non either side of the current frame tare included in the in-\nput features and different values of vandeare used asj\nis increased. It consists of two sets of convolutional, max\npooling, dropout [16], and batch normalisation [7] layers\nbefore a 100-neuron fully-connected layer and a two neu-\nron softmax output layer.\n2.4 Implementation\nThe newly proposed models are implemented using the\nTensorﬂow Python library. Four SA and PC systems ( SA1,\nSA2,SA3 andSA5) and ( PC1,PC2,PC3 andPC5) are im-\nplemented where a= [1, 2, 3, 5] and four CNN systems\n(CNN2 ,CNN5 ,CNN10 , and CNN20 ) are implemented where\nj= [2, 5, 10, 20]. These values are chosen as they cover\nvarious ranges of important information regarding the typ-\nical envelope length of drums.\n2.4.1 Input Features\nIn order for an audio ﬁle to be processed by the neural\nnetworks, it must be procedurally segmented into frame-\nwise spectral features. First, the input audio (16-bit .wav\nﬁle sampled at 44100 kHz) is segmented into Tframes\nusing a Hanning window of nsamples (n= 2048 ) with\nan\n4hopsize. A frequency representation of each of the\nframes is then created using the magnitudes of a discrete\nFourier transform resulting in an\n2xTspectrogram. The\nspectrogram is input into the SA systems in a frame-wise\nmanner and as a combination of frames ( jframes either\nside of the current frame t) for the CNN systems.\n2.4.2 Peak Picking\nOnce the activation functions ~Yare output from the sys-\ntems, peak-picking is used to identify the onset candidates.608 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017In this paper, we implement two peak-picking strategies\nfor each of the systems. The ﬁrst approach, termed mean\nthreshold ( MT), is an updated version of the technique used\nin [15], in which a threshold is determined for each frame\n(\u001ct) using:\n\u001ct=mean (~yt\u0000\u0012: ~yt+\u0012)\u0003\u0015 (13)\n\u001ct=\u001atmax; \u001c >tmax\ntmin; \u001c <tmin;(14)\nwhere\u0012sets the number of frames in each direction to cal-\nculate the mean, \u0015is a constant and tmax andtmin are\nthe possible maximum and minimum values. The current\nframe of ~yis accepted as an onset if it is the maximum of a\nsurrounding number of frames and above the threshold \u001c:\nOt=\u001a1;~yt==max(~yt\u0000\n: ~yt+\n) & ~yt>\u001ct\n0; otherwise;\n(15)\nwhereO(t)represents an onset at time step tand\nis the\nnumber of frames on either side of the current frame tused\nto calculate the maximum.\nFor the second approach we train an additional neural\nnetwork using the activation functions from the training\ndata in an attempt to learn to identify the drum onsets more\ndifﬁcult to detect. To do this we use a BRNN, with a single\n10 LSTMP-cell hidden layer and a softmax output layer.\nThe output of the new BRNN is then processed by the MT\ntechnique (eqns. 10–12), we refer to this second technique\nasBRNN-MT .\n2.4.3 Training\nThe three models and the BRNN-MT peak-picking networks\nare trained using the Adam optimiser [9] with a learning\nrate of 0.003. The training data is created by generating\na feature matrix from input features xand an associated\nclass vector from the target activation functions Y. Mini-\nbatch gradient descent (batch size = 1000) created from\n10 segments (segment length = 100) is used. The activa-\ntion function output from the models ~Yare used as the\ninput to the BRNN-MT networks which are trained using the\nsame targets used to train the systems. A new BRNN-MT\nnetwork is trained independently for each system in an at-\ntempt to increase adaptability, similar to [2]. Training is\nstopped when the following criteria have been met: (1) a\nminimum of 10 epochs have commenced; and (2) the vali-\ndation set accuracy has not increased between epochs. To\nensure training commences correctly, the weights are ini-\ntialised using a random uniform distribution scaled to keep\nconstant variance [17] and the biases are initialised to zero.\nCross entropy is used as the loss function.\n3. EVALUATION\nTo evaluate the newly proposed methods along with the\ncurrent state-of-the-art systems, we implement four eval-\nuations similar to those carried out in [15, 19, 20], along\nwith an additional evaluation to test the generalisabilityof the systems. The systems are trained to identify kick\ndrum, snare drum and hi-hat onsets. The ﬁrst evalua-\ntion, termed drum solo , aims to demonstrate system perfor-\nmance on drum solo recordings that contain only the three\ndrum instruments under observation. The second evalu-\nation, termed drum mixture , aims to demonstrate system\nperformance in a drum-only polyphonic context, where\nthe recordings contain additional drum instrumentation to\nthose under observation (e.g., toms and cymbals). The\nthird evaluation, termed multi-instrument mixture , aims\nto demonstrate system performance in a fully-polyphonic\ncontext where multiple instruments are present in addi-\ntion to the drum instruments under observation (e.g., pi-\nano and guitar) and the fourth evaluation, termed cross-\ncontext , aims to test the systems adaptability to before\nunseen timbres. The newly proposed evaluation, termed\nmulti-context , aims to test the ability of a single system to\nbe trained and used in multiple contexts.\n3.1 Evaluation Methodology\nF-measure is used as the evaluation metric with preci-\nsion and recall determined using the onset candidates from\nthe peak-picking stage. Detected onsets are accepted as\ntrue positives if they fall within 50ms of the ground truth\nannotations. The individual instrument F-measures are\ncalculated as the mean F-measure across test tracks and\nthe mean instrument F-measure is calculated as the mean\nF-measure across the individual instruments. The peak-\npicking parameters ( \u0012,\u0015,tmax ,tmin and\n) are found\nusing a grid-search on the validation set and the dropout\nprobabilitypis set to 0.25.\n3.1.1 Drum Solo Evaluation\nTo test the capability of the systems in the drum solo eval-\nuation, we use the updated version of the IDMT-SMT-\nDrums dataset [3]. This dataset contains 104 tracks di-\nvided into three subsets (20 real drum tracks, 14 techno\ndrum tracks, and 70 wave drum tracks) with an average\ntrack length of 15 seconds. The dataset is divided by track\nin equal distributions across the three subsets into 70%\ntraining 15% validation and 15% test sets. The training\nset is used to train the neural network systems, the vali-\ndation set to prevent overﬁtting during training and to op-\ntimise the peak-picking parameters, and the test subset is\nused as unseen data for testing. The four SA systems, the\nfour PC systems and the four CNN systems are evaluated\nalong with two current state-of-the-art ADT systems: (1)\ntanhB , a BRNN system containing tanh cells [15] and (2)\nlstmpB , a BRNN system containing LSTMP cells. The\nLSTMP architecture was chosen as it outperformed GRU\ncells in preliminary testing on the same datasets. Drum on-\nsets are selected from the output activation functions using\nthe two peak-picking techniques.\n3.1.2 Drum Mixture and Multi-instrument Evaluations\nTo determine system performance in a polyphonic context\nwe use the minusone subset of the ENST Drums datasetProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 609tanhBlstmpBSA1SA2SA3SA5CNN2CNN5CNN10CNN200.70.80.9F-measureDrum SoloMT Peak-PickingBRNN-MT Peak-Picking\ntanhBlstmpBSA1SA2SA3SA5CNN2CNN5CNN10CNN200.70.80.9F-measureDrum Mixture\ntanhBlstmpBSA1SA2SA3SA5CNN2CNN5CNN10CNN200.70.80.9F-measureMulti-Instrument Mixture\ntanhBlstmpBSA1SA2SA3SA5PC1PC2PC3PC5CNN2CNN5CNN10CNN200.70.80.9F-measureDrum SoloMT Peak-PickingBRNN-MT Peak-Picking\ntanhBlstmpBSA1SA2SA3SA5PC1PC2PC3PC5CNN2CNN5CNN10CNN200.70.80.9F-measureDrum Mixture\ntanhBlstmpBSA1SA2SA3SA5PC1PC2PC3PC5CNN2CNN5CNN10CNN200.70.80.9F-measureMulti-instrument MixturetanhB lstmpB     SA1   SA2  SA3  SA5  PC1   PC2  PC3  PC5  CNN2 CNN5 CNN10 CNN20\ntanhB lstmpB     SA1   SA2  SA3  SA5  PC1   PC2  PC3  PC5  CNN2 CNN5 CNN10 CNN20\ntanhB lstmpB     SA1   SA2  SA3  SA5  PC1   PC2  PC3  PC5  CNN2 CNN5 CNN10 CNN20[15]\n[15]\n[15]Figure 3 : Mean instrument F-measures for drum solo\n(top), drum mixture (middle) and multi-instrument mixture\n(bottom) evaluations. Previous state-of-the-art RNN sys-\ntems are on left and the SA, PC and CNN systems on right.\nlstmpBSA5PC3CNN50.60.70.80.9F-measureTrained On Drum Mixture\nlstmpBSA3PC5CNN50.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA3PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA3PC3CNN20.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA1PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA2PC2CNN20.60.70.80.9F-measureTrained On Drum MixturelstmpBSA5PC3CNN50.60.70.80.9F-measureTrained On Drum Mixture\nlstmpBSA3PC5CNN50.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA3PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA3PC3CNN20.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA1PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA2PC2CNN20.60.70.80.9F-measureTrained On Drum MixturelstmpBSA5PC3CNN50.60.70.80.9F-measureTrained On Drum Mixture\nlstmpBSA3PC5CNN50.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA3PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA3PC3CNN20.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA1PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA2PC2CNN20.60.70.80.9F-measureTrained On Drum MixturelstmpBSA5PC3CNN50.60.70.80.9F-measureTrained On Drum Mixture\nlstmpBSA3PC5CNN50.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA3PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA3PC3CNN20.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA1PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA2PC2CNN20.60.70.80.9F-measureTrained On Drum MixturelstmpBSA5PC3CNN50.60.70.80.9F-measureTrained On Drum Mixture\nlstmpBSA3PC5CNN50.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA3PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA3PC3CNN20.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA1PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA2PC2CNN20.60.70.80.9F-measureTrained On Drum MixturelstmpBSA5PC3CNN50.60.70.80.9F-measureTrained On Drum Mixture\nlstmpBSA3PC5CNN50.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA3PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA3PC3CNN20.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA1PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA2PC2CNN20.60.70.80.9F-measureTrained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum Mixture\nlstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturetanhBlstmpBSA1SA2SA3SA5CNN2CNN5CNN10CNN200.70.80.9F-measureDrum SoloMT Peak-PickingBRNN-MT Peak-Picking\nlstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum Mixture\nlstmpBlstmpBSA5SA3CNN5CNN100.60.70.80.9Trained On Drum Mixture\nlstmpBtanhBSA3SA1CNN5CNN20.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA3SA2CNN5CNN100.60.70.80.9Trained On Drum Solo\nlstmpBtanhBSA3SA1CNN5CNN50.60.70.80.9Trained On Multi-Instrument Mixure\nlstmpBtanhBSA1SA2CNN5CNN20.60.70.80.9Trained On Drum Solo\nlstmpBlstmpBSA2SA1CNN5CNN20.60.70.80.9Trained On Drum MixturelstmpBSA5PC3CNN50.60.70.80.9F-measureTrained On Drum Mixture\nlstmpBSA3PC5CNN50.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA3PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA3PC3CNN20.60.70.80.9F-measureTrained On Multi-instrument Mixure\nlstmpBSA1PC2CNN50.60.70.80.9F-measureTrained On Drum Solo\nlstmpBSA2PC2CNN20.60.70.80.9F-measureTrained On Drum Mixture\nlstmpBlstmpB\nlstmpBlstmpB\nlstmpBlstmpBSA5PC3CNN5CNN5SA3PC5\nCNN5SA3PC2CNN2SA3PC3\nCNN5CNN2PC2PC2SA1SA2\nFigure 4 : Mean instrument F-measure results with MT\npeak-picking for the cross-context evaluation: drum solo\ncombinations (top); drum mixture combinations (middle);\nandmulti-instrument mixture combinations (bottom).[5]. The dataset contains 64 tracks divided into three dif-\nferent drummers (21 tracks by drummer 1, 22 tracks by\ndrummer 2, and 21 tracks by drummer 3) with an aver-\nage track length of 55 seconds. The dataset is composed\nof drum-only recordings which contain multiple drum in-\nstruments as well as accompaniment ﬁles. The drum only\nrecordings are used for the drum mixture evaluation and the\ndrum-only recordings are mixed with the accompaniment\nﬁles using a ratio of 2/3 to 1/3 respectively for the multi-\ninstrument mixture evaluation. The same training, valida-\ntion and evaluation procedures are used as in the drum solo\nevaluation (Section 3.1.1).\n3.1.3 Cross-context Evaluation\nTo test the adaptability of the trained systems to before un-\nseen contexts we use the three systems trained in the previ-\nous evaluations (i.e., drum solo ,drum mixture , and multi-\ninstrument mixture ) to process the datasets from the other\ntwo evaluations. This results in six cross-context evalu-\nation combinations (e.g., train with drum solo test with\nmulti-instrument mixture ).\n3.1.4 Multi-context Evaluation\nTo test how well a single system can be trained to adapt\nand perform in multiple contexts, we combine the training\nand validation data from the drum solo ,drum mixture and\nmulti-instrument mixture evaluations. The test data from\nthe three evaluations is then processed using the single\nnewly trained systems. Of the ﬁve evaluations this is the\nmost realistic scenario.\n4. RESULTS AND DISCUSSION\n4.1 Drum Solo, Drum Mixture and Multi-instrument\nMixture Results\nFigure 3 highlights the mean instrument F-measure results\nof the SA, PC, CNN, and two previous state-of-the-art sys-\ntems with both of the peak-picking strategies for the drum\nsolo,drum mixture andmulti-instrument mixture evalua-\ntions. The SA systems achieve the highest mean instru-\nment F-measure in all three evaluations; 0.9880 ( SA3),\n0.9287 ( SA1) and 0.9274 ( SA2) respectively. The PC sys-\ntems achieve higher F-measures in the drum mixture and\nmulti-instrument mixture evaluations and the CNN sys-\ntems achieve higher F-measures than the state-of-the-art\nsystems in the multi-instrument mixture evaluation. This\ndemonstrates that within the harder polyphonic contexts,\nallowing the output layer to access multiple hidden states\nand including the input features of multiple frames does\nenable higher performance to be achieved. The BRNN-MT\npeak-picking strategy improves the results of some of\nthe SA and PC systems in both the drum mixture and\nmulti-instrument mixture evaluations, demonstrating that\ntheBRNN-MT strategy is able to improve performance in\nsome contexts by learning to identify peaks within the\nnoisier activation functions. For both the SA and PC sys-\ntems the systems where a\u00143achieved the highest F-\nmeasures, we believe this is because of the extra informa-\ntion in the SA5andPC5systems is beyond the scope of the610 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017tanhBlstmpBSA1SA2SA3SA5PC1PC2PC3PC5CNN2CNN5CNN10CNN200.70.80.9F-measureMulti-contextDrum Solo F-measureDrum Mixture F-measureMulti-instrument F-measureMean-context F-measureMean-context PrecisionMean-context Recall\nSA1SA2SA3SA5PC1PC2PC3PC5CNN2CNN5CNN10CNN20tanhBlstmpB[15]Figure 5 : Results of the multi-context evaluation. For each system using the MTpeak-picking technique the drum solo ,\ndrum mixture ,multi-instrument mixture , and mean-context F-measures are shown in addition to the mean-context precision\nand mean-context recall.\nonset and so has a negative effect on the performance. A\nsimilar trend is seen with the CNN systems which again\ncan be explained by the larger input feature sizes reduc-\ning the impact of the relevant features. We believe that\ndue to the drum solo evaluation being a relatively simple\ntask, the less-complex RNN systems are able to achieve\nsimilar accuracies to the newly proposed systems and the\nCNN performs poorly on this same task due to noisy out-\nput activation functions which are the result of not passing\ninformation between time steps. This would also explain\nwhy the BRNN-MT strategy did not improve the results for\nthe CNN systems and for any of the systems in the drum\nsolo evaluation.\n4.2 Cross-context Results\nFor each cross evaluation combination the top performing\nconﬁguration of the existing state-of-the-art RNN, SA, PC\nand CNN systems using the MTpeak-picking technique is\ndisplayed in Figure 4. The highest performing CNN sys-\ntem achieves a higher mean instrument F-measure than\nthe highest performing current state-of-the-art RNN sys-\ntem ( lsmtpB ) in three out of the six combinations, the\nhighest performing SA system only outperforms the cur-\nrent state-of-the-art RNN system in one of the combina-\ntions and the PC doesn’t out perform the RNN system in\nany combinations. This suggests that the CNN system is\nmore adaptable than the SA and PC systems even though\nthe SA and PC systems achieve higher mean instrument\nF-measures than the CNN systems in the previous three\nevaluations. None of the highest accuracies were achieved\nby systems that used the BRNN-MT peak-picking strategy,\nwhich suggests that it is not suited for adapting to unseen\nsituations.\n4.3 Multi-context Results\nFigure 5 highlights the drum solo ,drum mixture ,multi-\ninstrument mixture , and mean-context F-measures using\ntheMTpeak-picking technique. Also included are the\nmean-context precision, and recall for each of the sys-\ntems in the multi-context evaluation. The SA and CNN\nsystems outperform the existing state-of-the-art and PC\nsystems, further demonstrating the high performance ofthe SA systems and the adaptability of the CNN systems.\nThis is achieved through higher recall, but not necessarily\nhigher precision, suggesting that the improvement made\nby these systems is due to their ability to produce fewer\nfalse spikes within the resulting activation functions. All\nof the highest context F-measures were lower than the F-\nmeasures achieved by the systems trained in the single con-\ntext focused evaluations (i.e., drum solo ,drum mixture , and\nmulti-instrument mixture evaluation) demonstrating that a\nsystem trained in multiple contexts can not outperform sys-\ntems trained solely in one situation. The BRNN-MT peak-\npicking strategy again does not improve the performance\nof any of the systems in this evaluation.\n5. CONCLUSIONS AND FUTURE WORK\nWe have presented three new neural network based sys-\ntems for ADT in a polyphonic context: First, SA and PC\nsystems that enable multiple time-step hidden states to be\naccessed by the output layer; and second, a CNN system\nthat allows larger input feature sizes to be used. The re-\nsults from the conducted evaluations demonstrate that all\nof the newly proposed systems achieve higher accuracies\nthan the current state-of-the-art systems in polyphonic con-\ntexts, highlighting the effect of increased access to more\ninformation. Of all the tested systems, the SA performs\nbest in either the single or multi-context, while the CNN\nsystems perform best in situations in which the context is\nunseen. A possible future step would be to combine the\nSA and CNN systems into a single system possibly allow-\ning the system to work in both situations (i.e., single and\nmultiple contexts). An open source version of the newly\nproposed ADT systems can be found within the ADT li-\nbray (ADTLib).1\n6. REFERENCES\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. Neural machine translation by jointly learning to\nalign and translate. In Proceedings of the international\nConference on Learning Representations , 2015.\n1https://github.com/CarlSouthall/ADTLibProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 611[2] Sebastian B ¨ock, Jan Schl ¨uter, and Gerhard Widmer.\nEnhanced peak picking for onset detection with recur-\nrent neural networks. In Proceedings of the 6th Inter-\nnational Workshop on Machine Learning and Music\n(MML) , pages 15–18, Prague, Czech Republic, 9 2013.\n[3] Christian Dittmar and Daniel G ¨artner. Real-time tran-\nscription and separation of drum recordings based on\nNMF decomposition. In Proceedings of the Interna-\ntional Conference on Digital Audio Effects (DAFx) ,\npages 187–194, Erlangen, Germany, September 2014.\n[4] Simon Durand, Juan Pablo Bello, Bertrand David, and\nGa¨el Richard. Robust downbeat tracking using an en-\nsemble of convolutional networks. IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing ,\n25(1):76–89, 2017.\n[5] Olivier Gillet and Ga ¨el Richard. Transcription and sep-\naration of drum signals from polyphonic music. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 16(3):529–540, 2008.\n[6] Karl Moritz Hermann, Tom ´as Kocisk ´y, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. Teaching machines to read\nand comprehend. In NIPS , 2015.\n[7] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on\nMachine Learning , pages 448–456, 2015.\n[8] Jeremy Irvin, Elliott Chartock, and Nadav Hollander.\nRecurrent neural networks with attention for genre\nclassiﬁcation. 2016.\n[9] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[10] Marius Miron, Matthew E. P. Davies, and Fabien\nGouyon. An open-source drum transcription system\nfor pure data and max MSP. In Proceedings of the\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 221–225, Van-\ncouver, BC, Canada, 2013.\n[11] V olodymyr Mnih, Nicolas Heess, Alex Graves, et al.\nRecurrent models of visual attention. In Advances in\nneural information processing systems , pages 2204–\n2212, 2014.\n[12] Jouni Paulus. Signal Processing Methods for Drum\nTranscription and Music Structure Analysis . PhD the-\nsis, Tampere University of Technology, Tampere, Fin-\nland, 2009.\n[13] Axel R ¨obel, Jordi Pons, Marco Liuni, and Mathieu La-\ngrange. On automatic drum transcription using non-\nnegative matrix deconvolution and itakura saito diver-\ngence. In Proceedings of the IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 414–418, Brisbane, Australia, 2015.[14] Jan Schl ¨uter and Sebastian B ¨ock. Improved musical\nonset detection with convolutional neural networks.\nInProceedings of the 2014 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 6979–6983. IEEE, 2014.\n[15] Carl Southall, Ryan Stables, and Jason Hockman. Au-\ntomatic drum transcription using bi-directional recur-\nrent neural networks. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 591–597, New York City, United\nStates, August 2016.\n[16] Nitish Srivastava, Geoffrey E. Hinton, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. Journal of Machine\nLearning Research , 15(1):1929–1958, 2014.\n[17] David Sussillo and Laurence F. Abbott. Training very\ndeepnonlinear feed-forward networks with smart ini-\ntialization. arXiv preprint arXiv , 1412, 2014.\n[18] Lucas Thompson, Simon Dixon, and Matthias Mauch.\nDrum transcription via classiﬁcation of bar-level rhyth-\nmic patterns. In Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 187–192, Taipei, Taiwan, 2014.\n[19] Richard V ogl, Matthias Dorfer, and Peter Knees. Re-\ncurrent neural networks for drum transcription. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 730–736,\nNew York City, United States, August 2016.\n[20] Richard V ogl, Matthias Dorfer, and Peter Knees. Drum\ntranscription from polyphonic music with recurrent\nneural networks. In Proceedings of the IEEE Interna-\ntional Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP) , pages 201–205, New Orleans,\nLouisiana, United States, March 2017.\n[21] Chih-Wei Wu and Alexander Lerch. Drum transcrip-\ntion using partially ﬁxed non-negative matrix factor-\nization with template adaptation. In Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 257–263, Malaga, Spain,\nOctober 2015.\n[22] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. Show, attend and tell: Neural\nimage caption generation with visual attention. In In-\nternational Conference on Machine Learning , pages\n2048–2057, 2015.\n[23] Kazuyoshi Yoshii, Masataka Goto, and Hiroshi G.\nOkuno. Drum sound recognition for polyphonic au-\ndio signals by adaptation and matching of spectrogram\ntemplates with harmonic structure suppression. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 15(1):333–345, 2007.612 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Informed Automatic Meter Analysis of Music Recordings.",
        "author": [
            "Ajay Srinivasamurthy",
            "Andre Holzapfel",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415688",
        "url": "https://doi.org/10.5281/zenodo.1415688",
        "ee": "https://zenodo.org/records/1415688/files/Srinivasamurthy17.pdf",
        "abstract": "Automatic meter analysis aims to annotate a recording of a metered piece of music with its metrical structure. This analysis subsumes correct estimation of the type of meter, the tempo, and the alignment of the metrical structure with the music signal. Recently, Bayesian models have been successfully applied to several of meter analysis tasks, but depending on the musical context, meter analysis still poses significant challenges. In this paper, we investigate if there are benefits to automatic meter analysis from additional a priori information about the metrical structure of music. We explore informed automatic meter analysis, in which varying levels of prior information about the metrical struc- ture of the music piece is available to analysis algorithms. We formulate different informed meter analysis tasks and discuss their practical applications, with a focus on Indian art music. We then adapt state of the art Bayesian meter analysis methods to these tasks and evaluate them on cor- pora of Indian art music. The experiments show that the use of additional information aids meter analysis and im- proves automatic meter analysis performance, with signif- icant gains for analysis of downbeats.",
        "zenodo_id": 1415688,
        "dblp_key": "conf/ismir/Srinivasamurthy17",
        "keywords": [
            "Automatic meter analysis",
            "annotating meter structure",
            "Bayesian models",
            "metrical structure",
            "tempo estimation",
            "alignment with music signal",
            "informed automatic meter analysis",
            "prior information about metrical structure",
            "Indian art music",
            "corpora of Indian art music"
        ],
        "content": "INFORMED AUTOMATIC METER ANALYSIS OF MUSIC RECORDINGS\nAjay Srinivasamurthy\u0003\najays.murthy@upf.eduAndre Holzapfely\nholzap@kth.seXavier Serra\u0003\nxavier.serra@upf.edu\n\u0003MusicTechnologyGroup,UniversitatPompeuFabra,Barcelona,Spain\nyMediaTechnologyandInteractionDesignDepartment,KTHRoyalInstituteofTechnology,Stockholm,Sweden\nABSTRACT\nAutomatic meter analysis aims to annotate a recording of\na metered piece of music with its metrical structure. This\nanalysissubsumescorrectestimationofthetypeofmeter,\nthetempo,andthealignmentofthemetricalstructurewith\nthe music signal. Recently, Bayesian models have been\nsuccessfullyappliedtoseveralofmeteranalysistasks,but\ndependingonthemusicalcontext,meteranalysisstillposes\nsignificantchallenges. Inthispaper,weinvestigateifthere\nare benefits to automatic meter analysis from additional a\nprioriinformation about the metrical structure of music.\nWe explore informed automatic meter analysis, in which\nvaryinglevelsofpriorinformationaboutthemetricalstruc-\ntureofthemusicpieceisavailabletoanalysisalgorithms.\nWe formulate different informed meter analysis tasks and\ndiscusstheirpracticalapplications,withafocusonIndian\nart music. We then adapt state of the art Bayesian meter\nanalysismethodstothesetasksandevaluatethemoncor-\npora of Indian art music. The experiments show that the\nuse of additional information aids meter analysis and im-\nprovesautomaticmeteranalysisperformance,withsignif-\nicantgainsforanalysisofdownbeats.\n1. INTRODUCTION\nAutomatic meter analysis of a music recording aims at\ndetermining different components of its metrical struc-\nture such as the type of meter, the tempo, the beats and\ndownbeats. It is an important Music Information Re-\nsearch (MIR) task that provides useful musically relevant\nmetadata not only for enriched listening, but also for pre-\nprocessing of music for several higher level tasks such\nas section segmentation, structural analysis and defining\nrhythm similarity measures. Initial approaches to meter\nanalysis explored individual tasks of meter analysis, such\nastempoestimation[8,9],beattracking[5,13],timesigna-\ntureestimation[15]anddownbeattracking[10,14]. Recent\napproaches consider a joint estimation of several of these\ncomponentsandhavesuccessfullyappliedBayesianmod-\nels to jointly estimate beat and downbeats using rhythmic\npatterns learned from onset detection features [1,11,12].\nRecentinteresthasalsobeentoexploreneuralnetworksfor\n© Ajay Srinivasamurthy, Andre Holzapfel, Xavier Serra.\nLicensedunderaCreativeCommonsAttribution4.0InternationalLicense\n(CC BY 4.0). Attribution: Ajay Srinivasamurthy, Andre Holzapfel,\nXavier Serra. “Informed Automatic Meter Analysis of Music Record-\nings”,18thInternationalSocietyforMusicInformationRetrievalConfer-\nence,Suzhou,China,2017.beatanddownbeattrackingwithseveralmusicallyinspired\nfeaturesandnetworktopologies[7]. Despitetherecentsuc-\ncess, meter analysis still poses significant challenges de-\npendingonthemusicalcontext[18,20].\nIn this paper, we investigate the potential to improve\nmeteranalysismethodsbyprovidingthemwithadditional\nprior information about the underlying metrical structure.\nThis is a research problem we define as informed meter\nanalysis, referring to a class of analysis tasks that utilize\nsomeformofadditionalinformationabouttheunderlying\nmetricalstructureofthepiece. Apartfrombuildingmeter-\naware analysis methods, informed meter analysis is mo-\ntivated by its potential applications and the need for im-\nprovedmeteranalysisperformance. Itishypothesizedthat\ninformationavailableasmetadataorobtainablefromanex-\npertusercanbeeffectivelyutilizedtosignificantlyimprove\nmeter analysis performance. Such informed approaches\ncanhelptoestablishafocusinthespaceofpossiblesolu-\ntionsbytheincorporationof a prioriinformation,support-\ningmeteranalysisespeciallyinthecontextofcomputation-\nally challenging samples. Some informed meter analysis\ntasks have been studied before, such as the task of down-\nbeat tracking from a set of known beats [10]. However,\nthere has been no formal treatment of the problem, which\nisthefocusofthispaper.\nCarnatic and Hindustani music are Indian Art Music\n(IAM) traditions from Southern and Northern parts of the\nIndiansubcontinent,respectively. Boththesemusicshave\nalonghistoryofperformanceandcontinuetothriveincur-\nrent sociocultural contexts. While the two musics differ\nin performance practices, they share similar melodic and\nrhythmic concepts. The rhythmic framework is based on\ncyclic metrical structures called the tāḷa in Carnatic mu-\nsic (CM) or tāl in Hindustani music (HM), which provide\na broad structure for repetition of music phrases, motifs\nand improvisations. A cycle of a tāḷa (or tāl) is divided\nintoisochronousbeats(calledthemātrāinHM),whichare\ngroupedintopossiblyunequallengthsections. Thebegin-\nningofacycle(thedownbeat)isreferredtoassama(sam\nin HM). Given the central importance of tāḷa in defining\nrhythmic structures, meter analysis in the context of IAM\naims to time-align and tag a music recording with tāḷa re-\nlated events and metadata. Clayton [3] and Sambamoor-\nthy [16] provide an in depth discussion of rhythm in Hin-\ndustaniandCarnaticmusic,respectively.\nWithsignificantimprovisationandexpressivetiming,a\nwiderangeoftempoandcyclesaslongasaminute,IAM\nhasbeenshowntoposeseveralchallengestoautomaticme-679teranalysis[20]. Further,largeandcontinuouslygrowing\narchivesofIAMareavailablewithvaryingamountsoftāḷa\nrelatedmetadata[17]. Inthispaper,weusecorporaofIAM\nas a challenging case to explore the potential of informed\nmeteranalysis,andincludeasetofBallroomdancestoen-\nablecomparisonwithotherstyles.\n2. INFORMED METER ANALYSIS\nDifferent kinds of prior information about the underlying\nmetrical structure can be made available to analysis algo-\nrithms. Inthefollowingsubsections, wedescribespecific\ninformed analysis tasks and emphasize different practical\nscenariosforeachtask. Attheoutset,weassumethatsome\nbasicinformationaboutthemusicpieceisavailableforall\ninformed analysis tasks. We assume that the music tradi-\ntionisknown,andthattherhythmclass(tāḷa)ofthepiece\nisfromasetofknown(frommusicologicalliterature)tāḷas.\nFurther,weassumeweknowtherangeoftempogenerally\nused in a music culture. A piece of IAM is performed in\na single tāḷa (rare exceptions exist, but outside the scope\nofregularperformancepractice)andmostcommercialre-\nleasesaresegmentedsothatanaudiorecordingisasingle\npiece. However,therearecaseswhenanentireconcertor\nparts of concert with multiple pieces (and hence possibly\ndifferent tāḷas) are stored in a single audio recording. We\nassume that such a recording has been segmented into in-\ndividual pieces of music with a single tāḷa. The case of\nchangeoftāḷaswithinarecordingisnotaddressed.\nFinally,forbetterreadability,weusethecommonlyused\nterminology of tempo, beats and downbeats in the paper,\nwhilewecarefullynotethattheequivalenceoftheseterms\nacrossdifferentmusicculturescannotbeassumed.\n2.1 Meter Inference ( Inference )\nMeter inference aims for a complete meter analysis of a\nrecording starting with no prior information. Given an\naudio music recording, meter inference task aims to esti-\nmatetherhythmclass(ormetertypeortāḷa),time-varying\ntempo, beats and downbeats. Meter inference in IAM\naims to recognize the tāḷa/tāl, estimate the time varying\ntempo (measured as the inter beat interval), the beat and\nthesama/sam(downbeat)locations. Itistheleastinformed\nand most difficult task owing to the large range of tempi\nand different tāḷas. While meter inference is the only ap-\nplicabletaskwithunlabeledcollectionsofmusic,itisoften\nthe case that some tāḷa related information is available or\ncanbeinferred,e.g. fromtheeditorialmetadataofamusic\npiece. Most of commercially released music in both Car-\nnaticandHindustanimusichasthenameofthetāḷained-\nitorial metadata. Even within a live concert, the musician\noften announces the tāḷa of a piece and hence tāḷa recog-\nnition is a redundant task. However, meter inference can\nbeusedasabaselinetasktounderstandthecomplexityof\nuninformedmeteranalysis.\n2.2 Meter Tracking ( Track )\nGiven an audio music recording and its rhythm class (or\nmetertypeortāḷa),metertrackingaimstoestimatethetimevaryingtempo,beatanddownbeatlocations. Metertrack-\ninginIAMaimstotrackthetimevaryingtempo,beatsand\nthe sama from an audio music recording, given the tāḷa.\nAssumingthatthetāḷa,andhencethemetricalstructureis\nknown in advance is a fair and practical assumption mak-\ningmetertrackingthemostrelevantmeteranalysistaskfor\nIAM.\n2.3 Informed Meter Tracking\nInformed meter tracking is meter tracking in which some\nadditionalinformationapartfromthetāḷaisavailable. The\nadditional information could be in the form of a tempo\nrange, a few instances of beats and downbeats annotated,\nor even partially tracked metrical cycles. The additional\nmetadatacouldcomefrommanualannotationorasanout-\nput of other automatic algorithms, e.g. the median tempo\nofapiececanbeobtainedfromastandalonetempoestima-\ntionalgorithm,orsomemelodicanalysisalgorithmsmight\noutput(withahighprobability)somebeats/downbeatsasa\nbyproduct.\nFrom a practical standpoint, while it is prohibitively\nresource intensive to manually annotate all the beats and\ndownbeats of a large music collection, it might be possi-\nbletoseedthemetertrackingalgorithmswiththefirstfew\nbeatsanddownbeats. Foramusicianorevenanexpertlis-\ntener, it would be easy to tap some instances of the beat\nand sama/downbeats, which could then be used to auto-\nmatically track meter in the whole recording. In specific,\nweexplorethreevariantsofinformedmetertracking,with\nvaryinglevelsofavailableinformation:\nSama-informed meter tracking ( SI-Track )task in\nwhich a few instances of sama/downbeat of the piece are\nprovidedasanadditionalinputtothemetertrackingalgo-\nrithm. Anexampledownbeatisexpectedtohelpthealgo-\nrithmtobetteraligntheaudiototheunderlyingmeter. We\nonlyexploretheuseoffirstdownbeatofthepiece,without\nanyknowledgeoftempo.\nTempo-informed meter tracking ( TI-Track )task in\nwhichthemediantempo(oranarrowrangeoftempi)ofthe\npieceisprovidedasanadditionalinputtothemetertrack-\ningalgorithm. Providingthemediantempoishypothesized\nto help reduce metrical level errors - tracking the metri-\ncal cycles at the correct metrical level instead of tracking\nhalfanddoublecycles. Themediantempocanbeobtained\nmanuallyorthroughotherautomatictempoestimational-\ngorithms[8,22].\nSama-Tempo-informed meter tracking ( STI-Track )\ntask in which the median tempo and a few downbeat lo-\ncations in the excerpt are provided as additional inputs to\nthe meter tracking algorithm. We only explore the use of\nmedian tempo value and the first downbeat of the music\npieceprovidedtothemetertrackingalgorithm.\nThe informed meter tracking tasks formulated in this sec-\ntion are relevant and designed to require minimal human\nefforttoprovidethenecessaryadditionalinformation. Ina\nbestcasescenario,themostinformed STI-Track taskcan\nbeappliedtoamusicpiecebylisteningtojustthefirstfew680 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 1: The bar pointer model for meter analysis. The\ncircles and squares denote continuous and discrete vari-\nables,respectively. Greynodesandwhitenodesrepresent\nobservedandlatentvariables,respectively.\nseconds of the piece and marking two consecutive down-\nbeats. An estimate of the initial tempo can be obtained\nthetwodownbeatsandusedbytheanalysisalgorithm. Fi-\nnally, the various tasks were described using terminology\nof IAM, but they are applicable to any music with hierar-\nchicalmetricalstructuresthatcanbedescribedwithbeats,\ndownbeatsandrhythmpatterns.\n3. METER ANALYSIS MODEL\nTo compare different informed analysis tasks, we use and\nadaptastateoftheartDynamicBayesianNetwork(DBN).\nReferredtoasbarpointermodel(BP-model)[23],hasbeen\nsuccessfully applied for meter analysis in different music\ncultures [11,19]. We describe the model briefly while a\ndetailed description is presented in [12]. We then explain\nhowitcanbeadaptedtotheinformedanalysistasks.\nIn a DBN, an observed sequence of features derived\nfrom an audio signal y1:K=fy1; : : : ; yKgis gener-\nated by a sequence of hidden (latent) variables x1:K=\nfx1; : : : ; xKg, where Kis the length of the feature se-\nquence (number of audio frames). The joint probability\ndistributionofhiddenandobservedvariablesfactorizesas,\nP(y1:K;x0:K) =P(x0)\u0001K∏\nk=1P(xkjxk\u00001)P(ykjxk)\nwhere, P(x0)is the initial state distribution, P(xkjxk\u00001)\nis the transition model, and P(ykjxk)is the observation\nmodel. The structure of the BP-model in Figure1 shows\nthe conditional dependence relations between the vari-\nables.\n3.1 Hidden Variables\nAt each audio frame k, the hidden variable vector xk\ndescribes the state of a hypothetical bar pointer xk=\n[ϕk_ϕkrk], representing the bar position, instantaneous\ntempoandarhythmicpatternindicator,respectively.\nRhythmic pattern indicator: Therhythmicpatternvari-\nabler2 f1; : : : ; R gisanindicatorvariabletoselectoneof\ntheRobservationmodelscorrespondingtoeachbar(cycle)\nlength rhythmic pattern of a rhythm class that are learned\nfromtrainingdata. Eachpattern rcorrespondstoarhythm\nclass(ormetertypeortāḷa)andhasanassociatedlengthof\ncycle Mrandnumberofbeat(ormātrā)pulses Br.Bar position: The bar position ϕ2[0; Mr)variable\ntracks the progression through the bar and indicates a po-\nsitioninthebaratanyaudioframe. Thevariabletraverses\nthe whole bar and wraps around to zero at the end of the\nbartotrackthenextbar.\nInstantaneous tempo: Instantaneous tempo _ϕis the rate\nat which the bar position variable progresses through the\nbarateachframe,measuredinbarpositionspertimeframe.\n3.2 Transition and Observation Model\nThe initial state distribution P(x0)can be used to incor-\nporatepriorinformationaboutthemetricalstructureofthe\nmusic into the model. Given the conditional dependence\nrelationsinFigure1,thetransitionmodelfactorizesas,\nP(xkjxk\u00001) =P(ϕkjϕk\u00001;_ϕk\u00001; rk\u00001)P(_ϕkj_ϕk\u00001)\nP(rkjrk\u00001; ϕk; ϕk\u00001)(1)\nTheindividualtermsoftheequationcanbeexpandedas,\nP(ϕkjϕk\u00001;_ϕk\u00001; rk\u00001) = /x31ϕ (2)\nwhere /x31ϕisanindicatorfunctionthattakesavalueofone\nifϕk= (ϕk\u00001+_ϕk\u00001)mod (Mrk\u00001)and zero otherwise.\nThetempotransitionisgivenby,\nP(_ϕkj_ϕk\u00001)/ N (_ϕk\u00001; \u001b2\n_ϕk)\u0002 /x31_ϕ(3)\nwhere /x31_ϕis an indicator function that equals one if _ϕk2\n[_ϕmin;_ϕmax]andzerootherwise,restrictingthetempotobe\nbetween a predefined range. N(\u0016; \u001b2)denotes a normal\ndistributionwithmean \u0016andvariance \u001b2. Thevalueof \u001b_ϕkdepends on the value of tempo to allow for larger tempo\nvariationsathighertempi. Weset \u001b_ϕk=\u001bn\u0001_ϕk\u00001,where\n\u001bn(= 0:02)isauserparameterthatcontrolstheamountof\nlocaltempovariationsweallowinthemusicpiece.\nThe transition probability of pattern indicator variable\nP(rkjrk\u00001; ϕk; ϕk\u00001)is governed by A, aR\u0002Rtime-\nhomogeneoustransitionmatrixwhere A(i; j)isthetransi-\ntionprobabilityfrom ritorj. However,sincetherhythmic\npatternsareonebar(cycle)inlength,patterntransitionsare\nallowedonlyattheendofthebar( ϕk< ϕk\u00001).\nThe observation model is identical to the one used in\n[12], and depends only on the bar position and rhythmic\npattern variables, without any influence from tempo. To\nmodel rhythm patterns, we compute spectral flux feature\nfromaudiointwofrequencybands(Low: \u0014250Hz,High:\n>250 Hz). Using beat and downbeat annotated training\ndata,theaudiofeaturesaregroupedintobarlengthpatterns\non a bar discretized into 64thnote cells. A k-means clus-\nteringalgorithmthenassignseachbarofthedatasettoone\nofthe Rrhythmicpatterns. Allthefeatureswithinthecell\nofeachpatternarecollectedandmaximumlikelihoodesti-\nmatesoftheparametersofatwocomponentGaussianMix-\nture Model (GMM) are obtained. The observation proba-\nbilitywithina64thnotecellisassumedtobeconstantand\niscomputedas,\nP(yjx) =P(yjϕ; r) =2∑\ni=1\u0019ϕ;r;iN(y;\u0016ϕ;r;i;\u0006ϕ;r;i)\nwhere, N(y;\u0016;\u0006)denotes a normal distribution of the\ntwo dimensional feature y. For the mixture component i,\n\u0019ϕ;r;i;\u0016ϕ;r;iand\u0006ϕ;r;iare the component weight, mean\n(2-dim.) andthecovariancematrix( 2\u00022),respectively.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 6813.3 Inference in BP-model\nThe goal of inference with the BP-model is to estimate a\nhiddenvariablesequence x\u0003\n1:Kthatmaximizestheposterior\nprobability P(x1:Kjy1:K)givenanobservedsequenceof\nfeatures y1:K. The sequence x\u0003\n1:Kcan then be translated\nintoasequenceofdownbeat(sama)instants( t\u0003\nkjϕ\u0003\nk= 0),\nbeatinstants( t\u0003\nkjϕ\u0003\nk=i\u0001Mr\u0003/Br\u0003,i= 1; : : : ; B r),local\ninstantaneoustempo( _ϕ\u0003\nk)andrhythmicpatterns( r\u0003).\nIn this paper, we use an approximate particle filter [6]\nbased inference scheme called the Auxiliary Mixture Par-\nticleFilter(AMPF),whichhasbeenshowntobeeffective\nformeteranalysis[12]. Inaparticlefilter,theposterioris\nestimatedpointwisebyapproximatingitusingaweighted\nsetofpoints(knownasparticles)inthestatespaceas,\nP(x1:Kjy1:K)\u0019Np∑\ni=1w(i)\nK\u000e(x1:K\u0000x(i)\n1:K)(4)\nHere, fx(i)\n1:Kgis a set of points (particles) with associated\nweights fw(i)\nKg,i= 1; : : : ; N p,x1:Kis the set of all state\ntrajectoriesuntilframe K,\u000e(x)istheDiracdeltafunction,\nandNpis the number of particles. The AMPF algorithm\nincludes several enhancements to make it suitable for in-\nferencewiththeBP-model,adetaileddescriptionofwhich\nhasbeenpresentedin[12].\n3.4 BP-model and AMPF for Informed Meter Analysis\nTheAMPFalgorithmontheBP-modelisgenericandcan\nbe adapted to be applicable to the informed meter analy-\nsis tasks described in Section2. For meter inference, the\nrhythmclass(tāḷa)canbeestimatedbyallowingrhythmic\npatterns of different lengths from different rhythm classes\ntobepresentinthemodel,asusedby[12]. Formetertrack-\ningtasks,weassumethattherhythmclassisknownandall\nrhythm patterns belong to that class, i.e. Mr=Mand\nBr=B8r.\nThe initial state distribution P(x0)and the initializa-\ntionoftheparticlefiltersystemaremodifiedtosuitthein-\nformedmetertrackingtasks. Auniforminitializationover\nallallowedstatesisusedfor Inference andTracktasks,\nwhile a narrower informed initialization is done for in-\nformedmetertracking. For TI-Track task,weusetheme-\ndian ground truth tempo of the music piece being tracked\nand initialize the tempo variable _ϕwithin a tight bound\nallowing for 10% variation in tempo around the median\nvalue. This enables the tracking algorithm to restrict the\ntempo variable within the tight tempo range and track the\ncorrect tempo at the right metrical level. For SI-Track\ntask, the provided sama instance is used to initialize the\nbarpositionvariable ϕtozeroattherelatedtimeposition.\nForSTI-Track task,boththetempoandbarpositionvari-\nablesareinitializedappropriatelyusingthegiveninforma-\ntion. Thetrackingalgorithmhencegetsthetempoandthe\nbeginningofthecycleinthepiece,trackingtheremaining\nbeatsanddownbeats.\n4. EXPERIMENTS\nThe experiments aim to compare performance across dif-\nferent informed meter analysis tasks and investigate theDataset #Pieces #Ann. #Sama\nCMR 118 28725 5560\nHMRs 92 32731 2572\nHMRl 59 3280 304\nTotal(IAM) 269 64736 8436\nTable 1: The Carnatic (CMR) and Hindustani (HMR land\nHMRs)musicdatasetsshowingthenumberofpieces,sama\nandbeat/mātrāannotations.\nadvantage of the additional prior information they utilize.\nWhilethefocusofexperimentsisonIndianmusic,wealso\nreport the results on a collection of Ballroom dances to\nevaluate the extensibility of the informed analysis tasks.\nFurthermore,reproducibilitywillbeensuredbyproviding\nfree access for research purposes to all code repositories\nand datasets on the companion webpage, which also pro-\nvidesadditionalresourcesandmusicexamples.1\n4.1 Music Datasets\nFor the experiments, we use rhythm annotated datasets of\nCarnatic and Hindustani music (described in Table1) that\nhavebeenpreviouslyusedforevaluatingautomaticmeter\nanalysis algorithms. The Carnatic music rhythm dataset\n(CMRdataset)[19]includes118twominutelongexcerpts\nofCarnaticmusicsampledfromcommercialreleases. The\nrecordings span four commonly used tāḷas with different\nnumber of beats in a cycle, with a total duration of 236\nminutes. Thedatasetconsistsofaudio,manuallyannotated\ntime-aligned markers indicating the progression through\nthetāḷacycle,andtheassociatedtāḷarelatedmetadata.\nThe Hindustani music rhythm dataset consists of 151\ntwo minute long excerpts of Hindustani music sampled\nfrom the CompMusic Hindustani music research cor-\npus[21],acuratedcollectionofcommercialaudioreleases\nandmetadata. TheexcerptsspanfourpopulartālsofHin-\ndustanimusicthatarestructurallydifferentandofdifferent\nlengths. For each audio excerpt, the annotations consist\nofeditorialmetadataaboutthetāl,aswellastime-aligned\nmetricalannotationsofallbeatandsaminstances.\nThedatasetconsistsofexcerptswithawidetemporange\nfrom 10 MPM (mātrās per minute) to 370 MPM. Hindus-\ntani music divides tempo into three main tempo classes\n(lay). Since no exact tempo ranges are defined for these\nclasses, we determined suitable ranges in correspondence\nwith a professional Hindustani musician as 10-60 MPM,\n60-150 MPM, and >150 MPM for the slow (vilaṁbit),\nmedium (madhya), and fast (dr ̥t) tempi, respectively. The\ntempoclassofapiecehasasignificanteffectonmeteranal-\nysisduetothewiderangeofpossibletempi. Tostudyany\neffectsofthetempoclass,thefullHindustanidatasetisdi-\nvidedintotwoothersubsets-thelongcycledurationsubset\ncalled the HMR ldataset consisting of vilaṁbit pieces and\nthe short cycle duration subset HMR sdataset with mad-\nhyaandthedr ̥tlaypieces. ThecompletecollectionofCar-\nnaticandHindustanimusicdatasetstogetheriscalledIAM\ndataset.\n1http://compmusic.upf.edu/informed-meter-tracking682 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017In addition to Indian art music, we evaluate the tasks\non a set of Ballroom dances, which includes beat and bar\nannotations of audio recordings of several dance styles\nsourced from BallroomDancers.com [9,11]. The ball-\nroomdatasetcontainseightdifferentdancestyles(Chacha,\nJive, Quickstep, Rumba, Samba, Tango, Viennese Waltz,\nand (slow) Waltz) and has been widely used for several\nMIR tasks such as genre classification, tempo tracking,\nbeat and downbeat tracking [1,9,12]. It consists of 698\nthirtysecondlongaudioexcerptsandhastempoanddance\nstyle annotations. The dataset contains two different me-\nters(3/4and4/4)andallpieceshaveconstantmeter.\n4.2 Evaluation Measures\nWe evaluate the tasks through the relevant meter compo-\nnents they estimate - meter type, tempo, beats and down-\nbeats. Weevaluateonlytheapplicablecomponentsthatare\nnotassumedtobeknown a prioriinaninformedtask(e.g.\nmetertypeisknownin Tracktaskandhenceonlytempo,\nbeatsanddownbeatsareevaluated).\nAvarietyof measures areavailable forevaluating beat\nanddownbeattracking[4]. Wechosethef-measure( f)met-\nric that is widely used in beat tracking evaluation. Other\nmeasureswereappliedinadditionduringtheexperiments,\nbutdidnotaddfurtherdetailandhencearenotreported. It\nisanumberbetween 0and1computedfromestimatedand\ngroundtruthannotationsequencesastheharmonicmeanof\ntheprecisionandrecallmeasures. Thedefinitionextendsto\ntrackingboththebeat/mātrās( fb)andthedownbeats/samas\n(fs). For Inference andTracktasks,weadditionallyre-\nporttheresultsofmediantempoestimation,comparingthe\nmedianestimatedtempoandthemedianannotatedground\ntruthtempowitha5%errormargin. For Inference task,\nthe algorithms also detect the rhythm class (or tāḷa) and\nhencetheaccuracyofthisdetectionisalsoreported.\n4.3 Experimental Setup\nExperimentsaredoneseparatelyoneachofthethreeIAM\ndatasets (CMR, HMR s, HMRl) and the Ballroom dataset.\nTo compute the f-measure in CMR, HMR sand Ballroom\ndatasets, an error tolerance window of 70 ms is used be-\ntween the annotation and the estimated beat/sama. The\ncomputationoff-measurewithHMR ldatasetisanexcep-\ntion,whereabiggermarginwindowisallowed. Sincecy-\nclesareoflongdurationinHMR ldatasetandcurrenteval-\nuationapproacheswerenotdesignedwithsuchlongcycles\ninmind,anerrortolerancewindowof70msisverytight.\nTo account for the length of the cycle in the error margin,\na6.25%medianinterannotationintervalisusedasthetol-\nerance window, as used in many other beat tracking eval-\nuations (e.g. by [10]). This choice of a larger allowance\nwindow also corroborates well with the observation that\nin vilaṁbit pieces of the HMR ldataset, there can be sig-\nnificant freedom in pulsation and that larger timing devi-\nations go unnoticed since the pieces are not rhythmically\ndense. It can be argued that the beat pulsation in vilaṁbit\npiecesisbeyondthedurationofwhatiscalledthepercep-\ntualpresent[2],andcanthereforenotbeconsideredtobe-\nlong to metrical structure. However, it is to be noted thatDataset CMR HMR sHMRlIAM Ballroom\nAccuracy 68 63 27 57 89\nTable 2: Tāḷa recognition accuracy (%) in Inference\ntask. Time signature recognition accuracy is reported for\nBallroomdataset.\ntheallowanceusedinthispaperisacompromiseandbetter\nevaluationmeasuresthatcanhandlethesecomplexitiesare\ntobedeveloped.\nThe tempo ranges for initialization of AMPF in In-\nference,TrackandSI-Track tasks are learned from\ntrainingdataofeachfoldandanadditional20%marginis\nadded to extend to unseen data. However, if the learned\nrangesarebeyondtheminimumandmaximumtempolim-\nits of each music culture, we set it to the minimum or the\nmaximum. Weuseonerhythmicpatternpertāḷa(ordance\nstyle). Hence, we use R= 1for meter tracking, when a\nknown meter is being tracked, while R= 4(8in Ball-\nroom dataset) is used for meter inference, with one pat-\ntern per tāḷa/rhythm. We use the number of bar positions,\nMr= 1600forthelongestrhythmicpatternweencounter\nin the dataset and scale all other pattern lengths accord-\ningly. For the AMPF algorithm, we use 1500 particles\nperrhythmpattern,withotherparametersidenticaltothose\nused in [12]. A hop size of 20 ms is used to compute the\ntwodimensionalspectralfluxfeature.\n4.4 Results and Discussion\nTheresultsinTable2andFigures2-3summarizetheper-\nformance across different datasets and informed analysis\ntasks. All results are reported as the mean performance\noverthreerunsina2-fold(equalsize)crossvalidationex-\nperiment on each dataset. The results are presented for\neach dataset as an average over the pieces in all the tāḷas\n(orrhythmclasses). Table2showsthetāḷarecognitionac-\ncuracyfortheIndianmusicdatasets(andtimesignaturees-\ntimationaccuracyforBallroomdataset)fromthe Infer-\nencetask. Figure3 shows the median tempo estimation\naccuracyfordifferentdatasetsinthe Inference ,Track,\nandSI-Track tasks, where median tempo is not known\na priori. The beat and downbeat f-measure values are re-\nported for all the informed analysis tasks in Figure2. We\nuseapaired-samplet-testtoassessstatisticallysignificant\ndifferencesinbeatanddownbeattrackingperformanceby\npoolingtheresultsofIndianmusicdatasets.\nTable2showsasimilarperformancewiththeCMRand\nHMRsdatasets,butissignificantlypoorforthelongcycle\nsubset of Hindustani music (HMR ldataset). Whereas in\nthe Carnatic and Hindustani music datasets, each tāḷa has\nadistinctlength, theeightrhythmclassesintheBallroom\ndataareassignedtoonlytwotimesignaturesreducingthe\ntasktoaclassificationtaskbetween3/4and4/4timesigna-\ntures. Ballroom dataset hence shows the best recognition\nperformance.\nTāḷa recognition accuracy affects tempo estimation, as\nseeninFigure3withapoortempoestimationperformance\nwithinthe HMR ldataset. Median tempo estimation accu-\nracy is similar for CMR and HMR sdatasets. Tempo es-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 683Figure 2: Beatandsama(downbeat)trackingresultsshowingthef-measureasbarplotsfordifferentdatasetsandinformed\nanalysistasks. Thematrixontherightshowstheresultsofasignificancetestbetweenanalysistasks(numbers1-5corre-\nspondtotasksinthelegend)fortheIAMdataset. Aboxwithnumeral1indicatesastatisticallysignificantdifferenceina\npaired-samplet-test(at p= 0:05)whilenumeral0indicatesadifferencethatisnotstatisticallysignificant.\nFigure 3: Median tempo estimation accuracy in the In-\nference,TrackandSI-Track tasks.\ntimation accuracy improves for Tracktask compared to\nInference task, showing the utility of knowing the me-\nter type in estimating the correct tempo. However, addi-\ntional downbeat information in SI-Track task does not\naddmuchtotempoestimation,withmarginalornofurther\nimprovement. Ballroom dataset shows the best tempo es-\ntimation performance except for Inference task, where\nwrong estimations of the rhythm class leads to poorer\ntempoestimation.\nThe beat f-measure ( fb) results in Figure2 across dif-\nferentinformedanalysistasksshowsamarginalimprove-\nmentwithinformedtrackingtasks,butstatisticallysignifi-\ncantimprovementsareobservedonlywith TI-Track and\nSTI-Track tasks for IAM datasets, when median tempo\nis known a priori. This shows that the tempo informa-\ntionismorerelevantthantāḷaandsamainformationtoim-\nprovebeattrackingperformanceforIndianartmusic. The\nbiggestgainsininformedmeteranalysisareseeninsamaf-\nmeasure( fs),withsignificantimprovementsachievedwith\nmoreinformedanalysistasks. ForthepooledIAMdataset,\nstartingwitha fs=0.51with Inference task, STI-Track\ntask achieves fs= 0:82, showing the benefit and the util-\nityofbothtempoandsamainformationininformedmeter\nanalysisforamoredifficulttaskofdownbeatestimation.\nFor Ballroom dataset, compared to the Tracktask,\nwe observe that downbeat tracking performance for SI-\nTrackimprovesmoreover TI-Track task. Thisindicates\nthat downbeat information is more important than tempoinformation. It is perhaps due to the fact that Ballroom\ndanceshaveastabletempoandclearrepeatedrhythmicpat-\nterns. Accuratetempoestimationisachievedevenwithout\nprior tempo information (Figure3), and hence downbeat\ninformationismoreuseful.\nAcomparisonofperformanceacrossdatasetsshowsthat\nCMR,HMR sandBallroomdatasetshavesimilartrendsof\nimprovement in both beat and sama (downbeat) tracking\nwith informed tracking tasks. The largest gains however\nareobtainedwiththelongcycleHMR ldataset,whichim-\nproves from a poor fs= 0:26(Inference ) to fs= 0:99\n(STI-Track ). Whilewenotethatalargererrormarginand\nfewer sama examples in the long cycle dataset contribute\ntothishighperformance,theoverallresultsconsideringall\ndatasetsandtasksconcludethattheuseoftempoandsama\ninformation enhances the capabilities of automatic meter\nanalysisalgorithmstotrackdownbeats.\n5. CONCLUSIONS\nStarting with a hypothesis that automatic meter analysis\nperformance can be improved by utilizing additional in-\nformation about meter or tempo of a piece, we formu-\nlatedrelevantinformedmeteranalysistasksthatcanincor-\nporate varying levels of prior information about the me-\nter type, tempo and downbeat position. An evaluation on\ncorpora of Indian art music and Ballroom dances showed\nthe utility of prior information for automatic meter anal-\nysis, where tempo information is useful for beat tracking\nandthetempoanddownbeatinformationwasshowntobe\nuseful for downbeat tracking. We also showed that with\nminimal effort by a potential user of an annotation sys-\ntem,ahighaccuracyintempo,beatanddownbeatestima-\ntioncanbeachievedthroughinformedmeteranalysisalgo-\nrithms. Evaluationofinformedanalysistasksinthepaper\nwasdonethroughindividualcomponentsofmeter(tempo,\nbeat,downbeat). Infuturework,weplantodevelopunified\nmeter analysis evaluation measures that take into account\nthehierarchicalstructureofmusicalmeter.684 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. ACKNOWLEDGMENTS\nThis work is partly supported by the European Research\nCouncil as part of the CompMusic project (ERC grant\nagreement 267583). Ajay Srinivasamurthy is currently\nwithIdiapResearchInstitute,Martigny,Switzerland.\n7. REFERENCES\n[1]S. Böck, F. Krebs, and G. Widmer. A Multi-model\nApproachtoBeatTrackingConsideringHeterogenous\nMusicStyles.In Proc. of the 15th Intl. Society for Music\nInformation Retrieval Conference (ISMIR 2014) ,pages\n602–607,Taipei,Taiwan,October2014.\n[2]E. Clarke. Rhythm and timing in music. In Diana\nDeutsch, editor, The Psychology of Music , pages\n473–500.AcademicPress,SanDiego,IIedition,1999.\n[3]M.Clayton. Time in Indian Music : Rhythm, Metre and\nForm in North Indian Rag Performance . Oxford Uni-\nversityPress,2000.\n[4]M.E.P.Davies,N.Degara,andM.D.Plumbley.Eval-\nuation Methods for Musical Audio Beat Tracking Al-\ngorithms. Technical Report C4DM-TR-09-06, Queen\nMary University of London ,October2009.\n[5]M. E. P. Davies and M. D. Plumbley. Context-\nDependent Beat Tracking of Musical Audio. IEEE\nTrans. on Audio, Speech, and Language Processing ,\n15(3):1009–1020,March2007.\n[6]A. Doucet and A. M. Johansen. A tutorial on particle\nfilteringandsmoothing: Fifteenyearslater. Handbook\nof Nonlinear Filtering ,2009.\n[7]S. Durand, J. P. Bello, B. David, and G. Richard.\nDownbeat tracking with multiple features and deep\nneural networks. In Proc. of the 40th IEEE Intl. Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP 2015) ,Brisbane,Australia,May2015.\n[8]D. Ellis. Beat Tracking by Dynamic Programming.\nJournal of New Music Research ,36(1):51–60,2007.\n[9]F.Gouyon,A.Klapuri,S.Dixon,M.Alonso,G.Tzane-\ntakis, C. Uhle, and P. Cano. An experimental com-\nparison of audio tempo induction algorithms. IEEE\nTrans. on Audio, Speech and Language Processing ,\n14(5):1832–1844,2006.\n[10]J. Hockman, M. E. P. Davies, and I. Fujinaga. One in\nthe Jungle: Downbeat Detection in Hardcore, Jungle,\nandDrumandBass.In Proc. of the 13th Intl. Society for\nMusic Information Retrieval Conference (ISMIR 2012) ,\npages169–174,Porto,Portugal,October2012.\n[11]F. Krebs, S. Böck, and G. Widmer. Rhythmic Pattern\nModeling for Beat- and Downbeat Tracking in Musi-\ncal Audio. In Proc. of the 14th Intl. Society for Music\nInformation Retrieval Conference (ISMIR 2014) ,pages\n227–232,Curitiba,Brazil,November2013.[12]F. Krebs, A. Holzapfel, A. T. Cemgil, and G. Wid-\nmer. Inferring Metrical Structure in Music Using Par-\nticle Filters. IEEE/ACM Trans. on Audio, Speech, and\nLanguage Processing ,23(5):817–827,May2015.\n[13]M. F. McKinney, D. Moelants, M. E. P. Davies, and\nA. Klapuri. Evaluation of Audio Beat Tracking and\nMusic Tempo Extraction Algorithms. Journal of New\nMusic Research ,36(1):1–16,2007.\n[14]G. Peeters and H. Papadopoulos. Simultaneous Beat\nand Downbeat-Tracking Using a Probabilistic Frame-\nwork: Theory and Large-Scale Evaluation. IEEE\nTrans. on Audio, Speech and Language Processing ,\n19(6):1754–1769,2011.\n[15]A.Pikrakis,I.Antonopoulos,andS.Theodoridis.Mu-\nsicmeterandtempotrackingfromrawpolyphonicau-\ndio. In Proc. of the 5th Intl. Conference on Music In-\nformation Retrieval (ISMIR 2004) , Barcelona, Spain,\nOctober2004.\n[16]P. Sambamoorthy. South Indian Music Vol. I-VI . The\nIndianMusicPublishingHouse,1998.\n[17]X. Serra. Creating Research Corpora for the Compu-\ntational Study of Music: the case of the CompMusic\nProject. In Proc. of the 53rd AES Intl. Conference on\nSemantic Audio ,London,January2014.\n[18]A.Srinivasamurthy. A Data-driven Bayesian Approach\nto Automatic Rhythm Analysis of Indian Art Mu-\nsic. Doctoral dissertation, Universitat Pompeu Fabra,\nBarcelona,Spain,2016.\n[19]A. Srinivasamurthy, A. Holzapfel, Ali Taylan Cemgil,\nandX.Serra.ParticleFiltersforEfficientMeterTrack-\ning with Dynamic Bayesian Networks. In Proc. of the\n16th Intl. Society for Music Information Retrieval Con-\nference (ISMIR 2015) ,pages197–203,Malaga,Spain,\nOctober2015.\n[20]A. Srinivasamurthy, A. Holzapfel, and X. Serra. In\nSearch of Automatic Rhythm Analysis Methods for\nTurkish and Indian Art Music. Journal of New Music\nResearch,43(1):97–117,2014.\n[21]A.Srinivasamurthy,G.K.Koduri,S.Gulati,V.Ishwar,\nandX.Serra.CorporaforMusicInformationResearch\nin Indian Art Music. In Proc. of 13th Sound and Mu-\nsic Computing Conference ,pages1029–1036,Athens,\nGreece,September2014.\n[22]A. Srinivasamurthy and X. Serra. A Supervised Ap-\nproach to Hierarchical Metrical Cycle Tracking from\nAudio Music Recordings. In Proc. of the 39th IEEE\nIntl. Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP 2014) , pages 5237–5241, Florence,\nItaly,May2014.\n[23]N. Whiteley, A. T. Cemgil, and S. Godsill. Sequential\nInference of Rhythmic Structure in Musical Audio. In\nProc. of the IEEE Intl. Conference on Acoustics, Speech\nand Signal Processing (ICASSP 2007) ,volume4,pages\n1321–1325,Honolulu,USA,April2007.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 685"
    },
    {
        "title": "A Music Player with Song Selection Function for a Group of People.",
        "author": [
            "Junichi Suzuki",
            "Tetsuro Kitahara"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1414868",
        "url": "https://doi.org/10.5281/zenodo.1414868",
        "ee": "https://zenodo.org/records/1414868/files/SuzukiK17.pdf",
        "abstract": "There are often situations in which a group of people gather and listen to the same songs. However, major- ity of existing studies related to music information re- trieval (MIR) have focused on personalization for individ- ual users, and there have been only a few studies related to MIR intended for a group of people. Here, we present an Android music player with a music selection function for people who are listening to the same songs in the same place. We assume that each user owns his/her favorite songs on his/her Android device. Once a group of users gathers each user can launch this player on his/her smart- phone. Then, the player running on each device starts to communicate with other devices via Bluetooth. Informa- tion about songs stored in every device, along with the playback history, is collected to a device referred to as the master device. Then, the master device estimates each user’s preference for every song based on playback his- tory and music similarity. The master device then extracts songs that are highly preferred and sends a command to start playback to the devices storing these songs. Our ex- perimental results demonstrate the successful estimation of music preferences based on music similarity.",
        "zenodo_id": 1414868,
        "dblp_key": "conf/ismir/SuzukiK17",
        "keywords": [
            "Android",
            "music player",
            "group music listening",
            "Bluetooth communication",
            "personalized music selection",
            "music preference estimation",
            "playback history",
            "song similarity",
            "master device",
            "experimental results"
        ],
        "content": "A MUSIC PLAYER WITH SONG SELECTION FUNCTION\nFOR A GROUP OF PEOPLE\nJun’ichi Suzuki and Tetsuro Kitahara\nCollege of Humanities and Sciences, Nihon University, Tokyo 156-8550, Japan\nfjunichi,kitahara g@kthrlab.jp\nABSTRACT\nThere are often situations in which a group of people\ngather and listen to the same songs. However, major-\nity of existing studies related to music information re-\ntrieval (MIR) have focused on personalization for individ-\nual users, and there have been only a few studies related to\nMIR intended for a group of people. Here, we present an\nAndroid music player with a music selection function for\npeople who are listening to the same songs in the same\nplace. We assume that each user owns his/her favorite\nsongs on his/her Android device. Once a group of users\ngathers each user can launch this player on his/her smart-\nphone. Then, the player running on each device starts to\ncommunicate with other devices via Bluetooth. Informa-\ntion about songs stored in every device, along with the\nplayback history, is collected to a device referred to as\nthe master device. Then, the master device estimates each\nuser’s preference for every song based on playback his-\ntory and music similarity. The master device then extracts\nsongs that are highly preferred and sends a command to\nstart playback to the devices storing these songs. Our ex-\nperimental results demonstrate the successful estimation of\nmusic preferences based on music similarity.\n1. INTRODUCTION\nIn situations such as parties and carpooling, it is common\nthat a group of people gather and listen to the same back-\nground music. However, it is not easy to select songs in\nsuch situations: if a particular member selects songs based\non his/her musical preferences, other members with differ-\nent musical preferences may be unsatisﬁed. To resolve this\nproblem, we need a mechanism to extract each member’s\nmusical preferences and select songs taking into account\nthose preferences.\nAlthough music information retrieval (MIR) has a long\nhistory of technology development [3], relatively few at-\ntempts have been made to develop MIR techniques for a\ngroup of people. MusicFX, developed by Jseph et al. [6],\nc⃝Jun’ichi Suzuki and Tetsuro Kitahara. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Jun’ichi Suzuki and Tetsuro Kitahara. “A Music\nPlayer with Song Selection Function for a Group of People”, 18th In-\nternational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.selects a music broadcasting station from 91 stations spe-\ncializing in different music genres, and the selection pro-\ncess is indirectly inﬂuenced by the musical preference of\neach member present at that place. This system uses a pref-\nerence database consisting of every member’s preference\nfor 91 genres on a scale of -2 to 2. After detecting who is\npresent based on each member’s electronic badge, the sys-\ntem determines a broadcasting station using this database\nwith a group preference arbitration algorithm. This algo-\nrithm basically computes a group preferences as a squared\nsummation of individual preferences. Crossen et al. [4] de-\nveloped a similar system called Flytrap. This system also\nattempts to play songs that are pleasing to every person\npresent. The system detects the people who are present\nand sends information about each member’s previous mu-\nsic choices to a server. Based on a voting mechanism in\nwhich high votes are given to those that have been listened\nto previously, songs to be played back are determined. A\nweb application developed by Popescu et al. [8], called\nGroupFun, helps a group of friends agree on a common\nmusic playlist. With GroupFun, users can listen to and rate\ntheir own songs as well as their friends’ songs. The sys-\ntem then arbitrates between the users’ preferences using\nfour different algorithms to determine which songs to play.\nBlueMusic, developed by Mahato et al. [1], uses Bluetooth\nto send an indivdual’s musical preference data to a pub-\nlic music playback system. Individual users enter their\nmusical preferences into a web form in advance and ob-\ntain strings, such as “Bm+A1R3EST3,” that encodes their\nmusical preferences. Their mobile devices then broadcast\nthese data as Bluetooth device names. The public music\nplayback system collects such strings to determine which\nsongs to play back. In addition, some researchers devel-\noped music recommendation systems for a group of peo-\nple [5,7]. These systems typically assume that (1) informa-\ntion on individual users’ musical preferences is collected\n(or estimated from playback histories) in advance and (2)\nsongs to be played back are stored in a server or public\nplayback system.\nHere, we develop a music selection and playback appli-\ncation under the following assumptions:\n\u000fIndividual users own songs inside their own smart-\nphones (or mobile devices).\nThis means that songs to be played back are stored\nseparately in multiple devices. Appropriate songs\nshould be selected from a music collection dispers-229ing at multiple devices and should be played back\nwithout manually switching any settings.\n\u000fNo server or special equipment is necessary.\nThe application should run on individual users’ de-\nvices and songs are never copied to a server to avoid\ncopyright issues.\n\u000fAn individual device possesses information about\nonly its playback history.\nThe application running on each device has no prior\nknowledge about how much the device owner favors\neach song stored in other devices. The appliaction\nhas to estimate this information from data that the\ndevice has.\nTo meet these assumptions, we design the application\nbased on the following policy:\n\u000fOne of the devices is regarded as the master devices,\nand every device communicates the list of songs\nstored in it and its playback history (in particular,\nhow many times the device owner has played back\neach song) to that device.\n\u000fThe master device does not collect the waveforms of\nsongs stored in other devices but rather commands\nthe device storing a song to be played back to con-\nnect itself directly to the Bluetooth speaker. If a\nsong stored in a different device is selected next,\nthis device will be automatically connected to the\nspeaker after the current device is disconnected from\nthe speaker.\n\u000fEach user’s preference for songs stored in others’ de-\nvices is estimated based on the similarity to songs\nstored in his/her own device.\nThe rest of the paper is organized as follows: In Sec-\ntion 2, we describe the overview of our application and\npresent a method for estimating the degree of preference\nnoted above. In Section 3, we report the system implemen-\ntation and experiments. Finally, we conclude the paper in\nSection 4.\n2. SYSTEM OVERVIEW\nThis application aim to select songs from a collection sep-\narately stored in different devices and play the songs back\nseamlessly. As discussed in the Introduction section, we\ndesigned the application based on the following policies:\n\u000fEvery device communicates information about\nsongs (not the waveforms themselves) to the master\ndevice.\n\u000fAfter the song selection process occurs, the mas-\nter device commands the device storing the se-\nlected song to connect itself directly to the Bluetooth\nspeaker to avoid copying the waveform somewhere.\nFigure 1 . Overview of system ﬂow in the networked play-\nback mode\n\u000fEach user’s preferences for songs stored on other\nusers’ devices is estimated based on the similarity\nbetween these songs and the songs stored in his/her\ndevice.\nThe application has two different modes. One is the nor-\nmal playback mode, in which the user listens to songs in a\nnormal way. This mode provides basically the same func-\ntionalities as a typical music player and is used to store\nthe playback history, particularly the number of plays (i.e.,\nhow many times the user has listened to each song). The\nother mode is a networked playback mode, which is the\nprimary mode of this application. Once users gather and\nlaunch the application on their devices, the devices start\nto communicate with one other via Bluetooth, and one of\nthe devices is set to be the master device. After estab-\nlising aBluetooth connection, the master device collects\ninformation about the list of songs stored in each device\nand the number of plays (how many times each song is\nplayed back) from the other devices. Then, the master de-\nvice generates a playlist and commands the device storing\neach song in the playlist to play back the song.230 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Below, we illustrate the procedure of the networked\nplayback mode (Figure 1).\n2.1 Launching the Application\nLetU=fu1;\u0001 \u0001 \u0001; ungandD=fd1;\u0001 \u0001 \u0001; dngbe a\ngroup of users and a set of the users’ devices, respectively,\nwhere d1is the master device. Each device dicommuni-\ncates the list of songs M(di)=fm(di;1);\u0001 \u0001 \u0001; m(di;n(di))g\nand the number of plays for every song F(m(di;k))(k=\n1;\u0001 \u0001 \u0001; n(di)) to the master device d1.\n2.2 Calculating the Degree of Preference\nFor each user ui, the degree of preference for every song is\ncalculated. Let W(ui; m(dj;k))be the degree of preference\nof user uifor song m(dj;k). We formulate the degree of\npreference based on the following assumptions:\n1)If a user listen to a song frequently, his/her prefer-\nence for that song should be high.\n2)If Song A is similar to Song B, which is highly fa-\nvored, Song A should also highly favored.\nBased on the ﬁrst assumption, we can calculate the degree\nof a user’s preference for songs stored in his/her own de-\nvice using the number of plays. On the other hand, the\ndegree of preference for songs stored in others’ devices\ncannot be calculated based on the number of plays because\nsuch information is not available. Based on the second as-\nsumption, we calculate the degree of preference for such\nsongs using that for similar songs owned by oneself.\n2.2.1 The Degree of Preference for Owned Songs\nThe degree of preference for songs stored in a user’s own\ndevice is calculated based on the number of plays. Here\nwe prepare two different deﬁnitions:\nW(ui; m(di;k)) = 1 \u00001\nfF(m(di;k)) + 1g\u000b(1)\nand\nW(ui; m(di;k)) = cosh \fF(m(di;k)); (2)\nwhere \u000band\fare parameters.\n2.2.2 The Degree of Preference for Songs That Are Not\nOwned\nHere, we calculate the degree of preference for songs\nstored in other users’ devices W(ui; m(dj;k))(i̸=j).\nFirst of all, the similarity of m(dj;k)to every song stored\nindiis calculated. Musical similarity is a very difﬁ-\ncult concept and its calculation is still an open problem.\nHowever, various similarity measures have been developed\nfrom different points of view (e.g., [2]). Here, we combine\ntwo different similarity measures: acoustic similarity and\n(socially obtained) artist similarity.\nTo calculate acoustic similarity between two songs,\nm(dj;k)andm(di;l), a sequence of 20-dimensional mel-\nfrequency cepstral coefﬁcient (MFCC) vectors is calcu-\nlated from each song by adopting a shift by 160 sam-\nples after the waveform is resampled to 16 kHz. TheEarth mover’s distance between the two sequences, de-\nnoted by D(m(dj;k); m(di;l)), is calculated. The similarity\nsim\nMFCC(m(dj;k); m(di;l))is then calculated by using\nsim\nMFCC(m(dj;k); m(di;l)) =1\n1 +D(m(dj;k); m(di;l)):\nThe artist similarity between m(dj;k)andm(di;l)is cal-\nculated based on the Last.fm API1. The Last.fm API has a\nfunction, artist.getSimilar , which returns up to 100 similar\nartists to a speciﬁed artist, along with similarity values on a\nscale of 0 to 1. Let a(dj;k)be the artist of the song m(dj;k).\nThe list of artists similar to a(dj;k), denoted by A(dj;k),\nand their similarities sim\nlast:fm(a(dj;k); a′)(a′2 A (dj;k)) are\nobtained using Last.fm. In general, sim\nlast:fm(\u0001;\u0001)is not sym-\nmetric. We therefore deﬁne the artist similarity as follows:\ni)When a(dj;k)=a(di;l),\nsim\nartist(m(dj;k); m(di;l)) = 1 :0:\nii)When a(dj;k)2 A (di;l)anda(di;l)2 A (dj;k),\nsim\nartist(m(dj;k); m(di;l))\n=1\n2{\nsim\nlast:fm(a(dj;k); a(di;l)) + sim\nlast:fm(a(di;l); a(dj;m))}\niii)When a(dj;k)2 A (di;l)anda(di;l)=2 A (dj;k),\nsim\nartist(m(dj;k); m(di;l)) = sim\nlast:fm(a(dj;k); a(di;l))\niv)When a(dj;k)=2 A (di;l)anda(di;l)2 A (dj;k),\nsim\nartist(m(dj;k); m(di;l)) = sim\nlast:fm(a(di;l); a(dj;k))\nv)When a(dj;k)=2 A (di;l)anda(di;l)=2 A (dj;k),\nsim\nartist(m(dj;k); m(di;l)) =\";\nwhere \"is basically zero but can be set to a very\nsmall positive value to take into account the possibil-\nity of sparseness in similar artist responses ( \"= 0:01\nin the current implementation).\nThe similarity between two songs, denoted by\nsim(m(dj;k); m(di;l)), can be calculated as\nsim(m(dj;k); m(di;l))\n= sim\nMFCC(m(dj;k); m(di;l))\u0001sim\nartist(m(dj;k); m(di;l)):\nUsing this similarity measure, the degree of preference\ncan be calculated as follows:\nW(ui; m(dj;k)) =∑\nlsim(m(dj;k); m(di;l))W(ui; m(di;l))\n∑\nlsim(m(dj;k); m(di;l)):\n1http://www.last.fm/apiProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 2312.3 Integration of Degrees of Preference\nThe degree of every user’s preference is integrated as fol-\nlows:\nWall(m(dj;k)) =n∏\nq=1W(uq; m(dj;k))\n2.4 Generating a Playlist\nAfter the integrated degrees of preference Wall(m(dj;k))\nfor all songs stored in all of the devices are calculated, a\nnecessary number of songs are selected in order of the in-\ntegrated degree of preference.\n2.5 Playing Songs\nLetL=fm1;\u0001 \u0001 \u0001; mcgbe the list of the selected songs\nand let d(mi)be the device storing the song mi. For each\nsongmi, the following steps are executed:\n1)The master device commands d(mi)to connect it-\nself to the Bluetooth speaker.\n2)The device d(mi)starts to play back mi.\n3)The master device broadcasts the information (title,\nartist name, etc.) of the song being played to all de-\nvices to alert users about which song is being played.\n4)Once the playback ends, the device d(mi)discon-\nnects from the Bluetooth speaker and sends the mas-\nter device a message communicating the end of play-\nback.\n5)Return to 1) for the next song.\nAdvanced Audio Distribution Proﬁle (A2DP) is used for\nthe connection between each device and the speaker. For\ncommunication between devices, Serial Port Proﬁle (SPP)\nis used.\n3. IMPLEMENTATION AND EXPERIMENTS\n3.1 Implementation\nWe implemented this music player on Android 5.0 smart-\nphones. Screenshots are shown in Figure 2. A demo\nvideo of this application is available at https://www.\nyoutube.com/watch?v=gcOWjkBc_EA . For\nroughly one hour, we conﬁrmed that this application\nsucessfully selected songs, switched the connection to the\nBluetooth speaker, and played the selected songs without\nany troubles.\n3.2 Evaluation of the Degree of Preference for Owned\nSongs\nWe conﬁrmed the appropriateness of the calculation of the\ndegrees of preference for a user’s own songs by checking\nhow these degrees calculated with our method matched the\nactual level of preference reported by the users.3.2.1 Dataset\nData of playback histories and preferences were obtained\nfrom Last.fm. Using the Last.fm API, we obtained a user\nproﬁle containing playback histories and evaluations (the\n“Liked” tag or no tag) for vairous songs. We collected user\nproﬁles for 45,745 users, who were speciﬁed by choosing\none user at random and then traversing “Friend” links re-\ncursively. However, many users did not apply the “Liked”\ntag to any songs, we therefore chose to focus on the 11,074\nusers who gave “Liked” tags to 20–5000 songs. The total\nnumber of the playback histories is 98,504,128.\n3.2.2 Results\nWe analyzed the ratio of songs with the “Liked” tag with\nrespect to the numbers of plays binned by 5 (Figure 3).\nFrom this ﬁgure, one can see that as the number of plays in-\ncreases, the ratio of songs with the “Liked” tag increases.\nRegarding this ratio as the ground truth of the degree of\npreference, we evaluated the degree of preference calcu-\nlated with Equations (1) and (2). The results are shown in\nFigure 4. One can see that using Equation (1) with \u000b=\n0:01approximates the ground truth well. For songs with\nless than 20 times of plays, Equation (1) with \u000b= 0:10\nand Equation (2) with \f= 0:005approximate better.\n3.3 Evaluation of the Degree of Preference for Songs\nThat Are Not Owned\nNext, we conﬁrm the appropriateness of the calculation of\nthe degree of preference for songs not owned by a user.\n3.3.1 Dataset\nWe used the Last.fm Dataset2, which consists of playback\nhistories of 1,000 users. Of the songs contained in this\ndatabase, some songs were regarded as owned songs and\nother songs were regarded as unowned songs and their\nplayback histories were accordingly hidden. From the\nplayback histories of the songs regared as unowned, the de-\ngrees of preference were estimated. Ideally, the estimated\ndegrees should be compared with real favor values (e.g.,\nfrom questionnaires), but such data were not available from\nthis database. We therefore regarded the degrees of pref-\nerence calculated from the playback histories as the quasi\nground truth. Because this database does not include the\nwaveforms themselves, a 30-second version of every song\nis downloaded via 7digital API3for feature extraction for\ncalculating acoustic similarity.\n3.3.2 Procedure\nWe extracted 118 three-user groups such that the number\nof songs listened to by three users of every group exceeded\n300. For each group, we divided the songs listened to by\nthe group members into three sets; each set was regarded as\nbeing owned by each member. We calculated the degrees\nof preference for unowned songs of each member using the\n2http://ocelma.net/\n3https://www.7digital.com/232 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 2 . Screenshot of our Android music player. Left: search of other devices via Bluetooh, Center: playlist display,\nRight: music playback\nFigure 3 . The ratio of songs with the “Liked” tag with\nrespect to the numbers of plays.\nproposed method. At the same time, the degrees of pref-\nerence for the same songs were calculated using Equation\n(1) or (2) and were regarded to be the quasi ground truth.\n3.3.3 Results\nThe correlations between the degrees of preference calcu-\nlated using our method and the quasi ground truth are listed\nin Table 1. These data show that the correlation is approx-\nimately 0.6 with almost any parameters and therefore that\nthe degree of preference is fairly appropriately estimated.\nFigure 5 shows the correlations for each user. The correla-\ntions were higher than 0.5 for roughly half of the users.\n4. CONCLUSION\nWe have proposed an Android application that makes it\npossible to seamlesssly enjoy songs that are separately\nstored on different smartphones or devices. This applica-\n(a) With Equation (1)\n(b) With Equation (2)\nFigure 4 . Estimation of degree of preference for owned\nsongs\ntion has two features. One feature is networked playback.\nThe master device commands other devices to connect to\nor disconnect from the Bluetooth speaker. Users are ac-\ncordingly freed from manually switching the device con-\nnection. The other feature is music selection. Using each\nuser’s playback history and musical similarity, the appli-\ncation estimates the degree of each user’s preference even\nfor songs that have not been listened to. Experimental re-\nsults reveal that the degrees of preference estimated by our\nmethod are correlated with the quasi ground truth.\nThere is still a lot of future work. First, the currentProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 233(a) With Equation (1) for owned songs\n\u000b 0.01 0.1 0.2 0.3 0.4\nCorr. 0.60548 0.60419 0.60217 0.5960 0.59658\n(b) With Equation (2) for owned songs\n\f 0.002 0.005 0.100 0.205 0.300\nCorr. 0.56625 0.56708 0.60060 0.60132 0.59658\nTable 1 . Estimation of degree of preference for unowned\nsongs\nFigure 5 . Histogram of per-user correlations between the\nestimated degree of preference and the quasi ground truth.\nplaylist generation process—simply placing songs in order\nof degree of preference—is too naive. We have to improve\nthis process to maintain the users’ satisfaction from the be-\nginning to the end of the playlist. We should also conduct\nusability tests because experiments presented here were fo-\ncused only on the music selection process. In addition, we\nplan to distribute this application on Google Play to obtain\nuser feedback for further improvements.\n5. ACKNOWLEDGMENTS\nThis work was supported by JSPS KAKENHI Grant\nNumbers 26240025, 26280089, 16K16180, 16H01744,\n16KT0136, and 17H00749. We approiate Prof. Tomonobu\nOzaki for his fruitful comments.\n6. REFERENCES\n[1]Implicit personalization of public environments using\nBluetooth. In Proceedings of CHI EA ’08 Extended\nAbstracts on Human Factors in Computing Systems ,\npages 3093–3098, 2008.\n[2]J.-J. Aucouturier and F. Pachet. Tools and architecture\nfor the evaluation of similarity measures: Case study\nof timbre similarity. In Proc. ISMIR , pages 198–203,\n2004.\n[3]M. Casey. General sound classiﬁcation and similarity\nin mpeg-7. Organaized Sound , 6(2), 2002.[4]A. Crossen and J. Budzik an K. J. Hammond. Flytrap:\nIntelligent group music recommendation. In Proceed-\nings of International Conference on Intelligent User\nInterfaces (IUI ’02) , pages 184–185, 2002.\n[5]Pedro Dias and Jo ˜ao Magalh ˜aes. Music recommenda-\ntions for groups of users. In Proceedings of the 2013\nACM international workshop on Immersive media ex-\nperiences , pages 21–24. ACM, 2013.\n[6]Joseph F. McCarthy and Theodore D. Anagnost. Mu-\nsicfx: an arbiter of group preferences for computer sup-\nported collaborative workouts. In Proceedings of the\n1998 ACM conference on Computer supported coop-\nerative work (CSCW ’98) , pages 363–372, 1998.\n[7]Auste Piliponyte, Francesco Ricci, and Julian\nKoschwitz. Sequential music recommendations for\ngroups by balancing user satisfaction. In UMAP\nWorkshops , 2013.\n[8]George Popescu and Pearl Pu. What’s the best music\nyou have? designing music recommendation for group\nenjoyment in GroupFun. In Works-in-Progress of the\nACM SIGCHI Conference on Human Factors in Com-\nputing Systems (CHI ’12) , pages 1673–1678, 2012.234 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Accurate Audio-to-Score Alignment for Expressive Violin Recordings.",
        "author": [
            "Jia-Ling Syue",
            "Li Su 0002",
            "Yi-Ju Lin",
            "Pei-Ching Li",
            "Yen-Kuang Lu",
            "Yu-Lin Wang",
            "Alvin W. Y. Su"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416188",
        "url": "https://doi.org/10.5281/zenodo.1416188",
        "ee": "https://zenodo.org/records/1416188/files/SyueSLLLWS17.pdf",
        "abstract": "An audio-to-score alignment system adaptive to various playing styles and techniques, and also with high accuracy for onset/offset annotation is the key step toward advanced research on automatic music expression analysis. Techni- cal barriers include the processing of overlapped notes, re- peated note sequences, and silence. Most of these charac- teristics vary with expressions. In this paper, the audio-to- score alignment problem of expressive violin performance is addressed. We propose a two-stage alignment system composed of the dynamic time warping (DTW) algorithm, simulation of overlapped sustain notes, background noise model, silence detection, and refinement process, to better capture the onset. More importantly, we utilize the non- negative matrix factorization (NMF) method for synthesis of the reference signal in order to deal with highly diverse timbre in real-world performance. A dataset of annotated expressive violin recordings in which each piece is played with various expressive musical terms is used. The opti- mal choice of basic parameters considered in conventional alignment systems, such as features, distance functions in DTW, synthesis methods for the reference signal, and en- ergy ratios, is analyzed. Different settings on different ex- pressions are compared and discussed. Results show that the proposed methods notably improve the conventional DTW-based alignment method.",
        "zenodo_id": 1416188,
        "dblp_key": "conf/ismir/SyueSLLLWS17",
        "keywords": [
            "audio-to-score alignment system",
            "various playing styles",
            "high accuracy",
            "onset/offset annotation",
            "advanced research",
            "automatic music expression analysis",
            "overlapped notes",
            "repeated note sequences",
            "silence",
            "expressive violin performance"
        ],
        "content": "ACCURATE AUDIO-TO-SCORE ALIGNMENT FOR EXPRESSIVE VIOLIN\nRECORDINGS\nJia-Ling Syue1\u0003Li Su2\u0003Yi-Ju Lin1Pei-Ching Li1\nYen-Kuang Lu1Yu-Lin Wang1Alvin W. Y. Su1\n1SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan\n2Music and Culture Technology Lab., IIS, Academia Sinica, Taiwan\nP76044457@mail.ncku.edu.tw, lisu@iis.sinica.edu.tw\nABSTRACT\nAn audio-to-score alignment system adaptive to various\nplaying styles and techniques, and also with high accuracy\nfor onset/offset annotation is the key step toward advanced\nresearch on automatic music expression analysis. Techni-\ncal barriers include the processing of overlapped notes, re-\npeated note sequences, and silence. Most of these charac-\nteristics vary with expressions. In this paper, the audio-to-\nscore alignment problem of expressive violin performance\nis addressed. We propose a two-stage alignment system\ncomposed of the dynamic time warping (DTW) algorithm,\nsimulation of overlapped sustain notes, background noise\nmodel, silence detection, and reﬁnement process, to better\ncapture the onset. More importantly, we utilize the non-\nnegative matrix factorization (NMF) method for synthesis\nof the reference signal in order to deal with highly diverse\ntimbre in real-world performance. A dataset of annotated\nexpressive violin recordings in which each piece is played\nwith various expressive musical terms is used. The opti-\nmal choice of basic parameters considered in conventional\nalignment systems, such as features, distance functions in\nDTW, synthesis methods for the reference signal, and en-\nergy ratios, is analyzed. Different settings on different ex-\npressions are compared and discussed. Results show that\nthe proposed methods notably improve the conventional\nDTW-based alignment method.\n1. INTRODUCTION\nAn audio-to-score alignment algorithm captures note-level\ninformation of a music performance with the aid of sym-\nbolic data such as MIDI. It has numbers of applications\nin the ﬁeld of music information retrieval (MIR), such as\nautomatic accompaniment [4], and music retrieval through\nmatching a MIDI ﬁle to a polyphonic audio recording [7].\nBesides, an effective audio-to-score alignment algorithm\nc\rJia-Ling Syue, Li Su, Yi-Ju Lin, Pei-Ching Li, Yen-\nKuang Lu, Yu-Lin Wang, Alvin W. Y . Su. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Jia-Ling Syue, Li Su, Yi-Ju Lin, Pei-Ching Li, Yen-Kuang Lu,\nYu-Lin Wang, Alvin W. Y . Su. “Accurate audio-to-score alignment for\nexpressive violin recordings”, 18th International Society for Music Infor-\nmation Retrieval Conference, Suzhou, China, 2017.is also critical in computational music analysis, speciﬁ-\ncally in the case of extracting the note-level information\nin expressive music recordings for music expression anal-\nysis. For example, Li et al. [10] used an audio-to-score\nalignment algorithm [19] to annotate the onset and offset\npositions of each note in a dataset, including violin solo\npieces interpreted by professional violinists with 10 ex-\npressive musical terms.1However, such annotation still\nneeds to be checked and corrected manually as it suffers\nfrom low quality when there are overlapped sustain seg-\nments and unexpected silence between successive notes in\nexpressive violin performance. Therefore, an improved\naudio-to-score alignment algorithm for expressive violin\nperformance would be of great help to avoid such a tedious\nand labor-intensive process.\nThere have been numbers of audio-to-score align-\nment algorithms proposed in the past few decades, based\non graphical models [2, 15, 16], hidden Markov models\n(HMM) [3, 6, 13, 18], and DTW [5, 12, 14]. Among these\nmodels, an HMM [13] is of better potential in modeling\nhow the states of attack, decay or sustain evolve in a note,\nbut to train the model parameters one needs amounts of\ncorrectly annotated data which are, as mentioned, hard to\nget without an improved audio-to-score alignment algo-\nrithm. Therefore, as an attempt to with low-resource data,\nwe opt to use DTW with further processing steps, which\ncan be implemented without the need of a large dataset.\nAnother challenge of matching a score to a musical\nrecording is the diversity of timbre of the input signals,\ndepending on the performer, instrument, recording envi-\nronment, etc. Mismatch of spectral features between the\nMIDI-synthesized reference and the real violin sounds of\nthe same event leads to errors in alignment. Such an is-\nsue was addressed by an HMM-based model exploiting\nthe spectral templates adaptive to different recordings [9].\nIn this paper, we adopt a more straightforward approach,\n1The SCREAM-MAC-EMT dataset compiled by Li et al. contains\nrecordings of 10 different classical music pieces, each of which is in-\nterpreted with 5 selected expressions by 11 musicians [10]. It consid-\ners in total 10 expressive musical terms, including Scherzando (play-\nful), Tranquillo (calm), Con Brio (bright), Maestoso (majestic), Riso-\nluto (rigid), Affettuoso (affectionate), Agitato (agitated), Cantabile (like\nsinging), Grazioso (graceful), and Espressivo (expressive). The experi-\nments in this paper are performed on a subset of this dataset, which con-\ntains 50 recordings from randomly selected 3 musicians’ performance.250Figure 1 . Flowchart of the proposed audio-to-score align-\nment system for expressive violin performance.\nwhich utilizes the NMF method to directly learn the spec-\ntral template for synthesis under low-resource data.\nBy inspecting the data, we raise four issues which could\nbe seen in aligning an expressive violin recording with\nits corresponding MIDI. First, the strong sustain of one\nnote could be overlapped with the weak onset of its next\nnote, and this makes the algorithm fail to accurately cap-\nture the onset time of the next note. Second, in a repeated\nnote sequence, an algorithm is prone to erroneous onset\ndetection since all notes have the same pitch. Third, the\nstaccato technique usually causes unexpected silent seg-\nments which are not compatible to the ground-truth MIDI.\nLastly, the background noise in a real-world environment\nmay also cause mismatching. These issues are commonly\nseen; for instance, an inspection shows that in the dataset,\n37% of the notes have overlapped sustain with their suc-\ncessive notes, 35% are in a repeated note sequence, 6%\nhave unexpected silence, and 2% follow a rest symbol.\nTo this end, we add solutions to deal with the four prob-\nlems: First, we simulate the note overlap in two-stage\nalignment (cf. Section 2.3). Second, we perform the dura-\ntion ratio of repeated notes (cf. Section 2.4) and silence de-\ntection (cf. Section 2.2) to reﬁne onset positions. Besides,\nwe add a background noise template to model the rest parts\nin a recording (cf. Section 2.1). These processes are evalu-\nated after a systematic experiment which ﬁnds the optimal\nparameters such as features, timbres for synthesis, distance\nmeasures, and energy measures in an audio-to-score algo-\nrithm (cf. Section 3.1). Moreover, we also discuss the pre-\ncision of proposed alignment system within different levels\nof error tolerance, and draw an insight from analyzing the\nexpression-wise performance (cf. Section 3.2).\n2. THIS WORK\nFigure 1 shows the diagram of the proposed alignment sys-\ntem, whose goal is to ﬁnd the onset position accurately in\nexpressive violin performance. The system takes an au-\ndio signal and its corresponding MIDI ﬁle as input. We\nadopt the NMF to learn the spectral patterns of the audio\ninput of violin solo for MIDI-to-audio synthesis. Then, ei-\nFigure 2 . The audio played with staccato has extra silence\nsegments (green) that could not be found from the score.\nFigure 3 . An example of a note with strong energy in sus-\ntain segment overlaps with its successive note, which has\nweak energy in attack segment.\nther chroma or log-frequency spectral features, which are\nthe basic parameters considered in conventional alignment\nsystems (cf. Section 3.1), are extracted from both the au-\ndios. Incorporated with a silence detection process, a two-\nstage DTW-based audio-to-score alignment and a reﬁne-\nment process are conducted for resolving the discrepancies\nbetween audio and MIDI in expressive recordings includ-\ning overlapped sustain segments, unexpected silence seg-\nments, and repeated note sequences.\n2.1 NMF and Spectral Synthesis\nFor MIDI-to-audio conversion, the NMF with the\nKullback-Leibler divergence [17] is adopted to train the\nspectral patterns of each pitch in a recording. The NMF\ndecomposes an audio spectrogram Vinto two matrices W\nandH, i.e.V\u0019^V=WH , whereWis a spectral\ntemplate represented in column, and His a time-varying\nenergy activation represented in row. Following Joder et\nal.[8], we adopt a Gaussian mixture model to initialize the\ntemplate matrix Wfor each pitch with kGaussian func-\ntions centered at the fundamental frequency and the ﬁrst\nk-th harmonics of the pitch. Due to the weak energy in\nthe high-frequency range, we take kto be 4, the weight of\neach Gaussian function to be k\u00002, and the variance to be 30\ncents. Besides, we consider the frequency range from 65\nHz to 4 kHz, which removes the high- and low-frequency\nnoise. The activation matrix His initialized by a normal\ndistribution with zero mean and unit standard deviation.\nMoreover, we add an additional noise template (NT) with\nrandom numbers in [0, 1] to simulate the noise of silence\nparts in the recorded audio signal. Furthermore, we adopt\na preprocessing step of stretching or shortening the refer-\nence signal by insertion or deletion of frames so as to makeProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 251Figure 4 . Examples of accurate and erroneous alignment paths: (a) accurate result, (b) error caused by overlapped sustain\nsegment, (c) simulation of overlapped sustain segment, and error caused by (d) additional silence and (e) repeated notes.\nits length similar to the input, in order to reduce the effect\nof tempo changes in expressive violin performance.\n2.2 Silence Detection\nViolinists use different playing techniques to interpret dis-\ntinct expressions. The staccato technique might be the one\nwhich is most likely to cause errors in DTW alignment\namong others due to the silence segments caused by ar-\nticulation of successive notes. Such a silent segment do\nnot have any information in the reference signal but could\nbe found in the audio recording as shown in Figure 2, it\nresults in deterioration in the DTW alignment path. This\nissue is addressed by introducing an extra silence detection\nprocess with energy measurement. From the fast Fourier\ntransform (FFT)2, the energy curve is computed by sum-\nming the spectrum over all the frequency bins and is ex-\npressed in dB scale. A silence segment is one which is\nlonger than 100ms and whose energy is less than 12 dB.\n2.3 Two-stage Alignment\nThe main purpose of the proposed two-stage audio-to-\nscore alignment process is to capture the accurate onset\nfor overlapped successive notes where the former note has\nalong sustain and the latter one has a soft onset . Such\na speciﬁc energy characteristic of violin is likely to cause\nwrong alignment paths. As illustrated in Figure 3, the ﬁrst\nnoteC5, which has strong energy in sustain segment, over-\nlaps with the second note F4, which has weak energy in\nthe attack, and leads to a distorted alignment path. Figure\n2This paper uses a 2,048-point Hamming-windowed FFT with each\nframe staggered by 882 samples (20ms) throughout all the experiments.4(a) shows the ideal accurate result of this example: the\nonset of F4is at position (F;5), while Figure 4(b) shows\nthe actual erroneous result of F4, where the onset of F4\nlocates at (I;5). This is because the ﬁrst 3 frames of F4\nare submerged in the sustain segment of C5.\nThe two-stage alignment process is proposed to solve\nthis problem. In the ﬁrst stage, we adopt the conventional\nDTW-based alignment as our baseline , and obtain a rough\nestimation of the onset of each note. Next, we add the\ninformation of the silence segments mentioned in Section\n2.2. If there is no silence detected between two successive\nnotes, then the two notes are considered overlapped. For\nevery pair of the overlapped notes, we lengthen the ﬁrst\nnote of the reference MIDI with a duration of 3 frames,\nin order to simulate the behavior of overlapped notes.3\nThen, we perform DTW again to align the audio with the\nmodiﬁed MIDI. This is the second-stage alignment (SA)\nprocess. The SA process of the overlapped notes C5-F4is\nshown in Figure 4(c). The two-stage alignment is therefore\na combination of the baseline process and the SA process.\n2.4 Reﬁnement\nThe reﬁnement process contains two tasks. The ﬁrst task\nis to ﬁx the case of staccato andrest, where the errors of\nalignment are usually caused by silence rather than over-\nlapping between two consecutive notes. Figure 4(d) shows\nthe alignment result of an audio played with staccato . The\nideal accurate onset of F4is at position (F;5)as pointed\nby the dashed-line arrow; however, the actual result of F4\n3We observe that for most of the overlapped notes, the overlapping\ndurations are between 2 and 4 frames (40ms and 80ms). Such an obser-\nvation gives to an estimation 3 frames.252 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 5 . An example of repeated notes, whose spectrum\nis highly similar to each other. Their onsets are calibrated\nvia the duration ratio estimated from the reference signal.\nlocates at (D;5), a position within the silence segment, as\npointed by the solid-line arrow. To address this issue, the\nreﬁnement of silence (RS) is implemented as follows: if the\nonset of an aligned note locates in a silence segment of the\naudio, then it will be shifted to the correct position, that is,\nthe frame right after such a silence segment.\nThe second task is to adjust the duration of repeated\nnotes, where the alignment path is hard to estimate because\nof high similarity among the note spectra. This issue is il-\nlustrated in Figure 4(e), where one can see that except for\nthe ﬁrst C5, the onset results of the other notes are at the\nwrong positions, (C;5)and(I;7), respectively, as pointed\nby the solid-line arrows. The correct onsets of the second\nand the third C5are the positions pointed by the dashed-\nline arrows. Figure 5 shows a real-world example of re-\npeated notes in a more detailed manner: the spectra of the\nfour repeated notes A4are highly similar, especially for\nthe last three notes with the same note type. It is hard to\nﬁgure out the accurate onset of each repeated note when\nperforming DTW alignment.\nWe therefore come up with a strategy to deal with this\nproblem. Our assumption is that both audio and its cor-\nresponding MIDI ﬁle have approximately similar duration\nratio of repeated notes. In other words, we can modify the\nonsets of repeated notes by referring to the duration ratio\nof such notes in the reference signal. Given a sequence\nof repeated notes, r= (r1;r2;:::;r m), the reﬁnement of\nrepeated notes (RRN) is realized as follows:\nSTEP 1 Computing the duration of each repeated note,\nS= (S1;S2;:::;S m)andL= (L1;L2;:::;L m),\naccording to the reference signal and the onset re-\nsults of the two-stage alignment, respectively.\nSTEP 2 Calculating the duration ratio for each repeated\nnote, i.e. ratio k=Sk=Pm\nk=1Sk.\nSTEP 3 Estimating the predicted duration for such notes\nin the audio recording, ^L= (^L1;^L2;:::;^Lm), via\nthe calculation of ^Lk=Lk\u0002ratio k.Timbre MIDI synthesizer NMF\nFeature 12-D chroma 84-D linear-log spectrum\nPrecision 72.68 81.37 95.12\nTable 1 . Comparison of precision (in %) using chroma\nand the 84-D spectrum feature. Since the 84-D spectrum\nperforms better, the timbre synthesis via MIDI synthesizer\nor NMF is considered.\nSTEP 4 Adjusting the duration of the ﬁrst m\u00001repeated\nnotes according to the criterion of j^Lk\u0000Lkj>\u0012.\nIf^Lk>Lkthen the onset of the ( k+1)-th note is\nshifted backward by j^Lk\u0000Lkj\u0000\u0012ms. Besides,\nLk+1is also updated by the shifted value. On the\ncontrary, it is updated by shifting forward j^Lk\u0000\nLkj\u0000\u0012ms, and so does the Lk+1.\nOur pilot study shows that choosing \u0012= 60 ms gives better\nperformance.\n3. EV ALUATION\nThe experiments are separated into two parts. The ﬁrst\npart is to ﬁnd the optimal setting used in the conventional\nDTW-based alignment. The second part is the results of the\nproposed system, including the performance of baseline\nprocess, proposed processes, and expression-wise. The test\ndataset contains 10 expressions, each with 5 classical mu-\nsic pieces, totaling 50 excerpts (2,925 notes). We use pre-\ncision as our evaluation method, which is the percentage\nof the number of correct onsets among all the excerpts. A\ncorrect onset is deﬁned as the difference between aligned\nonset time and its corresponding ground-truth onset time,\nbeing less than 100ms.\n3.1 Factors Experiment\nWe consider four types of factors: feature, timbre, distance\nfunction, and energy. The brief introduction and results of\neach factor are described one by one as follows.\nTwo features are considered: chroma [1] and linear-log\nfrequency spectrum [5]. The chroma is a 12-dimensional\nvector representing the energy of the 12 pitch classes (i.e.\nC, C#, ..., B). The linear-log frequency spectrum is a spec-\ntral feature with reduced dimension, performed by a 84-D\nﬁlterbank, which is in linear scale in the low frequencies\nand in logarithm scale in the high frequencies [5]. Such a\nfeature simulates the linear-log frequency sensitivity in hu-\nman auditory systems. Table 1 compares the two features\naccording to the averaged precision (in %) of all the 50 ex-\ncerpts. The results indicate that the linear-log spectrum is\nbetter than the chroma. We therefore select the 84-D spec-\ntrum for the feature factor.\nThen, we compare two methods of synthesizing the ref-\nerence signal from MIDI, where the one is directly through\na MIDI synthesizer, and the other uses the NMF to learn\nthe spectral features from the original audio recording, a\nsimilar strategy of the HMM-based timbre learning methodProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 253Ea=Em 13 dB 10 dB 0 dB -10 dB -13 dB\nCosine 96.10 96.10 96.14 96.00 96.00\nEuclidean 77.81 88.62 95.12 78.39 60.62\nSKL 95.73 95.86 95.41 96.85 96.75\nTable 2 . Comparison of precision (in %) using three types\nof distance functions for DTW with ﬁve different levels of\nenergy ratios of audio recording to reference signal.\nProcess Precision\nBaseline 96.14\nBaseline+NT 97.03\nBaseline+NT+SA 97.64\nBaseline+NT+SA+RS 97.88\nBaseline+NT+SA+RS+RRN (‘Proposed’) 98.43\nTable 3 . Performance (in %) of the baseline and the pro-\nposed system. NT: noise template; SA: second stage align-\nment; RS: reﬁnement of silence; RRN: reﬁnement of re-\npeated notes.\nProcess NT SA RS RRN Other\n# Notes 45 1094 173 1015 598\n# Baseline 15 42 6 40 10\nErrors Proposed 2 26 1 10 7\nTable 4 . Comparison of the number of error notes between\nthe baseline and the proposed system based on the four\nraised issues.\nby Joder et al. [9]. To simulate the silence parts, we sim-\nply apply zero values for silence segments, which is the\nsame means used in the MIDI synthesizer. Results in Table\n1 also shows that using NMF for timbre synthesis yields\nmuch better precision (95.12%) than using a MIDI synthe-\nsizer (81.37%), since a common MIDI synthesizer can not\nwell resemble the wide variety of timbre in expressive vi-\nolin performance. We therefore take the NMF-based syn-\nthesis method for the following experiments.\nMoreover, since the dynamics of notes vary largely in\nexpressive violin performance, the distance functions in\nDTW and the frame-level energy are also essential fac-\ntors in the alignment process. We compare three types of\ndistance functions in the DTW algorithm: cosine similar-\nity [11], Euclidean distance [5], and symmetric Kullback-\nLeibler (SKL) divergence [9]. Cosine similarity is the\nnormalized inner product of two non-zero vectors. Since\nwe would like to ﬁnd the minimal value of the cost func-\ntion, the inner product is subtracted by 1. Besides, Eu-\nclidean distance calculates the straight-line distance of two\nfeature vectors. Further, SKL divergence is deﬁned as:\ndSKL(i;j) =dKL(ikj) +dKL(jki), whereiandjare\ntwon-dimensional vectors. The performance of a distance\nfunction is highly related to the effect of energy ratios , i.e.\nthe ratio of the energy levels of the audio recording ( Ea)to the reference signal ( Em). Table 2 presents the aver-\naged precision values using the three distance functions\nfor DTW with ﬁve different levels of energy ratio from -\n13 dB to 13 dB. All the three distance functions have sim-\nilar performance when audio and reference signals have\nsimilar levels of energy. However, when the energy ratio\nexceeds 10 dB, performance degrades signiﬁcantly for Eu-\nclidean distance, while the cosine similarity turns out to be\nthe most stable distance function among all levels of en-\nergy ratio (STD =0.06%). Therefore, we opt to use cosine\nsimilarity in the following experiments.\nIn short, the optimal setting of the conventional DTW-\nbased alignment algorithm (i.e. the baseline ) encompasses\nlinear-log spectral features, the reference signal synthe-\nsized with NMF on the input signal, and cosine similarity\nas a distance function. We will use these settings in the\nfollowing experiments if not mentioned.\n3.2 System Experiment\n3.2.1 Overview\nTable 3 lists the performance of the proposed system and\na comparison to the individual building blocks mentioned\nin Section 2, i.e. one-stage DTW only (Baseline), noise\ntemplate (NT), second-stage alignment (SA), reﬁnement of\nsilence (RS), and reﬁnement of repeated notes (RRN). Re-\nsults show that the baseline achieves a precision of 96.14%.\nIts performance is then increased by 0.89% after adding\nNT. A further improvement of 0.61% is seen after adding\nSA and addressing the issue of overlapped sustain notes.\nFinally, the RS and RRN also give a slight improvement of\n0.24% and 0.55% subsequently. As a result, the averaged\nprecision of the proposed system comes to 98.43%, show-\ning a signiﬁcant improvement from the baseline system as\nvalidated by a two-tailed t-test ( p<0.05, d.f.=98).\nTable 4 gives a more in-depth comparison of the num-\nber of error notes between the baseline and the proposed\nsystem according to the four raised issues.4We ﬁnd that\nevery process reduces the number of error notes of their\ncorresponding types, and the RRN process leads to the\ngreatest improvement: 40 error notes within repeated note\nsequences are reduced to 10 notes.\nTable 5 shows the precision of the proposed system\nwithin different levels of error tolerance values not only\nat 100ms but ranged from 20ms (1 frame) to 700ms (35\nframes). We ﬁnd that the performance is over 90% when\nusing the tolerance with 60ms (3 frames). In addition, the\nmaximal erroneous time of onset is within 700ms.\n3.2.2 Expression-wise Performance\nTable 6 presents the averaged precision for the 10 violin ex-\npressions based on the Baseline process with two distinct\ntimbre synthesis methods, MIDI synthesizer and NMF, and\nthe proposed system, respectively. Comparing the MIDI\n4An NT refers to a note which follows a rest symbol; SA counts the\nnote that overlaps its successive notes over 60ms; RS includes the notes\nplayed with staccato ; RRN contains the notes belonging to a sequence of\nrepeated notes; the remaining ones are marked as‘Other’.254 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Error\u0014ProposedFrames Seconds\n1 0.02 58.60\n2 0.04 85.61\n3 0.06 94.84\n5 0.1 98.43\n10 0.2 99.62\n15 0.3 99.79\n20 0.4 99.83\n25 0.5 99.93\n30 0.6 99.97\n35 0.7 100.00\nTable 5 . Performance (in %) of the proposed system\nwithin different levels of error tolerance values.\nExpressionBaselineProposedMIDI NMF\nScherzando 78.25 96.37 96.98\nTranquillo 69.65 93.06 98.55\nCon Brio 87.19 98.44 98.75\nMaestoso 82.73 97.48 98.56\nRisoluto 76.86 96.92 99.49\nAffettuoso 87.41 96.67 100.00\nAgitato 88.78 97.96 96.94\nCantabile 86.43 93.97 95.98\nGrazioso 86.44 95.25 99.66\nEspressivo 78.07 95.35 98.00\nTable 6 . Performance (in %) of the ten violin expressions\nvia the baseline process with two distinct timbre synthesis\nmethods and the proposed system.\nsynthesizer (i.e. the second column) to NMF-based synthe-\nsis from audio recording (i.e. the third column), a signif-\nicant improvement can be observed ( p < 0.005, d.f. =18),\nespecially for Tranquillo andRisoluto , where the improve-\nment is over 20% for both cases. Besides, the proposed\nsystem (i.e. the fourth column) has signiﬁcant improve-\nment from both the Baseline cases ( p < 0.05, d.f. =18).\nParticularly, Tranquillo ,Grazioso , and Affettuoso are im-\nproved the most; this implies that the proposed system can\nenhance the onset precision for such violin performance\nwith plentiful expression and intense vibrato. For Risoluto ,\nthe expression played with staccato technique mostly, the\nproposed system also gives excellent result. Furthermore,\nwe see that the improvement of Con Brio andScherzando\nis limited, probably due to their intense characteristics of\nperformance such as clear attack of energy envelope.\n3.3 Discussion\nAccording to the expression-wise performance as illus-\ntrated in Table 6, we ﬁnd that Agitato is the only one ex-\npression which has degraded precision via the proposed\nsystem. The reason is possibly that the energy of sustain\nsegment might be weak such that the simulation of sustainperhaps cause additional errors. Except for Agitato , the\nproposed system has improvement for other expressions.\nAlthough we use a reﬁnement process to deal with the\nunexpected silence segments caused by the staccato tech-\nnique, this process actually could be merged into the two-\nstage alignment. For example, we can adopt similar means\nwhich is used in the simulation of overlapped sustain notes,\nby inserting additional frames in a reference signal based\non the information of silence segments. Thereby, the sys-\ntem will be made more succinct.\nIn this paper, we only consider a subset of the vio-\nlin expression dataset, which includes 50 solo recordings\nfrom randomly selected 3 musicians’ performance. In or-\nder to obtain more reliable performance and to develop\na robust alignment system, the test data needs to be ex-\npanded such as using the recordings from other musicians\nin the SCREAM-MAC-EMT dataset as well as data of\npolyphonic recordings, where the latter suggest a future\nwork of constructing a new dataset for expression analysis\nof violin solo in polyphonic music.\nMoreover, this study only considers the accuracy of\nonset-only alignment. Another important task for music\nexpression analysis of notes is offset alignment, which is\nstill a challenging problem. An extension of the proposed\nalignment system such as to cover the offset alignment is-\nsue is also left as future work.\n4. CONCLUSION\nTo have better expression analysis of violin recordings, it is\ndesired to have the precise onset information of each note.\nThe conventional DTW algorithm is modiﬁed for accurate\naudio-to-score alignment for the violin dataset, including\nthe simulation of sustain notes, silence detection, reﬁne-\nment of duration ratio of repeated notes, and background\nnoise model, which are used to deal with the four common\nissues usually seen in violin recordings. Experiments show\nthat high precision is achieved if instrumental timbre and\nthe 84-dimensional spectral feature vector are used. Co-\nsine similarity is adopted as our distance formula for its\nrobustness to various violin playing techniques. The pro-\nposed two-stage alignment system obtains signiﬁcant im-\nprovement, not only for the addressed issues but also for\nthe distinct expressions, from the baseline process.\n5. ACKNOWLEDGEMENTS\nThe authors would like to thank the Ministry of Science\nand Technology of Taiwan for its ﬁnancial support of this\nwork, under contract MOST 105-2221-E-006-148.\n6. REFERENCES\n[1] M. A. Bartsch and G. H. Wakeﬁeld. Audio thumbnail-\ning of popular music using chroma-based representa-\ntions. IEEE Transactions on multimedia , pages 96–\n104, 2005.\n[2] B. Catteau, J. P. Martens, and M. Leman. A proba-\nbilistic framework for audio-based tonal key and chordProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 255recognition. Advances in Data Analysis , pages 637–\n644, 2007.\n[3] A. Cont. Realtime audio to score alignment for poly-\nphonic music instruments, using sparse non-negative\nconstraints and hierarchical HMMs. In ICASSP , pages\n245–248, 2006.\n[4] R. B. Dannenberg and C. Raphael. Music score align-\nment and computer accompaniment. Communications\nof the ACM , pages 38–43, 2006.\n[5] S. Dixon. Live tracking of musical performances using\non-line time warping. In DAFx , pages 92–97, 2005.\n[6] Z. Duan and B. Pardo. A state space model for online\npolyphonic audio-score alignment. In ICASSP , pages\n197–200, 2011.\n[7] N. Hu, R. B. Dannenberg, and G. Tzanetakis. Poly-\nphonic audio matching and alignment for music re-\ntrieval. In WASPAA , pages 185–188, 2003.\n[8] C. Joder, S. Essid, and G. Richard. Optimizing the\nmapping from a symbolic to an audio representation\nfor music-to-score alignment. In WASPAA , pages 121–\n124, 2011.\n[9] C. Joder and B. Schuller. Off-line reﬁnement of audio-\nto-score alignment by observation template adaptation.\nInICASSP , pages 206–210, 2013.\n[10] P.-C. Li, L. Su, Y .-H. Yang, and A. W. Y . Su. Analy-\nsis of expressive musical terms in violin using score-\ninformed and expression-based audio features. In IS-\nMIR, pages 809–815, 2015.\n[11] R. Macrae and S. Dixon. Accurate real-time windowed\ntime warping. In ISMIR , pages 423–428, 2010.\n[12] B. Niedermayer and G. Widmer. A multi-pass algo-\nrithm for accurate audio-to-score alignment. In ISMIR ,\npages 417–422, 2010.\n[13] N. Orio and F. D ´echelle. Score following using spectral\nanalysis and hidden markov models. In ICMC , pages\n151–154, 2001.\n[14] C. Raffel and D. P. W. Ellis. Optimizing DTW-based\naudio-to-midi alignment and matching. In ICASSP ,\npages 81–85, 2016.\n[15] C. Raphael. A hybrid graphical model for aligning\npolyphonic audio with musical scores. In ISMIR , pages\n387–394, 2004.\n[16] C. Raphael. Aligning music audio with symbolic\nscores using a hybrid graphical model. Machine learn-\ning, pages 389–409, 2006.\n[17] T. Virtanen. Monaural sound source separation by non-\nnegative matrix factorization with temporal continuity\nand sparseness criteria. IEEE Transactions on Audio,\nSpeech, and Language Processing , pages 1066–1074,\n2007.[18] S. Wang, S. Ewert, and S. Dixon. Robust and ef-\nﬁcient joint alignment of multiple musical perfor-\nmances. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing , pages 2132–2145, 2016.\n[19] T.-M. Wang, P.-Y . Tsai, and A. W. Y . Su. Note-based\nalignment using score-driven non-negative matrix fac-\ntorisation for audio recordings. IET Signal Processing ,\npages 1–9, 2014.256 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Generating Nontrivial Melodies for Music as a Service.",
        "author": [
            "Yifei Teng",
            "Anny Zhao",
            "Camille Goudeseune"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416336",
        "url": "https://doi.org/10.5281/zenodo.1416336",
        "ee": "https://zenodo.org/records/1416336/files/TengZG17.pdf",
        "abstract": "We present a hybrid neural network and rule-based sys- tem that generates pop music. Music produced by pure rule-based systems often sounds mechanical. Music pro- duced by machine learning sounds better, but still lacks hierarchical temporal structure. We restore temporal hi- erarchy by augmenting machine learning with a temporal production grammar, which generates the music’s overall structure and chord progressions. A compatible melody is then generated by a conditional variational recurrent au- toencoder. The autoencoder is trained with eight-measure seg- ments from a corpus of 10,000 MIDI files, each of which has had its melody track and chord progressions identified heuristically. The autoencoder maps melody into a multi-dimensional feature space, conditioned by the underlying chord pro- gression. A melody is then generated by feeding a random sample from that space to the autoencoder’s decoder, along with the chord progression generated by the grammar. The autoencoder can make musically plausible variations on an existing melody, suitable for recurring motifs. It can also reharmonize a melody to a new chord progression, keeping the rhythm and contour. The generated music compares favorably with that gen- erated by other academic and commercial software de- signed for the music-as-a-service industry.",
        "zenodo_id": 1416336,
        "dblp_key": "conf/ismir/TengZG17",
        "keywords": [
            "hybrid neural network",
            "rule-based system",
            "pop music",
            "temporal hierarchy",
            "machine learning",
            "temporal production grammar",
            "melody generation",
            "conditional variational recurrent autoencoder",
            "melodic motifs",
            "reharmonization"
        ],
        "content": "GENERATING NONTRIVIAL MELODIES FOR MUSIC AS A SERVICE\nYifei Teng\nU. of Illinois, Dept. of ECE\nteng9@illinois.eduAnny Zhao\nU. of Illinois, Dept. of ECE\nanzhao2@illinois.eduCamille Goudeseune\nU. of Illinois, Beckman Inst.\ncog@illinois.edu\nABSTRACT\nWe present a hybrid neural network and rule-based sys-\ntem that generates pop music. Music produced by pure\nrule-based systems often sounds mechanical. Music pro-\nduced by machine learning sounds better, but still lacks\nhierarchical temporal structure. We restore temporal hi-\nerarchy by augmenting machine learning with a temporal\nproduction grammar, which generates the music’s overall\nstructure and chord progressions. A compatible melody is\nthen generated by a conditional variational recurrent au-\ntoencoder.\nThe autoencoder is trained with eight-measure seg-\nments from a corpus of 10,000 MIDI ﬁles, each of which\nhas had its melody track and chord progressions identiﬁed\nheuristically.\nThe autoencoder maps melody into a multi-dimensional\nfeature space, conditioned by the underlying chord pro-\ngression. A melody is then generated by feeding a random\nsample from that space to the autoencoder’s decoder, along\nwith the chord progression generated by the grammar. The\nautoencoder can make musically plausible variations on an\nexisting melody, suitable for recurring motifs. It can also\nreharmonize a melody to a new chord progression, keeping\nthe rhythm and contour.\nThe generated music compares favorably with that gen-\nerated by other academic and commercial software de-\nsigned for the music-as-a-service industry.\n1. INTRODUCTION\nComputer-generated music has started to expand from its\npure artistic and academic roots into commerce. Compa-\nnies such as Jukedeck and Amper offer so-called music as\na service, by analogy with software as a service. However,\ntheir melodies, when present at all, often just arpeggiate\nthe underlying chord.\nWe extend this approach by generating music with both\nchord progressions and interesting, nontrivial melodies.\nWe expand a song structure such as AA0BA into a har-\nmonic plan, and then add a melody compatible with this\nstructure and harmony. This compatibility uses a chord-\nc\rYifei Teng, Anny Zhao, Camille Goudeseune. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC\nBY 4.0). Attribution: Yifei Teng, Anny Zhao, Camille Goudeseune.\n“Generating Nontrivial Melodies for Music as a Service”, 18th Inter-\nnational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.ML trainingCrawled MIDI ﬁles\nHarmonic\nanalysis\nLearned modelMelody Chords\nML generation Chord grammar\nGenerated melody\nFigure 1 : Machine learning (ML) workﬂow for generating\nmusic from a MIDI corpus.\nmelody relationship found by applying machine learning\nto a corpus of MIDI transcriptions of pop music (Figure 1).\nPrior research is discussed in section 2. Harmonic anal-\nysis is detailed in sections 3 and 4. Hierarchy generation\nand melody generation are described in section 5.\n2. RELATED WORK\nRecent approaches to machine composition use neural net-\nworks (NNs), hoping to approximate how humans com-\npose. Chu et al [5] generate a melody with a hierarchical\nNN that encodes a composition strategy for pop music, and\nthen accompany the melody with chords and percussion.\nHowever, this music lacks hierarchical temporal structure.\nBoulanger-Lewandowski et al [3] investigate hierarchical\ntemporal dependencies and long-term polyphonic struc-\nture. Inspired by how an opening theme often recurs at\na song’s end, they detect patterns with a recurrent tempo-\nral restricted Boltzmann machine (RTRBM). This can rep-\nresent more complicated temporal distributions of notes.\nSimilarly, Huang and Wu [10] generate structured music\nwith a 2-layer Long Short Term Memory (LSTM) net-\nwork. Although the resulting music often sounds plau-\nsible, it cannot produce clearly repeated melodic themes,\njust like a Markov resynthesis of the text of the famous\npoem “Jabberwocky” is unlikely to replicate the identical\nopening and closing stanzas of the original. Despite the\nLSTM network’s theoretical capability of long-term mem-\nory, it fails to generalize to arbitrary time lengths [8], and\nits generated melodies remain unimaginative.657In these approaches, tonic chords dominate, and melody\nis little more than arpeggiation. To avoid this banality, we\nwork in reverse. We ﬁrst create structure and chords, and\nthen ﬁt melody to that. This mimics how classical west-\nern Roman-numeral harmony is taught to beginners: only\nafter one has the underlying chord sequence, can one ex-\nplain the melody in terms of chord tones, passing tones,\nappoggiaturas, and so on.\n3. MELODY IDENTIFICATION\nFor pop music, a catchy and memorable melody is crucial.\nTo generate melodies that sound less robotic than those\ngenerated by other algorithms, we use machine learning.\nTo create a learning database, we started with a corpus of\n10,000 MIDI ﬁles [16], from which we extracted useful\ntraining data (melodies that sound vivid or fun). In partic-\nular, the training data was eight-measure excerpts labelled\nas melody and chords. We thus had to identify which of a\nMIDI ﬁle’s several tracks contained the melody. To do so,\nwe assigned each track the sum of a rubric score and an en-\ntropy score. Whichever track scored highest was declared\nto be the melody. (Ties between high-scoring tracks were\nbroken arbitrarily, because they were usually due to sev-\neral tracks having identical notes, differing only in which\ninstrument played them.)\n3.1 Rubric Score\nOur rubric considered attributes such as instrumentation,\nnote density, and pitch range.\nWe ﬁrst considered a track’s instrument name (MIDI\nmeta-event FF 04 ). Certain instruments are more com-\nmon for melody, such as violin or ﬂute. Others are more\nlikely to be applied as accompaniment or long sustained\nnotes, such as low brass. A third category is likely used\nas unpitched percussion. The instrument’s category then\nadjusted the rubric’s score.\nWe also considered the track’s note density, how often\nat least one note is sounding (between corresponding MIDI\nnote-on and note-off events), as a fraction of the track’s full\nduration. A track scored higher if this was between 0.4 and\n0.8, a typical value for pop melodies.\nFinally we considered pitch range, because we observed\nthat pop melodies often lie between C3 and C5. The score\nwas higher for a pitch range between C3 and C6, to exclude\nbass tracks from consideration.\nThe values for these attributes were chosen based on\nmanual inspection of 100 ﬁles in the corpus.\n3.2 Entropy Score\nWe empirically observed that melody tracks often have a\ngreater variety of pitches than other tracks. Thus, to quan-\ntify how varied, complex, and dynamic a track was, we\ncalculated each track’s entropy\nH(X) =\u0000X 12\ni=1P(xi) logP(xi) (1)where xirepresents the event that a particular note in the\noctave is isemitones from the pitch C, and P(xi)repre-\nsents that event’s probability. Higher entropy corresponds\nto a greater number of distinct pitches.\n3.3 Evaluation\nTo measure how well this scoring identiﬁed melody tracks,\nwe manually tagged the melody track of 160 randomly se-\nlected MIDI ﬁles. Comparing the scored prediction to this\nground truth showed that the error rate was 15%.\n4. CHORD DETECTION\nTo identify the chords in a MIDI ﬁle, we considered three\naspects of how pop music differs from genres like classi-\ncal music. First, chord inversions (where the lowest note\nis not the chord’s root) are rare. When a new chord is pre-\nsented, it is often in root position: most pop songs have\na clear melody line and bass line [14], and the onset of a\nnew chord is marked with the chord’s root in that bass line.\nSecond, chords may contain extensions (seventh), substitu-\ntions (ﬂattened ﬁfth), doublings, drop voicings (changing\nwhich octave a pitch sounds in), and omissions (third or\nﬁfth). Although such modiﬁcations complicate the task of\nfunctional harmony analysis, this is not a concern for our\napplication. Third, new chord onsets are often at the start\nof a measure; rarely are there more than two chords per\nmeasure. Combining these observations led us to the fol-\nlowing chord detection algorithm.\nWe ﬁrst partition the song into segments with constant\ntime signatures. (these are explicitly stated as MIDI meta\nmessages). Then each segment is evenly divided into bins,\nwhere we try to match the entire bin to a chord. Be-\ncause chords have different durations, we try different bin\nlengths: half a measure, one measure, and two measures.\nThen for each bin, containing all the notes sounding dur-\ning that time interval, we add all these notes to a set that is\nmatched against a ﬁxed collection of chords, based on how\nclose the pitches are, with a cost function:\nChord Detection: COST\n1:function BESTCHORD INBIN(Pitches )\n2: Root Lowest note starting before ﬁrst upbeat\n3: Chords All chords, as array of intervals\n4:return argminC2ChordsfCOST(Pitches; C; Root )g\n5:function COST(Pitches; Chord; Root )\n6: PitchCost 0\n7: forP2Pitches do\n8: interval No. semitones of PfromRoot\n9: d minvoice2Chordfdist(interval; voice )g\n10: PitchCost PitchCost +d\n11: ChordCost 0\n12: forvoice2Chord do\n13: d minP2Pitchesfdist(P\u0000Root; voice )g\n14: ChordCost ChordCost +d\n15:return PitchCost +ChordCost658 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Distance in semitones Compatibility distance\n0 0\n1 6\n2 2\n3 2\n4 2\n5 1\n6 4\n7 1\nTable 1 : Interval compatibility.\nC A7/aug/b9 Dm Dm744\n44\nGsus2 G13 C C7add9\nFigure 2 : Example of chord detection.\nEach chord’s cost is the sum of the distance of the near-\nest interval in the chord (from the root) to each interval\nin the input pitches, and the distance of the nearest inter-\nval in the input pitches (from its “root”) to each interval in\nthe chord, based on some deﬁnition of distance. The cost\nfunction then returns the lowest-cost chord.\nDeﬁning the distance in terms of mere pitch difference\nin semitones would be simple, but performs poorly. For\nexample, matching the pitch set [C; E; G ]to the chord\n[C; E[; G[ ]would yield a cost of two, which is far too\nlow. Instead, our distance function reﬂects how compat-\nible intervals are. The unison is the most compatible, with\ndistance zero; fourths and ﬁfths are next, with distance one\n(Table 1). This conveniently handles omitted-ﬁfth chords,\nbecause the chord’s root matches the omitted ﬁfth with a\ndistance of only one.\nFigure 2 demonstrates chord detection on the song Fly\nme to the moon . The bin size is half a measure, yielding 8\nidentiﬁed chords. The A7=aug=[ 9chord resulted from the\naccompaniment notes A; G; B[ (ﬂat ninth ); C]; E , and the\nmelody notes A; G; F (augmented ﬁfth ).\n5. WRITING MUSIC\nTo output pieces with audibly hierarchical structure, we\nstart with the harmonic structure produced by a temporal\ngenerative grammar. Then an autoencoder recurrent neu-\nral network (RNN) generates a melody compatible with\nthis harmonic scaffold. The RNN learns to play using the\nchord’s notes, with occasional surprising non-chord tone\ndecorations such as passing tones and appoggiaturas.5.1 Generating Melody\nWe ﬁrst search for a representation of the melody using\nML. This is traditionally done by an autoencoder, a pair of\nNNs that maps high-dimensional input data to and from a\nlower-dimensional space. Although this dimensionality re-\nduction can eliminate perfect mappings, this turns out not\nto be a problem because the subspace of “pleasant” music\nwithin all possible musics is sufﬁciently small. Thus, the\nautoencoder can extract the pleasant content and map only\nthat into the representation space.\nIt is tempting to feed a random point from the repre-\nsentation space to the autoencoder’s decoder, and observe\nhow much sense it makes of that point. However, because\none cannot control the shape of the distribution of melody\nrepresentations, one cannot guarantee that a given point\nfrom the representation space would be similar to those\nseen by the decoder during training. Thus, the vanilla\nautoencoder architecture [2] is not viable as a generative\nmodel. We propose the following improvements for gen-\nerating melodies:\n1.Condition the NN on the chord progression. The\nchord progression is provided to the NN at every\nlevel, so when reproducing a melody, the decoder\nhas access to both the representation and the chord\nprogression. This is useful because a melody has\nrhythmic information, intervallic content, and con-\ntour. The decoder can ignore the separately provided\nharmonic information, and use only the melody’s\nother aspects. This also lets the representation\nremain constant while altering the chord progres-\nsion, so the NN can adapt a melody to a changed\nchord progression, such as what happens when a key\nchanges from minor to major.\n2.Add a stochastic layer. Autoencoders which learn\na stochastic representation are called variational au-\ntoencoders, and perform well in generative mod-\nelling of images [11]. The representation is not de-\nterministic. We assume a particular (Gaussian) dis-\ntribution in the representation space, and then train\nthe NN to transform this distribution to match the\ndistribution of input melodies in their high dimen-\nsional space. This ensures that we can take a ran-\ndom sampling of the representation space following\nits associated probability distribution, then feed it\nthrough the decoder and expect a melody similar to\nthe set of musically sensible melodies.\n3.Use recurrent connections. Pop music has many\ntime-invariant elements, especially at time scales be-\nlow a few beats. A recurrent NN shares the same\nprocessing infrastructure for note sequences starting\nat different times, and thereby accelerates learning.\n4.Normalize all other notes relative to the tonic. Pop\nmusic is also largely pitch invariant, insofar as a song\ntransposed by a few semitones still sounds perceptu-\nally similar. The NN ignores the song’s key and con-\nsiders the tonic pitch to be abstract, as far as pitches\nin melody and chords are concerned.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 65916x8\n>>>><\n>>>>:-16 . . . 16 Silent Attack\u001b\nx8...............\nI . . . VI VII Silent\nPwr Maj Min Dim Aug\n...............\nMajor Dorian . . . Locrian Jazz Minor\nTable 2 : An encoding of 8 measures (see section 5.1.1).\n5.1.1 Implementation\nThe input melody is quantized to sixteenth notes. Only\nsections with an unchanging duple or quadruple meter are\nkept. The melody is converted to a series of one-hot vec-\ntors, whose slots represent offsets from the tonic in the\nrange of\u000016to16semitones, with one more slot repre-\nsenting silence. There is also an attack channel, where\na value of 1 indicates that the note is being rearticulated\nat the current time step. The encoding for chords sup-\nports up to two chords per measure, and uses a one-hot\nvector for scale degrees and separate boolean channels for\nchord qualities (Table 2). (Note that because this encod-\ning uses just seven Roman-numeral symbols, it does not\ntry to represent chords outside the current mode. Before\ntraining, we removed from the corpus the few songs that\ncontained signiﬁcant occurrences of this.) We use the ba-\nsic triad form for each chord identiﬁed using techniques\nfrom section 4, marking compatible chord qualities. For\nexample, G7is encoded by marking a 1 in the Maj and\nPwr columns. (The chord quality encoding could be ex-\ntended to seventh and ninth chords.) The table’s gray rows\nare data the network is conditioned on, while the other\nrows are input data that the network tries to reproduce. For\nan 8-measure example, the input and output vector size is\n35\u00028\u000216 = 4480 , and the conditional vector size is\n8\u000216 + 5\u000216 + 8 = 216 .\nThe network has 24 recurrent layers, 12 each for the en-\ncoder and decoder (Figure 3). Drawing on ideas of deep\nresidual learning from computer vision [9], we make ad-\nditional connections from the input to every third hidden\nlayer. To improve learning, the network accesses both the\noriginal melody and the transformed results from previous\nlayers during processing. The conditional part (chords and\nmode) is also provided to the network at every recurrent\nlayer, as extra incoming connections.\nThe network is implemented in Tensorﬂow, a machine\nlearning library for rapid prototyping and production train-\ning [1]. It was trained for four days on an Nvidia Tesla K80\nGPU. We used Gated Recurrent Units [4] to build the bidi-\nrectional recurrent layer and Exponential Linear Units [7]\nas activation functions. These signiﬁcantly accelerate\ntraining while simplifying the network [6, 7]. Figure 4\nshows the training error (the sum of model reproduction\nerrors) and the difference of the latent distribution from\na unit Gaussian distribution, as measured by Kullback-\nLeibler divergence [12]. The network’s input data (avail-\nable at https://goo.gl/VezNNA ) is a set of MIDIInput vector\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unitchords\nand\nmode\n8x300 to 1x1200\n800 FC 800 FC\nLatent distribution\n600 FC\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unit\n600 hidden unitchords\nand\nmode\n128x300 to 128x35\nOutput vector600\n600\n300\n600\n600\n300\n600\n600\n300\n600\n600\n2400\n800 800\nMeanStandard Deviation\nSampling\n600\n600\n300\n600\n600\n300\n600\n600\n300\n600\n600\n300\nFigure 3 : Network architecture. Rectangles are bidirec-\ntional recurrent neural network cells. Ellipses are strided\ntime-convolution cells. Rounded rectangles are fully con-\nnected (FC) layers. Numbered arrows indicate a connec-\ntion’s dimension.660 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170 2 4 6\n\u0001105200400600Reproduction loss\nKL divergence\nFigure 4 : Training error and Kullback-Leibler divergence\nof the NN. The horizontal axis indicates how many training\nsegments have elapsed ( \u0002105). Initial outliers have been\nremoved.\nsongs from various online sources. Our harmonic analy-\nsis converted this to 1:9\u0002106measures of melodies and\ncorresponding chords. We implemented KL warm up, be-\ncause that is crucial to learning for a variational autoen-\ncoder [15]. But instead of linearly scaling the KL term for\nthis, we found that a sigmoid reduced the network’s repro-\nduction loss.\n5.2 Generating Hierarchy and Chords\nHierarchy and chords are generated simultaneously, using\na temporal generative grammar [13], modiﬁed to suit the\nharmonies of pop music, and extended to enable repeated\nmotifs with variations. The original temporal generative\ngrammar has notions of sharing by binding a section to a\nsymbol. For example, the rule\nletx=I in IM5(x)IM5(x)I; (2)\nwhere M5indicates modulating to the 5thdegree, would\nexpand to ﬁve sections, with the second and fourth identi-\ncal because xis reused. We extend this by having symbols\nxcarry along a number: x1; x2; :::. Different subscripts of\nthe same symbol still expand to the same chord progres-\nsion, but denote slightly different latent representations\nwhen generating corresponding melodies for those sec-\ntions. The latent representations corresponding to xi>1are\nderived from that of x1by adding random Gaussian pertur-\nbations. This yields variations on the original melody.\n5.3 Training Examples in the Representation Space\nWe randomly chose 130 songs from the training set, fed\nthem through the network, and performed t-SNE analy-\nsis on the resulting 130 locations in the representation\nspace. Although a melody maps to a distribution in\nthe representation space, Figure 5 plots only each dis-\ntribution’s mean, for visual clarity. This t-SNE analy-\nsis effectively reduces the 800-dimensional representation\nFigure 5 : Example melodies in a t-SNE plot of the repre-\nsentation space.\n44\n44\nFigure 6 : Four-bar excerpts from the songs Indica (top)\nandControl (bottom).\nspace into a low-dimensional human-readable format [17].\n(A larger interactive visualization of 1,680 songs is at\nhttps://composing.ai/tsne .)\nTwo songs that are both in the techno genre, Indica by\nJushi and Control by Junk Project, are indeed very near in\nthe t-SNE plot, almost overlapping. Excerpts from them\nshow that both have a staccato rhythm with notes landing\non the upbeat, and have similar contours (Figure 6).\n5.4 Reharmonizing Melody\nWe hypothesized that, when building the neural network\narchitecture, providing the chord progression to both the\nencoder and the decoder would not preserve that infor-\nmation in the representation space, thus saving space for\nrhythmic nuances and contour. To test this hypothesis,\nwe gave the network songs disjoint from the training set\nand collected their representations. We then fed these rep-\nresentations along with a new chord progression to the\nnetwork. We hoped that it would respond by generat-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 661C C44\nC C C\nF C G C\nCm Cm Cm44\nCm Cm\nFm Gm CmFigure 7 : The song Jasmine Flower with original chords\n(top), and adapted to a new chord progression (bottom).\ning a melody that was harmonically compatible with the\nnew chord progression, while still resembling the origi-\nnal melody. We demonstrate this with the Chinese folk\nsong Jasmine Flower , in a genre unfamiliar to the NN\n(Figure 7). Note that we supplied the chords in Figure 7\n(bottom), for which the NN ﬁlled in the melody. The\nnetwork ﬂattened the E, A, and B, by observing that the\nchord progression looked minor. This is typically how a\nhuman would perform the reharmonization, demonstrating\nthe network’s comprehension of how melody and harmony\ninteract.\nAlthough the NN struggled to reproduce the melody,\nit provided interesting modiﬁcations. The grace notes in\nmeasure 6 could be due to similar ones in the training set,\nor due to vacillation between the A \\from the representa-\ntion and the A [from the chord conditioning.\n5.5 Examples of Generated Melodies\nBecause an entire multi-section composition cannot ﬁt\nhere, we merely show excerpts from two shorter examples.\nFigure 8 and Figure 9 demonstrate melodies generated\nfrom points in the representation space that are not near any\nparticular previously known melody. Structure is evident\nin Figure 8: measures 1–3 present a short phrase, and mea-\nsure 4 leads to the next four measures, which recapitulate\nthe ﬁrst three measures with elaborate variation. Figure 9\nshows an energetic melody where the grammar only pro-\nduced C minor chords. Although the ﬁnal two measures\nwander off, the ﬁrst six have consistent style and natural\ncontour.\nG G C44\nC G\nG C CFigure 8 : Generated melody for a grammar-generated\nchord progression.\n44\nFigure 9 : Generated melody for an extended C minor\nchord.\n6. CONCLUSION AND FUTURE WORK\nWe have combined generative grammars for structure and\nharmony with a NN, trained on a large corpus, to emit\nmelodies compatible with a given chord progression. This\nsystem generates compositions in a pop music style whose\nmelody, harmony, motivic development, and hierarchical\nstructure all ﬁt the genre.\nThis system is currently limited by assuming that the in-\nput data’s chords are in root position. More sophisticated\nchord detection would still let it exploit the relative har-\nmonic rigidity of popular music. Also, by investigating\nthe representation found by the NN, meaning could be as-\nsigned to some of its 800 dimensions, such as intensity,\nconsonance, and contour. This would let us boost or atten-\nuate a given melody along those dimensions.\nAcknowledgements. The authors are grateful to Mark\nHasegawa-Johnson for overall guidance, and to the anony-\nmous reviewers for insights into both philosophical issues\nand minute details.\n7. REFERENCES\n[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,\nJ. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,\nM. Kudlur, J. Levenberg, R. Monga, S. Moore, D. Mur-\nray, B. Steiner, P. Tucker, V . Vasudevan, P. Warden,\nM. Wicke, Y . Yu, and X. Zheng. Tensorﬂow: a sys-\ntem for large-scale machine learning. In Proc. USENIX662 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Conf. Operating Systems Design and Implementation ,\npages 265–283. USENIX Association, 2016.\n[2] Bengio. Learning deep architectures for AI. Founda-\ntions and Trends in Machine Learning , 2(1):1–127,\n2009.\n[3] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent. Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic\nmusic generation and transcription. arxiv.org/\nabs/1206.6392 , 2012.\n[4] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bah-\ndanau, F. Bougares, H. Schwenk, and Y . Bengio.\nLearning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arxiv.\norg/abs/1406.1078 , 2014.\n[5] H. Chu, R. Urtasun, and S. Fidler. Song from PI: a\nmusically plausible network for pop music generation.\narxiv.org/abs/1611.03477 , 2016.\n[6] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empiri-\ncal evaluation of gated recurrent neural networks on se-\nquence modeling. arxiv.org/abs/1412.3555 ,\n2014.\n[7] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and\naccurate deep network learning by exponential lin-\near units (elus). arxiv.org/abs/1511.07289 ,\n2015.\n[8] A. Graves, G. Wayne, and I. Danihelka. Neural turing\nmachines. arxiv.org/abs/1410.5401 , 2014.\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\nlearning for image recognition. arxiv.org/abs/\n1512.03385 , 2015.\n[10] A. Huang and R. Wu. Deep learning for music.\narxiv.org/abs/1606.04930 , 2016.\n[11] D. P. Kingma and M. Welling. Auto-encoding varia-\ntional bayes. arxiv.org/abs/1312.6114 , 2013.\n[12] S. Kullback and R. A. Leibler. On information and suf-\nﬁciency. Annals of Mathematical Statistics , 22(1):79–\n86, 1951.\n[13] D. Quick and P. Hudak. A temporal generative graph\ngrammar for harmonic and metrical structure. In Proc.\nInternational Computer Music Conference , pages 177–\n184, 2013.\n[14] M. P. Ryyn ¨anen and A. P. Klapuri. Automatic tran-\nscription of melody, bass line, and chords in poly-\nphonic music. Computer Music Journal , 32(3):72–86,\n2008.\n[15] C. K. Sønderby, T. Raiko, L. Maaløe, S. K. Sønderby,\nand O. Winther. Ladder variational autoencoders.\narxiv.org/abs/1602.02282 , 2016.[16] Y . Teng and A. Zhao. Composing.AI. http://\ncomposing.ai/ , 2017.\n[17] L. van der Maaten and G. E. Hinton. Visualizing high-\ndimensional data using t-SNE. Journal of Machine\nLearning Research , 9:2579–2605, 2008.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 663"
    },
    {
        "title": "Early MFCC and HPCP Fusion for Robust Cover Song Identification.",
        "author": [
            "Christopher J. Tralie"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417331",
        "url": "https://doi.org/10.5281/zenodo.1417331",
        "ee": "https://zenodo.org/records/1417331/files/Tralie17.pdf",
        "abstract": "While most schemes for automatic cover song identifi- cation have focused on note-based features such as HPCP and chord profiles, a few recent papers surprisingly showed that local self-similarities of MFCC-based features also have classification power for this task. Since MFCC and HPCP capture complementary information, we design an unsupervised algorithm that combines normalized, beat- synchronous blocks of these features using cross-similarity fusion before attempting to locally align a pair of songs. As an added bonus, our scheme naturally incorporates struc- tural information in each song to fill in alignment gaps where both feature sets fail. We show a striking jump in performance over MFCC and HPCP alone, achieving a state of the art mean reciprocal rank of 0.87 on the Cov- ers80 dataset. We also introduce a new medium-sized hand designed benchmark dataset called “Covers 1000,” which consists of 395 cliques of cover songs for a total of 1000 songs, and we show that our algorithm achieves an MRR of 0.9 on this dataset for the first correctly identified song in a clique. We provide the precomputed HPCP and MFCC features, as well as beat intervals, for all songs in the Cov- ers 1000 dataset for use in further research.",
        "zenodo_id": 1417331,
        "dblp_key": "conf/ismir/Tralie17",
        "keywords": [
            "automatic cover song identification",
            "note-based features",
            "local self-similarities",
            "MFCC-based features",
            "cross-similarity fusion",
            "normalized blocks",
            "structural information",
            "state of the art",
            "Covers80 dataset",
            "Covers 1000 dataset"
        ],
        "content": "EARLY MFCC AND HPCP FUSION FOR ROBUST COVER SONG\nIDENTIFICATION\nChristopher J. Tralie\nDuke University Department of Electrical and Computer Engineering\nctralie@alumni.princeton.edu\nABSTRACT\nWhile most schemes for automatic cover song identiﬁ-\ncation have focused on note-based features such as HPCP\nand chord proﬁles, a few recent papers surprisingly showed\nthat local self-similarities of MFCC-based features also\nhave classiﬁcation power for this task. Since MFCC and\nHPCP capture complementary information, we design an\nunsupervised algorithm that combines normalized, beat-\nsynchronous blocks of these features using cross-similarity\nfusion before attempting to locally align a pair of songs. As\nan added bonus, our scheme naturally incorporates struc-\ntural information in each song to ﬁll in alignment gaps\nwhere both feature sets fail. We show a striking jump\nin performance over MFCC and HPCP alone, achieving\na state of the art mean reciprocal rank of 0.87 on the Cov-\ners80 dataset. We also introduce a new medium-sized hand\ndesigned benchmark dataset called “Covers 1000,” which\nconsists of 395 cliques of cover songs for a total of 1000\nsongs, and we show that our algorithm achieves an MRR\nof 0.9 on this dataset for the ﬁrst correctly identiﬁed song\nin a clique. We provide the precomputed HPCP and MFCC\nfeatures, as well as beat intervals, for all songs in the Cov-\ners 1000 dataset for use in further research.\n1. INTRODUCTION\nA “cover song” is a different version of the same song, usu-\nally performed by a different artist, and often with different\ninstruments, recording settings, mixing/balance, tempo,\nand key. To sidestep a rigorous deﬁnition, like others, we\nevaluate algorithms on a set of songs that have been labeled\nas covers of each other, and we declare success when our\nalgorithm recognizes clusters of songs which have been\ndeemed covers of each other. In fact, this problem is more\nof a “high level music similarity” task beyond exact record-\ning retrieval, making the problem intrinsically more difﬁ-\ncult than traditional audio ﬁngerprinting [16, 31, 30].\nMost work on automatic cover song identiﬁcation to\ndate has focused on estimating and matching note-based\nfeatures such as chord estimates [1], chroma [8, 11],\nharmonic pitch class proﬁles (HPCP) [15, 23, 25], 2D\nc\rChristopher J. Tralie. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nChristopher J. Tralie. “Early MFCC And HPCP Fusion for Robust Cover\nSong Identiﬁcation”, 18th International Society for Music Information\nRetrieval Conference, Suzhou, China, 2017.Fourier magnitude coefﬁcients to approximate these fea-\ntures [12, 19]. This is natural, since regardless of all\nof the transformations that can happen between versions,\nnote sequences should be preserved up to transpositions.\nProblems occur, however, when note sequences are not\nthe deﬁning characteristic of a musical expression. This\nis common in hip hop, for example, such as the song\n“Tricky” in the “Covers 80 Dataset” ([10], Section 4.1)\nperformed by Run D.M.C. and The Beastie Boys. There\nare also songs which are entirely percussive, such as the\n8 covers of Frank Zappa’s song “The Black Page” that we\npresent in Section 4.3. Moreover, even for song pairs with\nstrong harmonic content, there may be sections with drum\nsolos, un-pitched spoken words, or other musical state-\nments on which pitch-based features fail. However, the is-\nsue with features complementary to Chroma, such as Mel-\nFrequency Cepstral Coefﬁcients (MFCCs), is that they are\nhighly sensitive to instrument and balance changes. In\nspite of this, some recent works have shown that MFCC-\nbased features can also be used in cover song identiﬁcation\n[5, 29]. Particularly, if the relative changes of MFCC are\ncaptured, as in [29], performance is still reasonable.\nNaturally, then, rather than relying on a single feature in\nisolation, recent works have shown the beneﬁts of feature\nfusion after song comparisons have been made with each\nfeature set alone. For instance, aggregating ranks from\nindividual features can improve results [20, 21]. Other\nworks show the advantage of using all pairwise similar-\nities computed with different features [6], using a cross-\ndiffusion process known as “similarity network fusion”\n(SNF) ([32, 33], Section 3.1) to come up with a consen-\nsus similarity score between all pairs of songs in a corpus.\nIn this work, we develop a similarity network-based\nearly fusion technique which achieves state of the art re-\nsults by combining complementary HPCP, MFCC, and\nself-similarity MFCCs (SSM MFCCs). Unlike [6], we ap-\nply SNF before alignment in the space of features. This fu-\nsion technique incorporates both cross-similarity between\ntwo different songs and self-similarity of each song, so it\nis able to combine information about matching sections\nbetween songs and structural elements within each song.\nWe also apply late fusion on similarity scores between\na network of songs to further boost the results. While\nstate of the art supervised techniques on the popular “Cov-\ners 80” benchmark dataset yield a mean reciprocal rank\n(MRR) of 0.625 [6]1, our completely unsupervised tech-\n1This technique scored the best in MIREX 2016294nique achieves a MRR of 0.87 (Section 4.1). We also in-\ntroduce our own dataset consisting of 395 cliques of songs,\nwhich we call “Covers1000” (Section 4.2), and on which\nwe report an MRR of 0.9 with our best fusion technique.\n2. BEAT-SYNCHRONOUS BLOCKED FEATURES\nIn this section, we describe three complementary features\nwhich we later fuse together in Section 3. One concept we\nrely on in our feature design is “block-windowing,” which\nwas described in [29]. The idea is that when a contiguous\nset of features in time are stacked into one large vector,\nthey give more information about the local dynamics of a\nsong. This is related to the concept of a delay embedding\nin dynamical systems [17]. Having many blocks across\nthe song also allows us to control for drift by normalizing\nwithin each block. To control for tempo differences, we\ncompute our blocks synchronized with beats, which is a\ncommon preprocessing step [8, 11, 29]. We use the sim-\nple dynamic programming approach of [9], since it allows\nthe speciﬁcation of a tempo bias. As in [11] and [29], we\nbias the beat tracker at 3 different tempo levels (60, 120,\n180bpm), and we compare all pairs of tempo levels, for\nup to 9 unique combinations, since the beat tracker may\nchoose an arbitrary level of subdivision in a rhythm hierar-\nchy. Once the beat onsets are computed, we form a block\nfor every contiguous group of Bbeat intervals, as in [29].\n2.1 HPCP Features\nOne proven set of pitch-based features for cover song iden-\ntiﬁcation are “harmonic pitch class proﬁles” (HPCPs) [14]\n2. Following [26], we compute a stacked delay embed-\nding of the HPCP features within Bbeats, with two HPCP\nwindows per beat, for a total of 2Bwindows per block.\nThis has an advantage over other works which do not use\na delay, as it gives more context in time, and it is con-\nsistent with the block/windowing framework. To normal-\nize for key transpositions, we need to determine an “opti-\nmal transposition index” (OTI) between two songs ([24]).\nGiven the average HPCP vector X2R+12from song A\nand the average HPCP vector Y2R+12from song B, we\ncompute the correlation XTYover all 12 half-step trans-\npositions of the original HPCP features in the block, and\nwe use the transposition that leads to the maximum corre-\nlation. Then, we compute cosine distance between all pairs\nof HPCP blocks between the two songs.\n2.2 MFCCs / MFCC Self-Similarity Matrices (SSMs)\nIn addition to HPCP features, we compute exponentially\nliftered MFCCs in beat-synchronous blocks. We take the\nMFCC window size to be 0.5 seconds, and we advance the\nwindow intervals evenly from the beginning of the block\nto the end of a block with a hop size of512samples. At\na sample rate of 44100 Hz, this leads to a window size\nof22050 and an overlap of roughly 97:5%between win-\ndows (Section 2.2). Longer windows have been shown to\n2To ensure we have a state of the art implementation, we use the Es-\nsentia library to compute HPCPs [4].\nFigure 1 : Example 8-beat Z-normalized MFCC SSMs\nblocks in correspondence between cover versions. A block\nfrom “Claudette” by the Everly Brothers and Roy Orbison.\nThe pattern in this block is Guitar + “Oooh ooh Claudette”\n+ Guitar.\nincrease robustness of SSM matching in [29] and audio ﬁn-\ngerprinting [16], which justiﬁes this choice. To allow di-\nrect comparisons between different blocks, we interpolate\nto 400 MFCCs per block, and we perform Z-normalization\n(as in [29]) to control for loudness and drift, which we\nfound to be an essential step.\nIn addition to block-synchronized and normalized raw\nMFCCs, we also compute self-similarity matrices (SSMs)\nof the Z-normalized MFCCs within each block, as in [29],\nleading to a sequence of SSMs for each song. That is, un-\nlike [2], who compares SSMs between entire songs, we\ncompare SSMs summarizing blocks of audio on the order\nof tens of beats, as recommended by [29]. For each beat-\nsynchronous block, we create a Euclidean SSM between\nall MFCC windows in that block. As with the raw MFCCs,\nto allow comparisons between blocks, we resize each SSM\nto a common image dimension d\u0002d. Figure 1 shows an\nexample of MFCC SSM blocks with 8 beats and 500 win-\ndows per block which were matched between a song and\nits cover in the Covers80 dataset. Although the underlying\nsounds are quite different (male to female, different instru-\nments and balance), the SSMs look similar. [29] argue that\nthis is why, counter to prior intuition, it is possible to use\nMFCCs in cover songs.\n2.3 Cross-Similarity Matrices (CSMs) between Blocks\nGiven a set of Mbeat-synchronous block features for a\nsong A and a set of Nbeat-synchronous block features\nfor a song B, we compare all pairs of blocks between the\ntwo songs for that feature set, yielding an M\u0002Ncross-\nsimilarity matrix (CSM), which can be used to align the\nsongs. For each song, we have 3 different CSMs for the\nthree different feature sets, which are each aligned at the\nsame beat intervals. For MFCC and MFCC SSMs, we\nuse the Euclidean distance (Frobenius norm) to create the\nCSM, while for HPCP, we use the cosine distance after\nOTI. We then use the Smith Waterman algorithm [28] to\nﬁnd the best locally aligned sections between a pair of\nsongs. To apply Smith Waterman, we turn the CSM into a\nbinary matrix BM, so thatBM\nij= 1ifCSMijis within the\n\u0014Nthsmallest values in row iof the CSM and if CSMijisProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 295within the\u0014Mthsmallest values in column jof the CSM,\nand 0 otherwise, where \u00142(0;1)is a mutual nearest\nneighbors threshold. As shown by [25] and [29], this is\na crucial step for improving robustness.\nOnce we have the BMmatrix, we use a diagonally con-\nstrained variant of Smith Waterman to locally align the two\nsongs. The score returned by this algorithm roughly corre-\nsponds to the length of the longest interval of consecutive\nblocks matched between songs, with more tolerance for\ngaps than a naive cross-correlation. More details can be\nfound in[25] and [29]. For comparison, we perform align-\nments with the CSMs obtained for each feature individu-\nally and on the CSM obtained from fusing them.\n3. FEATURE FUSION\nFigure 2 shows an overall pipeline for the fusion process.\nWe ﬁrst brieﬂy review the general mathematical technique\nthat we use to fuse cross-similarity matrices from different\nfeature sets, and then we show speciﬁcally how we apply\nit in our pipeline.\n3.1 Similarity Network Fusion (SNF)\nGiven all pairs of similarity scores between objects using\ndifferent features, Similarity Network Fusion (SNF) is de-\nsigned to ﬁnd a better matrix of all pairwise scores that\ncombines the strengths of each feature [32, 33]. Roughly,\nit performs a random walk using the nearest neighbor sets\nfrom one feature while using the transition probability ma-\ntrices from all of the other features, while continuously\nswitching which feature set is used to determine the near-\nest neighbors. More precisely, the algorithm is deﬁned\nas follows. First, start with a pairwise distance function\n\u001a(i;j)between objects for each feature type, and create an\nexponential kernel W(i;j) =e\u0000\u001a2(i;j)=2(\u001bij)2, where\u001bij\nis a neighborhood-autotuned scale which is the average of\n\u001a(i;j)and the mean k-nearest neighbor intervals of blocks\niandj, for somek(see [33] for more details). Now, create\nthe following Markov transition matrices\nP(i;j) =(\n1\n2W(i;j)P\nk6=iW(i;k)j6=i\n1=2 otherwise)\n(1)\nThis is simply a ﬁrst order Markov chain with a regular-\nized diagonal to promote self-recurrence. Once this matrix\nhas been obtained, create a truncated k-nearest neighbor\nversion of this matrix\nS(i;j) =(W(i;j)P\nk2N(i)W(i;k)j2N(i)\n0 otherwise)\n(2)\nwhereN(i)are theknearest neighbors of vertex i, for\nsome chosen k. Now letPfandSfbe thePandSmatri-\nces for thefthfeature set, and let Pf\nt=0=Pf. Then deﬁne\na ﬁrst order “cross-diffusion” random walk recursively as\nfollowsPf\nt+1=Sf\u0012P\nv6=fPv\nt\nm\u00001\u0013\n(Sf)T(3)\nwheremis the total number of features. In other words,\na random walk is occurring but with probabilities that are\nmodulated by similarity kernels from other features. As\nshown by [32], this process will eventually converge, but\nwe can cut it off early. Whenever it stops, the ﬁnal fused\ntransition probabilities are ^Pt=1\nmPM\nk=1Pk\nt.\n3.2 Late SNF\nOne way to use SNF is to let the matrix \u001a(i;j)be all pair-\nwise distance between songs, computed by some align-\nment [6]. In this case, the result should be a better net-\nwork of similarity scores between all songs3. We fol-\nlow a similar approach to [6], but we we work with the\nSmith Waterman scores we get from a unique combina-\ntion of MFCC, MFCC SSM, and HPCP blocks ([6] ap-\nplied SNF to different alignment schemes on the same\nfeature set). Given a particular score matrix Sbetween\nall pairs of songs, we compute the kernel matrix Was\nW(i;j) = 1=S(i;j). Since Smith Waterman gives a\nhigher score for better matching songs, this ensures that the\nkernel is close to 0in this case. At this point, we perform\nSNF, and we obtain a ﬁnal N\u0002Ntransition probability\nmatrixP. We can then look along each row to ﬁnd the\nneighboring songs with maximum fused probability. This\nprocess can be thought of as exploiting the network of all\nsongs in a collection in an unsupervised manner.\n3.3 Early SNF\nIn addition to SNF after Smith Waterman has given scores,\nwe can perform fusion at the feature level before running\nSmith Waterman. One advantage of doing fusion before\nscores are computed is that we don’t need a network of\nsongs to compute a score; we can obtain an improved score\nbetween two songs without any other context4. Our tech-\nnique for early fusion, which we found to be superior to\nthe “OR fusion” proposed in [13], is to apply SNF on\nthe beat-aligned cross-similarity matrices obtained from\ntwo or more different feature sets before creating a binary\ncross-similarity matrix (CSM) and applying Smith Water-\nman.\nAs deﬁned in Section 3.1, SNF would operate on self-\nsimlarity matrices (SSMs), so it cannot be directly applied\nto cross-similarity matrices. To make it so that CSMs ﬁt\ninto the framework, we create a “parent SSM” for each fea-\nture set that holds both SSMs and the CSM for that feature\nset. In particular, given song AwithMblocks in a partic-\nular feature set and song BwithNblocks in that feature\nset, form the SSM DABwhich is the SSM that results after\nconcatenating song Bto the end of song A. Let the SSM\nfor songAbeDA, the SSM for song BbeDB, and the\n3Note that [27] essentially do the same thing with only one feature set\n4Note that [6] refer to SNF after Smith Waterman as “early fusion”\nwith respect to rank aggregation, which they call “late fusion,” but we\ncall their technique “late fusion” because we fuse before Smith Waterman\nwith SNF, which is even earlier in the pipeline.296 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 2 : A block diagram of our system which performs early similarity network fusion of blocked MFCCs, MFCC\nSSMs, and HPCPs before scoring with Smith Waterman alignment.\nSSM\nSSMCSM\nCSMAB\nBAA\nBN\nNM\nM\nFigure 3 : A pictorial representation of the SSM that results\nwhen concatenating song Bto songA, which we feed to\nSNF for early fusion of low level features. Including self-\nsimilarity blocks of each song helps to promote structural\nelements in cross-similarity regions during SNF.\nCSM between them be CAB. ThenDABcan be split into\nfour sub-blocks:\nDAB(i; j) =8\n>>><\n>>>:DA(i; j) i < M; j < M\nDB(i\u0000M; j\u0000M)i >=M; j > =M\nCAB(i; j\u0000M) i < M; j > =M\nCBA(i\u0000M; j ) =\nCT\nAB(j; i\u0000M) i >=M; j < M9\n>>>=\n>>>;(4)\nFigure 3 shows this pictorially. Given such a matrix for\neach feature set, we could then run SNF and extract the\ncross-similarity sub-matrix at the end. The issue with this\nis the dynamic range of the SSM may be quite different\nfrom the dynamic range of the CSM, as it is likely that\nblocks in song Aare much more similar to other blocks\nin songAthan they are to blocks in B. To mitigate this,\ngiven a nearest neighbor threshold \u0014for the CSM, we com-\npute the kernel neighborhood scales \u001bijindividually for\nDA,DB, andCSMAB, and we put them together in the\nﬁnal kernel matrix WABaccording to Figure 3. Once we\nhave such a matrix WABfor each feature set, we can ﬁ-\nnally perform SNF. At the end, we will end up with a fused\nprobability matrix P, from which we can extract the cross-\nprobabilityPCAB. We can then take mutual highest prob-\nabilities (akin to mutual nearest neighbors) to extract a bi-\nnary matrix and perform Smith Waterman as normal. Fig-\nure 4 shows an example of the constructed matrices WAB\nFigure 4 : An example of early SNF on blocks of MFCC\nSSMs and blocks of HPCP features on the song “Before\nYou Accuse Me” with versions by Eric Clapton and Cree-\ndence Clearwater Revival. The block size is 20beats, and\nthere are three iterations of SNF. The kernels WABare\nshown for each, and the CSM portion is highlighted with\na blue box. The ﬁnal fused probability matrix Preturned\nfrom SNF is shown in the upper right. The correspond-\ning CSM portions for all three matrices shown for each\non the bottom. In the fused probability matrix, the diag-\nonal regions are much crisper and more distinct from the\nbackground than they are for the individual feature sets.\nThe result is that the mutual nearest neighbors binary CSM\nhas longer uninterrupted diagonals, which is reﬂected by a\nhigher Smith Waterman score.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 297MR MRR Top-01 Top-10 ../80\nMFCCs 29.7 0.538 79 97 42/80\nSSMs 15.1 0.615 91 111 48/80\nHPCPs 18.2 0.673 102 119 53/80\nLate\nSSMs/MFCCs14.0 0.7 107 125 55/80\nLate All 8.63 0.824 127 141 64/80\nEarly 7.76 0.846 131 143 68/80\nEarly + Late 7.59 0.873 136 144 69/80\n[6] ? 0.625 ? 114 ?\nTable 1 : Results of different features and fusion techniques\non the Covers 80 dataset.\nand the resulting fused probabilities P.\nOne advantage of this technique is that since the CSM\nand SSMs are treated together and normalized to a similar\nrange, any recurrent structure which exists in the SSMs\ncan reinforce otherwise weaker structure in the CSMs\nduring the diffusion process. This can potentially help\nto strengthen weaker beat matches in an otherwise well-\nmatching section, leading to longer uninterrupted diago-\nnals in the resulting binary CSM.\n3.4 Early Fusion Examples\nBefore we launch into a more comprehensive experiment,\nwe show a few examples of early SNF to illustrate the value\nadded. In each example, we used 20-beat blocks, \u0014= 0:1\nfor both similarity fusion and binary nearest neighbors, and\n3 iterations of SNF. Figure 5 shows an example where the\nthree individual features are rather poor by themselves, but\nwhere they happen to all pick up on similarities in comple-\nmentary regions. As a result, early SNF does a fantastic\njob fusing the features. Figure 6 shows an example where\nMFCC SSMs happen to do better than HPCP, but where the\nresults fusing both are still better than each individually.\n4. EXPERIMENTS\nWe are now ready to evaluate the performance of this new\nalgorithm. In all of our experiments below, we settle on\n\u0014= 0:1(the mutual nearest neighbor threshold for binary\nCSMs) and B= 20 beats per block. For similarity net-\nwork fusion, we take 20 nearest neighbors for both early\nfusion and late fusion, we perform 3 iterations for early fu-\nsion, and we perform 20 iterations for late fusion. We also\ninclude an “early + late” fusion result, which is applying\nlate fusion to the network of similarities obtained from all\nof the feature sets (MFCCs, MFCC SSMs, HPCPs) plus\nthe network of similarities obtained from the early fusion\nof the three feature sets.\n4.1 Covers 80 Dataset\nTo benchmark our algorithm, we begin by testing it on the\n“Covers 80” dataset [10]. This dataset contains 160 songs\nwhich are split into two disjoint subsets AandB, each\nwith exactly one version of a pair of songs, for a total of 80MR MRR Top-01 Top-10\nMFCCs 83.3 0.618 583 679\nSSMs 72.5 0.623 581 698\nHPCPs 44.4 0.757 727 809\nLate 19.8 0.875 855 931\nEarly 22.5 0.829 798 884\nEarly + Late 14 0.904 884 950\nTable 2 : Results of different features and fusion techniques\non the Covers 1000 dataset.\npairs. [8] and [11] assess performance as follows: given\na song in group A, declare its cover song to be the top\nranked song in set B, and record the total number of top\nranked songs that are correct. To get a better idea of the\nperformance, we also compute the mean rank (MR), mean\nreciprocal rank (MRR), and the number of songs correctly\nidentiﬁed past a certain number. All of these statistics are\ncomputed on the full set of 160 songs, which is more difﬁ-\ncult than simply looking in set Aor setB.\nTable 1 shows the results. By themselves, HPCP fea-\ntures perform better than MFCC-based features, which is\nconsistent with ﬁndings in the literature. However, there\nare big improvements when fusing them all. Surprisingly,\nwe obtain a score of 42/80 just by blocking and normaliz-\ning the MFCCs. This shows the power of having stacked\ndelay MFCCs and of normalizing within each block to cut\ndown on drift. Also, when fusing MFCCs and MFCC\nSSMs with late fusion, we get a large performance boost\nover either alone, showing that SSMs are adding comple-\nmentary information to the MFCCs they summarize.\n4.2 Covers 1000 Dataset\nTo test our algorithm more thoroughly, we created our\nown dataset by manually choosing 1000 cover songs\n(395 cliques total) based on annotations given by users\nonhttp://www.secondhandsongs.com5. This\ndataset covers over a century of Western music from 1905\n- 2016, and hence, it covers a wide variety of genres and\nstyles. Figure 7 shows the full distribution of years cov-\nered. By contrast, the Covers80 dataset contains almost\nexclusively pop music from the ‘80s and early ‘90s.\nMost cliques have only two songs as in the Covers80\ndataset, but there are a few cliques with 3 and 4 songs. In\nthis case, we report the MR and MRR of the ﬁrst correctly\nidentiﬁed song in the clique. Table 2 shows the results.\nSimilar trends are seen to the Covers80 case, and perfor-\nmance scales to this larger size. One difference is that late\nfusion on HPCPs/MFCCs/MFCC SSMs performed better\nrelative to early fusion, likely because the network was\nmuch richer with the additional volume of songs.\n5MFCC and HPCP features for our dataset are publicly available at\nhttp://www.covers1000.net , along with beat intervals and other\nmetadata including song title, album, and year298 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 5 : Smith Waterman tables/scores for “All Tomorrow’s Parties” by Japan and Velvet Underground.\nFigure 6 : Smith Waterman tables/scores for “Time” by Tom Waits and Tori Amos\n1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020\nYear051015202530Number of SongsCovers 1000 Song Years\nFigure 7 : A distribution of years of songs in the Covers\n1000 dataset.\n4.3 Frank Zappa: “The Black Page”\nIn our ﬁnal experiment, we test a clique of 8 cover versions\nof the song “The Black Page” by Frank Zappa, which is en-\ntirely a drum solo that has absolutely no harmonic content.\nWe query each song against all of the songs in the Cov-\ners1000 dataset, and we compute the mean average preci-\nsion (MAP) for the songs in the clique. Unsurprisingly,\nfor HPCP, the MAP is a mere 0.014, while for the rest of\nthe features these songs are quite distinct from the rest of\nthe songs in the Covers 1000 dataset. The best performing\nfeature set is early SNF, with a MAP of 0.98, followed by\nraw blocked MFCCs at a MAP of 0.97, followed by MFCC\nSSMs with a MAP of 0.905.\n5. DISCUSSION\nIn this work, we have demonstrated the beneﬁt of com-\nbining complementary features at a very early stage of the\ncover song identiﬁcation pipeline, in addition to the late fu-\nsion techniques in [6]. Unlike [6] and other techniques, our\nalgorithm works on a pair of songs and does not need a net-\nwork of songs to improve performance, though we show\nthat incorporating information from a network of songs\n(“late fusion”) can further improve results. We showed\nthat HPCP and MFCC features capture complementary in-formation and are able to boost performance substantially\nover either alone. In the process, we also developed a novel\ncross-similarity fusion scheme which was validated on sev-\neral datasets, and which we believe could be useful beyond\ncover song identiﬁcation in music structure analysis.\nThe main drawback of our technique is the requirement\nof beat tracking. In practice, beat trackers may not return\ncorrect onsets. Our current best remedy for this is to use\ndifferent tempo biases, which blows up computation by a\nfactor of 9. Also, coming up with a single beat level is ill-\nposed, since most music consists of a hierarchy of rhyth-\nmic subdivisions [22]. There does seem to be a recent con-\nvergence of techniques for rhythm analysis, though, [7, 18]\nso hopefully our system will beneﬁt.\nIn addition to imperfect beat intervals, there are also\ncomputational drawbacks in low level alignment, which is\nwhy most recent works on cover songs perform approx-\nimations to global cross-correlation, such as 2D Fourier\nMagnitude Coefﬁcients [12, 19]. By contrast, we rely\non Smith Waterman, which is a quadratic algorithm, and\nearly SNF adds another quadratic time complexity algo-\nrithm even with sparse nearest neighbor sets. To address\nthis, we are in the process of implementing GPU algo-\nrithms for every step of our pipeline, and we hope to apply\nit to the “Second Hand Songs Dataset,” which is a subset\nof the Million Songs Dataset [3].\n6. ACKNOWLEDGEMENTS\nChristopher Tralie was partially supported by an NSF\nGraduate Fellowship NSF under grant DGF-1106401 and\nan NSF big data grant DKA-1447491. We would also\nlike to thank Erling Wold for pointing out the 8 covers of\n“The Black Page” by Frank Zappa, and we would like to\nthank the community at www.secondhandsongs.com\nfor meticulously annotating songs which helped us to de-\nsign Covers 1000.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 2997. REFERENCES\n[1] Juan Pablo Bello. Audio-based cover song retrieval us-\ning approximate chord sequences: Testing shifts, gaps,\nswaps and beats. In ISMIR , volume 7, pages 239–244,\n2007.\n[2] Juan Pablo Bello. Grouping recorded music by struc-\ntural similarity. In ISMIR 2009, Kobe, Japan, 2009 ,\npages 531–536, 2009.\n[3] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whit-\nman, and Paul Lamere. The million song dataset. In\nISMIR , volume 2, page 10, 2011.\n[4] Dmitry Bogdanov, Nicolas Wack, Emilia G ´omez,\nSankalp Gulati, Perfecto Herrera, Oscar Mayor, Ger-\nard Roma, Justin Salamon, Jos ´e R. Zapata, and Xavier\nSerra. Essentia: An audio analysis library for music in-\nformation retrieval. In ISMIR 2013, Curitiba, Brazil,\n2013 , pages 493–498, 2013.\n[5] Ning Chen, J Stephen Downie, Haidong Xiao, Yu Zhu,\nand Jie Zhu. Modiﬁed perceptual linear prediction\nliftered cepstrum (mplplc) model for pop cover song\nrecognition. In ISMIR , pages 598–604, 2015.\n[6] Ning Chen, Wei Li, and Haidong Xiao. Fusing similar-\nity functions for cover song identiﬁcation. Multimedia\nTools and Applications , pages 1–24, 2017.\n[7] Norberto Degara, Enrique Argones R ´ua, Antonio Pena,\nSoledad Torres-Guijarro, Matthew EP Davies, and\nMark D Plumbley. Reliability-informed beat track-\ning of musical signals. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 20(1):290–\n301, 2012.\n[8] Daniel PW Ellis. Identifying’cover songs’ with beat-\nsynchronous chroma features. MIREX 2006 , pages 1–\n4, 2006.\n[9] Daniel PW Ellis. Beat tracking by dynamic program-\nming. Journal of New Music Research , 36(1):51–60,\n2007.\n[10] Daniel PW Ellis. The “covers80” cover song\ndata set. URL: http://labrosa. ee. columbia.\nedu/projects/coversongs/covers80 , 2007.\n[11] Daniel PW Ellis and Courtenay Valentine Cotton. The\n2007 labrosa cover song detection system. MIREX\n2007 , 2007.\n[12] Daniel PW Ellis and Bertin-Mahieux Thierry. Large-\nscale cover song recognition using the 2d fourier trans-\nform magnitude. In The 13th international society for\nmusic information retrieval conference , pages 241–\n246, 2012.\n[13] R ´emi Foucard, J-L Durrieu, Mathieu Lagrange, and\nGa¨el Richard. Multimodal similarity between musi-\ncal streams for cover version detection. In AcousticsSpeech and Signal Processing (ICASSP), 2010 IEEE\nInternational Conference on , pages 5514–5517. IEEE,\n2010.\n[14] Emilia G ´omez. Tonal description of polyphonic audio\nfor music content processing. INFORMS Journal on\nComputing , 18(3):294–304, 2006.\n[15] Emilia G ´omez and Perfecto Herrera. The song remains\nthe same: identifying versions of the same piece using\ntonal descriptors. In ISMIR , pages 180–185, 2006.\n[16] Jaap Haitsma and Ton Kalker. A highly robust audio\nﬁngerprinting system. In Ismir , volume 2002, pages\n107–115, 2002.\n[17] Holger Kantz and Thomas Schreiber. Nonlinear time\nseries analysis , volume 7. Cambridge university press,\n2004.\n[18] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nAn efﬁcient state-space model for joint tempo and me-\nter tracking. In ISMIR , pages 72–78, 2015.\n[19] Oriol Nieto and Juan Pablo Bello. Music segment\nsimilarity using 2d-fourier magnitude coefﬁcients. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2014 IEEE International Conference on , pages 664–\n668. IEEE, 2014.\n[20] Julien Osmalsky, Jean-Jacques Embrechts, Peter Fos-\nter, and Simon Dixon. Combining features for cover\nsong identiﬁcation. In 16th International Society for\nMusic Information Retrieval Conference , 2015.\n[21] Julien Osmalsky, Marc Van Droogenbroeck, and Jean-\nJacques Embrechts. Enhancing cover song identiﬁca-\ntion with hierarchical rank aggregation. In Proceed-\nings of the 17th International for Music Information\nRetrieval Conference , pages 136–142, 2016.\n[22] Elio Quinton, Christopher Harte, and Mark Sandler.\nExtraction of metrical structure from music recordings.\nInProc. of the 18th Int. Conference on Digital Audio\nEffects (DAFx). Trondheim , 2015.\n[23] J Serra. Music similarity based on sequences of de-\nscriptors: tonal features applied to audio cover song\nidentiﬁcation. Department of Information and Com-\nmunication Technologies, Universitat Pompeu Fabra,\nBarcelona, Spain , 2007.\n[24] Joan Serra, Emilia G ´omez, and Perfecto Herrera.\nTransposing chroma representations to a common key.\nInIEEE CS Conference on The Use of Symbols to Rep-\nresent Music and Multimedia Objects , pages 45–48,\n2008.\n[25] Joan Serra, Emilia G ´omez, Perfecto Herrera, and\nXavier Serra. Chroma binary similarity and local\nalignment applied to cover song identiﬁcation. Audio,\nSpeech, and Language Processing, IEEE Transactions\non, 16(6):1138–1151, 2008.300 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[26] Joan Serra, Xavier Serra, and Ralph G Andrzejak.\nCross recurrence quantiﬁcation for cover song identi-\nﬁcation. New Journal of Physics , 11(9):093017, 2009.\n[27] Joan Serr `a, Massimiliano Zanin, Perfecto Herrera,\nand Xavier Serra. Characterization and exploitation of\ncommunity structure in cover song networks. Pattern\nRecognition Letters , 33(9):1032–1041, 2012.\n[28] Temple F Smith and Michael S Waterman. Identiﬁca-\ntion of common molecular subsequences. Journal of\nmolecular biology , 147(1):195–197, 1981.\n[29] Christopher J Tralie and Paul Bendich. Cover song\nidentiﬁcation with timbral shape sequences. In 16th\nInternational Society for Music Information Retrieval\n(ISMIR) , pages 38–44, 2015.\n[30] Avery Wang. The shazam music recognition service.\nCommunications of the ACM , 49(8):44–48, 2006.\n[31] Avery Wang et al. An industrial strength audio search\nalgorithm. In ISMIR , pages 7–13. Washington, DC,\n2003.\n[32] Bo Wang, Jiayan Jiang, Wei Wang, Zhi-Hua Zhou, and\nZhuowen Tu. Unsupervised metric fusion by cross dif-\nfusion. In Computer Vision and Pattern Recognition\n(CVPR), 2012 IEEE Conference on , pages 2997–3004.\nIEEE, 2012.\n[33] Bo Wang, Aziz M Mezlini, Feyyaz Demir, Marc Fi-\nume, Zhuowen Tu, Michael Brudno, Benjamin Haibe-\nKains, and Anna Goldenberg. Similarity network fu-\nsion for aggregating data types on a genomic scale. Na-\nture methods , 11(3):333–337, 2014.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 301"
    },
    {
        "title": "Make Your Own Accompaniment: Adapting Full-Mix Recordings to Match Solo-Only User Recordings.",
        "author": [
            "T. J. Tsai 0001",
            "Steven K. Tjoa",
            "Meinard Müller"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417018",
        "url": "https://doi.org/10.5281/zenodo.1417018",
        "ee": "https://zenodo.org/records/1417018/files/TsaiTM17.pdf",
        "abstract": "We explore the task of generating an accompaniment track for a musician playing the solo part of a known piece. Un- like previous work in real-time accompaniment, we focus on generating the accompaniment track in an off-line fash- ion by adapting a full-mix recording (e.g. a professional CD recording or Youtube video) to match the user’s tempo preferences. The input to the system is a set of recorded passages of a solo part played by the user (e.g. solo part in a violin concerto). These recordings are contiguous seg- ments of music where the soloist part is active. Based on this input, the system identifies the corresponding passages within a full-mix recording of the same piece (i.e. contains both solo and accompaniment parts), and these passages are temporally warped to run synchronously to the solo- only recordings. The warped passages can serve as accom- paniment tracks for the user to play along with at a tempo that matches his or her ability or desired interpretation. As the main technical contribution, we introduce a segmen- tal dynamic time warping algorithm that simultaneously solves both the passage identification and alignment prob- lems. We demonstrate the effectiveness of the proposed system on a pilot data set for classical violin.",
        "zenodo_id": 1417018,
        "dblp_key": "conf/ismir/TsaiTM17",
        "keywords": [
            "accompaniment track",
            "real-time accompaniment",
            "full-mix recording",
            "tempo preferences",
            "contiguous segments",
            "solo part",
            "temporally warped",
            "soloist part",
            "tempo matching",
            "pilot data set"
        ],
        "content": "MAKE YOUR OWN ACCOMPANIMENT: ADAPTING FULL-MIX\nRECORDINGS TO MATCH SOLO-ONLY USER RECORDINGS\nTJ Tsai1Steven K. Tjoa2Meinard M ¨uller3\n1Harvey Mudd College, Claremont, CA\n2Galvanize, Inc., San Francisco, CA\n3International Audio Laboratories Erlangen, Erlangen, Germany\nttsai@hmc.edu, steve@stevetjoa.com, meinard.mueller@audiolabs-erlangen.de\nABSTRACT\nWe explore the task of generating an accompaniment track\nfor a musician playing the solo part of a known piece. Un-\nlike previous work in real-time accompaniment, we focus\non generating the accompaniment track in an off-line fash-\nion by adapting a full-mix recording (e.g. a professional\nCD recording or Youtube video) to match the user’s tempo\npreferences. The input to the system is a set of recorded\npassages of a solo part played by the user (e.g. solo part\nin a violin concerto). These recordings are contiguous seg-\nments of music where the soloist part is active. Based on\nthis input, the system identiﬁes the corresponding passages\nwithin a full-mix recording of the same piece (i.e. contains\nboth solo and accompaniment parts), and these passages\nare temporally warped to run synchronously to the solo-\nonly recordings. The warped passages can serve as accom-\npaniment tracks for the user to play along with at a tempo\nthat matches his or her ability or desired interpretation. As\nthe main technical contribution, we introduce a segmen-\ntal dynamic time warping algorithm that simultaneously\nsolves both the passage identiﬁcation and alignment prob-\nlems. We demonstrate the effectiveness of the proposed\nsystem on a pilot data set for classical violin.\n1. INTRODUCTION\nIma Amateur loves her recording of Itzhak Perlman per-\nforming the Tchaikovsky violin concerto with the Lon-\ndon Symphony Orchestra. She has been learning how to\nplay the ﬁrst movement herself, and she would love to play\nalong with the recording. Unfortunately, there are parts of\nthe recording that are simply too fast for her to play along\nwith. She ﬁnds an app that can slow down the parts of the\nPerlman recording that are difﬁcult. All she has to do is up-\nload several solo recordings of herself performing sections\nof the concerto, along with the original full-mix recording\nthat she would like to play along with. The app analyzes\nc\rTJ Tsai, Steven K. Tjoa, Meinard M ¨uller. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: TJ Tsai, Steven K. Tjoa, Meinard M ¨uller. “Make\nYour Own Accompaniment: Adapting full-mix recordings to match solo-\nonly user recordings”, 18th International Society for Music Information\nRetrieval Conference, Suzhou, China, 2017.her playing and generates a modiﬁed version of the Perl-\nman recording that runs in sync with her solo recordings.\nThis paper explores the technical feasibility of such an\napplication. In technical terms, the problem is this: given a\nfull-mix recording and an ordered set of solo-only record-\nings that each contain a contiguous segment of music\nwhere the soloist is active, design a system that can time-\nscale modify the full-mix recording to run synchronously\nwith the solo recordings.1\nThere are three main technical challenges underlying\nthis scenario. The ﬁrst challenge is to identify the passages\nin the full-mix recording that correspond to the solo-only\nrecordings. The second challenge is to temporally align\nthe corresponding passages in the full-mix and solo record-\nings. The third challenge is to time-scale modify the full-\nmix recording to follow the calculated alignment without\nchanging the pitch of the original recording. This paper fo-\ncuses primarily on the ﬁrst two challenges, and it assesses\nthe technical feasibility of solving these problems on a pi-\nlot data set. The main technical contribution of this work\nis to propose a segmental dynamic time warping (DTW)\nalgorithm that simultaneously solves the passage identiﬁ-\ncation and temporal alignment problems. We will simply\nadopt an out-the-box approach to solve the third challenge.\nThe idea of generating accompaniment for amateur mu-\nsicians has been explored in two different directions. On\none end of the spectrum, companies have explored ﬁxed\naccompaniment tracks. Some examples include the popu-\nlar Aebersold Play-A-Long recordings for jazz improvisa-\ntion and Music Minus One for classical music. The ben-\neﬁt of ﬁxed accompaniment tracks is their simplicity – all\nyou need is a device that can play audio. The drawback\nof ﬁxed accompaniment tracks is their lack of adaptivity\n– they do not respond or adapt to the user’s playing in\nany way. On the other end of the spectrum, academics\nhave explored real-time accompaniment (e.g. see work by\nRaphael [23] [24] and Cont [3]). These are complex sys-\ntems that can track a musician’s (or group’s) playing and\ngenerate accompaniment in real-time. The beneﬁt of real-\ntime accompaniment is the adaptivity of the system. The\ndrawbacks of real-time accompaniment systems are that\nthey are not easy to use for the general population (e.g. re-\nquire software packages on a laptop) and may not be very\n1Without changing the pitch, of course!79expressive (e.g. sound like MIDI). Also, for the purposes\nof academic study, another drawback is the difﬁculty of\nevaluating such a system in an objective way. Because the\nuser and the accompaniment system inﬂuence each other in\nreal-time, it is difﬁcult to decouple the effect of one from\nthe other. When there are errors, for example, it is difﬁ-\ncult to say whether the error is because the accompaniment\nsystem failed, the user failed to respond appropriately, or\nsome combination of both.\nThis work explores the realm in between these two ex-\ntremes. Like ﬁxed accompaniment tracks, the proposed\nsystem has the beneﬁt of simplicity – the user does not\nneed any specialized software or hardware, but simply re-\nceives an audio track that can be played on any audio de-\nvice. Like real-time accompaniment, the proposed sys-\ntem has the beneﬁt of (partial) adaptivity – the system tai-\nlors the accompaniment track to the user’s playing in an\noff-line manner. This middle realm has several additional\nbeneﬁts. Because the user and the accompaniment are no\nlonger coupled in real-time, we can measure how well the\naccompaniment system “follows” the user’s playing with\nobjective metrics. Another beneﬁt is that the off-line nature\nof this system makes it suitable for a client-server model,\nwhich is ideal for the envisioned app. Lastly, by approach-\ning this problem through adapting an existing recording,\nwe can also potentially get the beneﬁt of a very musical\nand expressive accompaniment track (assuming we don’t\nintroduce too many artifacts from time-scale modiﬁcation).\nThe two challenges we will focus on – passage identiﬁ-\ncation and temporal alignment – are closely related to pre-\nvious work in audio matching and music synchronization.\nThe passage identiﬁcation problem has strong similarities\nto audio matching, where the goal is to identify a given\npassage in other performances of the same (usually clas-\nsical) work. Previous work has introduced robust features\nfor this task [20] and efﬁcient ways to handle global tempo\nvariations such as using multiple versions of a query that\nhave been tempo-adjusted [19]. Subsequent work has ex-\nplored the use of indexing techniques to scale the system to\nlarge data sets [14] [2]. The temporal alignment problem\nhas strong similarities to music synchronization, where the\ngoal is to temporally align two performances of the same\npiece. The bread-and-butter approach is to apply DTW\nwith suitably designed features [12] [4] [10]. One problem\nwith this approach is that the memory and computation re-\nquirements increase quadratically as the feature sequences\nincrease in length. Many variants have been proposed to\nmitigate this issue, including limiting the search space to\na band [25] or parallelogram [13] around the cost matrix\ndiagonal, doing the time-warping in an online fashion [5]\n[15], or adopting a multiscale approach that estimates the\nalignment at different granularities [26] [21] [9]. Other\nvariants tackle issues like handling repeats [11], identify-\ning partial alignments between recordings [17] [18], deal-\ning with memory constraints [22], and taking advantage of\nmultiple recordings [27] [1].\nThough similar, the proposed scenario differs from\nmost previous work in three important ways. First, we arematching solo-only recordings to full-mix recordings (i.e.\nsolo and accompaniment). Most work in audio matching\nand music synchronization assumes that the recordings of\ninterest are different performances of the same piece, and\ntherefore have the same audio sources. One could think\nof the current scenario as audio matching with very high\nlevels of additive noise (i.e. the accompaniment). Sec-\nond, the task is off-line but there are still stringent runtime\nconstraints. In music synchronization, the best approach\nis the one with the highest alignment precision, and we\nare willing to accept signiﬁcant runtimes since the task\nis off-line. In the current scenario, however, the runtime\nis a very important factor because the application is user-\nfacing. A user will not be willing to wait 30 seconds for\nthe accompaniment track to be generated. For this rea-\nson, in this paper we will not consider any approaches to\nthese two challenges that require more than 5-6 seconds of\nruntime. Third, the current scenario deals with consumer-\nproduced recordings. Much previous work focuses on al-\nbum tracks from professional CDs and professional mu-\nsicians. In contrast to this, amateur musicians will play\nwrong notes, count incorrectly, rush, and play out of tune.\nThese issues will be important factors affecting system per-\nformance.\nThis paper is structured around our main goal: to as-\nsess the technical feasibility of solving the passage identi-\nﬁcation and temporal alignment problems in a robust and\nefﬁcient manner. Section 2 describes our system, includ-\ning an explanation of the proposed segmental DTW algo-\nrithm. Section 3 discusses the experimental setup. Section\n4 presents empirical results of our experiments on the pilot\ndata set. Section 5 investigates several questions of interest\nto gain more intuition into system performance. Section 6\nconcludes the work.\n2. SYSTEM DESCRIPTION\nWe describe the proposed system in three parts: the seg-\nmental DTW algorithm, the features, and the time-scale\nmodiﬁcation.\n2.1 Segmental DTW Algorithm\nThere are four main steps in the segmental DTW algo-\nrithm, each explained below.\nStep 1: Frame-level cost matrices. The ﬁrst step is\nto compute a subsequence DTW cumulative cost matrix\nfor each solo segment. Subsequence DTW is a variant of\nthe regular DTW algorithm in which one of the record-\nings (the query) is assumed to only match a section of the\nother recording (the reference), rather than matching the\nentire recording from beginning to end. This can be ac-\ncomplished by allowing the query to begin matching any-\nwhere in the reference without penalty, and allowing the\nquery to end matching anywhere in the reference with-\nout penalty. We allow the following (query; reference )\nsteps in the dynamic programming stage: (1;1),(1;2), and\n(2;1). These steps have weights of 1,1, and 2, respec-80 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 1 . A graphical overview of the segmental DTW\nalgorithm for aligning an ordered set of solo record-\nings against a full-mix recording. Rows correspond to\nsolo recording frames and columns correspond to full-mix\nrecording frames. Time increases from bottom to top and\nleft to right. In this example, N= 4.\ntively.2This set of steps assumes that the instantaneous\ntempo in the query and reference will differ at most by a\nfactor of 2. For more details about subsequence DTW, see\nchapter 7 in [16]. In the case of our proposed algorithm, we\ncompute the subsequence DTW cumulative cost matrix but\nrefrain from backtracing until step 4. Rather than backtrac-\ning from the local optimum in each cumulative cost matrix,\nwe will instead backtrace from the element on the globally\noptimum path. This globally optimum path will be deter-\nmined in steps 2 and 3.\nStep 2: Segment-level cost matrix. The second step is\nto compute a cumulative cost matrix of global path scores\nacross all solo segments. This can be done in two sub-\nsteps. The ﬁrst sub-step is to create a matrix that con-\ntains the last row of each subsequence cumulative cost ma-\ntrix from step 1.3This matrix will have Nrows and K\ncolumns, where Nis the number of solo segments and K\nis the number of frames in the reference (i.e. full-mix)\nrecording. Note that this matrix is analogous to a pairwise\ncost matrix, where instead of pairwise frame-level costs\nwe have segment-level subsequence path costs. The sec-\nond sub-step is to compute a (segment-level) cumulative\ncost matrix on this (segment-level) pairwise cost matrix by\ndoing dynamic programming. This dynamic programming\nstep differs from regular DTW dynamic programming in\none important way. Unlike most scenarios where the set\nof possible transitions is ﬁxed regardless of position in the\ncost matrix, here the possible transition steps changes from\nrow to row. Speciﬁcally, for an element in row n, the two\npossible transitions are (0;1)and(1;Ln+1\n2), where Ln+1\nis the length (in frames) of the (n+ 1)thsolo segment.\nThe weights on these two transitions are 0and1, respec-\ntively. In words, we are looking for the Nelements in the\nsegment-level pairwise cost matrix (one per row) that have\nthe minimum total path score under two constraints: (1)\n2Note that the (2;1)step should be weighted double to prevent de-\ngenerate matchings to very short sections.\n3Here, we assume that rows correspond to different query frames, and\ncolumns correspond to different reference frames.they are consistent with the given ordering (i.e. segment n\ncomes before segment n+ 1), and (2) elements in adjacent\nrows must be separated by a minimum distance, which is\ndetermined by the length of the solo segment and the max-\nimum tempo difference in the subsequence DTW step (in\nthis case, a factor of 2).\nStep 3: Segment-level backtrace. The third step is to\nbacktrace through the segment-level cumulative cost ma-\ntrix. We start at the last element of the matrix (i.e. the\nupper right hand corner) and backtrace until we reach the\nﬁrst element of the matrix (i.e. the lower left hand corner).\nNote that the (0;1)steps with 0weight allow for skipping\nportions of the full-mix recording without penalty. The\n(1;Ln+1\n2)transitions in the backtraced path indicate the el-\nement in each row that contributes to the globally optimal\npath.\nStep 4: Frame-level backtrace. The ﬁnal step is to\nbacktrace through each subsequence DTW cumulative cost\nmatrix from step 1, where we begin the backtracing at the\nelements selected in step 3. These elements have been\nselected to optimize a global path score across all solo\nsegments, rather than a local path score across a single\nsolo segment. After performing this frame-level backtrace\nstep, we have achieved our desired goal: identifying both\nsegment-level and frame-level alignments for each solo\nsegment.\nFigure 1 shows a graphical summary of the segmental\nDTW algorithm. In this ﬁgure, rows correspond to differ-\nent solo segment frames and columns correspond to dif-\nferent full-mix frames. Time increases from bottom to top\nand from left to right. The four rectangles in the lower left\nare the frame-level cumulative cost matrices for each solo\nrecording. The segment-level cost matrix (top left) is con-\nstructed by aggregating the last row from each frame-level\ncumulative cost matrix (highlighted in dark gray). We then\nbacktrace at the segment level, and use the predicted seg-\nment ending points to backtrace at the frame level. The\nﬁnal predicted alignments are shown in the lower right.\nNote that the proposed system only indicates how the full-\nmix recording should be warped during the segments of the\npiece when the soloist is playing. One could interpolate the\ntempo for the other segments.\n2.2 Features\nThe segmental DTW algorithm is compatible with any\nframe-based feature and cost metric. For the experiments\nin this paper, we computed L2-normalized chroma features\nevery 22ms and used a cosine distance metric. This com-\nbination was selected for two practical reasons. First, we\nwanted to demonstrate the segmental DTW algorithm with\na standard feature, so as not to conﬂate the performance\nbeneﬁts of both a new matching algorithm and a novel (or\nless widely used) feature. Second, this combination al-\nlows the subsequence DTW cost matrices to be computed\nvery efﬁciently with simple matrix multiplication. Given\nthe constraints on runtime of this consumer-facing applica-\ntion, efﬁciency is an important consideration. We selected\nthe feature rate to ensure that the average time required toProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 81Composition full solo avgLen segs\nSeitz concerto no2, mv3 5 5 187 s 5\nBach double concerto, mv1 5 5 250 s 5\nVivaldi concerto in a, mv1 5 5 229 s 5\nVeracini sonata in d, mv4 3 4 223 s 4\nTable 1 . Summary of the pilot data set. Each row indicates\nthe number of full-mix and solo recordings, the average\nlength, and the number of segments in the composition.\nalign a single query (i.e. multiple solo recordings against\na full-mix recording) was under 6 seconds. This thresh-\nold could be set arbitrarily depending on how long we are\nwilling to make the user wait. In the discussion section,\nwe will compare our main results with a system that uses\nmore state-of-the-art features, which were developed in an\noff-line context where runtime is not a signiﬁcant consid-\neration. These latter features can provide a lower bound on\nerror rate when we ignore runtime constraints.\n2.3 Time-Scale Modiﬁcation\nThe goal of the time-scale modiﬁcation (TSM) step is to\nstretch or compress the duration of a given audio signal\nwhile preserving properties like pitch and timbre. Typi-\ncally, TSM approaches stretch or compress an audio signal\nin a linear fashion by a constant stretching factor. In our\nscenario, we need to stretch the full-mix recording accord-\ning to the solo-mix alignment, which leads to a non-linear\ntime-stretch function. To deal with non-linear stretches,\nwe apply the strategy described in [8], where the positions\nof the TSM analysis frames are speciﬁed according to the\ntime-stretch function instead of a constant analysis hop-\nsize.\nTo attenuate artifacts and to improve the quality of\nthe time-scale modiﬁed signal, we use a recent TSM ap-\nproach [7] that involves harmonic-percussive separation\nand combines the advantages of a phase-vocoder TSM\napproach (preserving the perceptual quality of harmonic\nsignal components) and a time-domain TSM approach\n(preserving transient-like percussive signal components).\nAn overview of different TSM procedures can be found\nin [6, 8].\n3. EXPERIMENTAL SETUP\nThe experimental setup will be described in three parts:\nthe data collection, the data preparation, and the evaluation\nmetric.\n3.1 Data Collection\nOur data collection process was dictated by practicality. In\norder to evaluate the proposed system, we need two dif-\nferent types of audio data: full-mix recordings and solo\nrecordings. Clearly, the full-mix recordings are in abun-\ndant supply and can be selected from any professional CD\nrecording or Youtube video. The solo recordings, how-\never, are much more difﬁcult to ﬁnd, as musicians typicallydo not record performances that are missing the accompa-\nniment part. Our solution to this problem was to focus\ndata collection efforts on a small subset of pieces from the\nhighly popular Suzuki violin method. The Suzuki method\nprescribes a speciﬁc sequence of violin works in order to\ndevelop a violinist’s mastery of the instrument. Because\nof the popularity of the Suzuki method, we were able to\nﬁnd Youtube videos of violinists performing the solo parts\n(in isolation) from several works. Some of these record-\nings are violin teachers demonstrating how to perform a\npiece. Some recordings are young adults wishing to doc-\nument their progress on the violin. Other recordings are\ndoting parents trying to show off their talented children.\nTable 1 shows a summary of the audio recordings. The\ndata set contains four violin pieces or movements selected\nfrom Suzuki books ﬁve and six. For each piece, we col-\nlected multiple full-mix recordings and solo recordings\nfrom Youtube. By focusing on annotating multiple record-\nings of the same piece, we can make the most of the limited\namount of (annotated) data by considering different combi-\nnations of full-mix and solo recordings. At the same time,\nwe wanted several pieces of music from different com-\nposers and periods, so as to avoid a composer-speciﬁc bias.\nThe recordings range in length from 161to325seconds,\nand they range in quality from cell phone videos to pro-\nfessionally recorded performances. All audio tracks were\nconverted to mono wav format with 22050 Hz sampling\nrate. In total, there is approximately 2hours and 20min-\nutes of annotated audio data.\n3.2 Data Preparation\nOnce the audio data was collected, there were two addi-\ntional steps needed to prepare the data for use in our exper-\niments.\nThe ﬁrst preparation step was to generate beat-level an-\nnotations. The annotations were done in SonicVisualizer4\nby three different individuals with extensive training in\nclassical piano. We kept only those beats that had two or\nmore independent annotations, and we use the mean anno-\ntated time as the ground truth.\nThe second data preparation step was to divide the solo\nrecordings into segments. Recall that the input to the sys-\ntem is a set of contiguous segments of music where the\nsoloist is active. Each segment is speciﬁed by a pair of\nunique identiﬁers (e.g. start at measure 5 beat 1 and end at\nmeasure 37 beat 4), and the segments are non-overlapping.\nFor each composition, we manually selected segments by\nidentifying natural breakpoints where a violinist would\nlikely end a segment, such as section boundaries or the\nstart/end of a long rest.\nWe can summarize the prepared data set as follows.\nEach query in the benchmark is a pairing of a full-mix\nrecording and a solo recording (i.e. the 4-5 segments from\na solo recording). There are thus a total of 87queries in\nthe benchmark. This is clearly not a large data set. It is\nmeant to serve as a pilot data set to assess the feasibility of\nthe proposed system.\n4http://www.sonicvisualiser.org/82 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017tolerance global subseq segmental\n1s 40.2% 8.4% 2.2%\n2s 20.2% 6.1% 0.0%\n5s 14.9% 6.1% 0.0%\n10s 9.3% 6.1% 0.0%\nTable 2 . Boundary prediction error rates for global, subse-\nquence, and segmental DTW algorithms. Each entry indi-\ncates the percentage of predicted boundary points that are\nincorrect at a speciﬁed allowable error tolerance.\n3.3 Evaluation Metric\nIn this paper, we will focus only on the aspects of the\nsystem that can be evaluated objectively: the segment\nboundaries and frame-level alignments. To evaluate seg-\nment boundary predictions, we compare the predicted and\nground truth boundary points for each solo segment, and\nthen determine what fraction of predicted boundary points\nare correct (or incorrect) for a given allowable error toler-\nance. To evaluate frame-level alignments, we compare pre-\ndicted and ground truth timestamps in the full-mix record-\ning that correspond to the annotated beat locations in the\nsolo segments.5We then determine what fraction of align-\nments are correct (or incorrect) for a given allowable error\ntolerance. By considering a range of different error toler-\nances, we can determine an error tolerance curve. Note that\nthe error tolerances for the segment boundary metric are\nmuch larger than the error tolerances for frame alignment,\nsince the former is measuring retrieval at the segment level.\n4. RESULTS\nTo assess the effectiveness of the proposed segmental\nDTW algorithm, we compared its performance against two\nother baseline systems. The ﬁrst baseline system is to sim-\nply concatenate all of the solo audio segments and perform\na single global DTW against the full-mix recording. For\nthis baseline system, we use transition steps (0;1),(1;0),\nand (1;1)in order to handle the discontinuities between\nsolo segments. All steps are given equal weight. The sec-\nond baseline system is to perform subsequence DTW on\neach solo segment independently, where the best locally\noptimal path in each cost matrix is taken as the predicted\nsegment-level and frame-level alignment. In order to make\nthe comparison between systems fair, all three systems use\nthe same chroma features. Any differences in performance\nshould thus reﬂect the effectiveness of the matching algo-\nrithm.\nTable 2 compares the performance of the three systems\non passage identiﬁcation. The rows in the table show the\npercentage of predicted boundary points that are incor-\nrect at four different error tolerances. The three rightmost\ncolumns compare the performance of the global DTW\nbaseline (‘global’), the subsequence DTW baseline (‘sub-\n5Since the annotated beat locations generally fall between frames, we\nuse simple linear interpolation between the nearest predicted alignments.\nFigure 2 . Error tolerance curves for the global, subse-\nquence, and segmental DTW algorithms. Each point on\na curve indicates the percentage of predicted beat align-\nments that are incorrect for a given error tolerance. An\nadditional curve is shown for an oracle system, which pro-\nvides a lower bound on performance.\nseq’), and the proposed segmental DTW algorithm (‘seg-\nmental’).\nThere are three things to notice about Table 2. First, the\nerror rates clearly decrease from left to right. Thus, the rel-\native performance of the three algorithms is clear: global\nDTW performs worst, subsequence DTW performs better,\nand segmental DTW performs best. Second, subsequence\nDTW reaches an asymptotic error rate of 6:1%. These er-\nrors are passages that the subsequence DTW algorithm is\nmatching incorrectly because it fails to take into account\nthe temporal ordering of the solo segments. For example,\nit incorrectly matches the main theme to the recapitulation\nor matches repeated segments to the wrong repetition. Bet-\nter features are unlikely to ﬁx these errors. Third, the seg-\nmental DTW algorithm has perfect performance for error\ntolerances of 2 seconds and above. This suggests that the\n2:2%of errors at a 1 second error tolerance are an indica-\ntion of poor alignments but correctly identiﬁed passages.\nWe will investigate these errors in the discussion section.\nFigure 2 compares the performance of the three systems\non temporal alignments. The ﬁgure shows the error toler-\nance curves for error tolerances ranging from 0to250ms.\nEach point on a curve indicates the percentage of predicted\nbeat timestamps that are incorrect at a given error toler-\nance. There is also a curve for an oracle system, which\nwill be explained in section 5.2.\nThere are three things to notice about Figure 2. First,\nthe curves are identical for error tolerances <25ms. This\nindicates that when an algorithm is “locked onto” a sig-\nnal, the limit to its precision is the same for all three algo-\nrithms. This is what we expect, since all three algorithms\nare based on the same fundamental dynamic programming\napproach and use the same features. This is a realm where\nthe segmental DTW algorithm does not help, but where\nbetter features are needed to improve performance. Sec-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 83ond, the curves begin to diverge signiﬁcantly for error tol-\nerances >50ms. This is a realm where the segmental\nDTW algorithm provides signiﬁcant beneﬁt to system per-\nformance. For example, at 100ms error tolerance, the seg-\nmental DTW algorithm improves the error rate from 22:6%\nand17:1%to12:4%. Third, the curves do not intersect. In\nother words, the segmental DTW algorithm provides uni-\nlateral beneﬁt across all error tolerances.\n5. DISCUSSION\nIn this section, we investigate three questions of interest\nthat will give deeper insight into system performance.\n5.1 Investigation of Boundary Errors\nThe ﬁrst question of interest is: “What is causing the seg-\nment boundary errors?” We saw from Table 2 that 2:2%of\npredicted segment boundaries are incorrect at an error tol-\nerance of 1 second. We investigated all of these errors to\ndetermine the root cause of the problem.\nThere are three main observations we can make from\nour investigations of segment boundary errors. First, most\nsegment boundary errors are a result of a mistake on the\npart of the musician. In one instance, the violin player\nmesses up and stops playing for 3-4 beats at the end of\na phrase. In another instance, the group is very out of sync\non the last note. These two speciﬁc mistakes caused more\nthan 50% of the segment boundary errors, since a single\nmistake will cause errors on all of the queries that con-\ntain the recording. Second, the maximum tempo ratio of\n2x imposed by the DTW step sizes causes errors when the\ninstantaneous tempo difference is extreme. For example,\none recording has a very pronounced rubato at the end of\nthe piece, which causes problems when the recording is\npaired with a performance that has very little rubato at the\nend. Third, all of the segment boundary errors were pre-\ndictions of the end of a segment. The DTW algorithm (and\nits variants) do well in smoothing out errors in the begin-\nning and middle of segments, but it often fails at the end of\na segment because there is no signal “on the other side” to\nsmooth out the prediction.\n5.2 Lower Bound on Error Rate\nThe second question of interest is: “What is the lower\nbound on error rate?” In other words, what is the best\nerror rate that we could hope to achieve given a current\nstate-of-the-art alignment system? In order to answer this\nquestion, we ran an experiment with two major changes.\nThe ﬁrst change is that we assume this system is an oracle\nand knows the ground truth segment boundaries for each\nsolo segment. The second change is that we use an align-\nment system [22] that was designed to maximize alignment\nprecision in an off-line context. Note that this oracle sys-\ntem requires more than 45sec on average to align each\nquery (i.e. align multiple solo recordings against a full-mix\nrecording), so it would not be suitable given the runtime\nconstraints of our user-facing application. (In contrast, our\nproposed system required an average of 5:20sec.) Thus,we can interpret the performance of the oracle system as\na lower bound on error rate when runtime constraints are\nignored.\nThe performance of this oracle system is shown in\nFigure 2 (overlaid on the same ﬁgure from the results sec-\ntion). There are two things to point out about this lower\nbound curve. First, the proposed system approximately\nachieves the lower bound for error tolerances >175ms.\nSecond, the lower bound shows the most room for im-\nprovement in the 50to100ms error tolerance range. For a\n75ms error tolerance, the proposed system and oracle sys-\ntem achieve error rates of 17:8%and14:0%, respectively.\n5.3 Listening to the Accompaniment Track\nThe third question of interest is: “How does the time-scale\nmodiﬁed accompaniment track actually sound?” One use-\nful way we can get a sense of how well the accompaniment\nis “following” the solo recordings is to create a stereo track\nin which one channel contains the unchanged solo record-\ning and the other channel contains the time-stretched ac-\ncompaniment track. By listening to both tracks simulta-\nneously, we can gain an intuitive sense of how well the\nsystem is doing. We have posted several samples of these\nstereo recordings for interested readers.6\nThere are three qualitative observations we can make\nregarding these informal listening tests. First, the system\nperforms much more erratically when the solo part is not\ndominant. This was particularly a problem for the Bach\ndouble violin concerto since there are two equally impor-\ntant violin parts. When the 2ndviolin part is dominant, the\naccompaniment track has signiﬁcantly more time-warping\nartifacts. Second, the system handles rapid notes very well\nand prolonged notes very poorly. When the solo part is\nholding a single long note, the accompaniment track would\nsometimes have very severe temporal distortion artifacts.\nThird, the time-stretched accompaniment track often has\na “jerky” tempo, especially when the solo part has a pro-\nlonged note. The accompaniment track is clearly tracking\nthe solo recordings, but it often has short, sudden bursts of\ntempo speedups and slowdowns. One way to address this\nissue would be to do some type of temporal smoothing of\nthe predicted alignment.\n6. CONCLUSION\nWe have described a system that time-scale modiﬁes an\nexisting full-mix recording to run synchronously to an or-\ndered set of solo-only user recordings of the same piece.\nWe propose a segmental DTW algorithm that simultane-\nously solves the passage identiﬁcation and temporal align-\nment problems, and we demonstrate the beneﬁt of this al-\ngorithm over two other baseline systems on a pilot data set\nof classical violin music. Areas of future work include ex-\npanding the pilot data set, exploring features that are both\ncomputationally efﬁcient and well-suited to the asymmet-\nric nature of the scenario, and investigating pre-processing\nsteps for solo detection and separation.\n6http://pages.hmc.edu/ttsai84 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20177. ACKNOWLEDGMENTS\nThanks to Zhepei Wang and Thitaree Tanprasert for\nhelping with the data annotation. The International\nAudio Laboratories Erlangen are a joint institution of\nthe Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg\n(FAU) and Fraunhofer Institut f ¨ur Integrierte Schaltungen\nIIS.\n8. REFERENCES\n[1] Andreas Arzt and Gerhard Widmer. Real-time music\ntracking using multiple performances as a reference.\nInProc. of the International Conference on Music In-\nformation Retrieval (ISMIR) , pages 357–363, M ´alaga,\nSpain, 2015.\n[2] Michael A. Casey, Christophe Rhodes, and Mal-\ncolm Slaney. Analysis of minimum distances in high-\ndimensional musical spaces. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 16(5):1015–\n1028, 2008.\n[3] Arshia Cont, Jos ´e Echeveste, and Jean-Louis Giavitto.\nThe Cyber-Physical System Approach for Automatic\nMusic Accompaniment in Antescofo. In Acoustical So-\nciety Of America , Providence, Rhode Island, United\nStates, May 2014.\n[4] Roger B. Dannenberg and Ning Hu. Polyphonic au-\ndio matching for score following and intelligent audio\neditors. In Proc. of the International Computer Mu-\nsic Conference (ICMC) , pages 27–34, San Francisco,\nUSA, 2003.\n[5] Simon Dixon. Live tracking of musical performances\nusing on-line time warping. In Proc. of the 8th Interna-\ntional Conference on Digital Audio Effects , pages 92–\n97. Citeseer, 2005.\n[6] Mark Dolson and Jean Laroche. Improved phase\nvocoder time-scale modiﬁcation of audio. IEEE Trans-\nactions on Speech and Audio Processing , 7(3):323–\n332, 1999.\n[7] Jonathan Driedger and Meinard M ¨uller. Improv-\ning time-scale modiﬁcation of music signals using\nharmonic-percussive separation. IEEE Signal Process-\ning Letters , 21(1):105–109, 2014.\n[8] Jonathan Driedger and Meinard M ¨uller. A review on\ntime-scale modiﬁcation of music signals. Applied Sci-\nences , 6(2):57–82, February 2016.\n[9] Sebastian Ewert and Meinard M ¨uller. Reﬁnement\nstrategies for music synchronization. In Proceedings of\nthe International Symposium on Computer Music Mod-\neling and Retrieval (CMMR) , volume 5493 of Lecture\nNotes in Computer Science , pages 147–165, Copen-\nhagen, Denmark, May 2008.[10] Sebastian Ewert, Meinard M ¨uller, and Peter Grosche.\nHigh resolution audio synchronization using chroma\nonset features. In Proc. of IEEE International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , pages 1869–1872, Taipei, Taiwan, April\n2009.\n[11] Christian Fremerey, Meinard M ¨uller, and Michael\nClausen. Handling repeats and jumps in score-\nperformance synchronization. In Proc. of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 243–248, Utrecht, The Netherlands, 2010.\n[12] Ning Hu, Roger B. Dannenberg, and George Tzane-\ntakis. Polyphonic audio matching and alignment for\nmusic retrieval. In Proc. of the IEEE Workshop on Ap-\nplications of Signal Processing to Audio and Acoustics\n(WASPAA) , New Paltz, NY , USA, 2003.\n[13] Fumitada Itakura. Minimum prediction residual prin-\nciple applied to speech recognition. IEEE Transac-\ntions on Acoustics, Speech, and Signal Processing ,\n23(1):67–72, 1975.\n[14] Frank Kurth and Meinard M ¨uller. Efﬁcient index-based\naudio matching. IEEE Transactions on Audio, Speech,\nand Language Processing , 16(2):382–395, February\n2008.\n[15] Robert Macrae and Simon Dixon. Accurate real-time\nwindowed time warping. In Proc. of the International\nConference on Music Information Retrieval (ISMIR) ,\npages 423–428, 2010.\n[16] Meinard M ¨uller. Fundamentals of Music Processing:\nAudio, Analysis, Algorithms, Applications . Springer,\n2015.\n[17] Meinard M ¨uller and Daniel Appelt. Path-constrained\npartial music synchronization. In Proc. of the Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP) , volume 1, pages 65–68, Las Ve-\ngas, Nevada, USA, April 2008.\n[18] Meinard M ¨uller and Sebastian Ewert. Joint structure\nanalysis with applications to music annotation and\nsynchronization. In Proc. of the International Confer-\nence on Music Information Retrieval (ISMIR) , pages\n389–394, Philadelphia, Pennsylvania, USA, Septem-\nber 2008.\n[19] Meinard M ¨uller, Frank Kurth, and Michael Clausen.\nAudio matching via chroma-based statistical features.\nInProc. of the International Conference on Music In-\nformation Retrieval (ISMIR) , pages 288–295, London,\nUK, 2005.\n[20] Meinard M ¨uller, Frank Kurth, and Michael Clausen.\nChroma-based statistical audio features for audio\nmatching. In Proc. of the Workshop on Applications\nof Signal Processing (WASPAA) , pages 275–278, New\nPaltz, New York, USA, October 2005.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 85[21] Meinard M ¨uller, Henning Mattes, and Frank Kurth. An\nefﬁcient multiscale approach to audio synchronization.\nInProc. of the International Conference on Music In-\nformation Retrieval (ISMIR) , pages 192–197, Victoria,\nCanada, October 2006.\n[22] Thomas Pr ¨atzlich, Jonathan Driedger, and Meinard\nM¨uller. Memory-restricted multiscale dynamic time\nwarping. In Proc. of the IEEE International Confer-\nence on Acoustics, Speech, and Signal Processing\n(ICASSP) , pages 569–573, Shanghai, China, 2016.\n[23] Christopher Raphael. Music plus one and machine\nlearning. In Proc. of the International Conference on\nMachine Learning (ICML) , pages 21–28, 2010.\n[24] Christopher Raphael and Yupeng Gu. Orchestral ac-\ncompaniment for a reproducing piano. In Proc. of the\nInternational Computer Music Conference (ICMC) ,\n2009.\n[25] Hiroaki Sakoe and Seibi Chiba. Dynamic program-\nming algorithm optimization for spoken word recog-\nnition. IEEE Transactions on Acoustics, Speech, and\nSignal Processing , 26(1):43–49, 1978.\n[26] Stan Salvador and Philip Chan. FastDTW: Toward ac-\ncurate dynamic time warping in linear time and space.\nInProc. of the KDD Workshop on Mining Temporal\nand Sequential Data , 2004.\n[27] Siying Wang, Sebastian Ewert, and Simon Dixon. Ro-\nbust and efﬁcient joint alignment of multiple musi-\ncal performances. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , 24(11):2132–2145,\n2016.86 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Lyrics-Based Music Genre Classification Using a Hierarchical Attention Network.",
        "author": [
            "Alexandros Tsaptsinos"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417241",
        "url": "https://doi.org/10.5281/zenodo.1417241",
        "ee": "https://zenodo.org/records/1417241/files/Tsaptsinos17.pdf",
        "abstract": "Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Re- trieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure—in which words combine to form lines, lines form segments, and segments form a complete song—we adapt a hierarchical attention network (HAN) to exploit these layers and in ad- dition learn the importance of the words, lines, and seg- ments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher num- ber of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a com- putational perspective, into lyrical structure and language features that differentiate musical genres.",
        "zenodo_id": 1417241,
        "dblp_key": "conf/ismir/Tsaptsinos17",
        "keywords": [
            "Music genre classification",
            "recurrent neural network models",
            "lyrics alone",
            "Music Information Retrieval",
            "hierarchical attention network",
            "lyric structure",
            "language features",
            "genre classification",
            "words",
            "lines"
        ],
        "content": "LYRICS-BASED MUSIC GENRE CLASSIFICATION USING A\nHIERARCHICAL ATTENTION NETWORK\nAlexandros Tsaptsinos\nICME, Stanford University, USA\nalextsap@stanford.edu\nABSTRACT\nMusic genre classiﬁcation, especially using lyrics alone,\nremains a challenging topic in Music Information Re-\ntrieval. In this study we apply recurrent neural network\nmodels to classify a large dataset of intact song lyrics.\nAs lyrics exhibit a hierarchical layer structure—in which\nwords combine to form lines, lines form segments, and\nsegments form a complete song—we adapt a hierarchical\nattention network (HAN) to exploit these layers and in ad-\ndition learn the importance of the words, lines, and seg-\nments. We test the model over a 117-genre dataset and a\nreduced 20-genre dataset. Experimental results show that\nthe HAN outperforms both non-neural models and simpler\nneural models, whilst also classifying over a higher num-\nber of genres than previous research. Through the learning\nprocess we can also visualise which words or lines in a\nsong the model believes are important to classifying the\ngenre. As a result the HAN provides insights, from a com-\nputational perspective, into lyrical structure and language\nfeatures that differentiate musical genres.\n1. INTRODUCTION\nAutomatic classiﬁcation of music is an important and\nwell-researched task in Music Information Retrieval\n(MIR) [25]. Previous work on this topic has focused\nprimarily on classifying mood [13], genre [21], annota-\ntions [27], and artist [9]. Typically one or a combination\nof audio, lyrical, symbolic, and cultural data is used in ma-\nchine learning algorithms for these tasks [23].\nGenre classiﬁcation using lyrics presents itself as a nat-\nural language processing (NLP) problem. In NLP the aim\nis to assign meaning and labels to text; here this equates\nto a genre classiﬁcation of the lyrical text. Traditional ap-\nproaches in text classiﬁcation have utilised n-gram models\nand algorithms such as Support Vector Machines (SVM),\nk-Nearest Neighbour (k-NN), and Na ¨ıve Bayes (NB).\nIn recent years the use of deep learning methods such as\nrecurrent neural networks (RNNs) or convolutional neural\nnetworks (CNNs) has produced superior results and rep-\nresent an exciting breakthrough in NLP [16, 17]. Whilst\nc\rAlexandros Tsaptsinos. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nAlexandros Tsaptsinos. “Lyrics-Based Music Genre Classiﬁcation Using\nA Hierarchical Attention Network”, 18th International Society for Music\nInformation Retrieval Conference, Suzhou, China, 2017.linear and kernel models rely on good hand-selected fea-\ntures, these deep learning architectures circumvent this by\nletting models learn important features themselves.\nDeep learning has in recent years been utilised in sev-\neral MIR research topics including live score following [7],\nmusic instrument recognition [20], and automatic tagging\n[3]. In many cases, these approaches have led to signiﬁ-\ncant improvements in performance. For example, Kum et\nal. [18] utilise multi-column deep neural networks to ex-\ntract melody on vocal segments while Southall et al. [34]\napproach automatic drum transcription using bidirectional\nrecurrent neural networks.\nNeural methods have further been utilised for the genre\nclassiﬁcation task on audio and symbolic data. Sigtia and\nDixon [31] use the hidden states of a neural network as\nfeatures for song on which a Random Forest classiﬁer\nwas built, reporting an accuracy of 83% among 10 genres.\nCosta et al. [6] compare the performance of CNNs in genre\nclassiﬁcation through spectrograms with respect to results\nobtained through hand-selected features and SVMs. Jeong\nand Lee [14] learn temporal features in audio using a deep\nneural network and apply this to genre classiﬁcation. How-\never, not much research has looked into the performance\nof these deep learning methods with respect to the genre\nclassiﬁcation task on lyrics. Here, we attempt to remedy\nthis situation by extending deep learning approaches to text\nclassiﬁcation to the particular case of lyrics.\nHierarchical methods attempt to use some sort of struc-\nture of the data to improve the models and have previously\nbeen utilised in vision classiﬁcation tasks [30]. Yang et\nal. [37] propose a hierarchical attention network (HAN)\nfor the task of document classiﬁcation. Since documents\noften contain structure whereby words form to create sen-\ntences, sentences to paragraphs, etc. they introduce this\nknowledge to the model, resulting in superior classiﬁca-\ntion results. It is evident that songs and, in particular, lyrics\nsimilarly contain a hierarchical composition: Words com-\nbine to form lines, lines combine to form segments, and\nsegments combine to form the whole song. A segment of a\nsong is a verse, chorus, bridge, etc. of a song and typically\ncomprises several lines. The hierarchical nature of songs\nhas been previously exploited in genre classiﬁcation tasks\nwith Du et al. [8] utilising hierarchical analysis of spectro-\ngrams to help classify genre.\nHere, we propose application of an HAN for genre clas-\nsiﬁcation of intact lyrics. We train such a network, allow-\ning it to apply attention to words, lines, and segments. Re-694sults show the network produces higher accuracies in the\nlyrical classiﬁcation task than previous research and from\nthe attention learned by the network we can observe which\nwords are indicative of different genres.\nThe remainder of the paper is structured as follows. In\nSection 2 we describe our methods, including the dataset\nand a description of the HAN. In Section 3 we provide re-\nsults and visualisations from our experiments. We con-\nclude with a discussion in Section 4.\n2. METHODS\n2.1 Dataset\nResearch involving song lyrics has historically suffered\nfrom copyright issues. Consequently most previous liter-\nature has utilised count-based bag-of-words lyrics. In this\nformat, structure and word order are lost, and it has been\nshown that utilising intact lyrics reveals superior results in\nclassiﬁcation tasks [11, 32].\nSeeking an intact lyrics corpus for the present study, we\nobtained a collection of lyrics through a signed research\nagreement with LyricFind1. This corpus has been used in\nthe past to study novelty [10] and inﬂuence [1] in lyrics.\nThe complete set contained 1,039,151 song lyrics in JSON\nformat, as well as basic metadata including artist(s) and\ntrack name. As the corpus provided no genre information,\nwe aggregated it ourselves using the iTunes Search API2,\nextracting the value for the primaryGenreName key as\nbaseline truth. Several different sources were not used for\nconsistency reasons with iTunes found to be the largest,\neasily accessible source with reasonable genre tags. This\nunfortunately still greatly reduced the size of the dataset\ndue to the sparse iTunes database. We then further re-\nmoved any songs that were linked with a genre tag of ‘Mu-\nsic Video’, leaving a dataset comprising 244 genres. As\nthis dataset had a very long tail of sparse genres, we fur-\nther ﬁlter the dataset via two methods. Firstly we remove\nany genres with less than 50 instances, giving a dataset\nof size 495,188 lyrics and 117 genres. Secondly we re-\ntain only the top 20 genres, giving a dataset of 449,458\nlyrics. We note also that the dataset originally contained\nvarious versions of the same lyrics, due to the prevalence\nof cover songs; we retain only one of these versions cho-\nsen at random. The song lyrics are split into lines and seg-\nments which we tokenised using the nltk package3in\nPython. We split the dataset into a rough split of 80% for\ntraining, 10% for validation, and 10% for testing. All pre-\nprocessing was done via Python with the neural networks\nbuilt using Tensorﬂow4.\n2.2 Hierarchical Attention Networks\nThe structure of the model follows that of Yang et al. [37].\nEach layer is run through a bidirectional gated recurrent\n1http://lyricfind.com/\n2http://apple.co/1qHOryr\n3http://www.nltk.org/\n4https://www.tensorflow.org/\nHappy      Birthday    to              you\nA\nA\nA\nA\nB\nB\nB\nB}Happy      Birthday    to              you\nA\nA\nA\nA\nB\nB\nB\nB}Happy      Birthday    dear          Lucy\nA\nA\nA\nA\nB\nB\nB\nB}Happy      Birthday    to              you\nA\nA\nA\nA\nB\nB\nB\nB}\nC\nD\nE\nC\nD\nE\nC\nD\nE\nC\nD\nE}\nFWord BiGRUWord Attention\nLine  BiGRU\nLine Attention“Genre”SoftmaxFigure 1 : Representation of the HAN architecture; boxes\nrepresent vectors. A and B vectors represent the hidden\nstates for the forward and backward pass of the GRU at\nthe word level, respectively. The line vectors C are then\nobtained from these hidden states via the attention mech-\nanism. The D and E vectors represent the forward and\nbackward pass of the GRU at the line level, respectively.\nThe song vector F is then obtained from these hidden states\nvia the attention mechanism. Finally classiﬁcation is per-\nformed via the softmax activation function.\nunit (GRU) with attention applied to the output. The at-\ntention weights are used to create a vector via a weighted\nsum which is then passed as the input to the next layer. A\nrepresentation of the architecture for the example song of\n‘Happy Birthday’ can be seen in Figure 1, where the lay-\ners are applied at the word, line, and song level. We brieﬂy\nstep through the various components of the model.\n2.2.1 Word Embeddings\nAn important idea in NLP is the use of dense vectors to\nrepresent words. A successful methodology proposes that\nsimilar words have similar context and thus vectors can\nbe learned through their context, such as in the word2vec\nmodel [26]. Pennington et al. [29] propose the GloVe\nmethod which combines global matrix factorisation and lo-\ncal context window methods to produce word vectors that\noutperform previous word2vec and SVM based models.\nHere we take as our vocabulary the top 30,000 most fre-\nquent words from the whole LyricFind corpus, including\nthose from songs we did not match with a genre. We train\n100-dimensional GloVe embeddings for these words using\nmethods obtained from the GloVe website5. Previous re-\nsearch has shown that retraining these word vectors over\nthe extrinsic task at hand can improve results if the dataset\nis large enough [5]. In a preliminary genre classiﬁcation\ntask we found that retraining these word embeddings did\nimprove accuracy, and so we let our model learn superior\nembeddings to those provided by GloVe [29].\n2.2.2 Gated Recurrent Units\nIntroduced by Chung et al. [4], GRUs are a form of gat-\ning mechanism in RNNs designed to help overcome the\n5http://nlp.stanford.edu/projects/glove/Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 695struggle to capture long-term dependencies in RNNs. This\nis achieved by the introduction of intermediate states be-\ntween the hidden states in the RNN. An update gate ztis\nintroduced to help determine how important the previous\nhidden state is to the next hidden state. A reset gate rtis\nintroduced to help determine how important the previous\nhidden state is in the creation of the next memory. The\nhidden state is ht, whilst new memory is computed and\nstored in ~ht. Mathematically we describe the process as\nzt=sigmoid (Wzxt+Uzht\u00001+bz) (1)\nrt=sigmoid (Wrxt+Urht\u00001+br) (2)\n~ht=tanh(Whxt+rt\u000eUhht\u00001+bh) (3)\nht= (1\u0000zt)\u000eht\u00001+zt\u000e~ht; (4)\nwherextis the word vector input at time-step t,\u000eis the\nHadamard product, and sigmoid is the sigmoid activation\nfunction.Wz,Uz,Wr,Ur,Wh, andUhare weight ma-\ntrices randomly initialised and to be learned by the model\nalong with the bz,br, andbhbias terms. Bias terms were\nnot included in the original model by Chung et al. [4], how-\never have been included here as in Jozefowicz et al. [15].\n2.2.3 Hierarchical Attention\nAttention was ﬁrst proposed by Bahdanau et al. [2] with\nrespect to neural machine translation to allow the model to\nlearn which words were more important in the translation\nobjective. Along the lines of that study, we would like our\nmodel to learn which words are important in classifying\ngenre and then apply more weight to these words. Sim-\nilarly, we can apply attention again on lines or segments\nto let the model learn which lines or segments are more\nimportant in classiﬁcation.\nGiven input vectors hifori= 1;:::;n the attention\nmechanism can be formulated as\nui=tanh(Wahi+ba) (5)\n\u000bi=exp(uT\niua)Pn\nk=1exp(uT\nkua)(6)\ns=nX\ni=1\u000bihi; (7)\nwheresis the output vector passed to the next layer con-\nsisting of the weighted sum of the current layers vectors.\nParametersWa,ba, anduaare learned by the model after\nrandom initialisation.\nOne layer of the network takes in vectors x1;:::;x n,\napplies a bidirectional GRU to ﬁnd a forward hidden state\u0000 !hjand a backward hidden state \u0000hj, and then uses the at-\ntention mechanism to form a weighted sum of these hidden\nstates to output as the representation. Letting GRU indi-\ncate the output of a GRU and ATT represent the output\nfrom an attention mechanism, one layer is formulated as\n\u0000 !hj=\u0000\u0000\u0000!GRU (xj); (8)\n \u0000hj= \u0000\u0000\u0000GRU (xj); (9)\nhj= [\u0000 !hj; \u0000hj]; (10)\ns=ATT (h1;:::;h L): (11)Our HAN consists of two layers, one at the word level, and\none at the line/segment level. Consider a song of Llines\nor segments sj, each consisting of njwordswij. LetE\nbe the pre-trained word embedding matrix. Letting LAY\nrepresent the dimension reduction operation of a layer in\nthe network as in Eqns 8–11 the whole HAN can be for-\nmulated for i= 1;:::;n jandj= 1;:::;L as\nxij=Ewij (12)\nsj=LAY (x1j;:::;x njj); (13)\ns=LAY (s1;:::;s L): (14)\nEach layer has its own set of GRU weight matrix and bias\nterms to learn, as well as its own attention weight matrix,\nbias terms, and relevance vector to learn.\n2.2.4 Classiﬁcation\nWith the song vector snow obtained, classiﬁcation is per-\nformed by using a ﬁnal softmax layer\np=softmax (Wps+bp); (15)\nwhere intuitively we take the entry of highest magnitude\nas the prediction for that song. To train the model we min-\nimise over cross-entropy loss.\n3. EXPERIMENTS\n3.1 Baseline Models\nWe compare the performance of the HAN against various\nbaseline models.\n1. Majority classiﬁer (MC): ‘Rock’ is the most common\ngenre in our dataset. The MC simply predicts ‘Rock’.\n2. Logistic regression (LR): A LR run on the average song\nword vector produced from the GloVe embeddings.\n3. Long Short-Term Memory (LSTM): An LSTM, treat-\ning the whole song as a single sequence of words and\nuse max-pooling of the hidden states for classiﬁcation.\nFifty hidden units were used in the LSTM and each\nsong had a maximum of 600 words. For full discussion\nof the LSTM framework see Hochreiter and Schmidhu-\nber [12].\n4. Hierarchical network (HN-L): The HN structure in the\nabsence of attention run at the line level. At each layer\nall of the representations are simply averaged to pro-\nduce the next layer input.\nFor LR, LSTM, and HN-L we let the model retrain the\nword embeddings as it trained.\n3.2 Model Conﬁguration\nThe lyrics are padded/truncated to have uniform length. In\nthe line model, each line has a maximum of 10 words and\na maximum of 60 lines. In the segment model each seg-\nment has a maximum of 60 words and a maximum of 10\nsegments. Fifty hidden units are utilised in the bidirec-\ntional GRUs, whilst one hundred states are output from the696 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Model 117 Genres 20 Genres\nMC 24.71 27.17\nLR 35.21 38.13\nLSTM 43.66 49.77\nHN-L 45.85 49.09\nHAN-L 46.42 49.50\nHAN-S 45.05 47.60\nTable 1 : Genre classiﬁcation test accuracies for the two\ndatasets (%) using majority classiﬁer (MC), logistic regres-\nsion (LR), Long Short-Term Model (LSTM), hierarchical\nnetwork (HN-L), and line- and segment-level HAN (HAN-\nL, HAN-S).\nattention mechanisms. Before testing the model, hyper-\nparameters were tuned on the validation set. Dropout [35]\nand gradient clipping [28] were both found to beneﬁt the\nmodel. We dropout at each layer with probability p= 0:5\nand gradients are clipped at a maximum norm of 1 in the\nbackpropogation. We utilise a mini-batch size of 64 and\noptimise using RMSprop [36] with a learning rate of 0:01.\nThe models were all run until their validation loss did not\ndecrease for 3 successive epochs. In all the HAN models,\nthis occurred between the 5th and 8th epoch.\nThe code to train the model and perform the experi-\nments described are made publicly available6.\n3.3 Results\nFor both dataset sizes we run the baseline models and the\nHAN at the line and segment level. Let HAN-L represent\nrunning over lines and HAN-S represent running over seg-\nments. The test accuracies are seen in Table 1.\nFrom the results we see a trend between model com-\nplexity and classiﬁcation accuracy. The very simple major-\nity classiﬁer performs weakest and is improved upon by the\nsimple logistic regression on average bag-of-words. The\nneural-based models perform better than both of the simple\nmodels. The LSTM model, which takes into account word\norder and tries to implement a memory of these words,\ngives performances of 43.66% and 49.77%, outperform-\ning the HAN on the 20-genre dataset. Over the 117-genre\ndataset the best performing models were the HANs, with\na highest accuracy of 46.42% when run over lines. It is\nobserved that for the simpler 20-genre case, the more com-\nplex HAN is not required since the simpler LSTM beats\nit, although the LSTM took almost twice as long to train\nas the HAN. However for the more challenging 117-genre\ncase, the HAN-L outperforms the LSTM, perhaps picking\nup on more of the intricacies of rarer genres.\nIn both cases the HAN run at the line level produced su-\nperior results than that run over the segment level, giving\na bump of roughly 1.4% and 1.9% in the 117-genre and\n20-genre datasets, respectively. The HN-L, which is run\nat the line level, additionally outperforms the HAN at seg-\nment level. This indicates that the model performs better\nwhen looking at songs line by line rather than segment by\n6https://github.com/alexTsaptsinos/lyricsHAN\nFigure 2 : HAN-L confusion matrix for Rock, Pop, Al-\nternative (Alt), Country, and Hip-Hop/Rap (HHR) genres\nover larger (117-genre) dataset. Rows represent true genre,\nwhilst columns are predicted.\nsegment. In the HAN-L the model can pick up on many\nrepeated lines or lines of a similar ilk, rather than the few\nsimilar segments it attains in the HAN-S, and this may be\nattributive to the better performance. The network does\nbeneﬁt from the inclusion of attention, with HAN-L clas-\nsifying with higher accuracies than HN-L. This increase is\nmarginal and requires an increased cost, however allows\nfor the extraction of attention in the visualisations of the\nfollowing section.\nAs expected, classifying over the 20-genre dataset has\ngiven boosts of roughly 3% and 2.5% in the HAN-L and\nHAN-S, respectively. It is interesting to note that dis-\ncarding roughly 10% of the data by only keeping roughly\na sixth of the genres has not strengthened the model by\nmuch. Given the similarity of recognition performance be-\ntween the two datasets, even with the simplest of models,\nit is likely that the extra genres are predominantly noise\nadded to the 20-genre dataset. With the HAN-L outper-\nforming the LSTM over the 117-genre dataset this then in-\ndicates that the model is more robust to noise.\nThe confusion matrix for HAN-L run over the larger\ndataset for the top 5 genres can be seen in Figure 2. We can\nsee from the matrix that Rock, Pop, and Alternative (Alt)\nare all commonly confused; the model predicts Rock for\nAlternative almost as many times as it does Alternative. As\nthe most common genre in the dataset by about 30,000 it\nis unsurprising to see the model try and predict Rock more\noften, and it is unclear whether a person would be able\nto distinguish between the lyrics of these genres. How-\never, we see that both Country and Hip-Hop/Rap (HHR)\nare more separated. With their distinct lyrical qualities,\nespecially in the case of Hip-Hop/Rap, this is an encour-\naging result indicating that the model has learned some of\nthe qualities of both these genres.\n3.3.1 Attention Visualisation\nTo help illustrate the attention mechanism, we feed song\nlyrics into the HAN-L and observe the weights it applies\nto words and lines. For each song we extract the 5 most\nheavily weighted lines and a visualisation of their weights\nand the individual word weights for a few different cor-\nrectly predicted song lyrics can be seen in Figure 3.\nFrom these visualisations we notice that the model has\nplaced greater weights on words we may associate with\na certain genre. For example ‘baby’ and ‘ai’ are weightedProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 6970\n2\n4\n6\n8\n100.115\n0.062\n0.059\n0.048\n0.045Baby you ai n't gon na wan na come back\nWant a bad boy\nWell I 'll be out by your driveway when your\nGot a bad toy sittin ' in the parkin 'Predicted Class: Country, True Class: Country\n0.050.100.150.200.250.300.350.40\n0\n2\n4\n6\n8\n100.142\n0.104\n0.065\n0.053\n0.05I 'm gon na spread my word from standin on\n'Cause suckers like you just make me strong\n<unk> it out , y'all\nThis <unk> world , it just ai n't right\nI 'm gon na bust my shoes , I 'mPredicted Class: Hip-Hop/Rap, True Class: Hip-Hop/Rap\n0.10.20.30.40.50.60.70.8\n0\n2\n4\n6\n8\n100.162\n0.151\n0.111\n0.109\n0.004Do you promise not to tell woh woh woh closer\nLet me whisper in your ear\nI 'm in love with you oo\nSay the words you long to hear\nYou 'll never know how much I really love youPredicted Class: Rock, True Class: Rock\n0.10.20.30.40.5Figure 3 : Weights applied by the HAN-L for song lyrics that were correctly classiﬁed. Line weights appear to the left of\neach line and word weights are coloured according to the respective colorbars on the right.\nheavily in the Country song, and the most heavily weighted\nline in that song is characteristically Country. The model\nhas placed great weight on a blank line, indicating the\nbreak between segments; it is unclear whether the model\nis learning to place importance on how songs are seg-\nmented and the number of segments occurring. In the Hip-\nHop/Rap song the model places attention on colloquially\nspelled words ‘cause’ and ‘gonna’. Although not included\nhere, it was observed that for many rap songs swear words\nand racial terms were heavily weighted. The model picks\nup the ‘woh’ and ‘oo’ in the Rock song and also heavily\nweights occurrences of second-person determiner ‘your’\nand pronoun ‘you’. It was found that for many Rock songs\nthis was the case.\nIn addition some visualisations of lyrics that were in-\ncorrectly classiﬁed by the HAN-L can be seen in Figure 4.\nWe observe the model predicting Country for a Pop song,\napplying weights to ‘sin’ and ‘strong’ which could be char-\nacteristic of Country songs. The dataset contains songs\nwith foreign language lyrics. Here we observe a song with\nSpanish lyrics classed as Pop Latino by the model whilst\niTunes deems it Pop. This seems like a fair mistake for\nthe model to have made since it has evidently recognised\nthe Spanish language. The model also incorrectly classi-\nﬁes the Hip-Hop/Rap song as Pop. In the 5 most heavily\nweighted lines we do not spot any instances of language\nthat indicate a Hip-Hop/Rap song and we hypothesise thatthe genericness of the lyrics has led the model to predict\nPop.\n4. DISCUSSION\nGenre is an inherently ambiguous construct, but one that\nplays a major role in categorising musical works [24, 33].\nFrom one standpoint, genre classiﬁcation by lyrics will\nalways be inherently ﬂawed by vague genre boundaries\nand many genres borrowing lyrics and styles from one an-\nother. Previous research has shown that lyrical data per-\nforms weakest in genre classiﬁcation compared to other\nforms of data [23]. As a consequence, this problem is not\nas well researched and preference has been given to other\nmethods.\nSVMs, k-NN, and NB have been heavily used in previ-\nous lyrical classiﬁcation research. In addition very rarely\nhas research looked into classifying more than between 10\ngenres despite the prevalence of clearly many more gen-\nres. Fell and Sporleder classify among 8 genres using\nn-grams along with other hand-selected features to help\nrepresent vocabulary, style, structure, and semantics [11].\nYing et al. make use of POS tags and classify among 10\ngenres using SVMs, k-NN, NB with a highest accuracy of\n39.94% [38]. McKay et al. utilise hand-selected features to\nproduce classiﬁcation accuracies of 69% among 5 genres\nand 43% among 10 genres [23].698 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170\n2\n4\n6\n8\n100.076\n0.075\n0.07\n0.07\n0.06I long to take each breath beside you\nI ai n't strong enough to hide\nReach for me , sweet as sin\nAll at once our worlds <unk>Predicted Class: Country, True Class: Pop\n0.20.40.60.8\n0\n2\n4\n6\n8\n100.048\n0.04\n0.038\n0.036\n0.035Yo que nunca te he <unk>\nY siento que muero por dentro\nY no sé cómo salir de este infierno\nMe falta ilusión en mis días\nSi alguien me salva de este castigo eres túPredicted Class: Pop Latino, True Class: Pop\n0.20.40.60.8\n0\n2\n4\n6\n8\n100.05\n0.048\n0.048\n0.044\n0.044See the kid with the memory he cant\n shake\nThem things that haunt you , let them be\nDo what you want to do if you feel that\nAll your idols were just like youPredicted Class: Pop, True Class: Hip-Hop/Rap\n0.10.20.30.40.50.6Figure 4 : Weights applied by the HAN-L for song lyrics that were incorrectly classiﬁed. Line weights appear to the left of\neach line and word weights are coloured according to the respective colorbars on the right.\nIn this paper we have shown that an HAN and other\nneural-based methods can improve on the genre classiﬁ-\ncation accuracy. In large part this model has beaten all\npreviously reported lyrical-only genre classiﬁcation model\naccuracies, except for the classiﬁcation among 5 gen-\nres. Whilst having been trained on different datasets the\njump in classiﬁcation accuracies achieved by the HAN and\nLSTM across the 20-genre datasets compared to previous\nresearch indicate that neural structures are clearly beneﬁ-\ncial. However, with very similar results between the neural\nstructures it is still unclear what the optimal neural struc-\nture may be and there is certainly room for further exper-\nimentation. We have shown that the HAN works better\nwith layers at the word, line, and song level rather than\nword, segment, and song level. One known issue of the\npresent dataset is that iTunes attributes genres by artist,\nnot by track; this is a problem for artists whose work may\ncover multiple genres and is something that should be ad-\ndressed in the future. A larger issue concerns the accuracy\nof the iTunes genre labels more generally, especially for\nthe larger 117-genre dataset which naturally includes more\nsubjective and vague genre deﬁnitions.\nVisualisations of the weights the HAN applies to words\nand lines were produced to help see what the model was\nlearning. In a good amount of cases, words and lines were\nheavily weighted that were cohesive with the song genre;\nhowever, this was not always the case. We note that in gen-eral the model tended to let one word dominate a single line\nwith the greatest weight. However this was not as apparent\nacross lines, with weights among lines more evenly spread.\nWith a large amount of foreign-language lyrics also present\nin the dataset, an idea for further research is to build a clas-\nsiﬁer that identiﬁes language, and from there classiﬁes by\ngenre. Any such research would be inhibited, however, by\nthe lack of such a rich dataset to train on.\nTo produce a state-of-the-art classiﬁer it is evident that\nthe classiﬁer must take into account more than just the lyri-\ncal content of the song. Mayer et al. combine audio and\nlyrical data to produce a highest accuracy of 63.50% within\n10 genres via SVMs [21]. Mayer and Rauber then use a\ncartesian ensemble of lyric and audio features to gain a\nhighest accuracy of 74.08% within 10 genres [22]. Further\nresearch could look into employing this hierarchical atten-\ntion model to the audio and symbolic data, and combining\nwith the lyrics to build a stronger classiﬁer. Employment of\nthe HAN in the task of mood classiﬁcation via sentiment\nanalysis is another possible area of research. In addition\nthe HAN could be extended to include both a layer at the\nline and segment level, or even at the character level, to\nexplore performance.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 6995. ACKNOWLEDGEMENTS\nMany thanks to Will Mills and Mohamed Moutadayne\nfrom LyricFind for providing access to the data, and the\nISMIR reviewers for their helpful comments.\n6. REFERENCES\n[1] Jack Atherton and Blair Kaneshiro. I said it ﬁrst: Topo-\nlogical analysis of lyrical inﬂuence networks. In IS-\nMIR, pages 654–660, 2016.\n[2] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine\ntranslation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473 , 2014.\n[3] Keunwoo Choi, George Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional neural\nnetworks. arXiv preprint arXiv:1606.00298 , 2016.\n[4] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empir-\nical evaluation of gated recurrent neural networks on\nsequence modeling. arXiv preprint arXiv:1412.3555 ,\n2014.\n[5] R. Collobert, J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. Natural language pro-\ncessing (almost) from scratch. Journal of Machine\nLearning Research , 12(Aug):2493–2537, 2011.\n[6] Y . MG Costa, L. S. Oliveira, and C. N. Silla. An evalu-\nation of convolutional neural networks for music clas-\nsiﬁcation using spectrograms. Applied Soft Computing ,\n52:28–38, 2017.\n[7] Matthias Dorfer, Andreas Arzt, Sebastian B ¨ock,\nAmaury Durand, and Gerhard Widmer. Live score\nfollowing on sheet music images. arXiv preprint\narXiv:1612.05076 , 2016.\n[8] W. Du, H. Lin, J. Sun, B. Yu, and H. Yang. A new\nhierarchical method for music genre classiﬁcation. In\nCISP-BMEI , pages 1033–1037. IEEE, 2016.\n[9] Hamid Eghbal-Zadeh, Markus Schedl, and Gerhard\nWidmer. Timbral modeling for music artist recognition\nusing i-vectors. In EUSIPCO , pages 1286–1290. IEEE,\n2015.\n[10] R. J. Ellis, Fang J. Xing, Z., and Y . Wang. Quantifying\nlexical novelty in song lyrics. In ISMIR , pages 694–\n700, 2015.\n[11] M. Fell and C. Sporleder. Lyrics-based analysis and\nclassiﬁcation of music. In COLING , volume 2014,\npages 620–631, 2014.\n[12] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural computation , 9(8):1735–1780, 1997.\n[13] X. Hu and J. S. Downie. Improving mood classiﬁca-\ntion in music digital libraries by combining lyrics and\naudio. In Proceedings of the 10th annual joint confer-\nence on Digital libraries , pages 159–168. ACM, 2010.\n[14] Il-Young Jeong and Kyogu Lee. Learning temporal fea-\ntures using a deep neural network and its application to\nmusic genre classiﬁcation. In ISMIR , pages 434–440,\n2016.[15] Rafal Jozefowicz, Wojciech Zaremba, and Ilya\nSutskever. An empirical exploration of recurrent net-\nwork architectures. In ICML , pages 2342–2350, 2015.\n[16] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A\nconvolutional neural network for modelling sentences.\narXiv preprint arXiv:1404.2188 , 2014.\n[17] Y . Kim. Convolutional neural networks for sentence\nclassiﬁcation. arXiv preprint arXiv:1408.5882 , 2014.\n[18] Sangeun Kum, Changheun Oh, and Juhan Nam.\nMelody extraction on vocal segments using multi-\ncolumn deep neural networks. In ISMIR , pages 819–\n825, 2016.\n[19] T. LH Li, A. B. Chan, and A. Chun. Automatic musi-\ncal pattern feature extraction using convolutional neu-\nral network. In Proc. Int. Conf. Data Mining and Ap-\nplications , 2010.\n[20] Vincent Lostanlen and Carmine-Emanuele Cella.\nDeep convolutional networks on the pitch spiral\nfor musical instrument recognition. arXiv preprint\narXiv:1605.06644 , 2016.\n[21] R. Mayer, R. Neumayer, and A. Rauber. Combination\nof audio and lyrics features for genre classiﬁcation in\ndigital audio collections. In Proceedings of the 16th\nACM international conference on Multimedia , pages\n159–168. ACM, 2008.\n[22] R. Mayer and A. Rauber. Musical genre classiﬁcation\nby ensembles of audio and lyrics features. In ISMIR ,\npages 675–680, 2011.\n[23] C. McKay, J. A. Burgoyne, J. Hockman, J. BL Smith,\nG. Vigliensoni, and I. Fujinaga. Evaluating the genre\nclassiﬁcation performance of lyrical features relative to\naudio, symbolic and cultural features. In ISMIR , pages\n213–218, 2010.\n[24] C. McKay and I. Fujinaga. Musical genre classiﬁca-\ntion: Is it worth pursuing and how can it be improved?\nInISMIR , pages 101–106, 2006.\n[25] M. McKinney and J. Breebaart. Feature for audio and\nmusic classiﬁcation. In ISMIR , pages 151–158, 2003.\n[26] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado,\nand J. Dean. Distributed representations of words and\nphrases and their compositionality. In Advances in neu-\nral information processing systems , pages 3111–3119,\n2013.\n[27] J. Nam, J. Herrera, M. Slaney, and J. O. Smith. Learn-\ning sparse feature representations for music annotation\nand retrieval. In ISMIR , pages 565–570, 2012.\n[28] R. Pascanu, T. Mikolov, and Y . Bengio. On the dif-\nﬁculty of training recurrent neural networks. ICML ,\n28:1310–1318, 2013.\n[29] J. Pennington, R. Socher, and C. D. Manning. Glove:\nGlobal vectors for word representation. In EMNLP ,\nvolume 14, pages 1532–1543, 2014.\n[30] P. H. Seo, Z. Lin, S. Cohen, X. Shen, and B. Han. Pro-\ngressive attention networks for visual attribute predic-\ntion. arXiv preprint arXiv:1606.02393 , 2016.700 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[31] S. Sigtia and S. Dixon. Improved music feature learn-\ning with deep neural networks. In ICASSP , pages\n6959–6963. IEEE, 2014.\n[32] A. Smith, C. Zee, and A. Uitdenbogerd. In your eyes:\nIdentifying clich ´es in song lyrics. In Australasian Lan-\nguage Technology Workshop , pages 88–96, 2012.\n[33] M. Sordo, O. Celma, M. Blech, and E. Guaus. The\nquest for musical genres: Do the experts and the wis-\ndom of crowds agree? In ISMIR , pages 255–260, 2008.\n[34] Carl Southall, Ryan Stables, and Jason Hockman. Au-\ntomatic drum transcription using bi-directional recur-\nrent neural networks. In ISMIR , pages 591–597, 2016.\n[35] N. Srivastava, G. E. Hinton, A. Krizhevsky,\nI. Sutskever, and R. Salakhutdinov. Dropout: a\nsimple way to prevent neural networks from over-\nﬁtting. Journal of Machine Learning Research ,\n15(1):1929–1958, 2014.\n[36] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Di-\nvide the gradient by a running average of its recent\nmagnitude. COURSERA: Neural networks for machine\nlearning , 4(2), 2012.\n[37] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and\nE. Hovy. Hierarchical attention networks for docu-\nment classiﬁcation. In NAACL-HLT , pages 1480–1489,\n2016.\n[38] T. C. Ying, S. Doraisamy, and L. N. Abdullah. Genre\nand mood classiﬁcation using lyric features. In Interna-\ntional Conference on Information Retrieval & Knowl-\nedge Management , pages 260–263. IEEE, 2012.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 701"
    },
    {
        "title": "Lyric Jumper: A Lyrics-Based Music Exploratory Web Service by Modeling Lyrics Generative Process.",
        "author": [
            "Kosetsu Tsukuda",
            "Keisuke Ishida",
            "Masataka Goto"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417749",
        "url": "https://doi.org/10.5281/zenodo.1417749",
        "ee": "https://zenodo.org/records/1417749/files/TsukudaIG17.pdf",
        "abstract": "Each artist has their own taste for topics of lyrics such as “love” and “friendship.” Considering such artist’s taste brings new applications in music information retrieval: choosing an artist based on topics of lyrics and finding un- familiar artists who have similar taste to a favorite artist. Although previous studies applied latent Dirichlet alloca- tion (LDA) to lyrics to analyze topics, LDA was not able to capture the artist’s taste. In this paper, we propose a topic model that can deal with the artist’s taste for topics of lyrics. Our model assumes each artist has a topic dis- tribution and a topic is assigned to each song according to the distribution. Our experimental results using a real- world dataset show that our model outperforms LDA in terms of the perplexity. By applying our model to estimate topics of 147,990 lyrics by 3,722 artists, we implement a web service called Lyric Jumper that enables users to ex- plore lyrics based on the estimated topics. Lyric Jumper provides functions such as artist’s topic taste visualization and topic-similarity-based artist recommendation. We also analyze operation logs obtained from 12,353 users on Lyric Jumper and show the usefulness of Lyric Jumper especially in recommending topic-related phrases in lyrics.",
        "zenodo_id": 1417749,
        "dblp_key": "conf/ismir/TsukudaIG17",
        "keywords": [
            "artists taste",
            "topics of lyrics",
            "music information retrieval",
            "latent Dirichlet allocation",
            "perplexity",
            "web service",
            "Lyric Jumper",
            "topic-similarity-based artist recommendation",
            "topic taste visualization",
            "topic-related phrases"
        ],
        "content": "LYRIC JUMPER: A LYRICS-BASED MUSIC EXPLORATORY\nWEB SERVICE BY MODELING LYRICS GENERATIVE PROCESS\nKosetsu Tsukuda Keisuke Ishida Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\nfk.tsukuda, ksuke-ishida, m.goto g@aist.go.jp\nABSTRACT\nEach artist has their own taste for topics of lyrics such as\n“love” and “friendship.” Considering such artist’s taste\nbrings new applications in music information retrieval:\nchoosing an artist based on topics of lyrics and ﬁnding un-\nfamiliar artists who have similar taste to a favorite artist.\nAlthough previous studies applied latent Dirichlet alloca-\ntion (LDA) to lyrics to analyze topics, LDA was not able\nto capture the artist’s taste. In this paper, we propose a\ntopic model that can deal with the artist’s taste for topics\nof lyrics. Our model assumes each artist has a topic dis-\ntribution and a topic is assigned to each song according\nto the distribution. Our experimental results using a real-\nworld dataset show that our model outperforms LDA in\nterms of the perplexity. By applying our model to estimate\ntopics of 147,990 lyrics by 3,722 artists, we implement a\nweb service called Lyric Jumper that enables users to ex-\nplore lyrics based on the estimated topics. Lyric Jumper\nprovides functions such as artist’s topic taste visualization\nand topic-similarity-based artist recommendation. We also\nanalyze operation logs obtained from 12,353 users on Lyric\nJumper and show the usefulness of Lyric Jumper especially\nin recommending topic-related phrases in lyrics.\n1. INTRODUCTION\nDifferent artists have different tastes in lyrics. Some\nartists tend to sing about “love,” while other artists tend\nto sing about “friendship.” When listening to music, peo-\nple choose artists according to not only musical audio con-\ntent, such as music genre, mood, melody, vocal timbre, and\nrhythm, but also the topics of lyrics [2, 21]. However, the\npotential of using the topics of lyrics has not yet been fully\nexploited in the ﬁeld of music information retrieval (MIR).\nFor example, it is difﬁcult to choose an artist based on the\ntopics of their lyrics, ﬁnd unfamiliar artists that are simi-\nlar to the user’s favorite artist in terms of the topics of the\nlyrics, and listen to a song that has the user’s favorite topic\nof lyrics. The goal of this research is to achieve lyrics-\nbased MIR that can leverage the topics of lyrics at both\nartist and song levels.\nOne approach for lyrics-based MIR is to directly use the\nc⃝Kosetsu Tsukuda, Keisuke Ishida, Masataka Goto.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Kosetsu Tsukuda, Keisuke Ishida,\nMasataka Goto. “Lyric Jumper: A Lyrics-based Music Exploratory\nWeb Service by Modeling Lyrics Generative Process”, 18th International\nSociety for Music Information Retrieval Conference, 2017.words in lyrics. Users input some words as a query [5, 27]\nor can ﬁnd the same phrase in the lyrics of another song\nwhile they are listening to music [9]. Another approach is\nto use a topic model because it can deal with the underlying\nmeanings of lyrics. The topic is usually represented by a\ndistribution over the vocabulary, and the meaning of topics\n(e.g., “love” or “friendship”) is determined based on the\ndistribution. In lyrics-based MIR, it has been popular to\nuse latent Dirichlet allocation (LDA) [1] as a topic model.\nLDA models each song as a mixture of topics and assigns\na topic to each word in the song’s lyrics. Since LDA does\nnot take the set of songs of each artist into account, it is not\nable to capture the artist’s taste for topics of lyrics.\nIn light of the above, we propose a topic model that\nconsiders the artist’s taste for topics of lyrics. In the lyrics\ngenerative process of our model, each artist has a distribu-\ntion over topics that reﬂects the artist’s taste for topics in\ntheir lyrics. In addition, since it is common to decide the\ntheme for a song before starting to write its lyrics [4, 36],\nour model assigns one topic to each song. That is, given a\ntopickassigned to a song, topic kis also assigned to the\nwords in its lyrics. We also use the background word dis-\ntribution because not all the words in lyrics are related to\nthe topic.\nBy using our proposed model, we implemented a\nlyrics-based music exploratory web service, called Lyric\nJumper1 2. Lyric Jumper aims to enable users to explore\nlyrics and enjoy music in a more ﬂexible way by consid-\nering songs’ topics. Our proposed model automatically as-\nsigns 1 of 20 topics for each song, where the 20 topics are\nalso automatically estimated by our model. Lyric Jumper\nprovides several topic-based functions such as visualiza-\ntion of the topic tendency for a given artist, artist ranking\nbased on topics, and artist recommendation based on the\ntopic distribution similarity.\nOur main contributions in this paper are as follows.\n\u000fTo the best of our knowledge, this is the ﬁrst study\nmodeling a lyrics generative process by considering\nthe artist’s taste for topics of lyrics and assuming\neach song has one topic. (Section 3)\n\u000fWe quantitatively evaluated our model by using a\nreal-world song dataset provided by a lyrics distri-\nbution company. Our experimental results show that\nour proposed model outperformed the conventional\nLDA in terms of the perplexity. (Section 4)\n1https://lyric-jumper.petitlyrics.com\n2The demonstration video: https://youtu.be/5V9kHnelSAk544\u000fBy using our proposed model, we implemented a\nweb service, called Lyric Jumper , that enables users\nto search for songs based on the topics of their lyrics.\nWe also analyzed the search logs obtained from\nmore than 12,000 users and showed the impact of\nLyric Jumper on users’ search behavior. (Section 5)\n2. RELATED WORK\nPrevious studies have used lyrics for various objectives\nsuch as lyrics-to-audio alignment [7, 22, 35], analyzing\nlyrics characteristics [8, 13, 16, 29, 34], accurately ﬁnding\nlyrics [11, 19, 24], genre or mood classiﬁcation [15, 25,\n26, 38–40], songwriting support [30], and video genera-\ntion [10]. This section describes more related studies in\nterms of (1) lyrics-based music retrieval/browsing systems\nand (2) topic-based lyrics analysis and applications.\n2.1 Lyrics-Based Music Retrieval/Browsing Systems\nBrochu and de Freitas [5] modeled music and text jointly\nso that users can search song databases using music and/or\ntext as input. M ¨uller et al. [27] automatically annotated au-\ndio recordings of a given song with its corresponding lyrics\nand realized a query-by-lyrics retrieval system. When a\nuser selects a query result, the system can directly navigate\nto the corresponding matching positions within the audio.\nDetecting songs from the user’s singing lyrics is also a pop-\nular research topic [14,37]. Fujihara et al. [9] proposed the\nconcept of a “Music Web” where songs were hyperlinked\nto each other based on the phrases of lyrics. This enables\nusers to jump to the same phrase in the lyrics of another\nsong by clicking a linked phrase while they are listening to\nmusic. Visualization is also a useful approach to browse a\nmusic collection. SongWords [3], which is an application\nfor tabletop computers, displays a music collection on a\ntwo-dimensional canvas based on self-organizing maps for\nlyrics and tags. Lyricon [23] is a system that automatically\nselects and displays icons that match the word sequences\nof lyrics so that users can intuitively understand the lyrics.\nAlthough these studies directly use the words in lyrics,\nwe consider topics that are automatically estimated from\nlyrics. Our approach has an advantage in that users can ex-\nplore lyrics based on the underlying meanings of the lyrics.\n2.2 Topic-Based Lyrics Analysis and Applications\nSince a topic model can learn the underlying meanings of\nlyrics, it has been used in various studies, including lyrics\nanalysis [17, 31, 33], lyrics retrieval applications [32], and\na music player [28]. In terms of lyrics analysis, Sharma\nand Murty [33] analyzed the hidden sentimental structure\nbehind lyrics by using LDA and revealed that some of the\ndetected topics correspond to sentiments. Similarly, by ap-\nplying LDA to rap lyrics, not only expected topics such\nas “street life” and “religion” but also unexpected ones\nsuch as “family/childhood” can be discovered [17]. Ren\net al. [31] tackled the problem of predicting the popular-\nity of a music track by considering lyrics topics and found\nthat more than half of the popular tracks are related to the\ntopic of “love.” Regarding applications, LyricsRadar [32]is a lyrics retrieval system that visualizes the topic ratio\nfor each song by using the topic radar chart and enables\nusers to ﬁnd their favorite lyrics interactively. Nakano and\nGoto [28] presented a music playback interface LyricList-\nPlayer that enables users to see word sequences of other\nsongs similar to the sequence currently being played back,\nwhere the similarity is computed based on the topic.\nIn these studies, LDA is used as a topic model, where\nit is assumed that each song has a topic distribution and\na topic is assigned to each word in the lyrics. We pro-\npose a new topic model that assumes each artist has a topic\ndistribution and a topic is assigned to each song. Since\nour model outperforms LDA (see Section 4), there is the\npotential for improving previous studies on lyrics analysis\nand applications by using our model.\nThe study closest to ours is that of Kleedorfer et al. [18],\nwho applied non-negative matrix factorization to lyrics for\nclustering them and manually labeled the cluster names.\nOur study differs from theirs in that we consider the artist’s\ntaste for topics of lyrics, and this enables users to ﬁnd their\nfavorite lyrics based on the relationships between artists\nand topics. Moreover, we not only propose a new model\nbut also implement a web service so that everyone can ex-\nplore lyrics with a real world dataset.\n3. MODEL AND INFERENCE\nIn this section, after summarizing the notations used in our\nmodel in Section 3.1, we ﬁrst describe LDA in Section 3.2\nand then propose our model in Section 3.3.\n3.1 Notations\nGiven a lyrics dataset, let Abe the set of artists in the\ndataset. Let Rabe the number of songs of artist a2Ain\nthe dataset; then the set of a’s songs is given by fSargRa\nr=1,\nwhereSarrepresents the rth song ofa. Moreover, let Var\nbe the number of words in the lyrics of Sar; thenSarcan\nbe represented by Sar=fvarjgVar\nj=1, wherevarjis thejth\nword inSar. Hence, the set of words of all artists’ lyrics is\ngiven byD=fffvarjgVar\nj=1gRa\nr=1ga2A.\n3.2 LDA (Latent Dirichlet Allocation)\nWhen LDA is used as a generative process of lyrics, it is as-\nsumed that (1) each song has a distribution over topics, (2)\na topic is assigned to each word in the song’s lyrics accord-\ning to the distribution, and (3) a word is generated from\nthe topic’s distribution over words. Figure 1(a) shows the\ngraphical model of LDA, where the shaded and unshaded\ncircles represent the observed and unobserved variables,\nrespectively. In the ﬁgure, Kis the number of topics, \u0012is\nthe song-topic distribution, and ϕis the topic-word distri-\nbution. We assume that \u0012andϕhave Dirichlet priors of\n\u000band\f, respectively. The generative process of LDA is\ndescribed in Algorithm 1.\n3.3 Artist’s Taste (AT) Model\nAlthough previous studies reported the usefulness of ap-\nplying LDA to lyrics [17, 28, 31–33], LDA does not take\nartist information into account in the generative process.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 545(a)\n(b)Figure 1 .Graphical models of (a) baseline LDA and (b)\nproposed artist’s taste (AT) model.\nIt is reasonable to assume that each artist has their own\ntaste for topics of lyrics. For example, one artist may tend\nto sing lyrics related to the topic of “love,” while another\nartist may tend to sing lyrics related to the topic of “life.”\nIn light of the above, we propose a model that considers\ntheartist’s taste for topics. Figure 1(b) shows the graphi-\ncal model of our proposed model. In our model, each artist\nhas a distribution over topics ( \u0012). When people write lyrics,\nthe writer typically decides the theme ( i.e., the topic) be-\nfore starting to write the lyrics [4, 36]. Hence, we assume\neach song has a topic zthat is generated from \u0012. However,\nnot all of the words in the lyrics are related to the topic. For\nexample, although “thing” and “this” frequently appear in\nmany lyrics, usually these words do not represent a spe-\nciﬁc topic. To solve this problem, we use the idea of back-\nground words [6]. In Figure 1(b), represents the back-\nground word distribution, where words that are not related\nto any topic have high occurrence probabilities. Each artist\nhas a Bernoulli distribution \u0015that controls the weights of\ninﬂuence for a song topic and background words. To be\nmore speciﬁc, when artist achooses a word in a song, we\nassume that the choice is inﬂuenced by the song topic with\nprobability\u0015a0(x= 0) and by background words with\nprobability\u0015a1(x= 1) , where\u0015a0+\u0015a1= 1. When\nx= 0, a word is generated from the topic’s distribution\nover words, while when x= 1, a word is generated from\nthe background word distribution  . The generative pro-\ncess of the AT model is described in Algorithm 2.\n3.4 Inference\nTo learn the parameters of our proposed model, we use\ncollapsed Gibbs sampling [12] to obtain samples of hid-\nden variable assignment. Since we use a Dirichlet prior\nfor\u0012,ϕ, and and a Beta prior for \u0015, we can ana-\nlytically calculate the marginalization over the parame-\nters. The marginalized joint distribution of D, latent vari-\nablesZ=ffzargRa\nr=1ga2A, and latent variables X=\nfffxarjgVar\nj=1gRa\nr=1ga2Ais computed as follows:Algorithm 1 LDA generative process\nforeach topick2 f1;\u0001 \u0001 \u0001;Kgdo\nDrawϕk\u0018Dirichlet (\f)\nend for\nforeach artistainAdo\nforeach songSardo\nDraw\u0012ar\u0018Dirichlet (\u000b)\nforeach wordvarjinSardo\nDraw a topic zarj\u0018Multinomial (\u0012ar)\nDraw a word varj\u0018Multinomial (ϕzarj)\nend for\nend for\nend for\nAlgorithm 2 AT model generative process\nforeach topick2 f1;\u0001 \u0001 \u0001;Kgdo\nDrawϕk\u0018Dirichlet (\f)\nend for\nDraw \u0018Dirichlet (\r)\nforeach artistainAdo\nDraw\u0012a\u0018Dirichlet (\u000b)\nDraw\u0015a\u0018Beta (\u001a).\nforeach songSardo\nDraw a topic zar\u0018Multinomial (\u0012a)\nforeach wordvarjinSardo\nDraw switch x\u0018Bernoulli (\u0015a)\nifx= 0then\nDraw a word varj\u0018Multinomial (ϕzar)\nelse ifx= 1then\nDraw a word varj\u0018Multinomial ( )\nend if\nend for\nend for\nend for\nP(D;Z;X j\u000b;\f;\r;\u001a )\n=∫∫∫∫\nP(D;Z;X j\u0002;\b; ;\u0003)P(\u0002j\u000b)\n\u0002P(\bj\f)P( j\r)P(\u0003j\u001a)d\u0002d\bd d\u0003;(1)\nwhere\u0002=f\u0012aga2A,\b=fϕkgK\nk=1, and\u0003=f\u0015aga2A.\nBy integrating out those parameters, we can compute\nEquation ( 1) as follows:\nP(D;Z;X j\u000b;\f;\r;\u001a )\n/∏\na2A\u0000(\u001a+Na0)\u0000(\u001a+Na1)\n\u0000(2\u001a+Na)∏\nv2V\u0000(N1v+\r)\n\u0000(N1+\rjVj)\n\u0002K∏\nk=1∏\nv2V\u0000(Nkv+\f)\n\u0000(Nk+\fjVj)∏\na2A∏K\nk=1\u0000(Rak+\u000b)\n\u0000(Ra+\u000bK):(2)\nHere,Na0andNa1are the number of words in a’s songs\nsuch thatx= 0 andx= 1, respectively, and Na=\nNa0+Na1. The termN1vrepresents the number of times\nthat wordvwas chosen under the condition of x= 1, and\nN1=∑\nv2VN1vwhereVis the set of unique words in D.\nFurthermore, Nk=∑\nv2VNkvwhereNkvis the number\nof times word vis assigned to topic kunder the condition546 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017ofx= 0. Finally,Rakis the number of times topic kis\nassigned toa’s song, and Ra=∑K\nk=1Rak.\nFor the Gibbs sampler, given the current state of all but\none variable zar, the new latent assignment of zaris sam-\npled from the following probability:\nP(zar=kjD;X;Z nar;\u000b;\f;\r;\u001a )\n/Raknar+\u000b\nRa\u00001 +\u000bK\u0000(Nknar+\fjVj)\n\u0000(Nknar+Nar+\fjVj)\n\u0002∏\nv2V\u0000(Nkvnar+Narv+\f)\n\u0000(Nkvnar+\f); (3)\nwhere narrepresents the procedure excluding the rth song\nofa. Moreover, NarandNarvrepresent the number of\nwords in the rth song ofaand the number of times word v\nappears in the rth song ofa, respectively.\nIn addition, given the current state of all but one variable\nxarj, the probability at which xarj= 0is given by:\nP(xarj= 0jD;X narj;Z;\u000b;\f;\r;\u001a )\n/\u001a+Na0narj\n2\u001a+Na\u00001Nzarvarjnarj+\f\nNzarnarj+\fjVj; (4)\nwhere narj represents the procedure excluding the jth\nword in the rth song ofa. Similarly, the probability at\nwhichxarj= 1is computed as follows:\nP(xarj= 1jD;X narj;Z;\u000b;\f;\r;\u001a )\n/\u001a+Na1narj\n2\u001a+Na\u00001N1varjnarj+\r\nN1narj+\rjVj: (5)\nFinally, we can make the point estimates of the inte-\ngrated out parameters as follows:\n\u0012ak=Rak+\u000b\nRa+\u000bK; ϕkv=Nkv+\f\nNk+\fjVj;  v=N1v+\r\nN1+\rjVj;\n\u0015a0=Na0+\u001a\nNa+ 2\u001a; \u0015 a1=Na1+\u001a\nNa+ 2\u001a: (6)\n4. EVALUATION\nIn this section, we carry out a quantitative evaluation to\nanswer the following research question: is adopting the\nartist’s taste for topics effective to model the lyrics gen-\nerative process?\n[Dataset] We used the lyrics of commercially available\npopular music. Those lyrics with the song’s title and artist\nname were provided by one of the largest companies for\ncommercial lyrics distribution. We collected data on the\ntop 1,000 artists in terms of the number of lyrics that are\navailable as of the end of December 2016; this gave us\n93,716 songs in total. We then extracted Japanese nouns\nfrom each song’s lyrics by using MeCab [20], which is a\nJapanese morphological analyzer. Nouns that appeared in\nless than 10 lyrics were eliminated. Although our proposed\nmodel is language-independent, we used only Japanese\nwords because of the understandability of the estimated\ntopics for Japanese users of Lyric Jumper that we will de-\nscribe in Section 5. From each of lyrics, we randomly\nsampled 80% of the nouns for training data and used the\nremaining 20 % for test data.\n[Settings] In terms of hyperparameters, in line with\nother topic modeling work, we set \u000b=1\nKand\f=50\njVj\nPerplexity\nNumber of topics ( K)1100120013001400\n0 10 20 30 40 50LDA\nATFigure 2 .Perplexity for baseline LDA and proposed AT\nmodel (the lower, the better).\nin LDA and the artist’s taste (AT) model. In addition, in\nthe AT model, we set \r=50\njVjand\u001a= 0:5. To com-\npare the performance of LDA and the AT model, we use\nthe perplexities of the two models. Perplexity is widely\nused to compare the performance of statistical models [1],\nand the lower value represents the better performance. In\nterms of the number of topics, we compute the perplexity\nforK=2, 4, 6, 8, 10, 20, 30, 40, and 50.\n[Results] Figure 2shows the perplexity. As can be seen,\nregardless of the number of topics, the AT model outper-\nforms LDA. In both methods, the perplexity reaches a min-\nimum when the number of topics is eight. The difference\nof perplexity between the two models becomes larger as\nthe number of topics increases. From these results, we can\nconclude that the AT model is superior to LDA for model-\ning a lyrics generative process and conﬁrmed the effective-\nness of modeling a topic distribution for each artist.\n5. LYRIC JUMPER\nBy using the AT model, we implemented a lyrics-based\nmusic exploratory web service called Lyric Jumper that\nanyone can use for free without registration. In this section,\nwe describe the implementation and functions of Lyric\nJumper followed by the log analysis based on the users’\noperation logs obtained from the web service.\n5.1 Implementation\nFor Lyric Jumper, the lyrics are provided by the aforemen-\ntioned company for commercial lyrics distribution. We\nused all the lyrics that are available as of the end of De-\ncember 2016 and extracted Japanese nouns. To guaran-\ntee the topic quality, we eliminated artists who had <10\nsongs and nouns that appeared in <10songs. This gave\nus 147,990 songs by 3,722 artists.\nAs for the number of topics K, ifKis too small, users\nwould soon get bored of using Lyric Jumper, while if K\nis too large, it would be difﬁcult to understand the differ-\nence between topics since many similar topics are gener-\nated. Hence, after comparing the topic qualities for sev-\neralKvalues, we set K= 20 for Lyric Jumper, although\nK= 8 achieved the best result in terms of the perplexity\nin Section 4. After automatically estimating the 20 topics\nby using the AT model, we manually labeled topic names\nso that users can easily understand the characteristics of\neach topic. Examples of topic names are “life,” “sentimen-\ntal,” and “adolescence.” Although ﬁve topics are related\nto “love,” our model was able to distinguish between sub-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 547Click “Ar/g415st Ranking by Topic” bu/g425onA20 topics BAr/g415st ranking related to “Devoted love” topic ERecommended ar/g415sts CTop 5 characteris/g415c\ntopics for ar/g415st “ClariS”\nDDoughnut chart represen/g415ng topic tendency of ar/g415st “Cl ariS”Figure 3 .Overview of Lyric Jumper.\ntle differences of love: “eternal love,” “devoted love,” “ro-\nmantic love,” “sexual love (female subject),” and “sexual\nlove (male subject).”\n5.2 Function\nLyric Jumper mainly provides six functions. The following\nsections describe the functions one by one.\n5.2.1 Artist Ranking\nLyric Jumper displays 20 topic names as shown in Fig-\nure3A⃝. By clicking one of the 20 topics, Lyric Jumper\nshows up to 100 artists related to the topic (Figure 3B⃝).\nThis enables the user to see many artists related to the topic\nof his/her interest. The user can also ﬁnd that unexpected\nartists are related to the topic.\nIntuitively, given a topic k, artists are ranked based on\nboth their topic ratio of k(\u0012ak) and the number of songs\nassigned tok(Rak) so that artists more closely related to\nthe topic are ranked higher. To be more speciﬁc, we sort\nall the artists in Aby using the rank of topic kin\u0012aas the\nﬁrst key (the smaller the better) and using the number of\nsongs assigned to k(Rak) as the second key (the larger the\nbetter). Note that artists whose rank of kin\u0012ais lower than\nﬁve are not included in the ranking because Lyric Jumper\nshows the top ﬁve topics for each artist as we will describe\nin Section 5.2.2 . Finally, we select the top 100 artists in the\nsorted list and show them to the users.\n5.2.2 Topic Tendency Visualization\nWhen a user clicks an artist, Lyric Jumper visualizes the\ntopic tendency of the artist. In this function, given artist a,\nthe top ﬁve topics in terms of the occurrence probability in\n\u0012aare displayed in rectangles (Figure 3C⃝). The size of a\nrectangle corresponds to the topic probability: the larger it\nis, the higher the probability is. With this function, a user\ncan not only understand the topic tendency of the artist’s\nlyrics but also ﬁnd out that the artist sings songs with un-\nexpected topics. We also manually selected four charac-\nteristic words for each topic and displayed them below the\ntopic name so that users can more easily understand the\nmeaning of the topic. In addition, Lyric Jumper visualizesthe topic tendency using a doughnut chart where the cir-\ncle is divided according to the ratio of the top ﬁve topics\n(Figure 3D⃝).\n5.2.3 Artist Recommendation\nSince similar artists are one of the important information\nneeds in MIR [21], Lyric Jumper provides a similar artists\nrecommendation function. Lyric Jumper recommends 10\nartists in terms of the topic similarity (Figure 3E⃝). Among\nthe 10 artists, eight artists are popular and two artists are\nminor. By displaying minor artists as well as popular ones,\nLyric Jumper aims to encourage the user to listen to unfa-\nmiliar artists’ songs that are related to his/her favorite artist\nby the topic similarity. By clicking a recommended artist’s\ngraph, the user can jump to the artist’s search result.\nGiven a selected artist a, we compute the similarity be-\ntweenaand each artist a′2An fagbased on Jensen-\nShannon divergence (JSD) between \u0012aand\u0012a′. The\nsmaller the JSD value is, the higher the similarity between\nartists is. After computing the similarities, we select the\ntop eight similar artists who have \u0015msongs in the dataset\n(i.e., popular artists) and the top two artists who have <m\nsongs ( i.e., minor artists) and show those 10 artists to the\nusers. On Lyric Jumper, mis set to 100.\n5.2.4 Phrase Emphasized Lyrics Visualization\nWhen a user clicks a topic of the selected artist, Lyric\nJumper shows the list of song titles of the artist that are\nassigned to the topic (the song ranking method will be de-\nscribed in Section 5.2.5 ). When the user clicks a title in\nthe list, the song’s lyrics are displayed (Figure 4). In the\nlyrics, lines3related to the topic are displayed with em-\nphasis: the stronger the relation is, the larger the font size\nbecomes and the darker the color becomes. This enables\na user to easily understand the characteristics of the lyrics\nsuch as “the latter half of the lyrics is strongly related to\nthe topic.” Users can also watch the song’s videos on Lyric\nJumper by clicking the “YouTube Search” button. Lyric\nJumper shows the search results obtained from YouTube4\n3We use the terms “phrase” and “line” interchangeably.\n4https://www.youtube.com/548 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017ASong ranking related to topic “Roman/g415c love” for ar/g415st  “ClariS” Lyrics of song \n“Surely”Figure 4 .Phrase emphasized lyrics visualization.\nwhere the query is the artist name and the song title.\nThe relevance score between a line in the lyrics and the\ntopic is computed as follows. We ﬁrst assign scores for\nthe nouns in ϕk. Let rank (k;v)be the occurrence prob-\nability rank of noun vinϕk. The score of vis given by\nwrel(k;v) = 101 \u0000rank (k;v)ifrank (k;v)\u0014100and\nwrel(k;v) = 0 otherwise. Line lconsists ofn\u00150\nnouns and can be represented by l= (v1;\u0001 \u0001 \u0001;vn). The\nrelevance score of lwith topickis given by lrel(k;l) =∑n\ni=1wrel(k;vi). After computing the scores of all lines\nin a song’s lyrics, the scores are normalized to ﬁt into the\ninterval [0;1]by min-max normalization. The font size lin-\nearly changes from 16 pt for a score of 0 to 36 pt for a score\nof 1; the color density also linearly changes from #FFFFFF\nfor a score of 0 to the topic color for a score of 1.\n5.2.5 Artist’s Songs Ranking\nAs mentioned in Section 5.2.4 , when a user clicks a topic\nof the selected artist, Lyric Jumper returns the ranked list\nof songs in the topic (Figure 4A⃝). Songs are sorted in de-\nscending order of relevance to the topic so that the user can\neasily access songs that are strongly related to the topic.\nThe relevance score between song sand topickis given\nbysrel(k;s) =1\njLsj∑\nl2Lslrel(k;l), whereLsis the\nset of lines in s’s lyrics. That is, we assume that the relat-\nedness between sandkcan be represented by the average\nrelevance between kand each line in s’s lyrics.\n5.2.6 Phrase Recommendation\nBy clicking the “Phrase” button after selecting an artist’s\ntopic, Lyric Jumper recommends phrases related to the\ntopic in the artist’s songs (Figure 5). Moreover, every time\nthe user clicks the “PUSH!” button, a new phrase is recom-\nmended. This function enables users to understand there\nare various expressions to deliver messages about the topic.\nWhen the user clicks a phrase, Lyric Jumper shows the cor-\nresponding lyrics in the same way as in Section 5.2.4 .\nGiven artist aand topick, the recommended phrases\nare selected as follows. In the ith round (i= 1;2;\u0001 \u0001 \u0001), we\npool lines that have the ith highest score of lrel(k;l)from\na’s songs in order of decreasing srel(k;s). This round is\nrepeated until the number of pooled lines is equal to 100.\nLyric Jumper recommends phrases from the pooled list in\nrandom order so that users can see different phrases every\ntime the user accesses Lyric Jumper.\nRecommended phrasesFigure 5 .Phrase recommendation.\nFunction PC Smartphone\nArtist ranking 2,092 30,295\nArtist recommendation 1,706 4,016\nArtist’s songs ranking 5,399 14,665\nPhrase recommendation 4,997 253,430\nTable 1 .Statistics of use frequency of each function.\n5.3 Log Analysis\nWe released Lyric Jumper as a web service open to the pub-\nlic on 2/21/2017. To analyze users’ exploratory behavior\non Lyric Jumper, we obtained operation logs for 30 days\n(2/21 to 3/22). The numbers of unique PC users and smart-\nphone users are 1,288 and 11,065, respectively. The use\nfrequencies of each function are summarized in Table 1.\nWe can see that the use frequencies of the artist ranking\nand artist’s songs ranking are high. These results indicate\nthat exploratory search for artists and songs based on top-\nics can stimulate the user’s interest. It can also be observed\nthat for smartphone users in particular, the phrase recom-\nmendation function was used frequently: the push button\nwas clicked as many as 253,430 times. This data shows\nthe user’s high information needs regarding ﬁnding lyrics\nusing phrases related to a topic. Compared to these func-\ntions, the recommended artists were not clicked very often.\nTo encourage the artist-similarity-based lyrics exploratory\nsearch, a more sophisticated interface for the recommenda-\ntion deserves to be explored; we leave this as future work.\n6. CONCLUSION\nIn this paper we proposed a topic model that incorporates\nthe artist’s taste for topics of lyrics. Our experimental re-\nsults showed that our model outperformed the state-of-the-\nart LDA model regardless of the number of topics in terms\nof the perplexity. We also released a lyrics-based music\nexploratory web service called Lyric Jumper, where we ap-\nplied our model to 147,990 lyrics by 3,722 artists. Our\nlog analysis results show that the phrase recommendation\nfunction, which recommends phrases from the lyrics of the\nartist’s songs related to the selected topic, achieved a par-\nticularly high use frequency.\nFor future work, since our model is language-\nindependent, we plan to apply our model to English lyrics\nand implement an English version of Lyric Jumper. We\nare also interested in combining topics obtained by our\nmodel with other features such as audio content and tags.\nThis would enable users to explore songs by adapting their\nsearch intent with increased ﬂexibility.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 5497. ACKNOWLEDGMENTS\nThe authors would like to extend their appreciation to\nSyncPower Corporation for providing lyrics data. This\nwork was supported in part by JST ACCEL Grant Num-\nber JPMJAC1602, Japan.\n8. REFERENCES\n[1]E. M. Airoldi, D. M. Blei, S. E. Fienberg, and\nE. P. Xing. Mixed membership stochastic blockmodels.\nJournal of Machine Learning Research , 9:1981–2014,\n2008.\n[2]D. Bainbridge, S.J. Cunningham, and J.S. Downie.\nHow people describe their music information needs: A\ngrounded theory analysis of music queries. In Proceed-\nings of the 4th International Conference on Music In-\nformation Retrieval , ISMIR’03, pages 221–222, 2003.\n[3]D. Baur, B. Steinmayr, and A. Butz. SongWords: Ex-\nploring music collections through lyrics. In Proceed-\nings of the 11th International Conference on Music In-\nformation Retrieval , ISMIR’10, pages 531–536, 2010.\n[4]M. Baxter. Voices of resistance, voices of transcen-\ndence: Musicians as models of the poetic - political\nimagination. International Journal of Education & the\nArts, 11:1–24, 2010.\n[5]E. Brochu and N. de Freitas. ”Name that song!” a\nprobabilistic approach to querying on music and text.\nInProceedings of the 15th International Conference\non Neural Information Processing Systems , NIPS’02,\npages 1505–1512. 2002.\n[6]C. Chemudugunta, P. Smyth, and M. Steyvers. Mod-\neling general and speciﬁc aspects of documents with a\nprobabilistic topic model. In Proceedings of the 19th\nInternational Conference on Neural Information Pro-\ncessing Systems , NIPS’06, pages 241–248, 2006.\n[7]G. Dzhambazov, A. Srinivasamurthy, S. Sent ¨urk, and\nX. Serra. On the use of note onsets for improved lyrics-\nto-audio alignment in Turkish makam music. In Pro-\nceedings of the 17th International Conference on Mu-\nsic Information Retrieval , ISMIR’16, pages 716–722,\n2016.\n[8]R. J. Ellis, Z. Xing, J. Fang, and Y. Wang. Quantify-\ning lexical novelty in song lyrics. In Proceedings of the\n16th International Conference on Music Information\nRetrieval , ISMIR’15, pages 694–700, 2015.\n[9]H. Fujihara, M. Goto, and J. Ogata. Hyperlinking\nlyrics: A method for creating hyperlinks between\nphrases in song lyrics. In Proceedings of the 9th Inter-\nnational Conference on Music Information Retrieval ,\nISMIR’08, pages 281–286, 2008.\n[10] S. Funasawa, H. Ishizaki, K. Hoashi, Y. Takishima, and\nJ. Katto. Automated music slideshow generation us-\ning web images based on lyrics. In Proceedings of the11th International Conference on Music Information\nRetrieval , ISMIR’10, pages 63–68, 2010.\n[11] G. Geleijnse and J. H. M. Korst. Efﬁcient lyrics ex-\ntraction from the web. In Proceedings of the 7th Inter-\nnational Conference on Music Information Retrieval ,\nISMIR’06, pages 371–372, 2006.\n[12] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc top-\nics.Proceedings of the National Academy of Sciences ,\n101(Suppl. 1):5228–5235, 2004.\n[13] H. Hirjee and D. G. Brown. Automatic detection of in-\nternal and imperfect rhymes in rap lyrics. In Proceed-\nings of the 10th International Conference on Music In-\nformation Retrieval , ISMIR’09, pages 711–716, 2009.\n[14] T. Hosoya, M. Suzuki, A. Ito, and S. Makino. Lyrics\nrecognition from a singing voice based on ﬁnite state\nautomaton for music information retrieval. In Proceed-\nings of the 6th International Conference on Music In-\nformation Retrieval , ISMIR’05, pages 532–535, 2005.\n[15] X. Hu and J. S. Downie. When lyrics outperform audio\nfor music mood classiﬁcation: A feature analysis. In\nProceedings of the 11th International Conference on\nMusic Information Retrieval , ISMIR’10, pages 619–\n624, 2010.\n[16] X. Hu and B. Yu. Exploring the relationship between\nmood and creativity in rock lyrics. In Proceedings of\nthe 12th International Conference on Music Informa-\ntion Retrieval , ISMIR’11, pages 789–794, 2011.\n[17] C. Johnson-Roberson and M. Johnson-Roberson. Tem-\nporal and regional variation in rap lyrics. In NIPS\nWorkshop on Topic Models: Computation, Application\nand Evaluation , NIPSW’13, 2013.\n[18] F. Kleedorfer, P. Knees, and T. Pohle. Oh oh oh whoah!\ntowards automatic topic detection in song lyrics. In\nProceedings of the 9th International Conference on\nMusic Information Retrieval , ISMIR’08, pages 287–\n292, 2008.\n[19] P. Knees, M. Schedl, and G. Widmer. Multiple lyrics\nalignment: Automatic retrieval of song lyrics. In Pro-\nceedings of the 6th International Conference on Mu-\nsic Information Retrieval , ISMIR’05, pages 564–569,\n2005.\n[20] T. Kudo, K. Yamamoto, and Y. Matsumoto. Apply-\ning conditional random ﬁelds to Japanese morpholog-\nical analysis. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing ,\nEMNLP’04, pages 230–237, 2004.\n[21] J. H. Lee and J. S. Downie. Survey of music informa-\ntion needs, uses, and seeking behaviours: Preliminary\nﬁndings. In Proceedings of the 5th International Con-\nference on Music Information Retrieval , ISMIR’04,\npages 989–992, 2004.550 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[22] K. Lee and M. Cremer. Segmentation-based lyrics-\naudio alignment using dynamic programming. In Pro-\nceedings of the 9th International Conference on Mu-\nsic Information Retrieval , ISMIR’08, pages 395–400,\n2008.\n[23] W. Machida and T. Itoh. Lyricon: A visual music se-\nlection interface featuring multiple icons. In Proceed-\nings of the 15th International Conference on Informa-\ntion Visualisation , IV’11, pages 145–150, 2011.\n[24] R. Macrae and S. Dixon. Ranking lyrics for online\nsearch. In Proceedings of the 13th International Con-\nference on Music Information Retrieval , ISMIR’12,\npages 361–366, 2012.\n[25] R. Mayer, R. Neumayer, and A. Rauber. Rhyme and\nstyle features for musical genre classiﬁcation by song\nlyrics. In Proceedings of the 9th International Confer-\nence on Music Information Retrieval , ISMIR’08, pages\n337–342, 2008.\n[26] R. Mayer and A. Rauber. Music genre classiﬁcation by\nensembles of audio and lyrics features. In Proceedings\nof the 12th International Conference on Music Infor-\nmation Retrieval , ISMIR’11, pages 675–680, 2011.\n[27] M. M ¨uller, F. Kurth, D. Damm, C. Fremerey, and\nM. Clausen. Lyrics-based audio retrieval and multi-\nmodal navigation in music collections. In Proceedings\nof the 11th European Conference on Digital Libraries ,\nECDL’07, pages 112–123, 2007.\n[28] T. Nakano and M. Goto. LyricListPlayer: A\nconsecutive-query-by-playback interface for retrieving\nsimilar word sequences from different song lyrics. In\nProceedings of the Sound and Music Computing Con-\nference 2016 , SMC’16, pages 344–349, 2016.\n[29] E. Nichols, D. Morris, S. Basu, and C. Raphael. Rela-\ntionships between lyrics and melody in popular music.\nInProceedings of the 10th International Conference on\nMusic Information Retrieval , ISMIR’09, pages 471–\n476, 2009.\n[30] T. O’Hara, N. Sch ¨uler, Y. Lu, and D. Tamir. Inferring\nchord sequence meanings via lyrics: Process and eval-\nuation. In Proceedings of the 13th International Con-\nference on Music Information Retrieval , ISMIR’12,\npages 463–468, 2012.\n[31] J. Ren, J. Shen, and R. J. Kauffman. What makes a\nmusic track popular in online social networks? In Pro-\nceedings of the 25th International Conference Com-\npanion on World Wide Web , WWW’16 Companion,\npages 95–96, 2016.\n[32] S. Sasaki, K. Yoshii, T. Nakano, M. Goto, and S. Mor-\nishima. LyricsRadar: A lyrics retrieval system based on\nlatent topics of lyrics. In Proceedings of the 15th Inter-\nnational Conference on Music Information Retrieval ,\nISMIR’14, pages 585–590, 2014.[33] G. Sharma and M. N. Murty. Mining sentiments from\nsongs using latent Dirichlet allocation. In Proceedings\nof the 10th International Conference on Advances in\nIntelligent Data Analysis X , IDA’11, pages 328–339,\n2011.\n[34] A. Singhi and D. G. Brown. Are poetry and lyrics all\nthat different? In Proceedings of the 15th Interna-\ntional Conference on Music Information Retrieval , IS-\nMIR’14, pages 471–476, 2014.\n[35] V. Thomas, C. Fremerey, D. Damm, and M. Clausen.\nSlave: A score-lyrics-audio-video-explorer. In Pro-\nceedings of the 10th International Conference on Mu-\nsic Information Retrieval , ISMIR’09, pages 717–722,\n2009.\n[36] J. M. Toivanen, H. Toivonen, and A. Valitutti. Auto-\nmatical composition of lyrical songs. In Proceedings\nof the 4th International Conference on Computational\nCreativity , ICCC’13, pages 87–91, 2013.\n[37] C. Wang, J. R. Jang, and W. Wang. An improved\nquery by singing/humming system using melody and\nlyrics information. In Proceedings of the 11th Inter-\nnational Conference on Music Information Retrieval ,\nISMIR’10, pages 45–50, 2010.\n[38] X. Wang, X. Chen, D. Yang, and Y. Wu. Music emotion\nclassiﬁcation of chinese songs based on lyrics using\nTF*IDF and rhyme. In Proceedings of the 12th Inter-\nnational Conference on Music Information Retrieval ,\nISMIR’11, pages 765–770, 2011.\n[39] B. Wei, C. Zhang, and M. Ogihara. Keyword gen-\neration for lyrics. In Proceedings of the 8th Interna-\ntional Conference on Music Information Retrieval , IS-\nMIR’07, pages 121–122, 2007.\n[40] M. Zaanen and P. Kanters. Automatic mood classiﬁca-\ntion using TF*IDF based on lyrics. In Proceedings of\nthe 11th International Conference on Music Informa-\ntion Retrieval , ISMIR’10, pages 75–80, 2010.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 551"
    },
    {
        "title": "Function- and Rhythm-Aware Melody Harmonization Based on Tree-Structured Parsing and Split-Merge Sampling of Chord Sequences.",
        "author": [
            "Hiroaki Tsushima",
            "Eita Nakamura",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416848",
        "url": "https://doi.org/10.5281/zenodo.1416848",
        "ee": "https://zenodo.org/records/1416848/files/TsushimaNIY17.pdf",
        "abstract": "This paper presents an automatic harmonization method that, for a given melody (sequence of musical notes), gen- erates a sequence of chord symbols in the style of exist- ing data. A typical way is to use hidden Markov models (HMMs) that represent chord transitions on a regular grid (e.g., bar or beat grid). This approach, however, cannot explicitly describe the rhythms, harmonic functions (e.g., tonic, dominant, and subdominant), and the hierarchical structure of chords, which are supposedly important in tra- ditional harmony theories. To solve this, we formulate a hi- erarchical generative model consisting of (1) a probabilis- tic context-free grammar (PCFG) for chords incorporating their syntactic functions, (2) a metrical Markov model de- scribing chord rhythms, and (3) a Markov model generat- ing melodies conditionally on a chord sequence. To esti- mate a variable-length chord sequence for a given melody, we iteratively refine the latent tree structure and the chord symbols and rhythms using a Metropolis-Hastings sampler with split-merge operations. Experimental results show that the proposed method outperformed the HMM-based method in terms of predictive abilities.",
        "zenodo_id": 1416848,
        "dblp_key": "conf/ismir/TsushimaNIY17",
        "keywords": [
            "melody",
            "chord symbols",
            "hidden Markov models",
            "rhythms",
            "harmonic functions",
            "hierarchical structure",
            "traditional harmony theories",
            "probabilistic context-free grammar",
            "metrical Markov model",
            "Markov model"
        ],
        "content": "FUNCTION- AND RHYTHM-AWARE MELODY HARMONIZATION\nBASED ON TREE-STRUCTURED PARSING AND\nSPLIT-MERGE SAMPLING OF CHORD SEQUENCES\nHiroaki Tsushima1Eita Nakamura1Katsutoshi Itoyama1Kazuyoshi Yoshii1;2\n1Graduate School of Informatics, Kyoto University, Japan2RIKEN AIP, Japan\nftsushima, enakamura g@sap.ist.i.kyoto-u.ac.jp, fitoyama, yoshii g@kuis.kyoto-u.ac.jp\nABSTRACT\nThis paper presents an automatic harmonization method\nthat, for a given melody (sequence of musical notes), gen-\nerates a sequence of chord symbols in the style of exist-\ning data. A typical way is to use hidden Markov models\n(HMMs) that represent chord transitions on a regular grid\n(e.g., bar or beat grid). This approach, however, cannot\nexplicitly describe the rhythms, harmonic functions (e.g.,\ntonic, dominant, and subdominant), and the hierarchical\nstructure of chords, which are supposedly important in tra-\nditional harmony theories. To solve this, we formulate a hi-\nerarchical generative model consisting of (1) a probabilis-\ntic context-free grammar (PCFG) for chords incorporating\ntheir syntactic functions, (2) a metrical Markov model de-\nscribing chord rhythms, and (3) a Markov model generat-\ning melodies conditionally on a chord sequence. To esti-\nmate a variable-length chord sequence for a given melody,\nwe iteratively reﬁne the latent tree structure and the chord\nsymbols and rhythms using a Metropolis-Hastings sampler\nwith split-merge operations. Experimental results show\nthat the proposed method outperformed the HMM-based\nmethod in terms of predictive abilities.\n1. INTRODUCTION\nCreation of chord sequences plays a key role in music com-\nposition and arrangement since harmony affects the mood\nof music and characterizes the impression of a certain mu-\nsical style. Our aim is automatic melody harmonization, or\nautomatic generation of a sequence of chord symbols for a\ngiven melody (a sequence of musical notes). In this paper,\nwe restricted our focus to the automatic harmonization in\nthe style of popular music. Instead of manually describing\nmusic theories for the style such as jazz and classical mu-\nsic, we take a statistical approach to automatically learn\nmodel architectures and parameters from a music corpus\nand harmonize in the style of that data. We formulate a\nprobabilistic model that represents how likely a chord se-\nquence is to be generated and another model that represents\nc⃝Hiroaki Tsushima, Eita Nakamura, Katsutoshi Itoyama,\nKazuyoshi Yoshii. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Hiroaki Tsushima,\nEita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii. “Function- and\nRhythm-Aware Melody Harmonization Based on Tree-Structured Parsing\nand Split-Merge Sampling of Chord Sequences”, 18th International Soci-\nety for Music Information Retrieval Conference, Suzhou, China, 2017.\nFigure 1 : The hierarchical generative model for chord\nsymbol, chord rhythms, and melodies.\nhow likely a melody is to be generated conditionally on a\nchord sequence.\nSince chord sequences are usually described by Markov\nmodels [21, 25], a standard way to statistical harmoniza-\ntion is to use a hidden Markov model (HMM) that has a\nlatent Markov chain of chord symbols and assumes a mu-\nsical note sequence to be generated conditionally on the\nchords. This approach, however, does not consider the\nsyntactic roles and hierarchical structure of chords. In\ntraditional harmony theories (e.g., [14, 22]), such syntac-\ntic roles are often referred to as harmonic functions (e.g.,\ntonic, dominant, and subdominant), which are similar to\nparts-of-speech in language theories. Another problem of\nthe conventional HMMs lies in the description of the chord\nrhythms (onset score times or durations of chords). Since\nchord durations are described by self-transition probabili-\nties on a regular time grid (e.g., beat or bar grid), the chord\nrhythms are not explicitly described.\nTo solve these problems, we propose a tree-structured\nhierarchical generative model that consists of (1) a proba-\nbilistic context-free grammar (PCFG) that generates chord\nsymbols, (2) a metrical Markov model that generates chord\nrhythms, and (3) a Markov model that generates a melody\nfrom a chord sequence (Fig. 1). The use of the PCFG was\ninspired by Steedman’s pioneering work [27] that uses a\ncontext-free grammar (CFG) for representing the hierar-\nchical structure of chords. A key advantage of our study\nis that the rule probabilities and tree structure of the PCFG\ncan jointly be estimated in an unsupervised manner from\na corpus of chord sequences, expecting that the syntactic\nroles of chords are captured by the non-terminal symbols.502The metrical Markov model is used for explicitly describ-\ning transition probabilities between the onset beat positions\nof succeeding chords.\nUsing the tree-structured hierarchical generative model,\nwe propose a statistical harmonization method based on a\nsophisticated Metropolis-Hasting (MH) sampler with split-\nmerge operations. To estimate a variable-length chord se-\nquence with appropriate chord rhythms for a given melody,\nwe stochastically search for the most likely latent tree\nstructure, symbols, and onset score times of chords from\ntheir posterior distributions. In this search, our sampler has\nfour types of proposals: the whole latent tree structure is\nupdated using a variant of the Viterbi algorithm, one of the\nchords is split or two adjacent chords are merged according\nto the latent tree structure, and one of the chord onset score\ntime is moved back or forth. Such stochastic global or lo-\ncal updates can be interpreted as a repeated trial-and-error\nprocess of ﬁnding an optimal chord sequence.\n2. RELATED WORK\nThis section introduces related studies on the automatic\nharmonization and on the music language model for chords\nand notes.\n2.1 Automatic Harmonization\nSome studies on harmonization aim to generate sequences\nof chord symbols (as in this paper) and other studies aim\nto generate several (typically four) voices of musical notes.\nIn the former direction, Chuan and Chew [3] proposed a\nhybrid method that consists of three processes: selection\nof chord tones (constituent tones of chords) from given\nmelodies with a support vector machine (SVM), construc-\ntion of triad chords from chord tones, and generation of\nchord progressions by a rule-base method. Simon et al.\n[25] developed a method based on HMMs in which chord\ntransitions are described by Markov models. This method\nhas been implemented in a commercial system MySong .\nRaczy ´nski et al. [21] proposed similar Markov models\nin which chords are conditioned by melodies and time-\nvarying keys. To our knowledge, PCFG has not been used\nfor melody harmonization.\nIn the latter direction, Ebcio ˘glu [5] proposed a rule-\nbased method for generating four-part chorales in Bach’s\nstyle. Methods by using variants of genetic algorithms\n(GAs) based on music theories have also been studied\n[18, 19, 28]. Allan and Williams et al. [1] proposed a\nmethod based on HMMs which represent chords as hidden\nstates and musical notes as observed outputs. A hidden\nsemi-Markov model (HSMM) [9] has been used for ex-\nplicitly representing the durations of chords. Paiement et\nal. [17] proposed a hierarchical tree-structured model that\ndescribes chord movements from the viewpoint of hierar-\nchical time scales by dividing the notations of chords.\n2.2 Music Language Modeling\nSeveral language models for musical notes have been stud-\nied for music structure analysis [10, 11, 13, 15]. According\nto the Generative Theory of Tonal Music (GTTM) [13], a\nnote sequence is assumed to have a hierarchical structurethat describes which notes are important. This theory con-\nsists of rules for recursively reducing a note sequence into a\nsingle note. Computational implementation of GTTM and\nthe analysis of musical pieces using it have been studied\n[10, 11]. A probabilistic formulation of GTTM based on\nPCFG has been proposed and enabled unsupervised learn-\ning of production rules directly from note sequences [15].\nVarious language models for chord sequences have been\nproposed in the context of automatic chord recognition for\nmusic audio signals [16, 24, 29], music analysis [23, 27],\nand music arrangement [6, 20]. The conventional lan-\nguage model for chord sequences is n-gram models [6,24].\nTo avoid the sparseness problem with a large value of n,\nsmoothing methods have been studied for improving the\npredictive ability of the language model [2]. Yoshii et\nal. [29] proposed a vocabulary-free inﬁnity-gram model in\nwhich each chord depends on a variable-length history of\nchords. Paiement et al. [16] introduced several hidden lay-\ners of state transitions that represent the hierarchical struc-\nture of chords. Some studies attempted to explicitly de-\nscribe the generative grammar to represent the hierarchi-\ncal structure of chords [20, 23, 27]. Steedman [27] and\nRohrmeier [23] proposed a description of the production\nrules for chord sequences. A probabilistic extension is later\nstudied in the context of music arrangement and unsuper-\nvised learning of the probabilities has been performed [20].\nIn these studies, the lists of non-terminals and production\nrules were manually given based on music theories or mu-\nsical intuition.\n3. PROBABILISTIC MODELING\nThis section explains how to formulate and train our hi-\nerarchical generative model of chords and melodies. In\nour model, the PCFG for chord symbols is trained by\nunsupervised learning from a corpus of chord sequences\nwhile estimating the latent tree structures behind these se-\nquences. The metrical Markov model for chord rhythms\nis trained by supervised learning from a corpus including\nchord rhythms. The Markov model for pitch sequences\nis also trained by supervised learning from paired data of\nmelodies and chord sequences.\n3.1 Model Formulation\nThe PCFG stochastically generates a sequence of chord\nsymbols (or simply chords in the following) z=fzngN\nn=1\nand the metrical Markov model generates the correspond-\ning onset score times ϕ=fϕngN\nn=1described in units of\n16th notes, where Nis the number of chords. A subse-\nquence of pitches in the melody xn=fxn;igIn\ni=1in the\ntime span of each chord znis then generated, where In\nis the number of pitches in that time span. Concatenat-\ning all such subsequences, the whole sequence of pitches\nx=fxngN\nn=1is obtained. I=∑N\nn=1Indenotes the\nnumber of melody notes. Let  n;ibe the onset score\ntime of the melody note corresponding to xn;iand let\n =ff n;igIn\ni=1gN\nn=1.ϕnand n;ican take integer val-\nues from 0to16L\u00001, whereLis the total number of\nmeasures in the whole melody. Although in the training\nphase, we have multiple sequences of chords sequencesProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 503and melodies, we formulate here the case of a single se-\nquence for notational simplicity. Extension for multiple\nsequences is straightforward.\nThe PCFGGis deﬁned by\nG= (V;\u0006;R;S ); (1)\nwhereVis a set of non-terminal symbols, which are\nexpected to represent hierarchical structure and syntactic\nroles of chords, \u0006is a set of terminal symbols (chord sym-\nbols),Ris a set of rule probabilities, and Sis a start sym-\nbol (a non-terminal symbol located on the root of a syn-\ntax tree). Rule probabilities consist of the following three\ntypes.\u0012A!BCis the probability that a non-terminal sym-\nbolA2Vbranches to non-terminal symbols B2Vand\nC2V.\u0011A!\u000bis the probability that A2Vemits termi-\nnal symbol\u000b2\u0006. Each non-terminal symbol A2Vhas\na coin-toss probability \u0015Athat stochastically determines\nwhetherAemits (otherwise Abranches). These probabili-\nties should be normalized properly as follows:\n∑\nB;C2V\u0012A!BC= 1;∑\n\u000b2\u0006\u0011A!\u000b= 1: (2)\nWe deﬁne\u0012A=f\u0012A!BCgB;C2V,\u0011A=f\u0011A!\u000bg\u000b2\u0006,\n\u0012=f\u0012AgA2V,\u0011=f\u0011AgA2V, etc. Similar notations are\nused throughout this paper.\nThe metrical Markov model describes the transition\nprobabilities for chord onset beat positions (16th-note level\nrelative score time in a measure) as\np(ϕnjϕn\u00001) =\u0019\u0016ϕn\u00001;\u0016ϕn; (3)\nwhere \u0016ϕn=ϕnmod 16and\u0019ab(0\u0014a;b< 16)indicates\nthe transition probability from beat position kto beat po-\nsitionl. When \u0016ϕn\u0014\u0016ϕn\u00001, we interpret that the onset of\nchordnis in the next measure.\nThe Markov model is described with the following tran-\nsition probability:\np(xn;mjxn;m\u00001;zn) =\u001czn\nxn;m\u00001;xn;m; (4)\nwhere\u001cznxn;m\u00001;xn;mis the transition probability from pitch\nxn;m\u00001to pitchxn;munder chord zn. In addition, the\nprobability of the ﬁrst pitch in xnis given byp(xn;1jzn).\nWe put conjugate priors on the parameters of the PCFG\nas\n\u0012A\u0018Dirichlet (\u0018A); (5)\n\u0011A\u0018Dirichlet (\u0010A); (6)\n\u0015A\u0018Beta(\u0013A); (7)\nwhere\u0018A,\u0010Aand\u0013Aare hyperparameters. Similarly, we\nput conjugate priors on the parameters of the Markov mod-\nels as follows:\n\u0019a\u0018Dirichlet (\f); (8)\n\u001cz\nx\u0018Dirichlet (\r); (9)\nwhere\fand\rare hyperparameters.\nTo complete the generative model of chords and\nmelodies, we need to specify a model generating  . This\nmodel can be formulated as for the model of ϕand we omit\nthe details here since melodies are given as inputs for our\nharmonization problem.3.2 Bayesian Learning\nWe obtain the model parameters \u0002=f\u0012;\u0011;\u0015;\u0019;\u001cgby\nthe maximum posterior (MAP) estimation. To estimate\nthe parameters \u0012,\u0011, and\u0015of the PCFG, we use a vari-\nant of Gibbs sampling called the inside-ﬁltering-outside-\nsampling algorithm [12]. We assume that a chord sequence\nzwas derived from a latent syntactic tree t.tcan be repre-\nsented by a set of non-terminal nodes ftn:mg1\u0014n\u0014m\u0014N,\nwheretn:mis the root node of a subtree that derives a\nsubsequence of chords zn:m=fzn;zn+1;\u0001 \u0001 \u0001;zmg. The\nlatent treetand the parameters \u0012,\u0011, and\u0015are alter-\nnately sampled from the conditional posterior distribu-\ntionsp(tj\u0012;\u0011;\u0015;z)andp(\u0012;\u0011;\u0015jt;z). This algorithm is\nproven to yield samples of t,\u0012,\u0011,\u0015following the true\nposterior distribution p(\u0012;\u0011;\u0015;tjz).\nIn the inside ﬁltering step, we focus on the conditional\nprobability ( inside probability ) that a subsequence zn:mis\nderived from a subtree whose root node is A\npA\nn;m=p(zn:mjtn:m=A): (10)\nThis probability can be calculated recursively from the leaf\nnodes to the root node as follows:\npA\nn;n=\u0015A\u0011A!zn; (11)\npA\nn;n+k=∑\nB;C2V[\n(1\u0000\u0015A)\u0012A!BC∑\n1\u0014l\u0014kpB\nn;n+l\u00001pC\nn+l;n+k]\n:\nIn the outside sampling step, we recursively sample a\nlatent treetfrom a start symbol Sto the leaf nodes ac-\ncording top(tj\u0012;\u0011;\u0015;z)by using the inside probabilities.\nWhen a node tn:n+k=Ais already sampled, the two non-\nterminal symbols BandCinto whichtn:n+kbranches are\nsampled as follows:\np(l;B;C )\n=p(tn:n+l\u00001=B;tn+l:n+k=Cjtn:n+k=A;zn:n+k)\n= (1\u0000\u0015A)\u0012A!BCpB\nn;n+l\u00001pC\nn+l;n+k=pA\nn;n+k; (12)\nwhere 1\u0014l\u0014kindicates a split position.\nFinally, we sample parameters \u0012,\u0011, and\u0015according to\np(\u0012;\u0011;\u0015jt;z) =p(\u0012jt;z)p(\u0011jt;z)p(\u0015jt;z)given by\n\u0012A\u0018Dirichlet (\u0018A+uA); (13)\n\u0011A\u0018Dirichlet (\u0010A+vA); (14)\n\u0015A\u0018Beta(\u0013A+wA); (15)\nwhereuA!BC(vA!\u000b) is the number of times the binary\nproduction rule \u0012A!BC(the emission rule \u0011A!\u000b) is used\nint, andwA;0(wA;1) is the number of times a non-terminal\nsymbolAbranches (emits) and t.\nThe parameters \u0019and\u001cof the Markov models are ob-\ntained by supervised learning. Given ϕ, the posterior dis-\ntribution of\u0019can be calculated easily because of the con-\njugacy between the Dirichlet and categorical distributions.\nSimilarly, given paired data of z,ϕ,x, and , the posterior\ndistribution of \u001ccan be calculated.504 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20174. AUTOMATIC HARMONIZATION\nThis section explains how to generate sequences of chords\nfor a given melody by using the model in Section 3.\n4.1 Problem Speciﬁcation\nGiven a melody with pitches xand onset score times  and\ntrained model parameters \u0002, we aim to estimate a variable-\nlength sequence of chords zand their onset score times ϕ\nthat are not restricted to bar lines. Note that the number of\nchordsNis not ﬁxed and should be estimated and that a\nlatent treetfor chords is considered and estimated unlike\nconventional harmonization methods.\n4.2 Metropolis-Hastings Sampling\nWe propose a Metropolis-Hastings (MH) sampler with\nsplit-merge operations for generating samples of t,z, and\nϕfrom the posterior distribution p(t;z;ϕjx; ;\u0002)based\non the following four types of proposals:\n\u000fGlobal update: Update chords zand latent tree t\nwith a Viterbi algorithm for the PCFG, keeping the\nnumber and score times of chords unchanged.\n\u000fSplit operation: Randomly choose one of the\nchords and split it into two adjacent chords.\n\u000fMerge operation: Randomly choose two adjacent\nchords inzthat form a subtree with two leaves in t\nand merge them.\n\u000fRhythm update: Randomly choose one chord n\nand move its onset score time ϕnback or forth.\nAlthough it is more proper to use the inside-ﬁltering-\noutside-sampling algorithm for the global update, the\nViterbi algorithm is used for efﬁcient optimization in the\nposterior space.\nIn the MH sampling, one of these proposals is randomly\nselected. More speciﬁcally, the sampler proposes a sam-\nples\u0003= (t;z;ϕ)\u0003from a current sample s= (t;z;ϕ).\nThe sampler then judges whether s\u0003is accepted as the next\nsample or not according to the acceptance ratio given by\ng(s\u0003;s) = min{\n1;p(s\u0003)p(sjs\u0003)\np(s)p(s\u0003js)}\n; (16)\nwherep(s)is the complete joint likelihood of sbased on\nthe proposed model and p(s\u0003js)is a proposal distribution\nthat should be set appropriately. If the proposal is rejected,\nt,z, andϕare not updated. In our method, there are three\ntypes of proposal distributions for the second to fourth pro-\nposals in the above list. We estimate the most plausible z\nandϕby iterating the MH sampling a sufﬁcient number of\ntimes and then getting the latent variables that maximize\nthe likelihood of complete data.\n4.3 Updating Chord Symbols\nWe describe how to update the chord symbols zand the\ncorresponding latent tree taccording to the conditional\nposterior distribution p(t;zjϕ;x; ;\u0002).\n4.3.1 Viterbi Algorithm\nGiven a melody with pitches xand onset score times  ,\nwe can efﬁciently sample a sequence of chord symbols z\nFigure 2 : The split-merge operations of the MH sampling.\nand the corresponding latent tree tby using the Viterbi al-\ngorithm. Our algorithm differs from a standard Viterbi al-\ngorithm used for estimating tfor a givenzbecause botht\nandzare the latent variables to be estimated in this paper.\nWe ﬁrst recursively calculate the inside probabilities\nfrom the layer of terminal symbols zto the start symbol\nSaccording to\npA\nn;n=\u0015Amax\nc2\u0006\u0011A!cp(xnjc)1=In; (17)\npA\nn;n+k= (1\u0000\u0015A) max\nB;C2V\n1\u0014l\u0014k\u0012A!BCpB\nn;n+l\u00001pC\nn+l;n+k;\nwherep(xnjc)is the probability that a pitch subsequence\nxnis generated conditionally on a chord c:\np(xnjc) =p(xn;1jc)In∏\ni=2p(xn;ijxn;i\u00001;c): (18)\nThe most likely tandzare obtained by recursively back-\ntracking the most likely paths from the start symbol S.\n4.3.2 Split-Merge Operations\nUsing the MH sampler, we can split a chord or merge ad-\njacent chords by considering the underlying tree tand the\nemission probability of the melody. Note the split and\nmerge operations are inverse to each other and that the la-\ntent treetis locally updated by these operations (Fig. 2).\nIn the split operation, a new sample s\u0003is proposed\nby stochastically selecting a chord znfromz, splitting\nznintozLandzR, selecting the new onset score time\nϕ\u00032(ϕn;ϕn+1) = [ϕn+ 1;ϕn+1\u00001], and splitting the\nnon-terminal symbol tn:ninto two non-terminal symbols\ntLandtR. The proposal distribution p(s\u0003js)is thus\np(s\u0003js) ={\u0012tn:n!tLtR\u0011tL!zL\u0011tR!zR\nN(ϕn+1\u0000ϕn\u00001); ϕ n+1\u0015ϕn+ 1;\n0; otherwise:\n(19)\nThe reverse proposal distribution p(sjs\u0003), on the other\nhand, is same as the proposal distribution for the merge\noperation in which a sample sis proposed by stochasti-\ncally selecting a pair of adjacent chords zLandzR, merg-\ning those chords into zn, by selecting a chord znaccording\nto the probability \u0011tn:n!zn. Thus we have\np(sjs\u0003) =\u0011tn:n!zn\n#MergeableNodes (s\u0003); (20)\nwhere #MergeableNodes (s\u0003)is the number of pairs of ad-\njacent chords that can be merged in s\u0003, i.e., those chords\nforming a subtree with two leaves.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 505The likelihood ratio of p(s\u0003)top(s)is then given by\np(s\u0003)\np(s)=(1\u0000\u0015tn:n)\u0012tn:n!tLtR\u0015tL\u0011tL!zL\u0015tR\u0011tR!zR\n\u0015tn:n\u0011tn:n!zn\n\u0001p(xLjzL)p(xRjzR)p(ϕ\u0003jϕn)p(ϕn+1jϕ\u0003)\np(xnjzn)p(ϕn+1jϕn);\n(21)\nwherexLandxRare the subsequences of pitches obtained\nby splittingxnat the score time ϕ\u0003. Using Eqs. (19), (20),\nand (21), we can calculate the acceptance ratio of s\u0003ac-\ncording to Eq. (16).\nIn the merge operation, on the other hand, a new sample\ns\u0003is proposed in a similar way to the split operation. More\nspeciﬁcally, the acceptance ratio of s\u0003given by Eq. (16)\ncan be calculate by exchanging sands\u0003in Eqs. (19), (20),\nand (21). Through the split-merge operations, the number\nof chordsNis optimized stochastically.\n4.4 Updating Chord Rhythms\nWe describe how to update the chord rhythms ϕaccording\nto the conditional posterior distribution p(ϕjt;z;x; ;\u0002).\nA new sample s\u0003is proposed by stochastically selecting\na chordnand moving ϕnto a new score time ϕ\u0003\nn2\n(ϕn\u00001;ϕn+1). The proposal distribution p(s\u0003js)and the\nreverse proposal distribution p(sjs\u0003)are given by\np(s\u0003js) =p(sjs\u0003) =1\nN\u000011\nϕn+1\u0000ϕn\u00001\u00001:(22)\nThe likelihood ratio of p(s\u0003)top(s)is given by\np(s\u0003)\np(s)=p(x\u0003\nn\u00001jzn\u00001)p(x\u0003\nnjzn)p(ϕ\u0003\nnjϕn\u00001)q(ϕn+1jϕ\u0003\nn)\np(xn\u00001jzn\u00001)p(xnjzn)p(ϕnjϕn\u00001)p(ϕn+1jϕn);\n(23)\nwherex\u0003\nn\u00001andx\u0003\nnare the subsequences of pitches in the\ntime spans of chords n\u00001andnwith the new onset score\ntimeϕ\u0003\nn. Using Eqs. (22) and (23), we can calculate the\nacceptance ratio of s\u0003according to Eq. (16).\n5. EVALUATION\nIn this section, we report two experiments conducted to\nquantitatively evaluate the proposed generative model and\nthe proposed method of automatic harmonization based on\nthe model and discuss examples of chord sequences gener-\nated by the method.\n5.1 Experimental Conditions\nTo learn the PCFG unsupervisedly, we extracted 1002\nchord sequences corresponding to sections (e.g., verse,\nbridge, and chorus) from 468 pieces of popular music in-\ncluded in the SALAMI dataset [26]. Only those sequences\nwith a length between 8 and 32 were chosen. The vocab-\nulary of chord symbols was given by the combinations of\n12 root notes fC, C#, D, ..., B gand 2 chord types fmajor,\nminor g, and a special “other”. The values of the hyperpa-\nrameters were all set to 0:1.To train the two Markov models in a supervised manner,\nwe extracted 9902 pairs of melodies and the correspond-\ning chord sequences from 194 pieces of popular music in-\ncluded in Rock Corpus [4]. The values of the hyperparam-\neters were all set to 0:1.\nIn the testing phase, we extracted 339 pairs of melodies\nand the corresponding chord sequences as ground-truth\ndata for evaluation from 69 pieces of popular music in-\ncluded in the RWC music database [7, 8]. Note that all the\ndata (SALAMI, Rock Corpus, and RWC) were transposed\nto C major or C minor.\n5.2 Evaluating Ability of Melody Prediction\nTo evaluate the hierarchical generative model based on the\nPCFG in terms of the ability of melody prediction, we cal-\nculated the marginal likelihood for the melodies extracted\nfrom the RWC music database. The number of kinds of\nnon-terminal symbols, or the complexity of the PCFG, K\nwas changed from 1 to 20. In each of cases for K, we ob-\ntained different PCFG’s parameters with Gibbs sampling\nand calculated the marginal likelihood for each parameter\nset. The number of different parameter sets were between\n37 and 50 depending on the computational complexity. We\nassumed that the chord onsets were completely synchro-\nnized with bar lines such that the chord sequences were\nmarginalized analytically. The proposed model was com-\npared with an HMM that learns the chord-symbol tran-\nsition between adjacent units which were either musical\nnotes or measures. When minimum time units were mu-\nsical notes, each note was assumed to be generated con-\nditionally on the chord symbol at the time. When mini-\nmum time units were musical measures, notes accompa-\nnying each chord were assumed to be generated according\nto the probability described in Eq. (4).\nThe marginal likelihood of the trained model parame-\nters\u0002for an unseen melody Xwith can be calculated\nwith the inside algorithm in Section 3.2. To sum over all\npossibilities of a latent chord sequence Zand a latent tree\nT,pA\ni;iin Eq. (11) is replaced with\npA\ni;i=\u0015A∑\nc2\u0006\u0011A!cp(Xijc); (24)\nwherep(Xijc)is given by Eq. (18). The average marginal\nlikelihood Lper note for the melody is given by\nL=1\nIlogp(Xj ;\u0002) =1\nIlogpS\n0;N\u00001; (25)\nwhereI(N) is the number of notes (chords).\nThe experimental results are shown in Fig. 3. The pro-\nposed model outperformed the HMM, whether the mini-\nmum time unit is a musical note ( L=\u00003:2813 ) or a mea-\nsure ( L=\u00002:3218 ). The likelihood tended to decrease as\nthe value ofKincreased to eight, and the likelihood tended\nto increase as the value of Kincreased beyond eight.\n5.3 Evaluating Predictive Ability of Chord Sequences\nTo evaluate the proposed harmonization method in terms\nof the predictive ability of unseen chord sequences, we\ngenerated chord sequences for the melodies of the RWC506 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 3 : Marginal likelihood for melodies per note. In the\nbox plots, the red line, the black cross and the red crosses\nindicate the median, the mean and outliers, respectively.\nFigure 4 : Accuracy of harmonization per note. Indicators\nin the box plots are the same as those in Fig. 3\nmusic database and calculated the accuracy at a 16th-note\nlevel compared with the ground-truth. The complexity of\nthe PCFG,Kwas changed from 1 to 20. The proposed\nmethod was compared with a conventional HMM-based\nmethod that represents chord transitions on a 16th-note-\nlevel grid.\nThe experimental result are shown in Fig. 4. The\nproposed model clearly outperformed the HMM-based\nmethod with an accuracy of 16.6 %. While a certain range\nofKshowed much higher accuracy (e.g., 26 %) than the\nHMM-based method, there was little correlation between\nKand the median values of accuracies.\n5.4 Generated Example and Discussion\nFig. 5 shows how the proposed MH sampling method with\nsplit-merge operations worked for automatic harmoniza-\ntion1. The number of kinds of non-terminal symbols, K,\nwas set to 12. The chord sequence at the top shows an ini-\ntial sample in which the chord symbols were optimized by\nthe Viterbi algorithm, but the chord onsets were located at\nthe bar lines. The second chord sequence shows a sam-\nple proposed by moving the onset positions of 5th and\n6th chords (G major and C major). The third chord se-\nquence shows a sample proposed by merging the 7th and\n8th chords (F major and C major) into one chord (C ma-\njor). The bottom chord sequence shows the best sample\nthat maximizes the likelihood for the given melody. In\neach of the processes, the likelihood increased. The result\nindicates that the proposed method can successfully gen-\n1Some chord sequences generated in this experimental are available\nonline: http://sap.ist.i.kyoto-u.ac.jp/members/tsushima/ismir2017/\nFigure 5 : Sampling-based estimation of the most likely\nchord sequence for a given melody.\nerate a variable-length sequence of chords by considering\nthe latent tree structure behind the chord sequence.\nWe found some problems to be tackled in the future.\nThe proposed method tended to generate simple chords\n(e.g., C major and A minor). This is because the chord\nsymbols were reﬁned by using the Viterbi algorithm. In\naddition, the number of the most plausible chord sequences\nselected in our experiment was rarely more than those ini-\ntialized at the beginning of sampling. This is because the\nproposals of the split operation were accepted less fre-\nquently than the proposals of the merge operation.\n6. CONCLUSION\nThis paper presented an automatic harmonization method\nthat generates a variable-length chord sequence for a given\nmelody based on music rules hidden in corpora of popular\nmusic. The experimental results showed that the proposed\nmodel outperformed the HMM-based method in terms of\npredictive ability and has a large potential for statistical\nmusic composition or arrangement.\nSince our method is based on statistical learning, it was\nfound to prefer simpler and basic chord sequences. More\nspeciﬁcally, the number of generated chords tends to be\nless than the number of measures. This problem could be\nsolved by giving more chances to the split operation in\nMCMC sampling. To increase the diversity of generated\nchord symbols, a sampling or beam-search method is con-\nsidered to be effective instead of the Viterbi algorithm that\ntends to ﬁnd a popular chord sequence that has the highest\nposterior probability from the statistical viewpoint.\nWe still need further studies on our model. In this pa-\nper, one measure with the time signature of 4/4 is divided\ninto 16 time units. It is therefore important to investigate\nthe best time resolution and extend the model to deal with\nother kinds of time signatures. In addition, to evaluate the\nmusical appropriateness of generated chord sequences, we\nplan to conduct a subjective listening test and evaluate how\nconsistent our model is with music theories or musical in-\ntuition.\nAcknowledgement: This study was partially supported by JSPS\nKAKENHI Grant Numbers 26700020, 16H01744, and 16J05486\nand JST ACCEL No. JPMJAC1602.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 5077. REFERENCES\n[1]M. Allan and C. Williams. Harmonising chorales by\nprobabilistic inference. In Advances in Neural Infor-\nmation Processing Systems (NIPS) , pages 25–32, 2005.\n[2]S. F. Chen and J. Goodman. An empirical study of\nsmoothing techniques for language modeling. In As-\nsociation for Computational Linguistics (ACL) , pages\n310–318, 1996.\n[3]C. H. Chuan and E. Chew. A hybrid system for auto-\nmatic generation of style-speciﬁc accompaniment. In\nProceedings of the 4th International Joint Workshop\non Computational Creativity , pages 57–64, 2007.\n[4]T. D. Clercq and D. Temperley. A corpus analysis of\nrock harmony. Popular Music , 30(01):47–70, 2011.\n[5]K. Ebcio ˘glu. An expert system for harmonizing four-\npart chorales. Computer Music Journal , 12(3):43–51,\n1988.\n[6]S. Fukayama, K. Yoshii, and M. Goto. Chord-\nsequence-factory: A chord arrangement system mod-\nifying factorized chord sequence probabilities. In IS-\nMIR, 2013.\n[7]M. Goto. Aist annotation for the RWC music database.\nInISMIR , pages 359–360, 2006.\n[8]M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical and jazz mu-\nsic databases. In ISMIR , volume 2, pages 287–288,\n2002.\n[9]R. Groves. Automatic harmonization using a hidden\nsemi-Markov model. In Proceedings of the Artiﬁ-\ncial Intelligence and Interactive Digital Entertainment\nConference (Boston, MA) , pages 48–54, 2013.\n[10] M. Hamanaka, K. Hirata, and S. Tojo. Implementing\n‘A generative theory of tonal music’. Journal of New\nMusic Research , 35(4):249–277, 2006.\n[11] M. Hamanaka, K. Hirata, and S. Tojo. Musical struc-\ntural analysis database based on GTTM. In ISMIR ,\npages 325–330, 2014.\n[12] M. Johnson, T. L. Grifﬁths, and S. Goldwater. Bayesian\ninference for PCFGs via Markov chain Monte Carlo. In\nNorth American Chapter of the Association for Com-\nputational Linguistics Human Language Technologies\n(NAACL-HLT) , pages 139–146, 2007.\n[13] F. Lerdahl and R. Jackendoff. A Generative Theory of\nTonal Music . MIT press, 1985.\n[14] W. Maler. Beitrag zur Durmolltonalen Harmonielehre\nI (7th ed.) . F. E. C. Leuckart, 2007.\n[15] E. Nakamura, M. Hamanaka, K. Hirata, and K. Yoshii.\nTree-structured probabilistic model of monophonic\nwritten music based on the generative theory of tonal\nmusic. In IEEE ICASSP , pages 276–280, 2016.[16] J. F. Paiement, D. Eck, and S. Bengio. A probabilistic\nmodel for chord progressions. In ISMIR , pages 312–\n319, 2005.\n[17] J. F. Paiement, D. Eck, and S. Bengio. Probabilistic\nmelodic harmonization. In Conference of the Cana-\ndian Society for Computational Studies of Intelligence ,\npages 218–229, 2006.\n[18] G. Papadopoulos and G. Wiggins. AI methods for al-\ngorithmic composition: A survey, a critical view and\nfuture prospects. In AISB Symposium on Musical Cre-\nativity , pages 110–117, 1999.\n[19] R. D. Prisco and R. Zaccagnino. An evolutionary mu-\nsic composer algorithm for bass harmonization. Ap-\nplications of Evolutionary Computing , pages 567–572,\n2009.\n[20] D. Quick. Learning production probabilities for mu-\nsical grammars. Journal of New Music Research ,\n45(4):295–313, 2016.\n[21] S. A. Raczy ´nski, S. Fukayama, and E. Vincent. Melody\nharmonization with interpolated probabilistic models.\nJournal of New Music Research , 42(3):223–235, 2013.\n[22] H. Riemann. Harmony Simpliﬁed: or, The Theory of\nthe Tonal Functions of Chords . Augener, 1896.\n[23] M. Rohrmeier. Mathematical and computational ap-\nproaches to music theory, analysis, composition and\nperformance. Journal of Mathematics and Music ,\n5(1):35–53, 2011.\n[24] R. Scholz, E. Vincent, and F. Bimbot. Robust model-\ning of musical chord sequences using probablistic N-\ngrams. In IEEE ICASSP , pages 53–56, 2009.\n[25] I. Simon, D. Morris, and S. Basu. Mysong: automatic\naccompaniment generation for vocal melodies. In Pro-\nceedings of the SIGCHI Conference on Human Factors\nin Computing Systems , pages 725–734. ACM, 2008.\n[26] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga, D. D.\nRoure, and J. S. Downie. Design and creation of a\nlarge-scale database of structural annotations. In IS-\nMIR, pages 555–560, 2011.\n[27] M. J. Steedman. A generative grammar for jazz chord\nsequence. Music Perception , 2(1):52–77, 1984.\n[28] M. Towsey, A. Brown, S. Wright, and J. Diederich. To-\nwards melodic extension using genetic algorithms. Ed-\nucational Technology & Society , 4(2):54–65, 2001.\n[29] K. Yoshii and M. Goto. A vocabulary-free inﬁnity-\ngram model for nonparametric bayesian chord progres-\nsion analysis. In ISMIR , pages 645–650, 2011.508 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "The Music Listening Histories Dataset.",
        "author": [
            "Gabriel Vigliensoni",
            "Ichiro Fujinaga"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417499",
        "url": "https://doi.org/10.5281/zenodo.1417499",
        "ee": "https://zenodo.org/records/1417499/files/VigliensoniF17.pdf",
        "abstract": "We introduce the Music Listening Histories Dataset (MLHD), a large-scale collection of music listening events assembled from more than 27 billion time-stamped logs extracted from Last.fm. The logs are organized in the form of listening histories per user, and have been con- veniently preprocessed and cleaned. Attractive features of the MLHD are the self-declared metadata provided by users at the moment of registration whose identities have been anonymized, MusicBrainz identifiers for the music entities in each of the logs that allows for an easy linkage to other existing resources, and a set of user profiling fea- tures designed to describe aspects of their music listening behavior and activity. We describe the process of assem- bling the dataset, its content, its demographic characteris- tics, and discuss about the possible uses of this collection, which, currently, is the largest research dataset of this kind in the field.",
        "zenodo_id": 1417499,
        "dblp_key": "conf/ismir/VigliensoniF17",
        "keywords": [
            "Music Listening Histories Dataset (MLHD)",
            "large-scale collection of music listening events",
            "Last.fm logs",
            "time-stamped logs",
            "organized in the form of listening histories per user",
            "conveniently preprocessed and cleaned",
            "self-declared metadata",
            "MusicBrainz identifiers",
            "easy linkage to other existing resources",
            "user profiling features"
        ],
        "content": "THE MUSIC LISTENING HISTORIES DATASET\nGabriel Vigliensoni and Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Technology (CIRMMT)\nMcGill University, Montr ´eal, Qu ´ebec, Canada\n[gabriel.vigliensonimartin,ichiro.fujinaga]@mcgill.ca\nABSTRACT\nWe introduce the Music Listening Histories Dataset\n(MLHD), a large-scale collection of music listening events\nassembled from more than 27 billion time-stamped logs\nextracted from Last.fm. The logs are organized in the\nform of listening histories per user, and have been con-\nveniently preprocessed and cleaned. Attractive features\nof the MLHD are the self-declared metadata provided by\nusers at the moment of registration whose identities have\nbeen anonymized, MusicBrainz identiﬁers for the music\nentities in each of the logs that allows for an easy linkage\nto other existing resources, and a set of user proﬁling fea-\ntures designed to describe aspects of their music listening\nbehavior and activity. We describe the process of assem-\nbling the dataset, its content, its demographic characteris-\ntics, and discuss about the possible uses of this collection,\nwhich, currently, is the largest research dataset of this kind\nin the ﬁeld.\n1. INTRODUCTION\nThe modeling of users for multimedia information retrieval\nsystems has been a research topic since the ﬁrst Inter-\nnational Symposium on Music Information Retrieval (IS-\nMIR) in 2000. In that meeting, it was observed that to\ncreate modern, more efﬁcient, and personalized music in-\nformation retrieval systems, the modeling of users would\nbe necessary because many features of multimedia content\ndelivery are perceptual and user-dependent [6].\nSixteen years after the ﬁrst ISMIR meeting, the land-\nscape of music consumption has changed enormously. The\nrise and fall of peer-to-peer networking led to the reinven-\ntion of the music industry: the paradigmatic music product\nwas no longer a full album in a physical format, but indi-\nvidual music ﬁles available in online digital music stores.\nThanks to the miniaturization of portable media players\nand also to almost ubiquitous Internet access, a change of\nparadigm in music consumption has happened again, and\npeople seem to not want to pay for individual tracks. In-\nstead, they are willing to pay for services that allow them to\nc\rGabriel Vigliensoni and Ichiro Fujinaga. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Gabriel Vigliensoni and Ichiro Fujinaga. “The music\nlistening histories dataset”, 18th International Society for Music Informa-\ntion Retrieval Conference, Suzhou, China, 2017.access, search, and discover music items—artists, albums,\nor tracks—within large repositories [23].\nOn-demand digital music streaming services are cur-\nrently the fastest growing sector of the global music in-\ndustry [11]. In fact, in 2015 the digital revenues that these\nsystems generated overtook the income from physical mu-\nsic goods for the ﬁrst time in music industry history [12].\nAs a result, the on-demand music streaming landscape\nthese days seems to be a lucrative battleﬁeld, and one on\nwhich many companies want to compete. However, since\nthe majority of the listeners’ accounts in music streaming\nservices use the “free” or “freemium” business model—\nadvertisement-supported basic streaming services—a large\nshare of the income of music and media streaming com-\npanies comes from targeting ads more precisely at listen-\ners [18]. It seems that the streaming model is like modern\nadvertising.\nIn this model, people are no longer passive observers\nbut direct participants in the battleﬁeld that is the digi-\ntal media and music streaming landscape. In fact, the\ntraded goods in this business are individual proﬁles and\npsycographic traits (e.g., interests, lifestyle, personality,\nvalues) which are extracted from correlating people’s lis-\ntening habits with their sociographic characteristics [17].\nAs a result, listeners are the source of information, but they\nalso are the ﬁnal target for all the advertising these compa-\nnies are making money from.\nAs music information researchers, our community has\nto be able to observe, investigate, and to gather insights\nfrom the listening behavior of people in order to develop\nbetter, personalized music retrieval systems. Yet, since\nmost media streaming companies know that the data they\ncollect from their customers is very valuable, they usually\ndo not share their datasets. A honorable mention goes to\nNetﬂix, company that challenged the recommendation re-\nsearch community in 2006 with a large dataset of ratings\nof users on movies. Insights and techniques developed for\nthat competition are still being used widely today.\n2. PREVIOUSLY A VAILABLE DATASETS\nA number of datasets for music listening research have\nbeen collected and released by research groups. These\ndatasets provide information relating the interaction of a\nlarge number of listeners and music items.\nCelma assembled the Last.fm Dataset-360K, a dataset\nof playcounts with listeners’ demographic data for 360K96Dataset name Type Source Size Demographics Linkage Other\nLast.fm Dataset 360-K [5] Playcounts Last.fm 18M logs,\n360K usersYes Yes Only includes most frequently listened artists\nLast.fm Dataset 1-K [5] Listening\nhistoriesLast.fm 19M logs,\n1K usersYes Yes Full music listening histories\nYahoo! Music Dataset [7] Ratings Yahoo!\nMusic\nRadio262M logs,\n1M usersNo No Hierarchical structure of music items\nHetRec2011-last.fm.2k [4] Playcount Last.fm 2K users No No Bidirectional users’ relations and artist tags\nEcho Nest Taste Proﬁle subset [13] Playcounts Undisclosed 48M logs,\n1.2M usersNo Yes Linked to Million Song Dataset\nEMI Million Interview Dataset [8] Interviews Individual\ninterviews1M users Yes Unknown Partial information available\nMusicMicro 11.11-09.12 [19] Listening\nhistoriesTwitter 600K logs,\n137K usersGeolocalized\nlogsNo Precise geolocation data of each log\nMillion Musical Tweets Dataset [9] Listening\nhistoriesTwitter 1M logs,\n215K usersGeolocalized\nlogsYes Many users have only a few listening events\n#nowplaying Music Dataset [24] Listening\nhistoriesTwitter 50M logs,\n4.2M usersNo Yes Many users have only a few listening events\nLFM-1B [20] Listening\nhistoriesLast.fm 1B logs,\n120K usersYes No Comes with a set of features describing music con-\nsumption behavior. The music listening histories are\nshifted according to the time zone of listeners, and so\nthey are not directly comparable.\nMLHD Listening\nhistoriesLast.fm 27B logs,\n583K usersYes Yes Comes with MBIDs, estimation of listeners’ time\nzone, and users’ activity features.\nTable 1 . Comparison of freely available datasets of music listening events.\nlisteners, and the Last.fm Dataset-1K, a set of full listen-\ning histories with time-stamped logs [5]. Though richer,\nthe latter dataset included logs for only 1K listeners. Fol-\nlowing the Netﬂix prize, Dror, Koenigstein, and Koren re-\nleased the Yahoo! Music Dataset, a collection of 1M peo-\nple’s aggregated ratings on music items [7]. Later on, Can-\ntador, Brusilovsky, and Kuﬂik presented the HetRec2011-\nLast.fm-2K, another dataset with song playcounts for the\n50 most listened artists of 2K listeners [4]. McFee et al.\nintroduced The Echo Nest Taste Proﬁle subset, a dataset\nof song playcounts of 1M listeners collected from undis-\nclosed services [13]. Neither of these two datasets, how-\never, provided timestamps of the music logs or demo-\ngraphic information about the listeners. The EMI Group\nLimited promised a dataset of 1M interviews about peo-\nple’s music appreciation, behavior, and attitudes [8], but\nonly partial information was made available. None of the\naforementioned datasets simultaneously provided individ-\nual music listening logs as well as demographic data for a\nlarge amount of listeners.\nMore recently, music listening logs have been collected\nfrom the social networking service Twitter. Schedl re-\nleased MusicMicro 11.11-09.12, a dataset of about 600K\nmusic-related tweets with temporal and spatial data [19].\nHauger et al. released the Million Musical Tweets Dataset\n[9], a collection of 1M music-related geolocalized micro-\nblog posts with partial linkages to other services. Zangerle\net al. introduced the #nowplaying Music Dataset, a col-\nlection of 50M music-related posts linked to MusicBrainz\n[24]. In these collections, however, there were a large num-ber of listeners with only one or two logs, and so, in many\ncases, the datasets provided a few listening events for many\nusers instead of listening histories.\nFinally, Schedl introduced LFM-1B, a very large dataset\nof more than 1B logs collected from Last.fm user inter-\nactions [20]. Each log includes artist, album, and track\nnames, the timestamp of the log, as well as each user’s\nLast.fm identiﬁer. The dataset also comes with users’ de-\nmographic information as well as a set of features that\ndescribe music consumption behavior per user. However,\nthe dataset does not provide common identiﬁers with other\nmusic databases, and so the only way to link the music\nitems is by string matching.\nIn Table 1 we provide a summary of available databases\nof music listening logs. We can see that among all the\ndatasets reviewed, the only one that provides full music lis-\ntening histories, listeners’ self-declared demographic data,\nas well as identiﬁers easily linkable to other databases of\nmusic information is the Last.fm Dataset-1K. However, the\nsize of the dataset is very small to perform a large-scale\nanalysis with global reach. In order to ameliorate this sit-\nuation, we decided to collect our own dataset considering\nall the aforementioned characteristics.\n3. THE MUSIC LISTENING HISTORIES DATASET\nIn this section we will describe the creation of the Mu-\nsic Listening Histories Dataset (MLHD), a large dataset of\nfull music listening histories. We will review the concept\nof music listening history and will present the criteria forProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 97the data collection and cleaning of the data. We will also\nprovide insights about the demographic characteristics of\nusers in the dataset and will explain the need of providing\na value for normalizing the time zone of the logs.\n3.1 Music Listening Histories and Last.fm\nListening histories are a timeline of listening events. An-\nalyzing them in a linear fashion is interesting because we\ncan observe when people consume music, and what music\nthey enjoy or do not enjoy over time. However, since peo-\nple seem to follow periodic listening cycles [10], the aggre-\ngation of these listening histories by collapsing them into\ndifferent periods of time can provide extra layers of infor-\nmation that can be used to infer people’s listening patterns\nand preferences.\nLast.fm is an online digital music service available since\n2002. It was originally conceived as a web-based radio\nstation. Immediately after its launch, the company incor-\nporated the tracking of music listening logs as a core part\nof its service. However, Last.fm stands out from most mu-\nsic streaming services that collect user data because it not\nonly gathers listening logs (known as scrobbles ) from the\ninteraction of its users withing the system’s ecosystem, but\nalso from the interaction between users and a wide range\nof third-party music and media players by means of the\nscrobbler service.\nLast.fm offers free access to the listening data they col-\nlect from listeners, as well as music metadata, biographies,\npictures, charts, tags, ranking data by country, and other\ninformation by means of a well-documented API. At the\nmoment of registration, every user must accept the Last.fm\nTerms of Use and the Last.fm Privacy Policy.1These terms\nestablish that their listening habit data will be available\nto third parties via their API for commercial and/or non-\ncommercial purposes. The users are also asked to provide\nbasic demographic information such as their date of birth,\ncountry, and gender.\nAll aforementioned characteristics, added to the fact\nthat the Last.fm API Terms of Service establish that\nLast.fm offers a “limited terminable licence to copy and\nuse the Last.fm Data” that is free of charge “for non-\ncommercial purposes”2persuaded us to choose Last.fm\nas the data source to assemble the MLHD.\n3.2 Data Collection\nIn order to retrieve full music listening histories and to ob-\ntain even data across aggregated periods of time, we fol-\nlowed previous research [1] and searched only for listeners\nwith a minimum of two years of activity since they started\nto submit music logs to Last.fm. Also, in order to prevent\ncollecting data from casual users that registered for a ser-\nvice, tried it, but never used it again, we collected data only\nfrom listeners which had an arbitrary average of, at least,\nten scrobbles per day. The two constraints forced all lis-\n1Privacy Policy available at http://www.last.fm/legal/\nprivacy\n2Terms of Service available at http://www.last.fm/api/tosteners in our dataset to have a minimum of 7,300 (i.e., 365\n\u00022\u000210) music logs submitted to the Last.fm database.\nDifferently to all other datasets with Last.fm data,\nwe collected listening data by using an undocumented\n(but deprecated) method that allowed us to not need\nactual usernames for calling the Last.fm Web services\n[21]. Instead, we simply passed Last.fm users’ inter-\nnal identiﬁers as arguments of the API requests. Since\nthese IDs are sequential, this approach permitted us to\nsample users randomly across the entire database in-\nstead of sampling users based on their friends or on an\nartist’s top fans , which are methods probably more bi-\nased. We aimed to collect full listening histories, and so\nwe fetched people’s listening logs by using the Last.fm’s\nAPI method user.getRecentTracks() , and paginated it-\neratively throughout the chosen listeners’ full music listen-\ning histories.\n3.3 Data Cleaning, Sanitization, and Organization\nWithin each music listening history, we organized each\nof the logs in quadruples with the form of <timestamp,\nartist-MBID, release-MBID, track-MBID> , where\ntimestamp is a global coordinated universal time (UTC)\nstamp, and MBID stands for MusicBrainz identiﬁer.\nMBIDs are 36-character universally unique identiﬁers\n(UUID) that are permanently assigned to entities within\nthe MusicBrainz database to ensure a reliable and unam-\nbiguous form of identiﬁcation. Since Last.fm exposes\nMBIDs as public identiﬁers of music entities in their\ndatabase, we collected them directly for each artist,\nrelease, and track. These three entities are hereafter de-\nnominated “music entities.” Finally, all data per user was\nstored within a single ﬁle, with the logs sorted sequentially\nby their timestamp.\nAfter close inspection of the data, we realized that there\nwere two issues in some of the listening histories: (i)\nthere were duplicated music logs (i.e., same timestamp and\nMBIDs); and (ii) some logs were too close in time (i.e., less\nthan 30 seconds apart, which is the minimum that Last.fm\nrequires to consider a played track as a valid log). We hy-\npothesized that these issues were artifacts produced by the\ninteraction of the Last.fm servers and some scrobblers. As\na result, we decided to perform a cleaning process before\nstoring the data, and so we ﬁltered out all logs with the\nsame MBID and timestamp, and we also ﬁltered out all\nscrobbles that were less than 30 seconds apart in time. All\nin all, the average percentage of duplicated logs removed\nfor each user was eight percent, and one percent for those\nlogs that were too close.\nIt is worth mentioning that sometimes the metadata pro-\nvided by the scrobbler is not enough to produce a full\nmatch for artist, release, or track. In cases like this, the\nmusic listening log returned by the Last.fm API will have\nonly partial information. As a result, not all logs in the\nMLHD have a full set of MBIDs.\nIn Figure 1 we show the percentage of all combinations\nof MBIDs across all music logs in the dataset. It can be\nseen that about 58 percent of all music logs in the MLHD98 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170006.10010.60100.20110.510010.310118.81105.411158.1\n0255075100000001010011100101110111ArtistReleaseRecordingMBIDMBIDMBIDXYZ\n\u00001Figure 1 . Percentage of music logs with combination of\nMBIDs. 0 stands for no presence of the corresponding\nMBID in the scrobble and 1 for its existence.\nhave full data (i.e., MBIDs for the three music entities),\nand 93 percent of the logs have, at least, the artist MBID.\n3.4 Data Exploration\nWe computed from the music listening histories’ UTC\ntimestamps a series of features that aggregated the num-\nber of scrobbles of each listening history into several time\nspans. These low-dimensional representations of a user ac-\ntivity may facilitate the creation of plots and their visual in-\nspection in order to gain insights or detect anomalies from\nsingle listener or groups of them. These per-user features\nare: hourly activity ,hourly activity by week hour ,weekly\nactivity ,monthly activity ,yearly activity ,weekday activity ,\nSaturday activity , and Sunday activity .\n3.5 Demographics\nThe MLHD currently consists of more than 27 billion mu-\nsic logs taken from the listening histories of 583K people\nthat have linked their digital music players to Last.fm. In\nthis massive repository, we counted more than 555K differ-\nent artists, 900K albums, and seven million tracks. Table 2\nsummarizes the number of logs, unique listeners, and mu-\nsic entities in the dataset.\nDataset Logs Listeners Artists Albums Tracks\n27MM 583K 555K 900K 7M\nTable 2 . Music listening histories dataset summary.\nThe distribution of the average number of daily submit-\nted music logs per listener is shown in Figure 2. Axes in\nthe plot are in log scale. The curve exhibits a close to\npower law characteristic. As expected, due to the con-\nstraints we set for collecting listeners’ listening histories,\nthe minimum average daily number of music logs per user\nwas ten. Listeners with an average of eleven logs were the\nlargest group, with about 30K listeners. The median num-\nber of submitted logs per user was 35K. The median age of\nthe listening histories was 4.5 years.\nNow we will describe the nature of the users in the\ndataset according to their self-declared age, gender, and\ncountry. This information is asked to the users at the mo-\nment of registration.\n100101102103104105\n101102103\nNumber of daily submitted music logsNumber of listenersFigure 2 . Distribution of the average number of daily\nscrobbles per listener.\n3.5.1 Age\nIn terms of age, 71 percent of the listeners in the dataset de-\nclared their date of birth, which is much higher than similar\ndatasets [5, 20]. Among them, 98 percent of the users had\na self-declared age within 15 and 54 years old. In spite of\nthe small magnitude of the probably deceiving information\nfound in the two percent out of this age range, we decided\nto ﬁlter them out from the dataset. The mean age of lis-\nteners in the dataset is 25.4 years old, the median is 24,\nand the mode is 22. Since these are values similar to the\nones found in similar datasets, this skew in the distribution\nindicates a bias in our dataset—and probably in Last.fm\nusers—towards youth and young adults. We show the age\ndistribution of listeners of the MLHD in Figure 3.\n0.0%2.5%5.0%7.5%10.0%\n20 30 40 50\nAgePercent\nFigure 3 . Age distribution of MLHD listeners within the\n[15, 54] years old range.\n3.5.2 Gender\nIn terms of gender, about 82 percent of the people in the\ndataset declared a gender at the moment of their registra-\ntion or afterwards. In Figure 4 we show the self-declared\ngender distribution among these users.\nacegmagecountryexploratorynessgendermainstreamness\ndataobservationsusersartistsfull week53,783,01759,183424,221weekdays47,689,82959,183411,619Pearson correlation between users (Resnick et al 1994)However, recommenderlab formula is different! weekend30,689,68059,137370,857 gabrielantoniasantiagovitoantonia0.88santiago0.67NAvito0.501.00NAjustina0.501.000.500.97 DatasetListenersLogsArtistsAlbumsTracks594K27MM555K900K7MListener’sMin1st QuartileMedianMean3rd QuartileMaxAge (years)0212425.427113Number of logs7K24K37K49K60K998KLogs lifetime (days)73111921653172121883929GenderDeclaredNon-declaredFemaleMale(%)81.618.428.7071.30Cosine similarity (Lee 08)However, recommenderlab formula is different! User typeAlumniModeratorStaffSubscriberUserOBS: This method does not consider user ratings' average! (number and %)70 (~0%)21 (~0%)33 (~0%)14K (2.4%)580K (97.6%) gabrielantoniasantiagovitoantonia0.66santiago0.540.42vito0.610.430.49 DatasetLogsListenersArtistsAlbumsTracksjustina0.720.550.640.84 27MM594K555K900K7MListener’sMin1st QuartileMean3rd QuartileMaxAge (years)02125.427113Number of logs7K24K49K60K998KLogs lifetime (days)7311192172121883929WikiSame results with recommenderlab DeclaredNon-declaredAge70.529.5Country81.818.2side featurevaluelinear_termsfactor 1factor 2Gender81.618.4ageo0.3615-0.3872-0.1737agey-0.40560.16480.4031genderm-0.18340.3986-0.1552Age groups15—2425—3435—4445—54genderf-0.2302-0.35390.14510.5750.3580.0550.01217181920AVG Logs27MMDemographic%Age groups%Listeners594KAge70.515—2457.564.0 Artists555KCountry81.825—3435.858.8 Albums900KGender81.635—445.543.6 Tracks7M45—541.21.2\nItemsNo.Demographic%Age groups%Logs27MMAge70.515—2457.5Listeners594KCountry81.825—3435.8Artists555KGender81.635—445.5Albums900K45—541.2Tracks7MPEARSONItemsNo.Demographic%Age groups%useru1u2u3u4Logs27MMAge70.515—2457.5u20.93Listeners594KCountry81.825—3435.8u30.520.20Artists555KGender81.635—445.5u4-0.390.96-0.51Albums900K45—541.2u5-0.061.00-0.300.94Tracks7M\n%025507510015—2425—3435—4445—54\n%fmnWoman23.3916803953871136308339229107183582720Man58.214751510159323.391680395387158.214751510159318.3935680944536Undeclared18.3935680944536\n%0255075100\n23.4\n58.2\n18.4UndeclaredManWoman\n\u00001\nFigure 4 . Percentage of listeners’ self-declared gender.\nIt can be seen that there is a bias towards male listeners\nin the MLHD. Since this bias is also observed in similarProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 99dataset, this can be an indication that Last.fm has more\nmale than female users.\nWe compared the age within each self-declared gender\nwith balanced groups. The total number of listeners with-\nout self-declared gender was slightly more than 100K, and\nso we sampled 100K listeners from each group. The mean\nof the Not declared ( \u0016= 25:67) and Male ( \u0016= 25:60)\ngroups did not differ greatly ( p=:400), perhaps indicat-\ning that the ﬁrst group may have a large proportion of male\nusers. On the other hand, users self-declared as Female\n(\u0016= 22:99) had a different lower mean age than the Male\ngroup (p<: 001). In other words, users in our dataset self-\ndeclared as Female are younger than the ones declared as\nMale.\n3.5.3 Country\nIn terms of location, 82 percent of users in the MLHD self-\nreported a country. These users belong to 239 different\ncountries or territories as deﬁned in the ISO 3166-1 In-\nternational Standard for country codes. Among these ter-\nritories, 19 countries had at least one percent of the total\namount of listeners in the dataset. These “top countries”\ncombined accounted for more than 85 percent of the total\nnumber of listeners in the dataset.\nIn order to determine how countries were relatively rep-\nresented in the MLHD, we divided the percentage of users\nper country by the actual country population.3This metric\ngave us a better description about how different countries’\npopulations were represented in our dataset. In Figure 5\nwe show a map that presents the relative number of listen-\ners per country normalized by the corresponding number\nof inhabitants in each country.\n12020Last.fm penetration per country vigintiles1 \nFigure 5 . Relative number of listeners per country, nor-\nmalized by the number of inhabitants in each country.\nThe color palette of the plot was based on vigintiles\n(20 quantiles) of the data, with red indicating the high-\nest vigintile, and blue the lowest one. If our dataset has\nsimilar distinctive qualities in comparison with the overall\nLast.fm data, this map can be interpreted as the Last.fm\nmarket penetration by country. By looking at the higher\nvigintiles we can see that listeners from most zones are\nrepresented in the MLHD. In particular, Northern Euro-\npean, North American, and Australasian countries have\n3Population data for the year 2012 taken from the World Bank\nOpen Data repository, available at http://data.worldbank.org/\nindicator/SP.POP.TOTLthe largest proportion of listeners submitting music logs\nto Last.fm. Also, some countries in South America show\nsimilar penetration levels to some Mediterranean countries\nin Europe. People from Africa, South Asia, and Far East\nAsia are not extensively represented in our dataset.\nFinally, pair-wise mean age comparison using balanced\ngroups of listeners per country (N= 4.5K) showed signif-\nicant differences between listeners from some of the top\ncountries. For example, Brazilian listeners are younger on\naverage (\u0016= 22:6) than all other top countries ( p<: 001),\nexcept for Poland, Russia, and Ukraine. On the other hand,\nJapanese listeners are older on average ( \u0016= 29:0) than\nusers from all the other top countries ( p < : 001), except\nfor Spain and France.\n3.6 Time Zone Normalization\nLast.fm collects scrobbles using the Unix time stamp for-\nmat no matter where the logs were generated. Therefore,\nall music logs within the Last.fm database have the same\ntemporal point of reference. Beyond the timestamp and the\nMBID for the three music entities, the logs do not store any\nadditional geographical information such as city, country,\nor the time zone where they were generated.\nThe lack of information about where the logs were ac-\ntually generated can be a problem. If the researcher wants\nto ﬁnd trends in people’s daily, weekly, and monthly music\nlistening behavior, it is necessary to aggregate their mu-\nsic listening histories over time. However, the aggregated\nlistening patterns from people in different time zones is\nshifted depending on where they are. As a result, it would\nbe misleading to directly compare their patterns. The coun-\ntry information could be used to estimate a listener’s time\nzone, but many countries span their territories over several\ntime zones.\nIn previous studies with similar data, the researchers\nhave hand-picked listeners within the same time zone\n[2, 3, 16] or are just compared their daily listening patterns\ndirectly [20]. However, a research dataset to perform stud-\nies at the global level must provide this information in or-\nder to properly compare the music listening histories.\nWe followed an approach for time zone normalization\nbased on the assumption that people share hours of sleep at\nnight [21], and computed the time shift of the listening his-\ntories in the MLHD. In Figure 6 we show the estimated dis-\ntribution of the time zones of all listeners in the dataset. We\ncan observe a peak in the estimated time zone from where\npeople submitted music logs at time zone GMT +0, with\nabout 17 percent of the dataset users. Additionally, a large\nproportion of the listeners were estimated to be within time\nzones corresponding to Western Europe, but also spread\nout throughout the different time zones in America.\n4. CONCLUSION\nAll in all, the MLHD provides three sources of data for\neach user: (i) demographic metadata, (ii) sanitized full\nmusic listening histories, and (iii) low-dimensional fea-\nture vectors describing the full listening histories in terms100 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170.000.050.100.15\n−12−11−10−9−8−7−6−5−4−3−2−10123456789101112Time ZonePercentageListeners' timezone by correlation of their listening profile and the listening profile of listeners in the UK (N=594K)\nTime zone010051015Percentage\n-10246812-12-8-6-4-2Figure 6 . Distribution of the time zones for all users in the\ndataset (N = 583K).\nof user activity. As a result, the full music listening\nhistories compiled in the MLHD dataset offer a large\namount of information. On top of having a very ﬁne\ntime granularity—providing second-accurate data about\nthe music item played back in a media player by a speciﬁc\nuser—their aggregation into different spans of time may\nprovide clues about the people’s listening behavior charac-\nteristics and their listening trends over time.\nA big advantage of the MLHD dataset over other\ndatasets for listening behavioral research is that it is based\non MusicBrainz identiﬁers (MBID). This feature allows\nthe easy linkage of each log to, for example, all ser-\nvices of the MetaBrainz Foundation ecosystem (i.e., Mu-\nsicBrainz, AcousticBrainz, ListenBrainz, and Critique-\nBrainz) and to other services that provide additional data\naccessible through these IDs (e.g., Last.fm provides folk-\nsonomy tags for artists, albums, and tracks, and DBPedia\nlinks Wikipedia open music data to MusicBrainz by means\nof MBIDs). Therefore, music listening histories can be\nlinked to resources from other repositories, thus enabling\nthe aggregation, linkage, and expansion of the data and the\nknowledge about people’s music listening behavior.\nIn terms of possible uses of the dataset, data aggrega-\ntions extracted from the MLHD have already been used in\ncombination with other sources of data. In particular, it\nhas been used as part of the datasets for “Sound and music\nrecommendation with knowledge graphs” [15]. In these\ndatasets, a subset of music listening histories from the\nMLHD were aggregated into playcounts and used in com-\nbination with additional song data collected from Song-\nfacts.com to enable the study of hybrid music recommen-\ndation models using additional user-provided factual infor-\nmation describing songs and artists [14]. Additionally, it\nhas been used to ﬁnd listening behavioral patterns in four\ndifferent age groups and to evaluate the improvement of a\nmusic recommendation model by using demographic, pro-\nﬁling, and contextual features [22].\nWe plan to expand the MLHD by collecting more lis-\ntening data. This is a good idea in the eventual case that\nLast.fm stops providing this data or a full shutdown of theservice. Also, the data collected may be added to the Lis-\ntenBrainz project, an initiative of the MetaBrainz Founda-\ntion with the goal of allowing listeners to preserve their\nexisting music listening histories in Last.fm.\nAlthough we aimed to collect data from a large group\nof listeners of varied demographics—thus helping to over-\ncome biases from previous user-driven and data-driven\nresearch—the listening data we collected may be also bi-\nased. For example, the age distribution of listeners show\nthat the dataset is skewed towards late adolescent and early\nadult listeners. However, since this group will be older in a\nfew years from now, and younger generations are already\nborn into a digital era, we suppose that this trend may be\ndifferent in a few years, and the large skew towards listen-\ners in their early twenties may be less signiﬁcant. In any\ncase, the MLHD has a much larger amount of data than any\nof the studies of the datasets reviewed in Section 2, and so\nit allows for the undertaking of studies with balanced pop-\nulations of listeners of each age.\nWe also acknowledge that a limitation of conducting\ndata-centric studies using data collected from listening in-\nteractions with media players and music streaming services\nis the fact that it is hard to know if listeners actually chose\nthe music item they were exposed to, or it was the recom-\nmendation engine or shufﬂe algorithm of a music stream-\ning service the one that suggested the music item. As a\nresult, it is hard to say if a speciﬁc scrobble reﬂected the\nactual preference of a listener, or if it registered what was\nrecommended by a recommendation or shufﬂe algorithm.\nHowever, Wikstr ¨om [23] pointed out that ubiquitous ac-\ncess to music services with recommendation algorithms is\nhow the majority of people are actually experiencing music\nin the new music economy. Hence, the study of music pref-\nerence nowadays cannot separate self-chosen music from\nalgorithmically generated playlists and suggestions. These\ntwo approaches are occurring at the same time, and so both\nhave to be considered in order to obtain insights about\nlistening behaviors and music preferences. We hope the\ndataset we introduced will be useful for doing large-scale\nresearch on user modeling, music preference, and recom-\nmendation.\nThe MLHD can be accessed and downloaded at\nhttp://ddmal.music.mcgill.ca/research/\nmusiclisteninghistoriesdataset .\n5. ACKNOWLEDGEMENTS\nWe are extremely grateful of all users of Last.fm that have\nagreed to make their data available for non-commercial\nuse, and also to the Last.fm service, which has collected\nand offered this data since 2002 uninterruptedly, helping\nthe ﬁeld of music informatics research to move forward.\nThis research has been supported by BecasChile Bicen-\ntenario, Comisi ´on Nacional de Ciencia y Tecnolog ´ıa, Go-\nbierno de Chile, and the Social Sciences and Humanities\nResearch Council of Canada. Important parts of this work\nused ComputeCanada’s High Performance Computing re-\nsources.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 1016. REFERENCES\n[1] Dominikus Baur, Jennifer B ¨uttgen, and Andreas Butz.\nListening factors: A large-scale principal components\nanalysis of long-term music listening histories. In Pro-\nceedings of the 2012 ACM Annual Conference on Hu-\nman Factors in Computing Systems , pages 1273–6,\nAustin, TX, 2012.\n[2] Pauwke Berkers. Gendered scrobbling: Listening be-\nhaviour of young adults on Last.fm. Interactions: Stud-\nies in Communication & Culture , 2(3):279–96, 2010.\n[3] Jennifer B ¨uttgen. What’s in a history? A large-scale\nstatistical analysis of Last.fm data. Master’s thesis,\nLudwig-Maximilians-Universit ¨at M ¨unchen, M ¨unchen,\nGermany, 2010.\n[4] Iv ´an Cantador, Peter Brusilovsky, and Tsvi Kuﬂik.\nLast.fm web 2.0 dataset. In 2nd Workshop on Informa-\ntion Heterogeneity and Fusion in Recommender Sys-\ntems, RecSys 2011, Chicago, IL, 2011.\n[5]`Oscar Celma. Music Recommendation and Discovery:\nThe Long Tail, Long Fail, and Long Play in the Dig-\nital Music Space . Springer-Verlag Berlin Heidelberg,\nBerlin, Germany, 2010.\n[6] Wei Chai and Barry Vercoe. Using user models in mu-\nsic information retrieval systems. In Proceedings of the\n1st International Symposium on Music Information Re-\ntrieval , Plymouth, MA, 2000.\n[7] Gideon Dror, Noam Koenigstein, Yehuda Koren, and\nMarkus Weimer. The Yahoo! music dataset and KDD-\ncup’11. Journal of Machine Learning Research , 18:3–\n18, 2011.\n[8] EMI Group Limited. EMI Million Interview Dataset,\n2012. http://musicdatascience.com/\nemi-million-interview-dataset/ .\nAccessed 18 February 2017.\n[9] David Hauger, Markus Schedl, Andrej Ko ˇsir, and\nMarko Tkal ˇciˇc. The million musical tweets dataset:\nWhat can we learn from microblogs. In Proceedings\nof the 14th International Society for Music Information\nRetrieval Conference , Curitiba, Brazil, 2013.\n[10] Perfecto Herrera, Zuri ˜ne Resa, and Mohamed Sordo.\nRocking around the clock eight days a week: An explo-\nration of temporal patterns of music listening. In Pro-\nceedings of the 1st ACM RecSys Workshop on Music\nRecommendation and Discovery, , Barcelona, Spain,\n2010.\n[11] IFPI. IFPI global music report 2015. Annual report,\nInternational Federation of the Phonographic Industry,\n2015.\n[12] IFPI. IFPI global music report 2016. Annual report,\nInternational Federation of the Phonographic Industry,\n2016.[13] Brian McFee, Thierry Bertin-Mahieux, Daniel P. W.\nEllis, and Gert R. G. Lanckriet. The million song\ndataset challenge. In Proceedings of the 21st Interna-\ntional Conference on World Wide Web , pages 909–16,\nLyon, France, 2012.\n[14] Sergio Oramas, Vito Claudio Ostuni, Tomasso\nDi Noia, Xavier Serra, and Eugenio Di Sciascio. Sound\nand music recommendation with knowledge graphs.\nACM Transactions on Intelligent Systems and Technol-\nogy, 8(2):1–21, 2016.\n[15] Sergio Oramas, Vito Claudio Ostuni, and Gabriel\nVigliensoni. Sound and music recommendation with\nknowledge graphs (dataset), 2016. http://hdl.\nhandle.net/10230/27495 . Accessed 18 April\n2017.\n[16] Chan Ho Park and Minsuk Kahng. Temporal dynam-\nics in music listening behavior: A case study of online\nmusic service. In 9th IEEE International Conference\non Computer and Information Science , pages 573–8,\nKaminoyama, Japan, 2010.\n[17] Robert Prey. Musica analytica: The dataﬁcation of lis-\ntening , chapter 3, pages 31–48. Palgrave Macmillan\nUK, London, United Kingdom, 2016.\n[18] Paul Rutter. The Music Industry Handbook . Media\nPractice. Routledge, Oxfordshire, United Kingdom, 2\nedition, 2016.\n[19] Markus Schedl. Leveraging microblogs for spatiotem-\nporal music information retrieval , volume 7814 of Lec-\nture Notes in Computer Science , pages 796–9. Springer\nBerlin Heidelberg, Berlin, Heidelberg, 2013.\n[20] Markus Schedl. The LFM-1b dataset for music re-\ntrieval and recommendation. In Proceedings of the\n2016 ACM on International Conference on Multimedia\nRetrieval , pages 103–10. ACM, 2016.\n[21] Gabriel Vigliensoni and Ichiro Fujinaga. Identifying\ntime zones in a large dataset of music listening logs. In\nProceedings of the 1st International Workshop on So-\ncial Media Retrieval and Analysis , pages 27–32. ACM,\n2014.\n[22] Gabriel Vigliensoni and Ichiro Fujinaga. Automatic\nmusic recommendation systems: Do demographic,\nproﬁling, and contextual features improve their perfor-\nmance? In Proceedings of the 17th International Soci-\nety for Music Information Retrieval Conference , pages\n94–100, New York City, NY , 2016.\n[23] Patrik Wikstr ¨om. The Music Industry: Music in the\nCloud . Polity Press, Cambrige, UK, 2nd edition, 2013.\n[24] Eva Zangerle, Martin Pichl, Wolfgang Gassler, and\nG¨unther Specht. #nowplaying music dataset: Extract-\ning listening behavior from twitter. In Proceedings of\nthe 1st International Workshop on Internet-Scale Mul-\ntimedia Management , pages 21–6, Orlando, FL, 2014.102 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "A Statistical Analysis of Gamakas in Carnatic Music.",
        "author": [
            "Venkata Subramanian Viraraghavan",
            "Rangarajan Aravind",
            "Hema A. Murthy"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1417837",
        "url": "https://doi.org/10.5281/zenodo.1417837",
        "ee": "https://zenodo.org/records/1417837/files/ViraraghavanAM17.pdf",
        "abstract": "Carnatic Music, a form of classical music prevalent in South India, has a central concept called r¯agas, defined as melodic scales and/or a set of characteristic melodic phrases. These definitions also account for the continuous pitch movement in gamakas and micro-tonal adjustments to pitch values. In this paper, we present several statistics of gamakas to arrive at a model of Carnatic music. We draw upon the two-component model of Carnatic Music, which splits it into a slowly varying ‘stage’ and a detail, called ‘dance’. Based on the statistics, we propose slightly altered def- initions of two similar components called constant-pitch notes and transients. An automated implementation of these definitions is used in collecting statistics from 84 concert renditions. We then suggest that the constant-pitch notes and tran- sients can be considered as context and detail respectively of the r¯aga, but add that both are necessary for defining the r¯aga. This is verified by performing listening tests on only the constant-pitch notes and transients independently.",
        "zenodo_id": 1417837,
        "dblp_key": "conf/ismir/ViraraghavanAM17",
        "keywords": [
            "Carnatic Music",
            "Gamakas",
            "Ragas",
            "Pitch Movement",
            "Micro-tonal Adjustments",
            "Constant-Pitch Notes",
            "Transients",
            "Melodic Scales",
            "Indian Classical Music",
            "Automated Implementation"
        ],
        "content": "A STATISTICAL ANALYSIS OF GAMAKA S IN CARNATIC MUSIC\nVenkata Subramanian Viraraghavan1;2R Aravind2Hema A Murthy3\n1TCS Research and Innovation, Embedded Systems and Robotics, Bangalore, India\n2Department of Electrical Engineering, Indian Institute of Technology, Madras\n3Department of Computer Science and Engineering, Indian Institute of Technology, Madras\nvenkatasubramanian.v@tcs.com, aravind@ee.iitm.ac.in, hema@cse.iitm.ac.in\nABSTRACT\nCarnatic Music, a form of classical music prevalent in\nSouth India, has a central concept called r¯agas, deﬁned\nas melodic scales and/or a set of characteristic melodic\nphrases. These deﬁnitions also account for the continuous\npitch movement in gamaka s and micro-tonal adjustments\nto pitch values.\nIn this paper, we present several statistics of gamaka s\nto arrive at a model of Carnatic music. We draw upon the\ntwo-component model of Carnatic Music, which splits it\ninto a slowly varying ‘stage’ and a detail, called ‘dance’.\nBased on the statistics, we propose slightly altered def-\ninitions of two similar components called constant-pitch\nnotes and transients. An automated implementation of\nthese deﬁnitions is used in collecting statistics from 84\nconcert renditions.\nWe then suggest that the constant-pitch notes and tran-\nsients can be considered as context and detail respectively\nof the r¯aga, but add that both are necessary for deﬁning the\nr¯aga. This is veriﬁed by performing listening tests on only\nthe constant-pitch notes and transients independently.\n1. INTRODUCTION\nCarnatic music, a classical form of music prevalent in\nSouth India, has evolved into a sophisticated art and a\nprofessional ﬁeld. It uses svara s that roughly corre-\nspond to the twelve notes of the Western-music scale.\nSvara s are deﬁned with respect to a base pitch called the\n¯adh¯ara s .ad.ja, or the tonic. A subset of the 12 notes are\nplayed in speciﬁc orders and grouped together as phrases.\nThese scales and phrases are used in deﬁning r¯agas.\nIndian musicians use the regions ‘between the notes’\nas a means of introducing sophistication over monophonic\nmusic. This can be seen in Figure 1 (reproduced with per-\nmission from [6]). The scales are thus notes connected by\ngamaka s that traverse frequencies between them; they can\nglide over even intermediate notes to reach notes further\naway.\nc\rVenkata Subramanian Viraraghavan, R Aravind, Hema\nA Murthy. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Venkata Subramanian\nViraraghavan, R Aravind, Hema A Murthy. “A Statistical Analysis of\nGAMAKA s in Carnatic Music”, 18th International Society for Music In-\nformation Retrieval Conference, Suzhou, China, 2017.\n0 200 400 600 800 1000 120000.10.20.30.40.50.60.70.80.91\nFrequency (Cents)Normalized Density\n  \nWestern Classical − C Major Scale\nHindustani Classical − Raga Bilawal\nCarnatic Classical − Raga Shankarabharanam\nS\nR2\nG3M1\nD2\nN3PFigure 1 : Histogram of notes of Carnatic, Hindustani and\nWestern music renditions. Notice notes in Indian music\nspan the regions between the notes of Western music.\nThe term svara , initially named the seven major notes\nin an octave – Sa, Ri (Carnatic)/Re (Hindustani), Ga, Ma,\nPa, Da/Dha and Ni respectively corresponding to Do, Re,\nMi, Fa, So, La and Ti in Western music. However, it has\nnow expanded in meaning to identify even the transients\nbetween a sequence of notes.\nThe concepts of gamaka andr¯aga are central to Indian\nclassical music. There is a perceived identity of r¯agas that\nneeds to be preserved. The characteristics of a r¯aga are de-\ntermined in terms of its melodic phrases. Our observation\nis that a melodic phrase can be thought of as being made\nup of a sequence of constant-pitch notes (or CP-notes), and\ntransients. Speciﬁc combinations of CP-notes and trajecto-\nries of the transients together give a r¯aga its unique iden-\ntity. Further, we show that the constant-pitch notes serve\nas a context to interpret the transients in gamaka s. While\nthe focus is primarily on Carnatic music, the methodol-\nogy described could perhaps be extended to other genres\nthat include signiﬁcant continuous pitch movement, such\nas Hindustani music and possibly Jazz music.\nThe rest of the paper is organized as follows. Section 2\nlists previous work and in Section 3, an extensive analysis\nof CP-notes and transients from concert recordings moti-\nvates our proposed model. Relevant previous work is de-\nscribed in terms of our model in Section 4.1. The results\nof a listening experiment are presented in Section 4.2 to243verify the model and Section 5 concludes the paper.\n2. PREVIOUS WORK\nOne of the earliest attempts of analyzing gamaka s in Car-\nnatic music using signal processing techniques was [16].\nAfter an initial analysis of a few r¯agas, the author proposed\nthe use of ‘melodic atoms’ to describe music. Again, as in\nthe case of gamaka s, these are intuitive to the Carnatic mu-\nsician, but there was no automated way of extracting them.\nAt a similar time, gamaka s were analyzed for music infor-\nmation retrieval in [22]. This used the concept of ‘extrema’\nof the pitch contour, which were called ‘stationary points’\nin [8] for discovering r¯aga motifs.\nGiven the importance of gamaka s as borne out by\nFigure 1, it is not surprising that they were the subject of\nanalysis in the CompMusic project [2]. An initial attempt\nwas based on several types of gamaka s identiﬁed by mu-\nsicologists [5]. A Carnatic musician, especially an instru-\nmentalist, can intuitively relate to the descriptions of the\nmovements, but looking for these pitch patterns in audio\nsamples was not fruitful. Instead, r¯aga motifs (e.g. [8,14])\nwere identiﬁed. R¯aga motifs can be seen as signature\npitch contour movements of a r¯aga and occur only in that\nr¯aga and no other. This exclusivity is an important aspect\nbecause, in Carnatic music, r¯agas do share many common\nphrases and the unique phrases are expected to be small in\nnumber. A more recent, completely automated method of\nr¯aga identiﬁcation based on motifs is presented in [13].\nIn [21], we ﬁnd manual analysis of Carnatic music\ngamaka s, mainly for use in synthesis. However, it is based\non one rendition of one varn.am. At a high level, it views\nCarnatic music as two components: the stage, which has a\nslowly varying pitch-contour, and the dance – a detail on\ntop of the stage. We show in this paper that the variations in\ndetail are quite short (typically under 400ms) and propose\ndifferent deﬁnitions of the two components so that auto-\nmatic analysis is possible. In fact, [10] extends the stable\npart of the pitch contour to segments of varying pitch for\nHindustani music retrieval; however, it is incomplete be-\ncause signiﬁcant information about (especially Carnatic)\nr¯agas is in the segments of audio where the pitch varies (as\nwe will show in Figure 5 later). A more recent result [11]\nuses quantized forms of these segments for Hindustani mu-\nsic, but does not characterize the variations.\nIn Section 4.2, the two-component theory, motifs and\nthe context-detail aspects of Carnatic music will be revis-\nited in the light of the model we propose.\n3. ANALYSIS OF GAMAKA S IN CONCERT\nRECORDINGS\nWe analyzed 84Carnatic music concert pieces in seven\nr¯agas. The r¯aga-wise split of these pieces is given in Table\n1. They are from the CompMusic database [1]. We chose\nseven r¯agas from those considered as the major in Car-\nnatic music [4] – t¯od.¯i,bhairav ¯i,kharaharapriy ¯a,k¯ambh ¯oji,\n´sankar ¯abharan .am,var¯al.¯i, and kaly¯an.¯i.R¯aga Number of pieces\nMale Female\nT¯od.¯i 7 5\nBhairav ¯i 6 6\nKharaharapriy ¯a 7 5\nK¯ambh ¯oji 8 4\n´Sankar ¯abharan .am 6 6\nVar¯al.¯i 7 5\nKaly ¯an.¯i 9 3\nTable 1 :R¯aga-wise split of the 84vocal-concert rendi-\ntions.\nFigure 2 : Examples of CP-notes (solid lines) and four\ntypes of stationary points (unﬁlled circles). The two notes\nmarked exemplify EOb3.\n3.1 Some Deﬁnitions\n3.1.1 Constant-pitch Notes\nConstant-pitch notes serve as a starting point for various\nmusical forms including Carnatic music and we provide a\nworking deﬁnition below, where the preﬁx EOb stands for\n‘Empirical Observation’:\nEOb1 A constant-pitch note is one whose pitch does not\nvary from its mean pitch by more than \u0001semitones\nand lasts for at least Cminseconds.\nwhere, nominally, \u0001 = 0 :3andCmin= 80 milliseconds\n(ms). The value of 80ms is intuitive: it is the length of the\nshortest note played without gamaka s, in Carnatic music\npieces that are extremely fast ( gamaka s are not perceived\nat these speeds). This is possible on instruments for r¯agas\nsuch as m¯ohanam (e.g. [3]) and hamsadhwani . Such ren-\nditions from a personal collection were analyzed. The du-\nration of a CP-note at the fastest speed was found to be\naround 87ms in one rendition and 80ms in another. Only\n\u0001 = 0 :3was consistent with this observation. For exam-\nple, with \u0001 = 0 :1, the shortest CP-notes lasted more than\n200ms. Note that with other choices for \u0001andCmin,\nwhile the details of the statistics obtained may vary, the\nmeaningful split of Carnatic music into CP-notes and tran-\nsients is expected to hold.\n3.1.2 Stationary Points\nStationary points were deﬁned and used extensively in\n[7, 9, 22]. These are pitch positions where a continuous\npitch curve changes direction. An example of these two\nfeatures is shown in Figure 2. In the ﬁgure, a CP-note is244 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017easily recognized as a solid line drawn for its duration at\nits mean pitch value. The stationary points are marked by\nunﬁlled circles. (Note that statements 7 to 14 in Algorithm\n2 remove redundant stationary points.) The same ﬁgure\nalso shows a region of silence at the end of the time-axis.\nDuring silence, there is no prominent melodic pitch (i.e.\nneither of the voice nor of the accompanying violin), but a\ndrone may be present. Four out of the six possible types of\npitch curves – called transients – are marked in the ﬁgure.\nThese types are deﬁned by the neighbours of a stationary\npoint and are in fact, the six combinations possible from\nthe setfCPNote, STAtionary point, SILence g(the letters\nin upper-case are the short-forms used later) :\n1. CP-note on one side of the transient and, on the\nother, one offAnother CP-note; A curve, which has\na stationary point; Silence g.\n2. A curve with a stationary point on one side of the\ntransient, and on the other, one of fA curve with its\nown stationary point, Silence g.\nOur analysis counts the number of occurrences of each\nof the above and is presented next1.\n3.2 Method\nFor each of the renditions in Table 1, the prominent pitch\nand tonic extracted for the work reported in [12] were re-\nused. They had been found using the algorithms described\nin [18], with a window length of 46ms and a window shift\nof2:9ms [19]. Sometimes, the tracked pitch could be that\nof accompanying instruments and is prone to some octave-\nerrors. However, unlike in transcription (e.g. [17]), these\nerrors are inconsequential in identifying the type and du-\nration of each transient (Section 3.1.2) or CP-note. The\nonly impact was at most the splitting of a CP-note into\nmore than one. Manual inspection of several pieces and\ntheir pitch tracks suggested that this occurred infrequently\nenough to be neglected.\nAlgorithm 1 was used to identify CP-notes and Algo-\nrithm 2, stationary points2. Each stationary point is also\nmarked with one of the types described in Section 3.1.2.\nStatements 4 to 9 of Algorithm 1 look for the longest\npossible CP-note from the current pitch value (at index i).\nHowever, in statement 6 , a direct application of the deﬁni-\ntion of CP-notes (Section 3.1.1) resulted in too many false\nalarms. For example, if the pitch rose by 0:6semitones\nwithin 80ms, but was part of a continuous pitch move-\nment, it would still get detected as a CP-note. To elimi-\nnate these cases, a threshold of one semitone per second\nwas used on the slope of the best-ﬁt line . This is one-tenth\nthe nominal slope of continuous pitch variation reported\nin [20].\nThe utility of deﬁning CP-notes also emerges from Fig-\nures 3 and 4. Both r¯agas show that the distribution of CP-\nnotes is much sharper than all notes put together. (Ignore\nthe stationary- and remaining-points for now.) For exam-\n1Silence on both sides of the transient was seen in under 0:005% of\nthe cases and is excluded.\n2A more sophisticated algorithm can be found in [7], but the infer-\nences do not depend on precise locations of the stationary points.Algorithm 1 Algorithm to ﬁnd CP-notes from the pitch\ncurve.\n1:Track the pitch of each piece according to [18], which\nalso identiﬁes silence regions. These result in pitch\nvalues ,f[i]; i2f0; : : : ; L\u00001g.\n2:In the regions of music (i.e. not silence), with the tonic\nof the piece as f0, ﬁnd the pitch in semitones with re-\nspect to the tonic as n[i] = 12 log2(f[i]\nf0)\n3:i Index of the ﬁrst pitch sample in the ﬁrst non-\nsilence region\n4:while i < L do\n5: ﬂagCpNoteFound FALSE\n6: while n[j]; j2fi+Cmin; : : : kg; j < L is a CP-\nnote andthe slope is below the threshold do\n7: k k+ 1\n8: ﬂagCpNoteFound TRUE\n9: end while\n10: ifﬂagCpNoteFound = TRUE then\n11: Mark the region from itok\u00001as a CP-note\n12: end if\n13: i k\n14: ifiis in a silence region then\n15: i the index of the ﬁrst pitch sample in the next\nnon-silence region.\n16: end if\n17:end while\nple, in Figure 3, notice when the notes are all plotted to-\ngether (dashed line), they show a signiﬁcant value (deﬁned\nas occurring more often than 2%of the maximum occur-\nrence) over G2 (3 semitones from Sa). However, the CP-\nnotes show no such peak. This means there is no CP-note\npeak masked by the ﬂat sections of the histogram. Fur-\nther, there are no peaks found at ‘incorrect’ locations, say\nat9:5semitones. In fact, this behavior is sufﬁciently con-\nsistent to attach the name of the nearest semitone to the\nsvara . That is, in a system of music with continuous pitch\nvariation, whether the CP-notes occur as part of svara s\nwith gamaka sor without , the CP-notes cluster around the\nr¯aga’s scale notes. This result is intuitive, but it is not ob-\nvious from, say, Figure 1.\nIn Figure 3, the automatically identiﬁed svara s are\na subset of the written ¯arohan .aand avarohan .aof\n´sankar ¯abharan .am. An interesting exception occurs in the\nr¯aga t ¯od.¯i(Figure 4) where the peak at R2 (2 semitones\nfrom Sa), is not part of its ¯arohan .aandavarohan .a.\n3.3 Key Observations\nOne more step in the method remains to be described, but\nit needs to be motivated by a very signiﬁcant observation\nby looking ahead at the results. Of all the \u0019975;000\ntransients that were found using Algorithm 2, only 1:25%\nwere of the type SIL-STA-STA-STA-SIL, i.e. the central\nstationary point was ﬂanked on both sides by two station-\nary points that ended in silence segments. Further, when\none of the renditions was examined closely by ear (by a\nsemi-professional musician), allsuch instances were falseProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 245Figure 3 : Histograms of all notes, CP-notes, stationary\npoints and the remaining points in transients. Svara names\nare marked for signiﬁcant peaks of the CP-notes’ distribu-\ntion. These peaks are much sharper than others. This is for\nther¯aga ´sankar ¯abharan .am, whose nominal scale is S, R2,\nG3, M1, P, D2, N3, but N3 is not a signiﬁcant peak.\nFigure 4 : As in Figure 3, but this is for the r¯aga t ¯od.¯i. The\nnominal scale is S, R1, G2, M1, P, D1, N2, but G2 and N2\nare not signiﬁcant peaks, while R2 (not in the scale) is one.\nFigure 5 : Ratios of maximum and mean durations of\nCCTs to that of CP-notesalarms due to errors in marking silence-regions3. Thus,\nwe may posit that:\nEOb2 Transients do not occur in isolation . They occur\nalong with a CP-note or ‘chained’ to other tran-\nsients.\nFurther, from the intuitive experience of producing in-\ndividual svara sespecially at very slow speeds , we impose\nthat individual svara s, when sung in isolation, need to have\nat least one CP-note segment. If this were not true, we\nshould have had more instances of isolated transients. We\nuse the term ‘anchor note(s)’ to denote the CP-note(s) as-\nsociated with transient(s) in svara s.\nAlgorithm 2 Algorithm to ﬁnd stationary points.\n1:forEach segment of music (i.e. not silence) between\nCP-notes and silence (see Algorithm 1) do\n2: Smooth the pitch contour with a moving-average ﬁl-\nter of length L=Cmin\n4.\n3: Find the peaks and troughs of the pitch curve in the\nsegment.\n4: forEach peak (trough) do\n5: Retain only the maximum peak (minimum\ntrough) in a window of length L.\n6: end for\n7: forEach remaining peak OR trough, s, with pitch\nvalue nsdo\n8: ifA nearest neigbhour of sis a CP-note with\nmean pitch value ncandjnc\u0000nsj<\u0001then\n9: Discard stationary point s.\n10: else if The preceding neighbour is a stationary\npoint s0, with pitch value ns0andjns\u0000ns0j<\u0001\nthen\n11: Discard stationary point s0.\n12: end if\n13: end for\n14: Discard stationary points with pitch values between\nthose of adjacent stationary points/CP-notes.\n15:end for\nIf transients do not occur in isolation, it is instructive to\nsee how long a ‘contiguous chain of transients (CCTs)’ can\nlast without any CP-note appearing. A CCT is deﬁned as\na segment of music from the end of a silence segment/CP-\nnote to the start of another. An example corresponds to the\npitch curve in Figure 2 from t\u00190:65sec till t\u00191:3sec.\nThe histograms of the ratios of the maximum and mean\nlengths of CCTs and CP-notes in the 84pieces are shown\nin Figure 5. It reveals that the maximum length of a con-\ntiguous chain of transients is comparable to the maximum\nduration of a CP-note in that piece. Although there are\ncases where this ratio is >2, in around 80% of the cases,\nit is<1:5. Considering the mean durations: Over 50% of\nthe CCTs do not last longer than 1:5times the mean CP-\nnote duration . Note that in this ‘consistency check’, the\n3A similar percentage of CP-notes or transients ﬂanked by SIL may\nthus be erroneous, but this does not affect the inferences made later.246 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017deﬁnitions of the types of transients (Section 3.1.2) do not\nmatter, so it is an independent corroboration of the utility\nof EOb2.\nHowever, it does suggest the need to allow phrases to\nbe concatenated by merging svara s. This type of chaining\nof phrases is familiar to musicians [15]. We further specify\nin detail:\nEOb3 When two svara s are sung in sequence, their anchor\nnotes of the same pitch can be merged.\nThus, while individual svara s need at least one anchor\nnote each, Sconcatenated svara s can have N(\u0014S)CP-\nnote segments in them. For example, the two notes – Pa\nand Ma – marked in Figure 2 share a common anchor note.\n3.4 Statistical Analysis\nBased on the key observations in Section 3.3, speciﬁc\nstatistics relating to transients were collected. First, it is\npossible for merged anchor notes to reduce in duration and\nbecome stationary points in the limit. Thus, the starting\npoint of a transient is counted as the nearest among fthe\nend of a previous CP-note segment; another, earlier station-\nary point; or the end of a preceding silence segment g. Sim-\nilarly, the end of the transient is nearest among the starting\npoints offa following CP-note segment; another, later sta-\ntionary point; or the beginning of a succeeding silence seg-\nmentg. These are marked in Figure 2. The distributions of\nthe transient- and CP-note-durations across all r¯agas are\nshown in Figure 6. The following observations are made.\n1. The maximum duration of the transients is under 1\nsecond, while the maximum duration of CP-notes is\nover10seconds. This result depends upon the def-\ninitions of starting points and ending points given\nabove and Figure 5 vindicates the deﬁnitions.\n2. The long transients are quite rare; more than 90% of\nthem are shorter than 400ms.\n3. There is a variation in the ranges of the transient-\ndurations across r¯agas, but the mean values of the\ntransient durations are remarkably similar ( \u0019100\nms) and is quite close to the 80ms value set for the\nparameter Cmin. See Figures 7 and 8.\n4. About 40% of the transients have a very small du-\nration and need to be investigated further, but a\nmajor contributing factor are the ‘attacks’ of notes\nand where syllables are pronounced. During at-\ntacks, the spectrum changes and the pitch curve\nstops conforming to the deﬁnition of a CP-note (Sec-\ntion 3.1.1). Thus, not all transients are perceived as\ngamakas. Some may be perceived as svara -attacks.\n5. The distribution of different types of stationary\npoints deﬁned in Section 3.1.2 is given in Table 2.\nThis reafﬁrms the observations in Figures 3 and\n4, where stationary points and other non-CP non-\nstationary points densities show wider peaks. The\ndensity for stationary points has been deliberately\nraised so that it is visible in the ﬁgures.4. MODEL VERIFICATION\n4.1 Relation to the Two-Component Model\nThe narrow peaks of the histograms of CP-notes (Figures 3\nand 4) and the durations of transients being comparable to\nthat of CP-notes (Figure 5) suggest that the CP-notes and\ntransients identiﬁed by our method are meaningful. Thus,\nthey can be compared with the model presented in [21],\nwhere the authors view Carnatic music as consisting of two\ncomponents called the ‘stage’ and ‘dance’. Our model was\nderived independently and it is quite signiﬁcant that many\naspects are similar, but there are important practical differ-\nences. First, ‘stage’ in [21] approximately maps to anchor\nnotes, but we restrict anchor notes to be governed by EOb1.\nThis enables automatic segmentation of the audio into CP-\nnotes, transients and silence. Second, the stable and sus-\ntained focal pitches of the ‘dance’ may get classiﬁed as a\nCP-note in our model if they satisfy EOb1. The transient\nfocal pitches would get classiﬁed as our transients. In gen-\neral, focal pitches that do not satisfy EOb1 will be counted\nas transients. However, similar to [21], it is possible to\nview the CP-notes (approximately stage) as context of the\nmusic and the transients (at least the dance component) as\nits detail. Similarly, the oscillatory continuity condition\nof [20] can be seen as a special case of EOb3.\nNote that while [21] was based on a largely manual\nanalysis of one rendition of one varn.amin one r¯aga (in\naid of synthesis), we have found approximate stage-like\nand dance-like components of 84pieces in seven major\nr¯agas using an automated method (Sections 3.2 and 3.3).\n4.2 Listening Tests\nWe report the results of experiments designed to evaluate\nthe dependence of transients and CP-notes4. Approxi-\nmately 30-second snippets of violin ¯al¯apana s (non- t¯al.a-\nbound extempore improvisations in a r¯aga) were chosen\nfor the test. Songs were excluded because listeners could\nhave recognized the song’s tune rather than the r¯aga. Each\nsnippet was then split into a CP-notes-only and transients-\nonly parts according to Algorithms 1 and 2. R¯agast¯od.¯i,\nbhairav ¯i,´sankar ¯abharan .am,var¯al.¯i, and kaly¯an.¯iwere cho-\nsen and in each, two snippets were picked manually. The\nlistener was asked to identify the r¯agas of these 30pieces,\nwhile a superset of these r¯agas was given as possible\nchoices. Listeners could also choose ‘Cannot make out’\nif they actually could not or if they felt the r¯aga played\nwas not in the list. The order of pieces was randomized\nseparately for the transients-only (this set of 10was played\nﬁrst), CP-notes only (played next) and the unedited snip-\npets (played last). Fifty participants took the test.\nThe overall results point to the CP-notes’ and transients’\ndependencies on each other: 81:4%of the clips were iden-\ntiﬁed correctly, but only 58:4%of the transients-only snip-\npets, and 68:6%of the CP-notes only. R¯aga-wise results\nare presented in Table 3, which are restricted to the ex-\npert participants – those who could identify the r¯agas of\n4http://www.iitm.ac.in/donlab/wermusic/index.\nhtml?owner=venkat&testid=test1&wer=30Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 247Type CPN\u000eCPN CPN\u000eSTA CPN\u000eSIL STA\u000eSTA STA\u000eSIL\nPercentage 14:5 17:5 5:3 49:0 13:7\nTable 2 : Relative occurrence of stationary-point types.\nR¯aga Percentage of participants\nTransients only CP-notes only\nT¯od.¯i 95.0 81.3\nBhairav ¯i 43.2 81.1\n´Sankar ¯abharan .am 21.6 85.1\nVar¯al.¯i 86.5 89.2\nKaly ¯an.¯i 89.7 68.0\nTable 3 : Accuracy of identiﬁcation by experts\nall unedited snippets of that r¯aga. Only one of them could\nidentify all instances of snippets with transients only and\nCP-notes only. Among the r¯agas,t¯od.¯iandvar¯al.¯ifared\nbest. Bhairav ¯iwas surprisingly not easily identiﬁable with\nonly transients, even though it is considered a gamaka -\nheavy r¯aga.Kaly ¯an.¯iwas easier to recognize by transients\nthan only CP-notes, but ´sankar ¯abharan .amcould be identi-\nﬁed from transients-only by only about 20% of the experts.\nOverall, the results suggest that CP-notes and transients\nneed each other in Carnatic music.\nSome qualitative results are also signiﬁcant. A few par-\nticipants expressed surprise when presented with only CP-\nnotes or transients. Several said that the edited pieces were\nquite unpleasant to the ear (edited vocal pieces sounded\nworse). The edited pieces were noticeably fragmented,\nbut the artefacts were identical in both types of clips. Yet.\ninterestingly, the transients-only clips – a crucial compo-\nnent of gamaka s – were perceived as more unpleasant.\nThis clearly suggests the importance of context for tran-\nsients in Carnatic music, which is provided, at least par-\ntially, by the CP-notes.\n5. CONCLUSION\nThe qualitative stage and dance model model for Carnatic\nmusic is corroborated with an equivalent CP-note, tran-\nsient model. An analysis of CP-notes and transients shows\nthat the CP-notes can last much longer than transients in\nduration. We proposed that svara s can be viewed as CP-\nnotes providing the context for any transients. When com-\nbined with the chaining rule, these observations can ex-\nplain r¯aga motifs.\nIn a listening test, while 28experts correctly identiﬁed\nall original, unedited audio clips in ﬁve r¯agas, only one\ncorrectly identiﬁed all CP-notes-only and transients-only\nclips. Several listeners reported that the latter pieces were\nquite unpleasant on the ear.\nThus, the CP-notes and transients model is potentially a\ngood model of Carnatic music, with our analyses suggest-\ning that there is signiﬁcant contextual r¯aga information in\nCP-notes; they are, in fact, crucial for a pleasant listening\nexperience of the transients in the profusion of gamaka s in\nCarnatic music.\nFigure 6 : Distribution of the durations of transients and\nCP-notes across r¯agas described in Table 1. Each hor-\nizontal line shows the r¯aga-speciﬁc range of transient-\ndurations, and the circles, their mean durations. Numbers\nin brackets are the durations of the longest CP-note .\nFigure 7 : Distribution as in Figure 6 restricted to the\nr¯aga ´sankar ¯abharan .am. The range and means of\ntransient-durations correspond to each piece in the r¯aga.\nFigure 8 : Distribution as in Figure 7 restricted to the\ngamaka -laden r¯aga t ¯od.¯i. Its transient-duration means are\nsimilar to those of ´sankar ¯abharan .am.248 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20176. ACKNOWLEDGEMENTS\nThis research was partly funded by the European Research\nCouncil under the European Unions Seventh Framework\nProgram, as part of the CompMusic project (ERC grant\nagreement 267583). V Viraraghavan thanks Ms. Anju\nLeela Thomas for her help in setting up the listening test,\nall participants of the test, and Dr. Sankalp Gulati for pro-\nviding, and helping with, the CompMusic database.\n7. REFERENCES\n[1] Carnatic Corpus in Dunya.\nhttp://dunya.compmusic.upf.edu/carnatic/?a=10&r=55.\nAccessed: 2017-04-16.\n[2] CompMusic Project. http://compmusic.upf.edu/. Ac-\ncessed: 2017-04-16.\n[3] L. Subramaniam - Ninnu kori (varnam) - 15 speeds.\nhttps://www.youtube.com/watch?v=w4oOB8EOe5g.\nAccessed: 2017-04-16.\n[4] Ashwin Bellur, Vignesh Ishwar, and Hema A Murthy.\nMotivic analysis and its relevance to raga identiﬁcation\nin carnatic music. In Serra X, Rao P , Murthy H, Bozkurt\nB, editors. Proceedings of the 2nd CompMusic Work-\nshop; 2012 Jul 12-13; Istanbul, Turkey. Barcelona:\nUniversitat Pompeu Fabra; 2012. p. 153-157 . Univer-\nsitat Pompeu Fabra, 2012.\n[5] Subbarama Dikshita.\nSangita Sampradaya Pradarshini . Web version,\nhttp://www.ibiblio.org/guruguha/ssp.htm, 2008.\n[6] Shrey Dutta. Analysis of motifs in carnatic music:\nA computational perspective. Master’s thesis, IIT,\nMadras, Chennai, 2016.\n[7] Shrey Dutta, SPV Krishnaraj, and Hema A Murthy.\nRaga veriﬁcation in carnatic music using longest com-\nmon segment set. In Int. Soc. for Music Information\nRetrieval Conf.(ISMIR) , pages 605–611, 2015.\n[8] Shrey Dutta and Hema A Murthy. Discovering typical\nmotifs of a raga from one-liners of songs in carnatic\nmusic. In ISMIR , pages 397–402, 2014.\n[9] Shrey Dutta and Hema A Murthy. A modiﬁed rough\nlongest common subsequence algorithm for motif spot-\nting in an alapana of carnatic music. In Communica-\ntions (NCC), 2014 Twentieth National Conference on ,\npages 1–6. IEEE, 2014.\n[10] K. K. Ganguli, A. Rastogi, V . Pandit, P. Kantan, and\nP. Rao. Efﬁcient melodic query based audio search for\nhindustani vocal compositions. In Proc. of the Inter-\nnational Society for Music Information Retrieval (IS-\nMIR) , Oct. 2015, pp. 591597, Malaga, Spain.\n[11] Kaustuv Kanti Ganguli, Ashwin Lele, Saurabh Pinjani,\nPreeti Rao, Ajay Srinivasamurthy, and Sankalp Gulati.Melodic shape stylization for robust and efﬁcient mo-\ntif detection in hindustani vocal music. In Proc. of the\nNational Conference on Communication (NCC) , 2017.\n[12] Sankalp Gulati, Joan Serr `a, Kaustuv Kanti Gan-\nguli, Sertan Sent ¨urk, and Xavier Serra. Time-delayed\nmelody surfaces for r ¯aga recognition. In ISMIR , pages\n751–757, 2016.\n[13] Sankalp Gulati, Joan Serr `a, Vignesh Ishwar, and\nXavier Serra. Discovering Raga Motifs by Character-\nizing Communities in Networks of Melodic Patterns.\nIn41st IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP 2016) , pages\n286–290, Shanghai, China, 2016. IEEE.\n[14] Vignesh Ishwar, Shrey Dutta, Ashwin Bellur, and\nHema A Murthy. Motif spotting in an alapana in car-\nnatic music. In ISMIR , pages 499–504, 2013.\n[15] T. M. Krishna. A Southern Music: The Karnatik Story .\nHarper Collins India, 1st edition, 2016.\n[16] Arvindh Krishnaswamy. Melodic atoms for transcrib-\ning carnatic music. In International Symposium on Mu-\nsic Information Retrieval , number 228. ISMIR, 2004.\n[17] P V Krishna Raj, Venkata Subramanian Viraraghavan,\nSridharan Sankaran, and Hema A Murthy. An approach\nto transcription of varnams in carnatic music using hid-\nden markov models. In Proc. of the National Confer-\nence on Communication (NCC) , 2017.\n[18] Justin Salamon and Emilia G ´omez. Melody Extraction\nfrom Polyphonic Music Signals using Pitch Contour\nCharacteristics. IEEE Transactions on Audio, Speech\nand Language Processing , 20:1759–1770, 2012.\n[19] Justin Salamon, Sankalp Gulati, and Xavier Serra. A\nmultipitch approach to tonic identiﬁcation in indian\nclassical music. In Gouyon F , Herrera P , Martins LG,\nM¨uller M. ISMIR 2012: Proceedings of the 13th Inter-\nnational Society for Music Information Retrieval Con-\nference; 2012 Oct 8-12; Porto, Portugal. Porto: FEUP\nEdic ¸oes; 2012. International Society for Music Infor-\nmation Retrieval (ISMIR), 2012.\n[20] Srikumar Subramanian, Lonce Wyse, and Kevin\nMcGee. Modeling speed doubling in carnatic music.\nInICMC , 2011.\n[21] Srikumar K Subramanian, Lonce Wyse, and Kevin\nMcGee. A two-component representation for modeling\ngamakas of carnatic music. In Proc. of 2nd CompMusic\nWorkshop , 2012.\n[22] Venkatasubramanian V , K R Ramakrishnan, and H V\nSahasrabuddhe. Music information retrieval using con-\ntinuity. In Proceedings of the Symposium on the Fron-\ntiers of Research on Speech and Music (FRSM 2004) ,\nnumber 228, pages 74–83. Annamalai University, Chi-\ndambaram, 2004.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 249"
    },
    {
        "title": "Drum Transcription via Joint Beat and Drum Modeling Using Convolutional Recurrent Neural Networks.",
        "author": [
            "Richard Vogl",
            "Matthias Dorfer",
            "Gerhard Widmer",
            "Peter Knees"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415136",
        "url": "https://doi.org/10.5281/zenodo.1415136",
        "ee": "https://zenodo.org/records/1415136/files/VoglDWK17.pdf",
        "abstract": "Existing systems for automatic transcription of drum tracks from polyphonic music focus on detecting drum in- strument onsets but lack consideration of additional meta information like bar boundaries, tempo, and meter. We ad- dress this limitation by proposing a system which has the capability to detect drum instrument onsets along with the corresponding beats and downbeats. In this design, the sys- tem has the means to utilize information on the rhythmical structure of a song which is closely related to the desired drum transcript. To this end, we introduce and compare different architectures for this task, i.e., recurrent, convo- lutional, and recurrent-convolutional neural networks. We evaluate our systems on two well-known data sets and an additional new data set containing both drum and beat annotations. We show that convolutional and recurrent- convolutional neural networks perform better than state-of- the-art methods and that learning beats jointly with drums can be beneficial for the task of drum detection.",
        "zenodo_id": 1415136,
        "dblp_key": "conf/ismir/VoglDWK17",
        "keywords": [
            "drum transcription",
            "polyphonic music",
            "meta information",
            "bar boundaries",
            "tempo",
            "meter",
            "recurrent neural networks",
            "convolutional neural networks",
            "beat annotations",
            "drum detection"
        ],
        "content": "DRUM TRANSCRIPTION VIA JOINT BEAT AND DRUM MODELING\nUSING CONVOLUTIONAL RECURRENT NEURAL NETWORKS\nRichard Vogl1;2Matthias Dorfer2Gerhard Widmer2Peter Knees1\n1Institute of Software Technology & Interactive Systems, Vienna University of Technology, Austria\n2Dept. of Computational Perception, Johannes Kepler University Linz, Austria\nfrichard.vogl, peter.knees g@tuwien.ac.at\nABSTRACT\nExisting systems for automatic transcription of drum\ntracks from polyphonic music focus on detecting drum in-\nstrument onsets but lack consideration of additional meta\ninformation like bar boundaries, tempo, and meter. We ad-\ndress this limitation by proposing a system which has the\ncapability to detect drum instrument onsets along with the\ncorresponding beats and downbeats. In this design, the sys-\ntem has the means to utilize information on the rhythmical\nstructure of a song which is closely related to the desired\ndrum transcript. To this end, we introduce and compare\ndifferent architectures for this task, i.e., recurrent, convo-\nlutional, and recurrent-convolutional neural networks. We\nevaluate our systems on two well-known data sets and an\nadditional new data set containing both drum and beat\nannotations. We show that convolutional and recurrent-\nconvolutional neural networks perform better than state-of-\nthe-art methods and that learning beats jointly with drums\ncan be beneﬁcial for the task of drum detection.\n1. INTRODUCTION\nThe automatic creation of symbolic transcripts from music\nin audio ﬁles is an important high-level task in music infor-\nmation retrieval. Automatic music transcription systems\n(AMT) aim at solving this task and have been proposed in\nthe past (cf. [1]), but there is yet no general solution to this\nproblem. The transcription of the drum instruments from\nan audio ﬁle of a song is a sub-task of automatic music\ntranscription, called automatic drum transcription (ADT).\nUsually, such ADT systems focus solely on the detection\nof drum instrument note onsets. While this is the necessary\nﬁrst step, for a full transcript of the drum track more in-\nformation is required. Sheet music for drums—equally to\nsheet music for other instruments—contains additional in-\nformation required by a musician to perform a piece. This\ninformation comprises (but is not limited to): meter, over-\nall tempo, indicators for bar boundaries, indications for lo-\ncal changes in tempo, dynamics, and playing style of the\nc\rRichard V ogl, Matthias Dorfer, Gerhard Widmer, Peter\nKnees. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Richard V ogl, Matthias Dorfer, Ger-\nhard Widmer, Peter Knees. “Drum Transcription via Joint Beat and Drum\nModeling using Convolutional Recurrent Neural Networks”, 18th Inter-\nnational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.piece. To obtain some of this information, beat and down-\nbeat detection methods can be utilized. While beats pro-\nvide tempo information, downbeats add bar boundaries,\nand the combination of both provides indication for the\nmeter within the bars.\nIn this work, neural networks for joint beat and drum\ndetection are trained in a multi-task learning fashion.\nWhile it is possible to extract drums and beats separately\nusing existing work and combine the results afterwards,\nwe show that it is beneﬁcial to train for both tasks together,\nallowing a joint model to leverage commonalities of the\ntwo problems. Additionally, recurrent (RNN), convolu-\ntional (CNN) and convolutional-recurrent neural network\n(CRNN) models for drum transcription and joint beat and\ndrum detection are evaluated on two well-known, as well\nas a new data set.\nThe remainder of this work is structured as follows. In\nthe next section, we discuss related work. In sec. 3, we\ndescribe the implemented drum transcription pipeline used\nto evaluate the network architectures, followed by a sec-\ntion discussing the different network architectures (sec. 4).\nIn sec. 5, we explain the experimental setup to evaluate the\njoint learning approach. After that, a discussion of the re-\nsults follows in sec. 6 before we draw conclusions in sec. 7.\n2. RELATED WORK\nWhile in the past many different approaches for ADT have\nbeen proposed [11,13,15,16,22,24,25,34,38], recent work\nfocuses on end-to-end approaches calculating activation\nfunctions for each drum instrument. These methods uti-\nlize non-negative matrix factorization (NMF, e.g. adaptive-\nNMF in Dittmar et al. [7] and partially ﬁxed NMF in Wu\net al. [37]) as well as RNNs (RNNs with label time-shift\nin V ogl et al. [35, 36] and bidirectional RNNs in Southall\net al. [31]) to extract the activation functions from spec-\ntrograms of the audio signal. Such activation-function-\nbased end-to-end ADT systems circumvent certain issues\nassociated with other architectures. Methods which ﬁrst\nsegment the song (e.g. using onset detection) and subse-\nquently classify these segments [22, 23, 38] suffer from a\nloss of information after the segmentation step—i.e. when-\never the system fails to detect a segment, this information\nis lost. Such systems heavily depend on the accuracies of\nthe single components, and can never perform better than\nthe weakest component in the pipeline. Additionally, infor-\nmation of the input signal which is discarded after a pro-\ncessing step might still be of value for later steps.150Since RNNs, especially long short-term memory\n(LSTM) [17] and gated recurrent unit (GRU) [5] networks,\nare designed to model long term relationships, one might\nsuspect that systems based on RNNs [31,35,36] can lever-\nage the repetitive structure of the drum tracks and make\nuse of this information. Contrary to this intuition this is\nnot the case for RNN-based systems proposed so far. Both\nthe works of V ogl et al. [35, 36] and Southall et al. [31]\nuse snippets with length of only about one second to train\nthe RNNs. This prohibits learning long-term structures\nof drum rhythms which are typically in the magnitude of\ntwo or more seconds. In [35], it has been shown that\nRNNs with time-shift perform equally well as bidirectional\nRNNs, and that backward directional RNNs perform better\nthan forward directional RNNs. Combining these ﬁndings\nindicates that the learned models actually mostly consider\nlocal features. Therefore, RNNs trained in such a manner\nseem to learn only an acoustic, but not a structural model\nfor drum transcription.\nMany works on joint beat and downbeat tracking have\nbeen published in recent years [2, 9, 10, 19–21, 26]. A dis-\ncussion of all the different techniques would go beyond the\nscope of this work. One of the most successful methods by\nB¨ock et al. [2] is a joint beat and downbeat tracking sys-\ntem using bidirectional LSTM networks. This approach\nachieves top results in the 2016 MIREX task for beat de-\ntection and can be considered the current state of the art.1\nIn this work, a multi-task learning strategy is used to\naddress the discussed issues of current drum transcription\nsystems, cf. [4]. The use of a model jointly trained on\ndrum and beat annotations, combined with longer train-\ning snippets, allows the model to learn long-term relations\nof the drum patterns in combination with beats and down-\nbeats. Furthermore, learning multiple related tasks simul-\ntaneously at once can improve results for the single tasks.\nTo this end, different architectures of RNNs, CNNs, and\na combination of both, convolutional-recurrent neural net-\nworks (CRNNs) [8, 27, 39], are evaluated.\nThe rationale behind selecting these three methods for\ncomparison is as follows. RNNs have proven to be well-\nsuited for both drum and beat detection, as well as learning\nlong-term dependencies for music language models [30].\nCNNs are among the best performing methods for many\nimage processing and other machine learning tasks, and\nhave been used on spectrograms of music signals in the\npast. For instance, Schl ¨uter and B ¨ock [28] use CNNs to\nimprove onset detection results, while Gajhede et al. [12]\nuse CNNs to successfully classify samples of three drum\nsound classes on a non-public data set. CRNNs should re-\nsult in a model, in which the convolutional layers focus on\nacoustic modeling of the events, while the recurrent layers\nlearn temporal structures of the features.\n3. DRUM TRANSCRIPTION PIPELINE\nThe implemented method is an ADT system using a similar\npipeline as presented in [31] and [36]. Fig. 1 visualizes\n1http://www.music-ir.org/mirex/wiki/2016:\nMIREX2016_Results\nFigure 1 . System overview of the implemented drum tran-\nscription pipeline used to evaluate the different neural net-\nwork architectures.\nthe overall structure of the system. The next subsections\ndiscuss the single blocks of the system in more detail.\n3.1 Feature Extraction\nFirst, a logarithmic magnitude spectrogram is calculated\nusing a 2048-samples window size and a resulting frame\nrate of 100Hz from a 44.1kHz 16bit mono audio signal\ninput. Then, the frequency bins are transformed to a loga-\nrithmic scale using triangular ﬁlters (twelve per octave) in\na frequency range from 20 to 20,000 Hz. Finally, the posi-\ntive ﬁrst-order-differential over time of this spectrogram is\ncalculated and concatenated. This results in feature vectors\nwith a length of 168 values (2x84 frequency bins).\n3.2 Activation Function Calculation\nThe central block in ﬁg. 1 represents the activation func-\ntion calculation step. This task is performed using a neu-\nral network (NN) trained on appropriate training data (see\nsec. 4). As in most of the related work, we only consider\nthree drum instruments: bass- or kick drum, snare drum,\nand hi-hat.\nWhile the architectures of the single NNs are different,\nthey share certain commonalities: i.all NNs are trained\nusing the same input features; ii.the RNN architectures\nare implemented as bidirectional RNNs (BRNN) [29]; iii.\nthe output layers consist of three or ﬁve sigmoid units, rep-\nresenting three drum instruments under observation (drum\nonly) or three drum instruments plus beat and downbeat\n(drum and beats), respectively; and iv.the NNs are all\ntrained using the RMSprop optimization algorithm pro-\nposed by Tieleman et al. [33], using mini-batches of size\neight. For training, we follow a three-fold cross validation\nstrategy on all data sets. Two splits are used for training,\n15% of the training data is separated and used for valida-\ntion after each epoch, while testing/evaluation is done on\nthe third split. The NNs are trained using a ﬁxed learn-\ning rate with additional reﬁnement if no improvement on\nthe validation set is achieved for 10 epochs. During reﬁne-\nment the learning rate is reduced and training continues\nusing the parameters of the best performing model so far.\nMore details on the individual NN architectures are pro-\nvided in sec. 4.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 151Figure 2 . Comparison of mode of operation of RNNs, CNNs, and CRNNs on spectrograms of audio signals. RNNs process\nthe input in a sequential manner. Usually, during training, only sub-sequences of the input signal are used to reduce the\nmemory footprint of the networks. CNNs process the signal frame by frame without being aware of sequences. Because\nof this, a certain spectral context is added for each input frame. CRNNs, like RNNs, process the input sequentially, but\nadditionally, a spectral context is added to every frame on which convolution is performed by the convolutional layers.\n3.3 Preparation of Target Functions\nFor training the NNs, target functions of the desired out-\nput are required besides the input features. These target\nfunctions are generated by setting frames of a signal with\nthe same frame rate as the input features to 1whenever an\nannotation is present and to 0otherwise. A separate target\nfunction is created for each drum instrument as well as for\nbeats and downbeats.\n3.4 Peak Picking\nIn the last step of our pipeline (rightmost block of ﬁg. 1),\nthe drum instrument onsets (and beats if applicable) are\nidentiﬁed using a simple peak picking method introduced\nfor onset detection in [3]: A point nin the activation func-\ntionfa(n)is considered a peak if these terms are fulﬁlled:\n1.fa(n) =max (fa(n\u0000m);\u0001\u0001\u0001;fa(n));\n2.fa(n)\u0015mean (fa(n\u0000a);\u0001\u0001\u0001;fa(n)) +\u000e;\n3.n\u0000nlp>w;\nwhere\u000eis a variable threshold. A peak must be the\nmaximum value within a window of size m+ 1, and ex-\nceeding the mean value plus a threshold within a window\nof sizea+ 1. Additionally, a peak must have at least a\ndistance ofw+ 1 to the last detected peak ( nlp). Values\nfor the parameters were tuned on a development data set to\nbe:m=a=w= 2.\nThe threshold for peak picking is determined on the\nvalidation set. Since the activation functions produced by\nthe NN contain little noise and are quite spiky, rather low\nthresholds ( 0:1\u00000:2) give best results.\n4. NEURAL NETWORK MODELS\nIn this section, we explore the properties of the neural net-\nwork models considered more closely. Of the NN cat-\negories mentioned before, we investigate three different\ntypes: bidirectional recurrent networks (BRNN), convolu-\ntional networks (CNN), and convolutional bidirectional re-\ncurrent networks (CBRNN). For every class of networks,two different architectures are implemented: i.a smaller\nnetwork, with less capacity, trained on shorter subse-\nquences (with focus only on acoustic modeling), and ii.\na larger network, trained on longer subsequences (with ad-\nditional focus on pattern modeling).\nEven though we previously showed that RNNs with la-\nbel time-shift achieve similar performance as BRNNs [35,\n36], in this work, we will not use time-shift for target la-\nbels. This is due to three reasons: i.the focus of this work\nis not real-time transcription but a comparison of NN ar-\nchitectures and training paradigms, therefore using a bidi-\nrectional architecture has no downsides; ii.it is unclear\nhow label time-shift would affect CNNs; iii.in [2], the\neffectiveness of BRNNs (BLSTMs) for beat and down-\nbeat tracking is shown. Thus, in the context of this work,\nusing BRNNs facilitates combining state-of-the-art drum\nand beat detection methods while allowing us to compare\nCNNs and RNNs in a fair manner.\n4.1 Bidirectional Recurrent Neural Network\nGated recurrent units (GRUs [5]) are similar to LSTMs in\nthe sense that both are gated RNN-cell types that facilitate\nlearning of long-term relations in the data. While LSTMs\nfeature forget, input, and output gates, GRUs only exhibit\ntwo gates: update and output. This makes the GRU less\ncomplex in terms of number of parameters. It has been\nshown that both are equally powerful [6], with the differ-\nence that more GRUs are needed in an NN layer to achieve\nthe same model capacity as with LSTMs, resulting in more\nor less equal number of total parameters. An advantage of\nusing GRUs is that hyperparameter optimization for train-\ning is usually easier compared to LSTMs.\nIn this work, two bidirectional GRU (BGRU) architec-\ntures are used. The small model (BGRU-a) features two\nlayers of 50 nodes each, and is trained on sequences of\n100 frames; the larger model (BGRU-b) consists of three\nlayers of 30 nodes each, and is trained on sequences of\n400 frames. For training an initial learning rate of 0:007is\nused.152 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Frames Context Conv. Layers Rec. Layers Dense Layers\nBGRU-a 100 — — 2 x 50 GRU —\nBGRU-b 400 — — 3 x 30 GRU —\nCNN-a — 9 1xA + 1xB — 2 x 256\nCNN-b — 25 1xA + 1xB — 2 x 256\nCBGRU-a 100 9 1xA + 1xB 2 x 50 GRU —\nCBGRU-b 400 13 1xA + 1xB 3 x 60 GRU —\nTable 1 . Overview of used neural network model architectures and parameters. Every network additionally contains a\ndense sigmoid output layer. Conv. block A consists of 2 layers with 32 3x3 ﬁlters and 3x3 max-pooling; conv. block B\nconsists of 2 layers with 64 3x3 ﬁlters and 3x3 max-pooling; both use batch normalization.\n4.2 Convolutional Neural Network\nConvolutional neural networks have been successfully ap-\nplied not only in image processing, but also many other\nmachine learning tasks. The convolutional layers are con-\nstructed using two different building blocks: block Acon-\nsists of two layers with 32 3x3 ﬁlters and block Bconsists\nof two layers with 64 3x3 ﬁlters; both in combination with\nbatch normalization [18], and each followed by a 3x3 max\npooling layer and a drop-out layer ( \u0015= 0:3) [32].\nFor both CNN models, block Ais used as input, fol-\nlowed by block B, and two fully connected layers of size\n256. The only difference between the small (CNN-a) and\nthe large (CNN-b) model is the context used to classify a\nframe: 9 and 25 frames are used for CNN-a and CNN-b re-\nspectively. While plain CNNs do not feature any memory,\nthe spectral context allows the CNN to access surround-\ning information during training and classiﬁcation. How-\never, a context of 25 frames (250ms) is not enough to ﬁnd\nrepetitive structures in the rhythm patterns. Therefore, the\nCNN can only rely on acoustic, i.e., spectral features of the\nsignal. Nevertheless, with advanced training methods like\nbatch normalization, as well as the advantage that CNNs\ncan easily learn pitch invariant kernels, CNNs are well-\nequipped to learn a task adequate acoustic model. For\ntraining an initial learning rate of 0:001is used.\n4.3 Convolutional Bidirectional RNN\nConvolutional recurrent neural networks (CRNN) repre-\nsent a combination of CNNs and RNNs. They feature con-\nvolutional layers as well as recurrent layers. Different im-\nplementations are possible. In this work, the convolutional\nlayers directly process the input features, i.e. spectrogram\nrepresentations, meant to learn an acoustic model (cf. 2D\nimage processing tasks). The recurrent layers are placed\nafter the convolutional layers and are supposed to serve as\na means for the network to learn structural patterns.\nFor this class of NN, the two versions differ in the fol-\nlowing aspects: CBGRU-a features 2 recurrent layers with\n30 GRUs each, uses a spectral context of 9 frames for con-\nvolution, and is trained on sequences of length 100; while\nCBGRU-b features 3 recurrent layers with 60 GRUs each,\nuses a spectral context of 13 frames, and is trained on se-\nquences of length 400. For training an initial learning rate\nof0:0005 is used.Table 1 recaps the information of the previous sections\nin a more compact form. Figure 2 visualizes the modes\nof operation of the different NN architectures on the input\nspectrograms.\n5. EVALUATION\nFor evaluation of the introduced NN architectures, the dif-\nferent models are individually trained on single data sets in\na three-fold cross-validation manner. For data sets which\ncomprise beat annotations, three different experiments are\nperformed (explained in more detail in section 5.2); using\ndata sets only providing drum annotations, just the drum\ndetection task is performed.\n5.1 Data Sets\nIn this work, the different methods are evaluated using\nthree different data sets, consisting of two well-known and\na newly introduced set.\n5.1.1 IDMT-SMT-Drums v.1 (SMT)\nPublished along with [7], the IDMT-SMT-Drums2data\nset comprises tracks containing three different drum-set\ntypes. These are: i.real-world, acoustic drum sets (ti-\ntledRealDrum ),ii.drum synthesizers ( TechnoDrum ), and\niii.drum sample libraries ( WaveDrum ). It consists of 95\nsimple drum tracks containing bass drum, snare drum and\nhi-hat only. The tracks have an average length of 15s and\na total length of 24m. Also included are additional 285\nshorter, single-instrument training tracks as well as 180\nsingle instrument tracks for 60 of the 95 mixture tracks\n(from the WaveDrum02 subset)—intended to be used for\nsource separation experiments. These additional single in-\nstrument tracks are used as additional training samples (to-\ngether with their corresponding split) but not for evalua-\ntion.\n5.1.2 ENST Drums (ENST)\nThe ENST-Drums set [14] contains real drum recordings\nof three different drummers performing on different drum\nkits.3Audio ﬁles for separate solo instrument tracks\n2https://www.idmt.fraunhofer.de/en/business_\nunits/m2d/smt/drums.html\n3http://perso.telecom-paristech.fr/ ˜grichard/\nENST-drums/Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 153Input Features Target Functions\nSpectrogram Beats Drums Beats\nDT 3 3\nBF 3 3 3\nMT 3 3 3\nTable 2 . Overview of experimental setup. Rows repre-\nsent individual tasks and show their input feature and target\nfunction combinations.\nas well as for two mixtures are included. Additionally,\naccompaniment tracks are available for a subset of the\nrecordings—the so called minus-one tracks. In this work,\nthe wet mixes (contains standard post-processing like com-\npression and equalizing) of the minus-one tracks were\nused. They make up 64 tracks of 61s average length and a\ntotal length of 1h.\nEvaluation was performed on the drum-only tracks\n(ENST solo) as well as the mixes with their accompani-\nment tracks (ENST acc.). Since the ENST-Drums data set\ncontains more than the three instruments under observa-\ntion, only the snare, bass, and hi-hat annotations were used.\n5.1.3 RBMA Various Assets 2013 (RBMA13)\nThis new data set consists of the 30 tracks of the freely\navailable 2013 Red Bull Music Academy Various As-\nsets sampler.4The sampler covers a variety of elec-\ntronically produced music, which encompasses electronic\ndance music (EDM) but also singer-songwriter tracks and\neven fusion-jazz styled music. Three tracks on the sampler\ndo not contain any drums and are therefore ignored. An-\nnotations for drums, beats, and downbeats were manually\ncreated. Tracks in this set have an average length of 3m\n50s. The total length of the data set is 1h 43m.\nThis data set is different from the other two data sets in\nthree aspects: i.it contains quite diverse drum sounds, ii.\nthe drum patterns are arranged in the usual song-structure\nwithin a full length track, and iii.most of the tracks contain\nsinging voice, which showed to be a challenge for systems\nsolely trained on music without singing voice. The annota-\ntions for drums and beats have been manually created and\nare publicly available for download.5\n5.2 Experimental Setup\nTo compare the different NN architectures, and evaluate\nthem in the context of ADT using joint learning of beat\nand drum activations, the following experiments were per-\nformed.\n5.2.1 Drum Detection (DT)\nIn this set of experiments, the features as explained in\nsec. 3.1 and target functions generated from the drum an-\nnotations described in sec. 3.3 are used for NN training.\n4https://rbma.bandcamp.com/album/various-\nassets-not-for-sale-red-bull-music-academy-\nnew-york-2013\n5http://ifs.tuwien.ac.at/ ˜vogl/datasets/SMT ENST RBMA13\nsolo acc. DT BF MT\nGRUts [36] 92.5 83.3 75.0 - - -\nBGRU-a 93.0 80.9 70.1 59.8 63.6 64.6\nBGRU-b 93.3 82.9 72.3 61.8 64.5 64.3\nCNN-a 87.6 78.6 70.8 66.2 66.7 63.3\nCNN-b 93.4 85.0 78.3 66.8 65.2 64.8\nCBGRU-a 95.2 84.6 76.4 65.2 66.1 66.9\nCBGRU-b 93.8 83.9 78.4 67.3 68.4 67.2\nTable 3 . F-measure results for the evaluated models on\ndifferent data sets. The columns DT, BF, and MT show\nresults for models trained only for drum detection, trained\nusing oracle beats as additional input features, and simul-\ntaneously trained on drums and beats, respectively. Bold\nvalues represent the best performance for an experiment\nacross models. The baseline can be found in the ﬁrst row.\nThese experiments are comparable to the ones in the re-\nlated work, since we use a similar setup. As baseline, the\nresults in [36] are used. The results of this set of experi-\nments allow to compare the performance of different NN\narchitectures for drum detection.\n5.2.2 Drum Detection with Oracle Beat Features (BF)\nFor this set of experiments, in addition to the input features\nexplained in sec. 3.1, the annotated beats, represented as\nthe target functions for beats and downbeats, are included\nas input features. As targets for NN training only the drum\ntarget functions are utilized. Since beat annotations are re-\nquired for this experiment, only data sets comprising beat\nannotations can be used. Using the results of these experi-\nments, it can be investigated if the prior knowledge of beat\nand downbeat positions is beneﬁcial for drum detection.\n5.2.3 Joint Drum and Beat Detection (MT)\nThis set of experiments represents the multi-task learning\ninvestigation. As input for training, again, only the spec-\ntrogram features are used. Targets for training of the NNs\ncomprise, in this case, drum and beat activation functions.\nAs discussed in the introduction, in some cases it can be\nbeneﬁcial to train related properties simultaneously. Beats\nand drums are closely related, because usually drum pat-\ntern are repetitive on a bar-level (separated by downbeats)\nand drum onsets often correlate with beats.\nThe insight which can be drawn from these experi-\nments is whether simultaneous training of drums, beats,\nand downbeats is beneﬁcial. It is of interest if the result-\ning performance is higher than the one achieved for DT;\nand also if it is below, comparable, or even surpasses the\nresults in the BF experiment series.\nTable 2 gives an overview of the properties of the ex-\nperiments and the used feature/target combination.\n5.3 Evaluation Method\nTo evaluate the performance of the different architectures\nand training methods, the well-known metrics precision,154 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 3 . Results for RBMA13 data set, highlighting the\ninﬂuence of oracle beat features (BF) and multi-task learn-\ning (MT). While recurrent models (left and right) beneﬁt,\nconvolutional models (center) do not.\nrecall, and F-measure are used. These are calculated for\ndrum instrument onsets as well as beat positions. True pos-\nitive, false positive, and false negative onset and beat po-\nsitions are identiﬁed by using a 20mstolerance window.\nThis is in line with the evaluation in [36] which is used as\nbaseline for the experiments of this work. Note that other\nwork, e.g. [7, 25, 31], uses less strict tolerance windows of\n30msor50msfor evaluation.\n6. RESULTS AND DISCUSSION\nTable 3 shows the F-measure results for the individual NN\narchitectures on the data sets used for evaluation. The re-\nsults for BGRU-a and BGRU-b on the ENST data set are\nlower than for the baseline, although the models should be\ncomparable. This is due to the fact that in [36] data aug-\nmentation is applied. This is especially helpful in the case\nof the ENST data set, since e.g. the pitches of the base\ndrums vary greatly over the different drum kits. The re-\nsults for CNN-a are lower than the state of the art, which\nimplies that the context of 9 frames is too small to detect\ndrum events using a CNN. All other results on the ENST\nand SMT data sets represent an improvement over the state\nof the art. This shows that CNN with a large enough spec-\ntral context (25 frames in this work) can detect drum events\nbetter than RNNs. A part of the large increase for the ENST\ndata set can be attributed to the fact that CNNs can model\npitch invariance easier than RNNs.\nThe results for the MT experiments show the follow-\ning tendencies: For the BGRU-a and BGRU-b models, an\nimprovement can be observed when applying multi-task\nlearning. Compared to using oracle beats (BF) for train-\ning, the improvement is higher for BGRU-a and similar in\nthe case of BGRU-b. This result is interesting for two rea-\nsons: i.although BGRU-a is trained on short sequences, an\nimprovement can be observed, and ii.the improvement is\ncomparable to that when using oracle beats (BF) although\nthe beat tracking results are low. This could imply that\nmulti-task learning is also beneﬁcial for the acoustic model\nof the system. As expected, the CNNs (CNN-a, CNN-\nb) can not improve when using multi-task learning, but\nrather the results deteriorate. In case of the convolutional-BLSTM [2] 85.6\nBGRU-a 46.4\nBGRU-b 46.2\nCNN-a 44.9\nCNN-b 46.9\nCBGRU-a 47.6\nCBGRU-b 48.8\nTable 4 . F-measure results for beat detection for the multi-\ntask learning experiments compared to a state-of-the-art\napproach (ﬁrst row) on the RBMA13 set.\nrecurrent models, the result for CBGRU-a is similar to\nBGRU-a. In case of CBGRU-b no improvement of drum\ndetection performance using multi-task learning can be ob-\nserved, although it is the case using oracle beats (BF). We\nattribute this to the fact that CBGRU-b has enough capacity\nfor good acoustic modeling, while the low beat detection\nresults limit the effects of multi-task learning on this level.\nTable 4 shows the F-measure results for beat and down-\nbeat tracking. The results are all below the state-of-the-art\nbeat tracker used as baseline [2]. This is due to several\nfactors. In [2], i.much larger training sets for beat and\ndownbeat tracking are used, ii.the LSTMs are trained on\nfull sequences of the input data, giving the model more\ncontext, and iii.an additional music language model in the\nform of a dynamic Bayesian network (DBN) is used.\nThe results for CNNs and CRNNs show that convolu-\ntional feature processing is beneﬁcial for drum detection.\nThe ﬁnding considering drum detection results for multi-\ntask learning are also promising. The low results of beat\nand downbeat tracking are certainly a limiting factor and\nprobably the reason for the lack of improvement for MT\nover DT in the case of BGRU-b. As a next step, to better\nleverage multi-task learning effects, beat detection results\nmust be improved using similar techniques as in [2].\n7. CONCLUSIONS\nIn this work, convolutional and convolutional-recurrent\nNN models for drum transcription were introduced and\ncompared to the state of the art of recurrent models. The\nevaluation shows that the new models are able to outper-\nform this state of the art. Furthermore, an investigation\nwhether i.beat and downbeat input features are beneﬁ-\ncial for drum detection, and ii.this beneﬁt is also achiev-\nable using multi-task learning of drums, beats, and down-\nbeats, was conducted. The results show that this is the\ncase, although the low beat and downbeat detection results\nachieved with the implemented architectures is a limiting\nfactor. While the goal of this work was not to improve\nthe capabilities of beat and downbeat tracking per se, fu-\nture work will focus on improving these aspects, as we be-\nlieve this will have an overall positive impact on the per-\nformance of the joint model. The newly created data set\nconsisting of freely available music and annotations for\ndrums, beats and downbeats will be an asset for this line\nof research to the community.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 1558. ACKNOWLEDGEMENTS\nThis work has been partly funded by the Austrian FFG un-\nder the BRIDGE 1 project SmarterJam (858514), by the\nEU’s seventh Framework Programme FP7/2007-13 for re-\nsearch, technological development and demonstration un-\nder grant agreement no. 610591 ( GiantSteps ), as well\nby the Austrian ministries BMVIT and BMWFW, and the\nprovince of Upper Austria via the COMET Center SCCH.\nWe would like to thank Wulf Gaebele and the annotators\nMarc ¨Ubel and Jo Thalmayer from the Red Bull Music\nAcademy, as well as Sebastian B ¨ock for advice and help\nwith beat and downbeat annotations and detection.\n9. REFERENCES\n[1] Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, 2013.\n[2] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nJoint beat and downbeat tracking with recurrent neural\nnetworks. In Proc. 17th Intl Society for Music Informa-\ntion Retrieval Conf (ISMIR) , 2016.\n[3] Sebastian B ¨ock and Gerhard Widmer. Maximum ﬁlter\nvibrato suppression for onset detection. In Proc. 16th\nIntl Conf on Digital Audio Effects (DAFx) , 2013.\n[4] Rich Caruana. Multitask learning. In Thrun and Pratt\n(eds.) Learning to learn , pages 95–133. Springer,\n1998.\n[5] Kyunghyun Cho, Bart van Merri ¨enboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. Learning phrase represen-\ntations using rnn encoderdecoder for statistical ma-\nchine translation. In Proc. Conf on Empirical Methods\nin Natural Language Processing (EMNLP) , 2014.\n[6] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. Empirical evaluation of gated re-\ncurrent neural networks on sequence modeling. http:\n//arxiv.org/abs/1412.3555 , 2014.\n[7] Christian Dittmar and Daniel G ¨artner. Real-time tran-\nscription and separation of drum recordings based on\nnmf decomposition. In Proc. 17th Intl Conf on Digital\nAudio Effects (DAFx) , 2014.\n[8] Jeffrey Donahue, Lisa Anne Hendricks, Sergio\nGuadarrama, Marcus Rohrbach, Subhashini Venu-\ngopalan, Kate Saenko, and Trevor Darrell. Long-term\nrecurrent convolutional networks for visual recognition\nand description. In Proc. IEEE conference on computer\nvision and pattern recognition (CVPR) , 2015.\n[9] Simon Durand, Juan P Bello, Bertrand David, and Ga ¨el\nRichard. Downbeat tracking with multiple features and\ndeep neural networks. In Proc. 40th IEEE Intl Conf\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2015.[10] Simon Durand, Juan P Bello, Bertrand David, and\nGa¨el Richard. Feature adapted convolutional neural\nnetworks for downbeat tracking. In Proc. 41st IEEE\nIntl Conf on Acoustics, Speech and Signal Processing\n(ICASSP) , 2016.\n[11] Derry FitzGerald, Bob Lawlor, and Eugene Coyle.\nDrum transcription in the presence of pitched instru-\nments using prior subspace analysis. In Proc. Irish Sig-\nnals & Systems Conf , 2003.\n[12] Nicolai Gajhede, Oliver Beck, and Hendrik Purwins.\nConvolutional neural networks with batch normaliza-\ntion for classifying hi-hat, snare, and bass percussion\nsound samples. In Proc. Audio Mostly Conf , 2016.\n[13] Olivier Gillet and Ga ¨el Richard. Automatic transcrip-\ntion of drum loops. In Proc. 29th IEEE Intl Conf on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\n2004.\n[14] Olivier Gillet and Ga ¨el Richard. Enst-drums: an exten-\nsive audio-visual database for drum signals processing.\nInProc. 7th Intl Conf on Music Information Retrieval\n(ISMIR) , 2006.\n[15] Olivier Gillet and Ga ¨el Richard. Supervised and unsu-\npervised sequence modelling for drum transcription. In\nProc. 8th Intl Conf on Music Information Retrieval (IS-\nMIR) , 2007.\n[16] Olivier Gillet and Ga ¨el Richard. Transcription and sep-\naration of drum signals from polyphonic music. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 16(3):529–540, 2008.\n[17] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural Computation , 9(8):1735–1780,\nNovember 1997.\n[18] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reduc-\ning internal covariate shift. http://arxiv.org/\nabs/1502.03167 , 2015.\n[19] Florian Krebs, Sebastian B ¨ock, Matthias Dorfer,\nand Gerhard Widmer. Downbeat tracking using beat-\nsynchronous features and recurrent neural networks. In\nProc. 17th Intl Society for Music Information Retrieval\nConf (ISMIR) , 2016.\n[20] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nRhythmic pattern modeling for beat and downbeat\ntracking in musical audio. In Proc. 15th Intl Society\nfor Music Information Retrieval Conf (ISMIR) , 2013.\n[21] Florian Krebs, Filip Korzeniowski, Maarten Grachten,\nand Gerhard Widmer. Unsupervised learning and re-\nﬁnement of rhythmic patterns for beat and downbeat\ntracking. In Proc. 22nd European Signal Processing\nConf (EUSIPCO) , 2014.156 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[22] Marius Miron, Matthew EP Davies, and Fabien\nGouyon. Improving the real-time performance of a\ncausal audio drum transcription system. In Proc. 10th\nSound and Music Computing Conf (SMC) , 2013.\n[23] Marius Miron, Matthew EP Davies, and Fabien\nGouyon. An open-source drum transcription system for\npure data and max msp. In Proc. 38th IEEE Intl Conf\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2013.\n[24] Arnaud Moreau and Arthur Flexer. Drum transcription\nin polyphonic music using non-negative matrix factori-\nsation. In Proc. 8th Intl Conf on Music Information Re-\ntrieval (ISMIR) , 2007.\n[25] Jouni Paulus and Anssi Klapuri. Drum sound detec-\ntion in polyphonic music with hidden markov models.\nEURASIP Journal on Audio, Speech, and Music Pro-\ncessing , 2009.\n[26] Geoffroy Peeters and Helene Papadopoulos. Simulta-\nneous beat and downbeat-tracking using a probabilistic\nframework: Theory and large-scale evaluation. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 19(6):1754–1769, 2011.\n[27] Pedro HO Pinheiro and Ronan Collobert. Recurrent\nconvolutional neural networks for scene labeling. In\nProc. 31st Intl Conf on Machine Learning (ICML) ,\nBeijing, China, 2014.\n[28] Jan Schl ¨uter and Sebastian B ¨ock. Improved musical\nonset detection with convolutional neural networks. In\nProc. 39th IEEE Intl Conf on Acoustics, Speech and\nSignal Processing (ICASSP) , 2014.\n[29] Mike Schuster and Kuldip K Paliwal. Bidirectional re-\ncurrent neural networks. IEEE Transactions on Signal\nProcessing , 45(11):2673–2681, 1997.\n[30] Siddharth Sigtia, Emmanouil Benetos, Srikanth\nCherla, Tillman Weyde, Artur S d’Avila Garcez, and\nSimon Dixon. An RNN-based music language model\nfor improving automatic music transcription. In Proc.\n15th Intl Society for Music Information Retrieval Conf\n(ISMIR) , 2014.\n[31] Carl Southall, Ryan Stables, and Jason Hockman. Au-\ntomatic drum transcription using bidirectional recur-\nrent neural networks. In Proc. 17th Intl Society for Mu-\nsic Information Retrieval Conf (ISMIR) , 2016.\n[32] Srivastava, Nitish and Hinton, Geoffrey and\nKrizhevsky, Alex and Sutskever, Ilya and Salakhut-\ndinov, Ruslan. Dropout: A Simple Way to Prevent\nNeural Networks from Overﬁtting. Journal of Machine\nLearning Research , 15(1):1929–1958, 2014.\n[33] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5rm-\nsprop: Divide the gradient by a running average of its\nrecent magnitude. In COURSERA: Neural Networks\nfor Machine Learning , October 2012.[34] Christian Uhle, Christian Dittmar, and Thomas Sporer.\nExtraction of drum tracks from polyphonic music using\nindependent subspace analysis. In Proc. 4th Intl Sym-\nposium on Independent Component Analysis and Blind\nSignal Separation , 2003.\n[35] Richard V ogl, Matthias Dorfer, and Peter Knees. Re-\ncurrent neural networks for drum transcription. In\nProc. 17th Intl Society for Music Information Retrieval\nConf (ISMIR) , 2016.\n[36] Richard V ogl, Matthias Dorfer, and Peter Knees. Drum\ntranscription from polyphonic music with recurrent\nneural networks. In Proc. 42nd IEEE Intl Conf on\nAcoustics, Speech and Signal Processing (ICASSP) ,\n2017.\n[37] Chih-Wei Wu and Alexander Lerch. Drum transcrip-\ntion using partially ﬁxed non-negative matrix factoriza-\ntion with template adaptation. In Proc. 16th Intl Society\nfor Music Information Retrieval Conf (ISMIR) , 2015.\n[38] Kazuyoshi Yoshii, Masataka Goto, and Hiroshi G\nOkuno. Drum sound recognition for polyphonic au-\ndio signals by adaptation and matching of spectrogram\ntemplates with harmonic structure suppression. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 15(1):333–345, 2007.\n[39] Zhen Zuo, Bing Shuai, Gang Wang, Xiao Liu, Xingx-\ning Wang, Bing Wang, and Yushi Chen. Convolutional\nrecurrent neural networks: Learning spatial dependen-\ncies for image representation. In Proc. IEEE Conf on\nComputer Vision and Pattern Recognition Workshops\n(CVPRW) , 2015.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 157"
    },
    {
        "title": "Re-Visiting the Music Segmentation Problem with Crowdsourcing.",
        "author": [
            "Cheng-i Wang",
            "Gautham J. Mysore",
            "Shlomo Dubnov"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415944",
        "url": "https://doi.org/10.5281/zenodo.1415944",
        "ee": "https://zenodo.org/records/1415944/files/WangMD17.pdf",
        "abstract": "Identifying boundaries in music structural segmentation is a well studied music information retrieval problem. The goal is to develop algorithms that automatically identify segmenting time points in music that closely matches hu- man annotated data. The annotation itself is challenging due to its subjective nature, such as the degree of change that constitutes a boundary, the location of such bound- aries, and whether a boundary should be assigned to a sin- gle time frame or a range of frames. Existing datasets have been annotated by small number of experts and the anno- tators tend to be constrained to specific definitions of seg- mentation boundaries. In this paper, we re-examine the an- notation problem. We crowdsource the problem to a large number of annotators and present an analysis of the results. Our preliminary study suggests that although there is a cor- relation to existing datasets, this form of annotations re- veals additional information such as stronger vs. weaker boundaries, gradual vs. sudden boundaries, and the differ- ence in perception of boundaries between musicians and non-musicians. The study suggests that it could be worth re-defining certain aspects of the boundary identification in music structural segmentation problem with a broader definition.",
        "zenodo_id": 1415944,
        "dblp_key": "conf/ismir/WangMD17",
        "keywords": [
            "music information retrieval",
            "automatically identify segmenting time points",
            "subjective nature of annotation",
            "crowdsourcing",
            "analysis of results",
            "correlation to existing datasets",
            "additional information revealed",
            "stronger vs. weaker boundaries",
            "gradual vs. sudden boundaries",
            "difference in perception"
        ],
        "content": "RE-VISITING THE MUSIC SEGMENTATION PROBLEM WITH\nCROWDSOURCING\nCheng-i Wang\nUCSD\nchw160@ucsd.eduGautham J. Mysore\nAdobe Research\ngmysore@adobe.comShlomo Dubnov\nUCSD\nsdubnov@ucsd.edu\nABSTRACT\nIdentifying boundaries in music structural segmentation is\na well studied music information retrieval problem. The\ngoal is to develop algorithms that automatically identify\nsegmenting time points in music that closely matches hu-\nman annotated data. The annotation itself is challenging\ndue to its subjective nature, such as the degree of change\nthat constitutes a boundary, the location of such bound-\naries, and whether a boundary should be assigned to a sin-\ngle time frame or a range of frames. Existing datasets have\nbeen annotated by small number of experts and the anno-\ntators tend to be constrained to speciﬁc deﬁnitions of seg-\nmentation boundaries. In this paper, we re-examine the an-\nnotation problem. We crowdsource the problem to a large\nnumber of annotators and present an analysis of the results.\nOur preliminary study suggests that although there is a cor-\nrelation to existing datasets, this form of annotations re-\nveals additional information such as stronger vs. weaker\nboundaries, gradual vs. sudden boundaries, and the differ-\nence in perception of boundaries between musicians and\nnon-musicians. The study suggests that it could be worth\nre-deﬁning certain aspects of the boundary identiﬁcation\nin music structural segmentation problem with a broader\ndeﬁnition.\n1. INTRODUCTION\nMusic segmentation has been a fundamental task in auto-\nmatic music content analysis. The task includes detect-\ning boundaries between contiguous segments and labeling\neach detected segment within a music piece. In order to\nevaluate this task and train supervised machine learning\nalgorithms, researchers have developed datasets that con-\ntain boundary timing and segment labeling annotations. In\nthe majority of these datasets (such as Beatles-TUT [11],\nCHARM Mazurka [3] and Beatles-ISO [7], boundary tim-\nings are annotated by music experts and are deﬁned as the\ntime points that separate a music piece into non-overlapped\ncontiguous sections representing meaningful song struc-\ntures. These annotations provide clean-cut data for devel-\nc\rCheng-i Wang, Gautham J. Mysore, Shlomo Dubnov.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Cheng-i Wang, Gautham J. Mysore,\nShlomo Dubnov. “Re-visiting the Music Segmentation Problem with\nCrowdsourcing”, 18th International Society for Music Information Re-\ntrieval Conference, Suzhou, China, 2017.oping algorithms. During evaluation, the metrics are, in\nshort, a measure of how close the automatically detected\nboundaries are to the ground truth annotations [6, 14].\nNevertheless, the instructions and rules for expert anno-\ntators to annotate these boundaries differs between datasets\nand the annotations inevitably conform to the subjective\njudgments of the annotators [13]. Moreover, the “one time\npoint for one boundary” deﬁnition prevents the concept of\nshort ambiguous/transitional/developing musical regions\nto be explored by researchers. To be more speciﬁc, it is\nestablished that different listeners will disagree on whether\ncertain boundaries should exist or not in a music piece, and\nthe saliences between boundaries might be different, while\nalmost none of the existing datasets provide information\nabout these intuitions [1, 13].\nIn [13], the issues of inter-annotator disagreement and\nthe lack of multiple level annotations are discussed. The\ncreation of the SALAMI dataset then attempts to solve such\nissues by having two versions of labels (by two annotators)\nand two levels (long and short time scale) of annotations in\npart of the dataset. The SPAM dataset also has ﬁve an-\nnotators with two levels of annotation for 50songs [9].\nIn [8, 10], the issue of lacking support for hierarchical\nsegmentation is discussed. Although the evaluation met-\nrics for hierarchical segmentation problems are proposed\nin [8], only two datasets having two levels of hierarchy\ncurrently support this concept.\nApart from the inter-annotator agreement and between-\nboundaries saliency issues, the problem of not being able\nto model different types of boundaries is also an issue due\nto the current format of annotations. Sometimes the dis-\nagreements between annotators are not about whether one\nboundary should exist or not, but rather the timing of that\nboundary. The disagreement is likely because the exact\nchange point or boundary between two larger segments is\ndifﬁcult to recognize if there are smaller transitional, piv-\notal, building, fading, or developing musical region con-\nnecting these two segments.\nOne of the major reasons for these limitations in ex-\nisting datasets is the large amount of time and effort re-\nquired by music experts to annotate songs. It tends to pre-\nvent datasets from having numerous (more than 5) anno-\ntators, and also tends to prevent detailed annotations for\neach song. One can alleviate the amount of effort required\nfor annotating segmentation boundaries by crowdsourcing\nsuch task to the web. To the best of our knowledge, no\nmethodology has been proposed utilizing crowdsourcing738Dataset Song Name Artist\nBeatles-TUTAll You Need is Love\nThe BeatlesHelp!\nHere, There and Everywhere\nStrawberry Fields Forever\nCome Together( \u0003)\nSALAMISmoke Machines Atom Orr\nYou Done Me Wrong Cindy Woolf\nOut in the Cold Carole King\nBlack or White Michael Jackson\nWe will Rock You( \u0003) Queen\nTable 1 . Song lists of the subsets from Beatles-TUT and\nSALAMI . Songs followed by asterisks are the hidden refer-\nence songs during the task.\nto collect music segmentation boundaries. Crowdsourcing\nwith Amazon’s Mechanical Turks has been used to collect\nmusic similarity [4], music mood [5] data collection and\naudio sound quality evaluation [2].\nIn this paper, we present an preliminary study to sup-\nport the above observations and address the above issues.\nWe believe that this could lead to the creation of richer\ndatasets with signiﬁcantly less effort than previously re-\nquired. In order to investigate the inter-annotator, between-\nboundary saliency problems and explore different types of\nsegmentation boundaries, we used small subsets from ex-\nisting datasets and annotate these via crowdsourcing. The\nresults are a collection of annotations from at least 53an-\nnotators (with at least 6annotators annotated each song in\nfull coverage) for each song for a total of 8songs. The\nmethodology of collecting annotations via crowdsourcing\nis described in section 2. The validation and analysis of the\ncollected annotations are elaborated in section 3. Conclu-\nsions, and proposed future works are in section 4.\n2. CROWDSOURCING\nTo perform the music segmentation boundary annotation\ncollection task on the web, we use Amazon’s Mechanical\nTurk (AMT). The proposed methodology is implemented\nas an extension of the CAQE (The Crowdsourced Audio\nQuality Evaluation Toolkit1) python package [2].\n2.1 Data\nTwo subsets of songs from two music segmentation\ndatasets were selected randomly to be annotated by the\nproposed methodology. The two datasets are the Beatles-\nTUT andSALAMI datasets. From each dataset, 5songs\nwere randomly selected. Among the 5songs, 1song is\nused as the hidden reference during the collection task to\ndetermine whether to accept or reject a person’s annota-\ntions. Table 1 shows the 5songs from both datasets.\n2.2 Task Design\nThe typical off-line annotation process by music experts is\nto let them listen to a whole song and then annotate bound-\naries. This allows them to ﬁne tune their annotations with-\n1https://github.com/interactiveaudiolab/CAQEout time constraints. It also typically necessitates famil-\niarity with an audio editing software package. Annotators\non AMT, on the other hand, typically spend less time on\nsuch a task since their payment is ﬁxed for a given task.\nThey also typically have less (or no) experience with audio\nediting software packages. Therefore, the annotation pro-\ncess needs to be redesigned to accommodate it as an AMT\nbased task. There are two goals of the redesign. The ﬁrst\ngoal is to simplify the annotation process so that AMT an-\nnotators could learn how to annotate quickly and repeat the\nprocess easily. The second goal is to maintain the quality\nof the annotations so that the results are informative and\nusable.\nSince music structural segmentation is subjective in na-\nture, we aim to not bias AMT annotators toward listening\nto speciﬁc musical cues. Therefore, the working deﬁnition\nused in the description of the task is kept as concise as pos-\nsible and no musical terms are used. It is as follows:\nThis listening task aims at collecting the\nboundary timings between parts of a song.\nDuring this task, you will be asked to listen\nforwhen a part of a song changes to another.\nRather than asking AMT annotators to listen to an entire\nsong, we present them with short clips of music. For each\nclip, their task is to listen to the clip, determine if a bound-\nary exists in the clip, and label the location of the bound-\nary. We segment each song into clips of 20seconds long\nwith10seconds of overlap. The choice of 20seconds is\nmade according to the average length of segments deﬁned\nby ground truth annotations in existing datasets as reported\nin [12]. The annotator is only given the option of labeling a\nsingle boundary and asked to choose the strongest bound-\nary if they hear more than one. They also have the option\nof choosing no boundary.\nThe goal of the user interface design is to simplify the\ntask and not bias the annotators to any clues apart from\nwhat is heard in the clip. To ensure that the annotator\nlistened to the full clip and made an annotation decision\nbefore going to the next clip, all buttons and sliders are\ndisabled except the Play/Pause button until the ﬁrst full\nplayback of the clip. The annotator uses the Play/Pause\nbutton and audio progress bar to listen to and navigate the\nclip. The annotation is done by clicking on the bound-\nary selection slider. The darker green area surrounding the\nclicked location on the boundary selection slider indicates\nthe playback region for the check selection button. The\nannotator has the option to simply not choose a boundary\nif they do not hear one. The next trial button is disabled\nuntil the annotator clicks on the change heard (submit cur-\nrent clicked location on the boundary selection slider) or\nno change heard button. A snapshot of the user interface is\nshown in Figure 1.\nEach boundary collection task (“HIT” in AMT’s terms)\ncontains 10clips, with 9clips randomly selected from all\nnon-hidden reference songs and 1clip from the hidden ref-\nerence song. The randomization of selected clips is de-\nsigned such that annotators will not be presented with over-\nlapping clips within one task ( 10clips) and will cover theProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 739Figure 1 . The user interface for the AMT annotating task. The annotator uses the Play/Pause button and audio progress\nbar to listen to and navigate the clip. The annotation is done by clicking on the boundary selection slider. The darker green\narea surrounding the clicked location on the boundary selection slider indicates the playback region for the check selection\nbutton.\nfull range of all songs once and once only if they ﬁnish all\nthe tasks available to them. The order of the 10clips within\neach task is also randomized. In order to further make sure\nthat each song is covered with enough annotations, the an-\nnotation tasks are divided into two batches according to the\ntwo dataset subsets, meaning one batch for Beatles-TUT\nand one batch for SALAMI , and are collected separately.\nThe batch for Beatles-TUT has10tasks ( 100clips) and the\nbatch for SALAMI has8tasks ( 80clips) for each annotator.\nIt is mentioned in the previous section that out of the 5\nsongs, there is 1song selected as a hidden reference acting\nas a quality check after collecting boundary annotations\nfrom the AMT annotators. To use this song as a quality\ncheck, a few clips from the song that have clear and obvi-\nous boundary regions are selected and manually annotated\nby the authors. The authors avoided using the ground truth\nannotations from the original datasets since the goal for the\ntask is not to identify the “correct” boundaries deﬁned by\nthe original datasets, but rather simply annotating reason-\nable boundaries (that might differ from the ground truth\nannotations from the original datasets).\nIn order to take this concern into account, multiple\nboundary candidates for each hidden reference clip are\nallowed so the quality check accepts wider and reason-\nable results than just using ground truth annotations (only\none boundary annotation for each clip) from the original\ndatasets. After collecting boundary annotations from an\nAMT annotator, their boundary annotations on the hidden\nreference clips are compared against the author’s annota-\ntions on the same clips. If the average distance between\nAMT annotator’s boundary annotation to the closest anno-\ntation by the author for each clip is less than 3seconds,\nall of the AMT annotator’s annotations are accepted, oth-\nerwise they are rejected and not used for the analysis. The\nannotations by the authors for the hidden references couldbe found in the code repository2.\nIn order to determine if the annotator is follow-\ning basic instructions and listening on a device (speak-\ners/headphones) with a sufﬁcient frequency response, we\ninsert a hearing screening before the task begins, as done\nin [2]. After the completion of the task, the annotator is\npresented with a post task survey gathering demographi-\ncal, musical background, and qualitative information. For\nthe musical background, the AMT annotator is asked to\nanswer if they consider themself to be a musician. We also\nask for qualitative feedback on the task and what the anno-\ntators were listening for.\n3. ANALYSIS\nWe consolidated the annotations from the individual clips\nso that we have all annotations of a given song on a com-\nmon timeline. Since there is an overlap of 10seconds be-\ntween consecutive clips, there is a chance that the same\nboundary will be labeled twice by the same annotator\n(cases in which the time difference between the two anno-\ntations are less than 3seconds). In such cases, we simply\nrandomly discarded one of the annotations.\nTwo example songs showing the annotations from the\nAMT annotators along with a comparison to the ground\ntruth annotations from the original datasets are shown in\nFigure 2. A correlation can be seen between the two.\nIn Table 2 and Table 3, overall statistics of the collected\nannotations and AMT annotators are shown. Though the\ntask is only exercised on subsets of existing datasets, the\nnumber of AMT annotators and annotations easily out-\nnumbered those of existing datasets. This observation is\ntrue even when considering only the statistics from AMT\nannotators that are self-identiﬁed musicians (numbers in\n2https://github.com/wangsix/caqe_segmentation740 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20170:00 0:35 1:10 1:45 2:21\nTimeHelp!\nall annotations\nmusician annotations\nnon-musician annotations(a) Help! - Beatles\n0:00 0:41 1:22 2:03 2:45\nTimeOut in the Cold\nall annotations\nmusician annotations\nnon-musician annotations\n(b) Out in the Cold - Carole King\nFigure 2 . Two example songs with their annotations from\nselected subsets. The ﬁrst row of each subplot is the CQT-\nspectrogram of the example song. The light blue lines in\nthis row are the ground truth annotated by music experts.\nThe yellow lines in “Out in the Cold” are the lower level\nground truth by music experts. The vertical lines in the\nrest of the rows represent annotations from all AMT anno-\ntators, musician annotators and non-musician annotators\nrespectively.\nparentheses in Table 2 and Table 3). It is also true if only\ncomplete annotations from single annotator are considered.\nThe seemingly low average coverage rate of each song\nby each annotator (right most column of Table 3) is a natu-\nral result of distributing clips randomly to AMT annotators\nthroughout the AMT tasks. Even with the randomized task\ndistribution, there are still at least 6completed annotations\nfor each song ( 2nd right column of Table 3).\nThe counts of annotations for each clip in each song are\nshown in Figure 3. From the histograms it could be ob-\nserved that every song is fully covered by multiple annota-\ntors in a more or less evenly distributed manner. The be-\nginning and ending of songs have less accumulated counts\nsince they are not fully covered by overlaps.\n3.1 Validation\nTo validate the collected annotations with the proposed\nmethodology, the aggregated annotations for one song\n01020Strawberry Fields Forever\nMusician\nNon-musician\n01020Here, There And Everywhere\n01020All You Need Is Love\n01020Help!(a) Beatles subset\n0102030Out in the Cold\nMusician\nNon-musician\n0102030Smoke Machines\n01020You Done Me Wrong\n01020Black or White\n(b) Salami subset\nFigure 3 . Histograms of annotation count along the time\nline of each song. It shows a roughly evenly distributed\ncoverage for each song being annotated.\nare treated as one segmentation boundary prediction from\nan arbitrary algorithm and compared to the ground truth\nboundary annotations from the original datasets.\nThe annotated timings of a song via crowdsourcing are\nﬁrst discretized into a binary vector with ones representing\nthe presence of annotated boundaries. The discretization is\ndone with a sampling rate of 22050 Hz and 512sample hop\nsize. Then the binary vector of each song is normalized\nby its annotation count histogram (Figure 3) to account for\ndifferent number of times each time region is annotated.\nAfter the normalization, a Gaussian window with 0:5sec-\nond standard deviation is convolved with the binary vec-\ntors obtaining a boundary detection function for each song.\nThe boundary detection functions are renormalized to be\nbetween [0;1]. A simple peak-picking strategy using the\nsame principle as [12] with 0:5s window and 0:1thresh-\nold is applied to select segmentation boundaries from the\nboundary detection function. These functions are shown in\nFigure 4 with the ground truth plotted against them.\nThe validation of such predictions are done using the\nstandard MIREX 3seconds structural music segmentation\nboundary accuracy evaluation metric. The validation re-\nsults are shown in Table 4. From the F-measures and recall\nrate, it can be observed that the aggregated results from\nAMT based annotations in general agree with the musicProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 741Annotators Accepted Annotations\nDatasets Accepted(musician) Rejected Invalid Total Per Annotator\nBeatles-TUT 61(17) 11 89 1468 24.06\nSALAMI 61(14) 41 65 1652 27.08\nTable 2 . Statistics of the AMT annotators and their annotations. Accepted annotators are the ones that passed the hidden\nreference quality check. Rejected annotators are the ones that failed the hidden reference quality check. Invalid annotators\nare the ones that did not pass the hearing screening or failed to submit their results.\nSongCount(musician) Avg. Coverage\nAnnotators Annotations Complete Per Annotator( %)\nAll You Need is Love 61(17) 452(131) 6(3) 28.53\nHelp! 53(15) 275(80) 8(3) 30.69\nHere, There and Everywhere 53(13) 264(75) 8(3) 29.93\nStrawberry Fields Forever 59(16) 477(133) 7(3) 27.25\nSmoke Machines 58(14) 376(103) 13(4) 25.31\nYou Done Me Wrong 61(14) 405(110) 12(4) 23.35\nBlack or White 61(14) 502(131) 14(5) 24.6\nOut in the Cold 58(14) 369(92) 13(3) 23.7\nTable 3 . Song statistics from accepted annotations. The numbers in parentheses in columns 2and3(from the left) are\nthe numbers for self-identiﬁed musicians. The 4th column is the number of complete annotations by one annotator. The\naverage coverage of a song per annotator ( 5th column) is calculated by dividing the average number of annotated clips per\nannotator for a given song by the total number of clips for that song.\nexperts annotating the original datasets. Also the self-\nidentiﬁed musicians performed better than non-musicians\nin7cases out of 8. There might be two reasons for the\nhigher recall rates. One reason is a potential bias due to\nthe20seconds length clip. The other reason is that some of\nthe peaks representing different levels of saliency or conﬁ-\ndence (height of the boundary detection function), resulted\nin more peaks than the single level annotations by the mu-\nsic experts.\n3.2 Inter-Annotator Analysis\nIn [9], the problem of subjectivity in music structural seg-\nmentation problem is studied by showing annotator effects\nwith the two-way ANOV A factor analysis. The same anal-\nysis approach can not be applied here since the sets of\nannotators annotating each song are different (with over-\nlapped annotators). In order to analyze the degree of agree-\nment between AMT annotators, a simple measurement is\ndevised. The agreement degree aiof one annotation iof a\nclip to other annotations i0of the same clip is deﬁned as\nai=P\ni02I;i06=i[1ifiagrees with i0]\nNumber of items in I: (1)\nwhere Iis the set of annotations of a clip annotated by\nall annotators. The agreement of one annotation ito an-\notheri0is established if the annotated timing of iis within\n3seconds of i0’ annotated timing, or if both iandi0has\nempty annotation. aiis a value between [0;1]and could be\nthought of as the probability of one annotation agrees with\nother annotations in the same 20seconds region. Since ev-\nery annotator could only annotate a clip once, Equation 1\nbecomes a measurement of agreement between annotators.\nThe agreement between annotators of one song could then\nbe measured by calculating the average of all as over one\nsong. The inter-annotator agreement of each song is shownin Table 5. From Table 5, one can observe that although the\naverage agreement between annotators is above 70%, the\nstandard deviations show that the agreement between an-\nnotators is not consistent throughout the song but varying\na lot from time to time within a song. This observation\nsupports the propositions made in [10] that multiple hu-\nman annotations should be used during evaluation to take\nhuman subjectivity into account.\n3.3 Boundaries Investigation\nBy qualitatively examining the AMT annotations and lis-\ntening to the corresponding regions, it is evident that us-\ning a single time stamp representing the boundary between\nsegmentations is inadequate. For example, the ground\ntruth segmentation boundary around 0:50 in the song Help!\nby the Beatles is annotated at the beginning of the second\nverse when the lyrics start, but the AMT based annotations\nwere spread across the region from 0:46 to 0:50 synchro-\nnizing with the sentence “Would you please, please, help\nme?”. Musically, it could be correct to say that that region\nacts as the transition between two larger segments and the\nsingle boundary at 0:50 is inadequate to represent this mu-\nsical property since the boundary is actually a prolonged\nregion. This observation suggests that there exist different\ntypes of boundaries, with the easiest categorization being\na clear-cut one versus a smooth/prolonged one. The AMT\nannotations of Help! are shown in Figure 2(a).\nThe other qualitative assessment is that the boundary\ndetection function mentioned in section 3.1 shows that the\nhierarchical nature of structural segmentation boundaries\nexist and could be measured by the relative votes a bound-\nary has compared to other boundaries in the same song.\nThe boundary detection function also suggests that instead\nof having a discretized hierarchical representation of struc-\ntural segmentation boundaries, a continuous version where742 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017SongMusician Non-musician\nPrecision Recall F-measure Precision Recall F-measure\nAll You Need is Love 0.73 0.85 0.79 0.5 0.69 0.58\nHelp! 0.5 0.78 0.61 0.33 0.89 0.48\nHere, There and Everywhere 0.81 0.9 0.86 0.61 0.8 0.7\nStrawberry Fields Forever 0.36 0.72 0.48 0.25 0.9 0.4\nSmoke Machines 0.5 0.7 0.59 0.38 0.9 0.53\nYou Done Me Wrong 0.76 0.87 0.81 0.73 0.93 0.82\nBlack or White 0.67 0.8 0.72 0.33 0.73 0.46\nOut in the Cold 0.39 0.88 0.54 0.2 0.75 0.32\nTable 4 . Standard 3-seconds precision, recall and F-measure evaluation metrics on the AMT annotator’s annotation against\nground truth from original datasets.\nStrawberry Fields Forever\nMusician\nNon-musician\nGround Truth\nHere, There And Everywhere\nAll You Need Is Love\nHelp!\n(a) Beatles subset\nBlack or White\nMusician\nNon-musician\nGround Truth(low)\nGround Truth\nSmoke Machines\nYou Done Me Wrong\nOut in the Cold\n(b) Salami subset\nFigure 4 . The boundary detection functions obtained from\nAMT annotations against ground truth by music experts.\nthe saliency or conﬁdence of boundaries is represented by a\ncontinuous curve might be another intuitive choice in terms\nof evaluating boundary detection algorithms.\n4. FUTURE WORK AND CONCLUSION\nIn this paper, a methodology utilizing crowdsourcing for\ncollecting alternative ground truth data for structural seg-\nmentation boundaries is proposed and validated. This\nmethodology provides opportunities for researchers to cre-\nate new segmentation boundary datasets in a fast and efﬁ-\ncient way. To create a dataset with the proposed method-\nology, one has to make sure not to bias annotators towardSong Avg. agreement (Std.)\nAll You Need is Love 0.71 (0.29)\nHelp! 0.75 (0.26)\nHere, There and Everywhere 0.65 (0.28)\nStrawberry Fields Forever 0.72 (0.29)\nSmoke Machines 0.72 (0.28)\nYou Done Me Wrong 0.72 (0.28)\nOut in the Cold 0.76 (0.31)\nBlack or White 0.71 (0.31)\nTable 5 . The average and standard deviation of inter-\nannotator agreement of each song.\nspeciﬁc aural cues and manage the distribution of clips so\nthat annotators work on evenly distributed clips between\nsongs and songs get evenly distributed annotations from\nall annotators.\nAs suggested in section 3.3, different types of bound-\naries exist and could be investigated given the kind of data\ncollected by the methodology proposed in this work. These\nboundary types could be categorized. Also the different\ntypes of musical cues that lead to the perception of music\nsegmentation boundaries could be investigated by another\nround of crowdsourcing on the annotated data focusing on\nsurveying the reasoning behind each annotations.\n5. ACKNOWLEDGMENT\nThis work is supported by CREL (Center for Research\nin Entertainment and Learning at UCSD) and Adobe Re-\nsearch. Special thanks to Dr. Mark Cartwright (NYU) and\nDr. Ian Wehrman (Adobe Research) for their kind help.\n6. REFERENCES\n[1] Michael J Bruderer, Martin F Mckinney, and Armin\nKohlrausch. The perception of structural boundaries in\nmelody lines of western popular music. Musicae Sci-\nentiae , 13(2):273–313, 2009.\n[2] Mark Cartwright, Bryan Pardo, Gautham J Mysore,\nand Matt Hoffman. Fast and easy crowdsourced per-\nceptual audio evaluation. In Acoustics, Speech and\nSignal Processing (ICASSP), 2016 IEEE International\nConference on , pages 619–623. IEEE, 2016.\n[3] Nicholas Cook. Performance analysis and chopin’s\nmazurkas. Musicae scientiae , 11(2):183–207, 2007.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 743[4] Jin Ha Lee. Crowdsourcing music similarity judg-\nments using mechanical turk. In ISMIR , pages 183–\n188, 2010.\n[5] Jin Ha Lee and Xiao Hu. Generating ground truth for\nmusic mood classiﬁcation using mechanical turk. In\nProceedings of the 12th ACM/IEEE-CS joint confer-\nence on Digital Libraries , pages 129–138. ACM, 2012.\n[6] Mark Levy and Mark Sandler. Structural segmenta-\ntion of musical audio by constrained clustering. IEEE\ntransactions on audio, speech, and language process-\ning, 16(2):318–326, 2008.\n[7] Matthias Mauch, Chris Cannam, Matthew Davies, Si-\nmon Dixon, Christopher Harte, Sefki Kolozali, Dan\nTidhar, and Mark Sandler. Omras2 metadata project\n2009. In Proc. of 10th International Conference on Mu-\nsic Information Retrieval , page 1, 2009.\n[8] Brian McFee, Oriol Nieto, and Juan Pablo Bello. Hier-\narchical evaluation of segment boundary detection. In\nISMIR , pages 406–412, 2015.\n[9] Oriol Nieto. Discovering structure in music: Automatic\napproaches and perceptual evaluations . PhD thesis,\nPhD thesis, New York University, 2015.\n[10] Oriol Nieto. 4.7 approaching the ambiguity problem of\ncomputational structure segmentation. Computational\nMusic Structure Analysis , page 177, 2016.\n[11] Jouni Paulus and Anssi Klapuri. Music structure analy-\nsis by ﬁnding repeated parts. In Proceedings of the 1st\nACM workshop on Audio and music computing multi-\nmedia , pages 59–68. ACM, 2006.\n[12] Jean Serra, Mathias Muller, Peter Grosche, and\nJosep Ll Arcos. Unsupervised music structure annota-\ntion by time series structure features and segment simi-\nlarity. Multimedia, IEEE Transactions on , 16(5):1229–\n1240, 2014.\n[13] Jordan Bennett Louis Smith, John Ashley Burgoyne,\nIchiro Fujinaga, David De Roure, and J Stephen\nDownie. Design and creation of a large-scale database\nof structural annotations. In ISMIR , volume 11, pages\n555–560, 2011.\n[14] Douglas Turnbull, Gert RG Lanckriet, Elias Pampalk,\nand Masataka Goto. A supervised approach for detect-\ning boundaries in music using difference features and\nboosting. In ISMIR , pages 51–54, 2007.744 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "A Framework for Distributed Semantic Annotation of Musical Score: &quot;Take It to the Bridge!&quot;.",
        "author": [
            "David M. Weigl",
            "Kevin R. Page"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415894",
        "url": "https://doi.org/10.5281/zenodo.1415894",
        "ee": "https://zenodo.org/records/1415894/files/WeiglP17.pdf",
        "abstract": "Music notation expresses performance instructions in a way commonly understood by musicians, but printed paper parts are limited to encodings of static, a priori knowledge. In this paper we present a platform for multi-way com- munication between collaborating musicians through the dynamic modification of digital parts: the Music Encod- ing and Linked Data (MELD) framework for distributed real-time annotation of digital music scores. MELD users and software agents create semantic annotations of music concepts and relationships, which are associated with mu- sical structure specified by the Music Encoding Initiative schema (MEI). Annotations are expressed in RDF, allow- ing alternative music vocabularies (e.g., popular vs. clas- sical music structures) to be applied. The same underly- ing framework retrieves, distributes, and processes infor- mation that addresses semantically distinguishable music elements. Further knowledge is incorporated from exter- nal sources through the use of Linked Data. The RDF is also used to match annotation types and contexts to render- ing actions which display the annotations upon the digital score. Here, we present a MELD implementation and de- ployment which augments the digital music scores used by musicians in a group performance, collaboratively chang- ing the sequence within and between pieces in a set list.",
        "zenodo_id": 1415894,
        "dblp_key": "conf/ismir/WeiglP17",
        "keywords": [
            "Music notation",
            "performance instructions",
            "musicians",
            "printed paper parts",
            "dynamic modification",
            "digital parts",
            "distributed real-time annotation",
            "semantic annotations",
            "Music Encoding Initiative schema",
            "Linked Data"
        ],
        "content": "A FRAMEWORK FOR DISTRIBUTED SEMANTIC ANNOTATION OF\nMUSICAL SCORE: “TAKE IT TO THE BRIDGE!”\nDavid M. Weigl and Kevin R. Page\nOxford e-Research Centre\nUniversity of Oxford, United Kingdom\nfdavid.weigl, kevin.page g@oerc.ox.ac.uk\nABSTRACT\nMusic notation expresses performance instructions in a\nway commonly understood by musicians, but printed paper\nparts are limited to encodings of static, a priori knowledge.\nIn this paper we present a platform for multi-way com-\nmunication between collaborating musicians through the\ndynamic modiﬁcation of digital parts: the Music Encod-\ning and Linked Data (MELD) framework for distributed\nreal-time annotation of digital music scores. MELD users\nand software agents create semantic annotations of music\nconcepts and relationships, which are associated with mu-\nsical structure speciﬁed by the Music Encoding Initiative\nschema (MEI). Annotations are expressed in RDF, allow-\ning alternative music vocabularies (e.g., popular vs. clas-\nsical music structures) to be applied. The same underly-\ning framework retrieves, distributes, and processes infor-\nmation that addresses semantically distinguishable music\nelements. Further knowledge is incorporated from exter-\nnal sources through the use of Linked Data. The RDF is\nalso used to match annotation types and contexts to render-\ning actions which display the annotations upon the digital\nscore. Here, we present a MELD implementation and de-\nployment which augments the digital music scores used by\nmusicians in a group performance, collaboratively chang-\ning the sequence within and between pieces in a set list.\n1. INTRODUCTION\nMusic is a fundamental channel of communication [7], be-\ntween musicians and an audience, but also among musi-\ncians performing together, and as a record of a perfor-\nmance. Inter-performer communication can support semi-\nstructured performances such as jam sessions, where the\nset list is not entirely pre-determined, and repetitions and\nvariations can be added within pieces. These decisions are\nmade and communicated as the performance unfolds.\nMusic is richly structured, and annotations may serve to\ninterlink musical content along such a structure. Annota-\ntions must be able to speciﬁcally address elements within\nc\rDavid M. Weigl and Kevin R. Page. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: David M. Weigl and Kevin R. Page. “A framework for dis-\ntributed semantic annotation of musical score: “Take it to the bridge!””,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.this structure if they are to be described or related. In mul-\ntimedia information systems, this is typically achieved us-\ning timeline anchors, offsets along a reference recording\nspeciﬁed, e.g., in milliseconds. Such timed offsets are not\nintrinsically musically meaningful without context, limit-\ning their use when no reference recording is available.\nWe can address part of this issue using the Music En-\ncoding Initiative XML schema (MEI; [6]). MEI compre-\nhensively expresses the classes, attributes, and data types\nrequired to encode a broad range of musical documents and\nstructures. It does not, however, include or reference con-\ncepts, relationships, or existing descriptive forms of multi-\nmedia Linked Data external to its schema.\nWe present the Music Encoding and Linked Data\n(MELD) framework and implementation architecture that\naugments and extends MEI structures with semantic Web\nAnnotations capable of addressing musically meaningful\nscore sections. Through its use of Linked Data, our ap-\nproach deploys knowledge structures expressing relation-\nships unconstrained by boundaries of encoding schema,\nmusical sub-domain, or use-case context, supporting re-\ntrieval of a wide range of music information. We employ\nthe ﬂexible and extensible Web Annotation model. New\nkinds of annotations are easily incorporated through cus-\ntomisation of the MELD JavaScript web-client via drop-\nin rendering and interaction handlers. Annotations, cap-\ntured in the context of the performance session with prove-\nnance information, can seamlessly reference external data\nsources, and can in turn be referenced for external analysis,\nreuse, and repurposing in other contexts.\nTo demonstrate the feasibility of our approach, we\npresent a prototypical implementation of a performance\nscenario which collects, distributes, and displays semantic\nannotations of digital music score in a live jam session.\n2. RELATED WORK\nPrevious projects have applied digital notation to mu-\nsic performance scenarios for just-in-time composition\nand computer-assisted generation of musical score (e.g.,\n[5, 23, 24]). While fascinating, these approaches are con-\ncerned with generating, rather than augmenting, musical\nscore in real-time performance scenarios. Our work, in\ncontrast, concerns the ﬂexible targetting and interlinking\nof music resources and resource fragments within a frame-\nwork of meaningfully structured music information.2212.1 Addressing Musical Content\nAlthough typically delivered in a linear rendition, music is\nrichly structured at various levels, from individual notes\nand performance directions to higher-level musical sec-\ntions (intro, verse, chorus, bridge; all terms established\nin western popular music). Viewed as hyperstructures [2],\nthese concepts can be annotated with extra-musical infor-\nmation. Such annotations may be anchored to representa-\ntions of the music using media fragments1[20] expressing\ntemporal positions along a reference timeline in millisec-\nonds, beat instances, or MIDI clock ticks (e.g., [4, 16]).\nTemporal anchors are widely used in multimedia infor-\nmation systems – for instance to link to speciﬁc scan po-\nsitions within YouTube videos2. As such anchors are not\nmusically meaningful, their usefulness is limited when tar-\ngeting music in conceptual terms, rather than in terms of a\nreference recording, such as when annotating music score.\nThe Music Encoding Initative (MEI; [6]) provides an\nXML schema encompassing a comprehensive represen-\ntation of musical structure. Content is cleanly separated\nfrom presentation [14], allowing the identiﬁcation and ad-\ndressing of (elements of) a musical work – from an entire\ncomposition, to a collection of notes constituting a phrase\nwithin a particular measure on a speciﬁc instrumental part.\nMEI arranges musical elements, each of which may be\nnamed with an XML identiﬁer, within a well-speciﬁed hi-\nerarchy. These named elements provide anchor points for\nannotations targeting a musically meaningful structure.\nThe Open MEI Addressability Service (OMAS; [21])\naddresses granular portions of music notation using offsets\nemploying units of musical structure (measures, staves,\nbeats), rather than temporal units. OMAS responds to such\nan offset speciﬁcation (supplied via a templated URI syn-\ntax) by generating MEI documents containing copies of the\nspeciﬁed portion of source MEI, resulting in new resources\ncontaining only the portions of music to be addressed. This\nenables the addressing of musical score without requiring\na reference timeline. However, it is not equivalent to ad-\ndressing a fragment of a resource within its (source) con-\ntext, a requirement when using the score as a dynamic\ncommunication framework between multiple performers.\nAlthough specifying offsets in musical terms, OMAS does\nnot directly address musically meaningful sections of the\nscore (e.g., “verse 1,” “chorus 2,” “bridge”).\n2.2 Expressing Musical Relationships\nLinked Data extends the structure of the World Wide Web\nby employing URIs to specify directed relationships be-\ntween data instances. These data instances are themselves\nencoded by URIs or represented by literal values. In the\nmusic information domain, Linked Data has been em-\nployed to describe musical resources in terms of associ-\nated catalogue metadata (e.g., [8, 22]); to publish features\nderived from audio-signal content along with associated\nprovenance metadata [12, 13]; and to transcribe symbolic\n1http://www.w3.org/TR/media-frags/\n2https://developers.google.com/youtube/player_\nparameters#startmusic content [11]. Throughout this article, we apply\nLinked Data to express annotations about musical struc-\nture from a music performance perspective.\nSeveral ontologies – Linked Data formalisations of\nclasses, properties, and relationships within musical sub-\ndomains – support the relation, interlinking, reuse, and re-\npurposing of music information within and between data\nsets and associated applications. We now discuss three\npertinent examples: the Music Ontology [17]; the Segment\nOntology [3]; and the Common Hierarchical Abstract Rep-\nresentation of Music (CHARM) ontology [9].\nWhile it does not primarily focus on music perfor-\nmance , the Music Ontology is a widely used data model\ndescribing terms and relationships around the production\nof musical works, actors (e.g., artists, composers), items\n(e.g., recordings, published scores), and events (e.g., per-\nformances). Its classes extend the Functional Require-\nments for Bibliographic Records (FRBR) ontology3, dis-\ncriminating between musical entities at different levels of\nabstraction, ranging from (at the most abstract level) the\nintellectual conception of a musical work , to its expression\n(conceptualised, e.g., as musical score), to an embodied\nmanifestation (e.g., a publication of the musical score), to\n(at the most concrete level) a physical item representing a\nsingle exemplar of a manifestation (e.g., a musician’s per-\nsonal copy of the published score).\nThe Segment Ontology represents music as comprised\nof segments ordered along an abstractly deﬁned axis (the\nsegment line ). These music-generic segments are bridged\nto different ontological structures expressing elements of\nmusical form appropriate to speciﬁc musical sub-domains\n(e.g., intro, verse, chorus, bridge; or sonata, minuet, trio,\nor fugue). This separation of concerns supports cross-\napplication to different musical domains and use cases.\nCHARM describes music at a fundamental level of\npitches, times, and durations, expressing statements as log-\nical formulae operating upon abstract data types. In this\npaper, we apply more concrete conceptualisations of musi-\ncal score sufﬁcient for the presented use case; but we invite\nthe prospect of ontological mappings from CHARM to our\nframework, which could offer intriguing opportunities for\nre-use and extension for music analytical purposes.\n3. MELD FRAMEWORK\nThe MELD4semantic framework combines and augments\npertinent subsets of a number of ontologies in a semantic\nscaffold supporting dynamic distributed annotation of mu-\nsical score (Figure 1). The Music and Segment Ontologies\ndescribe a musical work , the music score – a collection of\nmusical segments ordered along a segment line that express\nthe work (in FRBR terms) – and ﬁnally its manifestation\nas apublished score encoded as MEI. Collections of MEI\nfragments, manifestations of musical segments embodied\nwithin the published score, anchor annotations processed\nby rendering and interaction handlers (Section 4). These\nelements form the core of the MELD semantic framework.\n3http://vocab.org/frbr/core.html\n4http://github.com/oerc-music/meld222 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 1 . MELD semantic framework. Annotations address music segments embodied in a published score (MEI) resource.\nA music-generic core is linked to but seperable from domain-speciﬁc entities instantiating concrete music sections. Key\n(top right): External ontologies in bold italics .frbr: Functional Requirements for Bibliographic Records Core; ldp: Linked\nData Platform V ocabulary; meld : MELD V ocabulary; mo: Music Ontology; motivation: MELD-speciﬁc oa motivations;\noa:Web Annotation Ontology; pop, popRole, popSec : Pop music domain-speciﬁc semantics; prov: PROV Namespace;\nrdf:RDF Concepts; rdfs: RDF Schema; skos: Simple Knowledge Organization System; so:Segment Ontology.\nThese music-generic structures are linked with, but sep-\narable from, components expressing domain-speciﬁc enti-\nties associated with the concrete instantiations of musical\nsegments. Here, we have speciﬁed a taxonomy of popu-\nlar music performance terms sufﬁcient to accomodate our\nuse case. Notably, these domain- and use-case-speciﬁc\ntaxonomies may be modularly replaced by other ontologi-\ncal structures reﬂecting different domains (e.g., popular vs.\nclassical music) or use cases (e.g., annotations supporting\nmusic performance vs. musicological scholarship) without\nmodiﬁcation of the core.\nAs each entity, class, and relationship in the framework\nis assigned its own URI, the entire ontological structure,\nas well as the generated annotation and session metadata,\nis part of a wider web of Linked Data. This enables the\nseamless inclusion of external information within MELD\nannotations, as well as the referencing, reuse, and repur-\nposing of the generated information by external services.\n3.1 MELD Annotations\nMELD annotations build upon the Web Annotation Data\nModel,5a W3C recommendation providing an extensible,\ninteroperable, machine-readable means of creating annota-\ntions by asserting relationships between a set of connected\nresources, typically an annotation body and a target (or tar-\nget resource fragment).\nWeb Annotations may be associated with an explicit\nmotivation formalising the given annotation’s intended\n5https://www.w3.org/TR/annotation-model/purpose. In MELD, domain- and use-case-speciﬁc ren-\ndering and interaction clients (Section 4.2) make use of\nthis information to map the annotation to corresponding\nrendering and interaction handlers to effect changes to the\nscore displayed to the user. By deﬁning MELD-speciﬁc\nmotivations subclassing generic ones speciﬁed within the\nWeb Annotation model, we promote reuse and repurposing\nof MELD annotations in external contexts implementing\nWeb Annotation standards.\nWeb Annotations may also be associated with an in-\ntended audience to whom a given annotation applies. In\nMELD, this information is used to address annotations to\nonly certain speciﬁed participants – for instance, the player\nof a given part within a session. By default, annotations\nthat do not specify an audience are made available to every\nclient associated with a given session.\n3.2 Domain Ontologies\nWe have speciﬁed a taxonomy describing sections and\npart roles of popular music. Abstract, music-generic or-\ndered segments of a score are associated with more con-\ncrete notions of musical sections appropriate to popular\nmusic (e.g., “intro”, “verse”, “chorus”, “bridge”) via the\nSegment Ontology. In its original conception, this on-\ntology bridges music-generic and domain-speciﬁc concep-\ntions of musical segmentation by mapping an abstract seg-\nment line to a concrete reference timeline manifested along\na musical recording. To avoid the requirement of such a\nreference, we instead anchor music-generic segments toProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 223domain-speciﬁc musical sections, embodied as manifesta-\ntions within the published score. These are represented as\ncollections of media fragment URIs specifying the named\nMEI elements that comprise the given section. Where ref-\nerence recordings form a part of the use case, both ap-\nproaches are applicable, enabling ﬂexible, complementary\nstructuring of music information via temporal, symbolic,\nand semantic anchors. The simplicity of repurposing the\nSegment Ontology’s bridging mechanism within our novel\ncontext is afforded by our use of Linked Data.\nThe published score, represented in MELD by an MEI\nresource, is also associated with domain-speciﬁc part roles\n(e.g., “lead”, “bass”, “rhythm”) anchored within the MEI\nvia fragment URIs specifying the corresponding MEI staff\ndeﬁnition container element. We employ the PROV On-\ntology to express and track the provenance of relationships\nassociating speciﬁc musicians with such part roles within\nthe context of a particular performance session.\nBy virtue of the clean separation between music-\ngeneric andpopular-music-performance-speciﬁc ontolog-\nical structures, the domain-speciﬁc structures may be\nswapped out to address other use contexts while retaining\nthe rest of the presented framework, e.g. for an analytical\nontology of musicological terms supporting the use of dig-\nital score annotations to illustrate points in scholarly musi-\ncological arguments. This ﬂexibility of ontological schema\nis another key affordance of Linked Data.\n4. MELD ARCHITECTURE\nThe MELD architecture (Figure 2) implements server- and\nclient-side components: RESTful web services are used\nto manage session and annotation resources; client-side\nrendering and interaction handlers display and update an-\nnotated digital score parts relevant to each user, as well\nas capturing user interactions and updating server-side re-\nsources with interaction outcomes.\n4.1 Web Services\nMELD annotation and session management services are\nimplemented using a Python web server capable of han-\ndling operations on RDF and JSON-LD datasets encoding\nthe collection of MELD sessions, as well as the performer\npart-roles and annotations associated with each session.\n4.1.1 Performance Session Service\nThe server exposes a RESTful web service providing ac-\ncess to a resource representing the list of all MELD ses-\nsions available to a user (requested via HTTP GET).\nIn order to create a new session, a Linked Data rep-\nresentation of a basic session resource, related by a\nmo:performance ofpredicate to the published score\nMEI resource, is posted (via HTTP POST) to the list. The\nserver mints a URI to represent the new session. A ‘join’\nresource is exposed to establish qualiﬁed associations be-\ntween the respective performer and an instrumental part in\nthe session context.4.1.2 Annotation Service\nClients interact with the annotation service using an API\nbased on the Web Annotation Protocol6, which speciﬁes\ntransport mechanisms for creating and managing annota-\ntions that are consistent with Web Architecture and REST\nbest practices. This involves casting each session as an an-\nnotation container , a form of Linked Data Platform (LDP)\ncontainer7with additional constraints derived from the\nWeb Annotation data model. New annotations are posted\nto the annotation container, where they are associated with\nthe session using ldp:contains relationships.\nMELD extends the Web Annotation model by track-\ning the state of each annotation associated with each per-\nformer, in order to support the dynamic, real-time nature of\nthe distributed annotation activity. Annotations are created\nin arawstate. Upon the occurrence of certain events (e.g.,\nuser interaction), clients may optionally effect a processed\nannotation state (via HTTP PATCH), signifying that the\nannotation has been handled and is no longer of relevance\nwithin the session context. This approach is preferable to\nsimply deleting the handled annotation (e.g., via HTTP\nDELETE), as it may remain relevant in external contexts\n– for instance for post-session review by the performers,\nor by other interested observers.\n4.2 Rendering and Interaction Clients\nA JavaScript web client is responsible for rendering MEI\nscore parts to the user. The client dynamically augments\nthis display with currently relevant annotations; handles\nuser interactions; and communicates interaction outcomes\nusing the MELD web services (Section 4.1).\nThe procedure is illustrated in Figure 2. The client pro-\ncesses a JSON-LD [19] representation of the graph asso-\nciated with the session resource, framed [18] to include\nonly relevant annotations by ﬁltering on audience and state\n(Sections 3.1 and 4.1.2). It retrieves the MEI resource as-\nsociated with the session (HTTP GET), and renders the en-\ncoded score using Verovio [15], a tool that produces SVG\nengravings of MEI-encoded music notation. Crucially,\nVerovio retains the MEI hierarchy and element identiﬁers\ninto the produced SVG output, supporting the addressing\nof graphical score elements for visual markup and dynamic\ninteraction through the web browser.\nEach annotation is mapped to a corresponding handler,\nwhich may be customised to a speciﬁc motivation and use-\ncase using CSS styling and drop-in JavaScript functions.\nUser-interaction outcomes trigger AJAX calls using the\nweb services to POST a new annotation, or to PATCH an\nannotation’s state from rawtoprocessed .\nEach client continuously polls the session resource (us-\ning HTTP GET), maintaining the annotated score at its lat-\nest state in near real-time. Simple resource hashes (HTTP\nentity tags) and atomic server-side ﬁle writing reduce net-\nwork trafﬁc and address race conditions inherent in dis-\ntributed real-time interaction: the former by supplying\n6https://www.w3.org/TR/annotation-protocol/\n7https://www.w3.org/TR/ldp-primer/ . LDP provides\nread-write access to RDF datasets via RESTful HTTP services.224 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 2 . MELD architecture: performance session and annotation web services; MELD client; and rendering and interac-\ntion handlers. A tight polling loop, employing entity tagging techniques to reduce network load and address concurrency\nconcerns, distributes and maintains annotation state between performers in near-real-time.\nlightweight HTTP 304 (“Not Modiﬁed”) responses that ex-\nclude the resource body when a resource has not changed\nfrom its state last seen by the client; and the latter by the\nserver rejecting changes (via HTTP 412 – “Precondition\nFailed”) upon an entity tag mismatch, prompting the client\nto GET the latest version of the session resource before\nreattempting its modiﬁcation.\n5. SCENARIO\nTo validate the feasibility of our proposed approach, we\nhave produced an implementation of MELD to support\na simple scenario where musical performers collaborate\nwithin a semi-structured performance environment.\n5.1 Motivation\nThe selected use case is a performance – such as a jam ses-\nsion – where collaborating musicians make ﬂuid, ad-hoc\ndecisions about song repetitions and transitions, rather than\nadhering to a pre-determined set list. Musicians may add\ndirections to change dynamic or stylistic elements of the\nperformance, e.g., to reference a particular prior record-\ning they wished the group to emulate in style or interpreta-\ntions; or they may incorporate structural directions, e.g., to\nrepeat a chorus or a verse, or to move to a bridge section.\nSuch directions transcend the symbolic representation of\nthe music being performed, and may draw upon signiﬁcant\ncontextual information external to the performance itself,\nfor instance to adopt the style of a certain artist, or to tran-\nsition to another song by this artist.\n5.2 Implementation\nA session, corresponding to the performance of a given\nsong, is represented as an LDP container. It contains an-\nnotations, and connects participating performers to instru-\nmental part roles via qualiﬁed associations (Figure 1). The\ncorresponding MEI staff deﬁnition elements are used to\nﬁlter the music structure in order to display only the rel-\nevant instrumental parts to each performer on a personal\ntouchscreen (Figure 4). Only annotations pertinent to their\nperformance are displayed, using the Web Annotation au-\ndience property as a ﬁltering mechanism (Section 3.1).\nFigure 3 . The MELD client displays the annotated digital\nscore. A modal action pane enables users to generate anno-\ntations enacting jumps within a piece, or queueing actions\ndetermining the next piece to be performed.\nRendering and interaction handlers operate on annota-\ntions that enable performers to collaboratively change the\nperformance sequence within and between musical scores.\nAnnotations are generated by using the touch screen to in-\nteract with a modal action pane situated below the musical\nscore (Figure 3).\nNavigational changes of sequence within a score – e.g.,\na jump to the bridge section – are requested by specifying\na jump source (highlighted on the score in red) and desti-\nnation (highlighted in green). The former is speciﬁed by\ntapping on a measure of the score. The action pane then\ndisplays a list of musical sections representing potential\ndestinations. This list is automatically retrieved from the\nLinked Data, by traversal of the segments on the segment\nline associated with the current session’s score. When a\nselection is made, an annotation is POSTed to the session,\nspecifying an annotation target (the fragment URI of the\njump source measure); an annotation body (the URI of the\nselected destination); and motivation:jumpTo (a cus-\ntom specialisation of Web Annotation’s oa:linking ) as\nthe motivation. All performer clients retrieve the annota-\ntion upon the next polling cycle.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 225Figure 4 . MEI staff deﬁnition element identiﬁers are associated with each performer in a session resource (Figure 1),\nenabling ﬁltering according to role. Here, an annotation targeting a segment embodied as myScore.mei#msr0823\nretrieves the corresponding notes for Lead or Bass, according to the performer’s part-role association within the session.\nUpon tapping on the red source measure, the render-\ning and interaction handler ﬂips to the score page contain-\ning the destination measure. The highlights then fade, and\nthe performer’s copy of the annotation is PATCHed as pro-\ncessed to avoid rerendering of stale information.8\nAnnotations representing queueing instructions for nav-\nigation between songs may target individual source mea-\nsures as described above, or the URI of the current score (in\nwhich case the rendering and interaction handler creates a\n“next piece” button in place of the “next page” button on\nthe last page of the current piece). Songs to transition to\ncan be selected according to a criteria such as “More songs\nby this artist”. These parameterise SPARQL queries9,\nin this case with the URI of the artist from the current\nscore’s MEI responsibility statement. The SPARQL query\nretrieves a list of songs by this artist from DBPedia10[1],\nmatched against a cache of available MEI resources stored\non a local triplestore (Linked Data database). Upon se-\nlecting a song, the annotator’s client requests the creation\nof a new session associated with the corresponding MEI\nresource. It then posts an annotation to the current ses-\nsion, instructing all performers’ clients to join the next ses-\nsion when an interaction event on the annotation target (the\njump source) is handled.\n6. CONCLUSIONS AND FUTURE WORK\nWe have presented the MELD framework and architec-\nture applying musical structure as a semantic spine for\nreal-time annotation of digital music score. RESTful web\nservices manage the retrieval and distribution of annota-\ntions created by user interactions in a performance session.\nAnnotations address MEI-encoded score elements using\nLinked Data, enabling ﬂexible reuse, repurposing, and in-\nterlinking of the generated information in external con-\ntexts. The framework and associated implementation ar-\nchitecture comprise separable music-generic and domain-\nspeciﬁc semantic structures, and modular rendering and in-\n8Measures are deﬁned above staves within the MEI hierarchy, mean-\ning that the notes contained in the targeted measures, and the pages they\nappear on, likely differ between performers, according to their instrumen-\ntal part associations\n9SPARQL is a query language for Linked Data resources, analogous\nto SQL queries against relational databases, but capable of retrieving data\nspanning local and external data sets\n10DBPedia publishes Wikipedia information as structured Linked Datateraction handlers, allowing components to target different\nmusic domains and use cases. We have validated the feasi-\nbility of the proposed framework through a MELD imple-\nmentation supporting multi-way communication between\nmusicians collaboratively changing the performance se-\nquence within and between pieces in a group performance.\nIn future work, we will focus on extending the capa-\nbilities of the described framework to incorporate annota-\ntions enacting modiﬁcations of the MEI structure, for in-\nstance requesting changes in dynamics. Such modiﬁca-\ntions are readily incorporated within the client polling cy-\ncle (Section 4.2), given the speedy rendering performance\nof Verovio and the relation of the polled session resource\n(viamo:performance of) to the MEI notation being\nrendered. However, session management complexities in-\ncluding MEI resource duplication, versioning, and the cap-\nture of provenance information will need to be accomo-\ndated, and potential licensing issues carefully considered.\nWhile physical interactions with musical score are com-\nmonplace in music performance – consider the necessity of\npaging through paper parts – care must be taken in ongo-\ning interface development to minimise additional cognitive\nload upon the performer. We have developed an alterna-\ntive, multimodal interaction mechanism by integrating the\nMELD framework with a technology supporting real-time\ntriggering of HTTP actions in response to speciﬁed pat-\nterns matched to an audio or MIDI stream. This variation\nof MELD has been successfully applied to drive a gamiﬁed\ncomposition for disklavier and electronics [10], demon-\nstrating a more complex performance interaction than the\nsimple scenario used to explain the MELD framework in\nthis paper. Other interaction paradigms are imaginable, for\ninstance by means of foot pedals, or voice commands.\nFinally, we are exploring the application of MELD in\nnon-performance-related use cases. MELD annotations\nmay be speciﬁed to target MEI and other digital media,\nincluding audio, images, and textual commentary. Thus in-\nterlinked, such media resources, annotated to target musi-\ncally meaningful sections, may be used to illustrate schol-\narly musicological arguments. Together with the work pre-\nsented here, these applications demonstrate the utility and\nﬂexibility of adopting a semantic framework anchored in\naddressable musical structure to express, retrieve, and dis-\ntribute information in a variety of contexts and use cases.226 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20177. ACKNOWLEDGEMENTS\nThis work is undertaken as part of the Fusing Audio and\nSemantic Technologies for Intelligent Music Production\nand Consumption project, funded by the UK Engineering\nand Physical Sciences Research Council under grant num-\nber EP/L019981/1, a collaboration between Queen Mary\nUniversity of London, the University of Nottingham, and\nthe University of Oxford.\nWe gratefully acknowledge the support of our col-\nleagues. Particularly, we thank David Lewis for guidance\nand insight during the development of the MELD frame-\nwork, and his collaboration in extending our approach to\napplications in musicological scholarship; Graham Klyne\nfor consultation regarding PROV-O mappings; and Leia\nYeomans for her valuable assistance in realising the graph\nlayout for Figure 1 of this paper.\n8. REFERENCES\n[1] Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sren\nAuer, Christian Becker, Richard Cyganiak, and Sebas-\ntian Hellmann. DBpedia - a crystallization point for\nthe web of data. Web Semantics: Science, Services and\nAgents on the World Wide Web , 7(3):154 – 165, 2009.\nThe Web of Data.\n[2] David C. De Roure, Don G. Cruickshank, Danius T.\nMichaelides, Kevin R. Page, and Mark J. Weal. On hy-\nperstructure and musical structure. In Proceedings of\nthe thirteenth ACM conference on Hypertext and hy-\npermedia , pages 95–104. ACM, 2002.\n[3] Ben Fields, Kevin Page, David De Roure, and Tim\nCrawford. The segment ontology: Bridging music-\ngeneric and domain-speciﬁc. In Multimedia and Expo\n(ICME), 2011 IEEE International Conference on ,\npages 1–6. IEEE, 2011.\n[4] Martin Gasser, Andreas Arzt, Thassilo Gadermaier,\nMaarten Grachten, and Gerhard Widmer. Classical mu-\nsic on the web–user interfaces and data representations.\nInProc. 16th International Society for Music Informa-\ntion Retrieval Conference , 2015.\n[5] Georg Hajdu. Real-time composition and notation in\nnetwork music environments. In International Com-\nputer Music Conference , 2008.\n[6] Andrew Hankinson, Perry Roland, and Ichiro Fuji-\nnaga. The Music Encoding Initiative as a Document-\nEncoding Framework. In Proc. 12th International So-\nciety for Music Information Retrieval Conference ,\npages 293–298, 2011.\n[7] David J. Hargreaves, Raymond MacDonald, and\nDorothy Miell. How do people communicate using mu-\nsic? In Musical communication , pages 1–25. Oxford\nUniversity Press Oxford, 2005.[8] Kurt Jacobson, Simon Dixon, and Mark Sandler.\nLinkedbrainz: Providing the musicbrainz next genera-\ntion schema as linked data. In Late-breaking demo ses-\nsion at the 11th International Society for Music Infor-\nmation Retrieval Conference , 2010.\n[9] Kurt Jacobson, Simon Dixon, and Mark Sandler. An\nontology for abstract, hierarchical music representa-\ntion. In Late-breaking demo session at the 16th Inter-\nnational Society for Music Information Retrieval Con-\nference , 2015.\n[10] Maria Kallionp ¨a¨a, Chris Greenhalgh, Adrian Hazzard,\nDavid M. Weigl, Kevin R. Page, and Steve Benford.\nComposing and realising a game-like performance for\ndisklavier and electronics. In New Interfaces for Musi-\ncal Expression (NIME) , 2017.\n[11] Albert Mero ˜no-Pe ˜nuela and Rinke Hoekstra. The song\nremains the same: Lossless conversion and streaming\nof MIDI to RDF and back. In International Semantic\nWeb Conference , pages 194–199. Springer, 2016.\n[12] Kevin R. Page, Sean Bechhofer, Georgy Fazekas,\nDavid M. Weigl, and Thomas Wilmering. Realising a\nlayered digital library: Exploration and analysis of the\nlive music archive through linked data. In Joint Con-\nference on Digital Libraries , 2017.\n[13] Kevin R. Page, Benjamin Fields, Bart J. Nagel, Gianni\nO’Neill, David C. De Roure, and Tim Crawford. Se-\nmantics for music analysis through linked data: How\ncountry is my country? In e-Science (e-Science), 2010\nIEEE Sixth International Conference on , pages 41–48.\nIEEE, 2010.\n[14] Laurent Pugin, Johannes Kepper, Perry Roland, Maja\nHartwig, and Andrew Hankinson. Separating Presen-\ntation and Content in MEI. In Proc. 13th International\nSociety for Music Information Retrieval Conference ,\npages 505–510, 2012.\n[15] Laurent Pugin, Rodolfo Zitellini, and Perry Roland.\nVerovio: A library for engraving MEI music notation\ninto SVG. In Proc. 15th International Society for Mu-\nsic Information Retrieval Conference , pages 107–112,\n2014.\n[16] Fazilatur Rahman and Jawed Siddiqi. Semantic annota-\ntion of digital music. Journal of Computer and System\nSciences , 78(4):1219 – 1231, 2012.\n[17] Yves Raimond, Samer A Abdallah, Mark B Sandler,\nand Frederick Giasson. The music ontology. In 8th\nInternational Conference on Music Information Re-\ntrieval , pages 417–422, 2007.\n[18] Manu Sporny, Gregg Kellogg, Dave Longley, and\nMarkus Lanthaler. JSON-LD framing 1.1: An\napplication programming interface for the JSON-\nLD syntax. Community group draft report, W3C,\n2017. http://json-ld.org/spec/latest/\njson-ld-framing/ .Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 227[19] Manu Sporny, Dave Longley, Gregg Kel-\nlogg, and Markus Lanthaler. JSON-LD 1.0: A\nJSON-based serialization for linked data. Rec-\nommendation, W3C, January 2014. http:\n//www.w3.org/TR/json-ld/ .\n[20] Rapha ¨el Troncy, Lynda Hardman, Jacco Van Ossen-\nbruggen, and Michael Hausenblas. Identifying spatial\nand temporal media fragments on the web. In W3C\nVideo on the Web Workshop , pages 4–9, 2007.\n[21] Raffaele Viglianti. The music addressability API: A\ndraft speciﬁcation for addressing portions of music no-\ntation on the web. In Proceedings of the 3rd Interna-\ntional Workshop on Digital Libraries for Musicology ,\nDLfM 2016, pages 57–60, New York, NY , USA, 2016.\nACM.\n[22] David M. Weigl, David Lewis, Tim Crawford, Ian\nKnopke, and Kevin R. Page. On providing semantic\nalignment and uniﬁed access to music-library meta-\ndata. International Journal on Digital Libraries . Ac-\ncepted for publication: Special issue on Digital Li-\nbraries for Musicology.\n[23] Gerhard E Winkler. The realtime-score: A missing-\nlink in computer-music performance. Sound and Music\nComputing , 4, 2004.\n[24] Lonce Wyse and Jude Yew. A real-time score for col-\nlaborative just-in-time composition. Organised Sound ,\n19(3):260–267, 2014.228 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Optical Music Recognition with Convolutional Sequence-to-Sequence Models.",
        "author": [
            "Eelco van der Wel",
            "Karen Ullrich"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415664",
        "url": "https://doi.org/10.5281/zenodo.1415664",
        "ee": "https://zenodo.org/records/1415664/files/WelU17.pdf",
        "abstract": "Optical Music Recognition (OMR) is an important tech- nology within Music Information Retrieval. Deep learn- ing models show promising results on OMR tasks, but symbol-level annotated data sets of sufficient size to train such models are not available and difficult to develop. We present a deep learning architecture called a Convolu- tional Sequence-to-Sequence model to both move towards an end-to-end trainable OMR pipeline, and apply a learn- ing process that trains on full sentences of sheet music in- stead of individually labeled symbols. The model is trained and evaluated on a human generated data set, with vari- ous image augmentations based on real-world scenarios. This data set is the first publicly available set in OMR re- search with sufficient size to train and evaluate deep learn- ing models. With the introduced augmentations a pitch recognition accuracy of 81% and a duration accuracy of 94% is achieved, resulting in a note level accuracy of 80%. Finally, the model is compared to commercially available",
        "zenodo_id": 1415664,
        "dblp_key": "conf/ismir/WelU17",
        "keywords": [
            "Optical Music Recognition",
            "Music Information Retrieval",
            "Deep learning models",
            "symbol-level annotated data",
            "end-to-end trainable pipeline",
            "learned process",
            "sheet music",
            "image augmentations",
            "pitch recognition accuracy",
            "duration accuracy"
        ],
        "content": "OPTICAL MUSIC RECOGNITION WITH CONVOLUTIONAL\nSEQUENCE-TO-SEQUENCE MODELS\nEelco van der Wel\nUniversity of Amsterdam\neelcovdw@gmail.comKaren Ullrich\nUniversity of Amsterdam\nkaren.ullrich@uva.nl\nABSTRACT\nOptical Music Recognition (OMR) is an important tech-\nnology within Music Information Retrieval. Deep learn-\ning models show promising results on OMR tasks, but\nsymbol-level annotated data sets of sufﬁcient size to train\nsuch models are not available and difﬁcult to develop.\nWe present a deep learning architecture called a Convolu-\ntional Sequence-to-Sequence model to both move towards\nan end-to-end trainable OMR pipeline, and apply a learn-\ning process that trains on full sentences of sheet music in-\nstead of individually labeled symbols. The model is trained\nand evaluated on a human generated data set, with vari-\nous image augmentations based on real-world scenarios.\nThis data set is the ﬁrst publicly available set in OMR re-\nsearch with sufﬁcient size to train and evaluate deep learn-\ning models. With the introduced augmentations a pitch\nrecognition accuracy of 81% and a duration accuracy of\n94% is achieved, resulting in a note level accuracy of 80%.\nFinally, the model is compared to commercially available\nmethods, showing a large improvements over these appli-\ncations.\n1. INTRODUCTION\nOptical Music Recognition (OMR) is an application of\nrecognition algorithms to musical scores, to encode the\nmusical content to some kind of digital format. In mod-\nern Music Information Retrieval (MIR), these applications\nare of great importance. The digitization of sheet music\nlibraries is necessary ﬁrst step in various data-driven meth-\nods of musical analysis, search engines, or other applica-\ntions where digital formats are required.\nOMR is an active area of research in MIR, and a classi-\ncally hard problem. OMR systems need to deal with a large\nrange of challenges such as low quality scans, ambiguous\nnotation, long range dependencies, large variations in mu-\nsical font, and handwritten notation. Multiple commercial\napplications are available, each with their own strengths\nand weaknesses [3], but the accuracy of these products is\nc\rEelco van der Wel, Karen Ullrich. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Eelco van der Wel, Karen Ullrich. “Optical Music Recog-\nnition with Convolutional Sequence-to-Sequence Models”, 18th Inter-\nnational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.often too low to use without human supervision. An imple-\nmentation that deals with these challenges in a satisfactory\nway has yet to be developed.\nA traditional OMR system typically consists of multi-\nple parts: score pre-processing, staff line identiﬁcation and\nremoval, musical object location, object classiﬁcation and\nscore reconstruction [15]. Each of these individual parts\nhas its own difﬁculties, resulting in OMR systems with low\nconﬁdence. More recently, there has been a trend towards\nless segmented systems involving machine learning meth-\nods, such as OMR without stafﬂine removal [13] or with-\nout symbol segmentation [16]. However, a major difﬁculty\nof these algorithms is the need for large amounts of train-\ning data. Typically, scores need to be annotated on musi-\ncal symbol level to train such machine learning pipelines,\nbut large corpora of sufﬁciently diverse symbol-annotated\nscores are difﬁcult and expensive to produce [15].\nIn this study, we propose a novel deep learning archi-\ntecture to both move towards an end-to-end trainable OMR\npipeline, and greatly reduce the data requirements for train-\ning. This is achieved by using two common deep learning\narchitectures: Convolutional Neural Networks (CNN) and\nRecurrent Neural Networks (RNN). Convolutional archi-\ntectures have been a popular choice of algorithm in various\nMIR related tasks, due to the ability to learn local struc-\ntures in images, and combining them to useful features. In\nour method, we use a CNN to learn a feature representation\nof the input scores. Continuing, a Sequence-to-Sequence\nmodel [5, 18] is used, which is a stack of two RNN’s used\ncommonly in machine translation tasks. This model di-\nrectly produces a digital representation of the score from\nthe learned representation by the CNN. The combination of\nthese two architectures is called a Convolutional Sequence-\nto-Sequence model. By using a Sequence-to-Sequence ar-\nchitecture, we cast the problem of OMR as a translation\nproblem. Instead of training on individual segmented sym-\nbols without context, full lines of sheet music are translated\nsimultaneously. This approach has two major advantages;\nFirstly, by training the algorithm on full lines of sheet mu-\nsic, there is no need for symbol level annotated training\ndata. This means that in principle any corpus of sheet mu-\nsic with corresponding digital notation could be used for\ntraining, opening up many new possibilities for data-driven\nOMR systems. Secondly, because in the proposed model\neach of the classically segmented OMR steps is done by\na single algorithm, the model can use a large amount of\ncontextual information to solve ambiguity and long-range731dependency problems.\nTo train the proposed model, a large corpus of mono-\nphonic sheet music is generated from a MusicXML dataset\nas described in Section 3. Additionally, in Section 3.2 vari-\nous types of image augmentations based on real-world sce-\nnarios are proposed to enhance the models ﬂexibility to\ndifferent kinds of fonts and varying score quality. Finally\nin Section 5, the results of the method are discussed on\nboth clean and augmented data, and the weaknesses of the\nmodel are examined.\n2. RELATED WORK\nA starting point for any OMR research is the overview pa-\nper by Rebelo et al. [15], which contains a complete in-\ntroduction to OMR systems and a description of the cur-\nrent state of the ﬁeld. The paper describes four main\nstages that are necessary for any OMR pipeline: Image\npre-processing, musical symbol recognition, musical in-\nformation reconstruction and construction of musical no-\ntation. The second component, as the name suggests, is\nwhere the main recognition work is done. Detecting and\nremoving staff lines, segmenting individual symbols, and\nclassifying symbols. Systems where steps are conducted\nby different methods we call segmented systems.\nNot all methods follow this model, recent data-driven\napproaches suggest merging or omitting some of these seg-\nmented steps. An example of this is an approach suggested\nby Pugin et al. [12, 13], which applies Hidden Markov\nModels (HMM) to the recognition stage, without perform-\ning staff line removal. Shi et al. [16] incorporate a deep\nlearning approach with Connectionist Temporal Classiﬁ-\ncation function [6] as decoding mechanism. They pose a\nsimilar idea to the method proposed in this research, with a\ndifference in the encoder mechanism. Instead of using both\na CNN and RNN as encoder, only a CNN is used. This\nis less computationally expensive, but the additional RNN\nin the Sequence-to-sequence model can make the method\nproposed in this research more context aware.\nSymbol classiﬁcation involving neural networks has been\nresearched by several authors [14, 19]. Convolutional ar-\nchitectures have been used for different OMR sub-tasks,\nsuch as staff-line detection [4] or symbol recognition [11].\nIn a different paper, Rebelo et al. [14] research the use\nof Deformable Templates [8] with various classiﬁers to\nmake symbol recognition invariant to changes in musical\nfont. This method is very similar to the Elastic Transfor-\nmations [17] used in this research. However, we decide to\nuse Elastic Transformations for ease of use and application\nspeed.\n3. DATASET\nThe dataset used in this research is compiled from\nmonophonic MusicXML scores from the MuseScore\nsheet music archive [1]. The archive is made up of\nuser-generated scores, and is very diverse in both content\nand purpose. As a result, the dataset contains a large\nvariation in type of music, key signature, time signature,clef, and notation style.\nTo generate the dataset, each score is checked for mono-\nphonicity, and dynamics, expressions, chord symbols,\nand textual elements are removed. This process produces\na dataset of about 17 thousand MusicXML scores. For\ntraining and evaluation, these scores are split into three\ndifferent subsets. 60% is used for training, 15% for\nvalidation and 25% for the evaluation of the models. A\nspeciﬁcation to reproduce the data set is publicly available\nonline.1\n3.1 Preprocessing\nFrom the corpus of monophonic MusicXML ﬁles, a dataset\nof images of score fragments and corresponding note an-\nnotations is created. Each MusicXML score is split into\nfragments of up to four bars, with a two bar overlap be-\ntween adjacent fragments. The fragments are converted to\nsheet music using MuseScore [1], each image containing a\nsingle staff line. The corresponding labels are represented\nwith a pitch and duration vector, containing all information\nabout the notes and rests within the same four bars. Each\nmusical symbol is represented with two values: a pitch,\nand a duration. Pitch values are speciﬁed by a MIDI pitch,\nand durations by quarterlength. In case of a rest, the pitch\nis a special rest indicator, which we indicate with r. The\npossible duration classes contain only the durations that\ncan be speciﬁed by a single notehead. Notes with dura-\ntions that require multiple noteheads are split into multiple\nnotes. The ﬁrst note will contain the pitch, and pitches\nof subsequent tied notes are replaced with a tie indicator,\nwhich we indicate with t\nAs an example, a quarter rest followed by a note\nwith MIDI pitch 60 and a complex duration of a\ntied quarter note and a sixteenth note is notated as\n((r;1);(60;1);(t;0:25)). Applying this method to the full\nscore fragments produces the pitch and duration vector,\nand is a suitable representation for the model. A maxi-\nmum of 48 events per fragment is used to put a limit on\nthe sequence length the model has to decode. Finally, at\nthe end of each pitch and duration vector an extra event is\nadded to indicate the sequence has ended. This indicator is\nimplemented as a rest with duration of zero quarter notes.\nEach generated image is padded to the same width and\nheight, and images containing notes with more than ﬁve\nledger lines are discarded. These notes are extreme out-\nliers and do not occur in normal notation. The resulting\nfragments have a dimension of 2261\u0002400pixels.\n3.2 Image Augmentation\nThe computer generated score fragments contain no noise\nor variation in musical symbols. To make the proposed\nmodel robust to lower quality inputs and different kinds\nof musical fonts we propose four different augmentations,\neach simulating a real world source of input noise. Ad-\nditionally, for each augmentation, we choose two separate\n1https://github.com/eelcovdw/\nmono-musicxml-dataset732 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 1 . An example of each of the used image aug-\nmentations from the evaluation dataset. From top to bot-\ntom: No Augmentations, Additive White Gaussian Noise\n(AWGN), Additive Perlin Noise (APN), Small scale Elas-\ntic Transformations (ET small), Large scale Elastic Trans-\nformations (ET large), and all combined augmentations.\nsettings. For the augmented training data, the parameters\nare chosen such that the input sheet music is greatly de-\nformed but still readable. For the augmented evaluation\nset, parameters are chosen such that they resemble real-\nworld sheet music, with less deformation than the train-\ning data. The larger amount of deformation in training\nwill force our model to learn to recognize musical symbol\nin any situation, and should improve the accuracy of our\nmodel on both non-augmented and augmented evaluation\ndata.\nA popular choice of augmentation is Additive White\nGaussian Noise (AWGN). This augmentation introduces a\nnormally distributed random deviation in pixel intensities,\nto mimic noise introduced by low quality scans or photos.\nThis noise has a mean \u0016, which is chosen to be the same as\nthe mean pixel intensity of the full dataset. The standard\ndeviation\u001bis different between our training and evalua-\ntion set. In the training set, the \u001bof pixel intensities in our\nnon-augmented data set is used. The evaluation set has a \u001b\nof half that value.\nThe second type of noise augmentation used is Additive\nPerlin Noise [10]. Perlin noise is a procedurally generated\ngradient noise, that generates lighter and darker areas in\nthe image at larger scales than AWGN. This effect mim-\nics quality differences in parts of the score. Some sym-\nbols might be faded and parts of staff lines less visible, and\ndark areas in the image are created. The mean size of gen-\nerated clouds is controlled by a frequency parameter. For\neach augmented score, this frequency is chosen to be a ran-\ndom value between the size of one note head and the mean\nwidth of a full bar, to generate noise structures at different\nscales. The maximum intensity of the noise in our training\nset is chosen to be a strength of 0.8. The evaluation set uses\na maximum intensity of half this value.The ﬁnal two augmentations are achieved with Elastic\nTransformations (ET) [17], which apply a smoothed ﬁeld\nof local random afﬁne transformations, resulting in wave-\nlike displacements in the augmented image. An advantage\nof using this augmentation is that it applies a large range\nof possible afﬁne and geometric transformations to each\nimage, such as rotation, skewing, squeezing and stretch-\ning. This both enhances the diversity of the augmented\ndata and alleviates the need to use manually deﬁned geo-\nmetric transformations.\nTwo parameters are used to control an elastic transfor-\nmation: a strength factor \u001b, which reduces the strength of\nthe distortion if a larger value is used, and a smoothing\nfactor\u000b, which controls the scale of deformations. A very\nlarge\u000bwill apply a nearly linear translation to the image,\nwhile an\u000bof zero applies fully random displacements on\nindividual pixels.\nThe ﬁrst type of Elastic Transformation is applied on\nvery small scales, to change the characteristics of lines and\nsmaller symbols. Lines might appear to be drawn by pencil\nor pen, and the edges of symbols become less deﬁned. \u000b\nis chosen to be a random value between 2 and 8, with a \u001b\nof 0.5 for the training data, and a \u001bof 2 for the evaluation\ndata.\nThe second type of Elastic Transformation is applied on\na large scale to change the shape and orientation of musi-\ncal symbols. Barlines and note stems get skewed or bent,\nnote heads can be compressed or elongated, and many new\nshapes are introduced in the score. This transformation\nmimics the use of different musical fonts, or even hand-\nwritten notation. An \u000bbetween 2000 and 3000 is used,\nwith a\u001bof 40 for the training data, and 80 for the evalu-\nation data. To maintain straight and continuous stafﬂines,\nthe original algorithm is slightly adapted to reduce vertical\ntranslations of pixels by reducing the vertical component\nof transformations by 95%.\nIn Figure 1, an example of each of these four augmen-\ntation is shown, with the setting used for generating the\nevaluation data. The last example shows a combination of\nall four augmentations.\n4. METHOD\nWe introduce the Convolutional Sequence-to-Sequence\nmodel as applied to OMR tasks, translating lines of sheet-\nmusic to a sequence of (pitch, duration) pairs. Continuing,\nthe training and evaluation methods are deﬁned.\n4.1 Model\nWe deﬁne a Convolutional Sequence-to-Sequence network\nas a stack of three components. First, a CNN encodes the\ninput image windows to a sequence of vector representa-\ntions. Then, an encoder RNN encodes the vector sequence\nto a ﬁxed size representation, containing all information\nfrom the input score. Finally, a decoder RNN decodes\nthe ﬁxed size representation to a sequence of output labels.\nThe following section describes each component in detail.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 733Figure 2 . A diagram of the proposed Convolutional Sequence-to-Sequence model. On the left, a score fragment is pro-\ncessed by a CNN and Encoder RNN to a ﬁxed size representation. This representation is used by the decoder RNN to create\na sequence of (pitch, duration) pairs.\nNote that, while each component is described separately,\nthe model will be trained as a single algorithm.\nSliding window input . The image input of the algo-\nrithm is deﬁned as a sequence of image patches, gener-\nated by applying a sliding window over the original input\nscore. The implementation has two separate parameters:\nthe window width wand window stride s. By varying w,\nthe amount of information per window can be increased or\ndecreased.sdeﬁnes how much redundancy exists between\nadjacent windows. Increasing the value of wor decreasing\nthe value of sprovides the model with more information\nabout the score, but will raise the computational complex-\nity of the algorithm. Thus when determining the optimal\nparameters, a balance has to be struck between complexity\nand input coverage. As a rule of thumb, we use a wthat\nis approximately twice the width of a notehead, and an s\nof half the value of w. This will ensure that each musical\nobject is shown in full at least once in an input window.\nThis gives a wof 64 pixels with a sof 32.\nConvolutional neural network . To extract relevant\nfeatures from the image patches, each patch is fed into\na CNN. In this research, we keep the architecture of the\nCNN the same between different experiments, to ensure a\nfair comparison. First a max-pooling operation of 3\u00023is\napplied on the input window for dimensionality reduction.\nThen, a convolutional layer of 32 5\u00025kernels is applied,\nfollowed by a reluactivation and 2\u00022max-pooling opera-\ntion. These three layers are repeated, and a fully-connected\nlayer of 256 units with relu activation is applied, so each\ninput for the encoder will be a vector of size 256.\nSequence-to-Sequence network. After extracting a\nvector description of each image patch, the sequence of\nvectors is fed into a Sequence-to-Sequence network [5,18].\nThis architecture consists of two RNN’s. The ﬁrst RNN,\ntheencoder , encodes the full input sequence to a ﬁxed size\nrepresentation . The second RNN, the decoder , produces\na sequence of outputs from the encoded representation. In\nthe case of the OMR task, this sequence of outputs is the\nsequence of pitches and durations generated from the Mu-sicXML ﬁles. For both encoder and decoder, a single Long\nShort-Term Memory (LSTM) [7] layer is used with 256\nunits. To predict both the pitch and duration, the output\nof the decoder is split into two separate output layers with\nasoftmax activation and categorical cross-entropy loss.\nA diagram of the full model is shown in Figure 2, where\nfour input patches and output predictions are shown. On\nthe left side, A sliding window is applied to a 2 bar score\nfragment. Each image patch is sequentially fed into the\nsame CNN. This CNN is connected to the encoder net-\nwork, creating a ﬁxed size representation of the two in-\nput bars. The decoder uses this representation to produce\nthe output sequence of (pitch, duration) pairs. Note that\nthe second predicted pitch is an rpitch, representing a rest\nsymbol.\nUsing the described conﬁguration, the model has an in-\nput sequence length of 70 windows and an output sequence\nlength of 48 units. Shorter output sequences are padded to\nthis maximum length and the loss function is masked after\nlast element of the sequence. The number of pitch cate-\ngories is 108, and the number of duration categories is 48.\nIn total, the model contains approximately 1.67 million pa-\nrameters.\n4.2 Training\nSix separate models are trained, one on each of the pro-\nposed augmented data sets: No augmentations, AWGN,\nAPN, Small ET, large ET and all augmentations. Augmen-\ntations are applied during training, and will be different\neach time the network is presented with a training sam-\nple. All models are trained with a batch-size of 64 using\nthe ADAM optimizer [9], with an initial learning rate of\n8\u000310\u00004and a constant learning rate decay tuned so the rate\nis halved every ten epochs. Each model is trained to con-\nvergence, taking about 25 epochs on the non-augmented\ndataset. A single Nvidia Titan X Maxwell is used for train-\ning, which trains a model in approximately 30 hours.734 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20174.3 Evaluation Metrics\nOn the evaluation data, three different metrics are calcu-\nlated, similar to [3]:\n\u000fPitch accuracy, the proportion of correctly predicted\npitches.\n\u000fDuration accuracy, the proportion of correctly pre-\ndicted note durations.\n\u000fNote accuracy, the proportion of predicted events\nwhere both pitch and duration are correctly pre-\ndicted.\nThe accuracy is measured over all notes before the stop\nindicator, and the stop indicator is not included in the cal-\nculation of accuracy. The model is not given any a priori\nknowledge about how many notes are in the input frag-\nment, so a wrong number of notes could be predicted. In\ncase of a shorter predicted sequence, the missing notes\nare automatically marked as incorrect. If the predicted se-\nquence is longer than the ground truth, the additional pre-\ndicted notes are cut and only the notes within the length of\nthe ground truth are used. This method of measuring accu-\nracy is quite strict, as an insertion or omission of a note in\nthe middle of a sequence could mean subsequent notes are\nall marked as incorrect. This should be kept in mind when\nevaluating the results of the model, and perhaps more de-\nscriptive metrics could be applied in future work.\n5. RESULTS\n5.1 Model Evaluation\nThe six trained models are evaluated on both a clean eval-\nuation set, shown in Table 1, and augmented sets, shown in\nTable 2. The augmented evaluation sets are generated by\napplying the augmentations the model was trained on to\nthe full clean evaluation set, with the parameters described\nin Section 3.2.\nThe model trained on data with all augmentations is\ncompared against two commercially available methods in\nTable 3, similar to Shi et al. [16]. The comparison between\nthe different methods on the clean dataset gives a base-\nline performance on digital scores, while the comparison\non augmented data gives an indication of the difference of\nperformance on real-world sheet music.\nTraining\nAugmentationPitch\nAccuracyDuration\nAccuracyNote\nAccuracy\nNone 0.79 0.92 0.76\nAWGN 0.79 0.92 0.77\nAPN 0.82 0.91 0.79\nET - Small 0.78 0.91 0.76\nET - Large 0.79 0.94 0.78\nAll augmentations 0.81 0.94 0.80\nTable 1 . Measured accuracy on non-augmented scores.\nThe accuracy scores for augmentations with the highest\npositive impact are in bold.\n2https://www.capella-software.com/Training\nAugmentationPitch\nAccuracyDuration\nAccuracyNote\nAccuracy\nAWGN 0.79 0.90 0.75\nAPN 0.81 0.89 0.76\nET - Small 0.78 0.89 0.74\nET - Large 0.78 0.94 0.75\nAll augmentations 0.79 0.92 0.77\nTable 2 . Measured accuracies on scores with augmenta-\ntions. Each model trained on different augmented data is\nevaluated on an evaluation set with corresponding augmen-\ntations.\nModel Clean Augmented\nCapella Scan 820.53 0.14\nPhotoscore 830.61 0.09\nCS2S 0.80 0.77\nTable 3 . A comparison of accuracy between the proposed\nmodel (CS2S) and two popular commercially available\nmethods.\n5.2 Evaluation of Model Difﬁculties\nTo examine the difﬁculties the model has on different kinds\nof scores, three additional evaluations are performed on\ndifferent subsets of the evaluation data.\nFigure 3 . Top: The note-level accuracy for each number\nsharps/ﬂats in the key signature. Bottom: The note-level\naccuracy for the most common time signatures. The dotted\nlines indicate the mean accuracy of 0.80.\nFirst, an investigation into the impact of key signature\non the note level accuracy on the non-augmented evalua-\ntion data is conducted. Just like human performance, the\nadded complexity of many sharps or ﬂats in the key sig-\nnature of a fragment impacts the accuracy of the model.\nThe results of this experiment are displayed in Figure 3\n(top). At zero sharps or ﬂats, the reported accuracy is 0.86,\n3http://www.neuratron.com/Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 735Figure 4 . The mean note-level accuracy for each number\nof notes per fragment, with a conﬁdence interval at a 95%\nlevel ﬁt with a Gaussian Process.\nachieving 0.06 higher than the mean accuracy of 0.80.\nWith more than 4 sharps or ﬂats in the key signature the\nnote accuracy starts diminishing, down to a minimum of\n0.66 for key signatures with seven sharps or ﬂats.\nContinuing, the nine most common time signatures and\ntheir accuracies are examined. While the output notation\ndoes not encode any direct information about time signa-\nture, the model could use structural information imposed\nby the time signature on the score to aid in note recogni-\ntion. This evaluation will both look at if that is the case,\nand investigate which time signatures are potentially more\ndifﬁcult to transcribe. The results in Figure 3 (bottom) do\nnot show a signiﬁcant difference between the measured ac-\ncuracy of different time signatures. The complex time sig-\nnatures of 7=8and5=4both are slightly less accurate, but\nthis observation could be caused by a random deviation, or\nby features correlating with complex time signatures such\nas number of notes in a fragment.\nAs a ﬁnal evaluation, we look at the correlation between\nnumber of notes in a fragment and accuracy. The model\ncapacity and the representation between encoder and de-\ncoder are of a ﬁxed size, which forces the model to rep-\nresent more notes in the same space for fragments with a\nhigher note density. This higher density could cause a loss\nin accuracy. Figure 4 shows clear evidence that this is the\ncase; fragments containing more than 25 notes have a sig-\nniﬁcantly lower accuracy than the measured mean.\n6. DISCUSSION\nWe propose the Convolutional Sequence-to-Sequence\nmodel to deal with the difﬁculties OMR presents for learn-\ning systems. By using an end-to-end trainable sequential\nmodel, we completely move away from segmented sym-\nbol recognition, and perform the full OMR pipeline with a\nsingle algorithm. By incorporating Sequence-to-Sequence\nmodels into OMR, there are many new possibilities for\nobtaining development data. We view this aspect as the\nlargest advantage the proposed method has over segmented\nmodels, as the acquisition of quality training data can be alimiting factor. The proposed model shows that it is robust\nto noisy input, an important quality for any OMR model.\nAdditionally, the experiments show that it can deal with the\nlarge scale Elastic Transformations that essentially change\nthe musical font. In future research, this aspect could be\nexpanded to include handwritten notation.\nA weakness of the model is pitch classiﬁcation. Pool-\ning operations introduce a degree of translation invariance,\nwe hypothesize this invariance reduces the pitch recog-\nnition accuracy by discarding information about symbol\nposition. However, omitting pooling operations from the\nmodel would greatly reduce the dimensionality reduction\nperformed by the CNN. We propose incorporating a com-\nbination of convolutional layers and fully connected layers\nas a possible solution.\nFurthermore, on more complex scores the model per-\nforms signiﬁcantly worse. Both the number of sharps or\nﬂats in the key signature and the note density in the score\nfragment play a large role in the prediction accuracy. In fu-\nture work, these problems could be addressed in multiple\nways. A separate key signature recognition could be per-\nformed, and given as additional information to the model.\nThis would take away some of the long range computations\nthe key signature introduces and could improve the results\non more complex scores.\nThe difﬁculty of translating long sequences with\nSequence-to-Sequence models is a well studied problem\n[5, 18]. For longer sequences, the model needs to encode\nmore information in the same ﬁxed size representation, re-\nducing the amount of storage available per note. A pos-\nsible solution for this difﬁculty is proposed by Bahnadau\net al. [2]: they replace the ﬁxed size representation be-\ntween encoder and decoder with an attention mechanism, a\nmethod that essentially performs a search function between\nthe two networks. This mechanism has shown improve-\nments to Sequence-to-Sequence models in neural machine\ntranslation, and could be used in the proposed method to\nalleviate some of the problems introduced with long se-\nquences and long range dependencies.\nThe experiments performed in this research are exclu-\nsively on monophonic scores. The current representation\nof (pitch, duration) pairs does not allow for polyphonic\nnote sequences, and in order to apply the model to poly-\nphonic OMR tasks this representation needs to be adapted.\nA possible representation could be produced by using a\nmethod close like the MIDI-standard or piano roll repre-\nsentation.\nFinally, we propose that the Convolutional Sequence-\nto-Sequence model could be applied to tasks outside of\nOMR that translate a spatial sequential representation to a\nsequence of labels. Within MIR, tasks like Automatic Mu-\nsic Transcription can be considered as such a task, where a\nrepresentation of an audio signal is converted to a sequence\nof pitches and durations. Outside of MIR, tasks like video\ntagging or Optical Character Recognition are similar ex-\namples.736 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20177. ACKNOWLEDGEMENTS\nMany thanks to the reviewers for their valuable feedback,\nand to MuseScore for providing access to the material\navailable in their archive. This research has been partially\nfunded by Google.\n8. REFERENCES\n[1] Musescore. https://musescore.org/ .\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473 ,\n2014.\n[3] Donald Byrd and Megan Schindele. Prospects for im-\nproving omr with multiple recognizers. In ISMIR ,\npages 41–46, 2006.\n[4] Jorge Calvo-Zaragoza, Antonio Pertusa, and Jose\nOncina. Staff-line detection and removal using a con-\nvolutional neural network. Machine Vision and Appli-\ncations , pages 1–10, 2017.\n[5] Kyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning phrase rep-\nresentations using rnn encoder-decoder for statistical\nmachine translation. arXiv preprint arXiv:1406.1078 ,\n2014.\n[6] Alex Graves, Santiago Fern ´andez, Faustino Gomez,\nand J ¨urgen Schmidhuber. Connectionist temporal clas-\nsiﬁcation: labelling unsegmented sequence data with\nrecurrent neural networks. In Proceedings of the 23rd\ninternational conference on Machine learning , pages\n369–376. ACM, 2006.\n[7] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\n[8] Anil K Jain and Douglas Zongker. Representation and\nrecognition of handwritten digits using deformable\ntemplates. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 19(12):1386–1390, 1997.\n[9] Diederik Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv:1412.6980 , 2014.\n[10] Ken Perlin. Improving noise. In ACM Transactions on\nGraphics (TOG) , volume 21, pages 681–682. ACM,\n2002.\n[11] Roberto M Pinheiro Pereira, Caio EF Matos, Geraldo\nBraz Junior, Jo ˜ao DS de Almeida, and Anselmo C\nde Paiva. A deep approach for handwritten musi-\ncal symbols recognition. In Proceedings of the 22nd\nBrazilian Symposium on Multimedia and the Web ,\npages 191–194. ACM, 2016.[12] Laurent Pugin. Optical music recognitoin of early ty-\npographic prints using hidden markov models. In IS-\nMIR, pages 53–56, 2006.\n[13] Laurent Pugin, John Ashley Burgoyne, and Ichiro Fuji-\nnaga. Map adaptation to improve optical music recog-\nnition of early music documents using hidden markov\nmodels. In ISMIR , pages 513–516, 2007.\n[14] Ana Rebelo, G Capela, and Jaime S Cardoso. Opti-\ncal recognition of music symbols. International jour-\nnal on document analysis and recognition , 13(1):19–\n31, 2010.\n[15] Ana Rebelo, Ichiro Fujinaga, Filipe Paszkiewicz, An-\ndre RS Marcal, Carlos Guedes, and Jaime S Cardoso.\nOptical music recognition: state-of-the-art and open is-\nsues. International Journal of Multimedia Information\nRetrieval , 1(3):173–190, 2012.\n[16] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-\nend trainable neural network for image-based sequence\nrecognition and its application to scene text recogni-\ntion. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence , 2016.\n[17] Patrice Y Simard, David Steinkraus, John C Platt, et al.\nBest practices for convolutional neural networks ap-\nplied to visual document analysis. In ICDAR , volume 3,\npages 958–962. Citeseer, 2003.\n[18] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Se-\nquence to sequence learning with neural networks. In\nAdvances in neural information processing systems ,\npages 3104–3112, 2014.\n[19] Cuihong Wen, Ana Rebelo, Jing Zhang, and Jaime Car-\ndoso. A new optical music recognition system based on\ncombined neural network. Pattern Recognition Letters ,\n58:1–7, 2015.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 737"
    },
    {
        "title": "Automatic Drum Transcription Using the Student-Teacher Learning Paradigm with Unlabeled Music Data.",
        "author": [
            "Chih-Wei Wu",
            "Alexander Lerch 0001"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415904",
        "url": "https://doi.org/10.5281/zenodo.1415904",
        "ee": "https://zenodo.org/records/1415904/files/WuL17.pdf",
        "abstract": "Automatic drum transcription is a sub-task of automatic music transcription that converts drum-related audio events into musical notation. While noticeable progress has been made in the past by combining pattern recognition methods with audio signal processing techniques, the major limita- tion of many state-of-the-art systems still originates from the difficulty of obtaining a meaningful amount of anno- tated data to support the data-driven algorithms. In this work, we address the challenge of insufficiently labeled data by exploring the possibility of utilizing unlabeled mu- sic data from online resources. Specifically, a student neural network is trained using the labels generated from multiple teacher systems. The performance of the model is evalu- ated on a publicly available dataset. The results show the general viability of using unlabeled music data to improve the performance of drum transcription systems.",
        "zenodo_id": 1415904,
        "dblp_key": "conf/ismir/WuL17",
        "keywords": [
            "Automatic drum transcription",
            "sub-task of automatic music transcription",
            "audio events to musical notation",
            "pattern recognition methods",
            "audio signal processing techniques",
            "major limitation",
            "insufficiently labeled data",
            "unlabeled music data",
            "online resources",
            "student neural network"
        ],
        "content": "AUTOMATIC DRUM TRANSCRIPTION USING THE STUDENT-TEACHER\nLEARNING PARADIGM WITH UNLABELED MUSIC DATA\nChih-Wei Wu, Alexander Lerch\nGeorgia Institute of Technology, Center for Music Technology\nfcwu307, alexander.lerch g@gatech.edu\nABSTRACT\nAutomatic drum transcription is a sub-task of automatic\nmusic transcription that converts drum-related audio events\ninto musical notation. While noticeable progress has been\nmade in the past by combining pattern recognition methods\nwith audio signal processing techniques, the major limita-\ntion of many state-of-the-art systems still originates from\nthe difﬁculty of obtaining a meaningful amount of anno-\ntated data to support the data-driven algorithms. In this\nwork, we address the challenge of insufﬁciently labeled\ndata by exploring the possibility of utilizing unlabeled mu-\nsic data from online resources. Speciﬁcally, a student neural\nnetwork is trained using the labels generated from multiple\nteacher systems. The performance of the model is evalu-\nated on a publicly available dataset. The results show the\ngeneral viability of using unlabeled music data to improve\nthe performance of drum transcription systems.\n1. INTRODUCTION\nData availability, listed by Schedl et al. as one of the\nopen challenges in the ﬁeld of Music Information Retrieval\n(MIR) [21], is an important problem that concerns a large\nvariety of data-driven MIR systems. To create intelligent\nmusic (analysis) systems, music data with detailed anno-\ntations is crucial as training input for machine learning\nalgorithms. However, multiple constraints impede the avail-\nability of large datasets, including (i) the complexity and va-\nriety of music in terms of genres, instrumentation, tonality,\netc., (ii) the difﬁcult and time-consuming process of manu-\nally adding annotations which —- for most tasks — might\nalso depend on perception and thus require multiple anno-\ntators, and (iii) intellectual property laws, restricting the\ncompilation and sharing of music datasets. Many laudable\nefforts have been made to address (some of) these problems,\nleading to the release of new datasets or the extension of\nexisting datasets. Nevertheless, the majority of the com-\nmonly used datasets for various MIR tasks is still limited\nin different aspects, which can impact research focus. For\nexample, Benetos et al. pointed out that a large subset of\nc\rChih-Wei Wu, Alexander Lerch. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: Chih-Wei Wu, Alexander Lerch. “Automatic drum transcription\nusing the student-teacher learning paradigm with unlabeled music data”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.Automatic Music Transcription (AMT) approaches only\nperformed experiments on piano data for which the audio\naligned ground truth was easily obtained [1]. This empha-\nsis on piano may lead to models that are strongly biased\ntowards piano-like instruments and cannot be generalized\nto other melodic instruments.\nAutomatic Drum Transcription (ADT), a sub-task in\nAMT that involves the extraction of drum events from au-\ndio signals, is also conﬁned to the scope of the existing\nlabeled datasets. Wu observed [30] that most of the ADT\nrelated datasets focus on collecting recordings of single\ndrum hits [18, 24] and simple drum sequences without ac-\ncompaniment [5]. Although these datasets provide the es-\nsential ingredients for building basic ADT systems, they\ncannot properly represent the real-world scenario of drum\nsounds embedded in a continuous stream of polyphonic au-\ndio sources. Thus, they might fail in addressing real-world\nuse cases. The ENST drum dataset [8] partly compensates\nthese drawbacks by offering more realistic and complex\ndrum sequences with accompaniments, however, its size\nand diversity of music styles are still limited. Previous stud-\nies attempt to alleviate these issues through data augmen-\ntation [26, 30], but the inherent limitations of the datasets\ncontinue to impede the advancement of ADT systems.\nOne potential solution to addressing this challenge in\na scalable way without introducing the additional cost of\nmanual annotations is to explore the usefulness of the vast\ncollection of unlabeled music data; this can be formulated\nas aSemi-supervised Learning problem as deﬁned in the\nﬁeld of machine learning [3]. The general goal of this\ntype of problem is to ﬁnd the optimal solution given both\nlabeled and unlabeled examples, and it has been applied\nsuccessfully to different applications such as music genre\nclassiﬁcation [19], music genre tagging [13], and music\nemotion recognition [28].\nInspired by the above-mentioned approaches, this paper\naims to address the issue of data availability in ADT systems\nby harnessing the information from the unlabeled music\ndata. Speciﬁcally, this paper focuses on improving ADT\nperformance on polyphonic mixtures. The contributions of\nthis paper include: (i) new insights into the viability of using\nunlabeled music data in ADT tasks, (ii) a general scheme for\nintegrating unlabeled data into ADT and other MIR systems,\nand (iii) the demonstration of potential improvements of\nADT systems using the proposed method. The remainder\nof the paper is structured as follows: Sect. 2 provides an\noverview of ADT research and the student-teacher learning613paradigm. In Sect. 3, we introduce our approach; the results\nand discussion are presented in Sect. 4. Sect. 5 provides a\nsummary, conclusion, and directions of future work.\n2. RELATED WORK\nIn the broadest deﬁnition of ADT, it can be described as\nthe process of converting drum related audio events, such\nas drum onset times and playing techniques, into musical\nrepresentations such as a score or sheet music. To simplify\nthis task while still capturing the essence, most of the ex-\nisting systems mainly focus on detecting the onset times\nofHi-Hat (HH), Snare Drum (SD) and Bass Drum (BD).\nIn many of the early systems, which are summarized by\nFitzGerald and Paulus [6], the focus was on transcribing\nsignals containing only drum sounds.\nGillet and Richard propose to categorize automatic drum\ntranscription systems into three categories [9]: (i) segment\nand classify [7, 9], which follows the basic pattern recog-\nnition approach by segmenting the signals into individual\ninstances, and subsequently classifying each instance with\npre-trained classiﬁers, (ii) separate and detect [5, 20, 29],\nin which the signal is converted into separated activation\nfunctions that represent the activities of different drums,\nfollowed by a simple peak picking process to identify their\ncorresponding onset times, and (iii) match and adapt [31],\nwhich identiﬁes the drum events by template matching us-\ning a set of pre-trained drum templates and customized\ndistance measures; the templates are iteratively adapted\nthroughout the process. In addition to these three categories,\na language-model-based approach using Hidden Markov\nModels (HMM) [17] and a pattern-matching approach us-\ning bar information [23] have also been applied to ADT\ntasks in previous work.\nFollowing the recent success in deep learning [10], sev-\neral state-of-the-art ADT systems utilize Deep Neural Net-\nworks (DNNs). Speciﬁcally, Recurrent Neural Networks\n(RNNs), a DNN variant modeling the temporal dependency\nof the input using recurrently connected nodes, have been\nadopted for this task [22, 25, 26]. Although this method is\ncapable of learning complicated representations of drums\nfrom the audio signals, it is extremely demanding in terms\nof the required amount of training data and computing\npower. To reach their full potential, DNNs require large\namounts of training data; the sizes of currently available\ndatasets appear to be insufﬁcient, as exempliﬁed by the\nperformance degradation in polyphonic mixtures reported\nin several ADT systems [22, 26, 29]\nTo overcome the problem of possibly insufﬁcient input\ndata for data-hungry approaches such as DNNs, the idea of\nutilizing the unlabeled data seems very appealing. Recently,\nthe concept of the student-teacher learning paradigm has\nemerged as an interesting way of incorporating unlabeled\ndata in the training of DNNs. Originally proposed as a\nmodel compression method [2], the basic idea of student-\nteacher learning is to transfer the knowledge of a large\nteacher model into a small and concise student model with\nminimum performance loss; this process, referred by Hin-\nton et al. as ”knowledge distillation” [11], is achieved by\nTeacher Model(s) PredictUnlabeled Music DataSoft TargetsStudent ModelPredictTesting Music DataRegression ModelPeak PickingTranscriptionTrainingTestingStudent ModelTrainFigure 1 . The ﬂowchart of the proposed method\ntraining the student model with the soft targets generated\nfrom the teacher model. In other words, instead of learning\nfrom the hard targets (i.e., the ground truth), the student\nmodel indirectly acquires the knowledge by mimicking the\noutput from the teacher model. As demonstrated by Li et\nal. [16], this process can use labeled as well as unlabeled\ndata. Successful applications of this paradigm can be found\nin tasks such as speech recognition [27] and multilingual\nmodels [4], in which superior performances from the stu-\ndent model have also been reported.\n3. METHOD\n3.1 System Overview\nThe processing steps of the proposed method, as shown in\nFigure 1 , can be split into two phases, namely the training\nand testing phase. In the training phase, the unlabeled mu-\nsic data are passed through the teacher models in order to\ngenerate the soft targets. Speciﬁcally, these teacher models\nare ADT systems that will convert the audio signals into\ndrum-related activation functions (i.e., soft targets). The\nsame unlabeled music data and the generated soft targets\nwill then be used to train a student model, which is a re-\ngression model that minimizes the differences between its\noutput and the soft targets. In the testing phase, the trained\nstudent model predicts the drum activations of the test mu-\nsic data. Finally, a simple peak picking algorithm with an\nadaptive threshold will be used to identify the drum onset\ntimes from each activation function, producing the ﬁnal\ntranscription output. More elaborate descriptions of the\nteacher and student models can be found in the following\nsections.\n3.2 Teacher Model\nThe teacher model used in this paper is the drum transcrip-\ntion system presented by Wu and Lerch [29]. This NMF-\nbased ADT system is chosen for its simplicity, its lack of\nneed for substantial amounts of training data, as well as the\nadaptability in polyphonic mixtures; it extends the basic\nNMF model to Partially-Fixed Non-negative Matrix Fac-\ntorization (PFNMF) by assuming the co-existence of both\npercussive and harmonic components in the audio signals.\nMore speciﬁcally, the template matrix is split into a pre-\ndeﬁned part containing the drum templates which kept ﬁxed\nand not iteratively updated and a randomly initialized part614 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Template ExtractionTraining dataTesting dataInput PreparationPFNMFDrum Activation Functions𝐻\"TemplateAdaptation𝑊\"𝑋Figure 2 . The ﬂowchart of PFNMF [29]\nfor modeling the remaining harmonic components in the\nsignal. Formally, this can be expressed as\nX\u0019WDHD+WHHH; (1)\nwithXbeing am\u0002nmagnitude spectrogram matrix with\nmfrequency bins and nblocks,WDandWHrepresenting\nthe drum and harmonic dictionary matrices with a dimen-\nsionality of m\u0002rDandm\u0002rH, andHDandHHtheir\ncorresponding activation matrices with dimensionality of\nrD\u0002nandrH\u0002n, respectively. rDusually corresponds\nto the number of drums to detect (e.g., rD= 3 for the\ndetection of HH, BD, and SD), and rHis an user-deﬁned\nparameter that varies according to the complexity of the\ntarget signal.\nThe basic ﬂowchart of PFNMF is shown in Figure 2 . It\nﬁrstly decomposes the magnitude spectrogram of the poly-\nphonic mixtures with a ﬁxed pre-trained drum dictionary\nWDand a randomly initialized harmonic dictionary WH.\nOnce the signal is decomposed, the NMF based activation\nfunctionHD(r;:)of each individual drum can be extracted,\nin whichr=f1;2;3gis the instrument index that corre-\nsponds to HH, BD, and SD, respectively. These activation\nfunctions can be interpreted as the activity level of each in-\nstrument over time, and a sharp peak indicates the presence\nof a single drum hit.\nThe conversion of the resulting activation functions into\nthe soft targets takes another step of standard min-max\nscaling across the training data for each instrument; this\nprocess scales the soft targets to a numerical range between\n0 and 1 and ensures the compatibility between the soft\ntargets and the student model output (see Sect. 3.3). Finally,\nto introduce diversity into the soft targets, two PFNMF\nsystems are created by initializing the algorithm with two\ndifferent sets of drum dictionaries, forming an ensemble-\nlike scenario that could potentially lead to better student\nperformance.\n3.3 Student Model\nThe proposed student model is a fully connected, feed-\nforward DNN with three hidden layers. A neural network\nis a graphical model that comprises multiple layers of in-\nterconnected non-linear units (i.e., neurons). The basic\nformulation of a neuron can be expressed in Eq. (2)\nal\nk=g0\n@MX\nj=1Wjal\u00001\nj+bl\u00001\nj1\nA; (2)\nin whichais the activation of the neuron, Wis the weight\nmatrix,bis the bias matrix, lis the layer index, jis the\nindex of input neuron, and kis the index of output neuron;g()is usually a non-linear function such as a sigmoid ,tanh\norrelu. When multiple layers of neurons are stacked, the\nmodel creates a non-linear transformation from the input\nto the output, which allows the model to approximate any\narbitrary function with great ﬂexibility.\nThe architecture of the DNN in this paper is as follows:\nthe input layer contains 1025 neurons that correspond to\nthe size of the input representation. The ﬁrst hidden layer\ncomprises of 1025 neurons of tanh units with Batch Nor-\nmalization [12]. The second and third hidden layers have\n512and32neurons with reluunits, respectively. Finally,\nthe output layer consists of 3 neurons with sigmoid units\nthat represent the activities of three different drums (i.e.,\nHH, SD, and BD). The architecture and type of neurons are\nselected based on the results of smaller-scale preliminary\nexperiments, and the fully connected layers are chosen for\ntheir simplicity and generality. To solve the optimization\nproblem of learning the weights Win a DNN, a stochastic\ngradient descent based optimization method, Adam [14],\nis selected as the optimizer. The student neural network is\nconﬁgured as a regressor that minimizes the mean squared\nerror between its output and the soft targets. A mini-batch\nconsisting of 640instances is used for training, and the\nearly stopping technique is applied to stop the training pro-\ncess when the loss decrease is less than 10\u00006for three\nconsecutive epochs.\n3.4 Implementation\nThe input representation to both the teacher and student\nmodels is the magnitude spectrogram of the Short Time\nFourier Transform (STFT) computed using a block size of\n2048 and hop size of 512 samples with a Hann window\napplied to each block. Prior to the calculation of STFT,\nthe audio signals are down-mixed to mono and resampled\nto a sampling rate of 44.1 kHz. The resulting magnitude\nspectrogram is a m\u0002nmatrix, in which m= 1025 andn\nequals the number of blocks.\nFor PFNMF, the authors’ open source Matlab imple-\nmentation1is used in our experiments. Since both the\nunlabeled music data and the test data are polyphonic mix-\ntures, the harmonic rank rHfor the PFNMF is set to 50as\nsuggested [29]. To speed up the process, template adapta-\ntion is deactivated. The extraction of the pre-deﬁned (ﬁxed)\ndrum templates takes place on two publicly available drum\ndatasets, namely the SMT-DRUM dataset [5] and 200 drum\nmachines.2\nPreliminary experiments show that these two sets of\ntemplates exhibit capabilities of capturing different types\nof drum sounds, thus adding diversity to this learning\nparadigm. The construction of the drum dictionary involves\nthe concatenation of all the spectra and the extraction of the\nmedian spectrum for each individual instrument. It should\nbe noted that, since the ENST drum dataset is the main test\ndataset for evaluation, no single drum hits from ENST are\n1https://github.com/cwu307/NmfDrumToolbox Last accessed:\n2017/04/26\n2http://www.hexawe.net/mess/200.Drum.Machines Last accessed:\n2017/04/26Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 615Experiments Averaged F-measure\nRole Method # Training Data HH BD SD\nTeacher Baseline PFNMF (SMT) N/A 0.69 0.80 0.50\nTeacher Baseline PFNMF (200D) N/A 0.68 0.85 0.48\nBaseline PFNMF (SMT + 200D) N/A 0.69 0.83 0.48\nStudent Baseline Linear SGD Regressor 200 * 4 = 800 0.43 0.69 0.43\nStudent Proposed DNN 200 * 4 = 800 0.78 0.86 0.45\nTable 1 . A comparison of the averaged F-measures between the proposed method and the baseline methods\nused for template extraction in order to ensure the generality\nof the proposed approach.\nThe DNN is implemented in Python using Keras3with\nthe Tensorﬂow4backend. The parameters of the optimizer\nare set to default.\nTo get the ﬁnal transcription results for evaluation, a\nstandard peak picking method with a signal adaptive median\nthreshold is used [15]. The median threshold t(n)can be\ncomputed using Eq. (3):\nt(n) =\u0015\u0003max(x) +median (x(n);p); (3)\nin whichxis a vector of novelty function, \u0015is the offset\ncoefﬁcient relative to the maximum value, pis the order\n(length) of the median ﬁlter, and the nis the block index. All\nsystems are using the peak picking parameters p= 0:1 sand\n\u0015= 0:12as described in [29]. No grid search is performed.\n4. EXPERIMENTS\n4.1 Dataset Description\nThe collection of the unlabeled data is a crucial step for\nensuring a successful learning process. Generally speak-\ning, the unlabeled dataset should have following attributes:\n(i) the collection should contain drums whenever possible,\n(ii) the collection should be diverse in terms of music gen-\nres or playing styles, (iii) the collection should contain no\nduplicates, and (iv) the collection should be as consistent\nas possible in terms of audio quality. To build a collection\nthat meets the above-mentioned criteria, we compile a list\nfrom the Billboard Charts.5In particular, we start with\nan uniform distribution across a set of 4 genres selected\nfor commonly featuring strong drum beats or rhythmic pat-\nterns, namely R&B =HipHop, Pop, Rock, and Latin. For\nthis study, 200 songs from each genre has been selected. All\nthe songs are cross-checked for duplicates, and a ﬁnal list of\n800 songs has been compiled and retrieved from Youtube6\nusing open source Python library pafy.7\nAll songs are converted into mp3 ﬁles with a sampling\nrate of 44.1 kHz using ffmepg.8The source code for con-\nstructing the unlabeled music dataset is available online on\nGithub.9In order to speed up the process while retaining\n3https://keras.io Last accessed: 2017/04/27\n4https://www.tensorﬂow.org Last accessed: 2017/04/27\n5http://www.billboard.com/charts Last accessed: 2017/04/25\n6https://www.youtube.com Last accessed: 2017/04/25\n7https://pypi.python.org/pypi/pafy Last accessed: 2017/04/25\n8https://ffmpeg.org/download.html Last accessed: 2017/04/25\n9https://github.com/cwu307/unlabeledDrumDatasetdiversity, only a segment of 30 s from each song is used for\ntraining. This segment starts at 30 s into the song in order to\navoid possible inactivity at the beginning. Since the same\nunlabeled data is trained twice with two different sets of\nsoft targets generated from two different teachers, the total\nduration of the training audio is 800 mins (approximately\n13.5 hours), which is signiﬁcantly larger than any existing\ndrum dataset.\nThe most popular labeled drum dataset, ENST drum [8],\nis used as the test set for evaluation. This dataset consists\nof recordings from three different drummers performing\non their own drum kits. The recordings from each drum-\nmer contain individual hits, short phrases of drum beats,\ndrum solos, and short excerpts played with accompaniments.\nSince this paper focuses on ADT in polyphonic mixtures\nof music, only the minus one subset is used for evaluation.\nThis subset has 64 tracks of polyphonic music with a sam-\npling rate of 44.1 kHz. Each track in this subset has a length\nof approximately 50–70 s with a variety of playing styles.\nMore speciﬁcally, the subset contains various drum playing\ntechniques such as ghost notes, ﬂam, and drag, which is\nclose to a real-world setting [30]. The accompaniments are\nmixed with their corresponding drum tracks using a scaling\nfactor of 1/3 and 2/3 in order to be consistent with prior\nstudies [17, 22, 29]. Only the wet mix recordings of the\ndataset are used.\n4.2 Experiment Setup\nThe performance of the following systems is evaluated and\ncompared:\n(i)PFNMF (SMT): a PFNMF system initialized with a\ndrum dictionary matrix extracted from SMT-DRUM\ndataset. This baseline system is used as a teacher\nmodel to generate the soft targets\n(ii)PFNMF (200D): a PFNMF system initialized with\na drum dictionary matrix extracted from 200 drum\nmachines dataset. This baseline system is the second\nteacher model for generating the soft targets\n(iii) PFNMF (SMT + 200D): another baseline system by\nsimply taking the averaged activation functions of the\nabove systems as the prediction output\n(iv) Linear SGD Regressor: a baseline student model us-\ning a simple linear regression with stochastic gradi-\nent descent optimization. A Python implementation616 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Experiments Averaged F-measure\nRole Method Genres # Training Data HH BD SD\nStudent DNN Rock 200 * 1 = 200 0.76 0.83 0.44\nStudent DNN Pop 200 * 1 = 200 0.78 0.85 0.45\nStudent DNN RnB 200 * 1 = 200 0.74 0.83 0.48\nStudent DNN Latin 200 * 1 = 200 0.78 0.83 0.44\nStudent DNN All 50 * 4 = 200 0.77 0.85 0.45\nTable 2 . A comparison of different student models trained with unlabeled music data of different genres\nof this method from the open source library scikit-\nlearn10is used with all parameters set to default val-\nues.\n(v) DNN: the proposed student model\n4.3 Metrics\nThe evaluation metrics follow the standard calculation of\nthe precision (P), recall (R), and F-measure (F). To be con-\nsistent with [9, 22, 29], an onset is considered to be a match\nwith the ground truth if the time deviation between refer-\nence and detected onset time is less or equal to 50 ms. It\nshould be noted that some authors use more restrictive set-\ntings, compare, for instance, the 30 ms and 20 ms tolerance\nwindows as used in [17] and [26], respectively.\n4.4 Results\nThe experiment results are shown in Table 1 . The reported\naccuracies are the averaged F-measures across all 64 tracks\nfrom the ENST minus-one subset. Since the proposed\nmethod does not use the ENST drum dataset for training\npurposes, a three-fold cross validation scheme as reported\nin [17, 22, 25, 26, 29] is not necessary; this ensures the gen-\nerality of the proposed method, but prohibits the direct\ncomparison of the results with other publications.\nThe evaluation results show that both teacher systems\nPFNMF (SMT) and PFNMF (200D) perform similarly ex-\ncept for BD. This could be due to the discrepancy of the\npre-deﬁned drum dictionaries. The 3rd simple baseline sys-\ntem PFNMF (SMT+200D) averaging the teacher outputs\ngives almost identical performance as the teacher systems.\nThis result shows that a simple combination of the two\nteacher systems does not result in any improvement. This\nmeans either that the performance cannot be improved given\nthe teacher information or that a more sophisticated method\nis required for combining the outputs. The student baseline\nsystem is a simple linear regression model trained using the\nstudent-teacher learning paradigm as described in Sect. 3.\nThis baseline serves as a sanity check for the necessity of\na complex model such as DNN. As expected, the perfor-\nmance of the linear regression model is the worst among\nall the evaluated systems, indicating the need of deploying\na non-linear model in order to beneﬁt from this training\nscheme. Finally, the proposed DNN-based student model\nis actually able to outperform both teachers with higher\nF-measures for both HH and BD. The results for the SD\n10http://scikit-learn.org Last accessed: 2017/04/25are somewhat inconclusive; here, one teacher outperforms\nall other systems. This could imply the similarity between\nthe SD sounds in SMT and ENST dataset, but the infe-\nrior performance from the student model still needs further\ninvestigation.\nBased on these results, another interesting question\narises: does music genre play a role in the preparation\nof unlabeled data? To answer this question, a follow-up\nexperiment has been conducted by training the DNN model\nwith unlabeled data of each individual genre. The experi-\nment results are shown in Table 2 . In this experiment, the\nnumber of training samples is ﬁxed at 200 in order to elimi-\nnate the inﬂuence of data size. For the Allcase, 50 songs\nfrom each genre are randomly selected. Interestingly, the\nbest performance of different instruments, as highlighted\nin the table, belongs to different genres. This implies the\nadvantage of having various genres in the training data, for\nthey could potentially complement each other and boost the\nperformance of the student model.\nAlthough the cross-genre model trained on the equally\ndistributed data does not achieve the highest accuracy in\nevery individual instrument, it is still better than majority of\nthe single-genre models and generally well-balanced. Over-\nall, providing diverse unlabeled training data in terms of mu-\nsic genre seems to be beneﬁcial in this learning paradigm.\nFrom all of the above experiment results, the results for\nHH show the most obvious and consistent improvement\nover the teacher models. This observation leads to another\nquestion: where do these improvements come from? A\ncloser look at the experiment results reveals the strength of\nthe DNN student model. As shown in Table 3 , the DNN\nstudent model outperforms the teacher models on both pre-\ncision and recall for HH. The DNN student model also\nachieves the highest BD precision. Since these improve-\nments in precision are achieved without sacriﬁcing recall,\nthey suggest a reduction in false positives from the student\nmodel output. One possible explanation is that the songs\npresented in the unlabeled music data have a higher agree-\nment on HH sound; this allows the student model to acquire\na more consistent internal representation of HH that leads\nto a more accurate estimation during testing.\nIt is noticeable that the DNN student model seems to\nconsistently have problems detecting SD. Since the snare\ndrum tends to have larger spectral overlap with the other\ninstruments, it is conceivable that DNN student model will\nhave difﬁculties learning a robust internal representation\nfor this instrument. A collection of unlabeled data with\na stronger presence of snare drum might be possibly ableProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 617MethodHH BD SD\nP R P R P R\nPFNMF (SMT) 0.77 0.69 0.74 0.91 0.67 0.49\nPFNMF (200D) 0.75 0.68 0.82 0.90 0.60 0.49\nDNN 0.87 0.72 0.83 0.89 0.60 0.44\nTable 3 . A comparison of precision (P) and recall (R) between student and teacher models\nto alleviate the problem, however, this issue requires fur-\nther investigation before any conclusion can be drawn. In\ngeneral, this deﬁciency in SD is also consistent with the pre-\nvious studies [17, 22, 25, 29], where the detection of Snare\nDrum in polyphonic mixtures has been reported as the most\ndifﬁcult task in ADT. It is also possible that the Snare Drum\nis for some reason particularly hard to detect in the ENST\nset that is commonly used for evaluation.\n5. CONCLUSION\nThis paper presents a system for Automatic Drum Transcrip-\ntion based on the student-teacher learning paradigm with\nthe unlabeled music data. The proposed method integrates\ntwo NMF-based ADT teacher systems with a DNN-based\nstudent model by transferring knowledge using unlabeled\nmusic data, and the evaluation results indicate the possi-\nbility of obtaining a student model that outperforms the\nteacher model based on this approach. This result is gen-\nerally encouraging and demonstrates the great potential of\nusing unlabeled music data in ADT tasks. The experiment\nresults also imply the beneﬁt of having relevant music gen-\nres in the unlabeled training data, which could lead to the\nconstruction of an improved unlabeled dataset in the future\nstudies. The proposed method has the following advantages:\nﬁrst, the approach allows for complete separation between\ntraining and test data, therefore reducing the likelihood of\nover-ﬁtting and supporting the claim of generality of this\napproach. Second, the proposed method is able to support\ndata-driven approaches with the need of large amounts of\ntraining data given the availability of existing teacher mod-\nels. Third, the proposed method could not only be easily\napplied to other ADT systems but also inform data-hungry\nsystems from other transcription tasks or MIR problems in\ngeneral. Last but not least, this learning scheme has the\npotential of summarizing multiple complicated teacher sys-\ntems, providing competitive performance with one concise\nstudent model.\nThe possible future directions of this work are:\n(i)Increasing the number and diversity of teacher sys-\ntems. Since the proposed training scheme does not tie\nto any particular ADT approach, the teacher models\ncan be easily swapped with other ADT expert sys-\ntems. Intuitively, more teacher models should lead to\na more versatile student model. However, the inﬂu-\nence of having a more diverse pool of teacher systems\nstill requires further investigation.\n(ii)Varying architectures and approaches of the student\nmodels. In addition to DNNs, other neural networksarchitecture may have great potential of achieving\nbetter student performance as well. For instance, the\nRNN based model that incorporates the temporal in-\nformation could be a good ﬁt in the context of ADT\ntasks.\n(iii) Evaluating different input representations. As re-\nported by Cui et al. [4], the student model is able\nto outperform the teacher model especially when it\nis trained on the same soft targets but with a stronger\ninput representation. Following this observation, one\npossible future direction of this work is to investigate\nthe effectiveness of other input representations, such\nas CQT, Cepstrum, or Wavelet transforms.\n(iv)Evaluating alternative approaches for using unla-\nbeled data. To fully beneﬁt from the unlabeled data, it\nis also worth investigating how the proposed method\ncompares to other approaches such as unsupervised\nfeature learning [19].\nThe presented work represents only a preliminary study\nof what the authors see as a likely path for the future of\ntraining MIR systems as the issue of an insufﬁcient amount\nof annotated data is likely to get worse with increasing\ncomplexity of machine learning systems applied to MIR\ntasks. Drawing on the vast potential of using existing state-\nof-the-art MIR-systems as teachers and the overwhelming\npublic availability of unlabeled music data might enable\nexciting ways of creating new and more powerful MIR\nsystems.\n6. REFERENCES\n[1]Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Automatic\nmusic transcription: challenges and future directions.\nJournal of Intelligent Information Systems , July 2013.\n[2]Cristian Bucilu, Rich Caruana, and Alexandru\nNiculescu-Mizil. Model compression. In Proc. of the\nInternational Conference on Knowledge Discovery and\nData mining (SIGKDD) , page 535, 2006.\n[3]Olivier Chapelle, Bernhard Sch ¨olkopf, and Alexander\nZien. Semi-Supervised Learning . The MIT Press, 2006.\n[4]Jia Cui, Brian Kingsbury, Bhuvana Ramabhadran,\nGeorge Saon, Tom Sercu, Kartik Audhkhasi, Abhinav\nSethy, Markus Nussbaum-Thom, and Andrew Rosen-\nberg. Knowledge Distillation Across Ensembles of Mul-\ntiplingual Models for Low-resource Languages. In Proc.\nof the International Conference on Acoustics, Speech618 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017and Signal Processing (ICASSP) , pages 4825–4829,\n2017.\n[5]Christian Dittmar and Daniel G ¨artner. Real-time Tran-\nscription and Separation of Drum Recording Based on\nNMF Decomposition. In Proc. of the International Con-\nference on Digital Audio Effects (DAFX) , pages 1–8,\n2014.\n[6]Derry FitzGerald and Jouni Paulus. Unpitched percus-\nsion transcription. In Signal Processing Methods for\nMusic Transcription . Springer, 2006.\n[7]Nicolai Gajhede, Oliver Beck, and Hendrik Purwins.\nConvolutional Neural Networks with Batch Normaliza-\ntion for Classifying Hi-hat, Snare, and Bass Percussion\nSound Samples. In Proc. of the Audio Mostly , pages\n111–115, 2016.\n[8]Olivier Gillet and Ga ¨el Richard. Enst-drums: an exten-\nsive audio-visual database for drum signals processing.\nInProc. of International Society for Music Information\nRetrieval Conference (ISMIR) , 2006.\n[9]Olivier Gillet and Ga ¨el Richard. Transcription and sep-\naration of drum signals from polyphonic music. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 16(3):529–540, March 2008.\n[10] Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh.\nA Fast Learning Algorithm for Deep Belief Nets. Neural\ncomputation , 18:1527–1554, 2006.\n[11] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistilling the Knowledge in a Neural Network.\narXiv:1503.02531 , pages 1–9, 2015.\n[12] Sergey Ioffe and Christian Szegedy. Batch Normaliza-\ntion: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift. arXiv:1502.03167 , pages 1–11,\n2015.\n[13] Ping-Keng Jao and Yi-Hsuan Yang. Music Annotation\nand Retrieval using Unlabeled Exemplars: Correlation\nand Sparse Codes. IEEE Signal Processing Letters ,\n22(10):1771–1775, 2015.\n[14] Diederik P. Kingma and Jimmy Lei Ba. Adam: a\nMethod for Stochastic Optimization. In Proc. of the\nInternational Conference on Learning Representations\n(ICLR) , pages 1–15, 2015.\n[15] Alexander Lerch. An Introduction to Audio Content\nAnalysis: Applications in Signal Processing and Music\nInformatics . John Wiley & Sons, 2012.\n[16] Jinyu Li, Rui Zhao, Jui Ting Huang, and Yifan Gong.\nLearning small-size DNN with output-distribution-\nbased criteria. In Proc. of the Conference of the Inter-\nnational Speech Communication Association (INTER-\nSPEECH) , pages 1910–1914, 2014.[17] Jouni Paulus and Anssi Klapuri. Drum Sound Detec-\ntion in Polyphonic Music with Hidden Markov Models.\nEURASIP Journal on Audio, Speech, and Music Pro-\ncessing , 2009:1–9, 2009.\n[18] Matthew Prockup, Erik M. Schmidt, Jeffrey Scott, and\nYoungmoo E. Kim. Toward Understanding Expressive\nPercussion Through Content Based Analysis. In Proc. of\nthe International Society of Music Information Retrieval\nConference (ISMIR) , 2013.\n[19] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin\nPacker, and Andrew Y Ng. Self-taught learning: trans-\nfer learning from unlabeled data. In Proc. of the Interna-\ntional Conference on Machine Learning (ICML) , pages\n759–766, 2007.\n[20] Axel Roebel, Jordi Pons, Marco Liuni, and Mathieu\nLagrange. On Automatic Drum Transcription Using\nNon-Negative Matrix Deconvolution and Itakura Saito\nDivergence. In Proc. of the International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2015.\n[21] Markus Schedl, Emilia G ´omez, and Juli ´an Urbano. Mu-\nsic Information Retrieval: Recent Developments and\nApplications. Foundations and Trends R\rin Information\nRetrieval , 8(2-3):127–261, 2014.\n[22] Carl Southall, Ryan Stables, and Jason Hockman. Auto-\nmatic Drum Transcription Using Bi-Directional Recur-\nrent Neural Networks. In Proc. of International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2016.\n[23] Lucas Thompson, Matthias Mauch, and Simon Dixon.\nDrum Transcription via Classiﬁcation of Bar-Level\nRhythmic Patterns. In Proc. of International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2014.\n[24] Adam R. Tindale, Ajay Kapur, George Tzanetakis, and\nIchiro Fujinaga. Retrieval of percussion gestures using\ntimbre classiﬁcation techniques. In Proc. of the Interna-\ntional Society of Music Information Retrieval Confer-\nence (ISMIR) , pages 541–544, 2004.\n[25] Richard. V ogl, Matthias Dorfer, and Peter Knees. Re-\ncurrent Neural Networks for Drum Transcription. In\nProc. of International Society for Music Information\nRetrieval Conference (ISMIR) , pages 730–736, 2016.\n[26] Richard V ogl, Matthias Dorfer, and Peter Knees. Drum\nTranscription From Polyphonic Music With Recurrent\nNeural Networks. In Proc. of the International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 201–205, 2017.\n[27] Shinji Watanabe, Takaaki Hori, Jonathan L. Roux, and\nJohn R. Hershey. Student-Teacher Network Learning\nwith Enhanced Features. In Proc. of the International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 5275–5279, 2017.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 619[28] Bin Wu, Erheng Zhong, Derek Hao Hu, Andrew Horner,\nand Qiang Yang. SMART : Semi-Supervised Music\nEmotion Recognition with Social Tagging. In SIAM\nConference on Data Mining , pages 279–287, 2013.\n[29] Chih-Wei Wu and Alexander Lerch. Drum transcription\nusing partially ﬁxed non-negative matrix factorization\nwith template adaptation. In Proc. of International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 257–263, 2015.\n[30] Chih-Wei Wu and Alexander Lerch. On drum playingtechnique detection in polyphonic mixtures. In Proc. of\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 218–224, 2016.\n[31] Kazuyoshi Yoshii, Masataka Goto, and Hiroshi G.\nOkuno. Drum sound recognition for polyphonic au-\ndio signals by adaptation and matching of spectrogram\ntemplates with harmonic structure suppression. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 15(1):333–345, 2007.620 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation.",
        "author": [
            "Li-Chia Yang",
            "Szu-Yu Chou",
            "Yi-Hsuan Yang"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415990",
        "url": "https://doi.org/10.5281/zenodo.1415990",
        "ee": "https://zenodo.org/records/1415990/files/YangCY17.pdf",
        "abstract": "Most existing neural network models for music genera- tion use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convo- lutional neural networks (CNNs) can also generate realis- tic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discrimina- tor to learn the distributions of melodies, making it a gen- erative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by con- ditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google’s MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet’s melodies are reported to be much more interesting.",
        "zenodo_id": 1415990,
        "dblp_key": "conf/ismir/YangCY17",
        "keywords": [
            "neural network models",
            "recurrent neural networks",
            "WaveNet model",
            "convolutional neural networks",
            "generative adversarial network",
            "melody",
            "MIDI notes",
            "symbolic domain",
            "conditional mechanism",
            "multiple MIDI channels"
        ],
        "content": "MIDINET: A CONVOLUTIONAL GENERATIVE ADVERSARIAL\nNETWORK FOR SYMBOLIC-DOMAIN MUSIC GENERATION\nLi-Chia Yang, Szu-Yu Chou, Yi-Hsuan Yang\nResearch Center for IT innovation, Academia Sinica, Taipei, Taiwan\nfrichard40148, fearofchou, yang g@citi.sinica.edu.tw\nABSTRACT\nMost existing neural network models for music genera-\ntion use recurrent neural networks. However, the recent\nWaveNet model proposed by DeepMind shows that convo-\nlutional neural networks (CNNs) can also generate realis-\ntic musical waveforms in the audio domain. Following this\nlight, we investigate using CNNs for generating melody (a\nseries of MIDI notes) one bar after another in the symbolic\ndomain. In addition to the generator, we use a discrimina-\ntor to learn the distributions of melodies, making it a gen-\nerative adversarial network (GAN). Moreover, we propose\na novel conditional mechanism to exploit available prior\nknowledge, so that the model can generate melodies either\nfrom scratch, by following a chord sequence, or by con-\nditioning on the melody of previous bars (e.g. a priming\nmelody), among other possibilities. The resulting model,\nnamed MidiNet, can be expanded to generate music with\nmultiple MIDI channels (i.e. tracks). We conduct a user\nstudy to compare the melody of eight-bar long generated\nby MidiNet and by Google’s MelodyRNN models, each\ntime using the same priming melody. Result shows that\nMidiNet performs comparably with MelodyRNN models\nin being realistic and pleasant to listen to, yet MidiNet’s\nmelodies are reported to be much more interesting.\n1. INTRODUCTION\nAlgorithmic composition is not a new idea. The ﬁrst com-\nputational model for algorithmic composition dates back\nto 1959 [16], according to the survey of Papadopoulos and\nWiggins [23]. People have also used (shallow) neural net-\nworks for music generation since 1989 [30]. It was, how-\never, only until recent years when deep neural networks\ndemonstrated their ability in learning from big data col-\nlections that generating music by neural networks became\na trending topic. Lots of deep neural network models for\nmusic generation have been proposed just over the past two\nyears [4, 7, 10, 15, 18, 19, 21, 22, 26, 28, 31, 33].\nThe majority of existing neural network models for mu-\nsic generation use recurrent neural networks (RNNs) and\nc\rLi-Chia Yang, Szu-Yu Chou, Yi-Hsuan Yang. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC\nBY 4.0). Attribution: Li-Chia Yang, Szu-Yu Chou, Yi-Hsuan\nYang. “MidiNet: A Convolutional Generative Adversarial Network for\nSymbolic-domain Music Generation”, 18th International Society for Mu-\nsic Information Retrieval Conference, Suzhou, China, 2017.their variants, presumably for music generation is inher-\nently about generating sequences [2, 3, 9, 14]. These mod-\nels differ in the model assumptions and the way musical\nevents are represented and predicted, but they all use in-\nformation from the previous events to condition the gen-\neration of the present one. Famous examples include the\nMelodyRNN models [33] for symbolic-domain generation\n(i.e. generating MIDIs) and the SampleRNN model [19]\nforaudio-domain generation (i.e. generating WA Vs).\nRelatively fewer attempts have been made to use deep\nconvolutional neural networks (CNNs) for music genera-\ntion. A notable exception is the WaveNet model [31] pro-\nposed recently for audio-domain generation. It generates\none audio sample at a time, with the predictive distribution\nfor each sample conditioned on previous samples through\ndilated causal convolutions [31]. WaveNet shows it possi-\nble for CNNs to generate realistic music. This is encourag-\ning, as CNNs are typically faster to train and more easily\nparallelizable than RNNs [32].\nFollowing this light, we investigate in this paper a novel\nCNN-based model for symbolic-domain generation, focus-\ning on melody generation.1Instead of creating a melody\nsequence continuously, we propose to generate melodies\none bar (measure) after another , in a successive manner.\nThis allows us to employ convolutions on a 2-D matrix\nrepresenting the presence of notes over different time steps\nin a bar. We can have such a score-like representation for\neach bar for either a real or a generated MIDI.\nMoreover, to emulate creativity [23] and encourage di-\nverse generation result, we use random noises as input to\nourgenerator CNN. The goal of the generator is to trans-\nform random noises into the aforementioned 2-D score-\nlike representation, that “appears” to be from real MIDI.\nThis transformation is achieved by a special convolution\noperator called transposed convolution [8]. Meanwhile, we\nlearn a discriminator CNN that takes as input a 2-D score-\nlike representation and predicts whether it is from a real or\na generated MIDI, thereby informing the generator how to\nappear to be real. This amounts to a generative adversarial\nnetwork (GAN) [11–13,24,27], which learns the generator\nand discriminator iteratively under the concept of minimax\ntwo-player game theory.\nThis GAN alone does not take into account the tem-\nporal dependencies across different bars. To address this\nissue, we propose a novel conditional mechanism to use\n1In general, a melody may be deﬁned as a succession of (monophonic)\nmusical notes expressing a particular musical idea.324MelodyRNN Song from PI DeepBach C-RNN-GAN MidiNet WaveNet\n[33] [7] [15] [21] (this paper) [31]\ncore model RNN RNN RNN RNN CNN CNN\ndata type symbolic symbolic symbolic symbolic symbolic audio\ngenre speciﬁcity — — Bach chorale — — —\nmandatory prior knowl- priming music scale &— — —priming\nedge melody melody proﬁle wave\nfollow a priming melodyp p pp\nfollow a chord sequencep\ngenerate multi-track musicp p pp\nuse GANpp\nuse versatile conditionsp\nopen source codep p p\nTable 1 . Comparison between recent neural network based music generation models\nmusic from the previous bars to condition the generation of\nthe present bar. This is achieved by learning another CNN\nmodel, which we call the conditioner CNN, to incorporate\ninformation from previous bars to intermediate layers of\nthe generator CNN. This way, our model can “look back”\nwithout a recurrent unit as used in RNNs. Like RNNs, our\nmodel can generate music of arbitrary number of bars.\nBecause we use random noises as inputs to our gener-\nator, our model can generate melodies from scratch , i.e.\nwithout any other prior information. However, due to the\nconditioner CNN, our model has the capacity to exploit\nwhatever prior knowledge that is available and can be rep-\nresented as a matrix. For example, our model can generate\nmusic by following a chord progression, or by following a\nfew starting notes (i.e. a priming melody). Given the same\npriming melody, our model can generate different results\neach time, again due to the random input.\nThe proposed model can be extended to generate differ-\nent types of music, by using different conditions. Based\non an idea called feature matching [27], we propose a way\nto control the inﬂuence of such conditions on the genera-\ntion result. We can then control, for example, how much\nthe current bar should sound like the previous bars. More-\nover, our CNNs can be easily extended to deal with tensors\ninstead of matrices, to exploit multi-channel MIDIs and to\ngenerate music of multiple tracks or parts. We believe such\na highly adaptive and generic model structure can be a use-\nful alternative to RNN-based designs. We refer to this new\nmodel as the MidiNet.\nIn our experiment, we conduct a user study to compare\nthe melodies generated by MidiNet and MelodyRNN mod-\nels [33]. For fair comparison, we use the same priming\nmelodies for them to generate melodies of eight-bar long\n(including the primers), without any other prior informa-\ntion. To demonstrate the ﬂexibility of MidiNet, we pro-\nvide the result of two additional settings: one uses addi-\ntionally chord progressions of eight-bar long to condition\nthe generation, and the other uses a slightly different net-\nwork architecture to generate more creative music. For re-\nproducibility, the source code and pre-trained models of\nMidiNet are released online2.\n2https://github.com/RichardYang40148/MidiNet2. RELATED WORK\nA large number of deep neural network models have been\nproposed lately for music generation. This includes mod-\nels for generating a melody sequence or audio waveforms\nby following a few priming notes [10,18,19,22,31,33], ac-\ncompanying a melody sequence with music of other parts\n[15], or playing a duet with human [4, 26].\nTable 1 compares MidiNet with a number of major re-\nlated models. We brieﬂy describe each of them below.\nThe MelodyRNN models [33] proposed by the Magenta\nProject from the Google Brain team are possibly among the\nmost famous examples of symbolic-domain music gener-\nation by neural networks. In total three RNN-based mod-\nels were proposed, including two variants that aim to learn\nlonger-term structures, the lookback RNN and the atten-\ntion RNN [33]. Source code and pre-trained models for\nthe three models are all publicly available.3As the main\nfunction of MelodyRNN is to generate a melody sequence\nfrom a priming melody, we use the MelodyRNN models as\nthe baseline in our evaluation.\nSong from PI [7] is a hierarchical RNN model that uses\na hierarchy of recurrent layers to generate not only the\nmelody but also the drums and chords, leading to a multi-\ntrack pop song. This model nicely demonstrates the ability\nof RNNs in generating multiple sequences simultaneously.\nHowever, it requires prior knowledge of the musical scale\nand some proﬁles of the melody to be generated [7], which\nis not needed in many other models, including MidiNet.\nDeepBach [15], proposed by Sony CSL, is speciﬁcally\ndesigned for composing polyphonic four-part chorale mu-\nsic in the style of J. S. Bach. It is an RNN-based model that\nallows enforcing user-deﬁned constraints such as rhythm,\nnotes, parts, chords and cadences.\nC-RNN-GAN [21] is to date the only existing model\nthat uses GAN for music generation, to our best knowl-\nedge. It also takes random noises as input as MidiNet\ndoes, to generate diverse melodies. However, it lacks a\nconditional mechanism [17, 20, 25] to generate music by\nfollowing either a priming melody or a chord sequence.\n3https://github.com/tensorflow/magenta/tree/\nmaster/magenta/models/melody_rnn (accessed 2017-4-26)Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 325Figure 1 . System diagram of the proposed MidiNet model for symbolic-domain music generation.\nWaveNet [10, 31] is a CNN-based model proposed by\nDeepMind for creating raw waveforms of speech and mu-\nsic. The advantage of audio-domain generation is the pos-\nsibility of creating new sounds, but we choose to focus on\nsymbolic-domain generation in this paper.\n3. METHODS\nA system diagram of MidiNet is shown in Figure 1. Below,\nwe present the technical details of each major component.\n3.1 Symbolic Representation for Convolution\nOur model uses a symbolic representation of music in ﬁxed\ntime length, by dividing a MIDI ﬁle into bars. The note\nevents of a MIDI channel can be represented by an h-\nby-wreal-valued matrix X, wherehdenotes the number\nof MIDI notes we consider, possibly including one more\ndimension for representing silence, and wrepresents the\nnumber of time steps we use in a bar. For melody gener-\nation, there is at most one active note per time step. We\nuse a binary matrix X2f0;1gh\u0002wif we omit the velocity\n(volume) of the note events. We use multiple matrices per\nbar if we want to generate multi-track music.\nIn this representation, we may not be able to easily dis-\ntinguish between a long note and two short repeating notes\n(i.e. consecutive notes with the same pitch). Future exten-\nsions can be done to emphasize the note onsets.\n3.2 Generator CNN and Discriminator CNN\nThe core of MidiNet is a modiﬁed deep convolutional gen-\nerative adversarial network (DCGAN) [24], which aims at\nlearning a discriminator Dto distinguish between real (au-\nthentic) and generated (artiﬁcial) data, and a generator G\nthat “fools” the discriminator. As typical in GANs, the in-\nput ofGis a vector of random noises z2Rl, whereas the\noutput ofGis anh-by-wmatrix bX=G(z)that “appears”\nto be real to D. GANs learn GandDby solving:\nmin\nGmax\nDV(D;G ) =EX\u0018pdata(X)[log(D(X))]+\nEz\u0018pz(z)[log(1\u0000D(G(z)))];(1)where X\u0018pdata(X)denotes the operation of sampling\nfrom real data, and z\u0018pz(z)the sampling from a random\ndistribution. As typical in GANs, we need to train Gand\nDiteratively multiple times, to gradually make a better G.\nOur discriminator is a typical CNN with a few convolu-\ntion layers, followed by fully-connected layers. These lay-\ners are optimized with a cross-entropy loss function, such\nthat the output of Dis close to 1 for real data (i.e. X) and\n0 for those generated (i.e. G(z)). We use a sigmoid neuron\nat the output layer of Dso its output is in [0,1].\nThe goal of the generator CNN, on the other hand, is\nto make the output of Dclose to 1 for the generated data.\nFor generation, it has to transform a vector zinto a matrix\nbX. This is achieved by using a few fully connected layers\nﬁrst, and then a few transposed convolution layers [8] that\n“upsamples” smaller vectors/matrices into larger ones.\nOwing to the nature of minimax games, the training\nof GANs is subject to issues of instability and mode col-\nlapsing [12]. Among the various possible techniques to\nimprove the training of GANs [1, 5, 27], we employ the\nso-called feature matching and one-sided label smooth-\ning [27] in our model. The idea of feature matching is\nto add additional L2 regularizers to Eq. 1, such that the\ndistributions of real and generated data are enforced to be\nclose. Speciﬁcally, we add the following two terms when\nwe learnG:\n\u00151kEX\u0000EG(z)k2\n2+\u00152kEf(X)\u0000Ef(G(z))k2\n2;\n(2)\nwherefdenotes the ﬁrst convolution layer of the discrim-\ninator, and\u00151,\u00152are parameters to be set empirically.\n3.3 Conditioner CNN\nIn GAN-based image generation, people often use a vec-\ntor to encode available prior knowledge that can be used\nto condition the generation. This is achieved by reshaping\nthe vector and then adding it to different layers of Gand\nD, to provide additional input [20]. Assuming that the con-\nditioning vector has length n, to add it to an intermediate\nlayer of shape a-by-bwe can duplicate the values abtimes326 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017to get a tensor of shape a-by-b-by-n, and then concatenate\nit with the intermediate layer in the feature map axis. This\nis illustrated by the light orange blocks in Figure 1. We call\nsuch a conditional vector 1-D conditions .\nAs the generation result of our GAN is an h-by-wma-\ntrix of notes and time steps, it is convenient if we can per-\nform conditioning directly on each entry of the matrix. For\nexample, the melody of a previous bar can be represented\nas anotherh-by-wmatrix and used to condition the genera-\ntion of the present bar. We can have multiple such matrices\nto learn from multiple previous bars. We can directly add\nsuch a conditional matrix to the input layer of Dto inﬂu-\nence all the subsequent layers. However, to exploit such\n2-D conditions inG, we need a mechanism to reshape the\nconditional matrix to smaller vectors of different shapes, to\ninclude them to different intermediate layers of G.\nWe propose to achieve this by using a conditioner CNN\nthat can be viewed as a reverse of the generator CNN. As\nthe blue blocks in Figure 1 illustrates, the conditioner CNN\nuses a few convolution layers to process the input h-by-w\nconditional matrix. The conditioner and generator CNNs\nuse exactly the same ﬁlter shapes in their convolution lay-\ners, so that the outputs of their convolution layers have\n“compatible” shapes. In this way, we can concatenate the\noutput of a convolution layer of the conditioner CNN to\nthe input of a corresponding transposed convolution layer\nof the generator CNN, to inﬂuence the generation process.\nIn the training stage, the conditioner and generator CNNs\nare trained simultaneously, by sharing the same gradients.\n3.4 Tunning for Creativity\nWe propose two methods to control the trade-off between\ncreativity and discipline of MidiNet. The ﬁrst method is to\nmanipulate the effect of the conditions by using them only\nin part of the intermediate transposed convolution layers\nofG, to giveGmore freedom from the imposed condi-\ntions. The second method capitalizes the effect of the fea-\nture matching technique [27]: we can increase the values\nof\u00151and\u00152to make the generated music sounds closer to\nexisting music (i.e. those observed in the training set).\n4. IMPLEMENTATION\n4.1 Dataset\nAs the major task considered in this paper is melody gen-\neration, for training MidiNet we need a MIDI dataset that\nclearly speciﬁes per ﬁle which channel corresponds to the\nmelody. To this end, we crawled a collection of 1,022\nMIDI tabs of pop music from TheoryTab,4which provides\nexactly two channels per tab, one for melody and the other\nfor the underlying chord progression. With this dataset, we\ncan implement at least two versions of MidiNets: one that\nlearns from only the melody channel for fair comparison\nwith MelodyRNN [33], which does not use chords, and\nthe other that additionally uses chords to condition melody\ngeneration, to test the capacity of MidiNet.\n4https://www.hooktheory.com/theorytabdimensions 1–12 13\nmajor C, C#, D, D#, E, F, F#, G, G#, A, A#, B 0\nminor A, A#, B, C, C#, D, D#, E, F, F#, G, G# 1\nTable 2 . 13-dimensional chord representation\nFor simplicity, we ﬁltered out MIDI tabs that contain\nchords other than the 24 basic chord triads (12 major and\n12 minor chords). Next, we segmented the remaining tabs\nevery 8 bars, and then pre-processed the melody channel\nand the chord channel separately, as described below.\nFor melodies, we ﬁxed the smallest note unit to be the\nsixteenth note, making w= 16 . Speciﬁcally, we prolonged\nnotes which have a pause note after them. If the ﬁrst note\nof a bar is a pause, we extended the second note to have\nit played while the bar begins. There are other exceptions\nsuch as triplets and shorter notes (e.g. 32nd notes), but\nwe chose to exclude them in this implementation. More-\nover, for simplicity, we shifted all the melodies into two oc-\ntaves, from C4toB5, and neglected the velocity of the note\nevents. Although our melodies would use only 24 possible\nnotes after these preprocessing steps, we considered all the\n128 MIDI notes (i.e. from C0toG10) in our symbolic\nrepresentation. In doing so, we can detect model collaps-\ning [12] more easily, by checking whether the model gen-\nerates notes outside these octaves. As there are no pauses\nin our data after preprocessing, we do not need a dimension\nfor silence. Therefore, h= 128 .\nFor chords, instead of using a 24-dimensional one-hot\nvector, we found it more efﬁcient to use a chord representa-\ntion that has only 13 dimensions— the ﬁrst 12 dimensions\nfor marking the key, and the last for the chord type (i.e.\nmajor or minor), as illustrated in Table 4.1. We pruned the\nchords such that there is only one chord per bar.\nAfter these preprocessing steps, we were left with 526\nMIDI tabs (i.e. 4,208 bars).5For data augmentation, we\ncircularly shifted the melodies and chords to any of the 12\nkeys in equal temperament, leading to a ﬁnal dataset of\n50,496 bars of melody and chord pairs for training.\n4.2 Network Speciﬁcation\nOur model was implemented in TensorFlow. For the gen-\nerator, we used as input random vectors of white Gaussian\nnoise of length l= 100 . Each random vector go through\ntwo fully-connected layers, with 1024 and 512 neurons re-\nspectively, before being reshaped into a 1-by-2 matrix. We\nthen used four transposed convolution layers: the ﬁrst three\nuse ﬁlters of shape 1-by-2 and two strides [8], and the last\nlayer uses ﬁlters of shape 128-by-1 and one stride. Accord-\ningly, our conditioner has four convolution layers, which\nuse 128-by-1 ﬁlters for the ﬁrst layer, and 1-by-2 ﬁlters for\nthe other three. For creating a monophonic note sequence,\nwe added a layer to the end of Gto turn off per time step\nall but the note with the highest activation.\nAs typical in GANs, the discriminator is likely to over-\npower the generator, leading to the so-called vanishing gra-\n5In contrast, MelodyRNN models [33] were trained on thousands of\nMIDI ﬁles, though the exact number is not yet disclosed.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 327dient problem [1,12]. We adopted two strategies to weaken\nthe discriminator. First, in each iteration, we updated the\ngenerator and conditioner twice, but the discriminator only\nonce. Second, we used only two convolution layers (14 ﬁl-\nters of shape 128-by-2, two strides, and 77 ﬁlters of shape\n1-by-4, two strides) and one fully-connected layer (1,024\nneurons) for the discriminator.\nWe ﬁne-tuned the other parameters of MidiNet and con-\nsidered the following three variants in our experiment.\n4.2.1 Model 1: Melody generator, no chord condition\nThis variant uses the melody of the previous bar to condi-\ntion the generation of the present bar. We used this 2-D\ncondition in all the four transposed convolution layers of\nG. We set the number of ﬁlters in all the four transposed\nconvolution layers of Gand the four convolution layers of\nthe conditioner CNN to 256. The feature matching param-\neters\u00151and\u00152are set to 0.1 and 1, respectively. We did\nnot use the 2-D condition for D, requiring it to distinguish\nbetween real and generated melodies from the present bar.\nIn the training stage, we ﬁrstly added one empty bar\nbefore all the MIDI tabs, and then randomly sampled two\nconsecutive bars from any tab. We used the former bar as\nan instance of real data (i.e. X) and the input to D, and the\nformer bar (which is a real melody or all zeros) as a 2-D\ncondition and the input to the conditioner CNN Once the\nmodel was trained, we used Gto generate melodies of 8-\nbar long in the following way: the ﬁrst bar was composed\nof a real, priming melody sampled from our dataset; the\ngeneration of the second bar was made by G, conditioned\nby this real melody; starting from the third bar, Ghad to\nuse the (artiﬁcial) melody it generated previously for the\nlast bar as the 2-D condition. This process repeated until\nwe had all the eight bars.6\n4.2.2 Model 2: Melody generator with chord condition,\nstable mode\nThis variant additionally uses the chord channel. Because\nour MIDI tabs use one chord per bar, we used the chord\n(a 13-dimensional vector; see Table 4.1) of the present bar\nas a 1-D condition for generating the melody for the same\nbar. We can say that our model is generating a melody\nsequence that ﬁts the given chord progression.\nTo highlight the chord condition, we used the 2-D\nprevious-bar condition only in the last transposed convo-\nlution layer of G. In contrast, we used the 1-D chord con-\ndition in all the four transposed convolution layer of G, as\nwell as the input layer for D. Moreover, we set \u00151= 0:01,\n\u00152= 0:1, and used 128 ﬁlters in the transposed convolu-\ntion layers of Gand only 16 ﬁlters in the convolution layers\nof the conditioner CNN. As a result, the melody generator\nis more chord-dominant andstable , for it would mostly fol-\nlow the chord progression and seldom generate notes that\nviolate the constraint imposed by the chords.\n6It is also possible to use multiple previous bars to condition our gen-\neration, but we leave this as a future extension.\nFigure 2 . Result of a user study comparing MelodyRNN\nand MidiNet models, for people (top row) with musical\nbackgrounds and (bottom) without musical backgrounds.\nThe middle bars indicate the mean values. Please note that\nMidiNet Model 2 takes the chord condition as additional\ninformation.\n4.2.3 Model 3: Melody generator with chord condition,\ncreative mode\nThis variant realizes a slightly more creative melody gen-\nerator by placing the 2-D condition in every transposed\nconvolution layer of G. In this way, Gwould sometimes\nviolate the constraint imposed by the chords, to somehow\nadhere to the melody of the previous bar. Such violations\nsometimes sound unpleasant, but can be sometimes cre-\native. Unlike the previous two variants, we need to listen\nto several melodies generated by this model to handpick\ngood ones. However, we believe such a model can still be\nuseful for assisting and inspiring human composers.\n5. EXPERIMENTAL RESULT\nTo evaluate the aesthetic quality of the generation result,\na user study that involves human listeners is needed. We\nconducted a study with 21 participants. Ten of them un-\nderstand basic music theory and have the experience of be-\ning an amateur musician, so we considered them as people\nwith musical backgrounds, or professionals for short.\nWe compared MidiNet with three MelodyRNN mod-\nels pre-trained and released by Google Magenta: the basic\nRNN, the lookback RNN, and the attention RNN [33]. We\nrandomly picked 100 priming melodies from the training\ndata7and asked the models create melodies of eight bars\nby following these primers. We considered two variants\nof MidiNet in the user study: model 1 (Section 4.2.1) for\nfair comparison with MelodyRNN, and model 2 (Section\n4.2.2) for probing the effects of using chords. Although the\nresult of model 2 was generated by additionally following\nthe chords, we did not playback the chord channel in the\nuser study.\nWe randomly selected the generation result of three out\nof the 100 priming melodies for each participant to listen\nto, leading to three sets of music. To avoid bias, we ran-\ndomly shufﬂed the generation result by the ﬁve considered\n7Even though these priming melodies are in the training data, MidiNet\ngenerates melodies that are quite different from the existing ones.328 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017(a) MidiNet model 1\n(b) MidiNet model 2\n(c) MidiNet model 3\nFigure 3 . Example result of the melodies (of 8 bars) generated by different implementations of MidiNet.\nmodels, such that in each set the ordering of the ﬁve mod-\nels is different. The participants were asked to stay in a\nquiet room separately and used a headphone for music lis-\ntening through the Internet, one set at a time. We told them\nthat some of the music “might be” real, and some might\nbe generated by machine, although all of them were actu-\nally automatically generated. They were asked to rate the\ngenerated melodies in terms of the following three metrics:\nhow pleasing ,how real , and how interesting , from 1 (low)\nto 5 (high) in a ﬁve-point Likert scale.\nThe result of the user study is shown in Figure 2 as vi-\nolin plots, where the following observations can be made.\nFirst, among the MelodyRNN models, lookback RNN and\nattention RNN consistently outperform basic RNN across\nthe three metrics and two user groups (i.e. people with and\nwithout musical backgrounds), which is expected accord-\ning to the report of Magenta [33]. The mean values for\nlookback RNN are around 3 (medium) for being pleasant\nand realistic, and around 2.5 for being interesting.\nSecond, MidiNet model 1, which uses only the previous\nbar condition, obtains similar ratings as the MelodyRNN\nmodels in being pleasant and realistic. This is encouraging,\nas MelodyRNN models can virtually exploit all the previ-\nous bars in generation. This result demonstrates the effec-\ntiveness of the proposed conditioner CNN in learning tem-\nporal information. Furthermore, we note that the melodies\ngenerated by MidiNet model 1 were found much more in-\nteresting than those generated by MelodyRNN. The mean\nvalue in being interesting is around 4 for people with musi-\ncal backgrounds, and 3.4 for people without musical back-\ngrounds. The violin plot indicates that the ratings of the\nprofessionals are mostly larger than 3.\nThird, MidiNet model 2, which further uses chords, ob-\ntains the highest mean ratings in being pleasant and realis-\ntic for both user groups. In terms of interestingness, it also\noutperforms the three MelodyRNN models, but is inferior\nto MidiNet model 1, especially for professionals.\nAccording to the feedback from the professionals, a\nmelody sounds artiﬁcial if it lacks variety or violates prin-\ncipals in (Western) music theory. The result of MidiNet\nmodel 1 can sound artiﬁcial, for it relies on only the pre-\nvious bar and hence occasionally generates “unexpected”notes. In contrast, the chord channel provides a musical\ncontext that can be effectively used by MidiNet model 2\nthrough the conditional mechanism. However, occasional\nviolation of music theory might be a source of interesting-\nness and thereby creativity. For example, the professionals\nreported that the melodies generated by MelodyRNN mod-\nels are sometimes too repetitive, or “safe,” making them\nartiﬁcial and less interesting. It might be possible to fur-\nther ﬁne tune our model to reach a better balance between\nbeing real and being interesting, but we believe our user\nstudy has shown the promise of MidiNet.\nFigure 3 shows some melodies generated by differ-\nent implementations of MidiNet, which may provide in-\nsights into MidiNet’s performance. Figure 3(a) shows that\nMidiNet model 1 can effectively exploit the previous bar\ncondition—most bars start with exactly the same ﬁrst two\nnotes (as the priming bar) and they use similar notes in be-\ntween. Figure 3(b) shows the result of MidiNet model 2,\nwhich highlights the chord condition. Figure 3(c) shows\nthat MidiNet can generate more creative result by mak-\ning the chord condition and previous bar condition equally\nstrong. We can see stronger connections between adjacent\nbars from the result of this MidiNet model 3. For more\naudio examples, please go to https://soundcloud.\ncom/vgtsv6jf5fwq/sets .\n6. CONCLUSION\nWe have presented MidiNet, a novel CNN-GAN based\nmodel for MIDI generation. It has a conditional mecha-\nnism to exploit versatile prior knowledge of music. It also\nhas a ﬂexible architecture and can generate different types\nof music depending on input and speciﬁcations. Our eval-\nuation shows that it can be a powerful alternative to RNNs.\nFor future work, we would extend MidiNet to generate\nmulti-track music, to include velocity and pauses by train-\ning the model by using richer and larger MIDI data. We\nare also interested in using ideas of reinforcement learn-\ning [29] to incorporate principles of music theory [18], and\nto take input from music information retrieval models such\nas genre recognition [6] and emotion recognition [34].Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 3297. REFERENCES\n[1] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou.\nWasserstein GAN. arXiv preprint arXiv:1701.07875 ,\n2017.\n[2] Jamshed J. Bharucha and Peter M. Todd. Modeling the\nperception of tonal structure with neural nets. Com-\nputer Music Journal , 13(4):44–53.\n[3] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. arXiv\npreprint arXiv:1206.6392 , 2012.\n[4] Mason Bretan, Gil Weinberg, and Larry Heck. A unit\nselection methodology for music generation using deep\nneural networks. arXiv preprint arXiv:1612.03789 ,\n2016.\n[5] Xi Chen, Yan Duan, Rein Houthooft, John Schul-\nman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:\nInterpretable representation learning by information\nmaximizing generative adversarial nets. In Proc. Ad-\nvances in Neural Information Processing Systems ,\npages 2172–2180, 2016.\n[6] Keunwoo Choi, George Fazekas, Mark B. Sandler,\nand Kyunghyun Cho. Convolutional recurrent neu-\nral networks for music classiﬁcation. arXiv preprint\narXiv:1609.04243 , 2016.\n[7] Hang Chu, Raquel Urtasun, and Sanja Fidler. Song\nfrom PI: A musically plausible network for pop music\ngeneration. arXiv preprint arXiv:1611.03477 , 2016.\n[8] Vincent Dumoulin and Francesco Visin. A guide\nto convolution arithmetic for deep learning. arXiv\npreprint arXiv:1603.07285 , 2016.\n[9] Douglas Eck and Juergen Schmidhuber. Finding tem-\nporal structure in music: Blues improvisation with\nLSTM recurrent networks. In Proc. IEEE Workshop\non Neural Networks for Signal Processing , pages 747–\n756, 2002.\n[10] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander\nDieleman, Douglas Eck, Karen Simonyan, and Mo-\nhammad Norouzi. Neural audio synthesis of musi-\ncal notes with WaveNet autoencoders. arXiv preprint\narXiv:1704.01279 , 2017.\n[11] Jon Gauthier. Conditional generative adversarial nets\nfor convolutional face generation. Class Project for\nStanford CS231N: Convolutional Neural Networks for\nVisual Recognition, Winter semester , 2014:5, 2014.\n[12] Ian J. Goodfellow. NIPS 2016 tutorial: Generative ad-\nversarial networks. arXiv preprint arXiv:1701.00160 ,\n2017.[13] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnets. In Proc. Advances in Neural Information Process-\ning Systems , pages 2672–2680, 2014.\n[14] Alex Graves. Generating sequences with recurrent neu-\nral networks. arXiv preprint arXiv:1308.0850 , 2013.\n[15] Ga ¨etan Hadjeres and Franc ¸ois Pachet. DeepBach: a\nsteerable model for bach chorales generation. arXiv\npreprint arXiv:1612.01010 , 2016.\n[16] Lejaren Hiller and Leonard M. Isaacson. Experimen-\ntal Music: Composition with an Electronic Computer .\nNew York: McGraw-Hill, 1959.\n[17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and\nAlexei A Efros. Image-to-image translation with\nconditional adversarial networks. arXiv preprint\narXiv:1611.07004 , 2016.\n[18] Natasha Jaques, Shixiang Gu, Richard E. Turner,\nand Douglas Eck. Tuning recurrent neural net-\nworks with reinforcement learning. arXiv preprint\narXiv:1611.02796 , 2016.\n[19] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani,\nRithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C.\nCourville, and Yoshua Bengio. SampleRNN: An un-\nconditional end-to-end neural audio generation model.\narXiv preprint arXiv:1612.07837 , 2016.\n[20] Mehdi Mirza and Simon Osindero. Conditional gener-\native adversarial nets. arXiv preprint arXiv:1411.1784 ,\n2014.\n[21] Olof Mogren. C-RNN-GAN: Continuous recurrent\nneural networks with adversarial training. arXiv\npreprint arXiv:1611.09904 , 2016.\n[22] Tom Le Paine, Pooya Khorrami, Shiyu Chang, Yang\nZhang, Prajit Ramachandran, Mark A. Hasegawa-\nJohnson, and Thomas S. Huang. Fast WaveNet gen-\neration algorithm. arXiv preprint arXiv:1611.09482 ,\n2016.\n[23] George Papadopoulos and Geraint Wiggins. AI meth-\nods for algorithmic composition: A survey, a critical\nview and future prospects. In Proc. AISB Symposium\non Musical Creativity , pages 110–117, 1999.\n[24] Alec Radford, Luke Metz, and Soumith Chintala. Un-\nsupervised representation learning with deep convolu-\ntional generative adversarial networks. arXiv preprint\narXiv:1511.06434 , 2015.\n[25] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen\nLogeswaran, Bernt Schiele, and Honglak Lee. Genera-\ntive adversarial text to image synthesis. arXiv preprint\narXiv:1605.05396 , 2016.330 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017[26] Adam Roberts, Jesse Engel, Curtis Hawthorne, Ian Si-\nmon, Elliot Waite, Sageev Oore, Natasha Jaques, Cin-\njon Resnick, and Douglas Eck. Interactive musical im-\nprovisation with Magenta. In Proc. Neural Information\nProcessing Systems , 2016.\n[27] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba,\nVicki Cheung, Alec Radford, and Xi Chen. Improved\ntechniques for training GANs. In Proc. Advances in\nNeural Information Processing Systems , pages 2226–\n2234, 2016.\n[28] Zheng Sun, Jiaqi Liu, Zewang Zhang, Jingwen\nChen, Zhao Huo, Ching Hua Lee, and Xiao Zhang.\nComposing music with grammar argumented neu-\nral networks and note-level encoding. arXiv preprint\narXiv:1611.05416 , 2016.\n[29] Richard S Sutton and Andrew G Barto. Reinforcement\nlearning: An introduction , volume 1. MIT press Cam-\nbridge, 1998.\n[30] Peter M. Todd. A connectionist approach to algorith-\nmic composition. Computer Music Journal , 13(4):27–\n43.\n[31] A ¨aron van den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu. WaveNet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499 , 2016.\n[32] A ¨aron van den Oord, Nal Kalchbrenner, Lasse Espe-\nholt, Oriol Vinyals, Alex Graves, et al. Conditional\nimage generation with pixelCNN decoders. In Proc.\nAdvances in Neural Information Processing Systems ,\npages 4790–4798, 2016.\n[33] Elliot Waite, Douglas Eck, Adam Roberts, and\nDan Abolaﬁa. Project Magenta: Generating long-\nterm structure in songs and stories, 2016. https:\n//magenta.tensorflow.org/blog/2016/\n07/15/lookback-rnn-attention-rnn/ .\n[34] Yi-Hsuan Yang and Homer H. Chen. Music Emotion\nRecognition . CRC Press, 2011.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 331"
    },
    {
        "title": "A Study on LSTM Networks for Polyphonic Music Sequence Modelling.",
        "author": [
            "Adrien Ycart",
            "Emmanouil Benetos"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415018",
        "url": "https://doi.org/10.5281/zenodo.1415018",
        "ee": "https://zenodo.org/records/1415018/files/YcartB17.pdf",
        "abstract": "Neural networks, and especially long short-term memory networks (LSTM), have become increasingly popular for sequence modelling, be it in text, speech, or music. In this paper, we investigate the predictive power of simple LSTM networks for polyphonic MIDI sequences, using an empirical approach. Such systems can then be used as a music language model which, combined with an acoustic model, can improve automatic music transcription (AMT) performance. As a first step, we experiment with synthetic MIDI data, and we compare the results obtained in vari- ous settings, throughout the training process. In particu- lar, we compare the use of a fixed sample rate against a musically-relevant sample rate. We test this system both on synthetic and real MIDI data. Results are compared in terms of note prediction accuracy. We show that the higher the sample rate is, the better the prediction is, because self transitions are more frequent. We suggest that for AMT, a musically-relevant sample rate is crucial in order to model note transitions, beyond a simple smoothing effect.",
        "zenodo_id": 1415018,
        "dblp_key": "conf/ismir/YcartB17",
        "keywords": [
            "Neural networks",
            "long short-term memory networks (LSTM)",
            "sequence modelling",
            "polyphonic MIDI sequences",
            "music language model",
            "automatic music transcription (AMT)",
            "synthetic MIDI data",
            "real MIDI data",
            "note prediction accuracy",
            "musically-relevant sample rate"
        ],
        "content": "A STUDY ON LSTM NETWORKS FOR\nPOLYPHONIC MUSIC SEQUENCE MODELLING\nAdrien Ycart and Emmanouil Benetos\nCentre for Digital Music, Queen Mary University of London, UK\nfa.ycart, emmanouil.benetos g@qmul.ac.uk\nABSTRACT\nNeural networks, and especially long short-term memory\nnetworks (LSTM), have become increasingly popular for\nsequence modelling, be it in text, speech, or music. In\nthis paper, we investigate the predictive power of simple\nLSTM networks for polyphonic MIDI sequences, using an\nempirical approach. Such systems can then be used as a\nmusic language model which, combined with an acoustic\nmodel, can improve automatic music transcription (AMT)\nperformance. As a ﬁrst step, we experiment with synthetic\nMIDI data, and we compare the results obtained in vari-\nous settings, throughout the training process. In particu-\nlar, we compare the use of a ﬁxed sample rate against a\nmusically-relevant sample rate. We test this system both\non synthetic and real MIDI data. Results are compared in\nterms of note prediction accuracy. We show that the higher\nthe sample rate is, the better the prediction is, because self\ntransitions are more frequent. We suggest that for AMT, a\nmusically-relevant sample rate is crucial in order to model\nnote transitions, beyond a simple smoothing effect.\n1. INTRODUCTION\nRecurrent neural networks (RNNs) have become increas-\ningly popular for sequence modelling in various domains\nsuch as text, speech or video [7]. In particular, long short-\nterm memory networks (LSTMs) [10] have helped make\ntremendous progress in natural language modelling [15].\nThose so-called language models can, in turn, be com-\nbined with acoustic models to improve speech recogni-\ntion systems. Many recent improvements in this ﬁeld have\nstemmed from better language models [8].\nAutomatic music transcription (AMT) is to music what\nspeech recognition is to natural language: it is deﬁned as\nthe problem of extracting a symbolic representation from\nmusic signals, usually in the form of a time-pitch repre-\nsentation called piano-roll , or in a MIDI-like represen-\ntation. Despite being one of the most widely discussed\ntopics in music information retrieval (MIR), it remains an\nopen problem, in particular in the case of polyphonic mu-\nc\rAdrien Ycart and Emmanouil Benetos. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Adrien Ycart and Emmanouil Benetos. “A Study on\nLSTM Networks for Polyphonic Music Sequence Modelling”, 18th In-\nternational Society for Music Information Retrieval Conference, Suzhou,\nChina, 2017.sic [2]. While there has been extensive research on acous-\ntic models for music transcription, music language models\n(MLMs) have received little attention until quite recently.\nIn this paper, we propose a study on the use of LSTM\nneural networks for symbolic polyphonic music modelling,\nin the form of piano-roll representations. We evaluate\nthe impact of various parameters on the predictive perfor-\nmance of our system. Instead of relying on ever more com-\nplex architectures, we choose to use an LSTM with only\none layer, and see how each parameter inﬂuences the ﬁnal\nresult, namely, the number of hidden nodes, the learning\nrate, the sampling rate of the piano-roll, and doing data\naugmentation. We also compare the use of time frames of\nﬁxed length against the use of beat-quantised time frames\nof ﬁxed musical length (such as a 16th note). We evalu-\nate the predictive performance of our system in terms of\nprecision, recall and F-measure, and we monitor the evo-\nlution of these metrics throughout the learning process. We\nalso conduct proof-of-concept experiments on AMT by\npost-processing the output of an existing acoustic model\nwith our predictive models. We show that time-based time\nsteps yield better results in terms of prediction because\nself-transitions are more frequent. This results in a simple\nsmoothing effect when used in the context of transcription.\nOn the other hand, note-based time steps perform worse for\nprediction, but show interesting behaviour that might be\ncrucial for transcription, in particular the ability to model\nnote transitions and some basic rhythmic features. To the\nbest of our knowledge, such a study has not yet been done\nin the context of polyphonic music prediction.\nThe remainder of the paper is organised as follows. In\nsection 2, we review existing works on symbolic music se-\nquence modelling. In section 3, we describe the neural\nnetwork architecture chosen for the experiments. In sec-\ntion 4, we describe the two employed datasets, one made\nof synthetic MIDI data, the other of real-life MIDI data. In\nsection 6, we describe the various experiments performed\nfor prediction and their results. In section 7, we show pre-\nliminary results on the application of the language model\nin the context of AMT. Finally, in section 8, we discuss the\nresults obtained and their implications for future work.\n2. STATE OF THE ART\nAlthough MLMs have been discussed for quite a long time\n[14], they have not been speciﬁcally used in audio anal-\nysis until quite recently. Temperley [18] was one of the421ﬁrst to propose a joint probabilistic model for harmony,\nrhythm and stream separation, and suggested its use for\nAMT. Since then, several other audio analysis systems,\nsuch as [16], have used probabilistic models of high-level\nmusical knowledge in order to improve their performance.\nMore recently, some approaches have used neural net-\nworks to post-process the output of an acoustic model.\nIndeed, it seems that neural networks are more suitable\nto model polyphonic sequences compared to probabilistic\nmodels such as hidden Markov models (HMMs). In [4], a\nneural network architecture combining a RNN with a Re-\nstricted Boltzman Machine (RBM) was proposed to esti-\nmate at each time-step a pitch distribution, given the pre-\nvious pitches. This architecture was later integrated in var-\nious systems. In [17], it was integrated in an end-to-end\nneural-network for multi-pitch detection in piano music,\ncoupled with a variety of neural-network-based acoustic\nmodels. In all these models, the time-step is of the order\nof 10 ms, which is small compared to the typical duration\nof a music note. Moreover, this time-step is constant, and\ndoes not depend on the tempo of the input music signal.\nSome systems have also modelled symbolic music se-\nquences for other purposes. Pachet et al. [9] proposed an\narchitecture consisting of four joint neural networks in or-\nder to generate Bach chorales. In [11], another architecture\nusing reinforcement learning to enforce musical rules in a\nRNN was proposed for music generation.\nAll the above neural architectures rely on sophisticated\ncombinations of neural networks, and have many param-\neters, which means that they need a lot of training data,\nand can be prone to over-ﬁtting. In this study, we focus\non a simple architecture, and try to build from that us-\ning an experimental method to assess the importance of\nvarious hyper-parameters. A study similar to the present\nhas been conducted in [13] on chord sequence modelling\n(thus on modelling monophonic sequences instead of poly-\nphonic ones). In this previous study, the advantage of\nRNNs over HMMs is questioned in the context of chord\nsequence modelling. In particular, it is argued that when\nthe frame rate is high (order of 10 fps), the RNN only has\na smoothing effect, and thus is no more efﬁcient than sim-\npler models such as an HMM. On the other hand, it is sug-\ngested that on the chord-level (ie. one symbol per chord,\nno matter how long), the RNN used is signiﬁcantly better\nthan an HMM. We aim at investigating similar questions in\nthe context of polyphonic note sequence modelling in the\ncurrent study.\n3. MODEL\nIn this section, we describe the model we used in the exper-\niments. This model is trained to predict the pitches present\nin the next time frame of a piano-roll, given the previous\nones.\n3.1 Data Representation\nThe input is a piano-roll representation, in the form of an\n88\u0002Tmatrix M, where Tis the number of timesteps, and\n88 corresponds to the number of keys on a piano, betweenMIDI notes A0 and C8. Mis binary, such that M[p; t] = 1\nif and only if the pitch pis active at the timestep t. In\nparticular, held notes and repeated notes are not differen-\ntiated. The output is of the same form, except it only has\nT\u00001timesteps (the ﬁrst timestep cannot be predicted since\nthere is no previous information). We use two different\ntimesteps:\n\u000fa ﬁxed time step of 10 milliseconds, that we refer to\nastime-based\n\u000fa variable time step with a ﬁxed musical length of a\nsixteenth note, referred to as note-based .\n3.2 Network Architecture\nOur primary goal is to study the inﬂuence of various pa-\nrameters, namely the number of hidden nodes, the learning\nrate, the use of data augmentation, and the time steps used.\nIn order to assess the inﬂuence of those parameters as ac-\ncurately as possible, without being inﬂuenced by other pa-\nrameters, we deliberately choose to use the most simple\nLSTM architecture possible. In particular, we choose not\nto use multiple layers, nor to use dropout or any other reg-\nularisation method during training. These will be investi-\ngated in future work.\nWe thus use an LSTM with 88 inputs, one single hid-\nden layer, and 88 outputs. The number of hidden nodes is\nchosen among: 64;128;256;512. The network is trained\nusing the Adam optimiser [12], using the cross-entropy be-\ntween the output of the network and the ground truth as\ncost function. The learning rate is kept constant, and is\nchosen among: 0:01;0:001;0:0001 .\nThe output of the network is then sent through a sig-\nmoid, and thresholded to obtain a binary piano-roll. The\nthreshold is determined by choosing the one that gives the\nbest results on the validation dataset (see section 4).\n4. DATASETS\nWe use two different datasets for training and testing our\nmodels. One is a synthetic dataset, the other is a dataset\nmade of real music pieces. Both datasets were split into\ntraining, validation and test datasets with the following re-\nspective proportions: 70%-10%-20%.\n4.1 Synthetic MIDI Dataset\nThe synthetic MIDI dataset used in this study consists of\ncombinations of notes and chords in the C major key. It\ncontains only notes on the C major scale, between C3 and\nC5. The chords are either a note and the note a third above\n(major or minor, such that the second note is also in C ma-\njor), or a note, the note a third above, and the note a ﬁfth\nabove. All generated ﬁles are 3sec long, with a tempo of\n60, so each ﬁle is 3-quarter-notes long. All notes have a\nduration multiple of a quarter note, so note changes can oc-\ncur after 1 second, 2 seconds, both or neither. We take all\npossible combinations of 3 notes or chords and we allow\nrepetition. When a note or a chord is repeated we create\ntwo distinct ﬁles, one corresponding to the note being held,\none corresponding to the note being played again. Overall,422 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017the dataset contains more than 36,000 ﬁles, representing\n30 hours of data and will be referred to as Synth dataset .\n4.2 Piano-midi.de Dataset\nWe use the Piano-midi.de dataset1as real-world MIDI\ndata. This dataset currently holds 307 pieces of classical\npiano music from various composers. It was made by man-\nually editing the velocities and the tempo curve of quan-\ntised MIDI ﬁles in order to give them a natural interpre-\ntation and feeling. This mode of production is the main\nreason why we chose it: it sounds real, with an expressive\ntiming, and at the same time, the rhythmic ground truth is\nreadily available. We can thus easily compute note-based\ntime steps, without having to rely on a beat and meter de-\ntection algorithm.\nThis dataset holds pieces of very different durations\n(from 20 seconds to 20 minutes). In order to avoid ex-\ncessive zero-padding for neural network training and to\nbe more computationally efﬁcient, we only keep the ﬁrst\nminute from each ﬁle (and we zero-pad the shorter ﬁles).\nThe resulting dataset is 5 hours long, and will be referred\nto as the Piano dataset . We also augment our dataset by\ntransposing every piece in all keys from 7 semitones below\nto 6 semitones above. This increases the size of the dataset\n12-fold, up to 60 hours. That way, all tonalities are equally\nrepresented, without shifting the range of notes too much.\n5. EVALUATION METRICS\nTo evaluate the performance of our system, we compute\nseveral metrics following the MIREX Multiple-F0 Estima-\ntion task [1], namely the precision, recall and F-measure.\nThose metrics are computed for each ﬁle, and then aver-\naged over a dataset. The progress throughout learning is\ncomputed on the validation dataset, and the performance\nof the trained model is computed on the test dataset.\n6. PREDICTION\nIn this section, we compare the results obtained in various\nconﬁgurations, both quantitatively and qualitatively.\n6.1 Number of Hidden Nodes and Learning Rate\nWe trained on the Synth dataset a series of mod-\nels, with various numbers of hidden nodes in the hid-\nden layer ( nhidden ), and various learning rates ( learn-\ningrate). We tried all possible combinations with\nnhidden2 f 64;128;256;512gand learning rate2\nf0:0001;0:001;0:01g. We trained each model for\n50 epochs, with a batch size of 50. All the im-\nplementations were made in Python, using Tensor-\nﬂow [6]. The code necessary for the experiments\ncan be found at: http://www.eecs.qmul.ac.uk/\n˜ay304/code/ismir17 .\nIn each case, the model converges to the same state:\nat epoch 50, all the models get the same precision, recall\nand F-measure with 10\u00002precision. The only difference\n1http://piano-midi.de/among all the models is the convergence speed. Similar\nobservations were made for the other numbers of hidden\nnodes.\nQuite expectedly, the parameter that has the most inﬂu-\nence on convergence speed is the learning rate. Generally\nspeaking, the larger the number of nodes is, the quicker the\nmodel converges (we could not compare when the learning\nrate is 0.01 since all the models converge in one epoch).\nThose empirical observations are consistently observed in\nall the other experiments as well (on the Piano dataset, with\nor without note-based time steps, with or without data aug-\nmentation).\nWhen inspecting the output of the network before go-\ning through the sigmoid, we can notice some interesting\nfeatures. All the notes that are outside the scale of C (that\nnever appear in the training set) have a very low output,\nshowing that the network is able to learn which notes might\nappear. This can be double-edged: notes outside the key\nare not mistakenly detected, but if they appear (which hap-\npens), they will not be detected either. In Section 7 we\nattempt to run this model on a real-life input ﬁle, and those\nﬁndings are conﬁrmed: the prediction clearly masks out\nevery note that was not in the set of notes seen during train-\ning.\nConsidering the results of this preliminary experiment,\nwe decide to keep for the rest of the experiments only\nnhidden2[128;256] and learning rate2[0:001;0:01].\nIndeed, 512 hidden nodes is too heavy computationally\n(around 20% longer training time compared to all the other\nconﬁgurations) without any clear improvement over 256\nnodes. Similarly, a learning rate of 0.0001 converges too\nslowly compared to the others, with no noticeable advan-\ntage in the end result. We nevertheless choose to keep sev-\neral models, not only the best one, because the best model\non this dataset will not necessarily be the best one on an-\nother.\n6.2 Data Augmentation\nOn the Piano dataset, we compare the performance of\nthe model trained on the original 5-hour dataset, and on\nthe augmented 60-hour dataset. The evolution of the F-\nmeasure on the validation dataset with and without data\naugmentation can be found in Figure 1. Results show that\ndata augmentation improves greatly the results. All mod-\nels trained on augmented data converge more quickly in\nterms of number of epochs, which is understandable since\n12 times more data is processed at each epoch. However,\nin both cases, the results obtained after 50 epochs are ap-\nproximately the same in terms of metrics.\n6.3 Time Step\nWe compare the behaviour of the network when using\ntime-based and note-based time steps, on both datasets. A\ncomparison of the evolution of the F-measure on the Piano\nvalidation dataset with time-based or note-based time steps\ncan be found in Figure 1.\nWith time-based time steps, we ﬁnd that all the models\nseem to achieve similar results: with data augmentation, allProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 423Figure 1 . Comparison of the evolution of the F-measure across epochs, on the Piano validation dataset, with time-based or\nnote-based timesteps, with or without data augmentation. A threshold of 0.5 is used.\nF-Measure Precision Recall\nWithout augmentation\n128, 0.001 0.451 0.409 0.509\n256, 0.001 0.513 0.484 0.549\n128, 0.01 0.548 0.536 0.560\n256, 0.01 0.553 0.544 0.562\nWith augmentation\n128, 0.001 0.558 0.557 0.560\n256, 0.001 0.571 0.552 0.592\n128, 0.01 0.597 0.61 0.588\n256, 0.01 0.601 0.615 0.592\nTable 1 . Prediction performance computed with note-\nbased time steps on the Piano test dataset.\nthe models achieve a F-measure score of 0.966. Without\ndata augmentation, the models trained with a learning rate\nof 0.01 achieve the same performance, with a learning rate\nof 0.001, the 128-hidden-node model achieves 0.917, and\nthe 256-hidden-node model achieves 0.944. This might be\ndue to the fact that they haven’t fully converged after 50\nepochs. All those scores were computed with a threshold\nof 0.5.\nWe also compute the precision, recall and F-measure on\nthe Piano test dataset with note-based time steps. These re-\nsults can be found in Table 1. We observe that this time,\nhigher learning rate, higher number of nodes and data aug-\nmentation not only lead to quicker convergence, but also to\nbetter results.\nFor both datasets, the predictive accuracy is better in\ntime-based conﬁgurations, since the frame rate is much\nhigher, and thus there are more self-transitions (ie. notes\nare prolongated from the previous time steps). It seems in-\ndeed that in both cases, the system is uncertain when there\nare note changes, but learns to repeat the ongoing notes.The difference in performance can thus be attributed to the\nfact that note changes are much more frequent in the note-\nbased case (once every 4 time steps versus once every 100\ntimesteps for the Synth dataset).\nHowever, the note-based model shows very interesting\nbehaviour at the uncertain time steps (ie. at each beat).\nOn the Synth dataset, when the note changes, it gives a\nsmall probability to every note of the scale (the notes that\nmight appear in the next frame), and rules out all the out-\nof-scale notes. Moreover, even when the note is kept for\nmore than one beat, the model still shows the same “un-\ncertain” behaviour, which does not happen with the time-\nbased model. This is an error (which partly explains the\nworse scores), because the note should be held, but it\nhas some very interesting consequences in terms of mu-\nsic modelling. This shows that the note-based model has\nlearned that periodically, notes have a strong probability to\nchange. This can be related to the rhythmic structure of\nmusic, as note changes are more frequent on strong metric\npositions. An example prediction output before threshold-\ning is shown in Figure 2. We can see those “uncertain”\ntime-steps in position 3 and 7, which correspond to time-\nsteps 4 and 8 in the input (ie note changes).\nWe also ﬁnd this behaviour with the Piano dataset, how-\never only appearing with data augmentation. It is not clear\nif this is speciﬁc to data augmentation, or if it is simply\nbecause models without data augmentation were under-\ntrained. In this case, the “uncertain” behaviour occurs at\nevery eighth note, and with stronger probabilities at each\nquarter note. This suggests that the model behaves this\nway at the smallest metrical level, and not only at strong\nmetrical positions. This might be a problem in the future,\nsince it might encourage transitions too frequently. How-\never, a small probability is only given to notes of the cor-424 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017Figure 2 . The prediction system output (n hidden = 256,\nlearning rate=0.01) with note-based time steps after going\nthrough the sigmoid, before thresholding. The ground truth\nis: E3, C4E4G4, F4A4. At each note change, a small prob-\nability is given to all notes in C major scale.\nrect scale, which shows that the model is able to get the\ntonal context of a piece to some extent. An example output\nbefore thresholding is shown in Figure 3.\nFigure 3 . The output of the prediction system (n hidden\n= 256, learning rate=0.01) with note-based time steps af-\nter going through the sigmoid, before thresholding. The\nground truth is Chopin’s Prelude, No. 7, Op. 28 in A Ma-\njor. At each eighth note, a small probability is given to\nsome notes in A major, the tonality of the piece.\n7. AUDIO TRANSCRIPTION\nA preliminary experiment on assessing the potential of pre-\ndiction models in the context of AMT is carried out. For\nthis experiment, we use a single piece taken from the Piano\ndataset: Chopin’s Prelude No. 7, Op. 28 in A Major.\n7.1 Acoustic Model\nFor the experiment on integrating AMT with polyphonic\nmusic prediction, we use the system of [3], which is based\non Probabilistic Latent Component Analysis. The system\ndecomposes an input spectrogram into several probability\ndistributions for pitch activations, instrument contributions\nand tuning deviations, using a dictionary of pre-extracted\nspectral templates. For this experiment, a piano-speciﬁc\nsystem is used, trained using isolated notes from the MAPS\ndatabase [5]. The output of the acoustic model is a poste-\nriogram of pitch activations over time.\n7.2 Method\nWe synthesise the MIDI ﬁle with GarageBand, using the\ndefault Steinway Grand Piano soundbank. We analyse 3\ndifferent audio ﬁles:\nFigure 4 . The ﬁrst 20 seconds of the thresholded output of\nthe baseline AMT system, compared with the ground truth.\n\u000fThe full audio ﬁle, with expressive timing.\n\u000fThe right-hand of the piece, transposed in C. In this\ncase, predictive models trained both on the Synth\nand Piano dataset are evaluated.\n\u000fThe full audio ﬁle, with quantised timing. The out-\nput of the transcription system is then downsampled\nto obtain a time step of a 16th note.\nWe take the posteriogram output by the previously de-\nscribed acoustic model, and feed it to various polyphonic\nprediction models, in various conditions:\n\u000fThe raw posteriogram, with positive values theoreti-\ncally unbounded ( rawpost)\n\u000fThe raw posteriogram thresholded in order to have a\nbinary piano-roll ( rawpiano )\nThe output of our predictive model is then thresholded\nusing the value determined on the validation dataset in the\nexperiments described in Section 6.3. The resulting piano-\nroll is compared to the ground truth using the accuracy\nmetrics described in Section 5. An example of output of\nthe baseline transcription system is shown in Figure 4.\n7.3 Results\nResults in terms of multi-pitch detection are shown in Ta-\nble 2. Although we tested every conﬁguration with all our\nmodels, we only report the results of the most meaningful\nexperiments.\nIn the vast majority of cases, the results with the pre-\ndictive model are below those of the acoustic model only,\nwithout post-processing. The only cases where the post-\nprocessing step improves the results is when the prediction\nis made on the whole piece, with time-based time steps,\ninrawpiano conﬁguration. Otherwise, the results are at\nbest equivalent to those of the baseline system. In the case\nwhere the results are improved, we inspect what improve-\nments are made by the predictive model. It seems that the\nonly improvements were few isolated frames that are tem-\nporally smoothed. We do not notice any missing notes be-Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 425F-Measure Precision Recall\nFull audio, raw piano\nBaseline 0.455 0.960 0.299\n128, 0.001 0.458 0.938 0.303\n256, 0.001 0.458 0.941 0.303\n128, 0.01 0.460 0.959 0.303\n256, 0.01 0.460 0.961 0.303\nRight hand in C,\nrawpost, Synth\nBaseline 0.670 0.898 0.535\n128, 0.001 0.556 0.955 0.393\n256, 0.001 0.607 0.966 0.442\n128, 0.01 0.522 0.834 0.380\n256, 0.01 0.527 0.877 0.377\nFull note-based, raw piano\nBaseline 0.526 0.963 0.361\n128, 0.001 0.434 0.624 0.332\n256, 0.001 0.440 0.651 0.332\n128, 0.01 0.478 0.852 0.332\n256, 0.01 0.481 0.875 0.332\nTable 2 . Some results on transcription from audio, com-\npared to the output of the baseline acoustic model.\ning added, and very few spurious notes are removed.\nWhen using the Synth-dataset-trained models on the\nright hand transposed in C, the results are mixed. The pre-\ncision measure is improved, due to the fact that many spu-\nrious notes are removed. On the other hand, some notes\nwent completely missing because they were not in the C\nmajor scale, which leads to a lower recall score. Overall,\nthe F-measure is lower than that of the acoustic model .\nWhen using frame-based time steps, in every conﬁgura-\ntion, the results are worse. We have identiﬁed two reasons\nfor that. The ﬁrst is that sometimes, the system would add\nevenly distributed noise at the beginning of the prediction.\nThis is probably due to the fact that the network takes a\nfew frames to build a memory good enough to make cor-\nrect predictions. More training removes that problem (the\nproblem does not appear with models trained with a learn-\ning rate of 0.01). The second reason is that the system has\nsome latency: a note is only activated one frame after it is\nseen in the input, and it is only deactivated one frame after\nit disappears of the input. When comparing the output of\nthe system shifted one frame back with the output of the\nbaseline system, the results were very close, and in some\ncases, identical (though never better).\n8. DISCUSSION\nIn this study, we compare the inﬂuence of various parame-\nters of an LSTM network on modelling polyphonic music\nsequences with respect to the training process and predic-\ntion performance. Those parameters are: the learning rate,\nthe number of hidden nodes, the use of data augmenta-\ntion by transposition, and the use of time-based time steps\nagainst note-based time steps. We found that with a given\ntime step and a given dataset, the learning rate is the most\nimportant factor for learning speed, and that the more hid-\nden nodes there are, the quicker the convergence is. We\nalso found that data augmentation greatly improves both\nthe convergence speed and the ﬁnal performance.When it comes to the choice of the time steps, it appears\nthat time-based time steps yield a better prediction perfor-\nmance, because self transitions are more frequent. On the\nother hand, note-based time steps seem to show better mu-\nsical properties. When trained on synthetic data containing\nonly notes of the scale, it seems rather evident that notes\nthat are have never been obeserved are very unlikely. More\ninterestingly, when trained with real data in all tonalities,\nthe system can still detect the scale of the piece : we can\nsee with the example in Figure 3 that only notes of the cor-\nrect tonality are given a some probability. We can also see\nthat the system has learned the places where note changes\nmight occur, and that the note changes are more frequent\nat beat positions than at half-beat positions.\nWe also use our system to post-process the output of\nan acoustic model for multi-pitch detection, in a proof-of-\nconcept experiment. The ﬁrst thing that we can notice from\nthis experiment is that a good prediction performance does\nnot necessarily translate to a good audio transcription per-\nformance. However, the order of performance for predic-\ntion seem to be kept for transcription: models with more\nnodes and higher learning rate tend to perform better.\nThe poor performance of our the predictive model for\nimproving AMT performance is understandable: the in-\nput presented to the system in the transcription process is\nvery different from those the models were trained on. Fu-\nture work will include training predictive models not with\npiano-rolls, but with acoustic model posteriograms.\nFrom all the above experiments, we can conclude that\nwith time-based time steps, what the LSTM does is a sim-\nple smoothing, albeit a good one, since it improves tran-\nscription performance in some cases. The very fact that\npost-processing the output of the acoustic model with our\nsystem can improve the transcription performance, even\nthough our language model was trained on a completely\ndifferent kind of data, shows that it has in fact not learned\nmuch from the data it was fed, except temporal smoothing\nsimilar to e.g. a median ﬁlter. Since we aim at modelling\nthe complex vertical and horizontal dependencies that exist\nwithin music, this behaviour is not sufﬁcient.\nOn the other hand, we found some very interesting fea-\ntures in the output of the note-based models: they are able\nto learn when note changes might occur and what note\nmight be activated which is very promising in terms of\npolyphonic music modelling. The downside of such mod-\nels is that they would rely on meter detection algorithms\nwhen applied to audio, which might lead to error propaga-\ntion. Future work will focus on investigating the possibili-\nties of those models in the context of AMT, assuming that\nthe meter and tempo are given as a ﬁrst step.\nFinally, we will extend this study in future work by\ngradually increasing the complexity of our model, and\nstudying how the performance varies. In particular, we will\nstudy the result of adding more hidden nodes, and using\nmore sophisticated regularisation techniques. We will also\nfurther investigate the results by visualising the learned pa-\nrameters, as well as the activations of the hidden nodes.426 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20179. ACKNOWLEDGEMENTS\nA. Ycart is supported by a QMUL EECS Research Stu-\ndentship. E. Benetos is supported by a RAEng Research\nFellowship (RF/128).\n10. REFERENCES\n[1] Mert Bay, Andreas F. Ehmann, and J. Stephen Downie.\nEvaluation of Multiple-F0 Estimation and Tracking\nSystems. In 10th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 315–320,\n2009.\n[2] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and\nA. Klapuri. Automatic music transcription: challenges\nand future directions. Journal of Intelligent Informa-\ntion Systems , 41(3):407–434, 2013.\n[3] E. Benetos and T. Weyde. An efﬁcient temporally-\nconstrained probabilistic model for multiple-\ninstrument music transcription. In 16th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , pages 701–707, 2015.\n[4] N. Boulanger-Lewandowski, P. Vincent, and Y . Ben-\ngio. Modeling Temporal Dependencies in High-\nDimensional Sequences: Application to Polyphonic\nMusic Generation and Transcription. 29th Interna-\ntional Conference on Machine Learning , pages 1159–\n1166, 2012.\n[5] V . Emiya, R. Badeau, and B. David. Multipitch esti-\nmation of piano sounds using a new probabilistic spec-\ntral smoothness principle. IEEE Transactions on Au-\ndio, Speech and Language Processing , 18(6):1643–\n1654, August 2010.\n[6] M. Abadi et al. TensorFlow: Large-scale machine\nlearning on heterogeneous systems, 2015. Software\navailable from tensorﬂow.org.\n[7] I. Goodfellow, Y . Bengio, and A. Courville. Deep\nlearning . MIT Press, 2016.\n[8] A. Graves, A. Mohamed, and G. Hinton. Speech recog-\nnition with deep recurrent neural networks. In IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 6645–6649. IEEE,\n2013.\n[9] G. Hadjeres and F. Pachet. DeepBach: a steerable\nmodel for Bach chorales generation. arXiv preprint\narXiv:1612.01010 , 2016.\n[10] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural computation , 9(8):1735–1780, 1997.\n[11] N. Jaques, S. Gu, R. E. Turner, and D. Eck. Tuning Re-\ncurrent Neural Networks with Reinforcement Learn-\ning.5th International Conference on Learning Repre-\nsentations (ICLR) , pages 1722–1728, 2017.[12] D. P. Kingma and J. Ba. Adam: A method for stochas-\ntic optimization. In 3rd International Conference on\nLearning Representations (ICLR) , 2015.\n[13] F. Korzeniowski and G. Widmer. On the Futility of\nLearning Complex Frame-Level Language Models for\nChord Recognition. In AES International Conference\non Semantic Audio , 2017.\n[14] F. Lerdahl and R. Jackendoff. A Generative Theory of\nTonal Music . MIT Press, 1983.\n[15] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock `y, and\nS. Khudanpur. Recurrent neural network based lan-\nguage model. In Interspeech , volume 2, page 3, 2010.\n[16] S. A. Raczy ´nski, E. Vincent, and S. Sagayama. Dy-\nnamic Bayesian networks for symbolic polyhonic pitch\nmodeling. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 21(9):1830 – 1840, 2013.\n[17] S. Sigtia, E. Benetos, and S. Dixon. An end-to-end neu-\nral network for polyphonic piano music transcription.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 24(5):927–939, May 2016.\n[18] D. Temperley. A Uniﬁed Probabilistic Model for Poly-\nphonic Music Analysis. Journal of New Music Re-\nsearch , 38(1):3–18, 2009.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 427"
    },
    {
        "title": "Exploring Tonal-Dramatic Relationships in Richard Wagner&apos;s Ring Cycle.",
        "author": [
            "Frank Zalkow",
            "Christof Weiß",
            "Meinard Müller"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1415760",
        "url": "https://doi.org/10.5281/zenodo.1415760",
        "ee": "https://zenodo.org/records/1415760/files/ZalkowWM17.pdf",
        "abstract": "Richard Wagner’s cycle Der Ring des Nibelungen, con- sisting of four music dramas, constitutes a comprehensive work of high importance for Western music history. In this paper, we indicate how MIR methods can be applied to ex- plore this large-scale work with respect to tonal properties. Our investigations are based on a data set that contains 16 audio recordings of the entire Ring as well as extensive annotations including measure positions, singer activities, and leitmotif regions. As a basis for the tonal analysis, we make use of common audio features, which capture local chord and scale information. Employing a cross- version approach, we show that global histogram repre- sentations can reflect certain tonal relationships in a robust way. Based on our annotations, a musicologist may eas- ily select and compare passages associated with dramatic aspects, for example, the appearance of specific characters or the presence of particular leitmotifs. Highlighting and investigating such passages may provide insights into the role of tonality for the dramatic conception of Wagner’s Ring. By giving various concrete examples, we indicate how our approach may open up new ways for exploring large musical corpora in an intuitive and interactive way.",
        "zenodo_id": 1415760,
        "dblp_key": "conf/ismir/ZalkowWM17",
        "keywords": [
            "Richard Wagner",
            "Der Ring des Nibelungen",
            "music dramas",
            "Western music history",
            "MIR methods",
            "tonal properties",
            "audio recordings",
            "measure positions",
            "singer activities",
            "leitmotif regions"
        ],
        "content": "EXPLORING TONAL-DRAMATIC RELATIONSHIPS IN RICHARD\nWAGNER’S RING CYCLE\nFrank Zalkow, Christof Weiß, Meinard M ¨uller\nInternational Audio Laboratories Erlangen, Germany\nffrank.zalkow,christof.weiss,meinard.mueller g@audiolabs-erlangen.de\nABSTRACT\nRichard Wagner’s cycle Der Ring des Nibelungen , con-\nsisting of four music dramas, constitutes a comprehensive\nwork of high importance for Western music history. In this\npaper, we indicate how MIR methods can be applied to ex-\nplore this large-scale work with respect to tonal properties.\nOur investigations are based on a data set that contains 16\naudio recordings of the entire Ring as well as extensive\nannotations including measure positions, singer activities,\nand leitmotif regions. As a basis for the tonal analysis,\nwe make use of common audio features, which capture\nlocal chord and scale information. Employing a cross-\nversion approach, we show that global histogram repre-\nsentations can reﬂect certain tonal relationships in a robust\nway. Based on our annotations, a musicologist may eas-\nily select and compare passages associated with dramatic\naspects, for example, the appearance of speciﬁc characters\nor the presence of particular leitmotifs. Highlighting and\ninvestigating such passages may provide insights into the\nrole of tonality for the dramatic conception of Wagner’s\nRing . By giving various concrete examples, we indicate\nhow our approach may open up new ways for exploring\nlarge musical corpora in an intuitive and interactive way.\n1. INTRODUCTION\nOriginating in late 16th-century Florence, opera evolved as\na central art form of Western culture [1]. Intended as a re-\nturn to ancient Greek dramatic style, the idea of accompa-\nnied singing ( monody ) laid the ground work for two central\nsinging styles of traditional opera: speech-like recitatives ,\nwhich serve as a means for developing the plot, and arias ,\nwhich emphasize the characters’ feelings through cantabile\nmelodic lines. For centuries, the structure of opera was de-\ntermined by alternating such individual pieces of music,\nwhich is also known as number opera . In the mid-19th\ncentury, Richard Wagner developed a novel approach to\noperatic composition. According to his theoretical writings\nsuch as Oper und Drama [15], the “drama of the future”\nshould integrate all forms of art (“Gesamtkunstwerk”). He\nbroke with the conventions of number opera in favor of a\n© Frank Zalkow, Christof Weiß, Meinard M ¨uller. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Frank Zalkow, Christof Weiß, Meinard M ¨uller. “Ex-\nploring Tonal-Dramatic Relationships in Richard Wagner’s Ring Cycle”,\n18th International Society for Music Information Retrieval Conference,\nSuzhou, China, 2017.\nDas Rheingold\nWWV 86 A\n3897 measuresDie Walküre\nWWV 86 B\n5322 measuresSiegfried\nWWV 86 C\n6682 measuresGötterdämmerung\nWWV 86 D\n6040 measures(a)\n1 50 100 150 200 250 300B\n/accidentals.flatmD\n/accidentals.flatFmA\n/accidentals.flatCmE\n/accidentals.flatGmB\n/accidentals.flatDmFAmCEmGBmDF\n/accidentals.sharpmAC\n/accidentals.sharpmEG\n/accidentals.sharpmBD\n/accidentals.sharpmF\n/accidentals.sharpChords(b)\n1 50 100 150 200 250 300\nTime (measures)−5−4−3−2−10+1+2+3+4+5+6Scales(d)\n(c)\n(e)Figure 1 . Overview of Wagner’s Ring and schematic de-\nscription of the histogram extraction. (a)The four parts of\ntheRing with catalogue number and length in measures.\nThe measures under consideration are highlighted in color.\n(b)A local chord representation with time given in mea-\nsures. (c)A histogram summarizing the chord represen-\ntation. (d)A local scale representation. (e)A histogram\nsummarizing the scale representation.\nunity of prose and music with a steady musical ﬂow, which\nis often referred to as through-composed style or “endless\nmelody” since it lacks both interruptions and exact repe-\ntitions. A central aspect of Wagner’s operatic style is the\nfrequent use of leitmotifs—short musical ideas associated\nwith a character, a place, an item, or with emotional cate-\ngories, among others.\nOne of the most impressive realizations of these ideas\nis the tetralogy Der Ring des Nibelungen , an extensive\nwork cycle of four music dramas created between 1848\nand 1874. In Figure 1a, we show an overview of the Ring ’s\nparts. A typical performance lasts 14–15 hours in total,\nwhich is demanding for listeners as well as performers.\nFurthermore, the through-composed form may appear less\nstructured to the naive listener than a traditional number\nopera. Therefore, navigation and visualization tools are\nparticularly useful for exploring this large-scale work. In\nthis paper, we present such visualizations and demonstrate\ntheir beneﬁt for musicological research.\nThe Ring has already obtained some attention in the\nﬁeld of Music Information Retrieval (MIR). Page et al. [9]\npresent a toolkit for annotating musical performances in a642case study based on the Ring . In other studies [17, 21],\nWagner’s tetralogy serves as a basis for investigating the\nreliability of measure annotations. Concerning leitmotifs,\nM¨ullensiefen et al. [7] consider the human memory recall\ntask. They found that the distance of chroma features re-\nlates to the perceived novelty of a leitmotif.\nIn this paper, we approach the Ring from the perspec-\ntive of tonal analysis. To this end, we perform experiments\non the basis of common tonal features extracted from dif-\nferent recordings. These audio features capture the local\npresence of chord [12] and scale structures [18]. Figure 1\nillustrates such tonal representations with respect to ma-\njor and minor triads (Figure 1b) and to diatonic scales1\n(Figure 1d) for the beginning of the ﬁrst part, Das Rhein-\ngold. Dark values indicate a higher salience of the respec-\ntive tonal structures. The prelude of this piece strongly\nrelies on a single E\n/accidentals.flat major triad, which corresponds to\nhomogeneous regions in both feature sequences. In gen-\neral, Wagner’s music is known to present a rich vocabulary\nof different chord and scale types. However, though be-\ning an oversimpliﬁcation, in this case study we consider\nonly the major and minor chords as well as diatonic scales.\nThese simple tonal structures still explain a relevant span\nof harmonically stable passages [19]. As a central tech-\nnique in this paper, we summarize such chord and scale\nfeature sequences in histogram representations as shown\nin Figures 1c/e. Visualizing these histograms may illus-\ntrate global trends in the tonal conception of the music.\nFor computing the histograms, we select and compare pas-\nsages associated with dramatic aspects, for example, the\nactivity of certain characters or the presence of certain leit-\nmotifs. This way, we show the ability of our visualizations\nto explore interesting tendencies and to study relationships\nbetween tonal and dramatic aspects within the Ring cycle.\nIn the ﬁeld of MIR, histogram-based features have\nbeen extensively used for tasks such as genre recogni-\ntion [13], tuning estimation [5], and other classiﬁcation\ntasks [10, 14]. Moreover, the visualization of music pieces\nis an important topic in MIR. Wu and Bello [20] present an\napproach for visualizing musical structure. Sapp’s scape\nplot representations [11] have come out useful for illustrat-\ning harmony analysis results in a hierarchical way. In [3],\nG´omez and Bonada demonstrate several visualization tech-\nniques concerning tonal aspects of musical pieces.\nTypically, tonal analysis is performed on the basis of\nmusical scores. In Section 2, we discuss why an analysis\nbased on audio recordings may be beneﬁcial for the Ring\nscenario. Having several recorded performances (versions)\nof the Ring allows us to employ a cross-version approach\nin order to stabilize the audio-based tonal representations.\nKonz et al. [6] show that visualizing the cross-version con-\nsistency of analysis results suppresses aspects of particular\nperformances and, thus, emphasizes aspects relevant to the\nmusical work in general.\nBased on previously mentioned works, we present the\nhistogram visualizations as a novel way to explore the\n1We refer to the diatonic scales according to the respective accidentals.\nFor example, +1 corresponds to a scale with 1\n/accidentals.sharp (G major or E minor scale)\nwhereas −2 indicates a scale with 2\n/accidentals.flat (B\n/accidentals.flatmajor or G minor scale).No. Conductor Recording hh:mm:ss\n1 Barenboim 1991–92 14:54:55\n2 Boulez 1980–81 13:44:38\n3 B ¨ohm 1967–71 13:39:28\n4 Furtw ¨angler 1953 15:04:22\n5 Haitink 1988–91 14:27:10\n6 Janowski 1980–83 14:08:34\n7 Karajan 1967–70 14:58:08\n8 Keilberth/Furtw ¨angler 1952–54 14:19:56\n9 Krauss 1953 14:12:27\n10 Levine 1987–89 15:21:52\n11 Neuhold 1993–95 14:04:35\n12 Sawallisch 1989 14:06:50\n13 Solti 1958–65 14:36:58\n14 Swarowsky 1968 14:56:34\n15 Thielemann 2011 14:31:13\n16 Weigle 2010–12 14:48:46\nTable 1 . Performances of the Ring used for this paper. In\nNo. 8, Furtw ¨angler only conducts Die Walk ¨ure(different\nfrom No. 4), the other parts are conducted by Keilberth.\nRing . The main contribution is the application of MIR\ntechniques in an exploratory manner for highlighting inter-\nesting trends and relations within this large-scale work cy-\ncle. The remainder of the paper is structured as follows. In\nSection 2 we explain the data set and describe the charac-\nteristics of our annotations. Then, we shortly recapitulate\nthe extraction of local chord and scale information and ex-\nplain the histogram computation (Section 3). In the central\nSection 4, we discuss these histograms in detail by means\nof several concrete examples relating to different dramatic\naspects of the Ring . Section 5 concludes our paper.\n2. DATA SET AND ANNOTATIONS\nFor an automated tonal analysis, we have to rely on a spe-\nciﬁc representation of a piece of music. Typically, mu-\nsicologists perform such analyses in a manual fashion on\nthe basis of musical scores. To automate this process, mu-\nsical scores need to be accessible in a machine-readable\nform (symbolic data) in high quality, which is a rare case\nfor Western classical music with a large instrumentation.\nTo overcome this problem, Optical Music Recognition\n(OMR) techniques are usually employed, which cannot\nprovide satisfactory results in many situations [2] so that\ntime-consuming manual correction steps are required. As\nan alternative, tonal analysis can be performed on the ba-\nsis of audio recordings, at least to a certain extent [12, 18].\nFor the Ring , for example, a high number of CD record-\nings are easily available. In our experiments, we use 16\ndifferent performances comprising nearly 232 hours of au-\ndio, see Table 1. To compare and combine analysis results\nobtained from different representations, a link between the\nrepresentations’ time axes is beneﬁcial. For instance, we\ncan use the positions of measure boundaries, as speciﬁed\nby the score, in the recordings. To this end, several stu-\ndents with a strong practical experience in Western clas-\nsical music manually annotated these positions for three\nperformances [17]. By means of synchronization tech-\nniques [8, Chap. 3], we jointly transferred the manual an-\nnotations from the three performances to all other perfor-\nmances. See [21] for details and an evaluation regardingProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 643the joint transfer. The measure positions constitute suit-\nable reference points for a cross-version analysis on the\nmeasure level since different performances can be related\nto each other using a musical time axis. In total, the Ring\nencompasses 21941 measures, including pickup measures.\nMoreover, the measure positions enable the transfer of\nsemantic annotations from a musical time axis to the indi-\nvidual performances and vice versa. In our scenario, we\nare interested in dramatic aspects of the Ring ’s plot. For\nexample, we annotated the regions where different charac-\nters are singing. In particular, we speciﬁed the start and\nend of the singing voice regions as well as the correspond-\ning character according to the verses of the libretto. If a\nverse is interrupted by a whole measure or more, the anno-\ntation is split accordingly. In total, our annotations com-\nprise 6792 singing voice regions.\nBeyond this, the presence of certain leitmotifs is of high\ninterest for studying the dramatic conception. Motivated\nby this, a musicologist annotated the occurrences of the\nleitmotifs by listening to a recording and analyzing the\nsheet music. The deﬁnition of leitmotifs used for this work\nrelies on a guide by Julius Burghold from 1913 [16], which\nserves as a reference both for the names and the musical\nshape of the motifs. The annotations comprise start and\nend positions as well as the corresponding name for each\noccurrence of a leitmotif. For example, within Siegfried\n2632 leitmotif occurrence regions have been identiﬁed.\n3. HISTOGRAM COMPUTATION\nIn this section, we summarize the computation of local\nchord and scale representations, describe our histogram ap-\nproach, and show why a cross-version strategy is beneﬁ-\ncial. To capture tonal characteristics in audio recordings,\nwe ﬁrst compute normalized chroma features [8, Chap. 3],\nwhich represent the energy within the twelve chromatic\npitch classes over time. Employing a common tem-\nplate matching strategy, we locally compare the chroma\nfeature sequence with binary templates corresponding to\nchords [12] or scales [18]. By means of normalizing, we\ncan interpret the results as probability for the occurrences\nof the particular chords or scales. Since these tonal struc-\ntures relate to a time span of several seconds, we suitably\nsmooth the chromagram before applying template match-\ning. To obtain musically meaningful windows, we make\nuse of the measure annotations to obtain performance-\nindependent window sizes w2Nspeciﬁed in measures,\nrather than seconds. Our experiments showed that w= 4\nfor the chord analysis and w= 12 for the scale analy-\nsis provides meaningful visualizations. From a traditional\nmusic theory perspective, a window size of 4 measures\nfor analyzing chords does not make sense. Indeed, chords\nlasting for such a long time rarely occur in Wagner’s mu-\nsic. However, such a parameter setting leads to visual-\nizations, which appear more structured, e.g. emphasizing\ntonic chords. One reason is that many chords in a pro-\ngression share common notes and, thus, often stabilize the\nresult for the respective tonic chord. Since we use a cen-\ntered window view and a hop size of one measure, we ob-\nB\n/accidentals.flatm\nD\n/accidentals.flatFm\nA\n/accidentals.flatCm\nE\n/accidentals.flatGm\nB\n/accidentals.flatDm\nFAm\nCEm\nGBm\nDF\n/accidentals.sharpm\nAC\n/accidentals.sharpm\nEG\n/accidentals.sharpm\nBD\n/accidentals.sharpm\nF\n/accidentals.sharp\nChords0:000:020:040:060:080:10\n(a)\n−5 −4 −3 −2 −1 0 +1 +2 +3 +4 +5 +6\nScales0:000:050:100:15\n(b)Figure 2 . Analysis for the complete Ring cycle. (a)Chord\nhistogram. (b)Scale histogram.\ntain one feature vector for each measure. For emphasizing\nthe locally salient structures, we apply an exponential post-\nprocessing step similar to the softmax function.\nWith this strategy, we compute an analysis matrix\nA 2RD\u0002Mfor a given performance, where M2Nde-\nnotes the number of measures and D2Nthe feature di-\nmension (with D= 24 for chords and D= 12 for scales).\nFigures 1b/d show such matrices. Based on this matrix\nA, we calculate a histogram h2RDby averaging over all\nmeasures:\nh(d) =1\nMMX\nm=1A(d; m) (1)\nwithd2[1 :D] :=f1;2; : : : ; D g. Figure 2 shows his-\ntograms for the complete Ring . The bar heights correspond\nto the presence of the chords or scales averaged over all\nmeasures. The distributions are rather ﬂat, which indicates\nthat Wagner seems to use the full range of chords and keys\nfor tonally shaping his tetralogy. Nevertheless, we observe\na stronger presence of the C major chord as well as the\nscales 0, +1, and −1 indicating that tonal regions with few\naccidentals seem to be slightly more prominent. Further-\nmore, we ﬁnd a small trend towards ﬂat key regions (left\nhalf) compared to sharp key regions (right half).\nAt this point, we may wonder about the reliability of\nthese results and the histograms’ dependency on a spe-\nciﬁc version. For example, Figure 3a shows two chord\nhistograms, which are computed for two different perfor-\nmances. Even though the global trends seem to be con-\nsistent among the two versions, one can observe some de-\nviations. For example, the E minor triad seems to be one\nof the most prominent chords in the Boulez version (blue),\nwhereas this chord is less important in the Furtw ¨angler ver-\nsion (red). Such performance-speciﬁc characteristics may\ncome into play for a variety of reasons. For example, per-\nformances may exhibit a different dynamic balance of the\ninstruments and singers. Furthermore, different recording\nconditions may suppress or enhance certain frequencies.644 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017B\n/accidentals.flatm\nD\n/accidentals.flatFm\nA\n/accidentals.flatCm\nE\n/accidentals.flatGm\nB\n/accidentals.flatDm\nFAm\nCEm\nGBm\nDF\n/accidentals.sharpm\nAC\n/accidentals.sharpm\nEG\n/accidentals.sharpm\nBD\n/accidentals.sharpm\nF\n/accidentals.sharp\nChords0:000:020:040:060:080:10\n(a)\nB\n/accidentals.flatm\nD\n/accidentals.flatFm\nA\n/accidentals.flatCm\nE\n/accidentals.flatGm\nB\n/accidentals.flatDm\nFAm\nCEm\nGBm\nDF\n/accidentals.sharpm\nAC\n/accidentals.sharpm\nEG\n/accidentals.sharpm\nBD\n/accidentals.sharpm\nF\n/accidentals.sharp\nChords0:000:020:040:060:080:10\n(b)Figure 3 . Comparison of single-version histograms and\ncross-version histograms for the complete Ring .(a)Chord\nhistograms for the performances conducted by Boulez\n(No. 2) in blue and Furtw ¨angler (No. 4) in red. (b)Chord\nhistograms with cross-version approach. The blue and the\nred histogram each correspond to eight different versions.\nTo attenuate the version-speciﬁc aspects, we introduce a\ncross-version approach similar to [6]. Having P2Nper-\nformances, we compute an analysis matrix Apfor each of\nthese versions p2[1 :P]. From these matrices, we derive\na cross-version analysis matrix simply by averaging over\nall performances\nAcv(d; m) =1\nPPX\np=1Ap(d; m) (2)\nwithd2[1 :D]andm2[1 :M]. Finally, we average\nover all measures of Acvto obtain a histogram as in Eq. 1.\nFigure 3b shows two histograms computed with our\ncross-version approach, each for P= 8 different perfor-\nmances.2Note that the two cross-version histograms are\nmore similar to each other than the two single-version his-\ntograms in Figure 3a. This indicates that the cross-version\napproach stabilizes the results. Furthermore, characteris-\ntic trends and peaks of the histograms are retained, which\nshows that the averaging procedure does not smooth out in-\nteresting details. In general, the cross-version approach en-\nhances work-related aspects and suppresses performance-\nspeciﬁc artifacts. In the following, all histograms are com-\nputed in a cross-version fashion using P= 16 perfor-\nmances. Also, the histograms in Figure 2 were computed\nin this way.\n4. EXPLORATION\nWe now want to show the potential of the introduced his-\ntograms for exploring relationships between dramatic as-\npects of the Ring and its tonal organization. To this end, we\n2Performances Nos. 1, 2, 5, 9, 11–13, 15 correspond to blue and\nperformances Nos. 3, 4, 6–8, 10, 14, 16 correspond to red.\nRSA B C D\n(a)\nB\n/accidentals.flatm\nD\n/accidentals.flatFm\nA\n/accidentals.flatCm\nE\n/accidentals.flatGm\nB\n/accidentals.flatDm\nFAm\nCEm\nGBm\nDF\n/accidentals.sharpm\nAC\n/accidentals.sharpm\nEG\n/accidentals.sharpm\nBD\n/accidentals.sharpm\nF\n/accidentals.sharp\nChords0:000:020:040:060:080:10\n(b)\n−5 −4 −3 −2 −1 0 +1 +2 +3 +4 +5 +6\nScales0:000:050:100:15\n(c)Figure 4 . Comparison of Das Rheingold to the complete\nRing .(a)Activations for the Ring (reference, 21941 mea-\nsures, gray) and Das Rheingold (selection, 3897 measures,\nred). Dotted lines correspond to the beginning of a new\nact.(b)Chord histogram. (c)Scale histogram.\napply a selection procedure with respect to different crite-\nria. As examples of such criteria, we consider the role of\na single part with respect to the full cycle, the behavior of\nsinging and instrumental regions, as well as the activity of\nspeciﬁc characters. Furthermore, we take the occurrence\nof certain leitmotifs into account. For all examples, we se-\nlect the measures or passages fulﬁlling these criteria while\ndiscarding the others before computing the histograms.\nAs a ﬁrst scenario, we looked at the role of individual\nparts of the Ring . For example, we compared the ﬁrst part,\nDas Rheingold , to the complete cycle. In Figure 4a, we\nshow a compact overview that illustrates the investigated\npassages with the complete cycle as reference (R) in gray\nand the ﬁrst part as selection (S) in red. In the following,\nwe refer to such illustrations as activation diagrams. Using\nthis color scheme, Figure 4b shows two chord histograms\nwith the reference histogram in gray and the selection his-\ntogram in red. In these histograms, we observe rather ﬂat\ndistributions. Regarding the individual chords, we observe\na high presence of the C major chord in Das Rheingold (se-\nlection). This coincides with the general distribution in the\nfullRing (reference). In contrast, the enhanced presence of\nthe E\n/accidentals.flat major chord in the selection deviates from the shape\nof the reference histogram. An important reason for this\npeak may be the prelude comprising 136 measures with a\nconstant harmony, an E\n/accidentals.flat major triad. Figure 4c shows the\ncorresponding histograms for the scales. The comparison\nof these histograms indicates a trend towards scales with\nﬂats (\n/accidentals.flat ) in their key signatures (–2 to –5) in the selection.\nThese observations suggest that the individual parts of the\nRing may indeed exhibit a characteristic tonal shape.\nAs a second example, we examine the characteristics\nof passages involving instrumental passages and singingProceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 645RSA B C D\n(a)\n−5 −4 −3 −2 −1 0 +1 +2 +3 +4 +5 +6\nScales0:000:050:100:15\n(b)\nRSA B C D\n(c)\n−5 −4 −3 −2 −1 0 +1 +2 +3 +4 +5 +6\nScales0:000:050:100:15\n(d)Figure 5 . Comparison of instrumental passages to all oth-\ners.(a)Activations for the Ring (reference, 21941 mea-\nsures, gray) and the instrumental passages (selection, 5103\nmeasures, red). (b)Scale histogram. (c)Activations for the\nRing (reference, 21941 measures, gray) and passages in-\nvolving singing (selection, 16838 measures, red). (d)Scale\nhistogram.\nactivity. In contrast to the previous experiment, our se-\nlections now comprise several local passages instead of\none contiguous region. To obtain two disjoint selections,\nwe assign each measure to either the instrumental or the\nsinging selection where measures with partial singing were\nassigned to the singing selection. Figures 5a/c display\nthe corresponding activation diagrams. The aggregated\nlengths of both subsets are in a proportion of roughly 1 : 3\n(5103 vs. 16838 measures). In the scale histograms in Fig-\nures 5b/d, the selections show a high similarity to the full\ncycle’s histogram even though the respective passages do\nnot overlap. This indicates that, globally speaking, there\nis no substantial tonal difference between these selections.\nOne may wonder if this similarity is a trivial observation.\nAs we will show in our next example, we observe different\nshapes when we investigate the singing activity of speciﬁc\ncharacters.\nAs the third scenario, we examine the behavior for\ntwo groups of characters, gods andmortals , which con-\nstitute central categories in the Ring ’s plot.3The activa-\ntion diagram in Figure 6a shows the singing activities of\ngods (in blue) and mortals (in red). Instead using a ref-\nerence, like the complete Ring as before, we now com-\n3For this exemplary scenario, we classiﬁed as gods: Wotan, Fricka,\nFreia, Donner, Froh, Erda, Loge, and the Norns. Characters classiﬁed as\nmortals are Siegmund, Sieglinde, Siegfried, Hunding, Gunther, Gutrune,\nHagen, as well as the male and female choirs. Even though there are bor-\nder cases such as the demigod Loge or Siegmund and Sieglinde, Wotans\nchildren, we consider this a meaningful categorization. We do not take\ninto account other categories such as the Valkyries, or the Nibelungs.\nS-GS-MA B C D\n(a)\nB\n/accidentals.flatm\nD\n/accidentals.flatFm\nA\n/accidentals.flatCm\nE\n/accidentals.flatGm\nB\n/accidentals.flatDm\nFAm\nCEm\nGBm\nDF\n/accidentals.sharpm\nAC\n/accidentals.sharpm\nEG\n/accidentals.sharpm\nBD\n/accidentals.sharpm\nF\n/accidentals.sharp\nChords0:000:020:040:060:080:10\n(b)\n−5 −4 −3 −2 −1 0 +1 +2 +3 +4 +5 +6\nScales0:000:050:100:15\n(c)Figure 6 . Comparison of gods and mortals singing.\n(a)Activations for passages with mortals singing (S-M,\n6409 measures, red) and passages with gods singing (S-\nG, 4203 measures, blue). (b)Chord histogram. (c)Scale\nhistogram.\npare two selections based on different selection criteria. In\nDas Rheingold , only gods are active whereas in the ﬁnal\nG¨otterd ¨ammerung , gods are barely active (only the Norns\nin the prologue). Thus, we observe some kind of global\ntrend from gods towards mortals over the course of the\nRing . Figures 6b/c show the corresponding chord and scale\nhistograms. The mortals’ scale histogram exhibits a slight\ntrend towards scales with few accidentals. In contrast, the\ngods’ histogram shows a somewhat higher presence of out-\nlying scales. For musicologists, such observations could be\na starting point for relating tonal characteristics to the in-\nterpretation of the drama. As an exemplary hypothesis, the\nprominence of far-off scales might be associated with the\ngods living far-off our human world.\nAs a ﬁnal selection criterion, we focus on the occur-\nrence of leitmotifs, which constitute a central dramatic el-\nement in the Ring . Even though Wagner did not invent this\ntechnique and never used the term “leitmotif” personally,\nthe extensive usage of such motifs makes the Ring a promi-\nnent example of a realization of this concept. We now indi-\ncate how one may explore the tonal characteristics during\nthe occurrences of certain leitmotifs. As a ﬁrst example,\nwe consider the “Valhalla motif,” which refers to the castle\nof the gods and is frequently used over the course of the\ntetralogy. Figures 7a/b show an activation diagram and a\nchord histogram with Das Rheingold as reference and all\nregions in this part with the Valhalla motif as selection. For\nthis motif, we notice a high peak for the D\n/accidentals.flat major chord, in\ncontrast to the ﬂat shape of the reference histogram. When\nwe examine this motif over the course of Die Walk ¨ure, we\nobserve a different trend (see Figures 7c/d). For this part,\nthe histogram exhibits a high peak for the E major chord\nwhereas the D\n/accidentals.flat major chord has only a slight peak. These646 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017RSA B C D\n(a)\nB\n/accidentals.flatm\nD\n/accidentals.flatFm\nA\n/accidentals.flatCm\nE\n/accidentals.flatGm\nB\n/accidentals.flatDm\nFAm\nCEm\nGBm\nDF\n/accidentals.sharpm\nAC\n/accidentals.sharpm\nEG\n/accidentals.sharpm\nBD\n/accidentals.sharpm\nF\n/accidentals.sharp\nChords0:00:10:20:30:40:5\n(b)\nRSA B C D\n(c)\nB\n/accidentals.flatm\nD\n/accidentals.flatFm\nA\n/accidentals.flatCm\nE\n/accidentals.flatGm\nB\n/accidentals.flatDm\nFAm\nCEm\nGBm\nDF\n/accidentals.sharpm\nAC\n/accidentals.sharpm\nEG\n/accidentals.sharpm\nBD\n/accidentals.sharpm\nF\n/accidentals.sharp\nChords0:00:10:20:30:40:5\n(d)Figure 7 . Passages containing the Valhalla motif. (a)Ac-\ntivations for Das Rheingold (reference, 3897 measures,\ngray) and passages containing the Valhalla motif therein\n(selection, 176 measures, red). (b)Chord histogram.\n(c)Activations for Die Walk ¨ure(reference, 5322 measures,\ngray) and passages containing the Valhalla motif therein\n(selection, 124 measures, red). (d)Chord histogram.\nobservations indicate that the Valhalla motif tends to ap-\npear in a speciﬁc tonal context. However, the correspond-\ning chord strongly differs between the parts of the Ring .\nThe correlation of the Valhalla motif to the D\n/accidentals.flat major and\nE major chords, respectively, is a known fact in musicol-\nogy [4, p. 172]. It is promising that automated methods\ncan conﬁrm such observations.\nAs a second leitmotif example, we investigate the oc-\ncurrences of the “sword motif.” Figure 8 shows activa-\ntion diagrams and scale histograms for this motif within\nDie Walk ¨ureandSiegfried . InDie Walk ¨ure, the motif has\na clear tendency towards the region of 0 (C major/A mi-\nnor) and +1 (G major/E minor). The situation is com-\npletely different in Siegfried , where the scale distribution\nis rather ﬂat, with a slight trend towards ﬂat scale regions.\nOne reason might be the integration of this motif into a\nmore complex tonal conception in Siegfried compared to\nDie Walk ¨ure.\n5. CONCLUSIONS AND OUTLOOK\nIn this paper we demonstrated how existing MIR tech-\nniques such as tonal audio features and global histograms\ncan be applied in a complex music scenario. Regarding\naudio-based analysis, we showed that a cross-version ap-\nproach is able to enhance work-related properties while\nsuppressing performance-speciﬁc details and, therefore,\nstabilizes the analysis results compared to a single-version\nRSA B C D\n(a)\n−5 −4 −3 −2 −1 0 +1 +2 +3 +4 +5 +6\nScales0:00:10:20:30:40:5\n(b)\nRSA B C D\n(c)\n−5 −4 −3 −2 −1 0 +1 +2 +3 +4 +5 +6\nScales0:00:10:20:30:40:5\n(d)Figure 8 . Passages containing the sword motif. (a)Acti-\nvations for Die Walk ¨ure(reference, 5322 measures, gray)\nand passages containing the sword motif therein (selection,\n200 measures, red). (b)Scale histogram. (c)Activations\nforSiegfried (reference, 6682 measures, gray) and pas-\nsages containing the sword motif therein (selection, 203\nmeasures, red). (d)Scale histogram.\napproach. For exploring relationships between dramatic\naspects and the tonal organization of Wagner’s Ring cy-\ncle, we presented compact visualizations of tonal features\nbased on global histograms with several selection criteria.\nWe showed that these visualizations can provide interest-\ning insights into large-scale works such as Wagner’s tetral-\nogy. Investigating a small selection of examples, we found\nthat the Ring ’s parts may each exhibit an individual tonal\nshape. Furthermore, the histograms indicated that charac-\nter groups can have different tonal preferences. Finally,\nwe showed that leitmotifs can have speciﬁc tonal conno-\ntations. In general, using global histograms exhibits some\nlimitations since we cannot address many ﬁne-granular is-\nsues with this method. Nevertheless, we showed the bene-\nﬁt of such visualizations for highlighting interesting trends\nand relations. Beyond the analyses shown in this paper,\nmany more ﬁltering criteria could be of interest in this\ncomplex scenario. Concerning subsections of the Ring , in-\ndividual acts or scenes could be analyzed. Regarding the\nmusical parts, different character groups or even individual\nsingers could be considered as well as the use of certain in-\nstruments or instrument families. Finally, the enormous\nnumber of leitmotifs with several thousand occurrences\nand their complex relationships lead to a vast amount of\npossible selections. Investigating the Ring with respect to\nsuch aspects could allow musicologists to conﬁrm or ad-\njust their hypotheses or be inspired to create new ones and,\nthus, might have potential for musicological research.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 647Acknowledgments: We thank Julia Zalkow, Vlora Ariﬁ-\nM¨uller, and all students for their assistance in generating\nthe annotations and preparing the data. This work has\nbeen supported by the German Research Foundation (DFG\nMU 2686/7-1). The International Audio Laboratories Er-\nlangen are a joint institution of the Friedrich-Alexander-\nUniversit ¨at Erlangen-N ¨urnberg (FAU) and Fraunhofer In-\nstitut f ¨ur Integrierte Schaltungen IIS.\n6. REFERENCES\n[1] Howard Mayer Brown, Ellen Rosand, Reinhard\nStrohm, Roger Parker, Arnold Whittall, Roger Savage,\nand Barry Millington. Opera. In Stanley Sadie, editor,\nThe New Grove Dictionary of Music and Musicians ,\npages 416–471. Macmillian Publishers, London, UK,\n2nd edition, 2001.\n[2] Donald Byrd and Jakob G. Simonsen. Towards a stan-\ndard testbed for optical music recognition: Deﬁnitions,\nmetrics, and page images. Journal of New Music Re-\nsearch , 44(3):169–195, 2015.\n[3] Emilia G ´omez and Jordi Bonada. Tonality visualiza-\ntion of polyphonic audio. In Proceedings of the Inter-\nnational Computer Music Conference (ICMC) , pages\n57–60, Barcelona, Spain, 2005.\n[4] Tobias Janz. Klangdramaturgie: Studien zur theatralen\nOrchesterkomposition in Wagners”Ring des Nibe-\nlungen“ , volume 2 of Wagner in der Diskussion .\nK¨onigshausen & Neumann, W ¨urzburg, Germany 2006.\n[5] Gopala Krishna Koduri, Joan Serr `a, and Xavier Serra.\nCharacterization of intonation in carnatic music by\nparametrizing pitch histograms. In Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 199–204, Porto, Portugal,\n2012.\n[6] Verena Konz, Meinard M ¨uller, and Rainer Kleinertz.\nA cross-version chord labelling approach for exploring\nharmonic structures—a case study on Beethoven’s Ap-\npassionata. Journal of New Music Research , 42(1):61–\n77, 2013.\n[7] Daniel M ¨ullensiefen, David Baker, Christophe Rhodes,\nTim Crawford, and Laurence Dreyfus. Recognition of\nleitmotives in Richard Wagner’s music: An item re-\nsponse theory approach. In Analysis of Large and Com-\nplex Data , pages 473–483. Springer, Cham, Switzer-\nland, 2016.\n[8] Meinard M ¨uller. Fundamentals of Music Processing .\nSpringer, Cham, Switzerland, 2015.\n[9] Kevin Page, Terhi Nurmikko-Fuller, Carolin Rind-\nﬂeisch, Richard Lewis, Laurence Dreyfus, and\nDavid De Roure. A Toolkit for Live Annotation of\nOpera Performance: Experiences Capturing Wagner’sRing Cycle. In Proceedings of the International Con-\nference on Music Information Retrieval (ISMIR) , pages\n211–217, M ´alaga, Spain, 2015.\n[10] Hendrik Purwins, Benjamin Blankertz, Klaus Ober-\nmayer, and Guido Dornhege. Scale degree proﬁles\nfrom audio investigated with machine learning. In Pro-\nceedings of the 116th Audio Engineering Society (AES)\nConvention , Berlin, Germany, 2004.\n[11] Craig Stuart Sapp. Visual hierarchical key analysis.\nACM Computers in Entertainment , 3(4):1–19, 2005.\n[12] Alexander Sheh and Daniel P.W. Ellis. Chord seg-\nmentation and recognition using EM-trained hidden\nMarkov models. In Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , pages 185–191, Baltimore, USA, 2003.\n[13] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nSpeech and Audio Processing , 10(5):293–302, 2002.\n[14] George Tzanetakis, Andrey Ermolinskyi, and Perry\nCook. Pitch histograms in audio and symbolic music\ninformation retrieval. Journal of New Music Research ,\n32(2):143–152, 2003.\n[15] Richard Wagner. Oper und Drama . Klaus Kropﬁnger\n(Ed.). Reclam, Ditzingen, Germany, 1986.\n[16] Richard Wagner. Der Ring des Nibelungen. Vollst ¨andi-\nger Text mit Notentafeln der Leitmotive . Julius\nBurghold (Ed.). Schott Music, Mainz, Germany, 2013.\n[17] Christof Weiß, Vlora Ariﬁ-M ¨uller, Thomas Pr ¨atzlich,\nRainer Kleinertz, and Meinard M ¨uller. Analyzing mea-\nsure annotations for western classical music record-\nings. In Proceedings of the International Conference\non Music Information Retrieval (ISMIR) , pages 517–\n523, New York, USA, 2016.\n[18] Christof Weiß and Julian Habryka. Chroma-based\nscale matching for audio tonality analysis. In Proceed-\nings of the 9th Conference on Interdisciplinary Musi-\ncology (CIM) , pages 168–173, Berlin, Germany, 2014.\n[19] Christof Weiß, Rainer Kleinertz and Meinard M ¨uller.\nM¨oglichkeiten der computergest ¨utzten Erkennung und\nVisualisierung harmonischer Strukturen – eine Fall-\nstudie zu Richard Wagners Die Walk ¨ure. In Bericht\nzur Jahrestagung der Gesellschaft f ¨ur Musikforschung\n(GfM) , Mainz, Germany, 2016.\n[20] Ho-Hsiang Wu and Juan P. Bello. Audio-based music\nvisualization for music structure analysis. In Proceed-\nings of the Sound and Music Computing Conference\n(SMC) , pages 496–501, Barcelona, Spain, 2010.\n[21] Frank Zalkow, Christof Weiß, Thomas Pr ¨atzlich, Vlora\nAriﬁ-M ¨uller, and Meinard M ¨uller. A multi-version ap-\nproach for transferring measure annotations between\nmusic recordings. In Proceedings of the AES Interna-\ntional Conference on Semantic Audio , pages 148–155,\nErlangen, Germany, 2017.648 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017"
    },
    {
        "title": "Understanding the Expressive Functions of Jingju Metrical Patterns Through Lyrics Text Mining.",
        "author": [
            "Shuo Zhang",
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1416608",
        "url": "https://doi.org/10.5281/zenodo.1416608",
        "ee": "https://zenodo.org/records/1416608/files/ZhangRS17.pdf",
        "abstract": "The emotional content of jingju (aka Beijing or Peking opera) arias is conveyed through pre-defined metrical pat- terns known as banshi, each of them associated with a specific expressive function. In this paper, we first re- port the work on a comprehensive corpus of jingju lyrics that we built, suitable for text mining and text analysis in a data-driven framework. Utilizing this corpus, we pro- pose a novel approach to study the expressive functions of banshi by applying text analysis techniques on lyrics. First we apply topic modeling techniques to jingju lyrics text documents grouped at different levels according to the banshi they are associated with. We then experiment with several different document vector representations of lyrics in a series of document classification experiments. The topic modeling results showed that sentiment polarity (pos- itive or negative) is better distinguished between different shengqiang-banshi (a more fine grained partition of ban- shi) than banshi alone, and we are able to achieve high ac- curacy scores in classifying lyrics documents into different banshi categories. We discuss the technical and musico- logical implications and possible future improvements.",
        "zenodo_id": 1416608,
        "dblp_key": "conf/ismir/ZhangRS17",
        "keywords": [
            "emotional content",
            "jingju arias",
            "pre-defined metrical patterns",
            "banshi",
            "expressive functions",
            "text mining",
            "text analysis",
            "novel approach",
            "topic modeling",
            "document classification"
        ],
        "content": "UNDERSTANDING THE EXPRESSIVE FUNCTIONS OF JINGJU\nMETRICAL PATTERNS THROUGH LYRICS TEXT MINING\nShuo Zhang\nMusic Technology Group\nUniversitat Pompeu Fabra\nssz6@georgetown.eduRafael Caro Repetto\nMusic Technology Group\nUniversitat Pompeu Fabra\nrafael.caro@upf.eduXavier Serra\nMusic Technology Group\nUniversitat Pompeu Fabra\nxavier.serra@upf.edu\nABSTRACT\nThe emotional content of jingju (aka Beijing or Peking\nopera) arias is conveyed through pre-deﬁned metrical pat-\nterns known as banshi , each of them associated with a\nspeciﬁc expressive function. In this paper, we ﬁrst re-\nport the work on a comprehensive corpus of jingju lyrics\nthat we built, suitable for text mining and text analysis in\na data-driven framework. Utilizing this corpus, we pro-\npose a novel approach to study the expressive functions\nofbanshi by applying text analysis techniques on lyrics.\nFirst we apply topic modeling techniques to jingju lyrics\ntext documents grouped at different levels according to the\nbanshi they are associated with. We then experiment with\nseveral different document vector representations of lyrics\nin a series of document classiﬁcation experiments. The\ntopic modeling results showed that sentiment polarity (pos-\nitive or negative) is better distinguished between different\nshengqiang-banshi (a more ﬁne grained partition of ban-\nshi) than banshi alone, and we are able to achieve high ac-\ncuracy scores in classifying lyrics documents into different\nbanshi categories. We discuss the technical and musico-\nlogical implications and possible future improvements.\n1. INTRODUCTION\nTraditionally, the emotional content of jingju (aka Beijing\nor Peking Opera) music is conveyed through pre-deﬁned\nmelodic and metrical patterns known as shengqiang and\nbanshi . With the general absence of professional com-\nposers, the melodic material of jingju was taken from local\ntunes, and lyrics were arranged by performers according\nto their poetic structure. In order to convey different emo-\ntional contents, the original melodic outlines were trans-\nformed rhythmically, according to a pre-deﬁned set of la-\nbelled metrical patterns. Each of the metrical patterns,\nknown as banshi , is associated with an expressive func-\ntion. Each of the melodic materials to which this metrical\npatterns were applied is known as shengqiang , and is also\nassociated with emotional content at a larger scale.\n© Shuo Zhang, Rafael Caro Repetto, Xavier Serra. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Shuo Zhang, Rafael Caro Repetto, Xavier\nSerra. “Understanding the expressive functions of jingju metrical pat-\nterns through lyrics text mining ”, 18th International Society for Music\nInformation Retrieval Conference, Suzhou, China, 2017.There exists many general descriptions and rules for the\nexpressive functions associated with each banshi in musi-\ncological literature [11, 15] and jingju textbooks [2, 3, 14].\nHowever, the actual realization of these associativities\nacross existing jingju repertoires has not been character-\nized in a clear manner. Such a task is well suited for a\ndata-driven computational analysis.\nIn this work, we ﬁrst report the work on constructing the\nJingju Lyrics Collection data collection, a comprehensive\ncorpus of jingju lyrics that we built through web scrap-\ningxikao.com , suitable for text mining and text analy-\nsis in a data-driven framework. We describe substructures\nof this data collection as well as relevant corpus statistics\nbased on musicological entities and considerations. Utiliz-\ning this corpus, we propose a novel approach to study the\nexpressive functions of banshi by applying text analytics\ntechniques on lyrics.\nThe rest of the paper is organized as follows. Section\n2 provides necessary musicological concepts and back-\nground that lead to the research questions we are concerned\nwith, namely, understanding the emotional content of ban-\nshimetrical patterns through lyrics text analytics. Section\n3 reports the construction of the JLC lyrics data collection\nand describes its substructures as well as relevant corpus\nstatistics. Following the introduction of the JLC data col-\nlection, we then report text analytics experiments aimed\nat revealing different semantic content in different banshi ,\nincluding topic modeling (Section 4) and document clas-\nsiﬁcation (Section 5). Finally we discuss the results and\nfuture directions.\n2. BACKGROUND\nAs stated above, shengqiang (SQ) and banshi (BS) are the\nmelodic and rhythmic devices used in arranging the music\nin jingju. They are selected in order to deliver the emo-\ntional content of lyrics, the psychological proﬁle of the\ncharacters or the general atmosphere of the play. The two\nmain shengqiang of jingju are xipianderhuang . There are\naround twelve types of most common banshi , each inter-\nrelated with others. For example, yuanban , literally mean-\ning ’original meter’, is considered a default medium tempo\nmeter, and the rest of banshi can be considered transfor-\nmations of this one: manban , the result of slowing down\nyuanban in tempo and stretching it in meter; kuaiban , the\nresult of speeding yuanban up in tempo and compressing it397Banshi CodePostulated expressive\nfunctionMusical\nfeature\nyuanban YBstraightforward,\nunemotional, narration,\nfacts and explanationmedium\nmanban MB peaceful, introspective slow\nkuaiban KBanimated, excitement,\nanticipationfast\nyaoban YABexterior calm and\ninterior tensionfree meter\nSheng\n-qiangCodePostulated expressive\nfunctionMusical\nfeature\nxipi XPsprightly, bright and\nclear, energetic, forceful,\nand purposefulmelodic\nskeleton\nerhuang EHdark, deep and profound,\nheavy and meticulousmelodic\nskeleton\nTable 1 . List of common banshi (rhythmic) and\nshengqiang (melodic) types and their acronyms (Code) in\nthis paper. The entity column contains 4 banshi types in\nthe ﬁrst 4 rows, and 2 shengqiang types in the last 2 rows.\nin meter, etc. The combination of a shengqiang with a par-\nticular banshi results in a unique musical form (henthforce\nreferred to as SQBS), which is referred to by combining\nboth elements, such as erhuang yuanban ,xipimanban ,\netc. In general, shengqiang are associated with general\nemotional frameworks, and banshi with speciﬁc expres-\nsive functions [11]. Table 1 lists the most common types\nofbanshi andshengqiang , their musical and postulated ex-\npressive functions.\nThe goal of this paper is twofold: ﬁrst, to introduce a\njingju lyrics corpus that we constructed especially for com-\nputational text analysis in this domain; second, to under-\nstand the expressive functions associated with each banshi\nthrough large-scale text analytics. In the current context,\nwe take the implicit assumption that the emotional content\n(i.e., the target of the expressive functions for a banshi ) of\nthe jingju can be represented by inspecting the semantic\ncontent expressed in lyrics. We deﬁne the following re-\nsearch questions: (1) What are the document-topic-word\ndistributions that characterize the lyrics texts found in each\ntype of banshi ? (2) How distinct are these distributions\namong different banshi ? (3) Are we able to distinguish be-\ntween one banshi and another from lyrics? (classiﬁcation)\n(4) How does the interplay between shengqiang andbanshi\naffect this characterization?\nIn recent years, there has been a number of studies em-\nploying data-driven and computational approaches to vari-\nous facets of jingju music [6–8,12,13]. However, all of the\nprevious works rely on audio recording or score as their\nprimary data source. To the best of our knowledge, the cur-\nrent work is novel in its use of large-scale lyrics text cor-\npus for jingju and the application of state-of-the-art NLP\nand text mining algorithms to uncover the associations be-\ntween jingju music and expressive functions.3. BUILDING THE JINGJU LYRICS\nCOLLECTION\n3.1 Web Scraping Xikao Database\nThere are a limited number of traditional style plays in the\njingju repertoire (as they are not being expanded much in\nmodern times). In order to build a comprehensive corpus\nof collection of jingju lyrics, we have chosen to extract\ndata from the well-maintained open source jingju libretto\ndatabase website xikao.com . As this website provides\njingju librettos in HTML and PDF formats not ready for\ncorpus analysis purposes, we have crawled the website to\nextract all lyrics in plain text format through web scrap-\ning. All texts from this website is of Creative Commons\nLicense and is free to use for non-commercial purposes.\nWe denote our overall collection of the lyrics data (includ-\ning subsequence creation of substructures within the col-\nlection) as JLC (Jingju Lyrics Collection). We use this\ngeneric name to accommodate future possibilities of ex-\npanding the collection from other sources.\nxikao.com is a community collaboration platform\naimed at building the most comprehensive collection of\njingju plays for jingju professionals and aﬁcionados by col-\nlaboratively digitizing published jingju librettos available\nin prints. It is being actively maintained since its inception\nin 2000, and there has been a steady growth in the number\nof digitized librettos. At the time of writing, there are a to-\ntal of 2163 published librettos in print being considered for\ndigitization, whereas there are 850 works already digitized\nand proof-read/edited, and there are currently 360 plays at\nthe various stages of being digitized by dozens of anony-\nmous users/editors/annotators. Due to the dynamic growth\nof its content, we can also periodically re-apply our web\nscraping pipeline in order to expand our data collection to\nreﬂect the most comprehensive coverage to date.\nLibrettos in Xikao is organized by play as a basic unit.\nMetadata, banshi (metrical pattern), shengqiang (melodic\nskeleton), role type, as well as other information such as\nthe instrumental interlude and oral delivery mode (spo-\nken dialogue, singing) are also annotated in the digitized\ndocuments. Meanwhile, as noted above, the overall goal\nof Xikao is not oriented towards computational analysis,\ntherefore we need to apply several transformations in or-\nder to create the most useful data sets for our study (de-\ntailed in Section 3.2 and 3.3). Several examples of these\nshortcomings are illustrated here. First, the organization\nby play may not be the most useful for analysis aimed at\nunderstanding shengqiang orbanshi or other musicolog-\nically meaningful categories. Second, the meta data in-\nformation are also spread within the documents, making\nit hard to retrieve in an straightforward way. Overall, the\nXikao website in its original form (before or after we have\nscraped its contents and stored in plain text ﬁles) is consid-\nered unstructured data that needs to be re-structured and\naugmented in order to use for large scale data-drive text\nanalysis.398 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20173.2 Text Processing\nIn a post-processing stage to the web scraping, we extract\nall lyrics that are sung to a particular banshi type clearly\nindicated (there are a good portion of a play that are spo-\nken dialogue). As part of the standard NLP pipeline for\nChinese1, we perform word segmentation2on all text\ndocuments using the state-of-the-art, Conditional Random\nField-based Stanford Word Segmenter3[10]. The result\nof the segmentation is veriﬁed by hand by a native speaker\nof Chinese and deemed reasonable.4After the segmenta-\ntion is obtained, we use Unicode based tokenization (split-\nting on whitespaces) in the study, where each token is de-\nﬁned as one or more Unicode character5. In all subsequent\nprocessing steps we remove 125 frequent single-character\nwords using a standard stop word list of Chinese. All\npunctuations are removed as a normalization step for NLP\npipeline. The resulting corpus contains lyrics text ﬁles for\n818 plays, a quite large size to study considering the small\nnumber of jingju plays that are still being performed today.\n3.3 Data Sets Permutation and Creation\nFollowing preprocessing, we extract subsets of the data\ncollection and restructure them in order to create musico-\nlogically meaningful datasets for computational text anal-\nysis of jingju lyrics. We consider the creation of several\ndata sets within this framework.\nSQBS Dataset : This general data set consists of lyrics\nfrom all lines in all plays that correspond to a SQBS, in\na tabular format, where the ﬁrst column indicates SQBS\ncategory, second contains the lyrics line. Here, it’s worth\npointing out a ’lyrics line’ is referring to the longest unit\nan actor is singing continuously in the same SQBS without\nswitching to or interrupted by any other SQBS. The total\nnumber of SQBS categories in this data set is 151. In this\ncase, it is possible to observe the distribution of the fre-\nquencies of each SQBS category. In Figure 1 and Figure 2,\nwe show the top 10 SQBS categories in the SQBS dataset\nby number of lines and by number of words/characters\n(where a ’line’ is deﬁned as above). With few exceptions,\nwe can see that the top 10 most frequent categories are\nmostly consistent when considered by number of lines vs.\nby number of words/characters. Meanwhile, we note that\nxipiyaoban (XPYAB) is the most frequent musical form\nin jingju by all measures, which is often used in singing in\nthe middle of spoken dialogues.\n1Here we apply a shallow pipeline of word segmentation, tokeniza-\ntion, and stop-word removal.\n2Since the Chinese language is written without spaces between char-\nacters and words, the word segmentation is a necessary and challenging\ntask for any NLP or text mining analysis of Chinese text.\n3Obtained at http://nlp.stanford.edu/software/segmenter.shtml.\n4The linguistic style of the jingju lyrics is a mixed style that is typi-\ncally similar to modern Chinese yet with occasional semi-classical style\nlanguage. Therefore, unless there is a segmenter trained speciﬁcally on\nthis language, using any pre-trained segmenter would not have yielded a\nperfect segmentation. To give an estimate of error, the original Stanford\nSegmenter paper [10] gives a overall F-score around 0.95, with a recall of\nknown vocabulary greater than 0.95 and a recall of unknown vocabulary\n(OOV) in the range of 0.7s.\n5In Chinese, a ’word’ can be any number of characters, most com-\nmonly 1,2,3, or 4.\nXPYABEHYB EHSB XPYB XPSBEHYABXPMB XPLSB XPDB XPKB010002000300040005000Figure 1 . Distribution of top 10 SQBS categories by num-\nber of lines\nXPYABEHYB EHSB XPYBXPELBXPSBEHYABXPMB XPLSBXPKB020000400006000080000\nFigure 2 . Distribution of SQBS top 10 categories by num-\nber of words/characters\nSQBS7 Dataset : Among the 151 SQBS categories, we\nhave selected a core set of 3 banshi types coupled with\nthe two shengqiang , based on their musicological impor-\ntance and the frequency of their occurrence in our corpus.\nConcretely, we consider the most basic banshi types for\nthe two main shengqiang , that is yuanban ,manban , and\nkuaiban forxipianderhuang6. To also include a non me-\ntered banshi , we have also considered the one with a more\nfrequent occurrence in our corpus, that is, yaoban , giving\nrise to 7 core SQBS categories that are most representative\nin analysis. We denote this data set as SQBS7, which is a\nsubset of SQBS data set. All following data sets to be used\nin this study are transformed from the SQBS7 dataset.\nPL* Datasets : The PL* data sets are grouped by play\nand one or two other musicological entities (BS or SQBS).\nFirst, we create the PLay-ShengQiang-BanShi (PLSQBS)\ndata set, where a document is deﬁned to be all the texts\nassociated with a particular shengqiang banshi within the\nsame play. For example, all the erhuang manban texts\nfrom one play form one PLSQBS document, whereas all\ntheerhuang yuanban texts from the same play form an-\nother PLSQBS document. This is aimed at looking at a\nparticular combination of shengqiang banshi type. Sec-\nond, we collapse all shengqiang categories and create the\n6It has to be noticed that in traditional plays erhuang was never set to\nkuaiban .Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 399PLay-BanShi (PLBS) documents. Each document in this\ndata set is deﬁned to be all the texts associated with a par-\nticular banshi within the same play. Therefore, regardless\nofshengqiang , all the manban texts from one play form\none PLBS document.\nOne potential problem with the PL* data sets is that the\npartition of documents may result in very short documents,\ncreating a data sparseness problem in document modeling\nalgorithms. Figure 4 shows the distribution of document\nlength in the PLSQBS data set. We observe that there is a\npeak at less than 200 words per document, whereas there\nare also a few documents with more than 2000 or 4000\nwords. This may be taken into consideration when per-\nforming text mining experiments on these data sets.\nAG* Datasets : We aggregate all PLSQBS documents\nto form 7 AGSQBS (’AG’ for aggregate) documents (as\nthere are 7 types of shengqiang banshi considered in the\ncurrent study), in order to study the characteristics in all\ntexts associated with a particular shengqiangbanshi . By\nanalogy, we aggregate all PLBS documents to form the 4\nAGBS documents (i.e., all texts for a particular banshi ).\nWe provide an overview of the relationships between\ndata sets in Figure 3. Table 2 gives a more detailed de-\nscription of the PL* and AG* data sets used in the sub-\nsequent experiments. The entire data collection is openly\navailable through Github7, and the datasets used in the ex-\nperiments of this paper are available through CompMusic\nproject website8.\nRaw data SQBS SQBS7 PLBS \nPLSQBS \nAGBS \nAGSQBS -spoken \n+preproc \n+extract \nSQBS Select \n7 SQBS Group by play \nAggregate all \ntext of a \nBS/SQBS \nFigure 3 . Block diagram overview of data sets created\nName Description of document#docu-\nments\nPLSQBSall the texts associated with a particular\nshengqiang-banshi within a particular\nplay1429\nPLBSall the texts associated with a particular\nbanshi within the a particular play1247\nAGSQBSall the texts associated with a particular\nshengqiang-banshi7\nAGBSall the texts associated with a particular\nbanshi4\nTable 2 . Overview of data sets used in the experiments of\nthis paper\n7https://github.com/MTG/Jingju-Lyrics-Collection\n8http://compmusic.upf.edu/jingju-lyrics-datasets\n0 1000 2000 3000 4000 5000\ndocument length by unicode character020406080100120140countFigure 4 . Distribution of document length in the PLSQBS\ndataset\n4. EXPLORING TOPIC STRUCTURES OF\nSUBSECTIONS OF JLC DATA COLLECTION\nIn this section, we use probabilistic topic models to ex-\nplore the topic structures of lyrics in different banshi in an\nunsupervised setting9.\n4.1 Topic Modeling\nTopic model is a class of unsupervised statistical models\nfor uncovering the underlying semantic structure of a doc-\nument collection. The idea is to model each document as\narising from multiple background (latent) topics, where a\ntopic is deﬁned to be a distribution over a ﬁxed vocabu-\nlary of terms. Speciﬁcally, we assume that K topics are\nassociated with a collection, and that each document ex-\nhibits these topics with different proportions. In a topic\nmodel, we typically obtain a document-topic distribution\n(the probabilistic distribution of all topics in a particular\ndocument), and a topic-word distribution (the distribution\nof the terms that are associated with one topic). In the cur-\nrent context, we are interested in the topics that character-\nize each banshi (BS) or shengqiang banshi (SQBS) in the\nAGBS and AGSQBS data sets.\nHere we consider a state-of-the-art topic modeling tech-\nniques known as Latent Dirichlet Allocation(LDA) [1]. In\nLDA, each topic zis associated with a multinomial dis-\ntribution over the vocabulary \bz, which is drawn from a\nDirichlet prior Dir(\f). A given document Diis then gen-\nerated by the following process:(1) Choose \u0002i˜Dir(\u000b), a\ntopic distribution for Di; (2) For each word wj2Di: (a)\nSelect a topic zj˜\u0002i(b) Select the word wj˜\bzj. We use\ncollapsed Gibbs sampling implementation in Mallet10to\ninfer the values of the latent variables \band\u0002.\nWe compute the perplexity measure for a held-out data\nset deﬁned in the LDA model to determine the optimal\nnumber of background latent topics in the current exper-\niments. The perplexity, used by convention in language\n9The code for all experiments in this paper is available at\nhttps://github.com/MTG/Jingju-Lyrics-Text-Analysis.\n10Downloaded from http://mallet.cs.umass.edu/400 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017YB0:Hanxin (military General), youth, wealth,\nruthless\nKB2:family, mother, husband-wife, brother; 16:\nwar, military, mails, destruction\nMB4:princess, death, crime, Chang’an, brave; 19:\nwish, sir, madam, pain, defeat\nYAB7: sudden news, human head, revenge,\naffection\nTable 3 . Top topics for each banshi and their top words.\nSorted by topic index number (0 is topic 0 assigned by the\nLDA model, etc.)\nmodeling, is monotonically decreasing in the likelihood\nof the test data, and is algebraically equivalent to the in-\nverse of the geometric mean per-word likelihood [1]. A\nlower perplexity score indicates better generalization per-\nformance. More formally, for a test set of M documents,\nthe perplexity is:\nperplexity =exp(\n\u0000PM\nd=1logp(wd)\nPM\nd=1Nd)\n(1)\nwherewdis a word in the document, and Ndis the\nlength of the document (total number of words). In our\nexperiments, we compute perplexity for each topic model\ngiven number of topics from 8 to 50. The result shows that\nusing 20 topics results in the lowest perplexity and is the\noptimal choice.\n4.2 Topic Modeling Results\nFirst, we present the topic modeling results for the AGBS\nand AGSQBS data sets. In this case, the output from the\nMALLET LDA model contains a document-topic distri-\nbution and a topic-word distribution. The former shows\nthe distribution of topics (number of topic K=20, as deter-\nmined in Section 4.1) in each BS/SQBS (i.e., each doc-\nument in the data set), and the latter shows the top 20\nwords associated with each topic. Since we are interested\nin characterizing the main topics found in each BS/SQBS,\nwe show results for both of these components.\nTo better understand the topics that characterize each\ncategory, we extracted the most salient topics from the\ntopic distribution of each BS or SQBS. In doing so, we\nremoved common topics with high occurrences in all cat-\negories, and only select those with a high occurrences in\neach category. We present these topics and summarize\ntheir top words in English in Table 3 and Table 4. Many of\nthese topics have to do with speciﬁc stories in Chinese his-\ntory that are well known in jingju repertoires. Comparing\nthese results to the general descriptions found in Table 1,\nwe see a reasonable interpretation for each category - al-\nthough we observe that the division between the positive\nand negative emotions of xipi banshi anderhuang banshi\nin Table 4 are much more salient than the topics that dis-\ntinguish the four banshi types in Table 3.EHYB 0: pity, old, heavy, prince, depart, pain, cold\nEHMB2: unfortunate, pity, worry; 14: war, military,\ncourtesy, hero\nEHYAB4: tears, sir, madam, wish, leave, pain, hurt,\nstab; 18: life, run, death, brave, sword\nXPYB7: Kings from The Three Kingdoms, drink,\nhappy\nXPKB 10: traitor, laugh, believe\nXPMB 12: youth, beautiful view, morning, world\nXPYAB11: general(military), prime minister, angry,\nmilitary, step forward; 14: (see EHMB)\nTable 4 . Top topics for each shengqiang-banshi (SQBS)\nand their top words. Sorted by topic index number(0 is\ntopic 0 assigned by the LDA model, etc.)\n5. DOCUMENT CLASSIFICATION IN JLC DATA\nCOLLECTION\nIn this section we propose a supervised document classiﬁ-\ncation task with the aim of classifying lyrics documents in\nthe PL* data sets into different banshi categories.\n5.1 Document Vector Representation\nIn the vector-space model (VSM) of information retrieval,\na text document is represented by a document-term vector\nwhere each attribute represents the frequency (count) with\nwhich a particular term wk;i(a word in the vocabulary)\noccurs in the document di(aka bag-of-words or BOW).\nA highly effective transformation of BOW word vector\nweights is tf-idf weighted vectorization.11However, a\nshortcoming of these types of traditional word vectors is\nthat it is high-dimensional and very sparse.\nRecent advances in NLP have concentrated on train-\ning word embeddings with neural networks that result in\nlow-dimensional dense vector representations of words [5]\nby predicting target words from context (or vice versa).\nThese high quality word embeddings also have the desired\nproperty of reﬂecting semantic similarity in vector space\n(semantic similarities can be captured by vector arith-\nmetics). Expanding on this idea of word embeddings, [4]\ntrained embeddings for sentences or longer units, which\nthey denote ”paragraph vectors”. These paragraph vectors\nhave the similar properties of reﬂecting semantic similarity\nabove the word level, and is shown to be highly effective\nin a series of document and sentiment classiﬁcation tasks.\n5.2 Document Classiﬁcation in JLC\nTo characterize the strength of the association between the\nlyrics text and its associated banshi , we deﬁne a document\nclassiﬁcation task: to what degree can we use textual fea-\ntures extracted from the lyrics to classify the documents in\nthe PLBS and PLSQBS data sets into one of the four BS\nor seven SQBS classes?\n11In the tf-idf (term frequency - inverse document frequency) weight-\ning scheme [9], the term frequency count is compared to an inverse doc-\nument frequency count, which measures the number of occurrences of a\nword in the entire corpus. Thus the tf-idf transforms the document into a\nweighted vector that assigns higher value to terms that have high occur-\nrences in a small number of documents.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 401We use several varieties of vector representation for\ndocuments in our classiﬁcation experiment, as described\nabove: (1) BOW; (2) tf-idf BOW; (3) Paragraph Vec-\ntors (D2V); (4) document-topic distribution from the topic\nmodels (TM) we derived in Section 4. The document-\ntopic distribution can be seen as a very low-dimension\nrepresentation of a document, which has been shown to\nperform well in document classiﬁcation tasks in place of\nBOW representation [1]. For Paragraph Vectors, we train\ndocument embeddings on our PL* data sets using the\nDoc2Vec (D2V) implementation available in the Python\nlibrary gensim . The resulting embeddings has 100 di-\nmensions for each document. We use Support Vector Ma-\nchine (SVM) with RBF kernel for all classiﬁcation experi-\nments.\n5.3 Document Classiﬁcation Results\nFor the PLSQBS and PLBS data sets, we present the docu-\nment classiﬁcation accuracy12in Table 5. We observe that\ndocument classiﬁcation accuracy scores are signiﬁcantly\nabove chance in both data sets (chance being 1=7 == 0:14\nin PLSQBS data set and 1=4 == 0:25in PLBS data\nset), with best accuracy scores of 0.41 and 0.53. This\nindicates that supervised learning is able to capture the\nimportant features that distinguish the different banshi or\nshengqiang-banshi classes, which in these particular JLC\ndata sets, are somewhat unintuitive even for human judg-\nments13.\nContrasting the performance of different features, we\nnote that tf-idf is more effective than BOW, as expected.\nThe D2V Paragraph Vectors achieves comparable or lower\nresults with the tf-idf (200 times higher in dimension).\nThis is somewhat unexpected since these Paragraph Vec-\ntors are supposed to be a higher quality vector represen-\ntation that captures both semantic similarity and word or-\nder that is absent from BOW representations such as tf-\nidf [4]. We attribute this under-performance of D2V to\nthe smaller amount of training data available in the PL*\ndata sets (comparing to much larger general-domain train-\ning corpora used in literature for D2V). In the mean time,\nwe observe that the topic models features are ineffective at\ncapturing the document-level distinctions.\n6. CONCLUSION\nIn this work, we have introduced the Jingju Lyrics Col-\nlection, a comprehensive data collection of jingju lyrics\nenriched and re-structured with several extracted datasets\nbased on musicological considerations. Utilizing this data,\nwe performed topic modeling in order to explore the topic\nstructures of the jingju lyrics as related to different ban-\nshiand SQBS types. The results show that while the\ntopics are in general reasonable, the distinctions between\n12The datasets are well balanced in their class sizes therefore accuracy\nis an appropriate measure.\n13Here we are referring to a layperson who is a native speaker of Chi-\nnese but may not be a jingju expert. We are yet to evaluate this task on\njingju experts.Dataset Feature Accuracy\nPLBS BOW (20000) 0.481\nPLBS tf-idf (20000) 0.527\nPLBS D2V (100) 0.528\nPLBS TM (20) 0.274\nPLSQBS BOW (20000) 0.356\nPLSQBS tf-idf (20000) 0.408\nPLSQBS D2V (100) 0.347\nPLSQBS TM (20) 0.112\nTable 5 . Document classiﬁcation with SVM RBF kernel,\nwith dimensionality of features shown in parenthesis\ndifferent banshi are less contrastive than between differ-\nentshengqiang-banshi14. Document classiﬁcation exper-\niments are carried out to further understand the association\nwithin a supervised setting. The strong results in document\nclassiﬁcation support the associations between the expres-\nsive functions (as expressed in the lyrics) and the banshi or\nshengqiang-banshi (SQBS) categories.\nWe observe that unexpectedly, neither D2V Paragraph\nVectors nor the topic models are more effective at docu-\nment classiﬁcation than the high-dimension tf-idf vectors.\nWe postulate that these have several implications (even\nthough our goal and contribution in this work do not lie\nin the use of these more advanced representations). First,\nit shows that latent topics may not be the most effective\nway to capture the different expressive functions in differ-\nentbanshi types (as opposed to, e.g., sentiment). It maybe\nof interest to perform feature and error analysis to under-\nstand what components of the document classiﬁcation have\nmade it more effective (e.g., sentiment polarity words, etc).\nSecond, in addition to the training size problem discussed\nin Section 5.3, we attribute lower performance of D2V/TM\nto the potential errors in the NLP pipeline applied to the\ncorpus, especially the Chinese segmentation (as already\nmentioned in Section 3.2). This includes two aspects: ﬁrst,\nthe automatic segmentation may introduce errors even for\nstandard Chinese text; second, the language of jingju falls\nsomewhere between modern and archaic Chinese, making\nit more challenging to segment automatically using a stan-\ndard segmenter trained on modern language. Our on-going\nand future work, therefore, includes making corrections to\nthe segmentation in the JLC while keeping expanding the\ncollection.\n7. ACKNOWLEDGEMENTS\nThis research is funded by the European Research Council\nunder the European Union’s Seventh Framework Program\n(FP7/2007- 2013), as part of the CompMusic project (ERC\ngrant agreement 267583).\n14Our main goal in this paper is to investigate the expressive functions\nofbanshi . Even though the result shows shengqiang ’s importance in dis-\ntinguishing positive from negative sentiments, we note that shengqiang-\nbanshi is still a unique form combining both shengqiang andbanshi .402 Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 20178. REFERENCES\n[1] David M Blei, Andrew Y Ng, and Michael I Jordan.\nLatent Dirichlet Allocation. Journal of Machine Learn-\ning Research , 3(4-5):993–1022, 2012.\n[2] Cao B.. Jingju changqiang banshi jiedu (Deciphering\nbanshi in jingju singing) . Renmin yinyue chubanshe,\nBeijing, 2010.\n[3] Liu J.. Jingju yinyue gailun(Introduction to jingju mu-\nsic). Renmin yinyue chubanshe, Beijing, 1998.\n[4] Quoc V . Le and Tomas Mikolov. Distributed represen-\ntations of sentences and documents. In Proceedings of\nthe 31th International Conference on Machine Learn-\ning, ICML 2014, Beijing, China, 21-26 June 2014 ,\npages 1188–1196, 2014.\n[5] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. Distributed representations\nof words and phrases and their compositionality. In\nAdvances in Neural Information Processing Systems\n26: 27th Annual Conference on Neural Information\nProcessing Systems 2013. Proceedings of a meeting\nheld December 5-8, 2013, Lake Tahoe, Nevada, United\nStates. , pages 3111–3119, 2013.\n[6] Rafael Caro Repetto, Rong Gong, Nadine Kroher, and\nXavier Serra. Comparison of the singing style of two\njingju schools. In 16th International Society for Music\nInformation Retrieval (ISMIR) Conference , pages 507–\n513, M ´alaga, Spain, 26/10/2015 2015.\n[7] Rafael Caro Repetto and Xavier Serra. Creating a cor-\npus of jingju (beijing opera) music and possibilities for\nmelodic analysis. In 15th International Society for Mu-\nsic Information Retrieval Conference , pages 313–318,\nTaipei, Taiwan, 27/10/2014 2014.\n[8] Rafael Caro Repetto and Xavier Serra. Melodic trans-\nformation processes in the arrangements of jingju ban-\nshi. In Fourth International Conference On Analytical\nApproaches To World Music (AAWM 2016) , New York,\nUSA, 08/06/2016 2016.\n[9] Gerard Salton and Michael J. McGill. Introduction\nto Modern Information Retrieval . McGraw-Hill, Inc.,\nNew York, NY , USA, 1986.\n[10] Huihsin Tseng. A conditional random ﬁeld word seg-\nmenter. In In Fourth SIGHAN Workshop on Chinese\nLanguage Processing , 2005.\n[11] Elizabeth Wichmann. Listening to theatre: the au-\nral dimension of Beijing opera . University of Hawaii\nPress, 1991.\n[12] Shuo Zhang, Rafael Caro Repetto, and Xavier Serra.\nStudy of the similarity between linguistic tones and\nmelodic pitch contours in beijing opera singing. In\n15th International Society for Music Information Re-\ntrieval Conference , pages 343–348, Taipei, Taiwan,\n27/10/2014 2014.[13] Shuo Zhang, Rafael Caro Repetto, and Xavier Serra.\nPredicting pairwise pitch contour relations based on\nlinguistic tone information in beijing opera singing. In\n16th International Society for Music Information Re-\ntrieval (ISMIR) Conference , pages 107–113, Malaga,\nSpain, 26/10/2015 2015.\n[14] Zhang Z. . Jingju chuantongxi pihuang changqiang\njiegou fenxi (Structural analysis of pihuang singing\nin jingju traditional plays) . Renmin yinyue chubanshe,\nBeijing, 1981.\n[15] Jiang J. . Zhongguo xiqu yinyue (Music of Chinese\ntraditional opera) . Renmin yinyue chubanshe, Beijing,\n2000.Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017 403"
    },
    {
        "title": "Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017",
        "author": [
            "Sally Jo Cunningham",
            "Zhiyao Duan",
            "Xiao Hu 0001",
            "Douglas Turnbull"
        ],
        "year": "2017",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": null,
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2017"
    }
]